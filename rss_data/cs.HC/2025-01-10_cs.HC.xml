<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Jan 2025 05:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Balancing Exploration and Cybersickness: Investigating Curiosity-Driven Behavior in Virtual Environments</title>
      <link>https://arxiv.org/abs/2501.04905</link>
      <description>arXiv:2501.04905v1 Announce Type: new 
Abstract: During virtual navigation, users exhibit varied interaction and navigation behaviors influenced by several factors. Existing theories and models have been developed to explain and predict these diverse patterns. While users often experience uncomfortable sensations, such as cybersickness, during virtual reality (VR) use, they do not always make optimal decisions to mitigate these effects. Although methods like reinforcement learning have been used to model decision-making processes, they typically rely on random selection to simulate actions, failing to capture the complexities of real navigation behavior. In this study, we propose curiosity as a key factor driving irrational decision-making, suggesting that users continuously balance exploration and cybersickness according to the free energy principle during virtual navigation. Our findings show that VR users generally adopt conservative strategies when navigating, with most participants displaying negative curiosity across trials. However, curiosity levels tend to rise when the virtual environment changes, illustrating the dynamic interplay between exploration and discomfort. This study provides a quantitative approach to decoding curiosity-driven behavior during virtual navigation, offering insights into how users balance exploration and the avoidance of cybersickness. Future research will further refine this model by incorporating additional psychological and environmental factors to improve the accuracy of navigation pattern predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04905v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tangyao Li, Yuyang Wang</dc:creator>
    </item>
    <item>
      <title>What Drives You to Interact?: The Role of User Motivation for a Robot in the Wild</title>
      <link>https://arxiv.org/abs/2501.04929</link>
      <description>arXiv:2501.04929v1 Announce Type: new 
Abstract: In this paper, we aim to understand how user motivation shapes human-robot interaction (HRI) in the wild. To explore this, we conducted a field study by deploying a fully autonomous conversational robot in a shopping mall over two days. Through sequential video analysis, we identified five patterns of interaction fluency (Smooth, Awkward, Active, Messy, and Quiet), four types of user motivation for interacting with the robot (Function, Experiment, Curiosity, and Education), and user positioning towards the robot. We further analyzed how these motivations and positioning influence interaction fluency. Our findings suggest that incorporating users' motivation types into the design of robot behavior can enhance interaction fluency, engagement, and user satisfaction in real-world HRI scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04929v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amy Koike, Yuki Okafuji, Kenya Hoshimure, Jun Baba</dc:creator>
    </item>
    <item>
      <title>Music and art: a study in cross-modal interpretation</title>
      <link>https://arxiv.org/abs/2501.05101</link>
      <description>arXiv:2501.05101v1 Announce Type: new 
Abstract: Our study has investigated the effect of music on the experience of viewing art, investigating the factors which create a sense of connectivity between the two forms. We worked with 138 participants, and included multiple choice and open-ended questions. For the latter, we performed both a qualitative analysis and also sentiment analysis using text-mining. We investigated the relationship between the user experience and the emotions in the artwork and music. We found that, besides emotion, theme, story, and to a lesser extent music tempo were factors which helped form connections between artwork and music. Overall, participants rated the music as being helpful in developing an appreciation of the art. We propose guidelines for using music to enhance the experience of viewing art, and we propose directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05101v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Paul Warren, Paul Mulholland, Naomi Barker</dc:creator>
    </item>
    <item>
      <title>"What's Happening"- A Human-centered Multimodal Interpreter Explaining the Actions of Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2501.05322</link>
      <description>arXiv:2501.05322v1 Announce Type: new 
Abstract: Public distrust of self-driving cars is growing. Studies emphasize the need for interpreting the behavior of these vehicles to passengers to promote trust in autonomous systems. Interpreters can enhance trust by improving transparency and reducing perceived risk. However, current solutions often lack a human-centric approach to integrating multimodal interpretations. This paper introduces a novel Human-centered Multimodal Interpreter (HMI) system that leverages human preferences to provide visual, textual, and auditory feedback. The system combines a visual interface with Bird's Eye View (BEV), map, and text display, along with voice interaction using a fine-tuned large language model (LLM). Our user study, involving diverse participants, demonstrated that the HMI system significantly boosts passenger trust in AVs, increasing average trust levels by over 8%, with trust in ordinary environments rising by up to 30%. These results underscore the potential of the HMI system to improve the acceptance and reliability of autonomous vehicles by providing clear, real-time, and context-sensitive explanations of vehicle actions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05322v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuewen Luo, Fan Ding, Ruiqi Chen, Rishikesh Panda, Junnyong Loo, Shuyun Zhang</dc:creator>
    </item>
    <item>
      <title>Video-Conferencing Beyond Screen-Sharing and Thumbnail Webcam Videos: Gesture-Aware Augmented Reality Video for Data-Rich Remote Presentations</title>
      <link>https://arxiv.org/abs/2501.05345</link>
      <description>arXiv:2501.05345v1 Announce Type: new 
Abstract: Synchronous data-rich conversations are commonplace within enterprise organizations, taking place at varying degrees of formality between stakeholders at different levels of data literacy. In these conversations, representations of data are used to analyze past decisions, inform future course of action, as well as persuade customers, investors, and executives. However, it is difficult to conduct these conversations between remote stakeholders due to poor support for presenting data when video-conferencing, resulting in disappointing audience experiences. In this position statement, I reflect on our recent work incorporating multimodal interaction and augmented reality video, suggesting that video-conferencing does not need to be limited to screen-sharing and relegating a speaker's video to a separate thumbnail view. I also comment on future research directions and collaboration opportunities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05345v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Brehmer</dc:creator>
    </item>
    <item>
      <title>Modeling User Preference in Single-Finger Grasping Interactions</title>
      <link>https://arxiv.org/abs/2501.05434</link>
      <description>arXiv:2501.05434v1 Announce Type: new 
Abstract: Grasp User Interfaces (GPUIs) are well-suited for dual-tasking between virtual and physical tasks. These interfaces do not require users to release handheld objects, supporting microinteractions that happen in short bursts and cause minimal interruption on the physical task. Design approaches for these interfaces include user elicitation studies and expert-based strategies, which can be combined with computational techniques for quicker and more cost-effective iterations. Current computational tools for designing GPUIs utilize simulations based on kinematic, geometric, and biomechanical parameters. However, the relationship between these low-level factors and higher-level user preferences remains underexplored. In this study, we gathered user preferences using a two-alternative forced choice paradigm with single-finger reach tasks performed while holding objects representative of real-world activities with different grasp types. We present a quantitative analysis of how various low-level factors influence user preference in grasp interactions, identifying the most significant ones. Leveraging this analysis, we developed a predictive model to estimate user preference and integrated it into an existing simulation tool for GPUI design. In addition to enhancing the understanding of design factors in grasp interactions, our predictive model provides a spatial utility metric based on user preferences, paving the way for adaptive GPUI and mixed-initiative systems for better dual-tasking between virtual and physical environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05434v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur Caetano, Yunhao Luo, Misha Sra</dc:creator>
    </item>
    <item>
      <title>Improving Human-Robot Teaching by Quantifying and Reducing Mental Model Mismatch</title>
      <link>https://arxiv.org/abs/2501.04755</link>
      <description>arXiv:2501.04755v1 Announce Type: cross 
Abstract: The rapid development of artificial intelligence and robotics has had a significant impact on our lives, with intelligent systems increasingly performing tasks traditionally performed by humans. Efficient knowledge transfer requires matching the mental model of the human teacher with the capabilities of the robot learner. This paper introduces the Mental Model Mismatch (MMM) Score, a feedback mechanism designed to quantify and reduce mismatches by aligning human teaching behavior with robot learning behavior. Using Large Language Models (LLMs), we analyze teacher intentions in natural language to generate adaptive feedback. A study with 150 participants teaching a virtual robot to solve a puzzle game shows that intention-based feedback significantly outperforms traditional performance-based feedback or no feedback. The results suggest that intention-based feedback improves instructional outcomes, improves understanding of the robot's learning process and reduces misconceptions. This research addresses a critical gap in human-robot interaction (HRI) by providing a method to quantify and mitigate discrepancies between human mental models and robot capabilities, with the goal of improving robot learning and human teaching effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04755v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phillip Richter, Heiko Wersing, Anna-Lisa Vollmer</dc:creator>
    </item>
    <item>
      <title>Exploring the Use of Robots for Diary Studies</title>
      <link>https://arxiv.org/abs/2501.04860</link>
      <description>arXiv:2501.04860v1 Announce Type: cross 
Abstract: As interest in studying in-the-wild human-robot interaction grows, there is a need for methods to collect data over time and in naturalistic or potentially private environments. HRI researchers have increasingly used the diary method for these studies, asking study participants to self-administer a structured data collection instrument, i.e., a diary, over a period of time. Although the diary method offers a unique window into settings that researchers may not have access to, they also lack the interactivity and probing that interview-based methods offer. In this paper, we explore a novel data collection method in which a robot plays the role of an interactive diary. We developed the Diary Robot system and performed in-home deployments for a week to evaluate the feasibility and effectiveness of this approach. Using traditional text-based and audio-based diaries as benchmarks, we found that robots are able to effectively elicit the intended information. We reflect on our findings, and describe scenarios where the utilization of robots in diary studies as a data collection instrument may be especially applicable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04860v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael F. Xu, Bilge Mutlu</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence in Environmental Protection: The Importance of Organizational Context from a Field Study in Wisconsin</title>
      <link>https://arxiv.org/abs/2501.04902</link>
      <description>arXiv:2501.04902v1 Announce Type: cross 
Abstract: Advances in Artificial Intelligence (AI) have generated widespread enthusiasm for the potential of AI to support our understanding and protection of the environment. As such tools move from basic research to more consequential settings, such as regulatory enforcement, the human context of how AI is utilized, interpreted, and deployed becomes increasingly critical. Yet little work has systematically examined the role of such organizational goals and incentives in deploying AI systems. We report results from a unique case study of a satellite imagery-based AI tool to detect dumping of agricultural waste, with concurrent field trials with the Wisconsin Department of Natural Resources (WDNR) and a non-governmental environmental interest group in which the tool was utilized for field investigations when dumping was presumptively illegal in February-March 2023. Our results are threefold: First, both organizations confirmed a similar level of ground-truth accuracy for the model's detections. Second, they differed, however, in their overall assessment of its usefulness, as WDNR was interested in clear violations of existing law, while the interest group sought to document environmental risk beyond the scope of existing regulation. Dumping by an unpermitted entity or just before February 1, for instance, were deemed irrelevant by WDNR. Third, while AI tools promise to prioritize allocation of environmental protection resources, they may expose important gaps of existing law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04902v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Rothbacher, Kit T. Rodolfa, Mihir Bhaskar, Erin Maneri, Christine Tsang, Daniel E. Ho</dc:creator>
    </item>
    <item>
      <title>OfficeMate: Pilot Evaluation of an Office Assistant Robot</title>
      <link>https://arxiv.org/abs/2501.05141</link>
      <description>arXiv:2501.05141v1 Announce Type: cross 
Abstract: Office Assistant Robots (OARs) offer a promising solution to proactively provide in-situ support to enhance employee well-being and productivity in office spaces. We introduce OfficeMate, a social OAR designed to assist with practical tasks, foster social interaction, and promote health and well-being. Through a pilot evaluation with seven participants in an office environment, we found that users see potential in OARs for reducing stress and promoting healthy habits and value the robot's ability to provide companionship and physical activity reminders in the office space. However, concerns regarding privacy, communication, and the robot's interaction timing were also raised. The feedback highlights the need to carefully consider the robot's appearance and behaviour to ensure it enhances user experience and aligns with office social norms. We believe these insights will better inform the development of adaptive, intelligent OAR systems for future office space integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05141v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiahe Pan, Sarah Sch\"ombs, Yan Zhang, Ramtin Tabatabaei, Muhammad Bilal, Wafa Johal</dc:creator>
    </item>
    <item>
      <title>Assisting MoCap-Based Teleoperation of Robot Arm using Augmented Reality Visualisations</title>
      <link>https://arxiv.org/abs/2501.05153</link>
      <description>arXiv:2501.05153v1 Announce Type: cross 
Abstract: Teleoperating a robot arm involves the human operator positioning the robot's end-effector or programming each joint. Whereas humans can control their own arms easily by integrating visual and proprioceptive feedback, it is challenging to control an external robot arm in the same way, due to its inconsistent orientation and appearance. We explore teleoperating a robot arm through motion-capture (MoCap) of the human operator's arm with the assistance of augmented reality (AR) visualisations. We investigate how AR helps teleoperation by visualising a virtual reference of the human arm alongside the robot arm to help users understand the movement mapping. We found that the AR overlay of a humanoid arm on the robot in the same orientation helped users learn the control. We discuss findings and future work on MoCap-based robot teleoperation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05153v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiushi Zhou, Antony Chacon, Jiahe Pan, Wafa Johal</dc:creator>
    </item>
    <item>
      <title>Human Delegation Behavior in Human-AI Collaboration: The Effect of Contextual Information</title>
      <link>https://arxiv.org/abs/2401.04729</link>
      <description>arXiv:2401.04729v3 Announce Type: replace 
Abstract: The integration of artificial intelligence (AI) into human decision-making processes at the workplace presents both opportunities and challenges. One promising approach to leverage existing complementary capabilities is allowing humans to delegate individual instances of decision tasks to AI. However, enabling humans to delegate instances effectively requires them to assess several factors. One key factor is the analysis of both their own capabilities and those of the AI in the context of the given task. In this work, we conduct a behavioral study to explore the effects of providing contextual information to support this delegation decision. Specifically, we investigate how contextual information about the AI and the task domain influence humans' delegation decisions to an AI and their impact on the human-AI team performance. Our findings reveal that access to contextual information significantly improves human-AI team performance in delegation settings. Finally, we show that the delegation behavior changes with the different types of contextual information. Overall, this research advances the understanding of computer-supported, collaborative work and provides actionable insights for designing more effective collaborative systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04729v3</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Philipp Spitzer, Joshua Holstein, Patrick Hemmer, Michael V\"ossing, Niklas K\"uhl, Dominik Martin, Gerhard Satzger</dc:creator>
    </item>
    <item>
      <title>Driving Towards Inclusion: A Systematic Review of AI-powered Accessibility Enhancements for People with Disability in Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2401.14571</link>
      <description>arXiv:2401.14571v2 Announce Type: replace 
Abstract: This paper provides a comprehensive and, to our knowledge, the first review of inclusive human-computer interaction (HCI) within autonomous vehicles (AVs) and human-driven cars with partial autonomy, emphasizing accessibility and user-centered design principles. We explore the current technologies and HCI systems designed to enhance passenger experience, particularly for individuals with accessibility needs. Key technologies discussed include brain-computer interfaces, anthropomorphic interaction, virtual reality, augmented reality, mode adaptation, voice-activated interfaces, haptic feedback, etc. Each technology is evaluated for its role in creating an inclusive in-vehicle environment. Furthermore, we highlight recent interface designs by leading companies and review emerging concepts and prototypes under development or testing, which show significant potential to address diverse accessibility requirements. Safety considerations, ethical concerns, and adoption of AVs are other major issues that require thorough investigation. Building on these findings, we propose an end-to-end design framework that addresses accessibility requirements across diverse user demographics, including older adults and individuals with physical or cognitive impairments. This work provides actionable insights for designers, researchers, and policymakers aiming to create safer and more comfortable environments in autonomous and regular vehicles accessible to all users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14571v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashish Bastola, Hao Wang, Sayed Pedram Haeri Boroujeni, Julian Brinkley, Ata Jahangir Moshayedi, Abolfazl Razi</dc:creator>
    </item>
    <item>
      <title>A Multi-Label EEG Dataset for Mental Attention State Classification in Online Learning</title>
      <link>https://arxiv.org/abs/2411.09879</link>
      <description>arXiv:2411.09879v2 Announce Type: replace 
Abstract: Attention is a vital cognitive process in the learning and memory environment, particularly in the context of online learning. Traditional methods for classifying attention states of online learners based on behavioral signals are prone to distortion, leading to increased interest in using electroencephalography (EEG) signals for authentic and accurate assessment. However, the field of attention state classification based on EEG signals in online learning faces challenges, including the scarcity of publicly available datasets, the lack of standardized data collection paradigms, and the requirement to consider the interplay between attention and other psychological states. In light of this, we present the Multi-label EEG dataset for classifying Mental Attention states (MEMA) in online learning. We meticulously designed a reliable and standard experimental paradigm with three attention states: neutral, relaxing, and concentrating, considering human physiological and psychological characteristics. This paradigm collected EEG signals from 20 subjects, each participating in 12 trials, resulting in 1,060 minutes of data. Emotional state labels, basic personal information, and personality traits were also collected to investigate the relationship between attention and other psychological states. Extensive quantitative and qualitative analysis, including a multi-label correlation study, validated the quality of the EEG attention data. The MEMA dataset and analysis provide valuable insights for advancing research on attention in online learning. The dataset is publicly available at \url{https://github.com/GuanjianLiu/MEMA}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09879v2</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Liu, Yuzhe Zhang, Guanjian Liu, Xinxin Du, Haochong Wang, Dalin Zhang</dc:creator>
    </item>
    <item>
      <title>Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education</title>
      <link>https://arxiv.org/abs/2501.01192</link>
      <description>arXiv:2501.01192v2 Announce Type: replace 
Abstract: Early childhood science education is crucial for developing scientific literacy, yet translating complex scientific concepts into age-appropriate content remains challenging for educators. Our study evaluates four leading Large Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their ability to generate preschool-appropriate scientific explanations across biology, chemistry, and physics. Through systematic evaluation by 30 nursery teachers using established pedagogical criteria, we identify significant differences in the models' capabilities to create engaging, accurate, and developmentally appropriate content. Unexpectedly, Claude outperformed other models, particularly in biological topics, while all LLMs struggled with abstract chemical concepts. Our findings provide practical insights for educators leveraging AI in early science education and offer guidance for developers working to enhance LLMs' educational applications. The results highlight the potential and current limitations of using LLMs to bridge the early childhood science literacy gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01192v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Annika Bush, Amin Alibakhshi</dc:creator>
    </item>
    <item>
      <title>WeAudit: Scaffolding User Auditors and AI Practitioners in Auditing Generative AI</title>
      <link>https://arxiv.org/abs/2501.01397</link>
      <description>arXiv:2501.01397v3 Announce Type: replace 
Abstract: There has been growing interest from both practitioners and researchers in engaging end users in AI auditing, to draw upon users' unique knowledge and lived experiences. However, we know little about how to effectively scaffold end users in auditing in ways that can generate actionable insights for AI practitioners. Through formative studies with both users and AI practitioners, we first identified a set of design goals to support user-engaged AI auditing. We then developed WeAudit, a workflow and system that supports end users in auditing AI both individually and collectively. We evaluated WeAudit through a three-week user study with user auditors and interviews with industry Generative AI practitioners. Our findings offer insights into how WeAudit supports users in noticing and reflecting upon potential AI harms and in articulating their findings in ways that industry practitioners can act upon. Based on our observations and feedback from both users and practitioners, we identify several opportunities to better support user engagement in AI auditing processes. We discuss implications for future research to support effective and responsible user engagement in AI auditing and red-teaming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01397v3</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wesley Hanwen Deng, Wang Claire, Howard Ziyu Han, Jason I. Hong, Kenneth Holstein, Motahhare Eslami</dc:creator>
    </item>
    <item>
      <title>LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs</title>
      <link>https://arxiv.org/abs/2307.10168</link>
      <description>arXiv:2307.10168v3 Announce Type: replace-cross 
Abstract: LLMs have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities. However, current efforts focus mainly on simple atomic tasks. We explore whether LLMs can replicate more complex crowdsourcing pipelines. We find that modern LLMs can simulate some of crowdworkers' abilities in these ``human computation algorithms,'' but the level of success is variable and influenced by requesters' understanding of LLM capabilities, the specific skills required for sub-tasks, and the optimal interaction modality for performing these sub-tasks. We reflect on human and LLMs' different sensitivities to instructions, stress the importance of enabling human-facing safeguards for LLMs, and discuss the potential of training humans and LLMs with complementary skill sets. Crucially, we show that replicating crowdsourcing pipelines offers a valuable platform to investigate 1) the relative LLM strengths on different tasks (by cross-comparing their performances on sub-tasks) and 2) LLMs' potential in complex tasks, where they can complete part of the tasks while leaving others to humans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10168v3</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3706690</arxiv:DOI>
      <arxiv:journal_reference>CHI 2025 Case Study Track</arxiv:journal_reference>
      <dc:creator>Tongshuang Wu, Haiyi Zhu, Maya Albayrak, Alexis Axon, Amanda Bertsch, Wenxing Deng, Ziqi Ding, Bill Guo, Sireesh Gururaja, Tzu-Sheng Kuo, Jenny T. Liang, Ryan Liu, Ihita Mandal, Jeremiah Milbauer, Xiaolin Ni, Namrata Padmanabhan, Subhashini Ramkumar, Alexis Sudjianto, Jordan Taylor, Ying-Jui Tseng, Patricia Vaidos, Zhijin Wu, Wei Wu, Chenyang Yang</dc:creator>
    </item>
    <item>
      <title>Generative manufacturing systems using diffusion models and ChatGPT</title>
      <link>https://arxiv.org/abs/2405.00958</link>
      <description>arXiv:2405.00958v2 Announce Type: replace-cross 
Abstract: In this study, we introduce Generative Manufacturing Systems (GMS) as a novel approach to effectively manage and coordinate autonomous manufacturing assets, thereby enhancing their responsiveness and flexibility to address a wide array of production objectives and human preferences. Deviating from traditional explicit modeling, GMS employs generative AI, including diffusion models and ChatGPT, for implicit learning from envisioned futures, marking a shift from a model-optimum to a training-sampling decision-making. Through the integration of generative AI, GMS enables complex decision-making through interactive dialogue with humans, allowing manufacturing assets to generate multiple high-quality global decisions that can be iteratively refined based on human feedback. Empirical findings showcase GMS's substantial improvement in system resilience and responsiveness to uncertainties, with decision times reduced from seconds to milliseconds. The study underscores the inherent creativity and diversity in the generated solutions, facilitating human-centric decision-making through seamless and continuous human-machine interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00958v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xingyu Li, Fei Tao, Wei Ye, Aydin Nassehi, John W. Sutherland</dc:creator>
    </item>
    <item>
      <title>3D Printing of Near-Ambient Responsive Liquid Crystal Elastomers with Enhanced Nematic Order and Pluralized Transformation</title>
      <link>https://arxiv.org/abs/2411.06931</link>
      <description>arXiv:2411.06931v2 Announce Type: replace-cross 
Abstract: Liquid Crystal Elastomers with near-ambient temperature-responsiveness (NAT-LCEs) have been extensively studied for building bio-compatible, low-power consumption devices and robotics. However, conventional manufacturing methods face limitations in programmability (e.g., molding) or low nematic order (e.g., DIW printing). Here, a hybrid cooling strategy is proposed for programmable 3D printing of NAT-LCEs with enhanced nematic order, intricate shape forming, and morphing capability. By integrating a low-temperature nozzle and a cooling platform into a 3D printer, the resulting temperature field synergistically facilitates mesogen alignment during extrusion and disruption-free UV cross-linking. This method achieves a nematic order 3000% higher than NAT-LCEs fabricated using traditional room temperature 3D printing. Enabled by shifting of transition temperature during hybrid cooling printing, printed sheets spontaneously turn into 3D structures after release from the platform, exhibiting bidirectional deformation with heating and cooling. By adjusting the nozzle and plate temperatures, NAT-LCEs with graded properties can be fabricated for intricate shape morphing. A wristband system with enhanced heart rate monitoring is also developed based on 3D-printed NAT-LCE. Our method may open new possibilities for soft robotics, biomedical devices, and wearable electronics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06931v2</guid>
      <category>cond-mat.soft</category>
      <category>cs.HC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongxiao Li, Yuxuan Sun, Xingjian Li, Xingxiang Li, Zhengqing Zhu, Boxi Sun, Shutong Nong, Jiyang Wu, Tingrui Pan, Weihua Li, Shiwu Zhang, Mujun Li</dc:creator>
    </item>
  </channel>
</rss>
