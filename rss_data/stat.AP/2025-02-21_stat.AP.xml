<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Feb 2025 05:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian Parameter Inference and Uncertainty Quantification for a Computational Pulmonary Hemodynamics Model Using Gaussian Processes</title>
      <link>https://arxiv.org/abs/2502.14251</link>
      <description>arXiv:2502.14251v1 Announce Type: new 
Abstract: Patient-specific modeling is a valuable tool in cardiovascular disease research, offering insights beyond what current clinical equipment can measure. Given the limitations of available clinical data, models that incorporate uncertainty can provide clinicians with better guidance for tailored treatments. However, such modeling must align with clinical time frameworks to ensure practical applicability. In this study, we employ a one-dimensional fluid dynamics model integrated with data from a canine model of chronic thromboembolic pulmonary hypertension (CTEPH) to investigate microvascular disease, which is believed to involve complex mechanisms. To enhance computational efficiency during model calibration, we implement a Gaussian process emulator. This approach enables us to explore the relationship between disease severity and microvascular parameters, offering new insights into the progression and treatment of CTEPH in a timeframe that is compatible with a reasonable clinical timeframe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14251v1</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <category>physics.bio-ph</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amirreza Kachabi, Sofia Altieri Correa, Naomi C. Chesler, Mitchel J. Colebank</dc:creator>
    </item>
    <item>
      <title>Advancing Model-Assisted Design in Phase I Trials through Bayesian Dose-Response Model</title>
      <link>https://arxiv.org/abs/2502.14278</link>
      <description>arXiv:2502.14278v1 Announce Type: new 
Abstract: Model-assisted designs have garnered significant attention in recent years due to their high accuracy in identifying the maximum tolerated dose (MTD) and their operational simplicity. To identify the MTD, they employ estimated dose limiting toxicity (DLT) probabilities via isotonic regression with pool-adjacent violators algorithm (PAVA) after trials have been completed. PAVA adjusts independently estimated DLT probabilities with the Bayesian binomial model at each dose level using posterior variances ensure the monotonicity that toxicity increases with dose. However, in small sample settings such as Phase I oncology trials, this approach can lead to unstable DLT probability estimates and reduce MTD selection accuracy. To address this problem, we propose a novel MTD identification strategy in model-assisted designs that leverages a Bayesian dose-response model. Employing the dose-response model allows for stable estimation of the DLT probabilities under the monotonicity by borrowing information across dose levels, leading to an improvement in MTD identification accuracy. We discuss the specification of prior distributions that can incorporate information from similar trials or the absence of such information. We examine dose-response models employing logit, log-log, and complementary log-log link functions to assess the impact of link function differences on the accuracy of MTD selection. Through extensive simulations, we demonstrate that the proposed approach improves MTD selection accuracy by more than 10\% in some scenarios and by approximately 6\% on average compared to conventional approach. These findings indicate that the proposed approach can contribute to further enhancing the efficiency of Phase I oncology trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14278v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rentaro Wakayama, Tomotaka Momozaki, Shuji Ando</dc:creator>
    </item>
    <item>
      <title>Spatially Varying Coefficient Models for Estimating Heterogeneous Mixture Effects</title>
      <link>https://arxiv.org/abs/2502.14651</link>
      <description>arXiv:2502.14651v1 Announce Type: new 
Abstract: Recent studies of associations between environmental exposures and health outcomes have shifted toward estimating the effect of simultaneous exposure to multiple chemicals. Summary index methods, such as the weighted quantile sum and quantile g-computation, are now commonly used to analyze environmental exposure mixtures in a broad range of applications. These methods provide a simple and interpretable framework for quantifying mixture effects. However, when data arise from a large geographical study region, it may be unreasonable to expect a common mixture effect. In this work, we explore the use of a recently developed spatially varying coefficient model based on Bayesian additive regression trees to estimate spatially heterogeneous mixture effects using quantile g-computation. We conducted simulation studies to evaluate the method's performance. We then applied this model to an analysis of multiple ambient air pollutants and birthweight in Georgia, USA from 2005-2016. We find evidence of county-level spatially varying mixture associations, where for 17 of 159 counties in Georgia, elevated concentrations of a mixture of PM2.5, nitrogen dioxide, sulfur dioxide, ozone, and carbon monoxide were associated with a reduction in birthweight by as much as -16.65 grams (95% credible interval: -33.93, -0.40) per decile increase in all five air pollutants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14651v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob Englert, Howard Chang</dc:creator>
    </item>
    <item>
      <title>Estimation and Evaluation of the Resource-Constrained Optimal Dynamic Treatment Rule: An Application to HIV Care Retention</title>
      <link>https://arxiv.org/abs/2502.14763</link>
      <description>arXiv:2502.14763v1 Announce Type: new 
Abstract: The optimal strategy for deploying a treatment in a population may recommend giving all in the population that treatment. Such a strategy may not be feasible, especially in resource-limited settings. One approach for determining how to allocate a treatment in such settings is the resource-constrained optimal dynamic treatment rule (RC ODTR) SuperLearner algorithm, developed by Luedtke and van der Laan. In this paper, we describe this algorithm, offer various novel approaches for presenting the RC ODTR and its value in terms of benefit and cost, and provide practical guidance on implementing the algorithm (including software). In particular, we apply this method to the Adaptive Strategies for Preventing and Treating Lapses of Retention in HIV care (NCT02338739) trial to determine how to best allocate conditional cash transfers (CCTs) for increasing HIV care adherence given varying constraints on the proportion of people who can receive CCTs in the population, providing one of the few applied illustrations of this method and novel substantive findings. We find that there are clinical and monetary benefits to deploying CCTs to a small percent (e.g., 10\%) of the population compared to administering the care standard for all; however, results suggest that these incremental benefits are only due to the loosening of constraints, rather than a presence of treatment effect heterogeneity strong enough to drive a more efficient and effective constrained allocation approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14763v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lina M. Montoya, Elvin H. Geng, Harriet F. Adhiambo, Maya L. Petersen</dc:creator>
    </item>
    <item>
      <title>Online detection of forecast model inadequacies using forecast errors</title>
      <link>https://arxiv.org/abs/2502.14173</link>
      <description>arXiv:2502.14173v1 Announce Type: cross 
Abstract: In many organisations, accurate forecasts are essential for making informed decisions for a variety of applications from inventory management to staffing optimization. Whatever forecasting model is used, changes in the underlying process can lead to inaccurate forecasts, which will be damaging to decision-making. At the same time, models are becoming increasingly complex and identifying change through direct modelling is problematic. We present a novel framework for online monitoring of forecasts to ensure they remain accurate. By utilizing sequential changepoint techniques on the forecast errors, our framework allows for the real-time identification of potential changes in the process caused by various external factors. We show theoretically that some common changes in the underlying process will manifest in the forecast errors and can be identified faster by identifying shifts in the forecast errors than within the original modelling framework. Moreover, we demonstrate the effectiveness of this framework on numerous forecasting approaches through simulations and show its effectiveness over alternative approaches. Finally, we present two concrete examples, one from Royal Mail parcel delivery volumes and one from NHS A\&amp;E admissions relating to gallstones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14173v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thomas Grundy, Rebecca Killick, Ivan Svetunkov</dc:creator>
    </item>
    <item>
      <title>Modelling the term-structure of default risk under IFRS 9 within a multistate regression framework</title>
      <link>https://arxiv.org/abs/2502.14479</link>
      <description>arXiv:2502.14479v1 Announce Type: cross 
Abstract: The lifetime behaviour of loans is notoriously difficult to model, which can compromise a bank's financial reserves against future losses, if modelled poorly. Therefore, we present a data-driven comparative study amongst three techniques in modelling a series of default risk estimates over the lifetime of each loan, i.e., its term-structure. The behaviour of loans can be described using a nonstationary and time-dependent semi-Markov model, though we model its elements using a multistate regression-based approach. As such, the transition probabilities are explicitly modelled as a function of a rich set of input variables, including macroeconomic and loan-level inputs. Our modelling techniques are deliberately chosen in ascending order of complexity: 1) a Markov chain; 2) beta regression; and 3) multinomial logistic regression. Using residential mortgage data, our results show that each successive model outperforms the previous, likely as a result of greater sophistication. This finding required devising a novel suite of simple model diagnostics, which can itself be reused in assessing sampling representativeness and the performance of other modelling techniques. These contributions surely advance the current practice within banking when conducting multistate modelling. Consequently, we believe that the estimation of loss reserves will be more timeous and accurate under IFRS 9.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14479v1</guid>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Botha, Tanja Verster, Roland Breedt</dc:creator>
    </item>
    <item>
      <title>Addressing Positivity Violations in Continuous Interventions through Data-Adaptive Strategies</title>
      <link>https://arxiv.org/abs/2502.14566</link>
      <description>arXiv:2502.14566v1 Announce Type: cross 
Abstract: Positivity violations pose a key challenge in the estimation of causal effects, particularly for continuous interventions. Current approaches for addressing this issue include the use of projection functions or modified treatment policies. While effective in many contexts, these methods can result in estimands that potentially do not align well with the original research question, thereby leading to compromises in interpretability. In this paper, we introduce a novel diagnostic tool, the non-overlap ratio, to detect positivity violations. To address these violations while maintaining interpretability, we propose a data-adaptive solution, specially a "most feasible" intervention strategy. Our strategy operates on a unit-specific basis. For a given intervention of interest, we first assess whether the intervention value is feasible for each unit. For units with sufficient support, conditional on confounders, we adhere to the intervention of interest. However, for units lacking sufficient support, as identified through the assessment of the non-overlap ratio, we do not assign the actual intervention value of interest. Instead, we assign the closest feasible value within the support region. We propose an estimator using g-computation coupled with flexible conditional density estimation to estimate high- and low support regions to estimate this new estimand. Through simulations, we demonstrate that our method effectively reduces bias across various scenarios by addressing positivity violations. Moreover, when positivity violations are absent, the method successfully recovers the standard estimand. We further validate its practical utility using real-world data from the CHAPAS-3 trial, which enrolled HIV-positive children in Zambia and Uganda.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14566v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Bao, Michael Schomaker</dc:creator>
    </item>
    <item>
      <title>Linking Science and Industry: Influence of Scientific Research on Technological Innovation through Patent Citations</title>
      <link>https://arxiv.org/abs/2502.14570</link>
      <description>arXiv:2502.14570v1 Announce Type: cross 
Abstract: This study explores the connection between patent citations and scientific publications across six fields: Biochemistry, Genetics, Pharmacology, Engineering, Mathematics, and Physics. Analysing 117,590 papers from 2014 to 2023, the research emphasises how publication year, open access (OA) status, and discipline influence patent citations. Openly accessible papers, particularly those in hybrid OA journals or green OA repositories, are significantly more likely to be cited in patents, seven times more than those mentioned in blogs, and over twice as likely compared to older publications. However, papers with policy-related references are less frequently cited, indicating that patents may prioritise commercially viable innovations over those addressing societal challenges. Disciplinary differences reveal distinct innovation patterns across sectors. While academic visibility via blogs or platforms like Mendeley increases within scholarly circles, these have limited impact on patent citations. The study also finds that increased funding, possibly tied to applied research trends and fully open access journals, negatively affects patent citations. Social media presence and the number of authors have minimal impact. These findings highlight the complex factors shaping the integration of scientific research into technological innovations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14570v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Dorta-Gonz\'alez, Alejandro Rodr\'iguez-Caro, Mar\'ia Isabel Dorta-Gonz\'alez</dc:creator>
    </item>
    <item>
      <title>General Uncertainty Estimation with Delta Variances</title>
      <link>https://arxiv.org/abs/2502.14698</link>
      <description>arXiv:2502.14698v1 Announce Type: cross 
Abstract: Decision makers may suffer from uncertainty induced by limited data. This may be mitigated by accounting for epistemic uncertainty, which is however challenging to estimate efficiently for large neural networks. To this extent we investigate Delta Variances, a family of algorithms for epistemic uncertainty quantification, that is computationally efficient and convenient to implement. It can be applied to neural networks and more general functions composed of neural networks. As an example we consider a weather simulator with a neural-network-based step function inside -- here Delta Variances empirically obtain competitive results at the cost of a single gradient computation. The approach is convenient as it requires no changes to the neural network architecture or training procedure. We discuss multiple ways to derive Delta Variances theoretically noting that special cases recover popular techniques and present a unified perspective on multiple related methods. Finally we observe that this general perspective gives rise to a natural extension and empirically show its benefit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14698v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Schmitt, John Shawe-Taylor, Hado van Hasselt</dc:creator>
    </item>
    <item>
      <title>Augmented quantization: a general approach to mixture models</title>
      <link>https://arxiv.org/abs/2309.08389</link>
      <description>arXiv:2309.08389v4 Announce Type: replace 
Abstract: The investigation of mixture models is a key to understand and visualize the distribution of multivariate data. Most mixture models approaches are based on likelihoods, and are not adapted to distribution with finite support or without a well-defined density function. This study proposes the Augmented Quantization method, which is a reformulation of the classical quantization problem but which uses the p-Wasserstein distance. This metric can be computed in very general distribution spaces, in particular with varying supports. The clustering interpretation of quantization is revisited in a more general framework. The performance of Augmented Quantization is first demonstrated through analytical toy problems. Subsequently, it is applied to a practical case study involving river flooding, wherein mixtures of Dirac and Uniform distributions are built in the input space, enabling the identification of the most influential variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08389v4</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charlie Sire, Didier Rulli\`ere, Rodolphe Le Riche, J\'er\'emy Rohmer, Yann Richet, Lucie Pheulpin</dc:creator>
    </item>
    <item>
      <title>Examining Differential Item Functioning (DIF) in Self-Reported Health Survey Data: Via Multilevel Modeling</title>
      <link>https://arxiv.org/abs/2408.13702</link>
      <description>arXiv:2408.13702v2 Announce Type: replace 
Abstract: Few health-related constructs or measures have received a critical evaluation in terms of measurement equivalence, such as self-reported health survey data. Differential item functioning (DIF) analysis is crucial for evaluating measurement equivalence in self-reported health surveys, which are often hierarchical in structure. Traditional single-level DIF methods in this case fall short, making multilevel models a better alternative. We highlight the benefits of multilevel modeling for DIF analysis, when applying a health survey data set to multilevel binary logistic regression (for analyzing binary response data) and multilevel multinominal logistic regression (for analyzing polytomous response data), and comparing them with their single-level counterparts. Our findings show that multilevel models fit better and explain more variance than single-level models. This article is expected to raise awareness of multilevel modeling and help healthcare researchers and practitioners understand the use of multilevel modeling for DIF analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13702v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Quality of Life Research, 2025</arxiv:journal_reference>
      <dc:creator>Dandan Chen Kaptur, Yiqing Liu, Bradley Kaptur, Nicholas Peterman, Jinming Zhang, Justin Kern, Carolyn Anderson</dc:creator>
    </item>
    <item>
      <title>Hierarchical Spatio-Temporal Uncertainty Quantification for Distributed Energy Adoption</title>
      <link>https://arxiv.org/abs/2411.12193</link>
      <description>arXiv:2411.12193v2 Announce Type: replace 
Abstract: The rapid deployment of distributed energy resources (DER) has introduced significant spatio-temporal uncertainties in power grid management, necessitating accurate multilevel forecasting methods. However, existing approaches often produce overly conservative uncertainty intervals at individual spatial units and fail to properly capture uncertainties when aggregating predictions across different spatial scales. This paper presents a novel hierarchical spatio-temporal model based on the conformal prediction framework to address these challenges. Our approach generates circuit-level DER growth predictions and efficiently aggregates them to the substation level while maintaining statistical validity through a tailored non-conformity score. Applied to a decade of DER installation data from a local utility network, our method demonstrates superior performance over existing approaches, particularly in reducing prediction interval widths while maintaining coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12193v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenbin Zhou, Shixiang Zhu, Feng Qiu, Xuan Wu</dc:creator>
    </item>
    <item>
      <title>Coordinated Trading Strategies for Battery Storage in Reserve and Spot Markets</title>
      <link>https://arxiv.org/abs/2406.08390</link>
      <description>arXiv:2406.08390v4 Announce Type: replace-cross 
Abstract: Quantity and price risks are key uncertainties market participants face in electricity markets with increased volatility, for instance, due to high shares of renewables. From day ahead until real-time, there is a large variation in the best available information, leading to price changes that flexible assets, such as battery storage, can exploit economically. This study contributes to understanding how coordinated bidding strategies can enhance multi-market trading and large-scale energy storage integration. Our findings shed light on the complexities arising from interdependencies and the high-dimensional nature of the problem. We show how stochastic dual dynamic programming is a suitable solution technique for such an environment. We include the three markets of the frequency containment reserve, day-ahead, and intraday in stochastic modelling and develop a multi-stage stochastic program. Prices are represented in a multidimensional Markov Chain, following the scheduling of the markets and allowing for time-dependent randomness. Using the example of a battery storage in the German energy sector, we provide valuable insights into the technical aspects of our method and the economic feasibility of battery storage operation. We find that capacity reservation in the frequency containment reserve dominates over the battery's cycling in spot markets at the given resolution on prices in 2022. In an adjusted price environment, we find that coordination can yield an additional value of up to 12.5%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08390v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul E. Seifert, Emil Kraft, Steffen Bakker, Stein-Erik Fleten</dc:creator>
    </item>
    <item>
      <title>A Bayesian joint model of multiple longitudinal and categorical outcomes with application to multiple myeloma using permutation-based variable importance</title>
      <link>https://arxiv.org/abs/2407.14311</link>
      <description>arXiv:2407.14311v2 Announce Type: replace-cross 
Abstract: Joint models have proven to be an effective approach for uncovering potentially hidden connections between various types of outcomes, mainly continuous, time-to-event, and binary. Typically, longitudinal continuous outcomes are characterized by linear mixed-effects models, survival outcomes are described by proportional hazards models, and the link between outcomes are captured by shared random effects. Other modeling variations include generalized linear mixed-effects models for longitudinal data and logistic regression when a binary outcome is present, rather than time until an event of interest. However, in a clinical research setting, one might be interested in modeling the physician's chosen treatment based on the patient's medical history to identify prognostic factors. In this situation, there are often multiple treatment options, requiring the use of a multiclass classification approach. Inspired by this context, we develop a Bayesian joint model for longitudinal and categorical data. In particular, our motivation comes from a multiple myeloma study, in which biomarkers display nonlinear trajectories that are well captured through bi-exponential submodels, where patient-level information is shared with the categorical submodel. We also present a variable importance strategy to rank prognostic factors. We apply our proposal and a competing model to the multiple myeloma data, compare the variable importance and inferential results for both models, and illustrate patient-level interpretations using our joint model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14311v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danilo Alvares, Jessica K. Barrett, Fran\c{c}ois Mercier, Jochen Schulze, Sean Yiu, Felipe Castro, Spyros Roumpanis, Yajing Zhu</dc:creator>
    </item>
    <item>
      <title>UAVDB: Trajectory-Guided Adaptable Bounding Boxes for UAV Detection</title>
      <link>https://arxiv.org/abs/2409.06490</link>
      <description>arXiv:2409.06490v4 Announce Type: replace-cross 
Abstract: The widespread deployment of Unmanned Aerial Vehicles (UAVs) in surveillance, security, and airspace management has created an urgent demand for precise, scalable, and efficient UAV detection. However, existing datasets often suffer from limited scale diversity and inaccurate annotations, hindering robust model development. This paper introduces UAVDB, a high-resolution UAV detection dataset constructed using Patch Intensity Convergence (PIC). This novel technique automatically generates high-fidelity bounding box annotations from UAV trajectory data~\cite{li2020reconstruction}, eliminating the need for manual labeling. UAVDB features single-class annotations with a fixed-camera setup and consists of RGB frames capturing UAVs across various scales, from large-scale UAVs to near-single-pixel representations, along with challenging backgrounds that pose difficulties for modern detectors. We first validate the accuracy and efficiency of PIC-generated bounding boxes by comparing Intersection over Union (IoU) performance and runtime against alternative annotation methods, demonstrating that PIC achieves higher annotation accuracy while being more efficient. Subsequently, we benchmark UAVDB using state-of-the-art (SOTA) YOLO-series detectors, establishing UAVDB as a valuable resource for advancing long-range and high-resolution UAV detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06490v4</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu-Hsi Chen</dc:creator>
    </item>
    <item>
      <title>Global Ease of Living Index: a machine learning framework for longitudinal analysis of major economies</title>
      <link>https://arxiv.org/abs/2502.06866</link>
      <description>arXiv:2502.06866v2 Announce Type: replace-cross 
Abstract: The drastic changes in the global economy, geopolitical conditions, and disruptions such as the COVID-19 pandemic have impacted the cost of living and quality of life. It is important to understand the long-term nature of the cost of living and quality of life in major economies. A transparent and comprehensive living index must include multiple dimensions of living conditions. In this study, we present an approach to quantifying the quality of life through the Global Ease of Living Index that combines various socio-economic and infrastructural factors into a single composite score. Our index utilises economic indicators that define living standards, which could help in targeted interventions to improve specific areas. We present a machine learning framework for addressing the problem of missing data for some of the economic indicators for specific countries. We then curate and update the data and use a dimensionality reduction approach (principal component analysis) to create the Ease of Living Index for major economies since 1970. Our work significantly adds to the literature by offering a practical tool for policymakers to identify areas needing improvement, such as healthcare systems, employment opportunities, and public safety. Our approach with open data and code can be easily reproduced and applied to various contexts. This transparency and accessibility make our work a valuable resource for ongoing research and policy development in quality-of-life assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06866v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanay Panat, Rohitash Chandra</dc:creator>
    </item>
  </channel>
</rss>
