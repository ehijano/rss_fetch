<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Feb 2025 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Statistical tests based on Renyi entropy estimation</title>
      <link>https://arxiv.org/abs/2502.08654</link>
      <description>arXiv:2502.08654v1 Announce Type: new 
Abstract: Entropy and its various generalizations are important in many fields, including mathematical statistics, communication theory, physics and computer science, for characterizing the amount of information associated with a probability distribution. In this paper we propose goodness-of-fit statistics for the multivariate Student and multivariate Pearson type II distributions, based on the maximum entropy principle and a class of estimators for Renyi entropy based on nearest neighbour distances. We prove the L2-consistency of these statistics using results on the subadditivity of Euclidean functionals on nearest neighbour graphs, and investigate their rate of convergence and asymptotic distribution using Monte Carlo methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08654v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehmet Siddik Cadirci, Dafydd Evans, Nikolai Leonenko, Vitali Makogin, Oleg Seleznjev</dc:creator>
    </item>
    <item>
      <title>Evaluating Decision Rules Across Many Weak Experiments</title>
      <link>https://arxiv.org/abs/2502.08763</link>
      <description>arXiv:2502.08763v1 Announce Type: new 
Abstract: Technology firms conduct randomized controlled experiments ("A/B tests") to learn which actions to take to improve business outcomes. In firms with mature experimentation platforms, experimentation programs can consist of many thousands of tests. To scale experimentation effectively, firms rely on decision rules: standard operating procedures for mapping the results of an experiment to a choice of treatment arm to launch to the general user population. Despite the critical role of decision rules in translating experimentation into business decisions, rigorous guidance on how to evaluate and choose decision rules is scarce. This paper proposes to evaluate decision rules based on their cumulative returns to business north star metrics. Although this quantity is intuitive and easy to explain to decision-makers, estimating it can be non-trivial, especially when experiments have weak signal-to-noise ratios. We develop a cross-validation estimator that is much less biased than the naive plug-in estimator under conditions realistic to digital experimentation. We demonstrate the efficacy of our approach via a case study of 123 historical A/B tests at Netflix, where we used it to show that a new decision rule would increase cumulative returns to the north star metric by an estimated 33%, leading directly to the adoption of the new rule.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08763v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Winston Chou, Colin Gray, Nathan Kallus, Aur\'elien Bibaut, Simon Ejdemyr</dc:creator>
    </item>
    <item>
      <title>Treatment response as a latent variable</title>
      <link>https://arxiv.org/abs/2502.08776</link>
      <description>arXiv:2502.08776v1 Announce Type: new 
Abstract: Scientists often need to analyze the samples in a study that responded to treatment in order to refine their hypotheses and find potential causal drivers of response. Natural variation in outcomes makes teasing apart responders from non-responders a statistical inference problem. To handle latent responses, we introduce the causal two-groups (C2G) model, a causal extension of the classical two-groups model. The C2G model posits that treated samples may or may not experience an effect, according to some prior probability. We propose two empirical Bayes procedures for the causal two-groups model, one under semi-parametric conditions and another under fully nonparametric conditions. The semi-parametric model assumes additive treatment effects and is identifiable from observed data. The nonparametric model is unidentifiable, but we show it can still be used to test for response in each treated sample. We show empirically and theoretically that both methods for selecting responders control the false discovery rate at the target level with near-optimal power. We also propose two novel estimands of interest and provide a strategy for deriving estimand intervals in the unidentifiable nonparametric model. On a cancer immunotherapy dataset, the nonparametric C2G model recovers clinically-validated predictive biomarkers of both positive and negative outcomes. Code is available at https://github.com/tansey-lab/causal2groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08776v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Tosh, Boyuan Zhang, Wesley Tansey</dc:creator>
    </item>
    <item>
      <title>Statistical inference for Levy-driven graph supOU processes: From short- to long-memory in high-dimensional time series</title>
      <link>https://arxiv.org/abs/2502.08838</link>
      <description>arXiv:2502.08838v1 Announce Type: new 
Abstract: This article introduces Levy-driven graph supOU processes, offering a parsimonious parametrisation for high-dimensional time-series, where dependencies between the individual components are governed via a graph structure. Specifically, we propose a model specification that allows for a smooth transition between short- and long-memory settings while accommodating a wide range of marginal distributions.
  We further develop an inference procedure based on the generalised method of moments, establish its asymptotic properties and demonstrate its strong finite sample performance through a simulation study.
  Finally, we illustrate the practical relevance of our new model and estimation method in an empirical study of wind capacity factors in an European electricity network context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08838v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shreya Mehta, Almut E. D. Veraart</dc:creator>
    </item>
    <item>
      <title>Infinitely divisible priors for multivariate survival functions</title>
      <link>https://arxiv.org/abs/2502.09162</link>
      <description>arXiv:2502.09162v1 Announce Type: new 
Abstract: This article introduces a novel framework for nonparametric priors on real-valued random vectors. The framework can be viewed as a multivariate generalization of neutral-to-the right priors, which also encompasses continuous priors. It is based on randomizing the exponent measure of a minimum-infinitely divisible random vector by an infinitely divisible random measure and naturally incorporates partially exchangeable data as well as exchangeable random vectors. We show how to construct hierarchical priors from simple building blocks and embed many models from Bayesian nonparametric survival analysis into our framework. The prior can concentrate on discrete or continuous distributions and other properties such as dependence, moments and moments of mean functionals are characterized. The posterior predictive distribution is derived in a general framework and is refined under some regularity conditions. In addition, a theoretical framework for the simulation from the posterior predictive distribution is provided. As a byproduct, the concept of subordination of homogeneous completely random measures by homogeneous completely random measures is extended to subordination by infinitely divisible random measures. This technique allows to create vectors of dependent infinitely divisible random measures with tractable Laplace transform and is expected to have applications beyond the scope of this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09162v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Br\"uck</dc:creator>
    </item>
    <item>
      <title>Just Trial Once: Ongoing Causal Validation of Machine Learning Models</title>
      <link>https://arxiv.org/abs/2502.09467</link>
      <description>arXiv:2502.09467v1 Announce Type: new 
Abstract: Machine learning (ML) models are increasingly used as decision-support tools in high-risk domains. Evaluating the causal impact of deploying such models can be done with a randomized controlled trial (RCT) that randomizes users to ML vs. control groups and assesses the effect on relevant outcomes. However, ML models are inevitably updated over time, and we often lack evidence for the causal impact of these updates. While the causal effect could be repeatedly validated with ongoing RCTs, such experiments are expensive and time-consuming to run. In this work, we present an alternative solution: using only data from a prior RCT, we give conditions under which the causal impact of a new ML model can be precisely bounded or estimated, even if it was not included in the RCT. Our assumptions incorporate two realistic constraints: ML predictions are often deterministic, and their impacts depend on user trust in the model. Based on our analysis, we give recommendations for trial designs that maximize our ability to assess future versions of an ML model. Our hope is that our trial design recommendations will save practitioners time and resources while allowing for quicker deployments of updates to ML models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09467v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob M. Chen, Michael Oberst</dc:creator>
    </item>
    <item>
      <title>Thresholding Nonprobability Units in Combined Data for Efficient Domain Estimation</title>
      <link>https://arxiv.org/abs/2502.09524</link>
      <description>arXiv:2502.09524v1 Announce Type: new 
Abstract: Quasi-randomization approaches estimate latent participation probabilities for units from a nonprobability / convenience sample. Estimation of participation probabilities for convenience units allows their combination with units from the randomized survey sample to form a survey weighted domain estimate. One leverages convenience units for domain estimation under the expectation that estimation precision and bias will improve relative to solely using the survey sample; however, convenience sample units that are very different in their covariate support from the survey sample units may inflate estimation bias or variance. This paper develops a method to threshold or exclude convenience units to minimize the variance of the resulting survey weighted domain estimator. We compare our thresholding method with other thresholding constructions in a simulation study for two classes of datasets based on degree of overlap between survey and convenience samples on covariate support. We reveal that excluding convenience units that each express a low probability of appearing in \emph{both} reference and convenience samples reduces estimation error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09524v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Terrance D. Savitsky, Matthew R. Williams, Julie Gerrshunskaya, Vladislav Beresovsky</dc:creator>
    </item>
    <item>
      <title>Spatial Transcriptomics Iterative Hierarchical Clustering (stIHC): A Novel Method for Identifying Spatial Gene Co-Expression Modules</title>
      <link>https://arxiv.org/abs/2502.09574</link>
      <description>arXiv:2502.09574v1 Announce Type: new 
Abstract: Recent advancements in spatial transcriptomics technologies allow researchers to simultaneously measure RNA expression levels for hundreds to thousands of genes while preserving spatial information within tissues, providing critical insights into spatial gene expression patterns, tissue organization, and gene functionality. However, existing methods for clustering spatially variable genes (SVGs) into co-expression modules often fail to detect rare or unique spatial expression patterns. To address this, we present spatial transcriptomics iterative hierarchical clustering (stIHC), a novel method for clustering SVGs into co-expression modules, representing groups of genes with shared spatial expression patterns. Through three simulations and applications to spatial transcriptomics datasets from technologies such as 10x Visium, 10x Xenium, and Spatial Transcriptomics, stIHC outperforms clustering approaches used by popular SVG detection methods, including SPARK, SPARK-X, MERINGUE, and SpatialDE. Gene Ontology enrichment analysis confirms that genes within each module share consistent biological functions, supporting the functional relevance of spatial co-expression. Robust across technologies with varying gene numbers and spatial resolution, stIHC provides a powerful tool for decoding the spatial organization of gene expression and the functional structure of complex tissues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09574v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Catherine Higgins, Jingyi Jessica Li, Michelle Carey</dc:creator>
    </item>
    <item>
      <title>Principles for Open Data Curation: A Case Study with the New York City 311 Service Request Data</title>
      <link>https://arxiv.org/abs/2502.08649</link>
      <description>arXiv:2502.08649v1 Announce Type: cross 
Abstract: In the early 21st century, the open data movement began to transform societies and governments by promoting transparency, innovation, and public engagement. The City of New York (NYC) has been at the forefront of this movement since the enactment of the Open Data Law in 2012, creating the NYC Open Data portal. The portal currently hosts 2,700 datasets, serving as a crucial resource for research across various domains, including health, urban development, and transportation. However, the effective use of open data relies heavily on data quality and usability, challenges that remain insufficiently addressed in the literature. This paper examines these challenges via a case study of the NYC 311 Service Request dataset, identifying key issues in data validity, consistency, and curation efficiency. We propose a set of data curation principles, tailored for government-released open data, to address these challenges. Our findings highlight the importance of harmonized field definitions, streamlined storage, and automated quality checks, offering practical guidelines for improving the reliability and utility of open datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08649v1</guid>
      <category>cs.DB</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Hussey, Jun Yan</dc:creator>
    </item>
    <item>
      <title>New Bounds for Sparse Variational Gaussian Processes</title>
      <link>https://arxiv.org/abs/2502.08730</link>
      <description>arXiv:2502.08730v1 Announce Type: cross 
Abstract: Sparse variational Gaussian processes (GPs) construct tractable posterior approximations to GP models. At the core of these methods is the assumption that the true posterior distribution over training function values ${\bf f}$ and inducing variables ${\bf u}$ is approximated by a variational distribution that incorporates the conditional GP prior $p({\bf f} | {\bf u})$ in its factorization. While this assumption is considered as fundamental, we show that for model training we can relax it through the use of a more general variational distribution $q({\bf f} | {\bf u})$ that depends on $N$ extra parameters, where $N$ is the number of training examples. In GP regression, we can analytically optimize the evidence lower bound over the extra parameters and express a tractable collapsed bound that is tighter than the previous bound. The new bound is also amenable to stochastic optimization and its implementation requires minor modifications to existing sparse GP code. Further, we also describe extensions to non-Gaussian likelihoods. On several datasets we demonstrate that our method can reduce bias when learning the hyperpaparameters and can lead to better predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08730v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michalis K. Titsias</dc:creator>
    </item>
    <item>
      <title>Mortality simulations for insured and general populations</title>
      <link>https://arxiv.org/abs/2502.08814</link>
      <description>arXiv:2502.08814v1 Announce Type: cross 
Abstract: This study presents a framework for high-resolution mortality simulations tailored to insured and general populations. Due to the scarcity of detailed demographic-specific mortality data, we leverage Iterative Proportional Fitting (IPF) and Monte Carlo simulations to generate refined mortality tables that incorporate age, gender, smoker status, and regional distributions. This methodology enhances public health planning and actuarial analysis by providing enriched datasets for improved life expectancy projections and insurance product development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08814v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asmik Nalmpatian, Christian Heumann</dc:creator>
    </item>
    <item>
      <title>WENDy for Nonlinear-in-Parameter ODEs</title>
      <link>https://arxiv.org/abs/2502.08881</link>
      <description>arXiv:2502.08881v1 Announce Type: cross 
Abstract: The Weak-form Estimation of Non-linear Dynamics (WENDy) algorithm is extended to accommodate systems of ordinary differential equations that are nonlinear-in-parameters (NiP). The extension rests on derived analytic expressions for a likelihood function, its gradient and its Hessian matrix. WENDy makes use of these to approximate a maximum likelihood estimator based on optimization routines suited for non-convex optimization problems. The resulting parameter estimation algorithm has better accuracy, a substantially larger domain of convergence, and is often orders of magnitude faster than the conventional output error least squares method (based on forward solvers).
  The WENDy.jl algorithm is efficiently implemented in Julia. We demonstrate the algorithm's ability to accommodate the weak form optimization for both additive normal and multiplicative log-normal noise, and present results on a suite of benchmark systems of ordinary differential equations. In order to demonstrate the practical benefits of our approach, we present extensive comparisons between our method and output error methods in terms of accuracy, precision, bias, and coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08881v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nic Rummel, Daniel A. Messenger, Stephen Becker, Vanja Dukic, David M. Bortz</dc:creator>
    </item>
    <item>
      <title>On (in)consistency of M-estimators under contamination</title>
      <link>https://arxiv.org/abs/2502.09145</link>
      <description>arXiv:2502.09145v1 Announce Type: cross 
Abstract: We consider robust location-scale estimators under contamination. We show that commonly used robust estimators such as the median and the Huber estimator are inconsistent under asymmetric contamination, while the Tukey estimator is consistent. In order to make nuisance parameter free inference based on the Tukey estimator a consistent scale estimator is required. However, standard robust scale estimators such as the interquartile range and the median absolute deviation are inconsistent under contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09145v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jens Klooster, Bent Nielsen</dc:creator>
    </item>
    <item>
      <title>Bayesian Matrix Factor Models for Demographic Analysis Across Age and Time</title>
      <link>https://arxiv.org/abs/2502.09255</link>
      <description>arXiv:2502.09255v1 Announce Type: cross 
Abstract: Analyzing demographic data collected across multiple populations, time periods, and age groups is challenging due to the interplay of high dimensionality, demographic heterogeneity among groups, and stochastic variability within smaller groups. This paper proposes a Bayesian matrix factor model to address these challenges. By factorizing count data matrices as the product of low-dimensional latent age and time factors, the model achieves a parsimonious representation that mitigates overfitting and remains computationally feasible even when hundreds of subpopulations are involved. Smoothness in age factors and a dynamic evolution of time factors are achieved through informative priors, and an efficient Markov chain Monte Carlo algorithm is developed for posterior inference. Applying the model to Austrian district-level emigration data from 2002 to 2023 demonstrates its ability to reconstruct demographic processes using only a fraction of the parameters required by conventional factor models. Extensive cross-validation and out-of-sample forecasting exercises show that the proposed matrix factor model consistently outperforms standard benchmarks. Beyond statistical demography, the framework holds promise for a wide range of applications involving noisy, heterogeneous, and high-dimensional non-Gaussian matrix-valued data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09255v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregor Zens</dc:creator>
    </item>
    <item>
      <title>Perturbation-based Effect Measures for Compositional Data</title>
      <link>https://arxiv.org/abs/2311.18501</link>
      <description>arXiv:2311.18501v4 Announce Type: replace 
Abstract: Existing effect measures for compositional features are inadequate for many modern applications, for example, in microbiome research, since they display traits such as high-dimensionality and sparsity that can be poorly modelled with traditional parametric approaches. Further, assessing -- in an unbiased way -- how summary statistics of a composition (e.g., racial diversity) affect a response variable is not straightforward. We propose a framework based on hypothetical data perturbations which defines interpretable statistical functionals on the compositions themselves, which we call average perturbation effects. These effects naturally account for confounding that biases frequently used marginal dependence analyses. We show how average perturbation effects can be estimated efficiently by deriving a perturbation-dependent reparametrization and applying semiparametric estimation techniques. We analyze the proposed estimators empirically on simulated and semi-synthetic data and demonstrate advantages over existing techniques on data from New York schools and microbiome data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18501v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton Rask Lundborg, Niklas Pfister</dc:creator>
    </item>
    <item>
      <title>Mediated probabilities of causation</title>
      <link>https://arxiv.org/abs/2404.07397</link>
      <description>arXiv:2404.07397v3 Announce Type: replace 
Abstract: We propose a set of causal estimands that we call the "mediated probabilities of causation." These estimands quantify the probabilities that an observed negative outcome was induced via a mediating pathway versus a direct pathway in a stylized setting involving a binary exposure or intervention, a single binary mediator, and a binary outcome. We outline a set of conditions sufficient to identify these effects given observed data, and propose a doubly-robust projection based estimation strategy that allows for the use of flexible non-parametric and machine learning methods for estimation. We argue that these effects may be more relevant than the probability of causation, particularly in settings where we observe both some negative outcome and negative mediating event, and we wish to distinguish between settings where the outcome was induced via the exposure inducing the mediator versus the exposure inducing the outcome directly. We motivate these estimands by discussing applications to legal and medical questions of causal attribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07397v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Rubinstein, Maria Cuellar, Daniel Malinsky</dc:creator>
    </item>
    <item>
      <title>Bayesian penalized empirical likelihood and MCMC sampling</title>
      <link>https://arxiv.org/abs/2412.17354</link>
      <description>arXiv:2412.17354v2 Announce Type: replace 
Abstract: In this study, we introduce a novel methodological framework called Bayesian Penalized Empirical Likelihood (BPEL), designed to address the computational challenges inherent in empirical likelihood (EL) approaches. Our approach has two primary objectives: (i) to enhance the inherent flexibility of EL in accommodating diverse model conditions, and (ii) to facilitate the use of well-established Markov Chain Monte Carlo (MCMC) sampling schemes as a convenient alternative to the complex optimization typically required for statistical inference using EL. To achieve the first objective, we propose a penalized approach that regularizes the Lagrange multipliers, significantly reducing the dimensionality of the problem while accommodating a comprehensive set of model conditions. For the second objective, our study designs and thoroughly investigates two popular sampling schemes within the BPEL context. We demonstrate that the BPEL framework is highly flexible and efficient, enhancing the adaptability and practicality of EL methods. Our study highlights the practical advantages of using sampling techniques over traditional optimization methods for EL problems, showing rapid convergence to the global optima of posterior distributions and ensuring the effective resolution of complex statistical inference challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17354v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Cheng Yong Tang, Yuanzheng Zhu</dc:creator>
    </item>
    <item>
      <title>Semiparametric Modeling and Analysis for Longitudinal Network Data</title>
      <link>https://arxiv.org/abs/2308.12227</link>
      <description>arXiv:2308.12227v3 Announce Type: replace-cross 
Abstract: We introduce a semiparametric latent space model for analyzing longitudinal network data. The model consists of a static latent space component and a time-varying node-specific baseline component. We develop a semiparametric efficient score equation for the latent space parameter by adjusting for the baseline nuisance component. Estimation is accomplished through a one-step update estimator and an appropriately penalized maximum likelihood estimator. We derive oracle error bounds for the two estimators and address identifiability concerns from a quotient manifold perspective. Our approach is demonstrated using the New York Citi Bike Dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12227v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinqiu He, Jiajin Sun, Yuang Tian, Zhiliang Ying, Yang Feng</dc:creator>
    </item>
    <item>
      <title>Bridging Root-$n$ and Non-standard Asymptotics: Dimension-agnostic Adaptive Inference in M-Estimation</title>
      <link>https://arxiv.org/abs/2501.07772</link>
      <description>arXiv:2501.07772v2 Announce Type: replace-cross 
Abstract: This manuscript studies a general approach to construct confidence sets for the solution of population-level optimization, commonly referred to as M-estimation. Statistical inference for M-estimation poses significant challenges due to the non-standard limiting behaviors of the corresponding estimator, which arise in settings with increasing dimension of parameters, non-smooth objectives, or constraints. We propose a simple and unified method that guarantees validity in both regular and irregular cases. Moreover, we provide a comprehensive width analysis of the proposed confidence set, showing that the convergence rate of the diameter is adaptive to the unknown degree of instance-specific regularity. We apply the proposed method to several high-dimensional and irregular statistical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07772v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenta Takatsu, Arun Kumar Kuchibhotla</dc:creator>
    </item>
  </channel>
</rss>
