<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Apr 2024 04:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>VARX Granger Analysis: Modeling, Inference, and Applications</title>
      <link>https://arxiv.org/abs/2404.10834</link>
      <description>arXiv:2404.10834v1 Announce Type: new 
Abstract: Vector Autoregressive models with exogenous input (VARX) provide a powerful framework for modeling complex dynamical systems like brains, markets, or societies. Their simplicity allows us to uncover linear effects between endogenous and exogenous variables. The Granger formalism is naturally suited for VARX models, but the connection between the two is not widely understood. We aim to bridge this gap by providing both the basic equations and easy-to-use code. We first explain how the Granger formalism can be combined with a VARX model using deviance as a test statistic. We also present a bias correction for the deviance in the case of L2 regularization, a technique used to reduce model complexity. To address the challenge of modeling long responses, we propose the use of basis functions, which further reduce parameter complexity. We demonstrate that p-values are correctly estimated, even for short signals where regularization influences the results. Additionally, we analyze the model's performance under various scenarios where model assumptions are violated, such as missing variables or indirect observations of the underlying dynamics. Finally, we showcase the practical utility of our approach by applying it to real-world data from neuroscience, physiology, and sociology. To facilitate its adoption, we make Matlab, Python, and R code available here: https://github.com/lcparra/varx</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10834v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas C. Parra, Aimar Silvan, Maximilian Nentwich, Jens Madsen, Behtash Babadi</dc:creator>
    </item>
    <item>
      <title>Modeling Interconnected Modules in Multivariate Outcomes: Evaluating the Impact of Alcohol Intake on Plasma Metabolomics</title>
      <link>https://arxiv.org/abs/2404.10884</link>
      <description>arXiv:2404.10884v1 Announce Type: new 
Abstract: Alcohol consumption has been shown to influence cardiovascular mechanisms in humans, leading to observable alterations in the plasma metabolomic profile. Regression models are commonly employed to investigate these effects, treating metabolomics features as the outcomes and alcohol intake as the exposure. Given the latent dependence structure among the numerous metabolomic features (e.g., co-expression networks with interconnected modules), modeling this structure is crucial for accurately identifying metabolomic features associated with alcohol intake. However, integrating dependence structures into regression models remains difficult in both estimation and inference procedures due to their large or high dimensionality. To bridge this gap, we propose an innovative multivariate regression model that accounts for correlations among outcome features by incorporating an interconnected community structure. Furthermore, we derive closed-form and likelihood-based estimators, accompanied by explicit exact and explicit asymptotic covariance matrix estimators, respectively. Simulation analysis demonstrates that our approach provides accurate estimation of both dependence and regression coefficients, and enhances sensitivity while maintaining a well-controlled discovery rate, as evidenced through benchmarking against existing regression models. Finally, we apply our approach to assess the impact of alcohol intake on $249$ metabolomic biomarkers measured using nuclear magnetic resonance spectroscopy. The results indicate that alcohol intake can elevate high-density lipoprotein levels by enhancing the transport rate of Apolipoproteins A1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10884v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yifan Yang, Chixiang Chen, Hwiyoung Lee, Ming Wang, Shuo Chen</dc:creator>
    </item>
    <item>
      <title>Compressive Bayesian non-negative matrix factorization for mutational signatures analysis</title>
      <link>https://arxiv.org/abs/2404.10974</link>
      <description>arXiv:2404.10974v1 Announce Type: new 
Abstract: Non-negative matrix factorization (NMF) is widely used in many applications for dimensionality reduction. Inferring an appropriate number of factors for NMF is a challenging problem, and several approaches based on information criteria or sparsity-inducing priors have been proposed. However, inference in these models is often complicated and computationally challenging. In this paper, we introduce a novel methodology for overfitted Bayesian NMF models using "compressive hyperpriors" that force unneeded factors down to negligible values while only imposing mild shrinkage on needed factors. The method is based on using simple semi-conjugate priors to facilitate inference, while setting the strength of the hyperprior in a data-dependent way to achieve this compressive property. We apply our method to mutational signatures analysis in cancer genomics, where we find that it outperforms state-of-the-art alternatives. In particular, we illustrate how our compressive hyperprior enables the use of biologically informed priors on the signatures, yielding significantly improved accuracy. We provide theoretical results establishing the compressive property, and we demonstrate the method in simulations and on real data from a breast cancer application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10974v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Zito, Jeffrey W. Miller</dc:creator>
    </item>
    <item>
      <title>Interval-censored linear quantile regression</title>
      <link>https://arxiv.org/abs/2404.11125</link>
      <description>arXiv:2404.11125v1 Announce Type: new 
Abstract: Censored quantile regression has emerged as a prominent alternative to classical Cox's proportional hazards model or accelerated failure time model in both theoretical and applied statistics. While quantile regression has been extensively studied for right-censored survival data, methodologies for analyzing interval-censored data remain limited in the survival analysis literature. This paper introduces a novel local weighting approach for estimating linear censored quantile regression, specifically tailored to handle diverse forms of interval-censored survival data. The estimation equation and the corresponding convex objective function for the regression parameter can be constructed as a weighted average of quantile loss contributions at two interval endpoints. The weighting components are nonparametrically estimated using local kernel smoothing or ensemble machine learning techniques. To estimate the nonparametric distribution mass for interval-censored data, a modified EM algorithm for nonparametric maximum likelihood estimation is employed by introducing subject-specific latent Poisson variables. The proposed method's empirical performance is demonstrated through extensive simulation studies and real data analyses of two HIV/AIDS datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11125v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taehwa Choi, Seohyeon Park, Hunyong Cho, Sangbum Choi</dc:creator>
    </item>
    <item>
      <title>Automated, efficient and model-free inference for randomized clinical trials via data-driven covariate adjustment</title>
      <link>https://arxiv.org/abs/2404.11150</link>
      <description>arXiv:2404.11150v1 Announce Type: new 
Abstract: In May 2023, the U.S. Food and Drug Administration (FDA) released guidance for industry on "Adjustment for Covariates in Randomized Clinical Trials for Drugs and Biological Products". Covariate adjustment is a statistical analysis method for improving precision and power in clinical trials by adjusting for pre-specified, prognostic baseline variables. Though recommended by the FDA and the European Medicines Agency (EMA), many trials do not exploit the available information in baseline variables or make use only of the baseline measurement of the outcome. This is likely (partly) due to the regulatory mandate to pre-specify baseline covariates for adjustment, leading to challenges in determining appropriate covariates and their functional forms. We will explore the potential of automated data-adaptive methods, such as machine learning algorithms, for covariate adjustment, addressing the challenge of pre-specification. Specifically, our approach allows the use of complex models or machine learning algorithms without compromising the interpretation or validity of the treatment effect estimate and its corresponding standard error, even in the presence of misspecified outcome working models. This contrasts the majority of competing works which assume correct model specification for the validity of standard errors. Our proposed estimators either necessitate ultra-sparsity in the outcome model (which can be relaxed by limiting the number of predictors in the model) or necessitate integration with sample splitting to enhance their performance. As such, we will arrive at simple estimators and standard errors for the marginal treatment effect in randomized clinical trials, which exploit data-adaptive outcome predictions based on prognostic baseline covariates, and have low (or no) bias in finite samples even when those predictions are themselves biased.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11150v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelly Van Lancker, Iv\'an D\'iaz, Stijn Vansteelandt</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization for Identification of Optimal Biological Dose Combinations in Personalized Dose-Finding Trials</title>
      <link>https://arxiv.org/abs/2404.11323</link>
      <description>arXiv:2404.11323v1 Announce Type: new 
Abstract: Early phase, personalized dose-finding trials for combination therapies seek to identify patient-specific optimal biological dose (OBD) combinations, which are defined as safe dose combinations which maximize therapeutic benefit for a specific covariate pattern. Given the small sample sizes which are typical of these trials, it is challenging for traditional parametric approaches to identify OBD combinations across multiple dosing agents and covariate patterns. To address these challenges, we propose a Bayesian optimization approach to dose-finding which formally incorporates toxicity information into both the initial data collection process and the sequential search strategy. Independent Gaussian processes are used to model the efficacy and toxicity surfaces, and an acquisition function is utilized to define the dose-finding strategy and an early stopping rule. This work is motivated by a personalized dose-finding trial which considers a dual-agent therapy for obstructive sleep apnea, where OBD combinations are tailored to obstructive sleep apnea severity. To compare the performance of the personalized approach to a standard approach where covariate information is ignored, a simulation study is performed. We conclude that personalized dose-finding is essential in the presence of heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11323v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Willard, Shirin Golchi, Erica EM Moodie</dc:creator>
    </item>
    <item>
      <title>Jacobi Prior: An Alternate Bayesian Method for Supervised Learning</title>
      <link>https://arxiv.org/abs/2404.11345</link>
      <description>arXiv:2404.11345v1 Announce Type: new 
Abstract: This paper introduces the `Jacobi prior,' an alternative Bayesian method, that aims to address the computational challenges inherent in traditional techniques. It demonstrates that the Jacobi prior performs better than well-known methods like Lasso, Ridge, Elastic Net, and MCMC-based Horse-Shoe Prior, especially in predicting accurately. Additionally, We also show that the Jacobi prior is more than a hundred times faster than these methods while maintaining similar predictive accuracy. The method is implemented for Generalised Linear Models, Gaussian process regression, and classification, making it suitable for longitudinal/panel data analysis. The Jacobi prior shows it can handle partitioned data across servers worldwide, making it useful for distributed computing environments. As the method runs faster while still predicting accurately, it's good for organizations wanting to reduce their environmental impact and meet ESG standards. To show how well the Jacobi prior works, we did a detailed simulation study with four experiments, looking at statistical consistency, accuracy, and speed. Additionally, we present two empirical studies. First, we thoroughly evaluate Credit Risk by studying default probability using data from the U.S. Small Business Administration (SBA). Also, we use the Jacobi prior to classifying stars, quasars, and galaxies in a 3-class problem using multinational logit regression on Sloan Digital Sky Survey data. We use different filters as features. All codes and datasets for this paper are available in the following GitHub repository: https://github.com/sourish-cmi/Jacobi-Prior/</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11345v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sourish Das, Shouvik Sardar</dc:creator>
    </item>
    <item>
      <title>Matern Correlation: A Panoramic Primer</title>
      <link>https://arxiv.org/abs/2404.11427</link>
      <description>arXiv:2404.11427v1 Announce Type: new 
Abstract: Matern correlation is of pivotal importance in spatial statistics and machine learning. This paper serves as a panoramic primer for this correlation with an emphasis on the exposition of its changing behavior and smoothness properties in response to the change of its two parameters. Such exposition is achieved through a series of simulation studies, the use of an interactive 3D visualization applet, and a practical modeling example, all tailored for a wide-ranging statistical audience. Meanwhile, the thorough understanding of these parameter-smoothness relationships, in turn, serves as a pragmatic guide for researchers in their real-world modeling endeavors, such as setting appropriate initial values for these parameters and parameter-fine-tuning in their Bayesian modeling practice or simulation studies involving the Matern correlation. Derived problems surrounding Matern, such as inconsistent parameter inference, extended forms of Matern and limitations of Matern, are also explored and surveyed to impart a panoramic view of this correlation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11427v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoqing Chen</dc:creator>
    </item>
    <item>
      <title>Improved bounds and inference on optimal regimes</title>
      <link>https://arxiv.org/abs/2404.11510</link>
      <description>arXiv:2404.11510v1 Announce Type: new 
Abstract: Point identification of causal effects requires strong assumptions that are unreasonable in many practical settings. However, informative bounds on these effects can often be derived under plausible assumptions. Even when these bounds are wide or cover null effects, they can guide practical decisions based on formal decision theoretic criteria. Here we derive new results on optimal treatment regimes in settings where the effect of interest is bounded. These results are driven by consideration of superoptimal regimes; we define regimes that leverage an individual's natural treatment value, which is typically ignored in the existing literature. We obtain (sharp) bounds for the value function of superoptimal regimes, and provide performance guarantees relative to conventional optimal regimes. As a case study, we consider a commonly studied Marginal Sensitivity Model and illustrate that the superoptimal regime can be identified when conventional optimal regimes are not. We similarly illustrate this property in an instrumental variable setting. Finally, we derive efficient estimators for upper and lower bounds on the superoptimal value in instrumental variable settings, building on recent results on covariate adjusted Balke-Pearl bounds. These estimators are applied to study the effect of prompt ICU admission on survival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11510v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Julien D. Laurendeau, Aaron L. Sarvet, Mats J. Stensrud</dc:creator>
    </item>
    <item>
      <title>Spatial Heterogeneous Additive Partial Linear Model: A Joint Approach of Bivariate Spline and Forest Lasso</title>
      <link>https://arxiv.org/abs/2404.11579</link>
      <description>arXiv:2404.11579v1 Announce Type: new 
Abstract: Identifying spatial heterogeneous patterns has attracted a surge of research interest in recent years, due to its important applications in various scientific and engineering fields. In practice the spatially heterogeneous components are often mixed with components which are spatially smooth, making the task of identifying the heterogeneous regions more challenging. In this paper, we develop an efficient clustering approach to identify the model heterogeneity of the spatial additive partial linear model. Specifically, we aim to detect the spatially contiguous clusters based on the regression coefficients while introducing a spatially varying intercept to deal with the smooth spatial effect. On the one hand, to approximate the spatial varying intercept, we use the method of bivariate spline over triangulation, which can effectively handle the data from a complex domain. On the other hand, a novel fusion penalty termed the forest lasso is proposed to reveal the spatial clustering pattern. Our proposed fusion penalty has advantages in both the estimation and computation efficiencies when dealing with large spatial data. Theoretically properties of our estimator are established, and simulation results show that our approach can achieve more accurate estimation with a limited computation cost compared with the existing approaches. To illustrate its practical use, we apply our approach to analyze the spatial pattern of the relationship between land surface temperature measured by satellites and air temperature measured by ground stations in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11579v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Zhang, Shan Yu, Zhengyuan Zhu, Xin Wang</dc:creator>
    </item>
    <item>
      <title>Automated Discovery of Functional Actual Causes in Complex Environments</title>
      <link>https://arxiv.org/abs/2404.10883</link>
      <description>arXiv:2404.10883v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) algorithms often struggle to learn policies that generalize to novel situations due to issues such as causal confusion, overfitting to irrelevant factors, and failure to isolate control of state factors. These issues stem from a common source: a failure to accurately identify and exploit state-specific causal relationships in the environment. While some prior works in RL aim to identify these relationships explicitly, they rely on informal domain-specific heuristics such as spatial and temporal proximity. Actual causality offers a principled and general framework for determining the causes of particular events. However, existing definitions of actual cause often attribute causality to a large number of events, even if many of them rarely influence the outcome. Prior work on actual causality proposes normality as a solution to this problem, but its existing implementations are challenging to scale to complex and continuous-valued RL environments. This paper introduces functional actual cause (FAC), a framework that uses context-specific independencies in the environment to restrict the set of actual causes. We additionally introduce Joint Optimization for Actual Cause Inference (JACI), an algorithm that learns from observational data to infer functional actual causes. We demonstrate empirically that FAC agrees with known results on a suite of examples from the actual causality literature, and JACI identifies actual causes with significantly higher accuracy than existing heuristic methods in a set of complex, continuous-valued environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10883v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caleb Chuck, Sankaran Vaidyanathan, Stephen Giguere, Amy Zhang, David Jensen, Scott Niekum</dc:creator>
    </item>
    <item>
      <title>What Hides behind Unfairness? Exploring Dynamics Fairness in Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2404.10942</link>
      <description>arXiv:2404.10942v1 Announce Type: cross 
Abstract: In sequential decision-making problems involving sensitive attributes like race and gender, reinforcement learning (RL) agents must carefully consider long-term fairness while maximizing returns. Recent works have proposed many different types of fairness notions, but how unfairness arises in RL problems remains unclear. In this paper, we address this gap in the literature by investigating the sources of inequality through a causal lens. We first analyse the causal relationships governing the data generation process and decompose the effect of sensitive attributes on long-term well-being into distinct components. We then introduce a novel notion called dynamics fairness, which explicitly captures the inequality stemming from environmental dynamics, distinguishing it from those induced by decision-making or inherited from the past. This notion requires evaluating the expected changes in the next state and the reward induced by changing the value of the sensitive attribute while holding everything else constant. To quantitatively evaluate this counterfactual concept, we derive identification formulas that allow us to obtain reliable estimations from data. Extensive experiments demonstrate the effectiveness of the proposed techniques in explaining, detecting, and reducing inequality in reinforcement learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10942v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihong Deng, Jing Jiang, Guodong Long, Chengqi Zhang</dc:creator>
    </item>
    <item>
      <title>Periodicity in New York State COVID-19 Hospitalizations Leveraged from the Variable Bandpass Periodic Block Bootstrap</title>
      <link>https://arxiv.org/abs/2404.11006</link>
      <description>arXiv:2404.11006v1 Announce Type: cross 
Abstract: The outbreak of the SARS-CoV-2 virus, which led to an unprecedented global pandemic, has underscored the critical importance of understanding seasonal patterns. This knowledge is fundamental for decision-making in healthcare and public health domains. Investigating the presence, intensity, and precise nature of seasonal trends, as well as these temporal patterns, is essential for forecasting future occurrences, planning interventions, and making informed decisions based on the evolution of events over time. This study employs the Variable Bandpass Periodic Block Bootstrap (VBPBB) to separate and analyze different periodic components by frequency in time series data, focusing on annually correlated (PC) principal components. Bootstrapping, a method used to estimate statistical sampling distributions through random sampling with replacement, is particularly useful in this context. Specifically, block bootstrapping, a model-independent resampling method suitable for time series data, is utilized. Its extensions are aimed at preserving the correlation structures inherent in PC processes. The VBPBB applies a bandpass filter to isolate the relevant PC frequency, thereby minimizing contamination from extraneous frequencies and noise. This approach significantly narrows the confidence intervals, enhancing the precision of estimated sampling distributions for the investigated periodic characteristics. Furthermore, we compared the outcomes of block bootstrapping for periodically correlated time series with VBPBB against those from more traditional bootstrapping methods. Our analysis shows VBPBB provides strong evidence of the existence of an annual seasonal PC pattern in hospitalization rates not detectible by other methods, providing timing and confidence intervals for their impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11006v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asmaa Ahmad, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Estimation for conditional moment models based on martingale difference divergence</title>
      <link>https://arxiv.org/abs/2404.11092</link>
      <description>arXiv:2404.11092v1 Announce Type: cross 
Abstract: We provide a new estimation method for conditional moment models via the martingale difference divergence (MDD).Our MDD-based estimation method is formed in the framework of a continuum of unconditional moment restrictions. Unlike the existing estimation methods in this framework, the MDD-based estimation method adopts a non-integrable weighting function, which could grab more information from unconditional moment restrictions than the integrable weighting function to enhance the estimation efficiency. Due to the nature of shift-invariance in MDD, our MDD-based estimation method can not identify the intercept parameters. To overcome this identification issue, we further provide a two-step estimation procedure for the model with intercept parameters. Under regularity conditions, we establish the asymptotics of the proposed estimators, which are not only easy-to-implement with analytic asymptotic variances, but also applicable to time series data with an unspecified form of conditional heteroskedasticity. Finally, we illustrate the usefulness of the proposed estimators by simulations and two real examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11092v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kunyang Song, Feiyu Jiang, Ke Zhu</dc:creator>
    </item>
    <item>
      <title>The Causal Chambers: Real Physical Systems as a Testbed for AI Methodology</title>
      <link>https://arxiv.org/abs/2404.11341</link>
      <description>arXiv:2404.11341v1 Announce Type: cross 
Abstract: In some fields of AI, machine learning and statistics, the validation of new methods and algorithms is often hindered by the scarcity of suitable real-world datasets. Researchers must often turn to simulated data, which yields limited information about the applicability of the proposed methods to real problems. As a step forward, we have constructed two devices that allow us to quickly and inexpensively produce large datasets from non-trivial but well-understood physical systems. The devices, which we call causal chambers, are computer-controlled laboratories that allow us to manipulate and measure an array of variables from these physical systems, providing a rich testbed for algorithms from a variety of fields. We illustrate potential applications through a series of case studies in fields such as causal discovery, out-of-distribution generalization, change point detection, independent component analysis, and symbolic regression. For applications to causal inference, the chambers allow us to carefully perform interventions. We also provide and empirically validate a causal model of each chamber, which can be used as ground truth for different tasks. All hardware and software is made open source, and the datasets are publicly available at causalchamber.org or through the Python package causalchamber.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11341v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan L. Gamella, Jonas Peters, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>DeepVARwT: Deep Learning for a VAR Model with Trend</title>
      <link>https://arxiv.org/abs/2209.10587</link>
      <description>arXiv:2209.10587v3 Announce Type: replace 
Abstract: The vector autoregressive (VAR) model has been used to describe the dependence within and across multiple time series. This is a model for stationary time series which can be extended to allow the presence of a deterministic trend in each series. Detrending the data either parametrically or nonparametrically before fitting the VAR model gives rise to more errors in the latter part. In this study, we propose a new approach called DeepVARwT that employs deep learning methodology for maximum likelihood estimation of the trend and the dependence structure at the same time. A Long Short-Term Memory (LSTM) network is used for this purpose. To ensure the stability of the model, we enforce the causality condition on the autoregressive coefficients using the transformation of Ansley &amp; Kohn (1986). We provide a simulation study and an application to real data. In the simulation study, we use realistic trend functions generated from real data and compare the estimates with true function/parameter values. In the real data application, we compare the prediction performance of this model with state-of-the-art models in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.10587v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xixi Li, Jingsong Yuan</dc:creator>
    </item>
    <item>
      <title>Subdata selection for big data regression: an improved approach</title>
      <link>https://arxiv.org/abs/2305.00218</link>
      <description>arXiv:2305.00218v2 Announce Type: replace 
Abstract: In the big data era researchers face a series of problems. Even standard approaches/methodologies, like linear regression, can be difficult or problematic with huge volumes of data. Traditional approaches for regression in big datasets may suffer due to the large sample size, since they involve inverting huge data matrices or even because the data cannot fit to the memory. Proposed approaches are based on selecting representative subdata to run the regression. Existing approaches select the subdata using information criteria and/or properties from orthogonal arrays. In the present paper we improve existing algorithms providing a new algorithm that is based on D-optimality approach. We provide simulation evidence for its performance. Evidence about the parameters of the proposed algorithm is also provided in order to clarify the trade-offs between execution time and information gain. Real data applications are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00218v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasilis Chasiotis, Dimitris Karlis</dc:creator>
    </item>
    <item>
      <title>Efficient Computation of High-Dimensional Penalized Generalized Linear Mixed Models by Latent Factor Modeling of the Random Effects</title>
      <link>https://arxiv.org/abs/2305.08201</link>
      <description>arXiv:2305.08201v2 Announce Type: replace 
Abstract: Modern biomedical datasets are increasingly high dimensional and exhibit complex correlation structures. Generalized Linear Mixed Models (GLMMs) have long been employed to account for such dependencies. However, proper specification of the fixed and random effects in GLMMs is increasingly difficult in high dimensions, and computational complexity grows with increasing dimension of the random effects. We present a novel reformulation of the GLMM using a factor model decomposition of the random effects, enabling scalable computation of GLMMs in high dimensions by reducing the latent space from a large number of random effects to a smaller set of latent factors. We also extend our prior work to estimate model parameters using a modified Monte Carlo Expectation Conditional Minimization algorithm, allowing us to perform variable selection on both the fixed and random effects simultaneously. We show through simulation that through this factor model decomposition, our method can fit high dimensional penalized GLMMs faster than comparable methods and more easily scale to larger dimensions not previously seen in existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.08201v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hillary M. Heiling (Department of Biostatistics, University of North Carolina Chapel Hill, Chapel Hill, NC), Naim U. Rashid (Department of Biostatistics, University of North Carolina Chapel Hill, Chapel Hill, NC), Quefeng Li (Department of Biostatistics, University of North Carolina Chapel Hill, Chapel Hill, NC), Xianlu L. Peng (Lineberger Comprehensive Cancer Center, University of North Carolina at Chapel Hill, Chapel Hill, NC), Jen Jen Yeh (Lineberger Comprehensive Cancer Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, Department of Surgery, University of North Carolina Chapel Hill, Chapel Hill, NC, Department of Pharmacology, University of North Carolina Chapel Hill, Chapel Hill, NC), Joseph G. Ibrahim (Department of Biostatistics, University of North Carolina Chapel Hill, Chapel Hill, NC)</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Genomic Data with Multiple Heterogeneous Outcomes</title>
      <link>https://arxiv.org/abs/2404.09119</link>
      <description>arXiv:2404.09119v2 Announce Type: replace 
Abstract: With the evolution of single-cell RNA sequencing techniques into a standard approach in genomics, it has become possible to conduct cohort-level causal inferences based on single-cell-level measurements. However, the individual gene expression levels of interest are not directly observable; instead, only repeated proxy measurements from each individual's cells are available, providing a derived outcome to estimate the underlying outcome for each of many genes. In this paper, we propose a generic semiparametric inference framework for doubly robust estimation with multiple derived outcomes, which also encompasses the usual setting of multiple outcomes when the response of each unit is available. To reliably quantify the causal effects of heterogeneous outcomes, we specialize the analysis to standardized average treatment effects and quantile treatment effects. Through this, we demonstrate the use of the semiparametric inferential results for doubly robust estimators derived from both Von Mises expansions and estimating equations. A multiple testing procedure based on Gaussian multiplier bootstrap is tailored for doubly robust estimators to control the false discovery exceedance rate. Applications in single-cell CRISPR perturbation analysis and individual-level differential expression analysis demonstrate the utility of the proposed methods and offer insights into the usage of different estimands for causal inference in genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09119v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Zhenghao Zeng, Edward H. Kennedy, Larry Wasserman, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Quantile Regression</title>
      <link>https://arxiv.org/abs/2404.10495</link>
      <description>arXiv:2404.10495v2 Announce Type: replace 
Abstract: Quantile regression is a powerful tool for detecting exposure-outcome associations given covariates across different parts of the outcome's distribution, but has two major limitations when the aim is to infer the effect of an exposure. Firstly, the exposure coefficient estimator may not converge to a meaningful quantity when the model is misspecified, and secondly, variable selection methods may induce bias and excess uncertainty, rendering inferences biased and overly optimistic. In this paper, we address these issues via partially linear quantile regression models which parametrize the conditional association of interest, but do not restrict the association with other covariates in the model. We propose consistent estimators for the unknown model parameter by mapping it onto a nonparametric main effect estimand that captures the (conditional) association of interest even when the quantile model is misspecified. This estimand is estimated using the efficient influence function under the nonparametric model, allowing for the incorporation of data-adaptive procedures such as variable selection and machine learning. Our approach provides a flexible and reliable method for detecting associations that is robust to model misspecification and excess uncertainty induced by variable selection methods. The proposal is illustrated using simulation studies and data on annual health care costs associated with excess body weight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10495v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgi Baklicharov, Christophe Ley, Vanessa Gorasso, Brecht Devleesschauwer, Stijn Vansteelandt</dc:creator>
    </item>
    <item>
      <title>Change Acceleration and Detection</title>
      <link>https://arxiv.org/abs/1710.00915</link>
      <description>arXiv:1710.00915v4 Announce Type: replace-cross 
Abstract: A novel sequential change detection problem is proposed, in which the goal is to not only detect but also accelerate the change. Specifically, it is assumed that the sequentially collected observations are responses to treatments selected in real time. The assigned treatments determine the pre-change and post-change distributions of the responses and also influence when the change happens. The goal is to find a treatment assignment rule and a stopping rule that minimize the expected total number of observations subject to a user-specified bound on the false alarm probability. The optimal solution is obtained under a general Markovian change-point model. Moreover, an alternative procedure is proposed, whose applicability is not restricted to Markovian change-point models and whose design requires minimal computation. For a large class of change-point models, the proposed procedure is shown to achieve the optimal performance in an asymptotic sense. Finally, its performance is found in simulation studies to be comparable to the optimal, uniformly with respect to the error probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:1710.00915v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanglei Song, Georgios Fellouris</dc:creator>
    </item>
    <item>
      <title>Local Projection Inference in High Dimensions</title>
      <link>https://arxiv.org/abs/2209.03218</link>
      <description>arXiv:2209.03218v3 Announce Type: replace-cross 
Abstract: In this paper, we estimate impulse responses by local projections in high-dimensional settings. We use the desparsified (de-biased) lasso to estimate the high-dimensional local projections, while leaving the impulse response parameter of interest unpenalized. We establish the uniform asymptotic normality of the proposed estimator under general conditions. Finally, we demonstrate small sample performance through a simulation study and consider two canonical applications in macroeconomic research on monetary policy and government spending.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.03218v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/ectj/utae012</arxiv:DOI>
      <dc:creator>Robert Adamek, Stephan Smeekes, Ines Wilms</dc:creator>
    </item>
    <item>
      <title>Additive Covariance Matrix Models: Modelling Regional Electricity Net-Demand in Great Britain</title>
      <link>https://arxiv.org/abs/2211.07451</link>
      <description>arXiv:2211.07451v3 Announce Type: replace-cross 
Abstract: Forecasts of regional electricity net-demand, consumption minus embedded generation, are an essential input for reliable and economic power system operation, and energy trading. While such forecasts are typically performed region by region, operations such as managing power flows require spatially coherent joint forecasts, which account for cross-regional dependencies. Here, we forecast the joint distribution of net-demand across the 14 regions constituting Great Britain's electricity network. Joint modelling is complicated by the fact that the net-demand variability within each region, and the dependencies between regions, vary with temporal, socio-economical and weather-related factors. We accommodate for these characteristics by proposing a multivariate Gaussian model based on a modified Cholesky parametrisation, which allows us to model each unconstrained parameter via an additive model. Given that the number of model parameters and covariates is large, we adopt a semi-automated approach to model selection, based on gradient boosting. In addition to comparing the forecasting performance of several versions of the proposed model with that of two non-Gaussian copula-based models, we visually explore the model output to interpret how the covariates affect net-demand variability and dependencies.
  The code for reproducing the results in this paper is available at https://doi.org/10.5281/zenodo.7315105, while methods for building and fitting multivariate Gaussian additive models are provided by the SCM R package, available at https://github.com/VinGioia90/SCM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.07451v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V. Gioia, M. Fasiolo, J. Browell, R. Bellio</dc:creator>
    </item>
    <item>
      <title>glmmPen: High Dimensional Penalized Generalized Linear Mixed Models</title>
      <link>https://arxiv.org/abs/2305.08204</link>
      <description>arXiv:2305.08204v2 Announce Type: replace-cross 
Abstract: Generalized linear mixed models (GLMMs) are widely used in research for their ability to model correlated outcomes with non-Gaussian conditional distributions. The proper selection of fixed and random effects is a critical part of the modeling process since model misspecification may lead to significant bias. However, the joint selection of fixed and random effects has historically been limited to lower-dimensional GLMMs, largely due to the use of criterion-based model selection strategies. Here we present the R package glmmPen, one of the first to select fixed and random effects in higher dimension using a penalized GLMM modeling framework. Model parameters are estimated using a Monte Carlo Expectation Conditional Minimization (MCECM) algorithm, which leverages Stan and RcppArmadillo for increased computational efficiency. Our package supports the Binomial, Gaussian, and Poisson families and multiple penalty functions. In this manuscript we discuss the modeling procedure, estimation scheme, and software implementation through application to a pancreatic cancer subtyping study. Simulation results show our method has good performance in selecting both the fixed and random effects in high dimensional GLMMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.08204v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hillary M. Heiling (Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC), Naim U. Rashid (Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC), Quefeng Li (Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC), Joseph G. Ibrahim (Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC)</dc:creator>
    </item>
    <item>
      <title>Adaptive Lasso, Transfer Lasso, and Beyond: An Asymptotic Perspective</title>
      <link>https://arxiv.org/abs/2308.15838</link>
      <description>arXiv:2308.15838v2 Announce Type: replace-cross 
Abstract: This paper presents a comprehensive exploration of the theoretical properties inherent in the Adaptive Lasso and the Transfer Lasso. The Adaptive Lasso, a well-established method, employs regularization divided by initial estimators and is characterized by asymptotic normality and variable selection consistency. In contrast, the recently proposed Transfer Lasso employs regularization subtracted by initial estimators with the demonstrated capacity to curtail non-asymptotic estimation errors. A pivotal question thus emerges: Given the distinct ways the Adaptive Lasso and the Transfer Lasso employ initial estimators, what benefits or drawbacks does this disparity confer upon each method? This paper conducts a theoretical examination of the asymptotic properties of the Transfer Lasso, thereby elucidating its differentiation from the Adaptive Lasso. Informed by the findings of this analysis, we introduce a novel method, one that amalgamates the strengths and compensates for the weaknesses of both methods. The paper concludes with validations of our theory and comparisons of the methods via simulation experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15838v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masaaki Takada, Hironori Fujisawa</dc:creator>
    </item>
    <item>
      <title>Optimization-based frequentist confidence intervals for functionals in constrained inverse problems: Resolving the Burrus conjecture</title>
      <link>https://arxiv.org/abs/2310.02461</link>
      <description>arXiv:2310.02461v3 Announce Type: replace-cross 
Abstract: We present an optimization-based framework to construct confidence intervals for functionals in constrained inverse problems, ensuring valid one-at-a-time frequentist coverage guarantees. Our approach builds upon the now-called strict bounds intervals, originally pioneered by Burrus (1965) and Rust and Burrus (1972), which offer ways to directly incorporate any side information about the parameters during inference without introducing external biases. This family of methods allows for uncertainty quantification in ill-posed inverse problems without needing to select a regularizing prior. By tying optimization-based intervals to an inversion of a constrained likelihood ratio test, we translate interval coverage guarantees into type I error control and characterize the resulting interval via solutions to optimization problems. Along the way, we refute the Burrus conjecture, which posited that, for possibly rank-deficient linear Gaussian models with positivity constraints, a correction based on the quantile of the chi-squared distribution with one degree of freedom suffices to shorten intervals while maintaining frequentist coverage guarantees. Our framework provides a novel approach to analyzing the conjecture, and we construct a counterexample employing a stochastic dominance argument, which we also use to disprove a general form of the conjecture. We illustrate our framework with several numerical examples and provide directions for extensions beyond the Rust-Burrus method for nonlinear, non-Gaussian settings with general constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02461v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pau Batlle, Pratik Patil, Michael Stanley, Houman Owhadi, Mikael Kuusela</dc:creator>
    </item>
    <item>
      <title>MixEHR-SurG: a joint proportional hazard and guided topic model for inferring mortality-associated topics from electronic health records</title>
      <link>https://arxiv.org/abs/2312.13454</link>
      <description>arXiv:2312.13454v3 Announce Type: replace-cross 
Abstract: Survival models can help medical practitioners to evaluate the prognostic importance of clinical variables to patient outcomes such as mortality or hospital readmission and subsequently design personalized treatment regimes. Electronic Health Records (EHRs) hold the promise for large-scale survival analysis based on systematically recorded clinical features for each patient. However, existing survival models either do not scale to high dimensional and multi-modal EHR data or are difficult to interpret. In this study, we present a supervised topic model called MixEHR-SurG to simultaneously integrate heterogeneous EHR data and model survival hazard. Our contributions are three-folds: (1) integrating EHR topic inference with Cox proportional hazards likelihood; (2) integrating patient-specific topic hyperparameters using the PheCode concepts such that each topic can be identified with exactly one PheCode-associated phenotype; (3) multi-modal survival topic inference. This leads to a highly interpretable survival topic model that can infer PheCode-specific phenotype topics associated with patient mortality. We evaluated MixEHR-SurG using a simulated dataset and two real-world EHR datasets: the Quebec Congenital Heart Disease (CHD) data consisting of 8,211 subjects with 75,187 outpatient claim records of 1,767 unique ICD codes; the MIMIC-III consisting of 1,458 subjects with multi-modal EHR records. Compared to the baselines, MixEHR-SurG achieved a superior dynamic AUROC for mortality prediction, with a mean AUROC score of 0.89 in the simulation dataset and a mean AUROC of 0.645 on the CHD dataset. Qualitatively, MixEHR-SurG associates severe cardiac conditions with high mortality risk among the CHD patients after the first heart failure hospitalization and critical brain injuries with increased mortality among the MIMIC-III patients after their ICU discharge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13454v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yixuan Li, Archer Y. Yang, Ariane Marelli, Yue Li</dc:creator>
    </item>
    <item>
      <title>JCGM 101-compliant uncertainty evaluation using virtual experiments</title>
      <link>https://arxiv.org/abs/2404.10530</link>
      <description>arXiv:2404.10530v2 Announce Type: replace-cross 
Abstract: Virtual experiments (VEs), a modern tool in metrology, can be used to help perform an uncertainty evaluation for the measurand. Current guidelines in metrology do not cover the many possibilities to incorporate VEs into an uncertainty evaluation, and it is often difficult to assess if the intended use of a VE complies with said guidelines. In recent work, it was shown that a VE can be used in conjunction with real measurement data and a Monte Carlo procedure to produce equal results to a supplement of the Guide to the Expression of Uncertainty in Measurement. However, this was shown only for linear measurement models. In this work, we extend this Monte Carlo approach to a common class of non-linear measurement models and more complex VEs, providing a reference approach for suitable uncertainty evaluations involving VEs. Numerical examples are given to show that the theoretical derivations hold in a practical scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10530v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Finn Hughes, Manuel Marschall, Gerd W\"ubbeler, Gertjan Kok, Marcel van Dijk, Clemens Elster</dc:creator>
    </item>
  </channel>
</rss>
