<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Jun 2025 04:04:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Combine and conquer: model averaging for out-of-distribution forecasting</title>
      <link>https://arxiv.org/abs/2506.03693</link>
      <description>arXiv:2506.03693v1 Announce Type: new 
Abstract: Travel behaviour modellers have an increasingly diverse set of models at their disposal, ranging from traditional econometric structures to models from mathematical psychology and data-driven approaches from machine learning. A key question arises as to how well these different models perform in prediction, especially when considering trips of different characteristics from those used in estimation, i.e. out-of-distribution prediction, and whether better predictions can be obtained by combining insights from the different models. Across two case studies, we show that while data-driven approaches excel in predicting mode choice for trips within the distance bands used in estimation, beyond that range, the picture is fuzzy. To leverage the relative advantages of the different model families and capitalise on the notion that multiple `weak' models can result in more robust models, we put forward the use of a model averaging approach that allocates weights to different model families as a function of the \emph{distance} between the characteristics of the trip for which predictions are made, and those used in model estimation. Overall, we see that the model averaging approach gives larger weight to models with stronger behavioural or econometric underpinnings the more we move outside the interval of trip distances covered in estimation. Across both case studies, we show that our model averaging approach obtains improved performance both on the estimation and validation data, and crucially also when predicting mode choices for trips of distances outside the range used in estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03693v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stephane Hess, Sander van Cranenburgh</dc:creator>
    </item>
    <item>
      <title>Conventional and Fuzzy Data Envelopment Analysis with deaR</title>
      <link>https://arxiv.org/abs/2506.03766</link>
      <description>arXiv:2506.03766v1 Announce Type: new 
Abstract: deaR is a recently developed R package for data envelopment analysis (DEA) that implements a large number of conventional and fuzzy models, along with super-efficiency models, cross-efficiency analysis, Malmquist index, bootstrapping, and metafrontier analysis. It should be noted that deaR is the only package to date that incorporates Kao-Liu, Guo-Tanaka and possibilistic fuzzy models. The versatility of the package allows the user to work with different returns to scale and orientations, as well as to consider special features, namely non-controllable, non-discretionary or undesirable variables. Moreover, it includes novel graphical representations that can help the user to display the results. This paper is a comprehensive description of deaR, reviewing all implemented models and giving examples of use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03766v1</guid>
      <category>econ.EM</category>
      <category>cs.MS</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vicente J. Bolos, Rafael Benitez, Vicente Coll-Serrano</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Learning in Finance</title>
      <link>https://arxiv.org/abs/2506.03780</link>
      <description>arXiv:2506.03780v1 Announce Type: cross 
Abstract: Recent advances in machine learning have shown promising results for financial prediction using large, over-parameterized models. This paper provides theoretical foundations and empirical validation for understanding when and how these methods achieve predictive success. I examine three key aspects of high-dimensional learning in finance. First, I prove that within-sample standardization in Random Fourier Features implementations fundamentally alters the underlying Gaussian kernel approximation, replacing shift-invariant kernels with training-set dependent alternatives. Second, I derive sample complexity bounds showing when reliable learning becomes information-theoretically impossible under weak signal-to-noise ratios typical in finance. Third, VC-dimension analysis reveals that ridgeless regression's effective complexity is bounded by sample size rather than nominal feature dimension. Comprehensive numerical validation confirms these theoretical predictions, revealing systematic breakdown of claimed theoretical properties across realistic parameter ranges. These results show that when sample size is small and features are high-dimensional, observed predictive success is necessarily driven by low-complexity artifacts, not genuine high-dimensional learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03780v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hasan Fallahgoul</dc:creator>
    </item>
    <item>
      <title>What Makes Treatment Effects Identifiable? Characterizations and Estimators Beyond Unconfoundedness</title>
      <link>https://arxiv.org/abs/2506.04194</link>
      <description>arXiv:2506.04194v1 Announce Type: cross 
Abstract: Most of the widely used estimators of the average treatment effect (ATE) in causal inference rely on the assumptions of unconfoundedness and overlap. Unconfoundedness requires that the observed covariates account for all correlations between the outcome and treatment. Overlap requires the existence of randomness in treatment decisions for all individuals. Nevertheless, many types of studies frequently violate unconfoundedness or overlap, for instance, observational studies with deterministic treatment decisions -- popularly known as Regression Discontinuity designs -- violate overlap.
  In this paper, we initiate the study of general conditions that enable the identification of the average treatment effect, extending beyond unconfoundedness and overlap. In particular, following the paradigm of statistical learning theory, we provide an interpretable condition that is sufficient and nearly necessary for the identification of ATE. Moreover, this condition characterizes the identification of the average treatment effect on the treated (ATT) and can be used to characterize other treatment effects as well. To illustrate the utility of our condition, we present several well-studied scenarios where our condition is satisfied and, hence, we prove that ATE can be identified in regimes that prior works could not capture. For example, under mild assumptions on the data distributions, this holds for the models proposed by Tan (2006) and Rosenbaum (2002), and the Regression Discontinuity design model introduced by Thistlethwaite and Campbell (1960). For each of these scenarios, we also show that, under natural additional assumptions, ATE can be estimated from finite samples.
  We believe these findings open new avenues for bridging learning-theoretic insights and causal inference methodologies, particularly in observational studies with complex treatment mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04194v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Cai, Alkis Kalavasis, Katerina Mamali, Anay Mehrotra, Manolis Zampetakis</dc:creator>
    </item>
    <item>
      <title>Identifying Causal Effects in Information Provision Experiments</title>
      <link>https://arxiv.org/abs/2309.11387</link>
      <description>arXiv:2309.11387v3 Announce Type: replace 
Abstract: Information treatments often shift beliefs more for people with weak belief effects. Since standard TSLS and panel specifications in information provision experiments have weights proportional to belief updating in the first-stage, this dependence attenuates existing estimates. This is natural if people whose decisions depend on their beliefs gather information before the experiment. I propose a local least squares estimator that identifies unweighted average effects in several classes of experiments under progressively stronger versions of Bayesian updating. In five of six recent studies, average effects are larger than-in several cases more than double-estimates in standard specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.11387v3</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dylan Balla-Elliott</dc:creator>
    </item>
    <item>
      <title>Lee Bounds with a Continuous Treatment in Sample Selection</title>
      <link>https://arxiv.org/abs/2411.04312</link>
      <description>arXiv:2411.04312v4 Announce Type: replace 
Abstract: We study causal inference in sample selection models where a continuous or multivalued treatment affects both outcomes and their observability (e.g., employment or survey responses). We generalized the widely used Lee (2009)'s bounds for binary treatment effects. Our key innovation is a sufficient treatment values assumption that imposes weak restrictions on selection heterogeneity and is implicit in separable threshold-crossing models, including monotone effects on selection. Our double debiased machine learning estimator enables nonparametric and high-dimensional methods, using covariates to tighten the bounds and capture heterogeneity. Applications to Job Corps and CCC program evaluations reinforce prior findings under weaker assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04312v4</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying-Ying Lee, Chu-An Liu</dc:creator>
    </item>
    <item>
      <title>Robust and Agnostic Learning of Conditional Distributional Treatment Effects</title>
      <link>https://arxiv.org/abs/2205.11486</link>
      <description>arXiv:2205.11486v3 Announce Type: replace-cross 
Abstract: The conditional average treatment effect (CATE) is the best measure of individual causal effects given baseline covariates. However, the CATE only captures the (conditional) average, and can overlook risks and tail events, which are important to treatment choice. In aggregate analyses, this is usually addressed by measuring the distributional treatment effect (DTE), such as differences in quantiles or tail expectations between treatment groups. Hypothetically, one can similarly fit conditional quantile regressions in each treatment group and take their difference, but this would not be robust to misspecification or provide agnostic best-in-class predictions. We provide a new robust and model-agnostic methodology for learning the conditional DTE (CDTE) for a class of problems that includes conditional quantile treatment effects, conditional super-quantile treatment effects, and conditional treatment effects on coherent risk measures given by $f$-divergences. Our method is based on constructing a special pseudo-outcome and regressing it on covariates using any regression learner. Our method is model-agnostic in that it can provide the best projection of CDTE onto the regression model class. Our method is robust in that even if we learn these nuisances nonparametrically at very slow rates, we can still learn CDTEs at rates that depend on the class complexity and even conduct inferences on linear projections of CDTEs. We investigate the behavior of our proposal in simulations, as well as in a case study of 401(k) eligibility effects on wealth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.11486v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>PMLR 206 (2023) 6037-6060</arxiv:journal_reference>
      <dc:creator>Nathan Kallus, Miruna Oprescu</dc:creator>
    </item>
    <item>
      <title>Torch-Choice: A PyTorch Package for Large-Scale Choice Modeling with Python</title>
      <link>https://arxiv.org/abs/2304.01906</link>
      <description>arXiv:2304.01906v4 Announce Type: replace-cross 
Abstract: The $\texttt{torch-choice}$ is an open-source library for flexible, fast choice modeling with Python and PyTorch. $\texttt{torch-choice}$ provides a $\texttt{ChoiceDataset}$ data structure to manage databases flexibly and memory-efficiently. The paper demonstrates constructing a $\texttt{ChoiceDataset}$ from databases of various formats and functionalities of $\texttt{ChoiceDataset}$. The package implements two widely used models, namely the multinomial logit and nested logit models, and supports regularization during model estimation. The package incorporates the option to take advantage of GPUs for estimation, allowing it to scale to massive datasets while being computationally efficient. Models can be initialized using either R-style formula strings or Python dictionaries. We conclude with a comparison of the computational efficiencies of $\texttt{torch-choice}$ and $\texttt{mlogit}$ in R as (1) the number of observations increases, (2) the number of covariates increases, and (3) the expansion of item sets. Finally, we demonstrate the scalability of $\texttt{torch-choice}$ on large-scale datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01906v4</guid>
      <category>cs.LG</category>
      <category>cs.MS</category>
      <category>econ.EM</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Du, Ayush Kanodia, Susan Athey</dc:creator>
    </item>
  </channel>
</rss>
