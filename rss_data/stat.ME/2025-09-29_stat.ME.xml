<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Sep 2025 03:28:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Modeling discrete lattice data using the Potts and tapered Potts models</title>
      <link>https://arxiv.org/abs/2509.21478</link>
      <description>arXiv:2509.21478v1 Announce Type: new 
Abstract: The Ising and Potts models, among the most important models in statistical physics, have been used for modeling binary and multinomial data on lattices in a wide variety of disciplines such as psychology, image analysis, biology, and forestry. However, these models have several well known shortcomings: (i) they can result in poorly fitting models, that is, simulations from fitted models often do not produce realizations that look like the observed data; (ii) phase transitions and the presence of ground states introduce significant challenges for statistical inference, model interpretation, and goodness of fit; (iii) intractable normalizing constants that are functions of the model parameters pose serious computational problems for likelihood-based inference.
  Here we develop a tapered version of the Ising and Potts models that addresses issues (i) and (ii). We develop efficient Markov Chain Monte Carlo Maximum Likelihood Estimation (MCMCMLE) algorithms that address issue (iii). We perform an extensive simulation study for the classical and Tapered Potts models that provide insights regarding the issues generated by the phase transition and ground states. Finally, we offer practical recommendations for modeling and computation based on applications of our approach to simulated data as well as data from the 2021 National Land Cover Database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21478v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Paula Duenas-Herrera, Stephen Berg, Murali Haran</dc:creator>
    </item>
    <item>
      <title>Conditional predictive inference with $L^k$-coverage control</title>
      <link>https://arxiv.org/abs/2509.21691</link>
      <description>arXiv:2509.21691v1 Announce Type: new 
Abstract: We consider the problem of distribution-free conditional predictive inference. Prior work has established that achieving exact finite-sample control of conditional coverage without distributional assumptions is impossible, in the sense that it necessarily results in trivial prediction sets. While several lines of work have proposed methods targeting relaxed notions of conditional coverage guarantee, the inherent difficulty of the problem typically leads such methods to offer only approximate guarantees or yield less direct interpretations, even with the relaxations. In this work, we propose an inferential target as a relaxed version of conditional predictive inference that is achievable with exact distribution-free finite sample control, while also offering intuitive interpretations. One of the key ideas, though simple, is to view conditional coverage as a function rather than a scalar, and thereby aim to control its function norm. We propose a procedure that controls the $L^k$-norm -- while primarily focusing on the $L^2$-norm -- of a relaxed notion of conditional coverage, adapting to different approaches depending on the choice of hyperparameter (e.g., local-conditional coverage, smoothed conditional coverage, or conditional coverage for a perturbed sample). We illustrate the performance of our procedure as a tool for conditional predictive inference, through simulations and experiments on a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21691v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Zhimei Ren</dc:creator>
    </item>
    <item>
      <title>Optimal Stopping for Sequential Bayesian Experimental Design</title>
      <link>https://arxiv.org/abs/2509.21734</link>
      <description>arXiv:2509.21734v1 Announce Type: new 
Abstract: In sequential Bayesian experimental design, the number of experiments is usually fixed in advance. In practice, however, campaigns may terminate early, raising the fundamental question: when should one stop? Threshold-based rules are simple to implement but inherently myopic, as they trigger termination based on a fixed criterion while ignoring the expected future information gain that additional experiments might provide. We develop a principled Bayesian framework for optimal stopping in sequential experimental design, formulated as a Markov decision process where stopping and design policies are jointly optimized. We prove that the optimal rule is to stop precisely when the immediate terminal reward outweighs the expected continuation value. To learn such policies, we introduce a policy gradient method, but show that na\"ive joint optimization suffers from circular dependencies that destabilize training. We resolve this with a curriculum learning strategy that gradually transitions from forced continuation to adaptive stopping. Numerical studies on a linear-Gaussian benchmark and a contaminant source detection problem demonstrate that curriculum learning achieves stable convergence and outperforms vanilla methods, particularly in settings with strong sequential dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21734v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Cheng, Xun Huan</dc:creator>
    </item>
    <item>
      <title>Federated Learning of Quantile Inference under Local Differential Privacy</title>
      <link>https://arxiv.org/abs/2509.21800</link>
      <description>arXiv:2509.21800v1 Announce Type: new 
Abstract: In this paper, we investigate federated learning for quantile inference under local differential privacy (LDP). We propose an estimator based on local stochastic gradient descent (SGD), whose local gradients are perturbed via a randomized mechanism with global parameters, making the procedure tolerant of communication and storage constraints without compromising statistical efficiency. Although the quantile loss and its corresponding gradient do not satisfy standard smoothness conditions typically assumed in existing literature, we establish asymptotic normality for our estimator as well as a functional central limit theorem. The proposed method accommodates data heterogeneity and allows each server to operate with an individual privacy budget. Furthermore, we construct confidence intervals for the target value through a self-normalization approach, thereby circumventing the need to estimate additional nuisance parameters. Extensive numerical experiments and real data application validate the theoretical guarantees of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21800v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leheng Cai, Qirui Hu, Shuyuan Wu</dc:creator>
    </item>
    <item>
      <title>General CoVaR Based on Entropy Pooling</title>
      <link>https://arxiv.org/abs/2509.21904</link>
      <description>arXiv:2509.21904v1 Announce Type: new 
Abstract: We propose a general CoVaR framework that extends the traditional CoVaR by incorporating diverse expert views and information, such as asset moment characteristics, quantile insights, and perspectives on the relative loss distribution between two assets. To integrate these expert views effectively while minimizing deviations from the prior distribution, we employ the entropy pooling method to derive the posterior distribution, which in turn enables us to compute the general CoVaR. Assuming bivariate normal distributions, we derive its analytical expressions under various perspectives. Sensitivity analysis reveals that CoVaR exhibits a linear relationship with both the expectations of the variables in the views and the differences in expectations between them. In contrast, CoVaR shows nonlinear dependencies with respect to the variance, quantiles, and correlation within these views.
  Empirical analysis of the US banking system during the Federal Reserve's interest rate hikes demonstrates the effectiveness of the general CoVaR when expert views are appropriately specified. Furthermore, we extend this framework to the general $\Delta$CoVaR, which allows for the assessment of risk spillover effects from various perspectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21904v1</guid>
      <category>stat.ME</category>
      <category>q-fin.MF</category>
      <category>q-fin.RM</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuhong Xu, Xinyao Zhao</dc:creator>
    </item>
    <item>
      <title>Deep learning for interval-censored failure time data from case-cohort studies</title>
      <link>https://arxiv.org/abs/2509.22081</link>
      <description>arXiv:2509.22081v1 Announce Type: new 
Abstract: Interval-censored data are common in fields such as epidemiology and demography. When the failure event of interest is relatively rare and the collection of covariates is costly, researchers often adopt the case-cohort design to reduce study costs. However, existing studies typically rely on the assumption of linearity in modeling covariates, which may not capture the complex and nonlinear relationships present in real data. To address this limitation, we consider a class of transformation models with unspecified covariate-dependent functions. We propose a sieve maximum weighted likelihood approach for interval-censored data arising from the case-cohort design, which combines deep neural networks with Bernstein polynomials. The method employs a deep neural network to flexibly represent the covariate-dependent function and uses Bernstein polynomials to approximate the cumulative baseline hazard function. We establish the consistency and convergence rate of the proposed estimator and show that the resulting nonparametric deep neural network estimator attains the minimax optimal rate of convergence (up to a polylogarithmic factor). Simulation studies suggest that the proposed method performs well in practice. Finally, we apply the method to a real dataset and use the SHAP (Shapley Additive Explanations) approach to attribute the neural network predictions of the covariate-dependent function to covariates. The results indicate that our method is both accurate and interpretable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22081v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeyu Xiao, Yonghong Long</dc:creator>
    </item>
    <item>
      <title>Bayesian approach to the PC component</title>
      <link>https://arxiv.org/abs/2509.22217</link>
      <description>arXiv:2509.22217v1 Announce Type: new 
Abstract: Time series with multiple periodically correlated components is a complex problem with comparatively limited prior research. Most existing time series models are designed to accommodate simple periodically correlated components and tend to be sensitive to over-parameterization and optimization issues and are also unable to model complex PC components patterns in a time series. Frequency separation techniques can be used to maintain the correlation structure of each specific PC component, whereas Bayesian techniques can combine new and existing prior information to update beliefs about these components. This study introduces a method to combine the frequency separation techniques and Bayesian techniques to forecast PC and MPC time series data in a two stage form which is expected to show the new method's suitability in modeling MPC components compared to classical methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22217v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Yao, Kai Zhang, Eric Rose, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Tail-robust estimation of factor-adjusted vector autoregressive models for high-dimensional time series</title>
      <link>https://arxiv.org/abs/2509.22235</link>
      <description>arXiv:2509.22235v1 Announce Type: new 
Abstract: We study the problem of modelling high-dimensional, heavy-tailed time series data via a factor-adjusted vector autoregressive (VAR) model, which simultaneously accounts for pervasive co-movements of the variables by a handful of factors, as well as their remaining interconnectedness using a sparse VAR model. To accommodate heavy tails, we adopt an element-wise truncation step followed by a two-stage estimation procedure for estimating the latent factors and the VAR parameter matrices. Assuming the existence of the $(2 + 2\epsilon)$-th moment only for some $\epsilon \in (0, 1)$, we derive the rates of estimation that make explicit the effect of heavy tails through $\epsilon$. Simulation studies and an application in macroeconomics demonstrate the competitive performance of the proposed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22235v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dylan Dijk, Haeran Cho</dc:creator>
    </item>
    <item>
      <title>Transfer Learning under Group-Label Shift: A Semiparametric Exponential Tilting Approach</title>
      <link>https://arxiv.org/abs/2509.22268</link>
      <description>arXiv:2509.22268v1 Announce Type: new 
Abstract: We propose a new framework for binary classification in transfer learning settings where both covariate and label distributions may shift between source and target domains. Unlike traditional covariate shift or label shift assumptions, we introduce a group-label shift assumption that accommodates subpopulation imbalance and mitigates spurious correlations, thereby improving robustness to real-world distributional changes. To model the joint distribution difference, we adopt a flexible exponential tilting formulation and establish mild, verifiable identification conditions via an instrumental variable strategy. We develop a computationally efficient two-step likelihood-based estimation procedure that combines logistic regression for the source outcome model with conditional likelihood estimation using both source and target covariates. We derive consistency and asymptotic normality for the resulting estimators, and extend the theory to receiver operating characteristic curves, the area under the curve, and other target functionals, addressing the nonstandard challenges posed by plug-in classifiers. Simulation studies demonstrate that our method outperforms existing alternatives under subpopulation shift scenarios. A semi-synthetic application using the waterbirds dataset further confirms the proposed method's ability to transfer information effectively and improve target-domain classification accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22268v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manli Cheng, Subha Maity, Qinglong Tian, Pengfei Li</dc:creator>
    </item>
    <item>
      <title>SensIAT: An R Package for Conducting Sensitivity Analysis of Randomized Trials with Irregular Assessment Times</title>
      <link>https://arxiv.org/abs/2509.22389</link>
      <description>arXiv:2509.22389v1 Announce Type: new 
Abstract: This paper introduces an R package SensIAT that implements a sensitivity analysis methodology, based on augmented inverse intensity weighting, for randomized trials with irregular and potentially informative assessment times. Targets of inference involve the population mean outcome in each treatment arm as well as the difference in these means (i.e., treatment effect) at specified times after randomization. This methodology is useful in settings where there is concern that study participants are either more, or less, likely to have assessments at times when their outcomes are worse. In such settings, unadjusted estimates can be biased. The methodology allows researchers to see how inferences are impacted by a range of assumptions about the strength and direction of informative timing in each arm, while incorporating flexible semi-parametric modeling. We describe the functions implemented in SensIAT and illustrate them through an analysis of a synthetic dataset motivated by the HAP2 asthma randomized clinical trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22389v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Redd, Yujing Gao, Bonnie B. Smith, Ravi Varadhan, Andrea J. Apter, Daniel O. Scharfstein</dc:creator>
    </item>
    <item>
      <title>Rescuing double robustness: safe estimation under complete misspecification</title>
      <link>https://arxiv.org/abs/2509.22446</link>
      <description>arXiv:2509.22446v1 Announce Type: new 
Abstract: Double robustness is a major selling point of semiparametric and missing data methodology. Its virtues lie in protection against partial nuisance misspecification and asymptotic semiparametric efficiency under correct nuisance specification. However, in many applications, complete nuisance misspecification should be regarded as the norm (or at the very least the expected default), and thus doubly robust estimators may behave fragilely. In fact, it has been amply verified empirically that these estimators can perform poorly when all nuisance functions are misspecified. Here, we first characterize this phenomenon of double fragility, and then propose a solution based on adaptive correction clipping (ACC). We argue that our ACC proposal is safe, in that it inherits the favorable properties of doubly robust estimators under correct nuisance specification, but its error is guaranteed to be bounded by a convex combination of the individual nuisance model errors, which prevents the instability caused by the compounding product of errors of doubly robust estimators. We also show that our proposal provides valid inference through the parametric bootstrap when nuisances are well-specified. We showcase the efficacy of our ACC estimator both through extensive simulations and by applying it to the analysis of Alzheimer's disease proteomics data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22446v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Testa, Francesca Chiaromonte, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Generative multi-fidelity modeling and downscaling via spatial autoregressive transport maps</title>
      <link>https://arxiv.org/abs/2509.22474</link>
      <description>arXiv:2509.22474v1 Announce Type: new 
Abstract: Spatial fields are often available at multiple fidelities or resolutions, where high-fidelity data is typically more costly to obtain than low-fidelity data. Statistical surrogates or emulators can predict high-fidelity fields from cheap low-fidelity output. We propose a highly scalable Bayesian approach that can learn the joint non-Gaussian distribution and nonlinear dependence structure of nonstationary spatial fields at multiple fidelities from a small number of training samples. Our method is based on fidelity-aware autoregressive GPs with suitably chosen regularization-inducing priors. Exploiting conjugacy, the integrated likelihood is available in closed form, enabling efficient hyperparameter optimization via stochastic gradient descent. After training, the method also characterizes in closed form the distribution of higher-fidelity fields given lower-fidelity data. In our numerical comparisons, we show that our approach substantially outperforms existing methods and that it can be used to characterize and simulate high-fidelity fine-scale climate behavior based on output from coarse (low-fidelity) global circulation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22474v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Calle-Saldarriaga, Paul F. V. Wiemann, Matthias Katzfuss</dc:creator>
    </item>
    <item>
      <title>A Multiplicative Instrumental Variable Model for Data Missing Not-at-Random</title>
      <link>https://arxiv.org/abs/2509.22499</link>
      <description>arXiv:2509.22499v1 Announce Type: new 
Abstract: Instrumental variable (IV) methods offer a valuable approach to account for outcome data missing not-at-random. A valid missing data instrument is a measured factor which (i) predicts the nonresponse process and (ii) is independent of the outcome in the underlying population. For point identification, all existing IV methods for missing data including the celebrated Heckman selection model, a priori restrict the extent of selection bias on the outcome scale, therefore potentially understating uncertainty due to missing data. In this work, we introduce an IV framework which allows the degree of selection bias on the outcome scale to remain completely unrestricted. The new approach instead relies for identification on (iii) a key multiplicative selection model, which posits that the instrument and any hidden common correlate of selection and the outcome, do not interact on the multiplicative scale. Interestingly, we establish that any regular statistical functional of the missing outcome is nonparametrically identified under (i)-(iii) via a single-arm Wald ratio estimand reminiscent of the standard Wald ratio estimand in causal inference. For estimation and inference, we characterize the influence function for any functional defined on a nonparametric model for the observed data, which we leverage to develop semiparametric multiply robust IV estimators. Several extensions of the methods are also considered, including the important practical setting of polytomous and continuous instruments. Simulation studies illustrate the favorable finite sample performance of proposed methods, which we further showcase in an HIV study nested within a household health survey study we conducted in Mochudi, Botswana, in which interviewer characteristics are used as instruments to correct for selection bias due to dependent nonresponse in the HIV component of the survey study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22499v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunshu Zhang, Chan Park, Jiewen Liu, Yonghoon Lee, Mengxin Yu, James M. Robins, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Modelling non-stationary extremal dependence through a geometric approach</title>
      <link>https://arxiv.org/abs/2509.22501</link>
      <description>arXiv:2509.22501v1 Announce Type: new 
Abstract: Non-stationary extremal dependence, whereby the relationship between the extremes of multiple variables evolves over time, is commonly observed in many environmental and financial data sets. However, most multivariate extreme value models are only suited to stationary data. A recent approach to multivariate extreme value modelling uses a geometric framework, whereby extremal dependence features are inferred through the limiting shapes of scaled sample clouds. This framework can capture a wide range of dependence structures, and a variety of inference procedures have been proposed in the stationary setting. In this work, we first extend the geometric framework to the non-stationary setting and outline assumptions to ensure the necessary convergence conditions hold. We then introduce a flexible, semi-parametric modelling framework for obtaining estimates of limit sets in the non-stationary setting. Through rigorous simulation studies, we demonstrate that our proposed framework can capture a wide range of dependence forms and is robust to different model formulations. We illustrate the proposed methods on financial returns data and present several practical uses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22501v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C. J. R. Murphy-Barltrop, J. L. Wadsworth, M. de Carvalho, B. D. Youngman</dc:creator>
    </item>
    <item>
      <title>Estimating average treatment effects when treatment data are absent in a target study</title>
      <link>https://arxiv.org/abs/2509.22543</link>
      <description>arXiv:2509.22543v1 Announce Type: new 
Abstract: Researchers are frequently interested in understanding the causal effect of treatment interventions. However, in some cases, the treatment of interest--readily available in a randomized controlled trial (RCT)--is either not directly measured or entirely unavailable in observational datasets. This challenge has motivated the development of stochastic incremental propensity score interventions which operate on post-treatment exposures affected by the treatment of interest with the aim of approximating the causal effects of the treatment intervention. Yet, a key challenge lies in the fact that the precise distributional shift of these post-treatment exposures induced by the treatment is typically unknown, making it uncertain whether the approximation truly reflects the causal effect of interest. The primary objective of this paper is to explore data integration methodologies to characterize a distribution of post-treatment exposures resulting from the treatment in an external dataset, and to use this information to estimate counterfactual mean outcomes under treatment interventions, in settings where the observational data lack treatment information and the external data may not contain measurements of the outcome of interest. We will discuss the underlying assumptions required for this approach and provide methodological guidance on estimation strategies to address these challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22543v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lan Wen, Aaron L Sarvet</dc:creator>
    </item>
    <item>
      <title>A Nonparametric Bayesian Solution of the Empirical Stochastic Inverse Problem</title>
      <link>https://arxiv.org/abs/2509.22597</link>
      <description>arXiv:2509.22597v1 Announce Type: new 
Abstract: The stochastic inverse problem is a key ingredient in making inferences, predictions, and decisions for complex science and engineering systems. We formulate and analyze a nonparametric Bayesian solution for the stochastic inverse problem. Key properties of the solution are proved and the convergence and error of a computational solution obtained by random sampling is analyzed. Several applications illustrate the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22597v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiyi Shi, Lei Yang, Jiarui Chi, Troy Butler, Haonan Wang, Derek Bingham, Don Estep</dc:creator>
    </item>
    <item>
      <title>Differentiable Structure Learning for General Binary Data</title>
      <link>https://arxiv.org/abs/2509.21658</link>
      <description>arXiv:2509.21658v1 Announce Type: cross 
Abstract: Existing methods for differentiable structure learning in discrete data typically assume that the data are generated from specific structural equation models. However, these assumptions may not align with the true data-generating process, which limits the general applicability of such methods. Furthermore, current approaches often ignore the complex dependence structure inherent in discrete data and consider only linear effects. We propose a differentiable structure learning framework that is capable of capturing arbitrary dependencies among discrete variables. We show that although general discrete models are unidentifiable from purely observational data, it is possible to characterize the complete set of compatible parameters and structures. Additionally, we establish identifiability up to Markov equivalence under mild assumptions. We formulate the learning problem as a single differentiable optimization task in the most general form, thereby avoiding the unrealistic simplifications adopted by previous methods. Empirical results demonstrate that our approach effectively captures complex relationships in discrete data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21658v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang Deng, Bryon Aragam</dc:creator>
    </item>
    <item>
      <title>SADA: Safe and Adaptive Inference with Multiple Black-Box Predictions</title>
      <link>https://arxiv.org/abs/2509.21707</link>
      <description>arXiv:2509.21707v1 Announce Type: cross 
Abstract: Real-world applications often face scarce labeled data due to the high cost and time requirements of gold-standard experiments, whereas unlabeled data are typically abundant. With the growing adoption of machine learning techniques, it has become increasingly feasible to generate multiple predicted labels using a variety of models and algorithms, including deep learning, large language models, and generative AI. In this paper, we propose a novel approach that safely and adaptively aggregates multiple black-box predictions with unknown quality while preserving valid statistical inference. Our method provides two key guarantees: (i) it never performs worse than using the labeled data alone, regardless of the quality of the predictions; and (ii) if any one of the predictions (without knowing which one) perfectly fits the ground truth, the algorithm adaptively exploits this to achieve either a faster convergence rate or the semiparametric efficiency bound. We demonstrate the effectiveness of the proposed algorithm through experiments on both synthetic and benchmark datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21707v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Shan, Yiming Dong, Jiwei Zhao</dc:creator>
    </item>
    <item>
      <title>Direct Bias-Correction Term Estimation for Propensity Scores and Average Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2509.22122</link>
      <description>arXiv:2509.22122v1 Announce Type: cross 
Abstract: This study considers the estimation of the average treatment effect (ATE). For ATE estimation, we estimate the propensity score through direct bias-correction term estimation. Let $\{(X_i, D_i, Y_i)\}_{i=1}^{n}$ be the observations, where $X_i \in \mathbb{R}^p$ denotes $p$-dimensional covariates, $D_i \in \{0, 1\}$ denotes a binary treatment assignment indicator, and $Y_i \in \mathbb{R}$ is an outcome. In ATE estimation, the bias-correction term $h_0(X_i, D_i) = \frac{1[D_i = 1]}{e_0(X_i)} - \frac{1[D_i = 0]}{1 - e_0(X_i)}$ plays an important role, where $e_0(X_i)$ is the propensity score, the probability of being assigned treatment $1$. In this study, we propose estimating $h_0$ (or equivalently the propensity score $e_0$) by directly minimizing the prediction error of $h_0$. Since the bias-correction term $h_0$ is essential for ATE estimation, this direct approach is expected to improve estimation accuracy for the ATE. For example, existing studies often employ maximum likelihood or covariate balancing to estimate $e_0$, but these approaches may not be optimal for accurately estimating $h_0$ or the ATE. We present a general framework for this direct bias-correction term estimation approach from the perspective of Bregman divergence minimization and conduct simulation studies to evaluate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22122v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Preventing Model Collapse Under Overparametrization: Optimal Mixing Ratios for Interpolation Learning and Ridge Regression</title>
      <link>https://arxiv.org/abs/2509.22341</link>
      <description>arXiv:2509.22341v1 Announce Type: cross 
Abstract: Model collapse occurs when generative models degrade after repeatedly training on their own synthetic outputs. We study this effect in overparameterized linear regression in a setting where each iteration mixes fresh real labels with synthetic labels drawn from the model fitted in the previous iteration. We derive precise generalization error formulae for minimum-$\ell_2$-norm interpolation and ridge regression under this iterative scheme. Our analysis reveals intriguing properties of the optimal mixing weight that minimizes long-term prediction error and provably prevents model collapse. For instance, in the case of min-$\ell_2$-norm interpolation, we establish that the optimal real-data proportion converges to the reciprocal of the golden ratio for fairly general classes of covariate distributions. Previously, this property was known only for ordinary least squares, and additionally in low dimensions. For ridge regression, we further analyze two popular model classes -- the random-effects model and the spiked covariance model -- demonstrating how spectral geometry governs optimal weighting. In both cases, as well as for isotropic features, we uncover that the optimal mixing ratio should be at least one-half, reflecting the necessity of favoring real-data over synthetic. We validate our theoretical results with extensive simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22341v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anvit Garg, Sohom Bhattacharya, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>A note on the relation between one-step, outcome regression and IPW-type estimators of parameters with the mixed bias property</title>
      <link>https://arxiv.org/abs/2509.22452</link>
      <description>arXiv:2509.22452v2 Announce Type: cross 
Abstract: Bruns-Smith et al. (2025) established an algebraic identity between the one-step estimator and a specific outcome regression-type estimator for a class of parameters that forms a strict subset of the class introduced in Chernozhukov et al. (2022), assuming both nuisance functions are estimated as linear combinations of given features. They conjectured that this identity extends to the broader mixed bias class introduced in Rotnitzky et al. (2021). In this note, we prove their conjecture and further extend the result to allow one of the nuisance estimators to be non-linear. We also relate these findings to the work of Robins et al. (2007), who established other identities linking one-step estimators to outcome regression-type and IPW-type estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22452v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Rotnitzky, Ezequiel Smucler, James M. Robins</dc:creator>
    </item>
    <item>
      <title>Online Resource Allocation with Average Budget Constraints</title>
      <link>https://arxiv.org/abs/2402.11425</link>
      <description>arXiv:2402.11425v5 Announce Type: replace 
Abstract: We consider the problem of online resource allocation with average budget constraints. At each time point the decision maker makes an irrevocable decision of whether to accept or reject a request before the next request arrives with the goal to maximize the cumulative rewards. In contrast to existing literature requiring the total resource consumption is below a certain level, we require the average resource consumption per accepted request does not exceed a given threshold. This problem can be casted as an online knapsack problem with exogenous random budget replenishment, and can find applications in various fields such as online anomaly detection, sequential advertising, and per-capita public service providers. We start with general arrival distributions and show that a simple policy achieves a $O(\sqrt{T})$ regret. We complement the result by showing that such a regret growing rate is in general not improvable. We then shift our focus to discrete arrival distributions. We find that many existing re-solving heuristics in the online resource allocation literature, albeit achieve bounded loss in canonical settings, may incur a $\Omega(\sqrt{T})$ or even a $\Omega(T)$ regret. With the observation that canonical policies tend to be too optimistic and over accept arrivals, we propose a novel policy that incorporates budget safety buffers. It turns out that a little more safety can greatly enhance efficiency -- small additional logarithmic buffers suffice to reduce the regret from $\Omega(\sqrt{T})$ or even $\Omega(T)$ to $O(\ln^2 T)$. From a practical perspective, we extend the policy to the scenario with continuous arrival distributions, time-dependent information structures, as well as unknown $T$. We conduct both synthetic experiments and empirical applications on a time series data of New York City taxi passengers to validate the performance of our proposed policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11425v5</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruicheng Ao, Hongyu Chen, David Simchi-Levi, Feng Zhu</dc:creator>
    </item>
    <item>
      <title>Principal stratification with U-statistics under principal ignorability</title>
      <link>https://arxiv.org/abs/2403.08927</link>
      <description>arXiv:2403.08927v4 Announce Type: replace 
Abstract: Principal stratification is a popular framework for causal inference in the presence of an intermediate outcome. While the principal average treatment effects have traditionally been the default target of inference, it may not be sufficient when the interest lies in the relative favorability of one potential outcome over the other within the principal stratum. We thus introduce the principal generalized causal effect estimands, which extend the principal average causal effects to accommodate nonlinear contrast functions. Under principal ignorability, we expand the theoretical results in Jiang et al.(2022) to a much wider class of causal estimands in the presence of a binary intermediate variable. We develop identification formulas and derive the efficient influence functions of the generalized estimands for principal stratification analyses. These efficient influence functions motivate a set of multiply robust estimators and lay the ground for obtaining efficient debiased machine learning estimators via cross-fitting based on U-statistics. The proposed methods are illustrated through simulations and the analysis of a data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08927v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Fan Li</dc:creator>
    </item>
    <item>
      <title>Ensemble Prediction via Covariate-dependent Stacking</title>
      <link>https://arxiv.org/abs/2408.09755</link>
      <description>arXiv:2408.09755v3 Announce Type: replace 
Abstract: This study proposes a novel approach to ensemble prediction, called "covariate-dependent stacking" (CDST). Unlike traditional stacking and model averaging methods, CDST allows model weights to vary flexibly as a function of covariates, thereby enhancing predictive performance in complex scenarios. We formulate the covariate-dependent weights through combinations of basis functions and estimate them via cross-validation optimization. To analyze the theoretical properties, we establish an oracle inequality regarding the expected loss to be minimized for estimating model weights. Through comprehensive simulation studies and an application to large-scale land price prediction, we demonstrate that the CDST consistently outperforms conventional model averaging methods, particularly on datasets where base models fail to capture the underlying complexity. Our findings suggest that the CDST is especially valuable for, but not limited to, spatio-temporal prediction problems, offering a powerful tool for researchers and practitioners across a wide spectrum of data analysis fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09755v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Conformal Calibration of Statistical Confidence Sets</title>
      <link>https://arxiv.org/abs/2411.19368</link>
      <description>arXiv:2411.19368v2 Announce Type: replace 
Abstract: Constructing valid confidence sets is a crucial task in statistical inference, yet traditional methods often face challenges when dealing with complex models or limited observed sample sizes. These challenges are frequently encountered in modern applications, such as Likelihood-Free Inference (LFI). In these settings, confidence sets may fail to maintain a confidence level close to the nominal value. In this paper, we introduce two novel methods, TRUST and TRUST++, for calibrating confidence sets to achieve distribution-free conditional coverage. These methods rely entirely on simulated data from the statistical model to perform calibration. Leveraging insights from conformal prediction techniques adapted to the statistical inference context, our methods ensure both finite-sample local coverage and asymptotic conditional coverage as the number of simulations increases, even if n is small. They effectively handle nuisance parameters and provide computationally efficient uncertainty quantification for the estimated confidence sets. This allows users to assess whether additional simulations are necessary for robust inference. Through theoretical analysis and experiments on models with both tractable and intractable likelihoods, we demonstrate that our methods outperform existing approaches, particularly in small-sample regimes. This work bridges the gap between conformal prediction and statistical inference, offering practical tools for constructing valid confidence sets in complex models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19368v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Luben M. C. Cabezas, Guilherme P. Soares, Thiago R. Ramos, Rafael B. Stern, Rafael Izbicki</dc:creator>
    </item>
    <item>
      <title>Multivariable Behavioral Change Modeling of Epidemics in the Presence of Undetected Infections</title>
      <link>https://arxiv.org/abs/2503.00982</link>
      <description>arXiv:2503.00982v2 Announce Type: replace 
Abstract: Epidemic models are invaluable tools to understand and implement strategies to control the spread of infectious diseases, as well as to inform public health policies and resource allocation. However, current modeling approaches have limitations that reduce their practical utility, such as the exclusion of human behavioral change in response to the epidemic or ignoring the presence of undetected infectious individuals in the population. These limitations became particularly evident during the COVID-19 pandemic, underscoring the need for more accurate and informative models. Motivated by these challenges, we develop a novel Bayesian epidemic modeling framework to better capture the complexities of disease spread by incorporating behavioral responses and undetected infections. In particular, our framework makes three contributions: 1) leveraging additional data on hospitalizations and deaths in modeling the disease dynamics, 2) accounting for data uncertainty arising from the large presence of asymptomatic and undetected infections, and 3) allowing the population behavioral change to be dynamically influenced by multiple data sources (cases and deaths). We thoroughly investigate the properties of the proposed model via simulation, and illustrate its utility on COVID-19 data from Montreal and Miami.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00982v2</guid>
      <category>stat.ME</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caitlin Ward, Rob Deardon, Alexandra M. Schmidt</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework for Co-Mediators of Zero-Inflated Single-Cell RNA-Seq Data</title>
      <link>https://arxiv.org/abs/2507.06113</link>
      <description>arXiv:2507.06113v2 Announce Type: replace 
Abstract: Single-cell RNA sequencing (scRNA-seq) has revolutionized the study of cellular heterogeneity, enabling detailed molecular profiling at the individual cell level. However, integrating high-dimensional single-cell data into causal mediation analysis remains challenging due to zero inflation and complex mediator structures. We propose a novel mediation framework leveraging zero-inflated negative binomial models to characterize cell-level mediator distributions and beta regression for zero-inflation proportions. The model can identify expression level as well as expressed proportion that could mediate disease-leading causal pathway. Extensive simulation studies demonstrate improved power and controlled false discovery rates. We further illustrate the utility of this approach through application to ROSMAP single-cell transcriptomic data, uncovering biologically meaningful mediation effects that enhance understanding of disease mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06113v2</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seungjun Ahn, Li Chen, Maaike van Gerwen, Zhigang Li</dc:creator>
    </item>
    <item>
      <title>Chiseling: Powerful and Valid Subgroup Selection via Interactive Machine Learning</title>
      <link>https://arxiv.org/abs/2509.19490</link>
      <description>arXiv:2509.19490v2 Announce Type: replace 
Abstract: In regression and causal inference, controlled subgroup selection aims to identify, with inferential guarantees, a subgroup (defined as a subset of the covariate space) on which the average response or treatment effect is above a given threshold. E.g., in a clinical trial, it may be of interest to find a subgroup with a positive average treatment effect. However, existing methods either lack inferential guarantees, heavily restrict the search for the subgroup, or sacrifice efficiency by naive data splitting. We propose a novel framework called chiseling that allows the analyst to interactively refine and test a candidate subgroup by iteratively shrinking it. The sole restriction is that the shrinkage direction only depends on the points outside the current subgroup, but otherwise the analyst may leverage any prior information or machine learning algorithm. Despite this flexibility, chiseling controls the probability that the discovered subgroup is null (e.g., has a non-positive average treatment effect) under minimal assumptions: for example, in randomized experiments, this inferential validity guarantee holds under only bounded moment conditions. When applied to a variety of simulated datasets and a real survey experiment, chiseling identifies substantially better subgroups than existing methods with inferential guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19490v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Cheng, Asher Spector, Lucas Janson</dc:creator>
    </item>
  </channel>
</rss>
