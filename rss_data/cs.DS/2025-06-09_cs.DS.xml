<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Jun 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Learning-Augmented Hierarchical Clustering</title>
      <link>https://arxiv.org/abs/2506.05495</link>
      <description>arXiv:2506.05495v1 Announce Type: new 
Abstract: Hierarchical clustering (HC) is an important data analysis technique in which the goal is to recursively partition a dataset into a tree-like structure while grouping together similar data points at each level of granularity. Unfortunately, for many of the proposed HC objectives, there exist strong barriers to approximation algorithms with the hardness of approximation. Thus, we consider the problem of hierarchical clustering given auxiliary information from natural oracles. Specifically, we focus on a *splitting oracle* which, when provided with a triplet of vertices $(u,v,w)$, answers (possibly erroneously) the pairs of vertices whose lowest common ancestor includes all three vertices in an optimal tree, i.e., identifying which vertex ``splits away'' from the others. Using such an oracle, we obtain the following results:
  - A polynomial-time algorithm that outputs a hierarchical clustering tree with $O(1)$-approximation to the Dasgupta objective (Dasgupta [STOC'16]).
  - A near-linear time algorithm that outputs a hierarchical clustering tree with $(1-o(1))$-approximation to the Moseley-Wang objective (Moseley and Wang [NeurIPS'17]).
  Under the plausible Small Set Expansion Hypothesis, no polynomial-time algorithm can achieve any constant approximation for Dasgupta's objective or $(1-C)$-approximation for the Moseley-Wang objective for some constant $C&gt;0$. As such, our results demonstrate that the splitting oracle enables algorithms to outperform standard HC approaches and overcome hardness constraints. Furthermore, our approaches extend to sublinear settings, in which we show new streaming and PRAM algorithms for HC with improved guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05495v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Braverman, Jon C. Ergun, Chen Wang, Samson Zhou</dc:creator>
    </item>
    <item>
      <title>On Differential Privacy for Adaptively Solving Search Problems via Sketching</title>
      <link>https://arxiv.org/abs/2506.05503</link>
      <description>arXiv:2506.05503v1 Announce Type: new 
Abstract: Recently differential privacy has been used for a number of streaming, data structure, and dynamic graph problems as a means of hiding the internal randomness of the data structure, so that multiple possibly adaptive queries can be made without sacrificing the correctness of the responses. Although these works use differential privacy to show that for some problems it is possible to tolerate $T$ queries using $\widetilde{O}(\sqrt{T})$ copies of a data structure, such results only apply to numerical estimation problems, and only return the cost of an optimization problem rather than the solution itself. In this paper, we investigate the use of differential privacy for adaptive queries to search problems, which are significantly more challenging since the responses to queries can reveal much more about the internal randomness than a single numerical query. We focus on two classical search problems: nearest neighbor queries and regression with arbitrary turnstile updates. We identify key parameters to these problems, such as the number of $c$-approximate near neighbors and the matrix condition number, and use different differential privacy techniques to design algorithms returning the solution vector with memory and time depending on these parameters. We give algorithms for each of these problems that achieve similar tradeoffs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05503v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiyuan Feng, Ying Feng, George Z. Li, Zhao Song, David P. Woodruff, Lichen Zhang</dc:creator>
    </item>
    <item>
      <title>Why is My Route Different Today? An Algorithm for Explaining Route Selection</title>
      <link>https://arxiv.org/abs/2506.05604</link>
      <description>arXiv:2506.05604v1 Announce Type: new 
Abstract: Users of routing services like Apple Maps, Google Maps, and Waze frequently wonder why a given route is proposed. This question particularly arises when dynamic conditions like traffic and road closures cause unusual routes to be proposed. While many dynamic conditions may exist in a road network at any time, only a small fraction of those conditions are typically relevant to a given user's route. In this work, we introduce the concept of a simple valid explanation (SVE), which consists of a small set of traffic-laden road segments that answer the following question: Which traffic conditions cause a particular shortest traffic-aware route to differ from the shortest traffic-free route? We give an efficient algorithm for finding SVEs and show that they theoretically and experimentally lead to small and interpretable answers to the question.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05604v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaron Schild, Sreenivas Gollapudi, Anupam Gupta, Kostas Kollias, Ali Sinop</dc:creator>
    </item>
    <item>
      <title>An O(log log n)-approximate budget feasible mechanism for subadditive valuations</title>
      <link>https://arxiv.org/abs/2506.04665</link>
      <description>arXiv:2506.04665v1 Announce Type: cross 
Abstract: In budget-feasible mechanism design, there is a set of items $U$, each owned by a distinct seller. The seller of item $e$ incurs a private cost $\overline{c}_e$ for supplying her item. A buyer wishes to procure a set of items from the sellers of maximum value, where the value of a set $S\subseteq U$ of items is given by a valuation function $v:2^U\to \mathbb{R}_+$. The buyer has a budget of $B \in \mathbb{R}_+$ for the total payments made to the sellers. We wish to design a mechanism that is truthful, that is, sellers are incentivized to report their true costs, budget-feasible, that is, the sum of the payments made to the sellers is at most the budget $B$, and that outputs a set whose value is large compared to $\text{OPT}:=\max\{v(S):\overline{c}(S)\le B,S\subseteq U\}$.
  Budget-feasible mechanism design has been extensively studied, with the literature focussing on (classes of) subadditive valuation functions, and various polytime, budget-feasible mechanisms, achieving constant-factor approximation, have been devised for the special cases of additive, submodular, and XOS valuations. However, for general subadditive valuations, the best-known approximation factor achievable by a polytime budget-feasible mechanism (given access to demand oracles) was only $O(\log n / \log \log n)$, where $n$ is the number of items.
  We improve this state-of-the-art significantly by designing a budget-feasible mechanism for subadditive valuations that \emph{achieves a substantially-improved approximation factor of $O(\log\log n)$ and runs in polynomial time, given access to demand oracles.}</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04665v1</guid>
      <category>cs.GT</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rian Neogi, Kanstantsin Pashkovich, Chaitanya Swamy</dc:creator>
    </item>
    <item>
      <title>Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors</title>
      <link>https://arxiv.org/abs/2506.05479</link>
      <description>arXiv:2506.05479v1 Announce Type: cross 
Abstract: We consider the following problem: We are given $\ell$ heuristics for Metrical Task Systems (MTS), where each might be tailored to a different type of input instances. While processing an input instance received online, we are allowed to query the action of only one of the heuristics at each time step. Our goal is to achieve performance comparable to the best of the given heuristics. The main difficulty of our setting comes from the fact that the cost paid by a heuristic at time $t$ cannot be estimated unless the same heuristic was also queried at time $t-1$. This is related to Bandit Learning against memory bounded adversaries (Arora et al., 2012). We show how to achieve regret of $O(\text{OPT}^{2/3})$ and prove a tight lower bound based on the construction of Dekel et al. (2013).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05479v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matei Gabriel Co\c{s}a, Marek Eli\'a\v{s}</dc:creator>
    </item>
    <item>
      <title>Smallest Suffixient Sets as a Repetitiveness Measure</title>
      <link>https://arxiv.org/abs/2506.05638</link>
      <description>arXiv:2506.05638v1 Announce Type: cross 
Abstract: Suffixient sets are a novel combinatorial object that capture the essential information of repetitive strings in a way that, provided with a random-access mechanism, supports various forms of pattern matching. In this paper we study the size $\chi$ of the smallest suffixient set as a repetitiveness measure: we place it between known measures and study its sensitivity to various string operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05638v1</guid>
      <category>cs.FL</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gonzalo Navarro, Giuseppe Romana, Cristian Urbina</dc:creator>
    </item>
    <item>
      <title>Perfect Matching with Few Link Activations</title>
      <link>https://arxiv.org/abs/2506.06102</link>
      <description>arXiv:2506.06102v1 Announce Type: cross 
Abstract: We consider the problem of computing a perfect matching problem in a synchronous distributed network, where the network topology corresponds to a complete bipartite graph. The communication between nodes is restricted to activating communication links, which means that instead of sending messages containing a number of bits, each node can only send a pulse over some of its incident links in each round. In the port numbering model, where nodes are unaware of their neighbor's IDs, we give a randomized algorithm that terminates in $O( \log n )$ rounds and has a pulse complexity of $O( n\log n )$, which corresponds to the number of pulses sent over all links. We also show that randomness is crucial in the port numbering model, as any deterministic algorithm must send at least $\Omega( n^2 )$ messages in the standard LOCAL model, where the messages can be of unbounded size. Then, we turn our attention to the KT_1 assumption, where each node starts out knowing its neighbors' IDs. We show that this additional knowledge enables significantly improved bounds even for deterministic algorithms. First, we give an $O( \log n )$ time deterministic algorithm that sends only $O( n )$ pulses. Finally, we apply this algorithm recursively to obtain an exponential reduction in the time complexity to $O( \log^*n\log\log n )$, while slightly increasing the pulse complexity to $O( n\log^*n )$. All our bounds also hold in the standard CONGEST model with single-bit messages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06102v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Mirault, Peter Robinson, Ming Ming Tan, Xianbin Zhu</dc:creator>
    </item>
    <item>
      <title>An extension of Dembo-Hammer's reduction algorithm for the 0-1 knapsack problem</title>
      <link>https://arxiv.org/abs/2506.06138</link>
      <description>arXiv:2506.06138v1 Announce Type: cross 
Abstract: Dembo-Hammer's Reduction Algorithm (DHR) is one of the classical algorithms for the 0-1 Knapsack Problem (0-1 KP) and its variants, which reduces an instance of the 0-1 KP to a sub-instance of smaller size with reduction time complexity $O(n)$. We present an extension of DHR (abbreviated as EDHR), which reduces an instance of 0-1 KP to at most $n^i$ sub-instances for any positive integer $i$. In practice, $i$ can be set as needed. In particular, if we choose $i=1$ then EDHR is exactly DHR. Finally, computational experiments on randomly generated data instances demonstrate that EDHR substantially reduces the search tree size compared to CPLEX.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06138v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yang Yang</dc:creator>
    </item>
    <item>
      <title>Longer Lists Yield Better Matchings</title>
      <link>https://arxiv.org/abs/2506.06217</link>
      <description>arXiv:2506.06217v1 Announce Type: cross 
Abstract: Many centralized mechanisms for two-sided matching markets that enjoy strong theoretical properties assume that the planner solicits full information on the preferences of each participating agent. In particular, they expect that participants compile and communicate their complete preference lists over agents from the other side of the market. However, real-world markets are often very large and agents cannot always be expected to even produce a ranking of all options on the other side. It is therefore important to understand the impact of incomplete or truncated lists on the quality of the resultant matching.
  In this paper, we focus on the Serial Dictatorship mechanism in a model where each agent of the proposing side (students) has a random preference list of length $d$, sampled independently and uniformly at random from $n$ schools, each of which has one seat. Our main result shows that if the students primarily care about being matched to any school of their list (as opposed to ending up unmatched), then all students in position $i\leq n$ will prefer markets with longer lists, when $n$ is large enough. Schools on the other hand will always prefer longer lists in our model. We moreover investigate the impact of $d$ on the rank of the school that a student gets matched to.
  Our main result suggests that markets that are well-approximated by our hypothesis and where the demand of schools does not exceed supply should be designed with preference lists as long as reasonable, since longer lists would favor all agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06217v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuri Faenza, Aapeli Vuorinen</dc:creator>
    </item>
    <item>
      <title>Approximation of Spanning Tree Congestion using Hereditary Bisection</title>
      <link>https://arxiv.org/abs/2410.00568</link>
      <description>arXiv:2410.00568v3 Announce Type: replace 
Abstract: The Spanning Tree Congestion (STC) problem is the following NP-hard problem: given a graph $G$, construct a spanning tree $T$ of $G$ minimizing its maximum edge congestion where the congestion of an edge $e\in T$ is the number of edges $uv$ in $G$ such that the unique path between $u$ and $v$ in $T$ passes through $e$; the optimal value for a given graph $G$ is denoted $STC(G)$.
  It is known that every spanning tree is an $n/2$-approximation for the STP problem. A long-standing problem is to design a better approximation algorithm. Our contribution towards this goal is an $O(\Delta\cdot\log^{3/2}n)$-approximation algorithm where $\Delta$ is the maximum degree in $G$ and $n$ the number of vertices. For graphs with a maximum degree bounded by a polylog of the number of vertices, this is an exponential improvement over the previous best approximation.
  Our main tool for the algorithm is a new lower bound on the spanning tree congestion which is of independent interest. Denoting by $hb(G)$ the hereditary bisection of $G$ which is the maximum bisection width over all subgraphs of $G$, we prove that for every graph $G$, $STC(G)\geq \Omega(hb(G)/\Delta)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00568v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petr Kolman</dc:creator>
    </item>
    <item>
      <title>KeBaB: $k$-mer based breaking for finding long MEMs</title>
      <link>https://arxiv.org/abs/2502.20338</link>
      <description>arXiv:2502.20338v2 Announce Type: replace 
Abstract: Long maximal exact matches (MEMs) are used in many genomics applications such as read classification and sequence alignment. Li's ropebwt3 finds long MEMs quickly because it can often ignore much of its input. In this paper we show that a fast and space efficient $k$-mer filtration step using a Bloom filter speeds up MEM-finders such as ropebwt3 even further by letting them ignore even more. We also show experimentally that our approach can accelerate metagenomic classification without significantly hurting accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20338v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathaniel K. Brown, Anas Alhadi, Nour Allam, Dove Begleiter, Nithin Bharathi Kabilan Karpagavalli, Suchith Sridhar Khajjayam, Hamza Wahed, Travis Gagie</dc:creator>
    </item>
    <item>
      <title>A Dynamic Working Set Method for Compressed Sensing</title>
      <link>https://arxiv.org/abs/2505.09370</link>
      <description>arXiv:2505.09370v2 Announce Type: replace 
Abstract: We propose a dynamic working set method (DWS) for the problem $\min_{\mathtt{x} \in \mathbb{R}^n} \frac{1}{2}\|\mathtt{Ax}-\mathtt{b}\|^2 + \eta\|\mathtt{x}\|_1$ that arises from compressed sensing. DWS manages the working set while iteratively calling a regression solver to generate progressively better solutions. Our experiments show that DWS is more efficient than other state-of-the-art software in the context of compressed sensing. Scale space such that $\|b\|=1$. Let $s$ be the number of non-zeros in the unknown signal. We prove that for any given $\varepsilon &gt; 0$, DWS reaches a solution with an additive error $\varepsilon/\eta^2$ such that each call of the solver uses only $O(\frac{1}{\varepsilon}s\log s \log\frac{1}{\varepsilon})$ variables, and each intermediate solution has $O(\frac{1}{\varepsilon}s\log s\log\frac{1}{\varepsilon})$ non-zero coordinates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09370v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siu-Wing Cheng, Man Ting Wong</dc:creator>
    </item>
    <item>
      <title>GPU-Parallelizable Randomized Sketch-and-Precondition for Linear Regression using Sparse Sign Sketches</title>
      <link>https://arxiv.org/abs/2506.03070</link>
      <description>arXiv:2506.03070v2 Announce Type: replace 
Abstract: A litany of theoretical and numerical results have established the sketch-and-precondition paradigm as a powerful approach to solving large linear regression problems in standard computing environments. Perhaps surprisingly, much less work has been done on understanding how sketch-and-precondition performs on graphics processing unit (GPU) systems. We address this gap by benchmarking an implementation of sketch-and-precondition based on sparse sign-sketches on single and multi-GPU systems. In doing so, we describe a novel, easily parallelized, rejection-sampling based method for generating sparse sign sketches. Our approach, which is particularly well-suited for GPUs, is easily adapted to a variety of computing environments. Taken as a whole, our numerical experiments indicate that sketch-and-precondition with sparse sign sketches is particularly well-suited for GPUs, and may be suitable for use in black-box least-squares solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03070v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tyler Chen, Pradeep Niroula, Archan Ray, Pragna Subrahmanya, Marco Pistoia, Niraj Kumar</dc:creator>
    </item>
    <item>
      <title>Labelling Data with Unknown References</title>
      <link>https://arxiv.org/abs/2506.03083</link>
      <description>arXiv:2506.03083v2 Announce Type: replace 
Abstract: An evaluator is trustworthy when there exists some agreed-upon way to measure its performance as a labeller. The two ways to establish trustworthiness are either by testing it, or by assuming the evaluator `knows' somehow the way to label the corpus. However, if labelled references (e.g., a development set) are unavailable, neither of these approaches work: the former requires the data, and the latter is an assumption, not evidence. To address this, we introduce an algorithm (the `No-Data Algorithm') by which to establish trust in an evaluator without any existing references. Our algorithm works by successively posing challenges to said evaluator. We show that this is sufficient to establish trustworthiness w.h.p., in such a way that when the evaluator actually knows the way to label the corpus, the No-Data Algorithm accepts its output; and, conversely, flags untrustworthy evaluators when these are unable to prove it. We present formal proofs of correctness and limited experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03083v2</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adrian de Wynter</dc:creator>
    </item>
    <item>
      <title>Prefix-free parsing for merging big BWTs</title>
      <link>https://arxiv.org/abs/2506.03294</link>
      <description>arXiv:2506.03294v2 Announce Type: replace 
Abstract: When building Burrows-Wheeler Transforms (BWTs) of truly huge datasets, prefix-free parsing (PFP) can use an unreasonable amount of memory. In this paper we show how if a dataset can be broken down into small datasets that are not very similar to each other -- such as collections of many copies of genomes of each of several species, or collections of many copies of each of the human chromosomes -- then we can drastically reduce PFP's memory footprint by building the BWTs of the small datasets and then merging them into the BWT of the whole dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03294v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diego Diaz-Dominguez, Travis Gagie, Veronica Guerrini, Ben Langmead, Zsuzsanna Liptak, Giovanni Manzini, Francesco Masillo, Vikram Shivakumar</dc:creator>
    </item>
    <item>
      <title>Testing and learning structured quantum Hamiltonians</title>
      <link>https://arxiv.org/abs/2411.00082</link>
      <description>arXiv:2411.00082v2 Announce Type: replace-cross 
Abstract: We consider the problems of testing and learning an unknown $n$-qubit Hamiltonian $H$ from queries to its evolution operator $e^{-iHt}$ under the normalized Frobenius norm. We prove:
  1. Local Hamiltonians: We give a tolerant testing protocol to decide if $H$ is $\epsilon_1$-close to $k$-local or $\epsilon_2$-far from $k$-local, with $O(1/(\epsilon_2-\epsilon_1)^{4})$ queries, solving open questions posed in a recent work by Bluhm et al. For learning a $k$-local $H$ up to error $\epsilon$, we give a protocol with query complexity $\exp(O(k^2+k\log(1/\epsilon)))$ independent of $n$, by leveraging the non-commutative Bohnenblust-Hille inequality.
  2. Sparse Hamiltonians: We give a protocol to test if $H$ is $\epsilon_1$-close to being $s$-sparse (in the Pauli basis) or $\epsilon_2$-far from being $s$-sparse, with $O(s^{6}/(\epsilon_2^2-\epsilon_1^2)^{6})$ queries. For learning up to error $\epsilon$, we show that $O(s^{4}/\epsilon^{8})$ queries suffice.
  3. Learning without memory: The learning results stated above have no dependence on $n$, but require $n$-qubit quantum memory. We give subroutines that allow us to learn without memory; increasing the query complexity by a $(\log n)$-factor in the local case and an $n$-factor in the sparse case.
  4. Testing without memory: We give a new subroutine called Pauli hashing, which allows one to tolerantly test $s$-sparse Hamiltonians with $O(s^{14}/(\epsilon_2^2-\epsilon_1^2)^{18})$ queries. A key ingredient is showing that $s$-sparse Pauli channels can be tolerantly tested under the diamond norm with $O(s^2/(\epsilon_2-\epsilon_1)^6)$ queries.
  Along the way, we prove new structural theorems for local and sparse Hamiltonians. We complement our learning results with polynomially weaker lower bounds. Furthermore, our algorithms use short time evolutions and do not assume prior knowledge of the terms in the support of the Pauli spectrum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00082v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Srinivasan Arunachalam, Arkopal Dutt, Francisco Escudero Guti\'errez</dc:creator>
    </item>
    <item>
      <title>Faster Approx. Top-K: Harnessing the Full Power of Two Stages</title>
      <link>https://arxiv.org/abs/2506.04165</link>
      <description>arXiv:2506.04165v2 Announce Type: replace-cross 
Abstract: We consider the Top-$K$ selection problem, which aims to identify the largest-$K$ elements from an array. Top-$K$ selection arises in many machine learning algorithms and often becomes a bottleneck on accelerators, which are optimized for dense matrix multiplications. To address this problem, \citet{chern2022tpuknnknearestneighbor} proposed a fast two-stage \textit{approximate} Top-$K$ algorithm: (i) partition the input array and select the top-$1$ element from each partition, (ii) sort this \textit{smaller subset} and return the top $K$ elements. In this paper, we consider a generalized version of this algorithm, where the first stage selects top-$K'$ elements, for some $1 \leq K' \leq K$, from each partition. Our contributions are as follows: (i) we derive an expression for the expected recall of this generalized algorithm and show that choosing $K' &gt; 1$ with fewer partitions in the first stage reduces the input size to the second stage more effectively while maintaining the same expected recall as the original algorithm, (ii) we derive a bound on the expected recall for the original algorithm in \citet{chern2022tpuknnknearestneighbor} that is provably tighter by a factor of $2$ than the one in that paper, and (iii) we implement our algorithm on Cloud TPUv5e and achieve around an order of magnitude speedups over the original algorithm without sacrificing recall on real-world tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04165v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yashas Samaga, Varun Yerram, Spandana Raj Babbula, Prateek Jain, Praneeth Netrapalli</dc:creator>
    </item>
  </channel>
</rss>
