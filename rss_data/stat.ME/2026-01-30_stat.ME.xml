<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Jan 2026 05:02:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Experimental Design for Matching</title>
      <link>https://arxiv.org/abs/2601.21036</link>
      <description>arXiv:2601.21036v1 Announce Type: new 
Abstract: Matching mechanisms play a central role in operations management across diverse fields including education, healthcare, and online platforms. However, experimentally comparing a new matching algorithm against a status quo presents some fundamental challenges due to matching interference, where assigning a unit in one matching may preclude its assignment in the other. In this work, we take a design-based perspective to study the design of randomized experiments to compare two predetermined matching plans on a finite population, without imposing outcome or behavioral models. We introduce the notation of a disagreement set, which captures the difference between the two matching plans, and show that it admits a unique decomposition into disjoint alternating paths and cycles with useful structural properties. Based on these properties, we propose the Alternating Path Randomized Design, which sequentially randomizes along these paths and cycles to effectively manage interference. Within a minimax framework, we optimize the conditional randomization probability and show that, for long paths, the optimal choice converges to $\sqrt{2}-1$, minimizing worst-case variance. We establish the unbiasedness of the Horvitz-Thompson estimator and derive a finite-population Central Limit Theorem that accommodates complex and unstable path and cycle structures as the population grows. Furthermore, we extend the design to many-to-one matchings, where capacity constraints fundamentally alter the structure of the disagreement set. Using graph-theoretic tools, including finding augmenting paths and Euler-tour decomposition on an auxiliary unbalanced directed graph, we construct feasible alternating path and cycle decompositions that allow the design and inference results to carry over.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21036v1</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>econ.EM</category>
      <category>eess.SY</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chonghuan Wang</dc:creator>
    </item>
    <item>
      <title>Adaptive Dirichlet Process mixture model with unknown concentration parameter and variance: Scaling high dimensional clustering via collapsed variational inference</title>
      <link>https://arxiv.org/abs/2601.21106</link>
      <description>arXiv:2601.21106v1 Announce Type: new 
Abstract: We propose a novel method that performs adaptive clustering with DPMM using collapsed VI, while incorporating weakly-informative priors for DP concentration parameter alpha and base distribution G0. We illustrate the importance of G0 covariance structure and prior choice by considering different parameterisations of the data covariance matrix. On high-dimensional Gaussian simulations, our model demonstrates substantially faster convergence than a state-of-the-art MCMC splice sampler. We further evaluate performances on Negative Binomial simulations and conduct sensitivity analyses to assess robustness on realistic data conditions. Application to a publicly available leukemia transcriptomic data set comprising 72 samples and 2,194 gene expression successfully recovers every known sub-type, all while identifying additional gene expression-based sub-clusters with meaningful biological interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21106v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annesh Pal, Aguirre Mimoun, Rodolphe Thi\'ebaut, Boris P. Hejblum</dc:creator>
    </item>
    <item>
      <title>A Poisson Factor Mixture Model for the Analysis of Linguistic Competence in Italian University Students' Writing</title>
      <link>https://arxiv.org/abs/2601.21493</link>
      <description>arXiv:2601.21493v1 Announce Type: new 
Abstract: Public debate on the alleged decline of language skills among younger generations often focuses on university students, the most highly educated segment of the population. Rather than addressing the ill posed question of linguistic decline, this paper examines how formal written Italian is currently used by university students and whether systematic patterns of competence and heterogeneity can be identified. The analysis is based on data from the UniversITA project, which collected formal texts written by a large and nationally representative sample of Italian university students. Texts were annotated for linguistically motivated features covering orthography, lexicon, syntax, morphosyntax, coherence, register, and sentence structure, yielding low frequency multivariate count data. To analyse these data, we propose a novel model-based clustering approach based on a Poisson factor mixture model that accounts for dependence among linguistic features and unobserved population heterogeneity. The results identify two correlated dimensions of writing competence, interpretable as communicative competence and linguistic grammatical competence. When educational and socio demographic information is incorporated, distinct student profiles emerge that are associated with field of study and educational background. These findings provide quantitative evidence on contemporary writing and offer insights relevant for language education and higher education policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21493v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Silvia Dallari, Laura Anderlucci, Nicola Grandi, Angela Montanari</dc:creator>
    </item>
    <item>
      <title>A Matrix-Variate Log-Normal Model for Covariance Matrices</title>
      <link>https://arxiv.org/abs/2601.21532</link>
      <description>arXiv:2601.21532v1 Announce Type: new 
Abstract: We propose a modeling framework for time-varying covariance matrices based on the assumption that the logarithm of a realized covariance matrix follows a matrix-variate oNrmal distribution. By operating in the space of symmetric matrices, the approach guarantees positive definiteness without imposing parameter constraints beyond stationarity. The conditional mean of the logarithmic covariance matrix is specified through a BEKK-type structure that can be rewritten as a diagonal vector representation, yielding a parsimonious specification that mitigates the curse of dimensionality. Estimation is performed by maximum likelihood exploiting properties of matrix-variate Normal distributions and expressing the scale parameter matrix as a function of the location matrix. The covariance matrix is recovered via the matrix exponential. Since this transformation induces an upward bias, an approximate, time-specific bias correction based on a second-order Taylor expansion is proposed. The framework is flexible and applicable to a wide class of problems involving symmetric positive definite matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21532v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Otranto</dc:creator>
    </item>
    <item>
      <title>Independent Component Discovery in Temporal Count Data</title>
      <link>https://arxiv.org/abs/2601.21696</link>
      <description>arXiv:2601.21696v1 Announce Type: new 
Abstract: Advances in data collection are producing growing volumes of temporal count observations, making adapted modeling increasingly necessary. In this work, we introduce a generative framework for independent component analysis of temporal count data, combining regime-adaptive dynamics with Poisson log-normal emissions. The model identifies disentangled components with regime-dependent contributions, enabling representation learning and perturbations analysis. Notably, we establish the identifiability of the model, supporting principled interpretation. To learn the parameters, we propose an efficient amortized variational inference procedure. Experiments on simulated data evaluate recovery of the mixing function and latent sources across diverse settings, while an in vivo longitudinal gut microbiome study reveals microbial co-variation patterns and regime shifts consistent with clinical perturbations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21696v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Chaussard, Anna Bonnet, Sylvain Le Corff</dc:creator>
    </item>
    <item>
      <title>Neural Wasserstein Two-Sample Tests</title>
      <link>https://arxiv.org/abs/2601.21732</link>
      <description>arXiv:2601.21732v1 Announce Type: new 
Abstract: The two-sample homogeneity testing problem is fundamental in statistics and becomes particularly challenging in high dimensions, where classical tests can suffer substantial power loss. We develop a learning-assisted procedure based on the projection 1-Wasserstein distance, which we call the neural Wasserstein test. The method is motivated by the observation that there often exists a low-dimensional projection under which the two high-dimensional distributions differ. In practice, we learn the projection directions via manifold optimization and a witness function using deep neural networks. To adapt to unknown projection dimensions and sparsity levels, we aggregate a collection of candidate statistics through a max-type construction, avoiding explicit tuning while potentially improving power. We establish the validity and consistency of the proposed test and prove a Berry--Esseen type bound for the Gaussian approximation. In particular, under the null hypothesis, the aggregated statistic converges to the absolute maximum of a standard Gaussian vector, yielding an asymptotically pivotal (distribution-free) calibration that bypasses resampling. Simulation studies and a real-data example demonstrate the strong finite-sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21732v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Hu, Zhenhua Lin</dc:creator>
    </item>
    <item>
      <title>Synthesizing Epileptic Seizures: Gaussian Processes for EEG Generation</title>
      <link>https://arxiv.org/abs/2601.21752</link>
      <description>arXiv:2601.21752v1 Announce Type: new 
Abstract: Reliable seizure detection from electroencephalography (EEG) time series is a high-priority clinical goal, yet the acquisition cost and scarcity of labeled EEG data limit the performance of machine learning methods. This challenge is exacerbated by the long-range, high-dimensional, and non-stationary nature of epileptic EEG recordings, which makes realistic data generation particularly difficult. In this work, we revisit Gaussian processes as a principled and interpretable foundation for modeling EEG dynamics, and propose a novel hierarchical framework, \textit{GP-EEG}, for generating synthetic epileptic EEG recordings. At its core, our approach decomposes EEG signals into temporal segments modeled via Gaussian process regression, and integrates a domain-adaptation variational autoencoder. We validate the proposed method on two real-world, open-source epileptic EEG datasets. The synthetic EEG recordings generated by our model match real-world epileptic EEG both quantitatively and qualitatively, and can be used to augment training sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21752v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nina Moutonnet, Joshua Corneck, Felipe Tobar, Danilo Mandic</dc:creator>
    </item>
    <item>
      <title>A Spacing Estimator</title>
      <link>https://arxiv.org/abs/2601.22103</link>
      <description>arXiv:2601.22103v1 Announce Type: new 
Abstract: The distribution of the spacing, or the difference between consecutive order statistics, is known only for uniform and exponential random variates. We add here logistic and Gumbel variates, and present an estimator for distributions with a known inverse cumulative density function. We show the estimator is accurate to the limit of numerical simulations for points near the middle of the order statistics, but degrades by up to 20% in the tails.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22103v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Greg Kreider</dc:creator>
    </item>
    <item>
      <title>Information-geometry-driven graph sequential growth</title>
      <link>https://arxiv.org/abs/2601.22106</link>
      <description>arXiv:2601.22106v1 Announce Type: new 
Abstract: We investigate the properties of a class of regularisation-free approaches for Gaussian graphical inference based on the information-geometry-driven sequential growth of initially edgeless graphs. Relating the growth of a graph to a coordinate descent process, we characterise the fully-corrective descents corresponding to information-optimal growths, and propose numerically efficient strategies for their approximation. We demonstrate the ability of the proposed procedures to reliably extract sparse graphical models while limiting the number of false detections, and illustrate how activation ranks can provide insight into the informational relevance of edge sets. The considered approaches are tuning-parameter-free and have complexities akin to coordinate descents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22106v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harry T. Bond, Bertrand Gauthier, Kirstin Strokorb</dc:creator>
    </item>
    <item>
      <title>Interval Spacing</title>
      <link>https://arxiv.org/abs/2601.22116</link>
      <description>arXiv:2601.22116v1 Announce Type: new 
Abstract: We define interval spacing as the difference in the order statistics of data over a gap of some width. We derive its density, expected value, and variance for uniform, exponential, and logistic variates. We show that interval spacing is equivalent to running a rectangular low-pass filter over the spacing, which simplifies the expressions for the expected values and introduces correlations between overlapping intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22116v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Greg Kreider</dc:creator>
    </item>
    <item>
      <title>Variance component score test for multivariate change point detection with applications to mobile health</title>
      <link>https://arxiv.org/abs/2601.22147</link>
      <description>arXiv:2601.22147v1 Announce Type: new 
Abstract: Multivariate change point detection is the process of identifying distributional shifts in time-ordered data across multiple features. This task is particularly challenging when the number of features is large relative to the number of observations. This problem is often present in mobile health, where behavioral changes in at-risk patients must be detected in real time in order to prompt timely interventions. We propose a variance component score test (VC*) for detecting changes in feature means and/or variances using only pre-change point data to estimate distributional parameters. Through simulation studies, we show that VC* has higher power than existing methods. Moreover, we demonstrate that reducing bias by using only pre-change point days to estimate parameters outweighs the increased estimator variances in most scenarios. Lastly, we apply VC* and competing methods to passively collected smartphone data in adolescents and young adults with affective instability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22147v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melissa Lynne Martin, Juliette Brook, Sage Rush, Theodore D. Satterthwaite, Ian J. Barnett</dc:creator>
    </item>
    <item>
      <title>Distributed Causality in the SDG Network: Evidence from Panel VAR and Conditional Independence Analysis</title>
      <link>https://arxiv.org/abs/2601.20875</link>
      <description>arXiv:2601.20875v1 Announce Type: cross 
Abstract: The achievement of the 2030 Sustainable Development Goals (SDGs) is dependent upon strategic resource distribution. We propose a causal discovery framework using Panel Vector Autoregression, along with both country-specific fixed effects and PCMCI+ conditional independence testing on 168 countries (2000-2025) to develop the first complete causal architecture of SDG dependencies. Utilizing 8 strategically chosen SDGs, we identify a distributed causal network (i.e., no single 'hub' SDG), with 10 statistically significant Granger-causal relationships identified as 11 unique direct effects. Education to Inequality is identified as the most statistically significant direct relationship (r = -0.599; p &lt; 0.05), while effect magnitude significantly varies depending on income levels (e.g., high-income: r = -0.65; lower-middle-income: r = -0.06; non-significant). We also reject the idea that there exists a single 'keystone' SDG. Additionally, we offer a proposed tiered priority framework for the SDGs namely, identifying upstream drivers (Education, Growth), enabling goals (Institutions, Energy), and downstream outcomes (Poverty, Health). Therefore, we conclude that effective SDG acceleration can be accomplished through coordinated multi-dimensional intervention(s), and that single-goal sequential strategies are insufficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20875v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Muhtasim Munif Fahim, Md Jahid Hasan Imran, Luknath Debnath, Tonmoy Shill, Md. Naim Molla, Ehsanul Bashar Pranto, Md Shafin Sanyan Saad, Md Rezaul Karim</dc:creator>
    </item>
    <item>
      <title>A Flexible Empirical Bayes Approach to Generalized Linear Models, with Applications to Sparse Logistic Regression</title>
      <link>https://arxiv.org/abs/2601.21217</link>
      <description>arXiv:2601.21217v1 Announce Type: cross 
Abstract: We introduce a flexible empirical Bayes approach for fitting Bayesian generalized linear models. Specifically, we adopt a novel mean-field variational inference (VI) method and the prior is estimated within the VI algorithm, making the method tuning-free. Unlike traditional VI methods that optimize the posterior density function, our approach directly optimizes the posterior mean and prior parameters. This formulation reduces the number of parameters to optimize and enables the use of scalable algorithms such as L-BFGS and stochastic gradient descent. Furthermore, our method automatically determines the optimal posterior based on the prior and likelihood, distinguishing it from existing VI methods that often assume a Gaussian variational. Our approach represents a unified framework applicable to a wide range of exponential family distributions, removing the need to develop unique VI methods for each combination of likelihood and prior distributions. We apply the framework to solve sparse logistic regression and demonstrate the superior predictive performance of our method in extensive numerical studies, by comparing it to prevalent sparse logistic regression approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21217v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dongyue Xie, Wanrong Zhu, Matthew Stephens</dc:creator>
    </item>
    <item>
      <title>Mean-field Variational Bayes for Sparse Probit Regression</title>
      <link>https://arxiv.org/abs/2601.21765</link>
      <description>arXiv:2601.21765v1 Announce Type: cross 
Abstract: We consider Bayesian variable selection for binary outcomes under a probit link with a spike-and-slab prior on the regression coefficients. Motivated by the computational challenges encountered by Markov chain Monte Carlo (MCMC) samplers in high-dimensional regimes, we develop a mean-field variational Bayes approximation in which all variational factors admit closed-form updates, and the evidence lower bound is available in closed form. This, in turn, allows the development of an efficient coordinate ascent variational inference algorithm to find the optimal values of the variational parameters. The approach produces posterior inclusion probabilities and parameter estimates, enabling interpretable selection and prediction within a single framework. As shown in both simulated and real data applications, the proposed method successfully identifies the important variables and is orders of magnitude faster than MCMC, while maintaining comparable accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21765v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Augusto Fasano, Giovanni Rebaudo</dc:creator>
    </item>
    <item>
      <title>Reddy: An open-source toolbox for analyzing eddy-covariance measurements in heterogeneous environments</title>
      <link>https://arxiv.org/abs/2601.21890</link>
      <description>arXiv:2601.21890v1 Announce Type: cross 
Abstract: Land-atmosphere exchange processes are determined by turbulent fluxes, which can be derived from eddy-covariance measurements. This method was established to quantify ecosystem-scale vertical atmosphere-vegetation exchange processes, but is also used to validate atmospheric turbulence theories with the ultimate aim to improve the representation of turbulence in numerical models. While the focus has long been on turbulence over idealized, homogeneous and flat surfaces, recent scientific developments are shifting towards investigating turbulent exchange processes in complex heterogeneous environments under non-idealized conditions, which pose particular challenges, e.g. advective fluxes between different surface types or non-stationarity of nighttime turbulence. This requires to rethink standard post-processing routines for determining turbulent fluxes from the high-frequency sonic and gas analyzer measurements. Here, we introduce the open-source R-package 'Reddy', which provides modular-built functions for post-processing, analysis and visualization of eddy-covariance measurements, including investigating spectra, coherent structures, anisotropy, flux footprints and surface energy balance closure. The 'Reddy' package is accompanied by a detailed documentation and a set of jupyter notebooks introducing new users hands-on to eddy-covariance data analysis. We showcase 'Reddy' based on measurements from three different sites in Norway: A case study during strong stratification over alpine tundra, for determining suitable averaging times during ice-cover transition at a boreal lake, and for fitting flux-variance relations for a permafrost peatland. 'Reddy' serves as extension of previously developed software packages, paving the way towards holistic turbulence data analysis in heterogeneous real-world environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21890v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Mack, Norbert Pirk</dc:creator>
    </item>
    <item>
      <title>Social Media Data for Population Mapping: A Bayesian Approach to Address Representativeness and Privacy Challenges</title>
      <link>https://arxiv.org/abs/2601.22104</link>
      <description>arXiv:2601.22104v1 Announce Type: cross 
Abstract: Accurate and timely population data are essential for disaster response and humanitarian planning, but traditional censuses often cannot capture rapid demographic changes. Social media data offer a promising alternative for dynamic population monitoring, but their representativeness remains poorly understood and stringent privacy requirements limit their reliability. Here, we address these limitations in the context of the Philippines by calibrating Facebook user counts with the country's 2020 census figures. First, we find that differential privacy techniques commonly applied to social media-based population datasets disproportionately mask low-population areas. To address this, we propose a Bayesian imputation approach to recover missing values, restoring data coverage for $5.5\%$ of rural areas. Further, using the imputed social media data and leveraging predictors such as urbanisation level, demographic composition, and socio-economic status, we develop a statistical model for the proportion of Facebook users in each municipality, which links observed Facebook user numbers to the true population levels. Out-of-sample validation demonstrates strong result generalisability, with errors as low as ${\approx}18\%$ and ${\approx}24\%$ for urban and rural Facebook user proportions, respectively. We further demonstrate that accounting for overdispersion and spatial correlations in the data is crucial to obtain accurate estimates and appropriate credible intervals. Crucially, as predictors change over time, the models can be used to regularly update the population predictions, providing a dynamic complement to census-based estimates. These results have direct implications for humanitarian response in disaster-prone regions and offer a general framework for using biased social media signals to generate reliable and timely population data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22104v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paolo Andrich, Shengjie Lai, Halim Jun, Qianwen Duan, Zhifeng Cheng, Seth R. Flaxman, Andrew J. Tatem</dc:creator>
    </item>
    <item>
      <title>Expected information gain estimation via density approximations: Sample allocation and dimension reduction</title>
      <link>https://arxiv.org/abs/2411.08390</link>
      <description>arXiv:2411.08390v3 Announce Type: replace 
Abstract: Computing expected information gain (EIG) from prior to posterior (equivalently, mutual information between candidate observations and model parameters or other quantities of interest) is a fundamental challenge in Bayesian optimal experimental design. We formulate flexible transport-based schemes for EIG estimation in general nonlinear/non-Gaussian settings, compatible with both standard and implicit Bayesian models. These schemes are representative of two-stage methods for estimating or bounding EIG using marginal and conditional density estimates. In this setting, we analyze the optimal allocation of samples between training (density estimation) and approximation of the outer prior expectation. We show that with this optimal sample allocation, the mean squared error (MSE) of the resulting EIG estimator converges more quickly than that of a standard nested Monte Carlo scheme. We then address the estimation of EIG in high dimensions, by deriving gradient-based upper bounds on the mutual information lost by projecting the parameters and/or observations to lower-dimensional subspaces. Minimizing these upper bounds yields projectors and hence low-dimensional EIG approximations that outperform approximations obtained via other linear dimension reduction schemes. Numerical experiments on a PDE-constrained Bayesian inverse problem also illustrate a favorable trade-off between dimension truncation and the modeling of non-Gaussianity, when estimating EIG from finite samples in high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08390v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fengyi Li, Ricardo Baptista, Youssef Marzouk</dc:creator>
    </item>
    <item>
      <title>Powering RCTs for marginal effects with GLMs using prognostic score adjustment</title>
      <link>https://arxiv.org/abs/2503.22284</link>
      <description>arXiv:2503.22284v2 Announce Type: replace 
Abstract: In randomized clinical trials (RCTs), the accurate estimation of marginal treatment effects is crucial for determining the efficacy of interventions. Enhancing the statistical power of these analyses is a key objective for statisticians. The increasing availability of historical data from registries, prior trials, and health records presents an opportunity to improve trial efficiency. However, many methods for historical borrowing compromise strict type-I error rate control. Building on the work by Schuler et al. [2022] on prognostic score adjustment for linear models, this paper extends the methodology to the plug-in analysis proposed by Rosenblum et al. [2010] using generalized linear models (GLMs) to further enhance the efficiency of RCT analyses without introducing bias. Specifically, we train a prognostic model on historical control data and incorporate the resulting prognostic scores as covariates in the plug-in GLM analysis of the trial data. This approach leverages the predictive power of historical data to improve the precision of marginal treatment effect estimates. We demonstrate that this method achieves local semi-parametric efficiency under the assumption of an additive treatment effect on the link scale. We expand the GLM plug-in method to include negative binomial regression. Additionally, we provide a straightforward formula for conservatively estimating the asymptotic variance, facilitating power calculations that reflect these efficiency gains. Our simulation study supports the theory. Even without an additive treatment effect, we observe increased power or reduced standard error. While population shifts from historical to trial data may dilute benefits, they do not introduce bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22284v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emilie H{\o}jbjerre-Frandsen, Mark J. van der Laan, Alejandro Schuler</dc:creator>
    </item>
    <item>
      <title>Simultaneous global and local clustering in multiplex networks with covariate information</title>
      <link>https://arxiv.org/abs/2505.03441</link>
      <description>arXiv:2505.03441v2 Announce Type: replace 
Abstract: Understanding both global and layer-specific group structures is useful for uncovering complex patterns in networks with multiple interaction types. In this work, we introduce a new model, the hierarchical multiplex stochastic blockmodel (HMPSBM), that simultaneously detects communities within individual layers of a multiplex network while inferring a global node clustering across the layers. A stochastic blockmodel is assumed in each layer, with probabilities of layer-level group memberships determined by a node's global group assignment. Our model uses a Bayesian framework, employing a probit stick-breaking process to construct node-specific mixing proportions over a set of shared Griffiths-Engen-McCloseky (GEM) distributions. These proportions determine layer-level community assignment, allowing for an unknown and varying number of groups across layers, while incorporating nodal covariate information to inform the global clustering. We propose a scalable variational inference procedure with parallelisable updates for application to large networks. Extensive simulation studies demonstrate our model's ability to accurately recover both global and layer-level clusters in complicated settings, and applications to real data showcase the model's effectiveness in uncovering interesting latent network structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03441v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/comnet/cnaf056</arxiv:DOI>
      <arxiv:journal_reference>Journal of Complex Networks, Volume 14, Issue 1, February 2026, cnaf056</arxiv:journal_reference>
      <dc:creator>Joshua Corneck, Edward A. K. Cohen, James S. Martin, Lekha Patel, Kurtis W. Shuler, Francesco Sanna Passino</dc:creator>
    </item>
    <item>
      <title>Causal Meta-Analysis: Rethinking the Foundations of Evidence-Based Medicine</title>
      <link>https://arxiv.org/abs/2505.20168</link>
      <description>arXiv:2505.20168v2 Announce Type: replace 
Abstract: Meta-analysis, by synthesizing effect estimates from multiple studies conducted in diverse settings, stands at the top of the evidence hierarchy in clinical research. Yet, conventional approaches based on fixed- or random-effects models lack a causal framework, which may limit their interpretability and utility for public policy. Incorporating causal inference reframes meta-analysis as the estimation of well-defined causal effects on clearly specified populations, enabling a principled approach to handling study heterogeneity. We show that classical meta-analysis estimators have a clear causal interpretation when effects are measured as risk differences. However, this breaks down for nonlinear measures like the risk ratio and odds ratio. To address this, we introduce novel causal aggregation formulas that remain compatible with standard meta-analysis practices and do not require access to individual-level data. To evaluate real-world impact, we apply both classical and causal meta-analysis methods to 500 published meta-analyses. While the conclusions often align, notable discrepancies emerge, revealing cases where conventional methods may suggest a treatment is beneficial when, under a causal lens, it is in fact harmful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20168v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cl\'ement Berenfeld, Ahmed Boughdiri, B\'en\'edicte Colnet, Wouter A. C. van Amsterdam, Aur\'elien Bellet, R\'emi Khellaf, Erwan Scornet, Julie Josse</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to the survivor average causal effect in cluster-randomized crossover trials</title>
      <link>https://arxiv.org/abs/2505.21447</link>
      <description>arXiv:2505.21447v2 Announce Type: replace 
Abstract: In cluster-randomized crossover (CRXO) trials, groups of individuals are randomly assigned to two or more sequences of alternating treatments. Since clusters serve as their own control, the CRXO design is typically more statistically efficient than the usual parallel-arm design. CRXO trials are increasingly popular in many areas of health research where the number of available clusters is limited. Further, in trials among severely ill patients, researchers often want to assess the effect of treatments on secondary non-terminal outcomes, but frequently in these studies, there are patients who do not survive to have these measurements fully recorded. In this paper, we provide a causal inference framework and treatment effect estimation methods for addressing truncation by death in the setting of CRXO trials. We target the survivor average causal effect (SACE) estimand, a well-defined subgroup treatment effect obtained via principal stratification. We propose novel structural and standard modeling assumptions that enable estimating the SACE within a Bayesian paradigm. We evaluate the small-sample performance of our proposed Bayesian approach for estimation of the SACE in CRXO trial settings via simulation studies. We apply our methods to a previously conducted two-period cross-sectional CRXO study examining the impact of proton pump inhibitors compared to histamine-2 receptor blockers on length of hospitalization among adults requiring invasive mechanical ventilation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21447v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dane Isenberg, Michael O. Harhay, Andrew B. Forbes, Paul J. Young, Fan Li, Nandita Mitra</dc:creator>
    </item>
    <item>
      <title>A Set of Rules for Model Validation</title>
      <link>https://arxiv.org/abs/2511.20711</link>
      <description>arXiv:2511.20711v2 Announce Type: replace 
Abstract: The validation of a data-driven model is the process of assessing the model's ability to generalize to new, unseen data in the population of interest. This paper proposes a set of general rules for model validation. These rules are designed to help practitioners create reliable validation plans and report their results transparently. While no validation scheme is flawless, these rules can help practitioners ensure their strategy is sufficient for practical use, openly discuss any limitations of their validation strategy, and report clear, comparable performance metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20711v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/cem.70110</arxiv:DOI>
      <dc:creator>Jos\'e Camacho</dc:creator>
    </item>
    <item>
      <title>A General Mixture Loss Function to Optimize a Personalized Predictive Model</title>
      <link>https://arxiv.org/abs/2601.20788</link>
      <description>arXiv:2601.20788v2 Announce Type: replace 
Abstract: Advances in precision medicine increasingly drive methodological innovation in health research. A key development is the use of personalized prediction models (PPMs), which are fit using a similar subpopulation tailored to a specific index patient, and have been shown to outperform one-size-fits-all models, particularly in terms of model discrimination performance. We propose a generalized loss function that enables tuning of the subpopulation size used to fit a PPM. This loss function allows joint optimization of discrimination and calibration, allowing both the performance measures and their relative weights to be specified by the user. To reduce computational burden, we conducted extensive simulation studies to identify practical bounds for the grid of subpopulation sizes. Based on these results, we recommend using a lower bound of 20\% and an upper bound of 70\% of the entire training dataset. We apply the proposed method to both simulated and real-world datasets and demonstrate that previously observed relationships between subpopulation size and model performance are robust. Furthermore, we show that the choice of performance measures in the loss function influences the optimal subpopulation size selected. These findings support the flexible and computationally efficient implementation of PPMs in precision health research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20788v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatiana Krikella, Joel A. Dubin</dc:creator>
    </item>
    <item>
      <title>Towards Identifiable Latent Additive Noise Models</title>
      <link>https://arxiv.org/abs/2403.15711</link>
      <description>arXiv:2403.15711v2 Announce Type: replace-cross 
Abstract: Causal representation learning (CRL) offers the promise of uncovering the underlying causal model by which observed data was generated, but the practical applicability of existing methods remains limited by the strong assumptions required for identifiability and by challenges in applying them to real-world settings. Most current approaches are applicable only to relatively restrictive model classes, such as linear or polynomial models, which limits their flexibility and robustness in practice. One promising approach to this problem seeks to address these issues by leveraging changes in causal influences among latent variables. In this vein we propose a more general and relaxed framework than typically applied, formulated by imposing constraints on the function classes applied. Within this framework, we establish partial identifiability results under weaker conditions, including scenarios where only a subset of causal influences change. We then extend our analysis to a broader class of latent post-nonlinear models. Building on these theoretical insights, we develop a flexible method for learning latent causal representations. We demonstrate the effectiveness of our approach on synthetic and semi-synthetic datasets, and further showcase its applicability in a case study on human motion analysis, a complex real-world domain that also highlights the potential to broaden the practical reach of identifiable CRL models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15711v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhang Liu, Zhen Zhang, Dong Gong, Erdun Gao, Biwei Huang, Mingming Gong, Anton van den Hengel, Kun Zhang, Javen Qinfeng Shi</dc:creator>
    </item>
    <item>
      <title>Investigating symptom duration using current status data: a case study of post-acute COVID-19 syndrome</title>
      <link>https://arxiv.org/abs/2407.04214</link>
      <description>arXiv:2407.04214v4 Announce Type: replace-cross 
Abstract: For infectious diseases, characterizing symptom duration is of clinical and public health importance. Symptom duration may be assessed by surveying infected individuals and querying symptom status at the time of survey response. For example, in a SARS-CoV-2 testing program at the University of Washington, participants were surveyed at least $28$ days after testing positive and asked to report current symptom status. This study design yielded current status data: outcome measurements for each respondent consisted only of the time of survey response and a binary indicator of whether symptoms had resolved by that time. Such study design benefits from limited risk of recall bias, but analyzing the resulting data necessitates tailored statistical tools. Here, we review methods for current status data and describe a novel application of modern nonparametric techniques to this setting. The proposed approach is valid under weaker assumptions compared to existing methods, allows use of flexible machine learning tools, and handles potential survey nonresponse. From the university study, under an assumption that the survey response time is conditionally independent of symptom resolution time within strata of measured covariates, we estimate that 19% of participants experienced ongoing symptoms 30 days after testing positive, decreasing to 7% at 90 days. We assess the sensitivity of these results to deviations from conditional independence, finding the estimates to be more sensitive to assumption violations at 30 days compared to 90 days. Female sex, fatigue during acute infection, and higher viral load were associated with slower symptom resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04214v4</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1097/EDE.0000000000001882</arxiv:DOI>
      <arxiv:journal_reference>Epidemiology 36(5) (2025)</arxiv:journal_reference>
      <dc:creator>Charles J. Wolock, Susan Jacob, Julia C. Bennett, Anna Elias-Warren, Jessica O'Hanlon, Avi Kenny, Nicholas P. Jewell, Andrea Rotnitzky, Stephen R. Cole, Ana A. Weil, Helen Y. Chu, Marco Carone</dc:creator>
    </item>
    <item>
      <title>Metric Graph Kernels via the Tropical Torelli Map</title>
      <link>https://arxiv.org/abs/2505.12129</link>
      <description>arXiv:2505.12129v2 Announce Type: replace-cross 
Abstract: We introduce the first graph kernels for metric graphs via tropical algebraic geometry. In contrast to conventional graph kernels based on graph combinatorics such as nodes, edges, and subgraphs, our metric graph kernels are purely based on the geometry and topology of the underlying metric space. A key characterizing property of our construction is its invariance under edge subdivision, making the kernels intrinsically well-suited for comparing graphs representing different underlying metric spaces. We develop efficient algorithms to compute our kernels and analyze their complexity, which depends primarily on the genus of the input graphs rather than their size. Through experiments on synthetic data and selected real-world datasets, we demonstrate that our kernels capture complementary geometric and topological information overseen by standard combinatorial approaches, particularly in label-free settings. We further showcase their practical utility with an urban road network classification task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12129v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueqi Cao, Anthea Monod</dc:creator>
    </item>
    <item>
      <title>POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes</title>
      <link>https://arxiv.org/abs/2506.20406</link>
      <description>arXiv:2506.20406v2 Announce Type: replace-cross 
Abstract: Dynamic treatment regimes (DTRs) provide a principled framework for optimizing sequential decision-making in domains where decisions must adapt over time in response to individual trajectories, such as healthcare, education, and digital interventions. However, existing statistical methods often rely on strong positivity assumptions and lack robustness under partial data coverage, while offline reinforcement learning approaches typically focus on average training performance, lack statistical guarantees, and require solving complex optimization problems. To address these challenges, we propose POLAR, a novel pessimistic model-based policy learning algorithm for offline DTR optimization. POLAR estimates the transition dynamics from offline data and quantifies uncertainty for each history-action pair. A pessimistic penalty is then incorporated into the reward function to discourage actions with high uncertainty. Unlike many existing methods that focus on average training performance or provide guarantees only for an oracle policy, POLAR directly targets the suboptimality of the final learned policy and offers theoretical guarantees, without relying on computationally intensive minimax or constrained optimization procedures. To the best of our knowledge, POLAR is the first model-based DTR method to provide both statistical and computational guarantees, including finite-sample bounds on policy suboptimality. Empirical results on both synthetic data and the MIMIC-III dataset demonstrate that POLAR outperforms state-of-the-art methods and yields near-optimal, history-aware treatment strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20406v2</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruijia Zhang, Xiangyu Zhang, Zhengling Qi, Yue Wu, Yanxun Xu</dc:creator>
    </item>
    <item>
      <title>Vecchia-Inducing-Points Full-Scale Approximations for Gaussian Processes</title>
      <link>https://arxiv.org/abs/2507.05064</link>
      <description>arXiv:2507.05064v2 Announce Type: replace-cross 
Abstract: Gaussian processes are flexible, probabilistic, non-parametric models widely used in machine learning and statistics. However, their scalability to large data sets is limited by computational constraints. To overcome these challenges, we propose Vecchia-inducing-points full-scale (VIF) approximations combining the strengths of global inducing points and local Vecchia approximations. Vecchia approximations excel in settings with low-dimensional inputs and moderately smooth covariance functions, while inducing point methods are better suited to high-dimensional inputs and smoother covariance functions. Our VIF approach bridges these two regimes by using an efficient correlation-based neighbor-finding strategy for the Vecchia approximation of the residual process, implemented via a modified cover tree algorithm. We further extend our framework to non-Gaussian likelihoods by introducing iterative methods that substantially reduce computational costs for training and prediction by several orders of magnitudes compared to Cholesky-based computations when using a Laplace approximation. In particular, we propose and compare novel preconditioners and provide theoretical convergence results. Extensive numerical experiments on simulated and real-world data sets show that VIF approximations are both computationally efficient as well as more accurate and numerically stable than state-of-the-art alternatives. All methods are implemented in the open source C++ library GPBoost with high-level Python and R interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05064v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Gyger, Reinhard Furrer, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>Optimal differentially private kernel learning with random projection</title>
      <link>https://arxiv.org/abs/2507.17544</link>
      <description>arXiv:2507.17544v3 Announce Type: replace-cross 
Abstract: Differential privacy has become a cornerstone in the development of privacy-preserving learning algorithms. This work addresses optimizing differentially private kernel learning within the empirical risk minimization (ERM) framework. We propose a novel differentially private kernel ERM algorithm based on random projection in the reproducing kernel Hilbert space using Gaussian processes. Our method achieves minimax-optimal excess risk for both the squared loss and Lipschitz-smooth convex loss functions under a local strong convexity condition. We further show that existing approaches based on alternative dimension reduction techniques, such as random Fourier feature mappings or $\ell_2$ regularization, yield suboptimal generalization performance. Our key theoretical contribution also includes the derivation of dimension-free generalization bounds for objective perturbation-based private linear ERM -- marking the first such result that does not rely on noisy gradient-based mechanisms. Additionally, we obtain sharper generalization bounds for existing differentially private kernel ERM algorithms. Empirical evaluations support our theoretical claims, demonstrating that random projection enables statistically efficient and optimally private kernel learning. These findings provide new insights into the design of differentially private algorithms and highlight the central role of dimension reduction in balancing privacy and utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17544v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bonwoo Lee, Cheolwoo Park, Jeongyoun Ahn</dc:creator>
    </item>
    <item>
      <title>On the Adversarial Robustness of Learning-based Conformal Novelty Detection</title>
      <link>https://arxiv.org/abs/2510.00463</link>
      <description>arXiv:2510.00463v2 Announce Type: replace-cross 
Abstract: This paper studies the adversarial robustness of conformal novelty detection. In particular, we focus on two powerful learning-based frameworks that come with finite-sample false discovery rate (FDR) control: one is AdaDetect (by Marandon et al., 2024) that is based on the positive-unlabeled classifier, and the other is a one-class classifier-based approach (by Bates et al., 2023). While they provide rigorous statistical guarantees under benign conditions, their behavior under adversarial perturbations remains underexplored. We first formulate an oracle attack setup, under the AdaDetect formulation, that quantifies the worst-case degradation of FDR, deriving an upper bound that characterizes the statistical cost of attacks. This idealized formulation directly motivates a practical and effective attack scheme that only requires query access to the output labels of both frameworks. Coupling these formulations with two popular and complementary black-box adversarial algorithms, we systematically evaluate the vulnerability of both frameworks on synthetic and real-world datasets. Our results show that adversarial perturbations can significantly increase the FDR while maintaining high detection power, exposing fundamental limitations of current error-controlled novelty detection methods and motivating the development of more robust alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00463v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daofu Zhang, Mehrdad Pournaderi, Hanne M. Clifford, Yu Xiang, Pramod K. Varshney</dc:creator>
    </item>
    <item>
      <title>Online Price Competition under Generalized Linear Demands</title>
      <link>https://arxiv.org/abs/2511.10718</link>
      <description>arXiv:2511.10718v4 Announce Type: replace-cross 
Abstract: We study sequential price competition among $N$ sellers, each influenced by the pricing decisions of their rivals. Specifically, the demand function for each seller $i$ follows the single index model $\lambda_i(\mathbf{p}) = \mu_i(\langle \boldsymbol{\theta}_{i,0}, \mathbf{p} \rangle)$, with known increasing link $\mu_i$ and unknown parameter $\boldsymbol{\theta}_{i,0}$, where the vector $\mathbf{p}$ denotes the vector of prices offered by all the sellers simultaneously at a given instant. Each seller observes only their own realized demand -- unobservable to competitors -- and the prices set by rivals. Our framework generalizes existing approaches that focus solely on linear demand models. We propose a novel decentralized policy, PML-GLUCB, that combines penalized MLE with an upper-confidence pricing rule, removing the need for coordinated exploration phases across sellers -- which is integral to previous linear models -- and accommodating both binary and real-valued demand observations. Relative to a dynamic benchmark policy, each seller achieves $O(N^{2}\sqrt{T}\log(T))$ regret, which essentially matches the optimal rate known in the linear setting. A significant technical contribution of our work is the development of a variant of the elliptical potential lemma -- typically applied in single-agent systems -- adapted to our competitive multi-agent environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10718v4</guid>
      <category>cs.GT</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Bracale, Moulinath Banerjee, Cong Shi, Yuekai Sun</dc:creator>
    </item>
    <item>
      <title>Diffusion Models in Simulation-Based Inference: A Tutorial Review</title>
      <link>https://arxiv.org/abs/2512.20685</link>
      <description>arXiv:2512.20685v2 Announce Type: replace-cross 
Abstract: Diffusion models have recently emerged as powerful learners for simulation-based inference (SBI), enabling fast and accurate estimation of latent parameters from simulated and real data. Their score-based formulation offers a flexible way to learn conditional or joint distributions over parameters and observations, thereby providing a versatile solution to various modeling problems. In this tutorial review, we synthesize recent developments on diffusion models for SBI, covering design choices for training, inference, and evaluation. We highlight opportunities created by various concepts such as guidance, score composition, flow matching, consistency models, and joint modeling. Furthermore, we discuss how efficiency and statistical accuracy are affected by noise schedules, parameterizations, and samplers. Finally, we illustrate these concepts with case studies across parameter dimensionalities, simulation budgets, and model types, and outline open questions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20685v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Arruda, Niels Bracher, Ullrich K\"othe, Jan Hasenauer, Stefan T. Radev</dc:creator>
    </item>
  </channel>
</rss>
