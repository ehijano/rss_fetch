<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Dec 2024 03:56:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Evaluating time-specific treatment effects in matched-pairs studies</title>
      <link>https://arxiv.org/abs/2412.09697</link>
      <description>arXiv:2412.09697v1 Announce Type: new 
Abstract: This study develops methods for evaluating a treatment effect on a time-to-event outcome in matched-pair studies. While most methods for paired right-censored outcomes allow determining an overall treatment effect over the course of follow-up, they generally lack in providing detailed insights into how the effect changes over time. To address this gap, we propose time-specific and overall tests for paired right-censored outcomes under randomization inference. We further extend our tests to matched observational studies by developing corresponding sensitivity analysis methods to take into account departures from randomization. Simulations demonstrate the robustness of our approach against various non-proportional hazards alternatives, including a crossing survival curves scenario. We demonstrate the application of our methods using a matched observational study from the Korean Longitudinal Study of Aging (KLoSA) data, focusing on the effect of social engagement on survival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09697v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sangjin Lee, Kwonsang Lee</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Conformalized Survival Analysis with Right-Censored Data</title>
      <link>https://arxiv.org/abs/2412.09729</link>
      <description>arXiv:2412.09729v1 Announce Type: new 
Abstract: We present a conformal inference method for constructing lower prediction bounds for survival times from right-censored data, extending recent approaches designed for type-I censoring. This method imputes unobserved censoring times using a suitable model, and then analyzes the imputed data using weighted conformal inference. This approach is theoretically supported by an asymptotic double robustness property. Empirical studies on simulated and real data sets demonstrate that our method is more robust than existing approaches in challenging settings where the survival model may be inaccurate, while achieving comparable performance in easier scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09729v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Sesia, Vladimir Svetnik</dc:creator>
    </item>
    <item>
      <title>A class of nonparametric methods for evaluating the effect of continuous treatments on survival outcomes</title>
      <link>https://arxiv.org/abs/2412.09786</link>
      <description>arXiv:2412.09786v1 Announce Type: new 
Abstract: In randomized trials and observational studies, it is often necessary to evaluate the extent to which an intervention affects a time-to-event outcome, which is only partially observed due to right censoring. For instance, in infectious disease studies, it is frequently of interest to characterize the relationship between risk of acquisition of infection with a pathogen and a biomarker previously measuring for an immune response against that pathogen induced by prior infection and/or vaccination. It is common to conduct inference within a causal framework, wherein we desire to make inferences about the counterfactual probability of survival through a given time point, at any given exposure level. To determine whether a causal effect is present, one can assess if this quantity differs by exposure level. Recent work shows that, under typical causal assumptions, summaries of the counterfactual survival distribution are identifiable. Moreover, when the treatment is multi-level, these summaries are also pathwise differentiable in a nonparametric probability model, making it possible to construct estimators thereof that are unbiased and approximately normal. In cases where the treatment is continuous, the target estimand is no longer pathwise differentiable, rendering it difficult to construct well-behaved estimators without strong parametric assumptions. In this work, we extend beyond the traditional setting with multilevel interventions to develop approaches to nonparametric inference with a continuous exposure. We introduce methods for testing whether the counterfactual probability of survival time by a given time-point remains constant across the range of the continuous exposure levels. The performance of our proposed methods is evaluated via numerical studies, and we apply our method to data from a recent pair of efficacy trials of an HIV monoclonal antibody.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09786v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yutong Jin, Peter B. Gilbert, Aaron Hudson</dc:creator>
    </item>
    <item>
      <title>Flexible Bayesian Nonparametric Product Mixtures for Multi-scale Functional Clustering</title>
      <link>https://arxiv.org/abs/2412.09792</link>
      <description>arXiv:2412.09792v1 Announce Type: new 
Abstract: There is a rich literature on clustering functional data with applications to time-series modeling, trajectory data, and even spatio-temporal applications. However, existing methods routinely perform global clustering that enforces identical atom values within the same cluster. Such grouping may be inadequate for high-dimensional functions, where the clustering patterns may change between the more dominant high-level features and the finer resolution local features. While there is some limited literature on local clustering approaches to deal with the above problems, these methods are typically not scalable to high-dimensional functions, and their theoretical properties are not well-investigated. Focusing on basis expansions for high-dimensional functions, we propose a flexible non-parametric Bayesian approach for multi-resolution clustering. The proposed method imposes independent Dirichlet process (DP) priors on different subsets of basis coefficients that ultimately results in a product of DP mixture priors inducing local clustering. We generalize the approach to incorporate spatially correlated error terms when modeling random spatial functions to provide improved model fitting. An efficient Markov chain Monte Carlo (MCMC) algorithm is developed for implementation. We show posterior consistency properties under the local clustering approach that asymptotically recovers the true density of random functions. Extensive simulations illustrate the improved clustering and function estimation under the proposed method compared to classical approaches. We apply the proposed approach to a spatial transcriptomics application where the goal is to infer clusters of genes with distinct spatial patterns of expressions. Our method makes an important contribution by expanding the limited literature on local clustering methods for high-dimensional functions with theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09792v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tsung-Hung Yao, Suprateek Kundu</dc:creator>
    </item>
    <item>
      <title>Sequential Change Point Detection in High-dimensional Vector Auto-regressive Models</title>
      <link>https://arxiv.org/abs/2412.09794</link>
      <description>arXiv:2412.09794v1 Announce Type: new 
Abstract: Sequential (online) change-point detection involves continuously monitoring time-series data and triggering an alarm when shifts in the data distribution are detected. We propose an algorithm for real-time identification of alterations in the transition matrices of high-dimensional vector autoregressive models. The algorithm estimates transition matrices and error term variances using regularization techniques applied to training data, then computes a specific test statistic to detect changes in transition matrices as new data batches arrive. We establish the asymptotic normality of the test statistic under the scenario of no change points, subject to mild conditions. An alarm is raised when the calculated test statistic exceeds a predefined quantile of the standard normal distribution. We demonstrate that, as the size of the change (jump size) increases, the test power approaches one. The effectiveness of the algorithm is validated empirically across various simulation scenarios. Finally, we present two applications of the proposed methodology: analyzing shocks in S&amp;P 500 data and detecting the timing of seizures in EEG data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09794v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhan Tian, Abolfazl Safikhani</dc:creator>
    </item>
    <item>
      <title>Unified optimal model averaging with a general loss function based on cross-validation</title>
      <link>https://arxiv.org/abs/2412.09804</link>
      <description>arXiv:2412.09804v1 Announce Type: new 
Abstract: Studying unified model averaging estimation for situations with complicated data structures, we propose a novel model averaging method based on cross-validation (MACV). MACV unifies a large class of new and existing model averaging estimators and covers a very general class of loss functions. Furthermore, to reduce the computational burden caused by the conventional leave-subject/one-out cross validation, we propose a SEcond-order-Approximated Leave-one/subject-out (SEAL) cross validation, which largely improves the computation efficiency. In the context of non-independent and non-identically distributed random variables, we establish the unified theory for analyzing the asymptotic behaviors of the proposed MACV and SEAL methods, where the number of candidate models is allowed to diverge with sample size. To demonstrate the breadth of the proposed methodology, we exemplify four optimal model averaging estimators under four important situations, i.e., longitudinal data with discrete responses, within-cluster correlation structure modeling, conditional prediction in spatial data, and quantile regression with a potential correlation structure. We conduct extensive simulation studies and analyze real-data examples to illustrate the advantages of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09804v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dalei Yu, Xinyu Zhang, Hua Liang</dc:creator>
    </item>
    <item>
      <title>$L$-estimation of Claim Severity Models Weighted by Kumaraswamy Density</title>
      <link>https://arxiv.org/abs/2412.09830</link>
      <description>arXiv:2412.09830v1 Announce Type: new 
Abstract: Statistical modeling of claim severity distributions is essential in insurance and risk management, where achieving a balance between robustness and efficiency in parameter estimation is critical against model contaminations. Two \( L \)-estimators, the method of trimmed moments (MTM) and the method of winsorized moments (MWM), are commonly used in the literature, but they are constrained by rigid weighting schemes that either discard or uniformly down-weight extreme observations, limiting their customized adaptability. This paper proposes a flexible robust \( L \)-estimation framework weighted by Kumaraswamy densities, offering smoothly varying observation-specific weights that preserve valuable information while improving robustness and efficiency. The framework is developed for parametric claim severity models, including Pareto, lognormal, and Fr{\'e}chet distributions, with theoretical justifications on asymptotic normality and variance-covariance structures. Through simulations and application to a U.S. indemnity loss dataset, the proposed method demonstrates superior performance over MTM, MWM, and MLE approaches, particularly in handling outliers and heavy-tailed distributions, making it a flexible and reliable alternative for loss severity modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09830v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chudamani Poudyal, Gokarna R. Aryal, Keshav Pokhrel</dc:creator>
    </item>
    <item>
      <title>Addressing Positivity Violations in Extending Inference to a Target Population</title>
      <link>https://arxiv.org/abs/2412.09845</link>
      <description>arXiv:2412.09845v1 Announce Type: new 
Abstract: Enhancing the external validity of trial results is essential for their applicability to real-world populations. However, violations of the positivity assumption can limit both the generalizability and transportability of findings. To address positivity violations in estimating the average treatment effect for a target population, we propose a framework that integrates characterizing the underrepresented group and performing sensitivity analysis for inference in the original target population. Our approach helps identify limitations in trial sampling and improves the robustness of trial findings for real-world populations. We apply this approach to extend findings from phase IV trials of treatments for opioid use disorder to a real-world population based on the 2021 Treatment Episode Data Set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09845v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Lu, Sanjib Basu</dc:creator>
    </item>
    <item>
      <title>Tail Risk Equivalent Level Transition and Its Application for Estimating Extreme $L_p$-quantiles</title>
      <link>https://arxiv.org/abs/2412.09872</link>
      <description>arXiv:2412.09872v1 Announce Type: new 
Abstract: $L_p$-quantile has recently been receiving growing attention in risk management since it has desirable properties as a risk measure and is a generalization of two widely applied risk measures, Value-at-Risk and Expectile. The statistical methodology for $L_p$-quantile is not only feasible but also straightforward to implement as it represents a specific form of M-quantile using $p$-power loss function. In this paper, we introduce the concept of Tail Risk Equivalent Level Transition (TRELT) to capture changes in tail risk when we make a risk transition between two $L_p$-quantiles. TRELT is motivated by PELVE in Li and Wang (2023) but for tail risk. As it remains unknown in theory how this transition works, we investigate the existence, uniqueness, and asymptotic properties of TRELT (as well as dual TRELT) for $L_p$-quantiles. In addition, we study the inference methods for TRELT and extreme $L_p$-quantiles by using this risk transition, which turns out to be a novel extrapolation method in extreme value theory. The asymptotic properties of the proposed estimators are established, and both simulation studies and real data analysis are conducted to demonstrate their empirical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09872v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qingzhao Zhong, Yanxi Hou</dc:creator>
    </item>
    <item>
      <title>Are you doing better than random guessing? A call for using negative controls when evaluating causal discovery algorithms</title>
      <link>https://arxiv.org/abs/2412.10039</link>
      <description>arXiv:2412.10039v1 Announce Type: new 
Abstract: New proposals for causal discovery algorithms are typically evaluated using simulations and a few select real data examples with known data generating mechanisms. However, there does not exist a general guideline for how such evaluation studies should be designed, and therefore, comparing results across different studies can be difficult. In this article, we propose a common evaluation baseline by posing the question: Are we doing better than random guessing? For the task of graph skeleton estimation, we derive exact distributional results under random guessing for the expected behavior of a range of typical causal discovery evaluation metrics (including precision and recall). We show that these metrics can achieve very large values under random guessing in certain scenarios, and hence warn against using them without also reporting negative control results, i.e., performance under random guessing. We also propose an exact test of overall skeleton fit, and showcase its use on a real data application. Finally, we propose a general pipeline for using random controls beyond the skeleton estimation task, and apply it both in a simulated example and a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10039v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anne Helby Petersen</dc:creator>
    </item>
    <item>
      <title>High-dimensional Statistics Applications to Batch Effects in Metabolomics</title>
      <link>https://arxiv.org/abs/2412.10196</link>
      <description>arXiv:2412.10196v1 Announce Type: new 
Abstract: Batch effects are inevitable in large-scale metabolomics. Prior to formal data analysis, batch effect correction (BEC) is applied to prevent from obscuring biological variations, and batch effect evaluation (BEE) is used for correction assessment. However, existing BEE algorithms neglect covariances between the variables, and existing BEC algorithms might fail to adequately correct the covariances. Therefore, we resort to recent advancements in high-dimensional statistics, and respectively propose "quality control-based simultaneous tests (QC-ST)" and "covariance correction (CoCo)". Validated by the simulation data, QC-ST can simultaneously detect the statistical significance of QC samples' mean vectors and covariance matrices across different batches, and has a satisfactory statistical performance in empirical sizes, empirical powers, and computational speed. Then, we apply four QC-based BEC algorithms to two large cohort datasets, and find that extreme gradient boost (XGBoost) performs best in relative standard deviation (RSD) and dispersion-ratio (D-ratio). After prepositive BEC, if QC-ST still suggests that batch effects between some two batches are significant, CoCo should be implemented. And after CoCo (if necessary), the four metrics (i.e., RSD, D-ratio, classification performance, and QC-ST) might be further improved. In summary, under the guidance of QC-ST, we can develop a matching strategy to integrate multiple BEC algorithms more rationally and flexibly, and minimize batch effects for reliable biological conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10196v1</guid>
      <category>stat.ME</category>
      <category>q-bio.BM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhendong Guo</dc:creator>
    </item>
    <item>
      <title>Collaborative Design of Controlled Experiments in the Presence of Subject Covariates</title>
      <link>https://arxiv.org/abs/2412.10213</link>
      <description>arXiv:2412.10213v1 Announce Type: new 
Abstract: We consider the optimal experimental design problem of allocating subjects to treatment or control when subjects participate in multiple, separate controlled experiments within a short time-frame and subject covariate information is available. Here, in addition to subject covariates, we consider the dependence among the responses coming from the subject's random effect across experiments. In this setting, the goal of the allocation is to provide precise estimates of treatment effects for each experiment. Deriving the precision matrix of the treatment effects and using D-optimality as our allocation criterion, we demonstrate the advantage of collaboratively designing and analyzing multiple experiments over traditional independent design and analysis, and propose two randomized algorithms to provide solutions to the D-optimality problem for collaborative design. The first algorithm decomposes the D-optimality problem into a sequence of subproblems, where each subproblem is a quadratic binary program that can be solved through a semi-definite relaxation based randomized algorithm with performance guarantees. The second algorithm involves solving a single semi-definite program, and randomly generating allocations for each experiment from the solution of this program. We showcase the performance of these algorithms through a simulation study, finding that our algorithms outperform covariate-agnostic methods when there are a large number of covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10213v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Fisher, Qiong Zhang, Lulu Kang, Xinwei Deng</dc:creator>
    </item>
    <item>
      <title>Regression trees for nonparametric diagnostics of sequential positivity violations in longitudinal causal inference</title>
      <link>https://arxiv.org/abs/2412.10245</link>
      <description>arXiv:2412.10245v1 Announce Type: new 
Abstract: Sequential positivity is often a necessary assumption for drawing causal inferences, such as through marginal structural modeling. Unfortunately, verification of this assumption can be challenging because it usually relies on multiple parametric propensity score models, unlikely all correctly specified. Therefore, we propose a new algorithm, called "sequential Positivity Regression Tree" (sPoRT), to check this assumption with greater ease under either static or dynamic treatment strategies. This algorithm also identifies the subgroups found to be violating this assumption, allowing for insights about the nature of the violations and potential solutions. We first present different versions of sPoRT based on either stratifying or pooling over time. Finally, we illustrate its use in a real-life application of HIV-positive children in Southern Africa with and without pooling over time. An R notebook showing how to use sPoRT is available at github.com/ArthurChatton/sPoRT-notebook.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10245v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arthur Chatton, Michael Schomaker, Miguel-Angel Luque-Fernandez, Robert W. Platt, Mireille E. Schnitzer</dc:creator>
    </item>
    <item>
      <title>What if we had built a prediction model with a survival super learner instead of a Cox model 10 years ago?</title>
      <link>https://arxiv.org/abs/2412.10252</link>
      <description>arXiv:2412.10252v1 Announce Type: new 
Abstract: Objective: This study sought to compare the drop in predictive performance over time according to the modeling approach (regression versus machine learning) used to build a kidney transplant failure prediction model with a time-to-event outcome.
  Study Design and Setting: The Kidney Transplant Failure Score (KTFS) was used as a benchmark. We reused the data from which it was developed (DIVAT cohort, n=2,169) to build another prediction algorithm using a survival super learner combining (semi-)parametric and non-parametric methods. Performance in DIVAT was estimated for the two prediction models using internal validation. Then, the drop in predictive performance was evaluated in the same geographical population approximately ten years later (EKiTE cohort, n=2,329).
  Results: In DIVAT, the super learner achieved better discrimination than the KTFS, with a tAUROC of 0.83 (0.79-0.87) compared to 0.76 (0.70-0.82). While the discrimination remained stable for the KTFS, it was not the case for the super learner, with a drop to 0.80 (0.76-0.83). Regarding calibration, the survival SL overestimated graft survival at development, while the KTFS underestimated graft survival ten years later. Brier score values were similar regardless of the approach and the timing.
  Conclusion: The more flexible SL provided superior discrimination on the population used to fit it compared to a Cox model and similar discrimination when applied to a future dataset of the same population. Both methods are subject to calibration drift over time. However, weak calibration on the population used to develop the prediction model was correct only for the Cox model, and recalibration should be considered in the future to correct the calibration drift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10252v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arthur Chatton, \'Emilie Pilote, Kevin Assob Feugo, H\'elo\"ise Cardinal, Robert W. Platt, Mireille E Schnitzer</dc:creator>
    </item>
    <item>
      <title>Matrix Completion via Residual Spectral Matching</title>
      <link>https://arxiv.org/abs/2412.10005</link>
      <description>arXiv:2412.10005v2 Announce Type: cross 
Abstract: Noisy matrix completion has attracted significant attention due to its applications in recommendation systems, signal processing and image restoration. Most existing works rely on (weighted) least squares methods under various low-rank constraints. However, minimizing the sum of squared residuals is not always efficient, as it may ignore the potential structural information in the residuals. In this study, we propose a novel residual spectral matching criterion that incorporates not only the numerical but also locational information of residuals. This criterion is the first in noisy matrix completion to adopt the perspective of low-rank perturbation of random matrices and exploit the spectral properties of sparse random matrices. We derive optimal statistical properties by analyzing the spectral properties of sparse random matrices and bounding the effects of low-rank perturbations and partial observations. Additionally, we propose algorithms that efficiently approximate solutions by constructing easily computable pseudo-gradients. The iterative process of the proposed algorithms ensures convergence at a rate consistent with the optimal statistical error bound. Our method and algorithms demonstrate improved numerical performance in both simulated and real data examples, particularly in environments with high noise levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10005v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyuan Chen, Fang Yao</dc:creator>
    </item>
    <item>
      <title>De-Biasing Structure Function Estimates From Sparse Time Series of the Solar Wind: A Data-Driven Approach</title>
      <link>https://arxiv.org/abs/2412.10053</link>
      <description>arXiv:2412.10053v1 Announce Type: cross 
Abstract: Structure functions, which represent the moments of the increments of a stochastic process, are essential complementary statistics to power spectra for analysing the self-similar behaviour of a time series. However, many real-world environmental datasets, such as those collected by spacecraft monitoring the solar wind, contain gaps, which inevitably corrupt the statistics. The nature of this corruption for structure functions remains poorly understood - indeed, often overlooked. Here we simulate gaps in a large set of magnetic field intervals from Parker Solar Probe in order to characterize the behaviour of the structure function of a sparse time series of solar wind turbulence. We quantify the resultant error with regards to the overall shape of the structure function, and its slope in the inertial range. Noting the consistent underestimation of the true curve when using linear interpolation, we demonstrate the ability of an empirical correction factor to de-bias these estimates. This correction, "learnt" from the data from a single spacecraft, is shown to generalize well to data from a solar wind regime elsewhere in the heliosphere, producing smaller errors, on average, for missing fractions &gt;25%. Given this success, we apply the correction to gap-affected Voyager intervals from the inner heliosheath and local interstellar medium, obtaining spectral indices similar to those from previous studies. This work provides a tool for future studies of fragmented solar wind time series, such as those from Voyager, MAVEN, and OMNI, as well as sparsely-sampled astrophysical and geophysical processes more generally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10053v1</guid>
      <category>astro-ph.SR</category>
      <category>physics.space-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Wrench, Tulasi N. Parashar</dc:creator>
    </item>
    <item>
      <title>Multiscale Dynamical Indices Reveal Scale-Dependent Atmospheric Dynamics</title>
      <link>https://arxiv.org/abs/2412.10069</link>
      <description>arXiv:2412.10069v1 Announce Type: cross 
Abstract: Geophysical systems are inherently complex and span multiple spatial and temporal scales, making their dynamics challenging to understand and predict. This challenge is especially pronounced for extreme events, which are primarily governed by their instantaneous properties rather than their average characteristics. Advances in dynamical systems theory, including the development of local dynamical indices such as local dimension and inverse persistence, have provided powerful tools for studying these short-lasting phenomena. However, existing applications of such indices often rely on predefined fixed spatial domains and scales, with limited discussion on the influence of spatial scales on the results. In this work, we present a novel spatially multiscale methodology that applies a sliding window method to compute dynamical indices, enabling the exploration of scale-dependent properties. Applying this framework to high-impact European summertime heatwaves, we reconcile previously different perspectives, thereby underscoring the importance of spatial scales in such analyses. Furthermore, we emphasize that our novel methodology has broad applicability to other atmospheric phenomena, as well as to other geophysical and spatio-temporal systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10069v1</guid>
      <category>physics.ao-ph</category>
      <category>math.DS</category>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyu Dong, Gabriele Messori, Davide Faranda, Adriano Gualandi, Valerio Lucarini, Gianmarco Mengaldo</dc:creator>
    </item>
    <item>
      <title>AMUSE: Adaptive Model Updating using a Simulated Environment</title>
      <link>https://arxiv.org/abs/2412.10119</link>
      <description>arXiv:2412.10119v1 Announce Type: cross 
Abstract: Prediction models frequently face the challenge of concept drift, in which the underlying data distribution changes over time, weakening performance. Examples can include models which predict loan default, or those used in healthcare contexts. Typical management strategies involve regular model updates or updates triggered by concept drift detection. However, these simple policies do not necessarily balance the cost of model updating with improved classifier performance. We present AMUSE (Adaptive Model Updating using a Simulated Environment), a novel method leveraging reinforcement learning trained within a simulated data generating environment, to determine update timings for classifiers. The optimal updating policy depends on the current data generating process and ongoing drift process. Our key idea is that we can train an arbitrarily complex model updating policy by creating a training environment in which possible episodes of drift are simulated by a parametric model, which represents expectations of possible drift patterns. As a result, AMUSE proactively recommends updates based on estimated performance improvements, learning a policy that balances maintaining model performance with minimizing update costs. Empirical results confirm the effectiveness of AMUSE in simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10119v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Chislett, Catalina A. Vallejos, Timothy I. Cannings, James Liley</dc:creator>
    </item>
    <item>
      <title>Performance evaluation of predictive AI models to support medical decisions: Overview and guidance</title>
      <link>https://arxiv.org/abs/2412.10288</link>
      <description>arXiv:2412.10288v1 Announce Type: cross 
Abstract: A myriad of measures to illustrate performance of predictive artificial intelligence (AI) models have been proposed in the literature. Selecting appropriate performance measures is essential for predictive AI models that are developed to be used in medical practice, because poorly performing models may harm patients and lead to increased costs. We aim to assess the merits of classic and contemporary performance measures when validating predictive AI models for use in medical practice. We focus on models with a binary outcome. We discuss 32 performance measures covering five performance domains (discrimination, calibration, overall, classification, and clinical utility) along with accompanying graphical assessments. The first four domains cover statistical performance, the fifth domain covers decision-analytic performance. We explain why two key characteristics are important when selecting which performance measures to assess: (1) whether the measure's expected value is optimized when it is calculated using the correct probabilities (i.e., a "proper" measure), and (2) whether they reflect either purely statistical performance or decision-analytic performance by properly considering misclassification costs. Seventeen measures exhibit both characteristics, fourteen measures exhibited one characteristic, and one measure possessed neither characteristic (the F1 measure). All classification measures (such as classification accuracy and F1) are improper for clinically relevant decision thresholds other than 0.5 or the prevalence. We recommend the following measures and plots as essential to report: AUROC, calibration plot, a clinical utility measure such as net benefit with decision curve analysis, and a plot with probability distributions per outcome category.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10288v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Van Calster (topic group 6 of the STRATOS initiative), Gary S. Collins (topic group 6 of the STRATOS initiative), Andrew J. Vickers (topic group 6 of the STRATOS initiative), Laure Wynants (topic group 6 of the STRATOS initiative), Kathleen F. Kerr (topic group 6 of the STRATOS initiative), Lasai Barre\~nada (topic group 6 of the STRATOS initiative), Gael Varoquaux (topic group 6 of the STRATOS initiative), Karandeep Singh (topic group 6 of the STRATOS initiative), Karel G. M. Moons (topic group 6 of the STRATOS initiative), Tina Hernandez-boussard (topic group 6 of the STRATOS initiative), Dirk Timmerman (topic group 6 of the STRATOS initiative), David J. Mclernon (topic group 6 of the STRATOS initiative), Maarten Van Smeden (topic group 6 of the STRATOS initiative), Ewout W. Steyerberg (topic group 6 of the STRATOS initiative)</dc:creator>
    </item>
    <item>
      <title>A Neyman-Orthogonalization Approach to the Incidental Parameter Problem</title>
      <link>https://arxiv.org/abs/2412.10304</link>
      <description>arXiv:2412.10304v1 Announce Type: cross 
Abstract: A popular approach to perform inference on a target parameter in the presence of nuisance parameters is to construct estimating equations that are orthogonal to the nuisance parameters, in the sense that their expected first derivative is zero. Such first-order orthogonalization may, however, not suffice when the nuisance parameters are very imprecisely estimated. Leading examples where this is the case are models for panel and network data that feature fixed effects. In this paper, we show how, in the conditional-likelihood setting, estimating equations can be constructed that are orthogonal to any chosen order. Combining these equations with sample splitting yields higher-order bias-corrected estimators of target parameters. In an empirical application we apply our method to a fixed-effect model of team production and obtain estimates of complementarity in production and impacts of counterfactual re-allocations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10304v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>St\'ephane Bonhomme, Koen Jochmans, Martin Weidner</dc:creator>
    </item>
    <item>
      <title>Tucker tensor factor models: matricization and mode-wise PCA estimation</title>
      <link>https://arxiv.org/abs/2206.02508</link>
      <description>arXiv:2206.02508v4 Announce Type: replace 
Abstract: High-dimensional, higher-order tensor data are gaining prominence in a variety of fields, including but not limited to computer vision and network analysis. Tensor factor models, induced from noisy versions of tensor decompositions or factorizations, are natural potent instruments to study a collection of tensor-variate objects that may be dependent or independent. However, it is still in the early stage of developing statistical inferential theories for the estimation of various low-rank structures, which are customary to play the role of signals of tensor factor models. In this paper, we attempt to ``decode" the estimation of a higher-order tensor factor model by leveraging tensor matricization. Specifically, we recast it into mode-wise traditional high-dimensional vector/fiber factor models, enabling the deployment of conventional principal components analysis (PCA) for estimation. Demonstrated by the Tucker tensor factor model (TuTFaM), which is induced from the noisy version of the widely-used Tucker decomposition, we summarize that estimations on signal components are essentially mode-wise PCA techniques, and the involvement of projection and iteration will enhance the signal-to-noise ratio to various extent. We establish the inferential theory of the proposed estimators, conduct rich simulation experiments, and illustrate how the proposed estimations can work in tensor reconstruction, and clustering for independent video and dependent economic datasets, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.02508v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xu Zhang, Guodong Li, Catherine C. Liu, Jianhua Guo</dc:creator>
    </item>
    <item>
      <title>Identification and estimation of causal effects using non-concurrent controls in platform trials</title>
      <link>https://arxiv.org/abs/2404.19118</link>
      <description>arXiv:2404.19118v3 Announce Type: replace 
Abstract: Platform trials are multi-arm designs that simultaneously evaluate multiple treatments for a single disease within the same overall trial structure. Unlike traditional randomized controlled trials, they allow treatment arms to enter and exit the trial at distinct times while maintaining a control arm throughout. This control arm comprises both concurrent controls, where participants are randomized concurrently to either the treatment or control arm, and non-concurrent controls, who enter the trial when the treatment arm under study is unavailable. While flexible, platform trials introduce the challenge of using non-concurrent controls, raising questions about estimating treatment effects. Specifically, which estimands should be targeted? Under what assumptions can these estimands be identified and estimated? Are there any efficiency gains? In this paper, we discuss issues related to the identification and estimation assumptions of common choices of estimand. We conclude that the most robust strategy to increase efficiency without imposing unwarranted assumptions is to target the concurrent average treatment effect (cATE), the ATE among only concurrent units, using a covariate-adjusted doubly robust estimator. Our studies suggest that, for the purpose of obtaining efficiency gains, collecting important prognostic variables is more important than relying on non-concurrent controls. We also discuss the perils of targeting ATE due to an untestable extrapolation assumption that will often be invalid. We provide simulations illustrating our points and an application to the ACTT platform trial, resulting in a 20% improvement in precision compared to the naive estimator that ignores non-concurrent controls and prognostic variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19118v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michele Santacatterina, Federico Macchiavelli Giron, Xinyi Zhang, Ivan Diaz</dc:creator>
    </item>
    <item>
      <title>Modeling frequency distribution above a priority in presence of IBNR</title>
      <link>https://arxiv.org/abs/2405.02871</link>
      <description>arXiv:2405.02871v2 Announce Type: replace 
Abstract: In reinsurance, Poisson and Negative binomial distributions are employed for modeling frequency. However, the incomplete data regarding reported incurred claims above a priority level presents challenges in estimation. This paper focuses on frequency estimation using Schnieper's framework for claim numbering. We demonstrate that Schnieper's model is consistent with a Poisson distribution for the total number of claims above a priority at each year of development, providing a robust basis for parameter estimation. Additionally, we explain how to build an alternative assumption based on a Negative binomial distribution, which yields similar results. The study includes a bootstrap procedure to manage uncertainty in parameter estimation and a case study comparing assumptions and evaluating the impact of the bootstrap approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02871v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/03461238.2024.2439815</arxiv:DOI>
      <dc:creator>Nicolas Baradel</dc:creator>
    </item>
    <item>
      <title>Model Selection for Causal Modeling in Missing Exposure Problems</title>
      <link>https://arxiv.org/abs/2406.12171</link>
      <description>arXiv:2406.12171v2 Announce Type: replace 
Abstract: In causal inference, properly selecting the propensity score (PS) model is an important topic and has been widely investigated in observational studies. There is also a large literature focusing on the missing data problem. However, there are very few studies investigating the model selection issue for causal inference when the exposure is missing at random (MAR). In this paper, we discuss how to select both imputation and PS models, which can result in the smallest root mean squared error (RMSE) of the estimated causal effect in our simulation study. Then, we propose a new criterion, called ``rank score'' for evaluating the overall performance of both models. The simulation studies show that the full imputation plus the outcome-related PS models lead to the smallest RMSE and the rank score can help select the best models. An application study is conducted to quantify the causal effect of cardiovascular disease (CVD) on the mortality of COVID-19 patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12171v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuliang Shi, Yeying Zhu, Joel A. Dubin</dc:creator>
    </item>
    <item>
      <title>A Deterministic Information Bottleneck Method for Clustering Mixed-Type Data</title>
      <link>https://arxiv.org/abs/2407.03389</link>
      <description>arXiv:2407.03389v2 Announce Type: replace 
Abstract: In this paper, we present an information-theoretic method for clustering mixed-type data, that is, data consisting of both continuous and categorical variables. The proposed approach is built on the deterministic variant of the Information Bottleneck algorithm, designed to optimally compress data while preserving its relevant structural information. We evaluate the performance of our method against four well-established clustering techniques for mixed-type data -- KAMILA, K-Prototypes, Factor Analysis for Mixed Data with K-Means, and Partitioning Around Medoids using Gower's dissimilarity -- using both simulated and real-world datasets. The results highlight that the proposed approach offers a competitive alternative to traditional clustering techniques, particularly under specific conditions where heterogeneity in data poses significant challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03389v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Efthymios Costa, Ioanna Papatsouma, Angelos Markos</dc:creator>
    </item>
    <item>
      <title>Quasi-Bayesian sequential deconvolution</title>
      <link>https://arxiv.org/abs/2408.14402</link>
      <description>arXiv:2408.14402v2 Announce Type: replace 
Abstract: Density deconvolution deals with the estimation of the probability density function $f$ of a random signal from $n\geq1$ data observed with independent and known additive random noise. This is a classical problem in statistics, for which frequentist and Bayesian nonparametric approaches are available to estimate $f$ in static or batch domains. In this paper, we consider the problem of density deconvolution in a streaming or online domain, and develop a principled sequential approach to estimate $f$. By relying on a quasi-Bayesian sequential (learning) model for the data, often referred to as Newton's algorithm, we obtain a sequential deconvolution estimate $f_{n}$ of $f$ that is of easy evaluation, computationally efficient, and with constant computational cost as data increase, which is desirable for streaming data. In particular, local and uniform Gaussian central limit theorems for $f_{n}$ are established, leading to asymptotic credible intervals and bands for $f$, respectively. We provide the sequential deconvolution estimate $f_{n}$ with large sample asymptotic guarantees under the quasi-Bayesian sequential model for the data, proving a merging with respect to the direct density estimation problem, and also under a ``true" frequentist model for the data, proving consistency. An empirical validation of our methods is presented on synthetic and real data, also comparing with respect to a kernel approach and a Bayesian nonparametric approach with a Dirichlet process mixture prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14402v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefano Favaro, Sandra Fortini</dc:creator>
    </item>
    <item>
      <title>Root cause discovery via permutations and Cholesky decomposition</title>
      <link>https://arxiv.org/abs/2410.12151</link>
      <description>arXiv:2410.12151v3 Announce Type: replace 
Abstract: This work is motivated by the following problem: Can we identify the disease-causing gene in a patient affected by a monogenic disorder? This problem is an instance of root cause discovery. In particular, we aim to identify the intervened variable in one interventional sample using a set of observational samples as reference. We consider a linear structural equation model where the causal ordering is unknown. We begin by examining a simple method that uses squared z-scores and characterize the conditions under which this method succeeds and fails, showing that it generally cannot identify the root cause. We then prove, without additional assumptions, that the root cause is identifiable even if the causal ordering is not. Two key ingredients of this identifiability result are the use of permutations and the Cholesky decomposition, which allow us to exploit an invariant property across different permutations to discover the root cause. Furthermore, we characterize permutations that yield the correct root cause and, based on this, propose a valid method for root cause discovery. We also adapt this approach to high-dimensional settings. Finally, we evaluate the performance of our methods through simulations and apply the high-dimensional method to discover disease-causing genes in the gene expression dataset that motivates this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12151v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinzhou Li, Benjamin B. Chu, Ines F. Scheller, Julien Gagneur, Marloes H. Maathuis</dc:creator>
    </item>
    <item>
      <title>Efficient inference for differential equation models without numerical solvers</title>
      <link>https://arxiv.org/abs/2411.10494</link>
      <description>arXiv:2411.10494v4 Announce Type: replace 
Abstract: Parameter inference is essential when interpreting observational data using mathematical models. Standard inference methods for differential equation models typically rely on obtaining repeated numerical solutions of the differential equation(s). Recent results have explored how numerical truncation error can have major, detrimental, and sometimes hidden impacts on likelihood-based inference by introducing false local maxima into the log-likelihood function. We present a straightforward approach for inference that eliminates the need for solving the underlying differential equations, thereby completely avoiding the impact of truncation error. Open-access Jupyter notebooks, available on GitHub, allow others to implement this method for a broad class of widely-used models to interpret biological data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10494v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Johnston, Ruth E. Baker, Matthew J. Simpson</dc:creator>
    </item>
    <item>
      <title>Optimal Multitask Linear Regression and Contextual Bandits under Sparse Heterogeneity</title>
      <link>https://arxiv.org/abs/2306.06291</link>
      <description>arXiv:2306.06291v3 Announce Type: replace-cross 
Abstract: Large and complex datasets are often collected from several, possibly heterogeneous sources. Multitask learning methods improve efficiency by leveraging commonalities across datasets while accounting for possible differences among them. Here, we study multitask linear regression and contextual bandits under sparse heterogeneity, where the source/task-associated parameters are equal to a global parameter plus a sparse task-specific term. We propose a novel two-stage estimator called MOLAR that leverages this structure by first constructing a covariate-wise weighted median of the task-wise linear regression estimates and then shrinking the task-wise estimates towards the weighted median. Compared to task-wise least squares estimates, MOLAR improves the dependence of the estimation error on the data dimension. Extensions of MOLAR to generalized linear models and constructing confidence intervals are discussed in the paper. We then apply MOLAR to develop methods for sparsely heterogeneous multitask contextual bandits, obtaining improved regret guarantees over single-task bandit methods. We further show that our methods are minimax optimal by providing a number of lower bounds. Finally, we support the efficiency of our methods by performing experiments on both synthetic data and the PISA dataset on student educational outcomes from heterogeneous countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06291v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2024.2439622</arxiv:DOI>
      <dc:creator>Xinmeng Huang, Kan Xu, Donghwan Lee, Hamed Hassani, Hamsa Bastani, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Semiparametric Inference for Regression-Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2403.05803</link>
      <description>arXiv:2403.05803v2 Announce Type: replace-cross 
Abstract: Treatment effects in regression discontinuity designs (RDDs) are often estimated using local regression methods. \cite{Hahn:01} demonstrated that the identification of the average treatment effect at the cutoff in RDDs relies on the unconfoundedness assumption and that, without this assumption, only the local average treatment effect at the cutoff can be identified. In this paper, we propose a semiparametric framework tailored for identifying the average treatment effect in RDDs, eliminating the need for the unconfoundedness assumption. Our approach globally conceptualizes the identification as a partially linear modeling problem, with the coefficient of a specified polynomial function of propensity score in the linear component capturing the average treatment effect. This identification result underpins our semiparametric inference for RDDs, employing the $P$-spline method to approximate the nonparametric function and establishing a procedure for conducting inference within this framework. Through theoretical analysis, we demonstrate that our global approach achieves a faster convergence rate compared to the local method. Monte Carlo simulations further confirm that the proposed method consistently outperforms alternatives across various scenarios. Furthermore, applications to real-world datasets illustrate that our global approach can provide more reliable inference for practical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05803v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weiwei Jiang, Rong J. B. Zhu</dc:creator>
    </item>
    <item>
      <title>Synthetic Potential Outcomes and Causal Mixture Identifiability</title>
      <link>https://arxiv.org/abs/2405.19225</link>
      <description>arXiv:2405.19225v4 Announce Type: replace-cross 
Abstract: Heterogeneous data from multiple populations, sub-groups, or sources is often represented as a ``mixture model'' with a single latent class influencing all of the observed covariates. Heterogeneity can be resolved at multiple levels by grouping populations according to different notions of similarity. This paper proposes grouping with respect to the causal response of an intervention or perturbation on the system. This definition is distinct from previous notions, such as similar covariate values (e.g. clustering) or similar correlations between covariates (e.g. Gaussian mixture models). To solve the problem, we ``synthetically sample'' from a counterfactual distribution using higher-order multi-linear moments of the observable data. To understand how these ``causal mixtures'' fit in with more classical notions, we develop a hierarchy of mixture identifiability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19225v4</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bijan Mazaheri, Chandler Squires, Caroline Uhler</dc:creator>
    </item>
  </channel>
</rss>
