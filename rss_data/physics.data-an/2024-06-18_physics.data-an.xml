<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Uncertainties in ROC (Receiver Operating Characteristic) Curves Derived from Counting Data</title>
      <link>https://arxiv.org/abs/2406.11396</link>
      <description>arXiv:2406.11396v1 Announce Type: new 
Abstract: The ROC (receiver operating characteristic) curve is a widely used device for assessing decision-making systems. It seems surprising, in view of its history dating back to World War Two, that the assignment of uncertainties to a ROC curve is apparently not settled. This note returns to the question, focusing on the application of ROC curves to the analysis of data from counting experiments and taking a practical operational approach to the concept of uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11396v1</guid>
      <category>physics.data-an</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>M. P. Fewell</dc:creator>
    </item>
    <item>
      <title>Insights into Dark Matter Direct Detection Experiments: Decision Trees versus Deep Learning</title>
      <link>https://arxiv.org/abs/2406.10372</link>
      <description>arXiv:2406.10372v1 Announce Type: cross 
Abstract: The detection of Dark Matter (DM) remains a significant challenge in particle physics. This study exploits advanced machine learning models to improve detection capabilities of liquid xenon time projection chamber experiments, utilizing state-of-the-art transformers alongside traditional methods like Multilayer Perceptrons and Convolutional Neural Networks. We evaluate various data representations and find that simplified feature representations, particularly corrected S1 and S2 signals, retain critical information for classification. Our results show that while transformers offer promising performance, simpler models like XGBoost can achieve comparable results with optimal data representations. We also derive exclusion limits in the cross-section versus DM mass parameter space, showing minimal differences between XGBoost and the best performing deep learning models. The comparative analysis of different machine learning approaches provides a valuable reference for future experiments by guiding the choice of models and data representations to maximize detection capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10372v1</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.HE</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel E. Lopez-Fogliani, Andres D. Perez, Roberto Ruiz de Austri</dc:creator>
    </item>
    <item>
      <title>Transient Measurement of Near-field Thermal Radiation between Macroscopic Objects</title>
      <link>https://arxiv.org/abs/2406.10619</link>
      <description>arXiv:2406.10619v1 Announce Type: cross 
Abstract: The involvement of evanescent waves in the near-field regime could greatly enhance the spontaneous thermal radiation, offering a unique opportunity to study nanoscale photon-phonon interaction. However, accurately characterizing this subtle phenomenon is very challenging. This paper proposes a transient all-optical method for rapidly characterizing near-field radiative heat transfer (NFRHT) between macroscopic objects, using the first law of thermodynamics. Significantly, a full measurement at a fixed gap distance is completed within tens of seconds. By simplifying the configuration, the transient all-optical method achieves high measurement accuracy and reliable reproducibility. The proposed method can effectively analyze the NFRHT in various material systems, including SiO2, SiC, and Si, which involve different phonon or plasmon polaritons. Experimental observations demonstrate significant super-Planckian radiation, which arises from the near-field coupling of bounded surface modes. Furthermore, the method achieves excellent agreement with theory, with a minimal discrepancy of less than 2.7% across a wide temperature range. This wireless method could accurately characterize the NFRHT for objects with different sizes or optical properties, enabling the exploration of both fundamental interests and practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10619v1</guid>
      <category>physics.optics</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sen Zhang, Yongdi Dang, Xinran Li, Yuxuan Li, Yi Jin, Pankaj K Choudhury, Jianbing Xu, Yungui Ma</dc:creator>
    </item>
    <item>
      <title>Predicting Exoplanetary Features with a Residual Model for Uniform and Gaussian Distributions</title>
      <link>https://arxiv.org/abs/2406.10771</link>
      <description>arXiv:2406.10771v1 Announce Type: cross 
Abstract: The advancement of technology has led to rampant growth in data collection across almost every field, including astrophysics, with researchers turning to machine learning to process and analyze this data. One prominent example of this data in astrophysics is the atmospheric retrievals of exoplanets. In order to help bridge the gap between machine learning and astrophysics domain experts, the 2023 Ariel Data Challenge was hosted to predict posterior distributions of 7 exoplanetary features. The procedure outlined in this paper leveraged a combination of two deep learning models to address this challenge: a Multivariate Gaussian model that generates the mean and covariance matrix of a multivariate Gaussian distribution, and a Uniform Quantile model that predicts quantiles for use as the upper and lower bounds of a uniform distribution. Training of the Multivariate Gaussian model was found to be unstable, while training of the Uniform Quantile model was stable. An ensemble of uniform distributions was found to have competitive results during testing (posterior score of 696.43), and when combined with a multivariate Gaussian distribution achieved a final rank of third in the 2023 Ariel Data Challenge (final score of 681.57).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10771v1</guid>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Sweet</dc:creator>
    </item>
    <item>
      <title>DustNet: skillful neural network predictions of Saharan dust</title>
      <link>https://arxiv.org/abs/2406.11754</link>
      <description>arXiv:2406.11754v1 Announce Type: cross 
Abstract: Suspended in the atmosphere are millions of tonnes of mineral dust which interacts with weather and climate. Accurate representation of mineral dust in weather models is vital, yet remains challenging. Large scale weather models use high power supercomputers and take hours to complete the forecast. Such computational burden allows them to only include monthly climatological means of mineral dust as input states inhibiting their forecasting accuracy. Here, we introduce DustNet a simple, accurate and super fast forecasting model for 24-hours ahead predictions of aerosol optical depth AOD. DustNet trains in less than 8 minutes and creates predictions in 2 seconds on a desktop computer. Created by DustNet predictions outperform the state-of-the-art physics-based model on coarse 1 x 1 degree resolution at 95% of grid locations when compared to ground truth satellite data. Our results show DustNet has a potential for fast and accurate AOD forecasting which could transform our understanding of dust impacts on weather patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11754v1</guid>
      <category>physics.geo-ph</category>
      <category>cs.AI</category>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Trish E. Nowak, Andy T. Augousti, Benno I. Simmons, Stefan Siegert</dc:creator>
    </item>
    <item>
      <title>Regularization of the ensemble Kalman filter using a non-parametric, non-stationary spatial model</title>
      <link>https://arxiv.org/abs/2306.14318</link>
      <description>arXiv:2306.14318v4 Announce Type: replace 
Abstract: The sample covariance matrix of a random vector is a good estimate of the true covariance matrix if the sample size is much larger than the length of the vector. In high-dimensional problems, this condition is never met. As a result, in high dimensions the Ensemble Kalman Filter's (EnKF) ensemble does not contain enough information to specify the prior covariance matrix accurately. This necessitates the need for regularization of the analysis (observation update) problem. We propose a regularization technique based on a new spatial model. The model is a constrained version of the general Gaussian process convolution model. The constraints include local stationarity and smoothness of local spectra. We regularize EnKF by postulating that its prior covariances obey the spatial model. Placing a hyperprior distribution on the model parameters and using the likelihood of the prior ensemble data allows for an optimized use of both the ensemble and the hyperprior. The respective estimator is shown to be consistent. Its neural Bayes implementation proved to be both accurate and computationally efficient. In simulation experiments, the new technique led to substantially better EnKF performance than several existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14318v4</guid>
      <category>physics.data-an</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Tsyrulnikov, Arseniy Sotskiy</dc:creator>
    </item>
    <item>
      <title>Deep Probabilistic Direction Prediction in 3D with Applications to Directional Dark Matter Detectors</title>
      <link>https://arxiv.org/abs/2403.15949</link>
      <description>arXiv:2403.15949v2 Announce Type: replace 
Abstract: We present the first method to probabilistically predict 3D direction in a deep neural network model. The probabilistic predictions are modeled as a heteroscedastic von Mises-Fisher distribution on the sphere $\mathbb{S}^2$, giving a simple way to quantify aleatoric uncertainty. This approach generalizes the cosine distance loss which is a special case of our loss function when the uncertainty is assumed to be uniform across samples. We develop approximations required to make the likelihood function and gradient calculations stable. The method is applied to the task of predicting the 3D directions of electrons, the most complex signal in a class of experimental particle physics detectors designed to demonstrate the particle nature of dark matter and study solar neutrinos. Using simulated Monte Carlo data, the initial direction of recoiling electrons is inferred from their tortuous trajectories, as captured by the 3D detectors. For $40\,$keV electrons in a $70\%$ $\textrm{He}$ $30 \%$ $\textrm{CO}_2$ gas mixture at STP, the new approach achieves a mean cosine distance of $0.104$ ($26^\circ$) compared to $0.556$ ($64^\circ$) achieved by a non-machine learning algorithm. We show that the model is well-calibrated and accuracy can be increased further by removing samples with high predicted uncertainty. This advancement in probabilistic 3D directional learning could increase the sensitivity of directional dark matter detectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15949v2</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Majd Ghrear, Peter Sadowski, Sven Einar Vahsen</dc:creator>
    </item>
    <item>
      <title>Entropy-based random models for hypergraphs</title>
      <link>https://arxiv.org/abs/2207.12123</link>
      <description>arXiv:2207.12123v2 Announce Type: replace-cross 
Abstract: Network theory has primarily focused on pairwise relationships, disregarding many-body interactions: neglecting them, however, can lead to misleading representations of complex systems. Hypergraphs represent an increasingly popular alternative for describing polyadic interactions: our innovation lies in leveraging the representation of hypergraphs based on the incidence matrix for extending the entropy-based framework to higher-order structures. In analogy with the Exponential Random Graphs, we name the members of this novel class of models Exponential Random Hypergraphs. Here, we focus on two explicit examples, i.e. the generalisations of the Erd\"os-R\'enyi Model and of the Configuration Model. After discussing their asymptotic properties, we employ them to analyse real-world configurations: more specifically, i) we extend the definition of several network quantities to hypergraphs, ii) compute their expected value under each null model and iii) compare it with the empirical one, in order to detect deviations from random behaviours. Differently from currently available techniques, ours is analytically tractable, scalable and effective in singling out the structural patterns of real-world hypergraphs differing significantly from those emerging as a consequence of simpler, structural constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.12123v2</guid>
      <category>cs.SI</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabio Saracco, Giovanni Petri, Renaud Lambiotte, Tiziano Squartini</dc:creator>
    </item>
    <item>
      <title>Background Modeling for Double Higgs Boson Production: Density Ratios and Optimal Transport</title>
      <link>https://arxiv.org/abs/2208.02807</link>
      <description>arXiv:2208.02807v3 Announce Type: replace-cross 
Abstract: We study the problem of data-driven background estimation, arising in the search of physics signals predicted by the Standard Model at the Large Hadron Collider. Our work is motivated by the search for the production of pairs of Higgs bosons decaying into four bottom quarks. A number of other physical processes, known as background, also share the same final state. The data arising in this problem is therefore a mixture of unlabeled background and signal events, and the primary aim of the analysis is to determine whether the proportion of unlabeled signal events is nonzero. A challenging but necessary first step is to estimate the distribution of background events. Past work in this area has determined regions of the space of collider events where signal is unlikely to appear, and where the background distribution is therefore identifiable. The background distribution can be estimated in these regions, and extrapolated into the region of primary interest using transfer learning with a multivariate classifier. We build upon this existing approach in two ways. First, we revisit this method by developing a customized residual neural network which is tailored to the structure and symmetries of collider data. Second, we develop a new method for background estimation, based on the optimal transport problem, which relies on modeling assumptions distinct from earlier work. These two methods can serve as cross-checks for each other in particle physics analyses, due to the complementarity of their underlying assumptions. We compare their performance on simulated double Higgs boson data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.02807v3</guid>
      <category>stat.AP</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tudor Manole, Patrick Bryant, John Alison, Mikael Kuusela, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Data Science Education in Undergraduate Physics: Lessons Learned from a Community of Practice</title>
      <link>https://arxiv.org/abs/2403.00961</link>
      <description>arXiv:2403.00961v2 Announce Type: replace-cross 
Abstract: It is becoming increasingly important that physics educators equip their students with the skills to work with data effectively. However, many educators may lack the necessary training and expertise in data science to teach these skills. To address this gap, we created the Data Science Education Community of Practice (DSECOP), bringing together graduate students and physics educators from different institutions and backgrounds to share best practices and lessons learned from integrating data science into undergraduate physics education. In this article we present insights and experiences from this community of practice, highlighting key strategies and challenges in incorporating data science into the introductory physics curriculum. Our goal is to provide guidance and inspiration to educators who seek to integrate data science into their teaching, helping to prepare the next generation of physicists for a data-driven world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00961v2</guid>
      <category>physics.ed-ph</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karan Shah, Julie Butler, Alexis Knaub, An{\i}l Zengino\u{g}lu, William Ratcliff, Mohammad Soltanieh-ha</dc:creator>
    </item>
    <item>
      <title>Synergy as the failure of distributivity</title>
      <link>https://arxiv.org/abs/2404.03455</link>
      <description>arXiv:2404.03455v2 Announce Type: replace-cross 
Abstract: A physical system is synergistic if it cannot be reduced to its constituents. Intuitively this is paraphrased into the common statement that 'the whole is greater than the sum of its parts'. In this manner, many basic elements in combination may give rise to some unexpected collective behavior. A paradigmatic example of such phenomenon is information. Several sources, which are already known individually, may provide some new knowledge when joined together. Here we take the trivial case of discrete random variables and explore whether and how it is possible get more information out of lesser parts. Our approach is inspired by set theory as the fundamental description of part-whole relations. If taken unaltered, synergistic behavior is forbidden by the set theoretical axioms. Indeed, the union of sets cannot contain extra elements not found in any particular one of them. However, random variables are not a perfect analogy of sets. We formalise the distinction, finding a single broken axiom - union/intersection distributivity. Nevertheless, it remains possible to describe information using Venn-type diagrams. We directly connect the existence of synergy to the failure of distributivity for random variables. When compared to the partial information decomposition framework (PID), our technique fully reproduces previous results while resolving the self-contradictions that plagued the field and providing additional constraints on the solutions. This opens the way towards quantifying emergence in large systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03455v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.bio-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivan A. Sevostianov, Ofer Feinerman</dc:creator>
    </item>
    <item>
      <title>When Pearson $\chi^2$ and other divisible statistics are not goodness-of-fit tests</title>
      <link>https://arxiv.org/abs/2406.09195</link>
      <description>arXiv:2406.09195v2 Announce Type: replace-cross 
Abstract: Thousands of experiments are analyzed and papers are published each year involving the statistical analysis of grouped data. While this area of statistics is often perceived - somewhat naively - as saturated, several misconceptions still affect everyday practice, and new frontiers have so far remained unexplored. Researchers must be aware of the limitations affecting their analyses and what are the new possibilities in their hands.
  Motivated by this need, the article introduces a unifying approach to the analysis of grouped data which allows us to study the class of divisible statistics - that includes Pearson's $\chi^2$, the likelihood ratio as special cases - with a fresh perspective. The contributions collected in this manuscript span from modeling and estimation to distribution-free goodness-of-fit tests.
  Perhaps the most surprising result presented here is that, in a sparse regime, all tests proposed in the literature are dominated by a class of weighted linear statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09195v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Algeri, Estate V. Khmaladze</dc:creator>
    </item>
  </channel>
</rss>
