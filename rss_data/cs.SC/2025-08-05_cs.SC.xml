<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SC</link>
    <description>cs.SC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Aug 2025 04:02:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Decomposition of Symmetrical Classes of Central Configurations</title>
      <link>https://arxiv.org/abs/2508.02918</link>
      <description>arXiv:2508.02918v1 Announce Type: cross 
Abstract: We study central configurations when the set of positions is symmetric. We use a theorem from representation theory of finite groups to explore the symmetry properties of equations for central configurations. This approach simplifies equations for central configurations by considering arbitrary numbers of bodies, symmetry groups, and dimensions. We discuss how to use this theorem to obtain a more refined decomposition of the equations than that given before. The decomposition presented here uses the symmetry-adapted basis method. As an application, we give a complete description of the existence and which masses are possible for central configurations of two nested regular tetrahedrons, two nested regular octahedrons, and two nested regular cubes. To do this, we employ some methods of rational parameterizations and isolation of zeros of multivariate polynomials. The decomposition obtained allows symbolic calculations to be used to study the expressions. This way, we summarize previous discussions and extend them by completing the analysis on the cube case, for both the inverse and direct problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02918v1</guid>
      <category>math.DS</category>
      <category>cs.SC</category>
      <category>math.RT</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcelo P. Santos (Federal Rural University of Pernambuco), Leon D. da Silva (Federal Rural University of Pernambuco)</dc:creator>
    </item>
    <item>
      <title>A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning</title>
      <link>https://arxiv.org/abs/2508.03366</link>
      <description>arXiv:2508.03366v1 Announce Type: cross 
Abstract: General logical reasoning, defined as the ability to reason deductively on domain-agnostic tasks, continues to be a challenge for large language models (LLMs). Current LLMs fail to reason deterministically and are not interpretable. As such, there has been a recent surge in interest in neurosymbolic AI, which attempts to incorporate logic into neural networks. We first identify two main neurosymbolic approaches to improving logical reasoning: (i) the integrative approach comprising models where symbolic reasoning is contained within the neural network, and (ii) the hybrid approach comprising models where a symbolic solver, separate from the neural network, performs symbolic reasoning. Both contain AI systems with promising results on domain-specific logical reasoning benchmarks. However, their performance on domain-agnostic benchmarks is understudied. To the best of our knowledge, there has not been a comparison of the contrasting approaches that answers the following question: Which approach is more promising for developing general logical reasoning? To analyze their potential, the following best-in-class domain-agnostic models are introduced: Logic Neural Network (LNN), which uses the integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the hybrid approach. Using both models as case studies and representatives of each approach, our analysis demonstrates that the hybrid approach is more promising for developing general logical reasoning because (i) its reasoning chain is more interpretable, and (ii) it retains the capabilities and advantages of existing LLMs. To support future works using the hybrid approach, we propose a generalizable framework based on LLM-SS that is modular by design, model-agnostic, domain-agnostic, and requires little to no human input.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03366v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SC</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael K. Chen</dc:creator>
    </item>
  </channel>
</rss>
