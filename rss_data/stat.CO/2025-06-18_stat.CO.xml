<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Jun 2025 04:02:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Branching Stein Variational Gradient Descent for sampling multimodal distributions</title>
      <link>https://arxiv.org/abs/2506.13916</link>
      <description>arXiv:2506.13916v1 Announce Type: cross 
Abstract: We propose a novel particle-based variational inference method designed to work with multimodal distributions. Our approach, referred to as Branched Stein Variational Gradient Descent (BSVGD), extends the classical Stein Variational Gradient Descent (SVGD) algorithm by incorporating a random branching mechanism that encourages the exploration of the state space. In this work, a theoretical guarantee for the convergence in distribution is presented, as well as numerical experiments to validate the suitability of our algorithm. Performance comparisons between the BSVGD and the SVGD are presented using the Wasserstein distance between samples and the corresponding computational times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13916v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaias Banales, Arturo Jaramillo, Heli Ricalde Guerrero</dc:creator>
    </item>
    <item>
      <title>An Exact and Efficient Sampler for Dynamic Discrete Distributions</title>
      <link>https://arxiv.org/abs/2506.14062</link>
      <description>arXiv:2506.14062v1 Announce Type: cross 
Abstract: Sampling from a dynamic discrete distribution involves sampling from a dynamic set of weighted elements, where elements can be added or removed at any stage of the sampling process. Although efficient for static sets, the Alias method becomes impractical in dynamic settings due to the need to reconstruct the sampler after each update, which incurs a computational cost proportional to the size of the distribution, making it unsuitable for applications requiring frequent insertions, deletions, or weight adjustments. To address this limitation, different approaches have been studied, such as the Forest of Trees method and the BUcket Sampling (BUS) method. However, all previous methods suffered from numerical issues which can bias the sampling process. In this paper, we describe EBUS (Exact BUcket Sampling), the first exact algorithm with $O(1)$ sampling and update cost. The sampler can be updated by base-$b$ numbers with bounded precision and exponent, and sample the distribution of its elements exactly and efficiently. We provide also a state of the art implementation of the method using IEEE 64-bit floating point numbers which we empirically show to be more efficient than several implementations of previous inexact methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14062v1</guid>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lilith Orion Hafner, Adriano Meligrana</dc:creator>
    </item>
    <item>
      <title>Adjustment for Confounding using Pre-Trained Representations</title>
      <link>https://arxiv.org/abs/2506.14329</link>
      <description>arXiv:2506.14329v1 Announce Type: cross 
Abstract: There is growing interest in extending average treatment effect (ATE) estimation to incorporate non-tabular data, such as images and text, which may act as sources of confounding. Neglecting these effects risks biased results and flawed scientific conclusions. However, incorporating non-tabular data necessitates sophisticated feature extractors, often in combination with ideas of transfer learning. In this work, we investigate how latent features from pre-trained neural networks can be leveraged to adjust for sources of confounding. We formalize conditions under which these latent features enable valid adjustment and statistical inference in ATE estimation, demonstrating results along the example of double machine learning. We discuss critical challenges inherent to latent feature learning and downstream parameter estimation arising from the high dimensionality and non-identifiability of representations. Common structural assumptions for obtaining fast convergence rates with additive or sparse linear models are shown to be unrealistic for latent features. We argue, however, that neural networks are largely insensitive to these issues. In particular, we show that neural networks can achieve fast convergence rates by adapting to intrinsic notions of sparsity and dimension of the learning problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14329v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rickmer Schulte, David R\"ugamer, Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>Generating uniform linear extensions using few random bits</title>
      <link>https://arxiv.org/abs/2506.14725</link>
      <description>arXiv:2506.14725v1 Announce Type: cross 
Abstract: A \emph{linear extension} of a partial order \(\preceq\) over items \(A = \{ 1, 2, \ldots, n \}\) is a permutation \(\sigma\) such that for all \(i &lt; j\) in \(A\), it holds that \(\neg(\sigma(j) \preceq \sigma(i))\). Consider the problem of generating uniformly from the set of linear extensions of a partial order. The best method currently known uses \(O(n^3 \ln(n))\) operations and \(O(n^3 \ln(n)^2)\) iid fair random bits to generate such a permutation. This paper presents a method that generates a uniform linear extension using only \(2.75 n^3 \ln(n)\) operations and \( 1.83 n^3 \ln(n) \) iid fair bits on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14725v1</guid>
      <category>cs.CC</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Huber</dc:creator>
    </item>
    <item>
      <title>Simulation study to evaluate when Plasmode simulation is superior to parametric simulation in estimating the mean squared error of the least squares estimator in linear regression</title>
      <link>https://arxiv.org/abs/2312.04077</link>
      <description>arXiv:2312.04077v4 Announce Type: replace-cross 
Abstract: Simulation is a crucial tool for the evaluation and comparison of statistical methods. How to design fair and neutral simulation studies is therefore of great interest for researchers developing new methods and practitioners confronted with the choice of the most suitable method. The term simulation usually refers to parametric simulation, that is, computer experiments using artificial data made up of pseudo-random numbers. Plasmode simulation, that is, computer experiments using the combination of resampling feature data from a real-life dataset and generating the target variable with a known user-selected outcome-generating model (OGM), is an alternative that is often claimed to produce more realistic data. We compare parametric and Plasmode simulation for the example of estimating the mean squared error (MSE) of the least squares estimator (LSE) in linear regression. If the true underlying data-generating process (DGP) and the OGM were known, parametric simulation would obviously be the best choice in terms of estimating the MSE well. However, in reality, both are usually unknown, so researchers have to make assumptions: in Plasmode simulation for the OGM, in parametric simulation for both DGP and OGM. Most likely, these assumptions do not exactly reflect the truth. Here, we aim to find out how assumptions deviating from the true DGP and the true OGM affect the performance of parametric and Plasmode simulations in the context of MSE estimation for the LSE and in which situations which simulation type is preferable. Our results suggest that the preferable simulation method depends on many factors, including the number of features, and on how and to what extent the assumptions of a parametric simulation differ from the true DGP. Also, the resampling strategy used for Plasmode influences the results. In particular, subsampling with a small sampling proportion can be recommended.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04077v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1371/journal.pone.0299989</arxiv:DOI>
      <arxiv:journal_reference>PLOS ONE (2024)</arxiv:journal_reference>
      <dc:creator>Marieke Stolte, Nicholas Schreck, Alla Slynko, Maral Saadati, Axel Benner, J\"org Rahnenf\"uhrer, Andrea Bommert</dc:creator>
    </item>
    <item>
      <title>Explicit convergence rates of underdamped Langevin dynamics under weighted and weak Poincar\'e--Lions inequalities</title>
      <link>https://arxiv.org/abs/2407.16033</link>
      <description>arXiv:2407.16033v2 Announce Type: replace-cross 
Abstract: We study the long-time behavior of the underdamped Langevin dynamics, in the case of so-called \emph{weak confinement}. Indeed, any $\mathrm{L}^\infty$ distribution (in position and velocity) relaxes to equilibrium over time, and we quantify the convergence rate. In our situation, the spatial equilibrium distribution does not satisfy a Poincar\'e inequality. Instead, we assume a weighted Poincar\'e inequality, which allows for fat-tail or sub-exponential potential energies. We provide constructive and fully explicit estimates in $\mathrm{L}^2$-norm for $\mathrm{L}^\infty$ initial data. A key-ingredient is a new space-time weighted Poincar\'e--Lions inequality, entailing, in turn, a weak Poincar\'e--Lions inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16033v2</guid>
      <category>math.PR</category>
      <category>math.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni Brigati, Gabriel Stoltz, Andi Q. Wang, Lihan Wang</dc:creator>
    </item>
  </channel>
</rss>
