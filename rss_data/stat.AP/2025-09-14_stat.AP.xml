<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Sep 2025 04:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Path Signature Framework for Detecting Creative Fatigue in Digital Advertising</title>
      <link>https://arxiv.org/abs/2509.09758</link>
      <description>arXiv:2509.09758v1 Announce Type: new 
Abstract: The finite lifespan of advertising effectiveness, commonly known as "creative fatigue", presents a significant challenge in computational marketing. Identifying the onset of performance degradation is critical for optimising media spend and maximising campaign returns, yet traditional methods often lack the sensitivity and scalability required in modern advertising ecosystems. This paper introduces a novel methodological framework for detecting creative fatigue by applying path signature analysis, a sophisticated technique from stochastic analysis, to performance time-series data. We treat an ad's performance trajectory as a path in two-dimensional space and use its signature as a rich feature descriptor. By calculating the distance between the signatures of consecutive time windows, our method identifies statistically significant change points in performance dynamics. We further translate these statistical events into a direct financial metric by quantifying the opportunity cost of continued investment in a fatigued creative. Through synthetic experiments and case studies, we demonstrate that this signature-based approach provides a mathematically principled and complementary framework for analysing creative fatigue patterns. To the best of our knowledge, this represents the first application of path signature methods to the advertising fatigue detection problem, opening new avenues for geometric approaches in marketing analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09758v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Shaw</dc:creator>
    </item>
    <item>
      <title>Benchmarking Classical, Machine Learning, and Bayesian Survival Models for Clinical Prediction</title>
      <link>https://arxiv.org/abs/2509.10073</link>
      <description>arXiv:2509.10073v1 Announce Type: new 
Abstract: Survival analysis is a statistical framework for modeling time-to-event data, particularly valuable in healthcare for predicting outcomes like patient discharge or recurrence. This study implements and compares several survival models - including Weibull, Weibull AFT, Weibull AFT with Gamma Frailty, Cox Proportional Hazards (CoxPH), Random Survival Forest (RSF), and DeepSurv - using a publicly available breast cancer dataset. This study aims to benchmark classical, machine learning, and Bayesian survival models in terms of their predictive performance, interpretability, and suitability for clinical deployment. The models are evaluated using performance metrics such as the Concordance Index (C-index) and the Root Mean Squared Error (RMSE). DeepSurv showed the highest predictive performance, while interpretable models like RSF and Weibull AFT with Gamma Frailty offered competitive results. We also explored the implementation of statistical models from a Bayesian perspective, including frailty models, due to their ability to properly quantify uncertainty. Notably, frailty models are not readily available in standard survival analysis libraries, necessitating custom implementation. Our results demonstrate that interpretable statistical models, when correctly implemented using parameters that are effectively estimated using a Bayesian approach, can perform competitively with modern black-box models. These findings illustrate the trade-offs between model complexity, interpretability, and predictive power, highlighting the potential of Bayesian survival models in clinical decision-making settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10073v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Irving G\'omez-M\'endez, Sivakorn Phromsiri, Ittiphat Kijpaisansak, Settawut Chaithurdthum</dc:creator>
    </item>
    <item>
      <title>Spatial Modeling and Risk Zoning of Global Extreme Precipitation via Graph Neural Networks and r-Pareto Processes</title>
      <link>https://arxiv.org/abs/2509.10362</link>
      <description>arXiv:2509.10362v1 Announce Type: new 
Abstract: Extreme precipitation events occurring over large spatial domains pose substantial threats to societies because they can trigger compound flooding, landslides, and infrastructure failures across wide areas. A hybrid framework for spatial extreme precipitation modeling and risk zoning is proposed that integrates graph neural networks with r-Pareto processes (GNN-rP). Unlike traditional statistical spatial extremes models, this approach learns nonlinear, nonstationary dependence structures from precipitation-derived spatial graphs and applies a data-driven tail functional to model joint exceedances in a low-dimensional embedding space. Using NASA's IMERG observations (2000-2021) and CMIP6 SSP5-8.5 projections, the framework delineates coherent high-risk zones, quantifies their temporal persistence, and detects emerging hotspots under climate change. Compared with two baseline approaches, the GNN-rP pipeline substantially improves pointwise detection of high-risk grid cells while yielding comparable clustering stability. Results highlight persistent high-risk regions in the tropical belt, especially monsoon and convective zones, and reveal decadal-scale persistence that is punctuated by episodic reconfigurations under high-emission scenarios. By coupling machine learning with extreme value theory, GNN-rP offers a scalable, interpretable tool for adaptive climate risk zoning, with direct applications in infrastructure planning, disaster preparedness, and climate-resilient policy design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10362v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zimu Wang, Yifan Wu, Daning Bi</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimum Warranty Region for Right Censored Two Dimensional Dependent Data</title>
      <link>https://arxiv.org/abs/2509.10421</link>
      <description>arXiv:2509.10421v1 Announce Type: new 
Abstract: Warranty policies play a crucial role in balancing customer satisfaction and cost of the manufacturer. Traditional one-dimensional warranty frameworks, based solely on either age or usage, often fail to capture the joint effect of product life factors. This article investigates two-dimensional warranty policies by combining Free Replacement Warranty, Pro-Rata Warranty, and Combination FRW-PRW Warranty schemes across both age and usage scales. A dissatisfaction cost function is proposed alongside the economic benefit and warranty cost functions, and the expected utility framework is employed to derive optimal warranty parameters. The expectation is taken with respect to the posterior predictive distribution of product lifetime and usage data, ensuring a data-driven approach. Finally, the methodology is validated using an open-source dataset, and a new two-dimensional starter motor dataset is introduced to demonstrate the practical advantages of adopting two-dimensional warranty policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10421v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tanmay Sen, Rathin Das, Ritwik Bhattacharya</dc:creator>
    </item>
    <item>
      <title>A meta-analysis on the performance of machine-learning based language models for sentiment analysis</title>
      <link>https://arxiv.org/abs/2509.09728</link>
      <description>arXiv:2509.09728v1 Announce Type: cross 
Abstract: This paper presents a meta-analysis evaluating ML performance in sentiment analysis for Twitter data. The study aims to estimate the average performance, assess heterogeneity between and within studies, and analyze how study characteristics influence model performance. Using PRISMA guidelines, we searched academic databases and selected 195 trials from 20 studies with 12 study features. Overall accuracy, the most reported performance metric, was analyzed using double arcsine transformation and a three-level random effects model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76, 0.84]. This paper provides two key insights: 1) Overall accuracy is widely used but often misleading due to its sensitivity to class imbalance and the number of sentiment classes, highlighting the need for normalization. 2) Standardized reporting of model performance, including reporting confusion matrices for independent test sets, is essential for reliable comparisons of ML classifiers across studies, which seems far from common practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09728v1</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elena Rohde, Jonas Klingwort, Christian Borgs</dc:creator>
    </item>
    <item>
      <title>Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management</title>
      <link>https://arxiv.org/abs/2509.09772</link>
      <description>arXiv:2509.09772v1 Announce Type: cross 
Abstract: Population health management programs for Medicaid populations coordinate longitudinal outreach and services (e.g., benefits navigation, behavioral health, social needs support, and clinical scheduling) and must be safe, fair, and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement Learning (HACO) framework that separates risk calibration from preference optimization to generate conservative action recommendations at scale. In our setting, each step involves choosing among common coordination actions (e.g., which member to contact, by which modality, and whether to route to a specialized service) while controlling the near-term risk of adverse utilization events (e.g., unplanned emergency department visits or hospitalizations). Using a de-identified operational dataset from Waymark comprising 2.77 million sequential decisions across 168,126 patients, HACO (i) trains a lightweight risk model for adverse events, (ii) derives a conformal threshold to mask unsafe actions at a target risk level, and (iii) learns a preference policy on the resulting safe subset. We evaluate policies with a version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit subgroup performance across age, sex, and race. HACO achieves strong risk discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at {\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses reveal systematic differences in estimated value across demographics, underscoring the importance of fairness auditing. Our results show that conformal risk gating integrates cleanly with offline RL to deliver conservative, auditable decision support for population health management teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09772v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanjay Basu, Sadiq Y. Patel, Parth Sheth, Bhairavi Muralidharan, Namrata Elamaran, Aakriti Kinra, Rajaie Batniji</dc:creator>
    </item>
    <item>
      <title>A sampling method based on highest density regions: Applications to surrogate models for rare events estimation</title>
      <link>https://arxiv.org/abs/2509.10149</link>
      <description>arXiv:2509.10149v1 Announce Type: cross 
Abstract: This paper introduces a practical sampling method for training surrogate models in the context of uncertainty propagation. We propose a heuristic method to uniformly draw samples within highest density regions of the density given by the random vector describing the uncertainty of the model parameters. The resulting experimental design aims to provide a better approximation of the underlying true model compared to the cases where experimental designs have been drawn according to the distribution of the random vector itself. To assess the quality of our approach, three error metrics are considered: The first is the leave-one-out error, the second the relative mean square error and the third is the error generated by the surrogate model when estimating the probability of failure of the system compared to its reference value. The highest density region-based designs are shown to globally outperform the random vector-based designs both in terms of relative mean square error as well as in estimating the probability of failure. The proposed method is applicable within a black-box context and is compatible with existing uncertainty quantification frameworks for low dimensional and moderately correlated inputs. It may thus be useful in case of reliability problems, Bayesian inverse analysis, or whenever the surrogate model is used in a predictor mode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10149v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jocelyn Minini, Micha Wasem</dc:creator>
    </item>
    <item>
      <title>The effects of retraining on the stability of global models in retail demand forecasting</title>
      <link>https://arxiv.org/abs/2506.05776</link>
      <description>arXiv:2506.05776v2 Announce Type: replace 
Abstract: Forecast stability, that is, the consistency of predictions over time, is essential in business settings where sudden shifts in forecasts can disrupt planning and erode trust in predictive systems. Despite its importance, stability is often overlooked in favor of accuracy. In this study, we evaluate the stability of point and probabilistic forecasts across several retraining scenarios using two large retail demand datasets (M5 and VN1) and ten different global forecasting models. To analyze stability in the probabilistic setting, we propose a new model-agnostic, distribution-free, and scale-free metric that measures probabilistic instability: the Scaled Multi-Quantile Change (SMQC). Furthermore, we also evaluate the effects of retraining on various ensemble configurations based on forecast pooling. The results show that, compared to continuous retraining, less frequent retraining not only preserves but often improves forecast stability, challenging the need for continuous retraining. The study promotes a shift toward stability-aware forecasting practices, proposing a new tool to effectively evaluate forecast stability in probabilistic settings, and offering practical guidelines for building more robust prediction systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05776v2</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Zanotti</dc:creator>
    </item>
    <item>
      <title>DRtool: An Interactive Tool for Analyzing High-Dimensional Clusterings</title>
      <link>https://arxiv.org/abs/2509.04603</link>
      <description>arXiv:2509.04603v2 Announce Type: replace 
Abstract: Technological advances have spurred an increase in data complexity and dimensionality. We are now in an era in which data sets containing thousands of features are commonplace. To digest and analyze such high-dimensional data, dimension reduction techniques have been developed and advanced along with computational power. Of these techniques, nonlinear methods are most commonly employed because of their ability to construct visually interpretable embeddings. Unlike linear methods, these methods non-uniformly stretch and shrink space to create a visual impression of the high-dimensional data. Since capturing high-dimensional structures in a significantly lower number of dimensions requires drastic manipulation of space, nonlinear dimension reduction methods are known to occasionally produce false structures, especially in noisy settings. In an effort to deal with this phenomenon, we developed an interactive tool that enables analysts to better understand and diagnose their dimension reduction results. It uses various analytical plots to provide a multi-faceted perspective on results to determine legitimacy. The tool is available via an R package named DRtool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04603v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Lin, Julia Fukuyama</dc:creator>
    </item>
    <item>
      <title>Integrative Variational Autoencoders for Generative Modeling of an Image Outcome with Multiple Input Images</title>
      <link>https://arxiv.org/abs/2402.02734</link>
      <description>arXiv:2402.02734v2 Announce Type: replace-cross 
Abstract: Understanding relationships across multiple imaging modalities is central to neuroimaging research. We introduce the Integrative Variational Autoencoder (InVA), the first hierarchical VAE framework for image-on-image regression in multimodal neuroimaging. Unlike standard VAEs, which are not designed for predictive integration across modalities, InVA models outcome images as functions of both shared and modality-specific features. This flexible, data-driven approach avoids rigid assumptions of classical tensor regression and outperforms conventional VAEs and nonlinear models such as BART. As a key application, InVA accurately predicts costly PET scans from structural MRI, offering an efficient and powerful tool for multimodal neuroimaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02734v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bowen Lei, Yeseul Jeon, Rajarshi Guhaniyogi, Aaron Scheffler, Bani Mallick, Alzheimer's Disease Neuroimaging Initiatives</dc:creator>
    </item>
    <item>
      <title>Zero-inflation in the Multivariate Poisson Lognormal Family</title>
      <link>https://arxiv.org/abs/2405.14711</link>
      <description>arXiv:2405.14711v2 Announce Type: replace-cross 
Abstract: Analyzing high-dimensional count data is a challenge and statistical model-based approaches provide an adequate and efficient framework that preserves explainability. The (multivariate) Poisson-Log-Normal (PLN) model is one such model: it assumes count data are driven by an underlying structured latent Gaussian variable, so that the dependencies between counts solely stems from the latent dependencies. However PLN doesn't account for zero-inflation, a feature frequently observed in real-world datasets. Here we introduce the Zero-Inflated PLN (ZIPLN) model, adding a multivariate zero-inflated component to the model, as an additional Bernoulli latent variable. The Zero-Inflation can be fixed, site-specific, feature-specific or depends on covariates. We estimate model parameters using variational inference that scales up to datasets with a few thousands variables and compare two approximations: (i) independent Gaussian and Bernoulli variational distributions or (ii) Gaussian variational distribution conditioned on the Bernoulli one. The method is assessed on synthetic data and the efficiency of ZIPLN is established even when zero-inflation concerns up to 90% of the observed counts. We then apply both ZIPLN and PLN to a cow microbiome dataset, containing 90.6% of zeroes. Accounting for zero-inflation significantly increases log-likelihood and reduces dispersion in the latent space, thus leading to improved group discrimination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14711v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bastien Batardi\`ere, Julien Chiquet, Fran\c{c}ois Gindraud, Mahendra Mariadassou</dc:creator>
    </item>
    <item>
      <title>Deep Survival Analysis from Adult and Pediatric Electrocardiograms: A Multi-center Benchmark Study</title>
      <link>https://arxiv.org/abs/2406.17002</link>
      <description>arXiv:2406.17002v4 Announce Type: replace-cross 
Abstract: Artificial intelligence applied to electrocardiography (AI-ECG) shows potential for mortality prediction, but heterogeneous approaches and private datasets have limited generalizable insights. To address this, we systematically evaluated model design choices across three large cohorts: Beth Israel Deaconess (MIMIC-IV: n = 795,546 ECGs, United States), Telehealth Network of Minas Gerais (Code-15: n = 345,779, Brazil), and Boston Children's Hospital (BCH: n = 255,379, United States). We evaluated models predicting all-cause mortality, comparing horizon-based classification and deep survival methods with neural architectures including convolutional networks and transformers, benchmarking against demographic-only and gradient boosting baselines. Top models performed well (median concordance: Code-15, 0.83; MIMIC-IV, 0.78; BCH, 0.81). Incorporating age and sex improved performance across all datasets. Classifier-Cox models showed site-dependent sensitivity to horizon choice (median Pearson's R: Code-15, 0.35; MIMIC-IV, -0.71; BCH, 0.37). External validation reduced concordance, and in some cases demographic-only models outperformed externally trained AI-ECG models on Code-15. However, models trained on multi-site data outperformed site-specific models by 5-22%. Findings highlight factors for robust AI-ECG deployment: deep survival methods outperformed horizon-based classifiers, demographic covariates improved predictive performance, classifier-based models required site-specific calibration, and cross-cohort training, even between adult and pediatric cohorts, substantially improved performance. These results emphasize the importance of model type, demographics, and training diversity in developing AI-ECG models reliably applicable across populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17002v4</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Platon Lukyanenko, Joshua Mayourian, Mingxuan Liu, John K. Triedman, Sunil J. Ghelani, William G. La Cava</dc:creator>
    </item>
    <item>
      <title>Priors from Envisioned Posterior Judgments: A Novel Elicitation Approach With Application to Bayesian Clinical Trials</title>
      <link>https://arxiv.org/abs/2409.05271</link>
      <description>arXiv:2409.05271v3 Announce Type: replace-cross 
Abstract: Background: The uptake of formalized prior elicitation from experts in Bayesian clinical trials has been limited due to challenges such as complex statistical modeling, lack of practical tools, and the cognitive burden placed on experts arising from needing to quantify their uncertainty probabilistically. Existing methods also fail to address prior-posterior coherence, i.e., how do we ensure that the posterior distribution, obtained mathematically from combining the estimated prior with the trial data, reflects the expert's actual posterior beliefs?
  Method: In this study, we propose a new elicitation approach that effectuates prior-posterior coherence and reduces cognitive burden. This is achieved by eliciting expert responses, comprising point estimates only, about envisioned posterior judgments under various data outcomes and inferring the prior distribution by minimizing discrepancies between these responses and expected responses derived from the posterior distribution. Via an iterative process, experts receive feedback on the degree of coherency of their responses, and are invited to revise their responses to achieve greater coherency. The feasibility and potential value of this new approach are illustrated through an application to an ongoing trial.
  Results: We involved 10 experts from Walk 'n watch trial research team. Experts were presented with 16 hypothetical outcome scenarios to experts and elicit the priors followed by the developed elicitation framework. Following two rounds of elicitation, experts' judgments showed substantial improvement in coherency, demonstrating the practical applicability of the proposed elicitation approach.
  Conclusion: The proposed method provides a practical solution to the challenges of formalized prior elicitation in Bayesian clinical trials by addressing prior-posterior coherence and reducing cognitive demands on experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05271v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yongdong Ouyang, Janice J Eng, Denghuang Zhan, Hubert Wong, The WnW Research Team</dc:creator>
    </item>
    <item>
      <title>Faster Random Walk-based Capacitance Extraction with Generalized Antithetic Sampling</title>
      <link>https://arxiv.org/abs/2504.20586</link>
      <description>arXiv:2504.20586v3 Announce Type: replace-cross 
Abstract: Floating random walk-based capacitance extraction has emerged in recent years as a tried and true approach for extracting parasitic capacitance in very large scale integrated circuits. Being a Monte Carlo method, its performance is dependent on the variance of sampled quantities and variance reduction methods are crucial for the challenges posed by ever denser process technologies and layout-dependent effects. In this work, we present a novel, universal variance reduction method for floating random walk-based capacitance extraction, which is conceptually simple, highly efficient and provably reduces variance in all extractions, especially when layout-dependent effects are present. It is complementary to existing mathematical formulations for variance reduction and its performance gains are experienced on top of theirs. Numerical experiments demonstrate substantial such gains of up to 50% in number of walks necessary as well as in actual extraction times compared to the best previously proposed variance reduction approaches for the floating random-walk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20586v3</guid>
      <category>physics.comp-ph</category>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Periklis Liaskovitis, Marios Visvardis, Efthymios Efstathiou</dc:creator>
    </item>
  </channel>
</rss>
