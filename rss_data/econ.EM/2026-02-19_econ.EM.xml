<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Feb 2026 02:50:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Two-way Clustering Robust Variance Estimator in Quantile Regression Models</title>
      <link>https://arxiv.org/abs/2602.16376</link>
      <description>arXiv:2602.16376v1 Announce Type: new 
Abstract: We study inference for linear quantile regression with two-way clustered data. Using a separately exchangeable array framework and a projection decomposition of the quantile score, we characterize regime-dependent convergence rates and establish a self-normalized Gaussian approximation. We propose a two-way cluster-robust sandwich variance estimator with a kernel-based density ``bread'' and a projection-matched ``meat'', and prove consistency and validity of inference in Gaussian regimes. We also show an impossibility result for uniform inference in a non-Gaussian interaction regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16376v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ulrich Hounyo, Jiahao Lin</dc:creator>
    </item>
    <item>
      <title>Model selection confidence sets for time series models with applications to electricity load data</title>
      <link>https://arxiv.org/abs/2602.16527</link>
      <description>arXiv:2602.16527v1 Announce Type: new 
Abstract: This paper studies the Model Selection Confidence Set (MSCS) methodology for univariate time series models involving autoregressive and moving average components, and applies it to study model selection uncertainty in the Italian electricity load data. Rather than relying on a single model selected by an arbitrary criterion, the MSCS identifies a set of models that are statistically indistinguishable from the true data-generating process at a given confidence level. The size and composition of this set reveal crucial information about model selection uncertainty: noisy data scenarios produce larger sets with many candidate models, while more informative cases narrow the set considerably. To study the importance of each model term, we consider numerical statistics measuring the frequency with which each term is included in both the entire MSCS and in Lower Boundary Models (LBM), its most parsimonious specifications. Applied to Italian hourly electricity load data, the MSCS methodology reveals marked intraday variation in model selection uncertainty and isolates a collection of model specifications that deliver competitive short-term forecasts while highlighting key drivers of electricity load like intraday hourly lags, temperature, calendar effects and solar energy generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16527v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Piersilvio De Bortoli, Davide Ferrari, Francesco Ravazzolo, Luca Rossini</dc:creator>
    </item>
    <item>
      <title>Partial Identification under Missing Data Using Weak Shadow Variables from Pretrained Models</title>
      <link>https://arxiv.org/abs/2602.16061</link>
      <description>arXiv:2602.16061v1 Announce Type: cross 
Abstract: Estimating population quantities such as mean outcomes from user feedback is fundamental to platform evaluation and social science, yet feedback is often missing not at random (MNAR): users with stronger opinions are more likely to respond, so standard estimators are biased and the estimand is not identified without additional assumptions. Existing approaches typically rely on strong parametric assumptions or bespoke auxiliary variables that may be unavailable in practice. In this paper, we develop a partial identification framework in which sharp bounds on the estimand are obtained by solving a pair of linear programs whose constraints encode the observed data structure. This formulation naturally incorporates outcome predictions from pretrained models, including large language models (LLMs), as additional linear constraints that tighten the feasible set. We call these predictions weak shadow variables: they satisfy a conditional independence assumption with respect to missingness but need not meet the completeness conditions required by classical shadow-variable methods. When predictions are sufficiently informative, the bounds collapse to a point, recovering standard identification as a special case. In finite samples, to provide valid coverage of the identified set, we propose a set-expansion estimator that achieves slower-than-$\sqrt{n}$ convergence rate in the set-identified regime and the standard $\sqrt{n}$ rate under point identification. In simulations and semi-synthetic experiments on customer-service dialogues, we find that LLM predictions are often ill-conditioned for classical shadow-variable methods yet remain highly effective in our framework. They shrink identification intervals by 75--83\% while maintaining valid coverage under realistic MNAR mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16061v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongyu Chen, David Simchi-Levi, Ruoxuan Xiong</dc:creator>
    </item>
    <item>
      <title>Introducing the b-value: combining unbiased and biased estimators from a sensitivity analysis perspective</title>
      <link>https://arxiv.org/abs/2602.16310</link>
      <description>arXiv:2602.16310v1 Announce Type: cross 
Abstract: In empirical research, when we have multiple estimators for the same parameter of interest, a central question arises: how do we combine unbiased but less precise estimators with biased but more precise ones to improve the inference? Under this setting, the point estimation problem has attracted considerable attention. In this paper, we focus on a less studied inference question: how can we conduct valid statistical inference in such settings with unknown bias? We propose a strategy to combine unbiased and biased estimators from a sensitivity analysis perspective. We derive a sequence of confidence intervals indexed by the magnitude of the bias, which enable researchers to assess how conclusions vary with the bias levels. Importantly, we introduce the notion of the b-value, a critical value of the unknown maximum relative bias at which combining estimators does not yield a significant result. We apply this strategy to three canonical combined estimators: the precision-weighted estimator, the pretest estimator, and the soft-thresholding estimator. For each estimator, we characterize the sequence of confidence intervals and determine the bias threshold at which the conclusion changes. Based on the theory, we recommend reporting the b-value based on the soft-thresholding estimator and its associated confidence intervals, which are robust to unknown bias and achieve the lowest worst-case risk among the alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16310v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhexiao Lin, Peter J. Bickel, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Logit-based alternatives to two-stage least squares</title>
      <link>https://arxiv.org/abs/2312.10333</link>
      <description>arXiv:2312.10333v2 Announce Type: replace 
Abstract: We propose logit-based IV and augmented logit-based IV estimators that serve as alternatives to the traditionally used 2SLS estimator in the model where both the endogenous treatment variable and the corresponding instrument are binary. Our novel estimators are as easy to compute as the 2SLS estimator but have an advantage over the 2SLS estimator in terms of causal interpretability. In particular, in certain cases where the probability limits of both our estimators and the 2SLS estimator take the form of weighted-average treatment effects, our estimators are guaranteed to yield non-negative weights whereas the 2SLS estimator is not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10333v2</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Denis Chetverikov, Jinyong Hahn, Zhipeng Liao, Shuyang Sheng</dc:creator>
    </item>
    <item>
      <title>Factor multivariate stochastic volatility models of high dimension</title>
      <link>https://arxiv.org/abs/2406.19033</link>
      <description>arXiv:2406.19033v2 Announce Type: replace 
Abstract: Building upon factor decomposition to overcome the curse of dimensionality inherent in multivariate volatility processes, we develop a factor model-based multivariate stochastic volatility (fMSV) framework. We propose a two-stage estimation procedure for the fMSV model: in the first stage, estimators of the factor model are obtained, and in the second stage, the MSV component is estimated using the estimated common factor variables. We derive the asymptotic properties of the estimators, taking into account the estimation of the factor variables. The prediction performances are illustrated by finite-sample simulation experiments and applications to portfolio allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19033v2</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Poignard, Manabu Asai</dc:creator>
    </item>
    <item>
      <title>Compositional difference-in-differences</title>
      <link>https://arxiv.org/abs/2510.11659</link>
      <description>arXiv:2510.11659v3 Announce Type: replace 
Abstract: Many policy evaluations involve vectors of category-specific quantities, either categorical outcomes (e.g., employment type, major choice) or compositional measures (e.g., GDP by sector, votes by party, electricity generation by source). In these settings, both intensive margins (shares) and extensive margins (totals) can matter. However, existing Difference-in-Differences (DiD) strategies typically focus only on the shares and do not jointly identify treatment effects on totals. In addition, these approaches usually lack a clear economic interpretation. I develop Compositional Difference-in-Differences (CoDiD), a new framework that identifies treatment effects on both shares and totals in a coherent way. The key assumption is parallel growth: in the absence of treatment, the log-quantities of each category would have evolved in parallel for the treated and control groups. I show that, under a random-utility discrete-choice model, this condition is equivalent to parallel trends in expected utilities, meaning that the change in average latent attractiveness for each alternative is identical across groups. Furthermore, geometrically, the counterfactual distributions (shares) follow parallel trajectories in the probability simplex. In settings with multiple time periods, I introduce a relaxation that delivers bounds when parallel growth may not hold. I illustrate the empirical relevance of the method by examining how early voting reforms affected the 2008 U.S. election.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11659v3</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onil Boussim</dc:creator>
    </item>
  </channel>
</rss>
