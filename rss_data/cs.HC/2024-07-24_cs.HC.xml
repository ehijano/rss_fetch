<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Jul 2024 01:38:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Future of Home-living: Designing Smart Spaces for Modern Domestic Life</title>
      <link>https://arxiv.org/abs/2407.15956</link>
      <description>arXiv:2407.15956v1 Announce Type: new 
Abstract: The evolution of smart home technologies, particularly agentic ones such as conversational agents, robots, and virtual avatars, is reshaping our understanding of home and domestic life. This shift highlights the complexities of modern domestic life, with the household landscape now featuring diverse cohabiting units like co-housing and communal living arrangements. These agentic technologies present specific design challenges and opportunities as they become integrated into everyday routines and activities. Our workshop envisions smart homes as dynamic, user-shaped spaces, focusing on the integration of these technologies into daily life. We aim to explore how these technologies transform household dynamics, especially through boundary fluidity, by uniting researchers and practitioners from fields such as design, sociology, and ethnography. Together, we will develop a taxonomy of challenges and opportunities, providing a structured perspective on the integration of agentic technologies and their impact on contemporary living arrangements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15956v1</guid>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.48340/ecscw2024_ws02</arxiv:DOI>
      <arxiv:journal_reference>ECSCW 2024: the 22nd European Conference on Computer-Supported Cooperative Work</arxiv:journal_reference>
      <dc:creator>Fatemeh Alizadeh, Dave Randall, Peter Tolmie, Minha Lee, Yuhui Xu, Sarah Mennicken, Miko{\l}aj P. Wo\'zniak, Dennis Paul, Dominik Pins</dc:creator>
    </item>
    <item>
      <title>"It's a Good Idea to Put It Into Words": Writing `Rudders' in the Initial Stages of Visualization Design</title>
      <link>https://arxiv.org/abs/2407.15959</link>
      <description>arXiv:2407.15959v1 Announce Type: new 
Abstract: Written language is a useful tool for non-visual creative activities like writing essays and planning searches. This paper investigates the integration of written language in to the visualization design process. We create the idea of a 'writing rudder,' which acts as a guiding force or strategy for the design. Via an interview study of 24 working visualization designers, we first established that only a minority of participants systematically use writing to aid in design. A second study with 15 visualization designers examined four different variants of written rudders: asking questions, stating conclusions, composing a narrative, and writing titles. Overall, participants had a positive reaction; designers recognized the benefits of explicitly writing down components of the design and indicated that they would use this approach in future design work. More specifically, two approaches - writing questions and writing conclusions/takeaways - were seen as beneficial across the design process, while writing narratives showed promise mainly for the creation stage. Although concerns around potential bias during data exploration were raised, participants also discussed strategies to mitigate such concerns. This paper contributes to a deeper understanding of the interplay between language and visualization, and proposes a straightforward, lightweight addition to the visualization design process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15959v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chase Stokes, Clara Hu, Marti A. Hearst</dc:creator>
    </item>
    <item>
      <title>Color and Sentiment: A Study of Emotion-Based Color Palettes in Marketing</title>
      <link>https://arxiv.org/abs/2407.16064</link>
      <description>arXiv:2407.16064v1 Announce Type: new 
Abstract: It's widely recognized that the colors used in branding significantly impact how a brand is perceived. This research explores the influence of color in logos on consumer perception and emotional response. We investigate the associations between color usage and emotional responses in food and beverage marketing. Using a dataset of 644 companies, we analyzed the dominant colors in brand logos using k-means clustering to develop distinct color palettes. Concurrently, we extracted customer sentiments and emotions from Google Maps reviews of these companies (n=30,069), categorizing them into five primary emotions: Happiness, Anger, Sadness, Fear, and Surprise. These emotional responses were further categorized into four intensity levels: Low, Medium, Strong, and Very Strong, using a fuzzy sets approach. Our methodology involved correlating specific color palettes with the predominant emotional reactions associated with each brand. By merging the color palettes of companies that elicited similar emotional responses, we identified unique color palettes corresponding to each emotional category. Our findings suggest that among the food companies analyzed, the dominant emotion was Happiness, with no instances of Anger. The colors red and gray were prevalent across all emotional categories, indicating their importance in branding. Specific color-emotion correlations confirmed by our research include associations of yellow with Happiness, blue with Sadness, and bright colors with Surprise. This study highlights the critical role of color in shaping consumer attitudes. The study findings have practical implications for brand designers in the food industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16064v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maksat Shagyrov, Pakizar Shamoi</dc:creator>
    </item>
    <item>
      <title>Cluster Haptic Texture Database: Haptic Texture Database with Variety in Velocity and Direction of Sliding Contacts</title>
      <link>https://arxiv.org/abs/2407.16206</link>
      <description>arXiv:2407.16206v1 Announce Type: new 
Abstract: Human perception integrates multisensory information, with tactile perception playing a key role in object and surface recognition. While human-machine interfaces with haptic modalities offer enhanced system performance, existing datasets focus primarily on visual data, overlooking comprehensive haptic information. Previous haptic texture databases have recorded sound and acceleration signals, but often ignore the nuanced differences between probe-texture and finger-texture interactions. Recognizing this shortcoming, we present the Cluster Haptic Texture Database, a multimodal dataset that records visual, auditory, and haptic signals from an artificial urethane rubber fingertip interacting with different textured surfaces. This database, designed to mimic the properties of the human finger, includes five velocity levels and eight directional variations, providing a comprehensive study of tactile interactions. Our evaluations reveal the effectiveness of classifiers trained on this dataset in identifying surfaces, and the subtleties of estimating velocity and direction for each surface.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16206v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michikuni Eguchi, Madoka Ito, Tomohiro Hayase, Yuichi Hiroi, Takefumi Hiraki</dc:creator>
    </item>
    <item>
      <title>Video Popularity in Social Media: Impact of Emotions, Raw Features and Viewer Comments</title>
      <link>https://arxiv.org/abs/2407.16272</link>
      <description>arXiv:2407.16272v1 Announce Type: new 
Abstract: The Internet has significantly affected the increase of social media users. Nowadays, informative content is presented along with entertainment on the web. Highlighting environmental issues on social networks is crucial, given their significance as major global problems. This study examines the popularity determinants for short environmental videos on social media, focusing on the comparative influence of raw video features and viewer engagement metrics. We collected a dataset of videos along with associated popularity metrics such as likes, views, shares, and comments per day. We also extracted video characteristics, including duration, text post length, emotional and sentiment analysis using the VADER and text2emotion models, and color palette brightness. Our analysis consisted of two main experiments: one evaluating the correlation between raw video features and popularity metrics and another assessing the impact of viewer comments and their sentiments and emotions on video popularity. We employed a ridge regression classifier with standard scaling to predict the popularity, categorizing videos as popular or not based on the median views and likes per day. The findings reveal that viewer comments and reactions (accuracy of 0.8) have a more substantial influence on video popularity compared to raw video features (accuracy of 0.67). Significant correlations include a positive relationship between the emotion of sadness in posts and the number of likes and negative correlations between sentiment scores, and both likes and shares. This research highlights the complex relationship between content features and public perception in shaping the popularity of environmental messages on social media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16272v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malika Ziyada, Pakizar Shamoi</dc:creator>
    </item>
    <item>
      <title>Offsetting Perceptual Bias in Visual Clustering: The Role of Point Size Adjustment in Variable Display Sizes</title>
      <link>https://arxiv.org/abs/2407.16322</link>
      <description>arXiv:2407.16322v1 Announce Type: new 
Abstract: Scatterplots are frequently shared across different displays in collaborative and communicative visual analytics. However, variations in displays diversify scatterplot sizes. Such variations can influence the perception of clustering patterns, introducing potential biases leading to misinterpretations in cluster analysis. In this research, we explore how scatterplot size affects cluster assignment and investigate how we can offset such bias. We first conduct a controlled study asking participants to perform visual clustering on scatterplots of varying sizes. We found that changes in scatterplot size significantly alter cluster perception in three key features. In our subsequent experiment, we examine how adjusting point sizes can mitigate this bias. As a result, we verify that adjusting point size can effectively counteract the perceptual biases caused by varying scatterplot sizes. We wrap up our research by discussing the necessity and applicability of our findings in realworld applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16322v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taehyun Yang, Hyeon Jeon, Jinwook Seo</dc:creator>
    </item>
    <item>
      <title>PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets</title>
      <link>https://arxiv.org/abs/2407.16329</link>
      <description>arXiv:2407.16329v1 Announce Type: new 
Abstract: Acute stroke demands prompt diagnosis and treatment to achieve optimal patient outcomes. However, the intricate and irregular nature of clinical data associated with acute stroke, particularly blood pressure (BP) measurements, presents substantial obstacles to effective visual analytics and decision-making. Through a year-long collaboration with experienced neurologists, we developed PhenoFlow, a visual analytics system that leverages the collaboration between human and Large Language Models (LLMs) to analyze the extensive and complex data of acute ischemic stroke patients. PhenoFlow pioneers an innovative workflow, where the LLM serves as a data wrangler while neurologists explore and supervise the output using visualizations and natural language interactions. This approach enables neurologists to focus more on decision-making with reduced cognitive load. To protect sensitive patient information, PhenoFlow only utilizes metadata to make inferences and synthesize executable codes, without accessing raw patient data. This ensures that the results are both reproducible and interpretable while maintaining patient privacy. The system incorporates a slice-and-wrap design that employs temporal folding to create an overlaid circular visualization. Combined with a linear bar graph, this design aids in exploring meaningful patterns within irregularly measured BP data. Through case studies, PhenoFlow has demonstrated its capability to support iterative analysis of extensive clinical datasets, reducing cognitive load and enabling neurologists to make well-informed decisions. Grounded in long-term collaboration with domain experts, our research demonstrates the potential of utilizing LLMs to tackle current challenges in data-driven clinical decision-making for acute ischemic stroke patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16329v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaeyoung Kim, Sihyeon Lee, Hyeon Jeon, Keon-Joo Lee, Hee-Joon Bae, Bohyoung Kim, Jinwook Seo</dc:creator>
    </item>
    <item>
      <title>AutoLegend: A User Feedback-Driven Adaptive Legend Generator for Visualizations</title>
      <link>https://arxiv.org/abs/2407.16331</link>
      <description>arXiv:2407.16331v1 Announce Type: new 
Abstract: We propose AutoLegend to generate interactive visualization legends using online learning with user feedback. AutoLegend accurately extracts symbols and channels from visualizations and then generates quality legends. AutoLegend enables a two-way interaction between legends and interactions, including highlighting, filtering, data retrieval, and retargeting. After analyzing visualization legends from IEEE VIS papers over the past 20 years, we summarized the design space and evaluation metrics for legend design in visualizations, particularly charts. The generation process consists of three interrelated components: a legend search agent, a feedback model, and an adversarial loss model. The search agent determines suitable legend solutions by exploring the design space and receives guidance from the feedback model through scalar scores. The feedback model is continuously updated by the adversarial loss model based on user input. The user study revealed that AutoLegend can learn users' preferences through legend editing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16331v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Can Liu, Xiyao Mei, Zhibang Jiang, Shaocong Tan, Xiaoru Yuan</dc:creator>
    </item>
    <item>
      <title>Datasets of Visualization for Machine Learning</title>
      <link>https://arxiv.org/abs/2407.16351</link>
      <description>arXiv:2407.16351v1 Announce Type: new 
Abstract: Datasets of visualization play a crucial role in automating data-driven visualization pipelines, serving as the foundation for supervised model training and algorithm benchmarking. In this paper, we survey the literature on visualization datasets and provide a comprehensive overview of existing visualization datasets, including their data types, formats, supported tasks, and openness. We propose a what-why-how model for visualization datasets, considering the content of the dataset (what), the supported tasks (why), and the dataset construction process (how). This model provides a clear understanding of the diversity and complexity of visualization datasets. Additionally, we highlight the challenges faced by existing visualization datasets, including the lack of standardization in data types and formats and the limited availability of large-scale datasets. To address these challenges, we suggest future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16351v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Can Liu, Ruike Jiang, Shaocong Tan, Jiacheng Yu, Chaofan Yang, Hanning Shao, Xiaoru Yuan</dc:creator>
    </item>
    <item>
      <title>A Survey on Trustworthiness in Foundation Models for Medical Image Analysis</title>
      <link>https://arxiv.org/abs/2407.15851</link>
      <description>arXiv:2407.15851v1 Announce Type: cross 
Abstract: The rapid advancement of foundation models in medical imaging represents a significant leap toward enhancing diagnostic accuracy and personalized treatment. However, the deployment of foundation models in healthcare necessitates a rigorous examination of their trustworthiness, encompassing privacy, robustness, reliability, explainability, and fairness. The current body of survey literature on foundation models in medical imaging reveals considerable gaps, particularly in the area of trustworthiness. Additionally, extant surveys on the trustworthiness of foundation models fail to address their specific variations and applications within the medical imaging domain. This survey paper reviews the current research on foundation models in the major medical imaging applications, with a focus on segmentation, medical report generation, medical question and answering (Q&amp;A), and disease diagnosis, which includes trustworthiness discussion in their manuscripts. We explore the complex challenges of making foundation models for medical image analysis trustworthy, associated with each application, and summarize the current concerns and strategies to enhance trustworthiness. Furthermore, we explore the future promises of these models in revolutionizing patient care. Our analysis underscores the imperative for advancing towards trustworthy AI in medical image analysis, advocating for a balanced approach that fosters innovation while ensuring ethical and equitable healthcare delivery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15851v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Congzhen Shi, Ryan Rezai, Jiaxi Yang, Qi Dou, Xiaoxiao Li</dc:creator>
    </item>
    <item>
      <title>When, Where, and What? An Novel Benchmark for Accident Anticipation and Localization with Large Language Models</title>
      <link>https://arxiv.org/abs/2407.16277</link>
      <description>arXiv:2407.16277v1 Announce Type: cross 
Abstract: As autonomous driving systems increasingly become part of daily transportation, the ability to accurately anticipate and mitigate potential traffic accidents is paramount. Traditional accident anticipation models primarily utilizing dashcam videos are adept at predicting when an accident may occur but fall short in localizing the incident and identifying involved entities. Addressing this gap, this study introduces a novel framework that integrates Large Language Models (LLMs) to enhance predictive capabilities across multiple dimensions--what, when, and where accidents might occur. We develop an innovative chain-based attention mechanism that dynamically adjusts to prioritize high-risk elements within complex driving scenes. This mechanism is complemented by a three-stage model that processes outputs from smaller models into detailed multimodal inputs for LLMs, thus enabling a more nuanced understanding of traffic dynamics. Empirical validation on the DAD, CCD, and A3D datasets demonstrates superior performance in Average Precision (AP) and Mean Time-To-Accident (mTTA), establishing new benchmarks for accident prediction technology. Our approach not only advances the technological framework for autonomous driving safety but also enhances human-AI interaction, making predictive insights generated by autonomous systems more intuitive and actionable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16277v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haicheng Liao, Yongkang Li, Chengyue Wang, Yanchen Guan, KaHou Tam, Chunlin Tian, Li Li, Chengzhong Xu, Zhenning Li</dc:creator>
    </item>
    <item>
      <title>Improving multidimensional projection quality with user-specific metrics and optimal scaling</title>
      <link>https://arxiv.org/abs/2407.16328</link>
      <description>arXiv:2407.16328v1 Announce Type: cross 
Abstract: The growing prevalence of high-dimensional data has fostered the development of multidimensional projection (MP) techniques, such as t-SNE, UMAP, and LAMP, for data visualization and exploration. However, conventional MP methods typically employ generic quality metrics, neglecting individual user preferences. This study proposes a new framework that tailors MP techniques based on user-specific quality criteria, enhancing projection interpretability.
  Our approach combines three visual quality metrics, stress, neighborhood preservation, and silhouette score, to create a composite metric for a precise MP evaluation. We then optimize the projection scale by maximizing the composite metric value. We conducted an experiment involving two users with different projection preferences, generating projections using t-SNE, UMAP, and LAMP. Users rate projections according to their criteria, producing two training sets. We derive optimal weights for each set and apply them to other datasets to determine the best projections per user.
  Our findings demonstrate that personalized projections effectively capture user preferences, fostering better data exploration and enabling more informed decision-making. This user-centric approach promotes advancements in multidimensional projection techniques that accommodate diverse user preferences and enhance interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16328v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maniru Ibrahim</dc:creator>
    </item>
    <item>
      <title>Gender in video games: Creativity for inclusivity</title>
      <link>https://arxiv.org/abs/2407.16536</link>
      <description>arXiv:2407.16536v1 Announce Type: cross 
Abstract: &lt;p&gt;Video game localisation, a field highly impacted by the lack of visual environment and text linearity, forces translators to create inclusive solutions in terms of gender to overcome the hurdles created by variables. This paper will introduce the specificities of this sector and present an analysis of some of those techniques extracted from parallel corpora compiled from video games that include female, transgender, non-binary, and non-sexualised characters.\&amp;nbsp;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LE GENRE DANS LES JEUX VID\&amp;Eacute;O : LA CR\&amp;Eacute;ATIVIT\&amp;Eacute; POUR L\&amp;\#39;INCLUSIVIT\&amp;Eacute;\&amp;nbsp;&lt;/strong&gt; La localisation de jeux vid\&amp;eacute;o, un domaine tr\&amp;egrave;s impact\&amp;eacute; par l\&amp;rsquo;absence d\&amp;rsquo;acc\&amp;egrave;s au jeu ainsi que par le manque de lin\&amp;eacute;arit\&amp;eacute; textuelle, oblige les traducteurs \&amp;agrave; trouver des solutions inclusives pour faire face aux probl\&amp;egrave;mes cr\&amp;eacute;\&amp;eacute;s par les variables. Cet article se propose d\&amp;rsquo;introduire les caract\&amp;eacute;ristiques du secteur ainsi que de pr\&amp;eacute;senter l\&amp;rsquo;analyse des techniques extraites des corpus parall\&amp;egrave;les compil\&amp;eacute;s \&amp;agrave; partir des jeux ayant des personnages f\&amp;eacute;minins, transgenres, non-binaires, et non-sexualis\&amp;eacute;s. \&amp;nbsp;&lt;/p&gt;</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16536v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Isabel Rivas Ginel (TIL), Sarah Theroine (TIL, LIB)</dc:creator>
    </item>
    <item>
      <title>Knowledge-driven AI-generated data for accurate and interpretable breast ultrasound diagnoses</title>
      <link>https://arxiv.org/abs/2407.16634</link>
      <description>arXiv:2407.16634v1 Announce Type: cross 
Abstract: Data-driven deep learning models have shown great capabilities to assist radiologists in breast ultrasound (US) diagnoses. However, their effectiveness is limited by the long-tail distribution of training data, which leads to inaccuracies in rare cases. In this study, we address a long-standing challenge of improving the diagnostic model performance on rare cases using long-tailed data. Specifically, we introduce a pipeline, TAILOR, that builds a knowledge-driven generative model to produce tailored synthetic data. The generative model, using 3,749 lesions as source data, can generate millions of breast-US images, especially for error-prone rare cases. The generated data can be further used to build a diagnostic model for accurate and interpretable diagnoses. In the prospective external evaluation, our diagnostic model outperforms the average performance of nine radiologists by 33.5% in specificity with the same sensitivity, improving their performance by providing predictions with an interpretable decision-making process. Moreover, on ductal carcinoma in situ (DCIS), our diagnostic model outperforms all radiologists by a large margin, with only 34 DCIS lesions in the source data. We believe that TAILOR can potentially be extended to various diseases and imaging modalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16634v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haojun Yu, Youcheng Li, Nan Zhang, Zihan Niu, Xuantong Gong, Yanwen Luo, Quanlin Wu, Wangyan Qin, Mengyuan Zhou, Jie Han, Jia Tao, Ziwei Zhao, Di Dai, Di He, Dong Wang, Binghui Tang, Ling Huo, Qingli Zhu, Yong Wang, Liwei Wang</dc:creator>
    </item>
    <item>
      <title>Understanding Dynamic Human-Robot Proxemics in the Case of Four-Legged Canine-Inspired Robots</title>
      <link>https://arxiv.org/abs/2302.10729</link>
      <description>arXiv:2302.10729v2 Announce Type: replace 
Abstract: Recently, quadruped robots have been well developed with potential applications in different areas, such as care homes, hospitals, and other social areas. To ensure their integration in such social contexts, it is essential to understand people's proxemic preferences around such robots. In this paper, we designed a human-quadruped-robot interaction study (N = 32) to investigate the effect of 1) different facing orientations and 2) the gaze of a moving robot on human proxemic distance. Our work covers both static and dynamic interaction scenarios. We found a statistically significant effect of both the robot's facing direction and its gaze on preferred personal distances. The distances humans established towards certain robot behavioral modes reflect their attitudes, thereby guiding the design of future autonomous robots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10729v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiangmin Xu, Emma Liying Li, Mohamed Khamis, Guodong Zhao, Robin Bretin</dc:creator>
    </item>
    <item>
      <title>Agreeing and Disagreeing in Collaborative Knowledge Graph Construction: An Analysis of Wikidata</title>
      <link>https://arxiv.org/abs/2306.11766</link>
      <description>arXiv:2306.11766v2 Announce Type: replace 
Abstract: In this work, we study disagreement in discussions around Wikidata, an online knowledge community that builds the data backend of Wikipedia. Discussions are important in collaborative work as they can increase contributor performance and encourage the emergence of shared norms and practices. While disagreements can play a productive role in discussions, they can also lead to conflicts and controversies, which impact contributor well-being and their motivation to engage. We want to understand if and when such phenomena arise in Wikidata, using a mix of quantitative and qualitative analyses to identify the types of topics people disagree about, the most common patterns of interaction, and roles people play when arguing for or against an issue. We find that decisions to create Wikidata properties are much faster than those to delete properties and that more than half of controversial discussions do not lead to consensus. Our analysis suggests that Wikidata is an inclusive community, considering different opinions when making decisions, and that conflict and vandalism are rare in discussions. At the same time, while one-fourth of the editors participating in controversial discussions contribute with legit and insightful opinions about Wikidata's emerging issues, they do not remain engaged in the discussions. We hope our findings will help Wikidata support community decision making, and improve discussion tools and practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11766v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elisavet Koutsiana, Tushita Yadav, Nitisha Jain, Albert Mero\~no-Pe\~nuela, Elena Simperl</dc:creator>
    </item>
    <item>
      <title>GeoViz: A Multi-View Visualization Platform for Spatio-temporal Knowledge Graph</title>
      <link>https://arxiv.org/abs/2405.03697</link>
      <description>arXiv:2405.03697v2 Announce Type: replace 
Abstract: In this paper, we propose a multi-view visualization technology for spatio-temporal knowledge graph(STKG), which utilizes three distinct perspectives: knowledge tree, knowledge net, and knowledge map, to facilitate a comprehensive analysis of the STKG. The knowledge tree enables the visualization of hierarchical interrelation within the STKG, while the knowledge net elucidates semantic relationships among knowledge entities. Additionally, the knowledge map displays spatial and temporal distributions via spatial maps and time axes, respectively. Our visualization technology addresses the limitations inherent in single-view approaches and the deficiency of interaction in spatio-temporal perspectives evident in existing visualization methods. Moreover, we have encapsulated this technology within an integrated, open-source platform named GeoViz. A demo video of GeoViz can be accessed at https://github.com/JeremyChou28/GeoViz.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03697v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianping Zhou, Junhao Li, Guanjie Zheng, Yunqiang Zhu, Xinbing Wang, Chenghu Zhou</dc:creator>
    </item>
    <item>
      <title>Enabling Generative Design Tools with LLM Agents for Building Novel Devices: A Case Study on Fluidic Computation Interfaces</title>
      <link>https://arxiv.org/abs/2405.17837</link>
      <description>arXiv:2405.17837v2 Announce Type: replace 
Abstract: In the field of Human-Computer Interaction (HCI), the development of interactive devices represents a significant area of focus. The advent of novel hardware and advanced fabrication techniques has underscored the demand for specialized design tools that democratize the prototyping process for such cutting-edge devices. While these tools simplify the process through parametric design and simulation, they typically require a certain learning curve and often fall short in facilitating creative ideation. In this study, we employ fluidic computation interface as a case study to investigate the potential of augmenting design tools of physical devices with Large Language Model (LLM) agents. Enhanced by LLM agents, the Generative Design Tool (GDT) can comprehend the capabilities and limitations of newly developed devices; it can propose varied, insightful, and practical application scenarios, and recommend device designs that are technically and contextually appropriate. Furthermore, it generates the necessary design parameters for the traditional part of the design tool to visualize results and produce support files for fabrication. This paper outlines the GDT's framework, implementation, and performance, while also contemplating its prospects and the obstacles encountered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17837v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qiuyu Lu, Jiawei Fang, Zhihao Yao, Yue Yang, Shiqing Lyu, Haipeng Mi, Lining Yao</dc:creator>
    </item>
    <item>
      <title>Data Guards: Challenges and Solutions for Fostering Trust in Data</title>
      <link>https://arxiv.org/abs/2407.14042</link>
      <description>arXiv:2407.14042v2 Announce Type: replace 
Abstract: From dirty data to intentional deception, there are many threats to the validity of data-driven decisions. Making use of data, especially new or unfamiliar data, therefore requires a degree of trust or verification. How is this trust established? In this paper, we present the results of a series of interviews with both producers and consumers of data artifacts (outputs of data ecosystems like spreadsheets, charts, and dashboards) aimed at understanding strategies and obstacles to building trust in data. We find a recurring need, but lack of existing standards, for data validation and verification, especially among data consumers. We therefore propose a set of data guards: methods and tools for fostering trust in data artifacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14042v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicole Sultanum, Dennis Bromley, Michael Correll</dc:creator>
    </item>
    <item>
      <title>IDA: Breaking Barriers in No-code UI Automation Through Large Language Models and Human-Centric Design</title>
      <link>https://arxiv.org/abs/2407.15673</link>
      <description>arXiv:2407.15673v2 Announce Type: replace 
Abstract: Business users dedicate significant amounts of time to repetitive tasks within enterprise digital platforms, highlighting a critical need for automation. Despite advancements in low-code tools for UI automation, their complexity remains a significant barrier to adoption among non-technical business users. However, recent advancements in large language models (LLMs) have created new opportunities to overcome this barrier by offering more powerful, yet simpler and more human-centric programming environments. This paper presents IDA (Intelligent Digital Apprentice), a novel no-code Web UI automation tool designed specifically to empower business users with no technical background. IDA incorporates human-centric design principles, including guided programming by demonstration, semantic programming model, and teacher-student learning metaphor which is tailored to the skill set of business users. By leveraging LLMs, IDA overcomes some of the key technical barriers that have traditionally limited the possibility of no-code solutions. We have developed a prototype of IDA and conducted a user study involving real world business users and enterprise applications. The promising results indicate that users could effectively utilize IDA to create automation. The qualitative feedback indicates that IDA is perceived as user-friendly and trustworthy. This study contributes to unlocking the potential of AI assistants to enhance the productivity of business users through no-code user interface automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15673v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Segev Shlomov, Avi Yaeli, Sami Marreed, Sivan Schwartz, Netanel Eder, Offer Akrabi, Sergey Zeltyn</dc:creator>
    </item>
    <item>
      <title>DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery from Videos</title>
      <link>https://arxiv.org/abs/2303.13397</link>
      <description>arXiv:2303.13397v4 Announce Type: replace-cross 
Abstract: Human mesh recovery (HMR) provides rich human body information for various real-world applications. While image-based HMR methods have achieved impressive results, they often struggle to recover humans in dynamic scenarios, leading to temporal inconsistencies and non-smooth 3D motion predictions due to the absence of human motion. In contrast, video-based approaches leverage temporal information to mitigate this issue. In this paper, we present DiffMesh, an innovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh establishes a bridge between diffusion models and human motion, efficiently generating accurate and smooth output mesh sequences by incorporating human motion within the forward process and reverse process in the diffusion model. Extensive experiments are conducted on the widely used datasets (Human3.6M \cite{h36m_pami} and 3DPW \cite{pw3d2018}), which demonstrate the effectiveness and efficiency of our DiffMesh. Visual comparisons in real-world scenarios further highlight DiffMesh's suitability for practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.13397v4</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ce Zheng, Xianpeng Liu, Qucheng Peng, Tianfu Wu, Pu Wang, Chen Chen</dc:creator>
    </item>
    <item>
      <title>Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs</title>
      <link>https://arxiv.org/abs/2404.01461</link>
      <description>arXiv:2404.01461v4 Announce Type: replace-cross 
Abstract: Although large language models (LLMs) have demonstrated remarkable proficiency in modeling text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example, versus considering broader facts or statistical evidence. This research investigates the impact of the representativeness heuristic on LLM reasoning. We created ReHeAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to ReHeAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps are often incorrectly based on a stereotype rather than on the problem's description. Interestingly, the performance improves when adding a hint in the prompt to remind the model to use its knowledge. This suggests the uniqueness of the representativeness heuristic compared to traditional biases. It can occur even when LLMs possess the correct knowledge while falling into a cognitive trap. This highlights the importance of future research focusing on the representativeness heuristic in model reasoning and decision-making and on developing solutions to address it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01461v4</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pengda Wang, Zilin Xiao, Hanjie Chen, Frederick L. Oswald</dc:creator>
    </item>
    <item>
      <title>The Efficacy of Conversational Artificial Intelligence in Rectifying the Theory of Mind and Autonomy Biases: Comparative Analysis</title>
      <link>https://arxiv.org/abs/2406.13813</link>
      <description>arXiv:2406.13813v5 Announce Type: replace-cross 
Abstract: Background: The increasing deployment of Conversational Artificial Intelligence (CAI) in mental health interventions necessitates an evaluation of their efficacy in rectifying cognitive biases and recognizing affect in human-AI interactions. These biases, including theory of mind and autonomy biases, can exacerbate mental health conditions such as depression and anxiety.
  Objective: This study aimed to assess the effectiveness of therapeutic chatbots (Wysa, Youper) versus general-purpose language models (GPT-3.5, GPT-4, Gemini Pro) in identifying and rectifying cognitive biases and recognizing affect in user interactions.
  Methods: The study employed virtual case scenarios simulating typical user-bot interactions. Cognitive biases assessed included theory of mind biases (anthropomorphism, overtrust, attribution) and autonomy biases (illusion of control, fundamental attribution error, just-world hypothesis). Responses were evaluated on accuracy, therapeutic quality, and adherence to Cognitive Behavioral Therapy (CBT) principles, using an ordinal scale. The evaluation involved double review by cognitive scientists and a clinical psychologist.
  Results: The study revealed that general-purpose chatbots outperformed therapeutic chatbots in rectifying cognitive biases, particularly in overtrust bias, fundamental attribution error, and just-world hypothesis. GPT-4 achieved the highest scores across all biases, while therapeutic bots like Wysa scored the lowest. Affect recognition showed similar trends, with general-purpose bots outperforming therapeutic bots in four out of six biases. However, the results highlight the need for further refinement of therapeutic chatbots to enhance their efficacy and ensure safe, effective use in digital mental health interventions. Future research should focus on improving affective response and addressing ethical considerations in AI-based therapy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13813v5</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marcin Rz\k{a}deczka, Anna Sterna, Julia Stoli\'nska, Paulina Kaczy\'nska, Marcin Moskalewicz</dc:creator>
    </item>
    <item>
      <title>FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs</title>
      <link>https://arxiv.org/abs/2407.02157</link>
      <description>arXiv:2407.02157v2 Announce Type: replace-cross 
Abstract: Dynamic Facial Expression Recognition (DFER) is crucial for understanding human behavior. However, current methods exhibit limited performance mainly due to the scarcity of high-quality data, the insufficient utilization of facial dynamics, and the ambiguity of expression semantics, etc. To this end, we propose a novel framework, named Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs (FineCLIPER), incorporating the following novel designs: 1) To better distinguish between similar facial expressions, we extend the class labels to textual descriptions from both positive and negative aspects, and obtain supervision by calculating the cross-modal similarity based on the CLIP model; 2) Our FineCLIPER adopts a hierarchical manner to effectively mine useful cues from DFE videos. Specifically, besides directly embedding video frames as input (low semantic level), we propose to extract the face segmentation masks and landmarks based on each frame (middle semantic level) and utilize the Multi-modal Large Language Model (MLLM) to further generate detailed descriptions of facial changes across frames with designed prompts (high semantic level). Additionally, we also adopt Parameter-Efficient Fine-Tuning (PEFT) to enable efficient adaptation of large pre-trained models (i.e., CLIP) for this task. Our FineCLIPER achieves SOTA performance on the DFEW, FERV39k, and MAFW datasets in both supervised and zero-shot settings with few tunable parameters. Project Page: https://haroldchen19.github.io/FineCLIPER-Page/</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02157v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haodong Chen, Haojian Huang, Junhao Dong, Mingzhe Zheng, Dian Shao</dc:creator>
    </item>
  </channel>
</rss>
