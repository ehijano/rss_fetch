<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Feb 2025 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Taxonomy of Real Faults in Hybrid Quantum-Classical Architectures</title>
      <link>https://arxiv.org/abs/2502.08739</link>
      <description>arXiv:2502.08739v1 Announce Type: new 
Abstract: With the popularity of Hybrid Quantum-Classical architectures, particularly noisy intermediate-scale quantum (NISQ) architectures, comes the need for quality assurance methods tailored to their specific faults. In this study, we propose a taxonomy of faults in Hybrid Quantum-Classical architectures accompanied by a dataset of real faults in the identified categories. To achieve this, we empirically analysed open-source repositories for fixed faults. We analysed over 5000 closed issues on GitHub and pre-selected 529 of them based on rigorously defined inclusion criteria. We selected 133 faults that we labelled around symptoms and the origin of the faults. We cross-validated the classification and labels assigned to every fault between two of the authors. As a result, we introduced a taxonomy of real faults in Hybrid Quantum-Classical architectures. Subsequently, we validated the taxonomy through interviews conducted with eleven developers. The taxonomy was dynamically updated throughout the cross-validation and interview processes. The final version was validated and discussed through surveys conducted with an independent group of domain experts to ensure its relevance and to gain further insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08739v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avner Bensoussan, Gunel Jahangirova, Mohammad Reza Mousavi</dc:creator>
    </item>
    <item>
      <title>Microkernel-Based Web Architecture: Design &amp; Implementation Considerations</title>
      <link>https://arxiv.org/abs/2502.08802</link>
      <description>arXiv:2502.08802v1 Announce Type: new 
Abstract: In this vision paper I propose a middle-ground alternative between monolithic and microservice web architectures. After identifying the key challenges associated with microservice architectures, I revised the design of a microkernel-based web architecture, considering these challenges as well as recent architectural advancements. Next, I examined contemporary approaches to various self-* properties and explored how this new architecture could enhance them, including a modified version of the MAPE-K loop. Once the high-level design of the microkernel architecture was finalized, I evaluated its potential to address the identified challenges. Lastly, I reflected on several implementation aspects of the proposed work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08802v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Vick Dini</dc:creator>
    </item>
    <item>
      <title>CLOVER: A Test Case Generation Benchmark with Coverage, Long-Context, and Verification</title>
      <link>https://arxiv.org/abs/2502.08806</link>
      <description>arXiv:2502.08806v1 Announce Type: new 
Abstract: Software testing is a critical aspect of software development, yet generating test cases remains a routine task for engineers. This paper presents a benchmark, CLOVER, to evaluate models' capabilities in generating and completing test cases under specific conditions. Spanning from simple assertion completions to writing test cases that cover specific code blocks across multiple files, these tasks are based on 12 python repositories, analyzing 845 problems with context lengths ranging from 4k to 128k tokens. Utilizing code testing frameworks, we propose a method to construct retrieval contexts using coverage information. While models exhibit comparable performance with short contexts, notable differences emerge with 16k contexts. Notably, models like GPT-4o and Claude 3.5 can effectively leverage relevant snippets; however, all models score below 35\% on the complex Task III, even with the oracle context provided, underscoring the benchmark's significance and the potential for model improvement. The benchmark is containerized for code execution across tasks, and we will release the code, data, and construction methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08806v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiacheng Xu, Bo Pang, Jin Qu, Hiroaki Hayashi, Caiming Xiong, Yingbo Zhou</dc:creator>
    </item>
    <item>
      <title>Quantum Software Engineering and Potential of Quantum Computing in Software Engineering Research: A Review</title>
      <link>https://arxiv.org/abs/2502.08925</link>
      <description>arXiv:2502.08925v1 Announce Type: new 
Abstract: Research in software engineering is essential for improving development practices, leading to reliable and secure software. Leveraging the principles of quantum physics, quantum computing has emerged as a new computational paradigm that offers significant advantages over classical computing. As quantum computing progresses rapidly, its potential applications across various fields are becoming apparent. In software engineering, many tasks involve complex computations where quantum computers can greatly speed up the development process, leading to faster and more efficient solutions. With the growing use of quantum-based applications in different fields, quantum software engineering (QSE) has emerged as a discipline focused on designing, developing, and optimizing quantum software for diverse applications. This paper aims to review the role of quantum computing in software engineering research and the latest developments in QSE. To our knowledge, this is the first comprehensive review on this topic. We begin by introducing quantum computing, exploring its fundamental concepts, and discussing its potential applications in software engineering. We also examine various QSE techniques that expedite software development. Finally, we discuss the opportunities and challenges in quantum-driven software engineering and QSE. Our study reveals that quantum machine learning (QML) and quantum optimization have substantial potential to address classical software engineering tasks, though this area is still limited. Current QSE tools and techniques lack robustness and maturity, indicating a need for more focus. One of the main challenges is that quantum computing has yet to reach its full potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08925v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashis Kumar Mandal, Md Nadim, Chanchal K. Roy, Banani Roy, Kevin A. Schneider</dc:creator>
    </item>
    <item>
      <title>Anchor Sponsor Firms in Open Source Software Ecosystems</title>
      <link>https://arxiv.org/abs/2502.09060</link>
      <description>arXiv:2502.09060v1 Announce Type: new 
Abstract: Firms are intensifying their involvement with open source software (OSS), going beyond contributing to individual projects and releasing their own core technologies as OSS. These technologies, from web frameworks to programming languages, are the foundations of large and growing ecosystems. Yet we know little about how these anchor sponsors shape the behavior of OSS contributors. We examine Mozilla Corporation's role as incubator and anchor sponsor in the Rust programming language ecosystem, leveraging data on nearly 30,000 developers and 40,000 OSS projects from 2015 to 2022. When Mozilla abruptly exited Rust in August 2020, event-study models estimate a negative impact on ecosystem activity: a 9\% immediate drop in weekly commits and a 0.6 percentage point decline in trend. We observe an asymmetry in the shock's effects: former Mozilla developers and close collaborators continued contributing relatively quickly, whereas more distant developers showed reduced or ceased activity even six months later. An agent-based model of an OSS ecosystem with an anchor sponsor replicates these patterns. We also find a marked slowdown in new developers and projects entering Rust post-shock. Our results suggest that Mozilla served as a critical signal of Rust's quality and stability. Once withdrawn, newcomers and less-embedded developers were the most discouraged, raising concerns about long-term ecosystem sustainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09060v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brigitta N\'emeth, Johannes Wachs</dc:creator>
    </item>
    <item>
      <title>Modeling in Jjodel: Bridging Complexity and Usability in Model-Driven Engineering</title>
      <link>https://arxiv.org/abs/2502.09146</link>
      <description>arXiv:2502.09146v1 Announce Type: new 
Abstract: Jjodel is a cloud-based reflective platform designed to address the challenges of Model-Driven Engineering (MDE), particularly the cognitive complexity and usability barriers often encountered in existing model-driven tools. This article presents the motivation and requirements behind the design of Jjodel and demonstrates how it satisfies these through its key features. By offering a low-code environment with modular viewpoints for syntax, validation, and semantics, Jjodel empowers language designers to define and refine domain-specific languages (DSLs) with ease. Its innovative capabilities, such as real-time collaboration, live co-evolution support, and syntax customization, ensure adaptability and scalability for academic and industrial contexts. A practical case study of an algebraic expression language highlights the ability of Jjodel to manage positional semantics and event-driven workflows, illustrating its effectiveness in simplifying complex modeling scenarios. Built on modern front-end technologies, Jjodel bridges the gap between theoretical MDE research and practical application, providing a versatile and accessible solution for diverse modeling needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09146v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Bucchiarone, Juri Di Rocco, Damiano Di Vincenzo, Alfonso Pierantonio</dc:creator>
    </item>
    <item>
      <title>Copilot Arena: A Platform for Code LLM Evaluation in the Wild</title>
      <link>https://arxiv.org/abs/2502.09328</link>
      <description>arXiv:2502.09328v1 Announce Type: new 
Abstract: Evaluating in-the-wild coding capabilities of large language models (LLMs) is a challenging endeavor with no clear solution. We introduce Copilot Arena, a platform to collect user preferences for code generation through native integration into a developer's working environment. Copilot Arena comprises a novel interface for comparing pairs of model outputs, a sampling strategy optimized to reduce latency, and a prompting scheme to enable code completion functionality. Copilot Arena has served over 4.5 million suggestions from 10 models and collected over 11k pairwise judgements. Our results highlight the importance of model evaluations in integrated settings. We find that model rankings from Copilot Arena differ from those of existing evaluations, which we attribute to the more realistic distribution of data and tasks contained in Copilot Arena. We also identify novel insights into human preferences on code such as an observed consistency in user preference across programming languages yet significant variation in preference due to task category. We open-source Copilot Arena and release data to enable human-centric evaluations and improve understanding of coding assistants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09328v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wayne Chi, Valerie Chen, Anastasios Nikolas Angelopoulos, Wei-Lin Chiang, Aditya Mittal, Naman Jain, Tianjun Zhang, Ion Stoica, Chris Donahue, Ameet Talwalkar</dc:creator>
    </item>
    <item>
      <title>Metamorphic Testing for Pose Estimation Systems</title>
      <link>https://arxiv.org/abs/2502.09460</link>
      <description>arXiv:2502.09460v1 Announce Type: new 
Abstract: Pose estimation systems are used in a variety of fields, from sports analytics to livestock care. Given their potential impact, it is paramount to systematically test their behaviour and potential for failure. This is a complex task due to the oracle problem and the high cost of manual labelling necessary to build ground truth keypoints. This problem is exacerbated by the fact that different applications require systems to focus on different subjects (e.g., human versus animal) or landmarks (e.g., only extremities versus whole body and face), which makes labelled test data rarely reusable. To combat these problems we propose MET-POSE, a metamorphic testing framework for pose estimation systems that bypasses the need for manual annotation while assessing the performance of these systems under different circumstances. MET-POSE thus allows users of pose estimation systems to assess the systems in conditions that more closely relate to their application without having to label an ad-hoc test dataset or rely only on available datasets, which may not be adapted to their application domain. While we define MET-POSE in general terms, we also present a non-exhaustive list of metamorphic rules that represent common challenges in computer vision applications, as well as a specific way to evaluate these rules. We then experimentally show the effectiveness of MET-POSE by applying it to Mediapipe Holistic, a state of the art human pose estimation system, with the FLIC and PHOENIX datasets. With these experiments, we outline numerous ways in which the outputs of MET-POSE can uncover faults in pose estimation systems at a similar or higher rate than classic testing using hand labelled data, and show that users can tailor the rule set they use to the faults and level of accuracy relevant to their application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09460v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias Duran, Thomas Laurent, Ellen Rushe, Anthony Ventresque</dc:creator>
    </item>
    <item>
      <title>From PowerPoint UI Sketches to Web-Based Applications: Pattern-Driven Code Generation for GIS Dashboard Development Using Knowledge-Augmented LLMs, Context-Aware Visual Prompting, and the React Framework</title>
      <link>https://arxiv.org/abs/2502.08756</link>
      <description>arXiv:2502.08756v1 Announce Type: cross 
Abstract: Developing web-based GIS applications, commonly known as CyberGIS dashboards, for querying and visualizing GIS data in environmental research often demands repetitive and resource-intensive efforts. While Generative AI offers automation potential for code generation, it struggles with complex scientific applications due to challenges in integrating domain knowledge, software engineering principles, and UI design best practices. This paper introduces a knowledge-augmented code generation framework that retrieves software engineering best practices, domain expertise, and advanced technology stacks from a specialized knowledge base to enhance Generative Pre-trained Transformers (GPT) for front-end development. The framework automates the creation of GIS-based web applications (e.g., dashboards, interfaces) from user-defined UI wireframes sketched in tools like PowerPoint or Adobe Illustrator. A novel Context-Aware Visual Prompting method, implemented in Python, extracts layouts and interface features from these wireframes to guide code generation. Our approach leverages Large Language Models (LLMs) to generate front-end code by integrating structured reasoning, software engineering principles, and domain knowledge, drawing inspiration from Chain-of-Thought (CoT) prompting and Retrieval-Augmented Generation (RAG). A case study demonstrates the framework's capability to generate a modular, maintainable web platform hosting multiple dashboards for visualizing environmental and energy data (e.g., time-series, shapefiles, rasters) from user-sketched wireframes. By employing a knowledge-driven approach, the framework produces scalable, industry-standard front-end code using design patterns such as Model-View-ViewModel (MVVM) and frameworks like React. This significantly reduces manual effort in design and coding, pioneering an automated and efficient method for developing smart city software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08756v1</guid>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haowen Xu, Xiao-Ying Yu</dc:creator>
    </item>
    <item>
      <title>Unlocking Mental Health: Exploring College Students' Well-being through Smartphone Behaviors</title>
      <link>https://arxiv.org/abs/2502.08766</link>
      <description>arXiv:2502.08766v1 Announce Type: cross 
Abstract: The global mental health crisis is a pressing concern, with college students particularly vulnerable to rising mental health disorders. The widespread use of smartphones among young adults, while offering numerous benefits, has also been linked to negative outcomes such as addiction and regret, significantly impacting well-being. Leveraging the longest longitudinal dataset collected over four college years through passive mobile sensing, this study is the first to examine the relationship between students' smartphone unlocking behaviors and their mental health at scale in real-world settings. We provide the first evidence demonstrating the predictability of phone unlocking behaviors for mental health outcomes based on a large dataset, highlighting the potential of these novel features for future predictive models. Our findings reveal important variations in smartphone usage across genders and locations, offering a deeper understanding of the interplay between digital behaviors and mental health. We highlight future research directions aimed at mitigating adverse effects and promoting digital well-being in this population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08766v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Xuan, Meghna Roy Chowdhury, Yi Ding, Yixue Zhao</dc:creator>
    </item>
    <item>
      <title>In Specs we Trust? Conformance-Analysis of Implementation to Specifications in Node-RED and Associated Security Risks</title>
      <link>https://arxiv.org/abs/2502.09117</link>
      <description>arXiv:2502.09117v1 Announce Type: cross 
Abstract: Low-code development frameworks for IoT platforms offer a simple drag-and-drop mechanism to create applications for the billions of existing IoT devices without the need for extensive programming knowledge. The security of such software is crucial given the close integration of IoT devices in many highly sensitive areas such as healthcare or home automation. Node-RED is such a framework, where applications are built from nodes that are contributed by open-source developers. Its reliance on unvetted open-source contributions and lack of security checks raises the concern that the applications could be vulnerable to attacks, thereby imposing a security risk to end users. The low-code approach suggests, that many users could lack the technical knowledge to mitigate, understand, or even realize such security concerns. This paper focuses on "hidden" information flows in Node-RED nodes, meaning flows that are not captured by the specifications. They could (unknowingly or with malicious intent) cause leaks of sensitive information to unauthorized entities. We report the results of a conformance analysis of all nodes in the Node-RED framework, for which we compared the numbers of specified inputs and outputs of each node against the number of sources and sinks detected with CodeQL. The results show, that 55% of all nodes exhibit more possible flows than are specified. A risk assessment of a subset of the nodes showed, that 28% of them are associated with a high severity and 36% with a medium severity rating.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09117v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Schneider, Komal Kashish, Katja Tuma, Riccardo Scandariato</dc:creator>
    </item>
    <item>
      <title>Autonomous Task Completion Based on Goal-directed Answer Set Programming</title>
      <link>https://arxiv.org/abs/2502.09208</link>
      <description>arXiv:2502.09208v1 Announce Type: cross 
Abstract: Task planning for autonomous agents has typically been done using deep learning models and simulation-based reinforcement learning. This research proposes combining inductive learning techniques with goal-directed answer set programming to increase the explainability and reliability of systems for task breakdown and completion. Preliminary research has led to the creation of a Python harness that utilizes s(CASP) to solve task problems in a computationally efficient way. Although this research is in the early stages, we are exploring solutions to complex problems in simulated task completion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09208v1</guid>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.416.39</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 416, 2025, pp. 381-389</arxiv:journal_reference>
      <dc:creator>Alexis R. Tudor</dc:creator>
    </item>
    <item>
      <title>ASP-driven User-interaction with Clinguin</title>
      <link>https://arxiv.org/abs/2502.09222</link>
      <description>arXiv:2502.09222v1 Announce Type: cross 
Abstract: We present clinguin, a system for ASP-driven user interface design. Clinguin streamlines the development of user interfaces for ASP developers by letting them build interactive prototypes directly in ASP, eliminating the need for separate frontend languages. To this end, clinguin uses a few dedicated predicates to define user interfaces and the treatment of user-triggered events. This simple design greatly facilitates the specification of user interactions with an ASP system, in our case clingo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09222v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.416.19</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 416, 2025, pp. 215-228</arxiv:journal_reference>
      <dc:creator>Alexander Beiser, Susana Hahn, Torsten Schaub</dc:creator>
    </item>
    <item>
      <title>Early Validation of High-level Requirements on Cyber-Physical Systems</title>
      <link>https://arxiv.org/abs/2502.09236</link>
      <description>arXiv:2502.09236v1 Announce Type: cross 
Abstract: The overarching, broad topic of my research are advancements in the area of safety-critical, cyber-physical systems (CPS) development with emphasis on validation and verification. The particular focus of my research is the early validation of high-level requirements on CPS. My current approach for tackling this problem is transforming the requirements into Event Calculus and subsequently reasoning about them using ASP solvers such as the grounding-free s(CASP). Below, I discuss my research, its current state, and the open issues that are still left to tackle. The first results of my work will be presented in a paper that was accepted for ICLP'24, which is my first paper in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09236v1</guid>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.416.40</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 416, 2025, pp. 390-397</arxiv:journal_reference>
      <dc:creator>Ond\v{r}ej Va\v{s}\'i\v{c}ek (Brno University of Technology, Czechia)</dc:creator>
    </item>
    <item>
      <title>Predicting Drive Test Results in Mobile Networks Using Optimization Techniques</title>
      <link>https://arxiv.org/abs/2502.09305</link>
      <description>arXiv:2502.09305v1 Announce Type: cross 
Abstract: Mobile network operators constantly optimize their networks to ensure superior service quality and coverage. This optimization is crucial for maintaining an optimal user experience and requires extensive data collection and analysis. One of the primary methods for gathering this data is through drive tests, where technical teams use specialized equipment to collect signal information across various regions. However, drive tests are both costly and time-consuming, and they face challenges such as traffic conditions, environmental factors, and limited access to certain areas. These constraints make it difficult to replicate drive tests under similar conditions. In this study, we propose a method that enables operators to predict received signal strength at specific locations using data from other drive test points. By reducing the need for widespread drive tests, this approach allows operators to save time and resources while still obtaining the necessary data to optimize their networks and mitigate the challenges associated with traditional drive tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09305v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>MohammadJava Taheri, Abolfazl Diyanat, MortezaAli Ahmadi, Ali Nazari</dc:creator>
    </item>
    <item>
      <title>The Cure is in the Cause: A Filesystem for Container Debloating</title>
      <link>https://arxiv.org/abs/2305.04641</link>
      <description>arXiv:2305.04641v2 Announce Type: replace 
Abstract: Containers have become a standard for deploying applications due to their convenience, but they often suffer from significant software bloat-unused files that inflate image sizes, increase provisioning times, and waste resources. These inefficiencies are particularly problematic in serverless and edge computing scenarios, where resources are constrained, and performance is critical. Existing debloating tools are limited in scope and effectiveness, failing to address the widespread issue of container bloat at scale. In this paper, we conduct a large-scale evaluation of container bloat, analyzing the top 20 most downloaded containers on DockerHub. We evaluate two state-of-the-art debloating tools, identify their limitations, and propose a novel solution, BAFFS, which addresses bloat at the filesystem level by introducing a flexible debloating layer that preserves the layered structure of container filesystems. The debloating layer can be organized in different ways to meet diverse requirements. Our evaluation demonstrates that over 50% of the top-downloaded containers have more than 60% bloat, and BAFFS reduces container sizes significantly while maintaining functionality. For serverless functions, BAFFS reduces cold start latency by up to 68%. Additionally, when combined with lazy-loading snapshotters, BAFFS enhances provisioning efficiency, reducing conversion times by up to 93% and provisioning times by up to 19%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04641v2</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huaifeng Zhang, Philipp Leitner, Mohannad Alhanahnah, Ahmed Ali-Eldin</dc:creator>
    </item>
    <item>
      <title>Towards Understanding the Characteristics of Code Generation Errors Made by Large Language Models</title>
      <link>https://arxiv.org/abs/2406.08731</link>
      <description>arXiv:2406.08731v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated unprecedented capabilities in code generation. However, there remains a limited understanding of code generation errors that LLMs can produce. To bridge the gap, we conducted an in-depth analysis of code generation errors across six representative LLMs on the HumanEval dataset. Specifically, we first employed open coding and thematic analysis to distill a comprehensive taxonomy of code generation errors. We analyzed two dimensions of error characteristics -- semantic characteristics and syntactic characteristics. Our analysis revealed that LLMs often made non-trivial, multi-line code generation errors in various locations and with various root causes. We further analyzed the correlation between these errors and task complexity as well as test pass rate. Our findings highlighted several challenges in locating and fixing code generation errors made by LLMs. In the end, we discussed several future directions to address these challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08731v3</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhijie Wang, Zijie Zhou, Da Song, Yuheng Huang, Shengmai Chen, Lei Ma, Tianyi Zhang</dc:creator>
    </item>
    <item>
      <title>LLMs Meet Library Evolution: Evaluating Deprecated API Usage in LLM-based Code Completion</title>
      <link>https://arxiv.org/abs/2406.09834</link>
      <description>arXiv:2406.09834v3 Announce Type: replace 
Abstract: Large language models (LLMs), pre-trained or fine-tuned on large code corpora, have shown effectiveness in generating code completions. However, in LLM-based code completion, LLMs may struggle to use correct and up-to-date Application Programming Interfaces (APIs) due to the rapid and continuous evolution of libraries. While existing studies have highlighted issues with predicting incorrect APIs, the specific problem of deprecated API usage in LLM-based code completion has not been thoroughly investigated. To address this gap, we conducted the first evaluation study on deprecated API usage in LLM-based code completion. This study involved seven advanced LLMs, 145 API mappings from eight popular Python libraries, and 28,125 completion prompts. The study results reveal the status quo (i.e., API usage plausibility and deprecated usage rate) of deprecated API and replacing API usage in LLM-based code completion from the perspectives of model, prompt, and library, and indicate the root causes behind. Based on these findings, we propose two lightweight fixing approaches, REPLACEAPI and INSERTPROMPT, which can serve as baseline approaches for future research on mitigating deprecated API usage in LLM-based completion. Additionally, we provide implications for future research on integrating library evolution with LLM-driven software development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09834v3</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chong Wang, Kaifeng Huang, Jian Zhang, Yebo Feng, Lyuye Zhang, Yang Liu, Xin Peng</dc:creator>
    </item>
    <item>
      <title>What can Large Language Models Capture about Code Functional Equivalence?</title>
      <link>https://arxiv.org/abs/2408.11081</link>
      <description>arXiv:2408.11081v2 Announce Type: replace 
Abstract: Code-LLMs, LLMs pre-trained on large code corpora, have shown great progress in learning rich representations of the structure and syntax of code, successfully using it to generate or classify code fragments. At the same time, understanding if they are able to do so because they capture code semantics, and how well, is still an open question. In this paper, we tackle this problem by introducing SeqCoBench, a benchmark for systematically assessing how Code-LLMs can capture code functional equivalence. SeqCoBench contains over 20 code transformations that either preserve or alter the semantics of Python programs. We conduct extensive evaluations in different settings, including zero-shot and parameter-efficient finetuning methods on state-of-the-art (Code)-LLMs to see if they can discern semantically equivalent or different pairs of programs in SeqCoBench. We find that the performance gap between these LLMs and classical match-based retrieval scores is minimal, with both approaches showing a concerning lack of depth in understanding code semantics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11081v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nickil Maveli, Antonio Vergari, Shay B. Cohen</dc:creator>
    </item>
    <item>
      <title>Are We Learning the Right Features? A Framework for Evaluating DL-Based Software Vulnerability Detection Solutions</title>
      <link>https://arxiv.org/abs/2501.13291</link>
      <description>arXiv:2501.13291v4 Announce Type: replace 
Abstract: Recent research has revealed that the reported results of an emerging body of DL-based techniques for detecting software vulnerabilities are not reproducible, either across different datasets or on unseen samples. This paper aims to provide the foundation for properly evaluating the research in this domain. We do so by analyzing prior work and existing vulnerability datasets for the syntactic and semantic features of code that contribute to vulnerability, as well as features that falsely correlate with vulnerability. We provide a novel, uniform representation to capture both sets of features, and use this representation to detect the presence of both vulnerability and spurious features in code. To this end, we design two types of code perturbations: feature preserving perturbations (FPP) ensure that the vulnerability feature remains in a given code sample, while feature eliminating perturbations (FEP) eliminate the feature from the code sample. These perturbations aim to measure the influence of spurious and vulnerability features on the predictions of a given vulnerability detection solution. To evaluate how the two classes of perturbations influence predictions, we conducted a large-scale empirical study on five state-of-the-art DL-based vulnerability detectors. Our study shows that, for vulnerability features, only ~2% of FPPs yield the undesirable effect of a prediction changing among the five detectors on average. However, on average, ~84% of FEPs yield the undesirable effect of retaining the vulnerability predictions. For spurious features, we observed that FPPs yielded a drop in recall up to 29% for graph-based detectors. We present the reasons underlying these results and suggest strategies for improving DNN-based vulnerability detectors. We provide our perturbation-based evaluation framework as a public resource to enable independent future evaluation of vulnerability detectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13291v4</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satyaki Das, Syeda Tasnim Fabiha, Saad Shafiq, Nenad Medvidovic</dc:creator>
    </item>
    <item>
      <title>One-for-All Does Not Work! Enhancing Vulnerability Detection by Mixture-of-Experts (MoE)</title>
      <link>https://arxiv.org/abs/2501.16454</link>
      <description>arXiv:2501.16454v2 Announce Type: replace 
Abstract: Deep Learning-based Vulnerability Detection (DLVD) techniques have garnered significant interest due to their ability to automatically learn vulnerability patterns from previously compromised code. Despite the notable accuracy demonstrated by pioneering tools, the broader application of DLVD methods in real-world scenarios is hindered by significant challenges. A primary issue is the "one-for-all" design, where a single model is trained to handle all types of vulnerabilities. This approach fails to capture the patterns of different vulnerability types, resulting in suboptimal performance, particularly for less common vulnerabilities that are often underrepresented in training datasets. To address these challenges, we propose MoEVD, which adopts the Mixture-of-Experts (MoE) framework for vulnerability detection. MoEVD decomposes vulnerability detection into two tasks, CWE type classification and CWE-specific vulnerability detection. By splitting the task, in vulnerability detection, MoEVD allows specific experts to handle distinct types of vulnerabilities instead of handling all vulnerabilities within one model. Our results show that MoEVD achieves an F1-score of 0.44, significantly outperforming all studied state-of-the-art (SOTA) baselines by at least 12.8%. MoEVD excels across almost all CWE types, improving recall over the best SOTA baseline by 9% to 77.8%. Notably, MoEVD does not sacrifice performance on long-tailed CWE types; instead, its MoE design enhances performance (F1-score) on these by at least 7.3%, addressing long-tailed issues effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16454v2</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xu Yang, Shaowei Wang, Jiayuan Zhou, Wenhan Zhu</dc:creator>
    </item>
    <item>
      <title>Characterizing Bugs in Login Processes of Android Applications: An Empirical Study</title>
      <link>https://arxiv.org/abs/2502.04200</link>
      <description>arXiv:2502.04200v3 Announce Type: replace 
Abstract: The login functionality, being the gateway to app usage, plays a critical role in both user experience and application security. As Android apps increasingly incorporate login functionalities, they support a variety of authentication methods with complicated login processes, catering to personalized user experiences. However, the complexities in managing different operations in login processes make it difficult for developers to handle them correctly. In this paper, we present the first empirical study of login issues in Android apps. We analyze 361 issues from 44 popular open-source Android repositories, examining the root causes, symptoms, and trigger conditions of these issues. Our findings indicate that the vast majority of the login issues are induced by the improper handling of complex state transitions during the login process, which can prevent users from logging in or misdirect them to incorrect subsequent actions. Additionally, we observed that issues related to this cause typically require the convergence of multiple trigger conditions to manifest. These findings can help developers to model the login processes which can help them to identify the causes of issues and design targeted test cases and precise test oracles. Our dataset has been made openly available to facilitate future research in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04200v3</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zixu Zhou, Rufeng Chen, Junfeng Chen, Yepang Liu, Lili Wei</dc:creator>
    </item>
    <item>
      <title>Predicting Safety Misbehaviours in Autonomous Driving Systems using Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2404.18573</link>
      <description>arXiv:2404.18573v2 Announce Type: replace-cross 
Abstract: The automated real-time recognition of unexpected situations plays a crucial role in the safety of autonomous vehicles, especially in unsupported and unpredictable scenarios. This paper evaluates different Bayesian uncertainty quantification methods from the deep learning domain for the anticipatory testing of safety-critical misbehaviours during system-level simulation-based testing. Specifically, we compute uncertainty scores as the vehicle executes, following the intuition that high uncertainty scores are indicative of unsupported runtime conditions that can be used to distinguish safe from failure-inducing driving behaviors. In our study, we conducted an evaluation of the effectiveness and computational overhead associated with two Bayesian uncertainty quantification methods, namely MC- Dropout and Deep Ensembles, for misbehaviour avoidance. Overall, for three benchmarks from the Udacity simulator comprising both out-of-distribution and unsafe conditions introduced via mutation testing, both methods successfully detected a high number of out-of-bounds episodes providing early warnings several seconds in advance, outperforming two state-of-the-art misbehaviour prediction methods based on autoencoders and attention maps in terms of effectiveness and efficiency. Notably, Deep Ensembles detected most misbehaviours without any false alarms and did so even when employing a relatively small number of models, making them computationally feasible for real-time detection. Our findings suggest that incorporating uncertainty quantification methods is a viable approach for building fail-safe mechanisms in deep neural network-based autonomous vehicles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18573v2</guid>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ruben Grewal, Paolo Tonella, Andrea Stocco</dc:creator>
    </item>
    <item>
      <title>VTutor: An Open-Source SDK for Generative AI-Powered Animated Pedagogical Agents with Multi-Media Output</title>
      <link>https://arxiv.org/abs/2502.04103</link>
      <description>arXiv:2502.04103v2 Announce Type: replace-cross 
Abstract: The rapid evolution of large language models (LLMs) has transformed human-computer interaction (HCI), but the interaction with LLMs is currently mainly focused on text-based interactions, while other multi-model approaches remain under-explored. This paper introduces VTutor, an open-source Software Development Kit (SDK) that combines generative AI with advanced animation technologies to create engaging, adaptable, and realistic APAs for human-AI multi-media interactions. VTutor leverages LLMs for real-time personalized feedback, advanced lip synchronization for natural speech alignment, and WebGL rendering for seamless web integration. Supporting various 2D and 3D character models, VTutor enables researchers and developers to design emotionally resonant, contextually adaptive learning agents. This toolkit enhances learner engagement, feedback receptivity, and human-AI interaction while promoting trustworthy AI principles in education. VTutor sets a new standard for next-generation APAs, offering an accessible, scalable solution for fostering meaningful and immersive human-AI interaction experiences. The VTutor project is open-sourced and welcomes community-driven contributions and showcases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04103v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eason Chen, Chenyu Lin, Xinyi Tang, Aprille Xi, Canwen Wang, Jionghao Lin, Kenneth R Koedinger</dc:creator>
    </item>
  </channel>
</rss>
