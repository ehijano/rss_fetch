<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Nov 2024 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>singleRcapture: An R Package for Single-Source Capture-Recapture Models</title>
      <link>https://arxiv.org/abs/2411.11032</link>
      <description>arXiv:2411.11032v1 Announce Type: new 
Abstract: Population size estimation is a major challenge in official statistics, social sciences, and natural sciences. The problem can be tackled by applying capture-recapture methods, which vary depending on the number of sources used, particularly on whether a single or multiple sources are involved. This paper focuses on the first group of methods and introduces a novel R package: singleRcapture. The package implements state-of-the-art single-source capture-recapture (SSCR) models (e.g.zero-truncated one-inflated regression) together with new developments proposed by the authors, and provides a user-friendly application programming interface (API). This self-contained package can be used to produce point estimates and their variance and implements several bootstrap variance estimators or diagnostics to assess quality and conduct sensitivity analysis. It is intended for users interested in estimating the size of populations, particularly those that are difficult to reach or measure, for which information is available only from one source and dual/multiple system estimation is not applicable. Our package serves to bridge a significant gap, as the SSCR methods are either not available at all or are only partially implemented in existing R packages and other open-source software. Furthermore, since many R users are familiar with countreg or VGAM packages, we have implemented a lightweight extension called singleRcaptureExtra which can be used to integrate singleRcapture with these packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11032v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piotr Chlebicki, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>Machine learning-based probabilistic forecasting of solar irradiance in Chile</title>
      <link>https://arxiv.org/abs/2411.11073</link>
      <description>arXiv:2411.11073v1 Announce Type: new 
Abstract: By the end of 2023, renewable sources cover 63.4% of the total electric power demand of Chile, and in line with the global trend, photovoltaic (PV) power shows the most dynamic increase. Although Chile's Atacama Desert is considered the sunniest place on Earth, PV power production, even in this area, can be highly volatile. Successful integration of PV energy into the country's power grid requires accurate short-term PV power forecasts, which can be obtained from predictions of solar irradiance and related weather quantities. Nowadays, in weather forecasting, the state-of-the-art approach is the use of ensemble forecasts based on multiple runs of numerical weather prediction models. However, ensemble forecasts still tend to be uncalibrated or biased, thus requiring some form of post-processing. The present work investigates probabilistic forecasts of solar irradiance for Regions III and IV in Chile. For this reason, 8-member short-term ensemble forecasts of solar irradiance for calendar year 2021 are generated using the Weather Research and Forecasting (WRF) model, which are then calibrated using the benchmark ensemble model output statistics (EMOS) method based on a censored Gaussian law, and its machine learning-based distributional regression network (DRN) counterpart. Furthermore, we also propose a neural network-based post-processing method resulting in improved 8-member ensemble predictions. All forecasts are evaluated against station observations for 30 locations, and the skill of post-processed predictions is compared to the raw WRF ensemble. Our case study confirms that all studied post-processing methods substantially improve both the calibration of probabilistic- and the accuracy of point forecasts. Among the methods tested, the corrected ensemble exhibits the best overall performance. Additionally, the DRN model generally outperforms the corresponding EMOS approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11073v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'andor Baran, Julio C. Mar\'in, Omar Cuevas, Mailiu D\'iaz, Marianna Szab\'o, Orietta Nicolis, M\'aria Lakatos</dc:creator>
    </item>
    <item>
      <title>Simple yet effective: a comparative study of statistical models for yearly hurricane forecasting</title>
      <link>https://arxiv.org/abs/2411.11112</link>
      <description>arXiv:2411.11112v1 Announce Type: new 
Abstract: In this paper, we study the problem of forecasting the next year's number of Atlantic hurricanes, which is relevant in many fields of applications such as land-use planning, hazard mitigation, reinsurance and long-term weather derivative market. Considering a set of well-known predictors, we compare the forecasting accuracy of both machine learning and simpler models, showing that the latter may be more adequate than the first. Quantile regression models, which are adopted for the first time for forecasting hurricane numbers, provide the best results. Moreover, we construct a new index showing good properties in anticipating the direction of the future number of hurricanes. We consider different evaluation metrics based on both magnitude forecasting errors and directional accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11112v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pietro Colombo, Raffaele Mattera, Philipp Otto</dc:creator>
    </item>
    <item>
      <title>Identifying good forecasters via adaptive cognitive tests</title>
      <link>https://arxiv.org/abs/2411.11126</link>
      <description>arXiv:2411.11126v1 Announce Type: new 
Abstract: Assessing forecasting proficiency is a time-intensive activity, often requiring us to wait months or years before we know whether or not the reported forecasts were good. In this study, we develop adaptive cognitive tests that predict forecasting proficiency without the need to wait for forecast outcomes. Our procedures provide information about which cognitive tests to administer to each individual, as well as how many cognitive tests to administer. Using item response models, we identify and tailor cognitive tests to assess forecasters of different skill levels, aiming to optimize accuracy and efficiency. We show how the procedures can select highly-informative cognitive tests from a larger battery of tests, reducing the time taken to administer the tests. We use a second, independent dataset to show that the selected tests yield scores that are highly related to forecasting proficiency. This approach enables real-time, adaptive testing, providing immediate insights into forecasting talent in practical contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11126v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edgar C. Merkle, Nikolay Petrov, Sophie Ma Zhu, Ezra Karger, Philip E. Tetlock, Mark Himmelstein</dc:creator>
    </item>
    <item>
      <title>Association between built environment characteristics and school run traffic congestion in Beijing, China</title>
      <link>https://arxiv.org/abs/2411.11390</link>
      <description>arXiv:2411.11390v1 Announce Type: new 
Abstract: School-escorted trips are a significant contributor to traffic congestion. Existing studies mainly compare road traffic during student pick-up/drop-off hours with off-peak times, often overlooking the fact that school-run traffic congestion is unevenly distributed across areas with different built environment characteristics. We examine the relationship between the built environment and school-run traffic congestion, using Beijing, China, as a case study. First, we use multi-source geospatial data to assess the built environment characteristics around schools across five dimensions: spatial concentration, transportation infrastructure, street topology, spatial richness, and scenescapes. Second, employing a generalized ordered logit model, we analyze how traffic congestion around schools varies during peak hours on school days, regular non-school days, and national college entrance exam days. Lastly, we identify the built environment factors contributing to school-run traffic congestion through multivariable linear regression and Shapley value explanations. Our findings reveal that: (1) School runs significantly exacerbate traffic congestion around schools, reducing the likelihood of free-flow by 8.34\% during school run times; (2) School-run traffic congestion is more severe in areas with multiple schools, bus stops, and scenescapes related to business and financial functions. These insights can inform the planning of new schools and urban upgrade strategies aimed at reducing traffic congestion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11390v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaogui Kang, Xiaxin Wu, Jialei Shi, Chao Yang</dc:creator>
    </item>
    <item>
      <title>Does wind affect the orientation of vegetation stripes? A copula-based mixture model for axial and circular data</title>
      <link>https://arxiv.org/abs/2411.11461</link>
      <description>arXiv:2411.11461v1 Announce Type: new 
Abstract: Motivated by a case study of vegetation patterns, we introduce a mixture model with concomitant variables to examine the association between the orientation of vegetation stripes and wind direction. The proposal relies on a novel copula-based bivariate distribution for mixed axial and circular observations and provides a parsimonious and computationally tractable approach to examine the dependence of two environmental variables observed in a complex manifold. The findings suggest that dominant winds shape the orientation of vegetation stripes through a mechanism of neighbouring plants providing wind shelter to downwind individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11461v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Mingione, Francesco Lagona, Priyanka Nagar, Francois von Holtzhausen, Andriette Bekker, Janine Schoombie, Peter C. le Roux</dc:creator>
    </item>
    <item>
      <title>Climate data selection for multi-decadal wind power forecasts</title>
      <link>https://arxiv.org/abs/2411.11630</link>
      <description>arXiv:2411.11630v1 Announce Type: new 
Abstract: Reliable wind speed data is crucial for applications such as estimating local (future) wind power. Global Climate Models (GCMs) and Regional Climate Models (RCMs) provide forecasts over multi-decadal periods. However, their outputs vary substantially, and higher-resolution models come with increased computational demands. In this study, we analyze how the spatial resolution of different GCMs and RCMs affects the reliability of simulated wind speeds and wind power, using ERA5 data as a reference. We present a systematic procedure for model evaluation for wind resource assessment as a downstream task. Our results show that higher-resolution GCMs and RCMs do not necessarily preserve wind speeds more accurately. Instead, the choice of model, both for GCMs and RCMs, is more important than the resolution or GCM boundary conditions. The IPSL model preserves the wind speed distribution particularly well in Europe, producing the most accurate wind power forecasts relative to ERA5 data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11630v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sofia Morelli, Nina Effenberger, Luca Schmidt, Nicole Ludwig</dc:creator>
    </item>
    <item>
      <title>The Noisy Work of Uncertainty Visualisation Research: A Review</title>
      <link>https://arxiv.org/abs/2411.10482</link>
      <description>arXiv:2411.10482v1 Announce Type: cross 
Abstract: Uncertainty visualisation is quickly becomming a hot topic in information visualisation. Exisiting reviews in the field take the definition and purpose of an uncertainty visualisation to be self evident which results in a large amout of conflicting information. This conflict largely stems from a conflation between uncertainty visualisations designed for decision making and those designed to prevent false conclusions. We coin the term "signal suppression" to describe a visualisation that is designed for preventing false conclusions, as the approach demands that the signal (i.e. the collective take away of the estimates) is suppressed by the noise (i.e. the variance on those estimates). We argue that the current standards in visualisation suggest that uncertainty visualisations designed for decision making should not be considered uncertainty visualisations at all. Therefore, future work should focus on signal suppression. Effective signal suppression requires us to communicate the signal and the noise as a single "validity of signal" variable, and doing so proves to be difficult with current methods. We illustrate current approaches to uncertainty visualisation by showing how they would change the visual apprearance of a choropleth map. These maps allow us to see why some methods succeed at signal suppression, while others fall short. Evaluating visualisations on how well they perform signal suppression also proves to be difficult, as it involves measuring the effect of noise, a variable we typically try to ignore. We suggest authors use qualitative studies or compare uncertainty visualisations to the relevant hypothesis tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10482v1</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harriet Mason, Dianne Cook, Sarah Goodwin, Emi Tanaka, Susan Vanderplus</dc:creator>
    </item>
    <item>
      <title>Feature Importance of Climate Vulnerability Indicators with Gradient Boosting across Five Global Cities</title>
      <link>https://arxiv.org/abs/2411.10628</link>
      <description>arXiv:2411.10628v1 Announce Type: cross 
Abstract: Efforts are needed to identify and measure both communities' exposure to climate hazards and the social vulnerabilities that interact with these hazards, but the science of validating hazard vulnerability indicators is still in its infancy. Progress is needed to improve: 1) the selection of variables that are used as proxies to represent hazard vulnerability; 2) the applicability and scale for which these indicators are intended, including their transnational applicability. We administered an international urban survey in Buenos Aires, Argentina; Johannesburg, South Africa; London, United Kingdom; New York City, United States; and Seoul, South Korea in order to collect data on exposure to various types of extreme weather events, socioeconomic characteristics commonly used as proxies for vulnerability (i.e., income, education level, gender, and age), and additional characteristics not often included in existing composite indices (i.e., queer identity, disability identity, non-dominant primary language, and self-perceptions of both discrimination and vulnerability to flood risk). We then use feature importance analysis with gradient-boosted decision trees to measure the importance that these variables have in predicting exposure to various types of extreme weather events. Our results show that non-traditional variables were more relevant to self-reported exposure to extreme weather events than traditionally employed variables such as income or age. Furthermore, differences in variable relevance across different types of hazards and across urban contexts suggest that vulnerability indicators need to be fit to context and should not be used in a one-size-fits-all fashion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10628v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lidia Cano Pecharroman, Melissa O. Tier, Elke U. Weber</dc:creator>
    </item>
    <item>
      <title>False Discovery Control in Multiple Testing: A Brief Overview of Theories and Methodologies</title>
      <link>https://arxiv.org/abs/2411.10647</link>
      <description>arXiv:2411.10647v1 Announce Type: cross 
Abstract: As the volume and complexity of data continue to expand across various scientific disciplines, the need for robust methods to account for the multiplicity of comparisons has grown widespread. A popular measure of type 1 error rate in multiple testing literature is the false discovery rate (FDR). The FDR provides a powerful and practical approach to large-scale multiple testing and has been successfully used in a wide range of applications. The concept of FDR has gained wide acceptance in the statistical community and various methods has been proposed to control the FDR. In this work, we review the latest developments in FDR control methodologies. We also develop a conceptual framework to better describe this vast literature; understand its intuition and key ideas; and provide guidance for the researcher interested in both the application and development of the methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10647v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jianliang He, Bowen Gang, Luella Fu</dc:creator>
    </item>
    <item>
      <title>Understanding Learning with Sliced-Wasserstein Requires Rethinking Informative Slices</title>
      <link>https://arxiv.org/abs/2411.10651</link>
      <description>arXiv:2411.10651v1 Announce Type: cross 
Abstract: The practical applications of Wasserstein distances (WDs) are constrained by their sample and computational complexities. Sliced-Wasserstein distances (SWDs) provide a workaround by projecting distributions onto one-dimensional subspaces, leveraging the more efficient, closed-form WDs for one-dimensional distributions. However, in high dimensions, most random projections become uninformative due to the concentration of measure phenomenon. Although several SWD variants have been proposed to focus on \textit{informative} slices, they often introduce additional complexity, numerical instability, and compromise desirable theoretical (metric) properties of SWD. Amidst the growing literature that focuses on directly modifying the slicing distribution, which often face challenges, we revisit the classical Sliced-Wasserstein and propose instead to rescale the 1D Wasserstein to make all slices equally informative. Importantly, we show that with an appropriate data assumption and notion of \textit{slice informativeness}, rescaling for all individual slices simplifies to \textbf{a single global scaling factor} on the SWD. This, in turn, translates to the standard learning rate search for gradient-based learning in common machine learning workflows. We perform extensive experiments across various machine learning tasks showing that the classical SWD, when properly configured, can often match or surpass the performance of more complex variants. We then answer the following question: "Is Sliced-Wasserstein all you need for common learning tasks?"</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10651v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huy Tran, Yikun Bai, Ashkan Shahbazi, John R. Hershey, Soheil Kolouri</dc:creator>
    </item>
    <item>
      <title>Program Evaluation with Remotely Sensed Outcomes</title>
      <link>https://arxiv.org/abs/2411.10959</link>
      <description>arXiv:2411.10959v1 Announce Type: cross 
Abstract: While traditional program evaluations typically rely on surveys to measure outcomes, certain economic outcomes such as living standards or environmental quality may be infeasible or costly to collect. As a result, recent empirical work estimates treatment effects using remotely sensed variables (RSVs), such mobile phone activity or satellite images, instead of ground-truth outcome measurements. Common practice predicts the economic outcome from the RSV, using an auxiliary sample of labeled RSVs, and then uses such predictions as the outcome in the experiment. We prove that this approach leads to biased estimates of treatment effects when the RSV is a post-outcome variable. We nonparametrically identify the treatment effect, using an assumption that reflects the logic of recent empirical research: the conditional distribution of the RSV remains stable across both samples, given the outcome and treatment. Our results do not require researchers to know or consistently estimate the relationship between the RSV, outcome, and treatment, which is typically mis-specified with unstructured data. We form a representation of the RSV for downstream causal inference by predicting the outcome and predicting the treatment, with better predictions leading to more precise causal estimates. We re-evaluate the efficacy of a large-scale public program in India, showing that the program's measured effects on local consumption and poverty can be replicated using satellite</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10959v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ashesh Rambachan, Rahul Singh, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>A novel density-based approach for estimating unknown means, distribution visualisations and meta-analyses of quantiles</title>
      <link>https://arxiv.org/abs/2411.10971</link>
      <description>arXiv:2411.10971v1 Announce Type: cross 
Abstract: In meta-analysis with continuous outcomes, the use of effect sizes based on the means is the most common. It is often found, however, that only the quantile summary measures are reported in some studies, and in certain scenarios, a meta-analysis of the quantiles themselves are of interest. We propose a novel density-based approach to support the implementation of a comprehensive meta-analysis, when only the quantile summary measures are reported. The proposed approach uses flexible quantile-based distributions and percentile matching to estimate the unknown parameters without making any prior assumptions about the underlying distributions. Using simulated and real data, we show that the proposed novel density-based approach works as well as or better than the widely-used methods in estimating the means using quantile summaries without assuming a distribution apriori, and provides a novel tool for distribution visualisations. In addition to this, we introduce quantile-based meta-analysis methods for situations where a comparison of quantiles between groups themselves are of interest and found to be more suitable. Using both real and simulated data, we also demonstrate the applicability of these quantile-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10971v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alysha M De Livera, Luke Prendergast, Udara Kumaranathunga</dc:creator>
    </item>
    <item>
      <title>Flood Risk Assessment of the National Harbor at Maryland, United States</title>
      <link>https://arxiv.org/abs/2411.11014</link>
      <description>arXiv:2411.11014v1 Announce Type: cross 
Abstract: Over the past few decades, floods have become one of the costliest natural hazards and losses have sharply escalated. Floods are an increasing problem in urban areas due to increased residential settlement along the coastline and climate change is a contributing factor to this increased frequency. In order to analyze flood risk, a model is proposed to identify the factors associated with increased flooding at a local scale. The study area includes National Harbor, MD, and the surrounding area of Fort Washington. The objective is to assess flood risk due to an increase in sea level rise for the study area of interest. The study demonstrated that coastal flood risk increased with sea level rise even though the predicted level of impact is fairly insignificant for the study area. The level of impact from increased flooding is highly dependent on the location of the properties and other topographic information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11014v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neftalem Negussie, Addis Yesserie, Chinchu Harris, Abou Keita, Huthaifa I. Ashqar</dc:creator>
    </item>
    <item>
      <title>Teaching Video Diffusion Model with Latent Physical Phenomenon Knowledge</title>
      <link>https://arxiv.org/abs/2411.11343</link>
      <description>arXiv:2411.11343v1 Announce Type: cross 
Abstract: Video diffusion models have exhibited tremendous progress in various video generation tasks. However, existing models struggle to capture latent physical knowledge, failing to infer physical phenomena that are challenging to articulate with natural language. Generating videos following the fundamental physical laws is still an opening challenge. To address this challenge, we propose a novel method to teach video diffusion models with latent physical phenomenon knowledge, enabling the accurate generation of physically informed phenomena. Specifically, we first pretrain Masked Autoencoders (MAE) to reconstruct the physical phenomena, resulting in output embeddings that encapsulate latent physical phenomenon knowledge. Leveraging these embeddings, we could generate the pseudo-language prompt features based on the aligned spatial relationships between CLIP vision and language encoders. Particularly, given that diffusion models typically use CLIP's language encoder for text prompt embeddings, our approach integrates the CLIP visual features informed by latent physical knowledge into a quaternion hidden space. This enables the modeling of spatial relationships to produce physical knowledge-informed pseudo-language prompts. By incorporating these prompt features and fine-tuning the video diffusion model in a parameter-efficient manner, the physical knowledge-informed videos are successfully generated. We validate our method extensively through both numerical simulations and real-world observations of physical phenomena, demonstrating its remarkable performance across diverse scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11343v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qinglong Cao, Ding Wang, Xirui Li, Yuntian Chen, Chao Ma, Xiaokang Yang</dc:creator>
    </item>
    <item>
      <title>Causal Effect of Group Diversity on Redundancy and Coverage in Peer-Reviewing</title>
      <link>https://arxiv.org/abs/2411.11437</link>
      <description>arXiv:2411.11437v1 Announce Type: cross 
Abstract: A large host of scientific journals and conferences solicit peer reviews from multiple reviewers for the same submission, aiming to gather a broader range of perspectives and mitigate individual biases. In this work, we reflect on the role of diversity in the slate of reviewers assigned to evaluate a submitted paper as a factor in diversifying perspectives and improving the utility of the peer-review process. We propose two measures for assessing review utility: review coverage -- reviews should cover most contents of the paper -- and review redundancy -- reviews should add information not already present in other reviews. We hypothesize that reviews from diverse reviewers will exhibit high coverage and low redundancy. We conduct a causal study of different measures of reviewer diversity on review coverage and redundancy using observational data from a peer-reviewed conference with approximately 5,000 submitted papers. Our study reveals disparate effects of different diversity measures on review coverage and redundancy. Our study finds that assigning a group of reviewers that are topically diverse, have different seniority levels, or have distinct publication networks leads to broader coverage of the paper or review criteria, but we find no evidence of an increase in coverage for reviewer slates with reviewers from diverse organizations or geographical locations. Reviewers from different organizations, seniority levels, topics, or publications networks (all except geographical diversity) lead to a decrease in redundancy in reviews. Furthermore, publication network-based diversity alone also helps bring in varying perspectives (that is, low redundancy), even within specific review criteria. Our study adopts a group decision-making perspective for reviewer assignments in peer review and suggests dimensions of diversity that can help guide the reviewer assignment process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11437v1</guid>
      <category>cs.DL</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Navita Goyal, Ivan Stelmakh, Nihar Shah, Hal Daum\'e III</dc:creator>
    </item>
    <item>
      <title>Spatio-Temporal Jump Model for Urban Thermal Comfort Monitoring</title>
      <link>https://arxiv.org/abs/2411.09726</link>
      <description>arXiv:2411.09726v2 Announce Type: replace 
Abstract: Thermal comfort is essential for well-being in urban spaces, especially as cities face increasing heat from urbanization and climate change. Existing thermal comfort models usually overlook temporal dynamics alongside spatial dependencies. We address this problem by introducing a spatio-temporal jump model that clusters data with persistence across both spatial and temporal dimensions. This framework enhances interpretability, minimizes abrupt state changes, and easily handles missing data. We validate our approach through extensive simulations, demonstrating its accuracy in recovering the true underlying partition. When applied to hourly environmental data gathered from a set of weather stations located across the city of Singapore, our proposal identifies meaningful thermal comfort regimes, demonstrating its effectiveness in dynamic urban settings and suitability for real-world monitoring. The comparison of these regimes with feedback on thermal preference indicates the potential of an unsupervised approach to avoid extensive surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09726v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Federico P. Cortese, Antonio Pievatolo</dc:creator>
    </item>
    <item>
      <title>Bayesian Quantile Regression with Subset Selection: A Decision Analysis Perspective</title>
      <link>https://arxiv.org/abs/2311.02043</link>
      <description>arXiv:2311.02043v4 Announce Type: replace-cross 
Abstract: Quantile regression is a powerful tool for inferring how covariates affect specific percentiles of the response distribution. Existing methods either estimate conditional quantiles separately for each quantile of interest or estimate the entire conditional distribution using semi- or non-parametric models. The former often produce inadequate models for real data and do not share information across quantiles, while the latter are characterized by complex and constrained models that can be difficult to interpret and computationally inefficient. Neither approach is well-suited for quantile-specific subset selection. Instead, we pose the fundamental problems of linear quantile estimation, uncertainty quantification, and subset selection from a Bayesian decision analysis perspective. For any Bayesian regression model -- including, but not limited to existing Bayesian quantile regression models -- we derive optimal point estimates, interpretable uncertainty quantification, and scalable subset selection techniques for all model-based conditional quantiles. Our approach introduces a quantile-focused squared error loss that enables efficient, closed-form computing and maintains a close relationship with Wasserstein-based density estimation. In an extensive simulation study, our methods demonstrate substantial gains in quantile estimation accuracy, inference, and variable selection over frequentist and Bayesian competitors. We use these tools to identify and quantify the heterogeneous impacts of multiple social stressors and environmental exposures on educational outcomes across the full spectrum of low-, medium-, and high-achieving students in North Carolina.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02043v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Feldman, Daniel Kowal</dc:creator>
    </item>
    <item>
      <title>SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2401.15299</link>
      <description>arXiv:2401.15299v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have gained traction across different domains such as transportation, bio-informatics, language processing, and computer vision. However, there is a noticeable absence of research on applying GNNs to supply chain networks. Supply chain networks are inherently graph-like in structure, making them prime candidates for applying GNN methodologies. This opens up a world of possibilities for optimizing, predicting, and solving even the most complex supply chain problems. A major setback in this approach lies in the absence of real-world benchmark datasets to facilitate the research and resolution of supply chain problems using GNNs. To address the issue, we present a real-world benchmark dataset for temporal tasks, obtained from one of the leading FMCG companies in Bangladesh, focusing on supply chain planning for production purposes. The dataset includes temporal data as node features to enable sales predictions, production planning, and the identification of factory issues. By utilizing this dataset, researchers can employ GNNs to address numerous supply chain problems, thereby advancing the field of supply chain analytics and planning. Source: https://github.com/CIOL-SUST/SupplyGraph</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15299v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Azmine Toushik Wasi, MD Shafikul Islam, Adipto Raihan Akib</dc:creator>
    </item>
    <item>
      <title>Two-Sided Flexibility in Platforms</title>
      <link>https://arxiv.org/abs/2404.04709</link>
      <description>arXiv:2404.04709v2 Announce Type: replace-cross 
Abstract: Flexibility is a cornerstone of operations management, crucial to hedge stochasticity in product demands, service requirements, and resource allocation. In two-sided platforms, flexibility is also two-sided and can be viewed as the compatibility of agents on one side with agents on the other side. Platform actions often influence the flexibility on either the demand or the supply side. But how should flexibility be jointly allocated across different sides? Whereas the literature has traditionally focused on only one side at a time, our work initiates the study of two-sided flexibility in matching platforms. We propose a parsimonious matching model in random graphs and identify the flexibility allocation that optimizes the expected size of a maximum matching. Our findings reveal that flexibility allocation is a first-order issue: for a given flexibility budget, the resulting matching size can vary greatly depending on how the budget is allocated. Moreover, even in the simple and symmetric settings we study, the quest for the optimal allocation is complicated. In particular, easy and costly mistakes can be made if the flexibility decisions on the demand and supply side are optimized independently (e.g., by two different teams in the company), rather than jointly. To guide the search for optimal flexibility allocation, we uncover two effects, flexibility cannibalization, and flexibility abundance, that govern when the optimal design places the flexibility budget only on one side or equally on both sides. In doing so we identify the study of two-sided flexibility as a significant aspect of platform efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04709v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Freund, S\'ebastien Martin, Jiayu Kamessi Zhao</dc:creator>
    </item>
    <item>
      <title>Gaussian Copula Models for Nonignorable Missing Data Using Auxiliary Marginal Quantiles</title>
      <link>https://arxiv.org/abs/2406.03463</link>
      <description>arXiv:2406.03463v2 Announce Type: replace-cross 
Abstract: We present an approach for modeling and imputation of nonignorable missing data. Our approach uses Bayesian data integration to combine (1) a Gaussian copula model for all study variables and missingness indicators, which allows arbitrary marginal distributions, nonignorable missingess, and other dependencies, and (2) auxiliary information in the form of marginal quantiles for some study variables. We prove that, remarkably, one only needs a small set of accurately-specified quantiles to estimate the copula correlation consistently. The remaining marginal distribution functions are inferred nonparametrically and jointly with the copula parameters using an efficient MCMC algorithm. We also characterize the (additive) nonignorable missingness mechanism implied by the copula model. Simulations confirm the effectiveness of this approach for multivariate imputation with nonignorable missing data. We apply the model to analyze associations between lead exposure and end-of-grade test scores for 170,000 North Carolina students. Lead exposure has nonignorable missingness: children with higher exposure are more likely to be measured. We elicit marginal quantiles for lead exposure using statistics provided by the Centers for Disease Control and Prevention. Multiple imputation inferences under our model support stronger, more adverse associations between lead exposure and educational outcomes relative to complete case and missing-at-random analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03463v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Feldman, Jerome P. Reiter, Daniel R. Kowal</dc:creator>
    </item>
    <item>
      <title>Valid standard errors for Bayesian quantile regression with clustered and independent data</title>
      <link>https://arxiv.org/abs/2407.09772</link>
      <description>arXiv:2407.09772v3 Announce Type: replace-cross 
Abstract: In Bayesian quantile regression, the most commonly used likelihood is the asymmetric Laplace (AL) likelihood. The reason for this choice is not that it is a plausible data-generating model but that the corresponding maximum likelihood estimator is identical to the classical estimator by Koenker and Bassett (1978), and in that sense, the AL likelihood can be thought of as a working likelihood. AL-based quantile regression has been shown to produce good finite-sample Bayesian point estimates and to be consistent. However, if the AL distribution does not correspond to the data-generating distribution, credible intervals based on posterior standard deviations can have poor coverage. Yang, Wang, and He (2016) proposed an adjustment to the posterior covariance matrix that produces asymptotically valid intervals. However, we show that this adjustment is sensitive to the choice of scale parameter for the AL likelihood and can lead to poor coverage when the sample size is small to moderate. We therefore propose using Infinitesimal Jackknife (IJ) standard errors (Giordano &amp; Broderick, 2023). These standard errors do not require resampling but can be obtained from a single MCMC run. We also propose a version of IJ standard errors for clustered data. Simulations and applications to real data show that the IJ standard errors have good frequentist properties, both for independent and clustered data. We provide an R-package, IJSE, that computes IJ standard errors for clustered or independent data after estimation with the brms wrapper in R for Stan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09772v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Ji, JoonHo Lee, Sophia Rabe-Hesketh</dc:creator>
    </item>
    <item>
      <title>Multiscale Multi-Type Spatial Bayesian Analysis for High-Dimensional Data with Application to Wildfires and Migration</title>
      <link>https://arxiv.org/abs/2410.02905</link>
      <description>arXiv:2410.02905v2 Announce Type: replace-cross 
Abstract: Wildfires have significantly increased in the United States (U.S.), making certain areas harder to live in. This motivates us to jointly analyze active fires and population changes in the U.S. from July 2020 to June 2021. The available data are recorded on different scales (or spatial resolutions) and by different types of distributions (referred to as multi-type data). Moreover, wildfires are known to have feedback mechanism that creates signal-to-noise dependence. We analyze point-referenced remote sensing fire data from National Aeronautics and Space Administration (NASA) and county-level population change data provided by U.S. Census Bureau's Population Estimates Program (PEP). We develop a multiscale multi-type spatial Bayesian model that assumes the average number of fires is zero-inflated normal, the incidence of fire as Bernoulli, and the percentage population change as normally distributed. This high-dimensional dataset makes Markov chain Monte Carlo (MCMC) implementation infeasible. We bypass MCMC by extending a recently introduced computationally efficient Bayesian framework to directly sample from the exact posterior distribution, which includes a term to model signal-to-noise dependence. Such signal-to-noise dependence is known to be present in wildfire data, but is commonly not accounted for. A simulation study is used to highlight the computational performance of our method. In our analysis, we obtained predictions of wildfire probabilities, identified several useful covariates, and found that regions with many fires were associated with population change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02905v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Zhou, Jonathan R. Bradley</dc:creator>
    </item>
    <item>
      <title>Zero-Coupon Treasury Yield Curve with VIX as Stochastic Volatility</title>
      <link>https://arxiv.org/abs/2411.03699</link>
      <description>arXiv:2411.03699v2 Announce Type: replace-cross 
Abstract: We study a multivariate autoregressive stochastic volatility model for the first 3 principal components (level, slope, curvature) of 10 series of zero-coupon Treasury bond rates with maturities from 1 to 10 years. We fit this model using monthly data from 1990. Next, we prove long-term stability for this discrete-time model and its continuous-time version. Unlike classic models with hidden stochastic volatility, here it is observed as VIX: the volatility index for the S\&amp;P 500 stock market index. It is surprising that this volatility, created for the stock market, also works for Treasury bonds. Since total returns of zero-coupon bonds can be easily found from these principal components, we prove long-term stability for total returns in discrete time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03699v2</guid>
      <category>q-fin.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jihyun Park, Andrey Sarantsev</dc:creator>
    </item>
  </channel>
</rss>
