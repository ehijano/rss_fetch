<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Oct 2024 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Navigating the Digital Chain in Concrete 3D Printing</title>
      <link>https://arxiv.org/abs/2410.16319</link>
      <description>arXiv:2410.16319v1 Announce Type: new 
Abstract: The advancement of concrete 3D printing (C3DP) technology has revolutionized the construction industry, offering unique opportunities for innovation and efficiency. At the heart of this process lies a comprehensive digital chain that integrates various stages, from initial design to post-processing. This article provides an overview of this digital chain, explaining each crucial step. The chain begins with design, utilizing Design for Additive Manufacturing (DFAM) concept and parametric modeling to create optimized structures. Path generation follows, determining the precise toolpath for extruding concrete layers. Simulations, both numerical and analytical, ensure the design's integrity and feasibility. Several articles have addressed parametric modeling, process and numerical simulation, and the post-processing phase. However, none has proposed an updated methodology for the workflow. This study aims to propose a robust digital chain for C3DP technology, using one platform (3Dexperience) and seamless data transfer between applications. These steps provide insights into the structural performance of printed components, enabling necessary adjustments and optimizations. In essence, the digital chain coordinates a seamless workflow that transforms digital designs into concrete structures, unlocking the full potential of C3DP and paving the way for innovative and efficient construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16319v1</guid>
      <category>cs.GR</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.24355/dbbs.084-202408130938-0</arxiv:DOI>
      <dc:creator>Ali El Hage (Nantes Univ - ECN), Elodie Paquet (Nantes Univ - ECN), Thibault Neu (Nantes Univ - ECN), Philippe Poullain (Nantes Univ - ECN), Ali-Nordine Leklou (Nantes Univ - ECN)</dc:creator>
    </item>
    <item>
      <title>Toroidal density-equalizing map for genus-one surfaces</title>
      <link>https://arxiv.org/abs/2410.16833</link>
      <description>arXiv:2410.16833v1 Announce Type: new 
Abstract: Density-equalizing map is a shape deformation technique originally developed for cartogram creation and sociological data visualization on planar geographical maps. In recent years, there has been an increasing interest in developing density-equalizing mapping methods for surface and volumetric domains and applying them to various problems in geometry processing and imaging science. However, the existing surface density-equalizing mapping methods are only applicable to surfaces with relatively simple topologies but not surfaces with topological holes. In this work, we develop a novel algorithm for computing density-equalizing maps for toroidal surfaces. In particular, different shape deformation effects can be easily achieved by prescribing different population functions on the torus and performing diffusion-based deformations on a planar domain with periodic boundary conditions. Furthermore, the proposed toroidal density-equalizing mapping method naturally leads to an effective method for computing toroidal parameterizations of genus-one surfaces with controllable shape changes, with the toroidal area-preserving parameterization being a prime example. Experimental results are presented to demonstrate the effectiveness of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16833v1</guid>
      <category>cs.GR</category>
      <category>cs.CG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunyu Yao, Gary P. T. Choi</dc:creator>
    </item>
    <item>
      <title>Toolpath Generation for High Density Spatial Fiber Printing Guided by Principal Stresses</title>
      <link>https://arxiv.org/abs/2410.16851</link>
      <description>arXiv:2410.16851v1 Announce Type: new 
Abstract: While multi-axis 3D printing can align continuous fibers along principal stresses in continuous fiber-reinforced thermoplastic (CFRTP) composites to enhance mechanical strength, existing methods have difficulty generating toolpaths with high fiber coverage. This is mainly due to the orientation consistency constraints imposed by vector-field-based methods and the turbulent stress fields around stress concentration regions. This paper addresses these challenges by introducing a 2-RoSy representation for computing the direction field, which is then converted into a periodic scalar field to generate partial iso-curves for fiber toolpaths with nearly equal hatching distance. To improve fiber coverage in stress-concentrated regions, such as around holes, we extend the quaternion-based method for curved slicing by incorporating winding compatibility considerations. Our proposed method can achieve toolpaths coverage between 87.5% and 90.6% by continuous fibers with 1.1mm width. Models fabricated using our toolpaths show up to 84.6% improvement in failure load and 54.4% increase in stiffness when compared to the results obtained from multi-axis 3D printing with sparser fibers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16851v1</guid>
      <category>cs.GR</category>
      <category>cs.CG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Tao Liu, Neelotpal Dutta, Yongxue Chen, Renbo Su, Zhizhou Zhang, Weiming Wang, Charlie C. L. Wang</dc:creator>
    </item>
    <item>
      <title>Multi-Layer Gaussian Splatting for Immersive Anatomy Visualization</title>
      <link>https://arxiv.org/abs/2410.16978</link>
      <description>arXiv:2410.16978v1 Announce Type: new 
Abstract: In medical image visualization, path tracing of volumetric medical data like CT scans produces lifelike three-dimensional visualizations. Immersive VR displays can further enhance the understanding of complex anatomies. Going beyond the diagnostic quality of traditional 2D slices, they enable interactive 3D evaluation of anatomies, supporting medical education and planning. Rendering high-quality visualizations in real-time, however, is computationally intensive and impractical for compute-constrained devices like mobile headsets.
  We propose a novel approach utilizing GS to create an efficient but static intermediate representation of CT scans. We introduce a layered GS representation, incrementally including different anatomical structures while minimizing overlap and extending the GS training to remove inactive Gaussians. We further compress the created model with clustering across layers.
  Our approach achieves interactive frame rates while preserving anatomical structures, with quality adjustable to the target hardware. Compared to standard GS, our representation retains some of the explorative qualities initially enabled by immersive path tracing. Selective activation and clipping of layers are possible at rendering time, adding a degree of interactivity to otherwise static GS models. This could enable scenarios where high computational demands would otherwise prohibit using path-traced medical volumes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16978v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Constantin Kleinbeck, Hannah Schieber, Klaus Engel, Ralf Gutjahr, Daniel Roth</dc:creator>
    </item>
    <item>
      <title>Recent consumer OLED monitors can be suitable for vision science</title>
      <link>https://arxiv.org/abs/2410.17019</link>
      <description>arXiv:2410.17019v1 Announce Type: new 
Abstract: Vision science imposes rigorous requirements for the design and execution of psychophysical studies and experiments. These requirements ensure precise control over variables, accurate measurement of perceptual responses, and reproducibility of results, which are essential for investigating visual perception and its underlying mechanisms. Since different experiments have different requirements, not all aspects of a display system are critical for a given setting. Therefore, some display systems may be suitable for certain types of experiments but unsuitable for others. An additional challenge is that the performance of consumer systems is often highly dependent on specific monitor settings and firmware behavior. Here, we evaluate the performance of four display systems: a consumer LCD gaming monitor, a consumer OLED gaming monitor, a consumer OLED TV, and a VPixx PROPixx projector system. To allow the reader to assess the suitability of these systems for different experiments, we present a range of different metrics: luminance behavior, luminance uniformity across display surface, estimated gamma values and linearity, channel additivity, channel dependency, color gamut, pixel response time, and pixel waveform. In addition, we exhaustively report the monitor firmware settings used. Our analyses show that current consumer-level OLED display systems are promising, and adequate to fulfill the requirements of some critical vision science experiments, allowing laboratories to run their experiments even without investing in high-quality professional display systems. For example, the tested Asus OLED gaming monitor shows excellent response time, a sharp square waveform even at 240 Hz, a color gamut that covers 94% of DCI-P3 color space, and the best luminance uniformity among all four tested systems, making it a favorable option on price-to-performance ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17019v1</guid>
      <category>cs.GR</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tarek Abu Haila, Korbinian Kunst, Tran Quoc Khanh, Thomas S. A. Wallis</dc:creator>
    </item>
    <item>
      <title>Joker: Conditional 3D Head Synthesis with Extreme Facial Expressions</title>
      <link>https://arxiv.org/abs/2410.16395</link>
      <description>arXiv:2410.16395v1 Announce Type: cross 
Abstract: We introduce Joker, a new method for the conditional synthesis of 3D human heads with extreme expressions. Given a single reference image of a person, we synthesize a volumetric human head with the reference identity and a new expression. We offer control over the expression via a 3D morphable model (3DMM) and textual inputs. This multi-modal conditioning signal is essential since 3DMMs alone fail to define subtle emotional changes and extreme expressions, including those involving the mouth cavity and tongue articulation. Our method is built upon a 2D diffusion-based prior that generalizes well to out-of-domain samples, such as sculptures, heavy makeup, and paintings while achieving high levels of expressiveness. To improve view consistency, we propose a new 3D distillation technique that converts predictions of our 2D prior into a neural radiance field (NeRF). Both the 2D prior and our distillation technique produce state-of-the-art results, which are confirmed by our extensive evaluations. Also, to the best of our knowledge, our method is the first to achieve view-consistent extreme tongue articulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16395v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malte Prinzler, Egor Zakharov, Vanessa Sklyarova, Berna Kabadayi, Justus Thies</dc:creator>
    </item>
    <item>
      <title>Polycubes via Dual Loops</title>
      <link>https://arxiv.org/abs/2410.16865</link>
      <description>arXiv:2410.16865v1 Announce Type: cross 
Abstract: We present a complete characterization of polycubes of any genus based on their dual structure: a collection of oriented loops which run in each of the axis directions and capture polycubes via their intersection patterns. A polycube loop structure uniquely corresponds to a polycube. We also describe all combinatorially different ways to add a loop to a loop structure while maintaining its validity; similarly, we show how to identify loops that can be removed from a polycube loop structure without invalidating it. Our characterization gives rise to an iterative algorithm to construct provably valid polycube-maps for a given input surface; polycube-maps are frequently used to solve texture mapping, spline fitting, and hexahedral meshing. We showcase some results of a proof-of-concept implementation of this iterative algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16865v1</guid>
      <category>cs.CG</category>
      <category>cs.GR</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxim Snoep, Bettina Speckmann, Kevin Verbeek</dc:creator>
    </item>
    <item>
      <title>An Eye for an AI: Evaluating GPT-4o's Visual Perception Skills and Geometric Reasoning Skills Using Computer Graphics Questions</title>
      <link>https://arxiv.org/abs/2410.16991</link>
      <description>arXiv:2410.16991v1 Announce Type: cross 
Abstract: CG (Computer Graphics) is a popular field of CS (Computer Science), but many students find this topic difficult due to it requiring a large number of skills, such as mathematics, programming, geometric reasoning, and creativity. Over the past few years, researchers have investigated ways to harness the power of GenAI (Generative Artificial Intelligence) to improve teaching. In CS, much of the research has focused on introductory computing. A recent study evaluating the performance of an LLM (Large Language Model), GPT-4 (text-only), on CG questions, indicated poor performance and reliance on detailed descriptions of image content, which often required considerable insight from the user to return reasonable results. So far, no studies have investigated the abilities of LMMs (Large Multimodal Models), or multimodal LLMs, to solve CG questions and how these abilities can be used to improve teaching.
  In this study, we construct two datasets of CG questions requiring varying degrees of visual perception skills and geometric reasoning skills, and evaluate the current state-of-the-art LMM, GPT-4o, on the two datasets. We find that although GPT-4o exhibits great potential in solving questions with visual information independently, major limitations still exist to the accuracy and quality of the generated results. We propose several novel approaches for CG educators to incorporate GenAI into CG teaching despite these limitations. We hope that our guidelines further encourage learning and engagement in CG classrooms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16991v1</guid>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3680533.3697064</arxiv:DOI>
      <dc:creator>Tony Haoran Feng (University of Auckland), Paul Denny (University of Auckland), Burkhard C. W\"unsche (University of Auckland), Andrew Luxton-Reilly (University of Auckland), Jacqueline Whalley (Auckland University of Technology)</dc:creator>
    </item>
    <item>
      <title>LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias</title>
      <link>https://arxiv.org/abs/2410.17242</link>
      <description>arXiv:2410.17242v1 Announce Type: cross 
Abstract: We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods -- from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps) -- addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality. Notably, our models surpass all previous methods even with reduced computational resources (1-2 GPUs). Please see our website for more details: https://haian-jin.github.io/projects/LVSM/ .</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17242v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, Zexiang Xu</dc:creator>
    </item>
    <item>
      <title>LLM Gesticulator: Leveraging Large Language Models for Scalable and Controllable Co-Speech Gesture Synthesis</title>
      <link>https://arxiv.org/abs/2410.10851</link>
      <description>arXiv:2410.10851v2 Announce Type: replace 
Abstract: In this work, we present LLM Gesticulator, an LLM-based audio-driven co-speech gesture generation framework that synthesizes full-body animations that are rhythmically aligned with the input audio while exhibiting natural movements and editability. Compared to previous work, our model demonstrates substantial scalability. As the size of the backbone LLM model increases, our framework shows proportional improvements in evaluation metrics (a.k.a. scaling law). Our method also exhibits strong controllability where the content, style of the generated gestures can be controlled by text prompt. To the best of our knowledge, LLM gesticulator is the first work that use LLM on the co-speech generation task. Evaluation with existing objective metrics and user studies indicate that our framework outperforms prior works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10851v2</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haozhou Pang, Tianwei Ding, Lanshan He, Ming Tao, Lu Zhang, Qi Gan</dc:creator>
    </item>
    <item>
      <title>Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models</title>
      <link>https://arxiv.org/abs/2409.10695</link>
      <description>arXiv:2409.10695v2 Announce Type: replace-cross 
Abstract: We introduce Playground v3 (PGv3), our latest text-to-image model that achieves state-of-the-art (SoTA) performance across multiple testing benchmarks, excels in graphic design abilities and introduces new capabilities. Unlike traditional text-to-image generative models that rely on pre-trained language models like T5 or CLIP text encoders, our approach fully integrates Large Language Models (LLMs) with a novel structure that leverages text conditions exclusively from a decoder-only LLM. Additionally, to enhance image captioning quality-we developed an in-house captioner, capable of generating captions with varying levels of detail, enriching the diversity of text structures. We also introduce a new benchmark CapsBench to evaluate detailed image captioning performance. Experimental results demonstrate that PGv3 excels in text prompt adherence, complex reasoning, and accurate text rendering. User preference studies indicate the super-human graphic design ability of our model for common design applications, such as stickers, posters, and logo designs. Furthermore, PGv3 introduces new capabilities, including precise RGB color control and robust multilingual understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10695v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, Daiqing Li</dc:creator>
    </item>
    <item>
      <title>Fully Explicit Dynamic Gaussian Splatting</title>
      <link>https://arxiv.org/abs/2410.15629</link>
      <description>arXiv:2410.15629v2 Announce Type: replace-cross 
Abstract: 3D Gaussian Splatting has shown fast and high-quality rendering results in static scenes by leveraging dense 3D prior and explicit representations. Unfortunately, the benefits of the prior and representation do not involve novel view synthesis for dynamic motions. Ironically, this is because the main barrier is the reliance on them, which requires increasing training and rendering times to account for dynamic motions. In this paper, we design a Explicit 4D Gaussian Splatting(Ex4DGS). Our key idea is to firstly separate static and dynamic Gaussians during training, and to explicitly sample positions and rotations of the dynamic Gaussians at sparse timestamps. The sampled positions and rotations are then interpolated to represent both spatially and temporally continuous motions of objects in dynamic scenes as well as reducing computational cost. Additionally, we introduce a progressive training scheme and a point-backtracking technique that improves Ex4DGS's convergence. We initially train Ex4DGS using short timestamps and progressively extend timestamps, which makes it work well with a few point clouds. The point-backtracking is used to quantify the cumulative error of each Gaussian over time, enabling the detection and removal of erroneous Gaussians in dynamic scenes. Comprehensive experiments on various scenes demonstrate the state-of-the-art rendering quality from our method, achieving fast rendering of 62 fps on a single 2080Ti GPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15629v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junoh Lee, Chang-Yeon Won, Hyunjun Jung, Inhwan Bae, Hae-Gon Jeon</dc:creator>
    </item>
  </channel>
</rss>
