<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Mar 2025 01:53:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>VeriContaminated: Assessing LLM-Driven Verilog Coding for Data Contamination</title>
      <link>https://arxiv.org/abs/2503.13572</link>
      <description>arXiv:2503.13572v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized code generation, achieving exceptional results on various established benchmarking frameworks. However, concerns about data contamination - where benchmark data inadvertently leaks into pre-training or fine-tuning datasets - raise questions about the validity of these evaluations. While this issue is known, limiting the industrial adoption of LLM-driven software engineering, hardware coding has received little to no attention regarding these risks. For the first time, we analyze state-of-the-art (SOTA) evaluation frameworks for Verilog code generation (VerilogEval and RTLLM), using established methods for contamination detection (CCD and Min-K% Prob). We cover SOTA commercial and open-source LLMs (CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1}, GPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5), in baseline and fine-tuned models (RTLCoder and Verigen). Our study confirms that data contamination is a critical concern. We explore mitigations and the resulting trade-offs for code quality vs fairness (i.e., reducing contamination toward unbiased benchmarking).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13572v1</guid>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeng Wang, Minghao Shao, Jitendra Bhandari, Likhitha Mankali, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique, Johann Knechtel</dc:creator>
    </item>
    <item>
      <title>FlexStep: Enabling Flexible Error Detection in Multi/Many-core Real-time Systems</title>
      <link>https://arxiv.org/abs/2503.13848</link>
      <description>arXiv:2503.13848v1 Announce Type: new 
Abstract: Reliability and real-time responsiveness in safety-critical systems have traditionally been achieved using error detection mechanisms, such as LockStep, which require pre-configured checker cores,strict synchronisation between main and checker cores, static error detection regions, or limited preemption capabilities. However, these core-bound hardware mechanisms often lead to significant resource over-provisioning, and diminished real-time responsiveness, particularly in modern systems where tasks with varying reliability requirements are consolidated on shared processors to improve efficiency, reduce costs, and save power. To address these challenges, this work presents FlexStep, a systematic solution that integrates hardware and software across the SoC, ISA, and OS scheduling layers. FlexStep features a novel microarchitecture that supports dynamic core configuration and asynchronous, preemptive error detection. The FlexStep architecture naturally allows for flexible task scheduling and error detection, enabling new scheduling algorithms that enhance both resource efficiency and real-time schedulability. We publicly release FlexStep's source code, at https://anonymous.4open.science/r/FlexStep-DAC25-7B0C.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13848v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tinglue Wang, Yiming Li, Wei Tang, Jiapeng Guan, Zhenghui Guo, Renshuang Jiang, Ran Wei, Jing Li, Zhe Jiang</dc:creator>
    </item>
    <item>
      <title>Streamlining SIMD ISA Extensions with Takum Arithmetic: A Case Study on Intel AVX10.2</title>
      <link>https://arxiv.org/abs/2503.14067</link>
      <description>arXiv:2503.14067v1 Announce Type: new 
Abstract: Modern microprocessors extend their instruction set architecture (ISA) with Single Instruction, Multiple Data (SIMD) operations to improve performance. The Intel Advanced Vector Extensions (AVX) enhance the x86 ISA and are widely supported in Intel and AMD processors. The latest version, AVX10.2, places a strong emphasis on low-precision, non-standard floating-point formats, including bfloat16 and E4M3/E5M2 float8 (OCP 8-bit Floating Point, OFP8), primarily catering to deep learning applications rather than general-purpose arithmetic. However, as these formats remain within the IEEE 754 framework, they inherit its limitations, introducing inconsistencies and added complexity into the ISA.
  This paper examines the recently proposed tapered-precision takum floating-point format, which has been shown to offer significant advantages over IEEE 754 and its derivatives as a general-purpose number format. Using AVX10.2 as a case study, the paper explores the potential benefits of replacing the multitude of floating-point formats with takum as a uniform basis. The results indicate a more consistent instruction set, improving readability and flexibility while offering potential for 8- and 16-bit general-purpose SIMD arithmetic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14067v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laslo Hunhold</dc:creator>
    </item>
    <item>
      <title>Retrospective: A CORDIC Based Configurable Activation Function for NN Applications</title>
      <link>https://arxiv.org/abs/2503.14354</link>
      <description>arXiv:2503.14354v1 Announce Type: new 
Abstract: A CORDIC-based configuration for the design of Activation Functions (AF) was previously suggested to accelerate ASIC hardware design for resource-constrained systems by providing functional reconfigurability. Since its introduction, this new approach for neural network acceleration has gained widespread popularity, influencing numerous designs for activation functions in both academic and commercial AI processors. In this retrospective analysis, we explore the foundational aspects of this initiative, summarize key developments over recent years, and introduce the DA-VINCI AF tailored for the evolving needs of AI applications. This new generation of dynamically configurable and precision-adjustable activation function cores promise greater adaptability for a range of activation functions in AI workloads, including Swish, SoftMax, SeLU, and GeLU, utilizing the Shift-and-Add CORDIC technique. The previously presented design has been optimized for MAC, Sigmoid, and Tanh functionalities and incorporated into ReLU AFs, culminating in an accumulative NEURIC compute unit. These enhancements position NEURIC as a fundamental component in the resource-efficient vector engine for the realization of AI accelerators that focus on DNNs, RNNs/LSTMs, and Transformers, achieving a quality of results (QoR) of 98.5%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14354v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <category>eess.IV</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omkar Kokane, Gopal Raut, Salim Ullah, Mukul Lokhande, Adam Teman, Akash Kumar, Santosh Kumar Vishvakarma</dc:creator>
    </item>
    <item>
      <title>A Modular Edge Device Network for Surgery Digitalization</title>
      <link>https://arxiv.org/abs/2503.14049</link>
      <description>arXiv:2503.14049v1 Announce Type: cross 
Abstract: Future surgical care demands real-time, integrated data to drive informed decision-making and improve patient outcomes. The pressing need for seamless and efficient data capture in the OR motivates our development of a modular solution that bridges the gap between emerging machine learning techniques and interventional medicine. We introduce a network of edge devices, called Data Hubs (DHs), that interconnect diverse medical sensors, imaging systems, and robotic tools via optical fiber and a centralized network switch. Built on the NVIDIA Jetson Orin NX, each DH supports multiple interfaces (HDMI, USB-C, Ethernet) and encapsulates device-specific drivers within Docker containers using the Isaac ROS framework and ROS2. A centralized user interface enables straightforward configuration and real-time monitoring, while an Nvidia DGX computer provides state-of-the-art data processing and storage. We validate our approach through an ultrasound-based 3D anatomical reconstruction experiment that combines medical imaging, pose tracking, and RGB-D data acquisition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14049v1</guid>
      <category>eess.SY</category>
      <category>cs.AR</category>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Schorp, Fr\'ed\'eric Giraud, Gianluca Parg\"atzi, Michael W\"aspe, Lorenzo von Ritter-Zahony, Marcel Wegmann, John Garcia Henao, Dominique Cachin, Sebastiano Caprara, Philipp F\"urnstahl, Fabio Carrillo</dc:creator>
    </item>
    <item>
      <title>Speculative Decoding for Verilog: Speed and Quality, All in One</title>
      <link>https://arxiv.org/abs/2503.14153</link>
      <description>arXiv:2503.14153v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) has revolutionized code generation tasks across various programming languages. However, the unique characteristics of programming languages, particularly those like Verilog with specific syntax and lower representation in training datasets, pose significant challenges for conventional tokenization and decoding approaches. In this paper, we introduce a novel application of speculative decoding for Verilog code generation, showing that it can improve both inference speed and output quality, effectively achieving speed and quality all in one. Unlike standard LLM tokenization schemes, which often fragment meaningful code structures, our approach aligns decoding stops with syntactically significant tokens, making it easier for models to learn the token distribution. This refinement addresses inherent tokenization issues and enhances the model's ability to capture Verilog's logical constructs more effectively. Our experimental results show that our method achieves up to a 5.05x speedup in Verilog code generation and increases pass@10 functional accuracy on RTLLM by up to 17.19% compared to conventional training strategies. These findings highlight speculative decoding as a promising approach to bridge the quality gap in code generation for specialized programming languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14153v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>cs.CL</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changran Xu, Yi Liu, Yunhao Zhou, Shan Huang, Ningyi Xu, Qiang Xu</dc:creator>
    </item>
    <item>
      <title>RapidChiplet: A Toolchain for Rapid Design Space Exploration of Chiplet Architectures</title>
      <link>https://arxiv.org/abs/2311.06081</link>
      <description>arXiv:2311.06081v2 Announce Type: replace 
Abstract: Chiplet architectures are on the rise as they promise to overcome the scaling challenges of monolithic chips. A key component of such architectures is an efficient inter-chiplet interconnect (ICI). The ICI design space is huge as there are many degrees of freedom such as the number, size, and placement of chiplets, the topology and bandwidth of links, the packaging technology, and many more. While ICI simulators are important to get reliable performance estimates, they are not fast enough to explore hundreds of thousands of design points or to be used as a cost function for optimization algorithms or machine learning models. To address this issue, we present RapidChiplet, a fast and easy to use ICI latency and throughput prediction toolchain. Compared to cycle-level simulations, we trade 0.25%-30.15% of accuracy for 427x-137,682x speedup.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06081v2</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Iff, Benigna Bruggmann, Blaise Morel, Maciej Besta, Luca Benini, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>NVR: Vector Runahead on NPUs for Sparse Memory Access</title>
      <link>https://arxiv.org/abs/2502.13873</link>
      <description>arXiv:2502.13873v2 Announce Type: replace 
Abstract: Deep Neural Networks are increasingly leveraging sparsity to reduce the scaling up of model parameter size. However, reducing wall-clock time through sparsity and pruning remains challenging due to irregular memory access patterns, leading to frequent cache misses. In this paper, we present NPU Vector Runahead (NVR), a prefetching mechanism tailored for NPUs to address cache miss problems in sparse DNN workloads. Rather than optimising memory patterns with high overhead and poor portability, NVR adapts runahead execution to the unique architecture of NPUs. NVR provides a general micro-architectural solution for sparse DNN workloads without requiring compiler or algorithmic support, operating as a decoupled, speculative, lightweight hardware sub-thread alongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an average 90% reduction in cache misses compared to SOTA prefetching in general-purpose processors, delivering 4x average speedup on sparse workloads versus NPUs without prefetching. Moreover, we investigate the advantages of incorporating a small cache (16KB) into the NPU combined with NVR. Our evaluation shows that expanding this modest cache delivers 5x higher performance benefits than increasing the L2 cache size by the same amount.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13873v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hui Wang, Zhengpeng Zhao, Jing Wang, Yushu Du, Yuan Cheng, Bing Guo, He Xiao, Chenhao Ma, Xiaomeng Han, Dean You, Jiapeng Guan, Ran Wei, Dawei Yang, Zhe Jiang</dc:creator>
    </item>
    <item>
      <title>Efficient Hardware Accelerator Based on Medium Granularity Dataflow for SpTRSV</title>
      <link>https://arxiv.org/abs/2406.10511</link>
      <description>arXiv:2406.10511v3 Announce Type: replace-cross 
Abstract: Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous studies have been conducted using CPUs, GPUs, and specific hardware accelerators, where dataflows can be categorized into coarse and fine granularity. Coarse dataflows offer good spatial locality but suffer from low parallelism, while fine dataflows provide high parallelism but disrupt the spatial structure, leading to increased nodes and poor data reuse. This paper proposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The accelerator implements a medium granularity dataflow through hardware-software codesign and achieves both excellent spatial locality and high parallelism. Additionally, a partial sum caching mechanism is introduced to reduce the blocking frequency of processing elements (PEs), and a reordering algorithm of intra-node edges computation is developed to enhance data reuse. Experimental results on 245 benchmarks with node counts reaching up to 85,392 demonstrate that this work achieves average performance improvements of 7.0$\times$ (up to 27.8$\times$) over CPUs and 5.8$\times$ (up to 98.8$\times$) over GPUs. Compared to the state-of-the-art technique (DPU-v2), this work shows a 2.5$\times$ (up to 5.9$\times$) average performance improvement and 1.7$\times$ (up to 4.1$\times$) average energy efficiency enhancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10511v3</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <category>math.NA</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TVLSI.2024.3497166</arxiv:DOI>
      <arxiv:journal_reference>IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 33 (2025) 807-820</arxiv:journal_reference>
      <dc:creator>Qian Chen, Xiaofeng Yang, Shengli Lu</dc:creator>
    </item>
    <item>
      <title>FQsun: A Configurable Wave Function-Based Quantum Emulator for Power-Efficient Quantum Simulations</title>
      <link>https://arxiv.org/abs/2411.04471</link>
      <description>arXiv:2411.04471v2 Announce Type: replace-cross 
Abstract: Quantum computers are promising powerful computers for solving complex problems, but access to real quantum hardware remains limited due to high costs. Although the software simulators on CPUs/GPUs such as Qiskit, ProjectQ, and Qsun offer flexibility and support for many qubits, they struggle with high power consumption and limited processing speed, especially as qubit counts scale. Accordingly, quantum emulators implemented on dedicated hardware, such as FPGAs and analog circuits, offer a promising path for addressing energy efficiency concerns. However, existing studies on hardware-based emulators still face challenges in terms of limited flexibility and lack of fidelity evaluation. To overcome these gaps, we propose FQsun, a quantum emulator that enhances performance by integrating four key innovations: efficient memory organization, a configurable Quantum Gate Unit (QGU), optimized scheduling, and multiple number precisions. Five FQsun versions with different number precisions are implemented on the Xilinx ZCU102, consuming a maximum power of 2.41W. Experimental results demonstrate high fidelity, low mean square error, and high normalized gate speed, particularly with 32-bit versions, establishing FQsun's capability as a precise quantum emulator. Benchmarking on famous quantum algorithms reveals that FQsun achieves a superior power-delay product, outperforming software simulators on CPUs in the processing speed range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04471v2</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuan Hai Vu, Vu Trung Duong Le, Hoai Luan Pham, Quoc Chuong Nguyen, Yasuhiko Nakashima</dc:creator>
    </item>
  </channel>
</rss>
