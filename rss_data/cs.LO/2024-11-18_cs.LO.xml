<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Nov 2024 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Exact Computation of Error in Approximate Circuits using SAT and Message-Passing Algorithms</title>
      <link>https://arxiv.org/abs/2411.10037</link>
      <description>arXiv:2411.10037v1 Announce Type: new 
Abstract: Effective usage of approximate circuits for various performance trade-offs requires accurate computation of error. Several average and worst case error metrics have been proposed in the literature. We propose a framework for exact computation of these error metrics, including the error rate (ER), mean absolute error (MAE), mean squared error (MSE) and the worst-case error (WCE). We use a combination of SAT and message-passing algorithms. Our algorithm takes as input the CNF formula for the exact and approximate circuits followed by a subtractor that finds the difference of the two outputs. This is converted into a tree, with each vertex of the tree associated with a sub-formulas and all satisfying solutions to it. Once this is done, any probability can be computed by setting appropriate error bits and using a message passing algorithm on the tree. Since message-passing is fast, besides ER and MAE, computation of metrics like MSE is also very efficient. In fact, it is possible to get the entire probability distribution of the error. Besides standard benchmarks, we could compute the error metrics exactly for approximate Gaussian and Sobel filters, which has not been done previously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10037v1</guid>
      <category>cs.LO</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>S Ramprasath, Marrivada Gopala Krishna Sai Charan, Vinita Vasudevan</dc:creator>
    </item>
    <item>
      <title>Optimally Rewriting Formulas and Database Queries: A Confluence of Term Rewriting, Structural Decomposition, and Complexity</title>
      <link>https://arxiv.org/abs/2411.10229</link>
      <description>arXiv:2411.10229v1 Announce Type: new 
Abstract: A central computational task in database theory, finite model theory, and computer science at large is the evaluation of a first-order sentence on a finite structure. In the context of this task, the \emph{width} of a sentence, defined as the maximum number of free variables over all subformulas, has been established as a crucial measure, where minimizing width of a sentence (while retaining logical equivalence) is considered highly desirable. An undecidability result rules out the possibility of an algorithm that, given a first-order sentence, returns a logically equivalent sentence of minimum width; this result motivates the study of width minimization via syntactic rewriting rules, which is this article's focus. For a number of common rewriting rules (which are known to preserve logical equivalence), including rules that allow for the movement of quantifiers, we present an algorithm that, given a positive first-order sentence $\phi$, outputs the minimum-width sentence obtainable from $\phi$ via application of these rules. We thus obtain a complete algorithmic understanding of width minimization up to the studied rules; this result is the first one -- of which we are aware -- that establishes this type of understanding in such a general setting. Our result builds on the theory of term rewriting and establishes an interface among this theory, query evaluation, and structural decomposition theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10229v1</guid>
      <category>cs.LO</category>
      <category>cs.DB</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hubie Chen, Stefan Mengel</dc:creator>
    </item>
    <item>
      <title>Guaranteed Bounds on Posterior Distributions of Discrete Probabilistic Programs with Loops</title>
      <link>https://arxiv.org/abs/2411.10393</link>
      <description>arXiv:2411.10393v1 Announce Type: cross 
Abstract: We study the problem of bounding the posterior distribution of discrete probabilistic programs with unbounded support, loops, and conditioning. Loops pose the main difficulty in this setting: even if exact Bayesian inference is possible, the state of the art requires user-provided loop invariant templates. By contrast, we aim to find guaranteed bounds, which sandwich the true distribution. They are fully automated, applicable to more programs and provide more provable guarantees than approximate sampling-based inference. Since lower bounds can be obtained by unrolling loops, the main challenge is upper bounds, and we attack it in two ways. The first is called residual mass semantics, which is a flat bound based on the residual probability mass of a loop. The approach is simple, efficient, and has provable guarantees.
  The main novelty of our work is the second approach, called geometric bound semantics. It operates on a novel family of distributions, called eventually geometric distributions (EGDs), and can bound the distribution of loops with a new form of loop invariants called contraction invariants. The invariant synthesis problem reduces to a system of polynomial inequality constraints, which is a decidable problem with automated solvers. If a solution exists, it yields an exponentially decreasing bound on the whole distribution, and can therefore bound moments and tail asymptotics as well, not just probabilities as in the first approach.
  Both semantics enjoy desirable theoretical properties. In particular, we prove soundness and convergence, i.e. the bounds converge to the exact posterior as loops are unrolled further. On the practical side, we describe Diabolo, a fully-automated implementation of both semantics, and evaluate them on a variety of benchmarks from the literature, demonstrating their general applicability and the utility of the resulting bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10393v1</guid>
      <category>cs.PL</category>
      <category>cs.DM</category>
      <category>cs.LO</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Zaiser, Andrzej S. Murawski, C. -H. Luke Ong</dc:creator>
    </item>
    <item>
      <title>Tighter Bounds for Query Answering with Guarded TGDs</title>
      <link>https://arxiv.org/abs/2212.11362</link>
      <description>arXiv:2212.11362v3 Announce Type: replace 
Abstract: We consider the complexity of the open-world query answering problem, where we wish to determine certain answers to conjunctive queries over incomplete datasets specified by an initial set of facts and a set of guarded TGDs. This problem has been well-studied in the literature and is decidable but with a high complexity, namely, it is 2EXPTIME complete. Further, the complexity shrinks by one exponential when the arity is fixed.
  We show in this paper how we can obtain better complexity bounds when considering separately the arity of the guard atom and that of the additional atoms, called the side signature. Our results make use of the technique of linearizing guarded TGDs, introduced in Gottlob, Manna, and Pieris. Specifically, we present a variant of the linearization process, making use of a restricted version of the chase that we recently introduced. Our results imply that open-world query answering with guarded TGDs can be solved in EXPTIME with arbitrary-arity guard relations if we simply bound the arity of the side signature; and that the complexity drops to NP if we fix the side signature and bound the width of the dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.11362v3</guid>
      <category>cs.LO</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Amarilli, Michael Benedikt</dc:creator>
    </item>
    <item>
      <title>Monoid Theory in Alonzo: A Little Theories Formalization in Simple Type Theory</title>
      <link>https://arxiv.org/abs/2312.05658</link>
      <description>arXiv:2312.05658v2 Announce Type: replace 
Abstract: Alonzo is a practice-oriented classical higher-order logic that extends first-order logic and that admits undefined expressions. Named in honor of Alonzo Church, Alonzo is based on Church's type theory, Church's formulation of simple type theory. The little theories method is a method for formalizing mathematical knowledge as a network of theories called a theory graph consisting of theories as nodes and theory morphisms as directed edges. The development of a mathematical topic is done in the "little theory" in the theory graph that has the most convenient level of abstraction and the most convenient vocabulary, and then the definitions and theorems produced in the development are transported, as needed, to other theories via the theory morphisms in the theory graph. The purpose of this paper is to illustrate how a body of mathematical knowledge can be formalized in Alonzo using the little theories method. This is done by formalizing monoid theory -- the body of mathematical knowledge about monoids -- in Alonzo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05658v2</guid>
      <category>cs.LO</category>
      <category>math.LO</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William M. Farmer, Dennis Y. Zvigelsky</dc:creator>
    </item>
    <item>
      <title>Alternating Quantifiers in Uniform One-Dimensional Fragments with an Excursion into Three-Variable Logic</title>
      <link>https://arxiv.org/abs/2404.03377</link>
      <description>arXiv:2404.03377v2 Announce Type: replace 
Abstract: The uniform one-dimensional fragment of first-order logic was introduced a few years ago as a generalization of the two-variable fragment to contexts involving relations of arity greater than two. Quantifiers in this logic are used in blocks, each block consisting only of existential quantifiers or only of universal quantifiers. In this paper we consider the possibility of mixing both types of quantifiers in blocks. We show the finite (exponential) model property and NExpTime-completeness of the satisfiability problem for two restrictions of the resulting formalism: in the first we require that every block of quantifiers is either purely universal or ends with the existential quantifier, in the second we restrict the number of variables to three; in both equality is not allowed. We also extend the second variation to a rich subfragment of the three-variable fragment (without equality) that still has the finite model property and decidable, NExpTime{}-complete satisfiability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03377v2</guid>
      <category>cs.LO</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oskar Fiuk, Emanuel Kieronski</dc:creator>
    </item>
  </channel>
</rss>
