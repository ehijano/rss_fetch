<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Aug 2025 02:21:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>HePGA: A Heterogeneous Processing-in-Memory based GNN Training Accelerator</title>
      <link>https://arxiv.org/abs/2508.16011</link>
      <description>arXiv:2508.16011v1 Announce Type: new 
Abstract: Processing-In-Memory (PIM) architectures offer a promising approach to accelerate Graph Neural Network (GNN) training and inference. However, various PIM devices such as ReRAM, FeFET, PCM, MRAM, and SRAM exist, with each device offering unique trade-offs in terms of power, latency, area, and non-idealities. A heterogeneous manycore architecture enabled by 3D integration can combine multiple PIM devices on a single platform, to enable energy-efficient and high-performance GNN training. In this work, we propose a 3D heterogeneous PIM-based accelerator for GNN training referred to as HePGA. We leverage the unique characteristics of GNN layers and associated computing kernels to optimize their mapping on to different PIM devices as well as planar tiers. Our experimental analysis shows that HePGA outperforms existing PIM-based architectures by up to 3.8x and 6.8x in energy-efficiency (TOPS/W) and compute efficiency (TOPS/mm2) respectively, without sacrificing the GNN prediction accuracy. Finally, we demonstrate the applicability of HePGA to accelerate inferencing of emerging transformer models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16011v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chukwufumnanya Ogbogu, Gaurav Narang, Biresh Kumar Joardar, Janardhan Rao Doppa, Krishnendu Chakrabarty, Partha Pratim Pande</dc:creator>
    </item>
    <item>
      <title>Set Transformer Architectures and Synthetic Data Generation for Flow-Guided Nanoscale Localization</title>
      <link>https://arxiv.org/abs/2508.16200</link>
      <description>arXiv:2508.16200v1 Announce Type: new 
Abstract: Flow-guided Localization (FGL) enables the identification of spatial regions within the human body that contain an event of diagnostic interest. FGL does that by leveraging the passive movement of energy-constrained nanodevices circulating through the bloodstream. Existing FGL solutions rely on graph models with fixed topologies or handcrafted features, which limit their adaptability to anatomical variability and hinder scalability. In this work, we explore the use of Set Transformer architectures to address these limitations. Our formulation treats nanodevices' circulation time reports as unordered sets, enabling permutation-invariant, variable-length input processing without relying on spatial priors. To improve robustness under data scarcity and class imbalance, we integrate synthetic data generation via deep generative models, including CGAN, WGAN, WGAN-GP, and CVAE. These models are trained to replicate realistic circulation time distributions conditioned on vascular region labels, and are used to augment the training data. Our results show that the Set Transformer achieves comparable classification accuracy compared to Graph Neural Networks (GNN) baselines, while simultaneously providing by-design improved generalization to anatomical variability. The findings highlight the potential of permutation-invariant models and synthetic augmentation for robust and scalable nanoscale localization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16200v1</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mika Leo Hube, Filip Lemic, Ethungshan Shitiri, Gerard Calvo Bartra, Sergi Abadal, Xavier Costa P\'erez</dc:creator>
    </item>
    <item>
      <title>Energy-Information Trade-Off in Self-Directed Channel Memristors</title>
      <link>https://arxiv.org/abs/2508.16236</link>
      <description>arXiv:2508.16236v1 Announce Type: new 
Abstract: Understanding the nature of information storage on memristors is vital to enable their use in novel data storage and neuromorphic applications. One key consideration in information storage is the energy cost of storage and what impact the available energy has on the information capacity of the devices. In this paper, we propose and study an energy-information trade-off for a particular kind of memristive device - Self-Directed Channel (SDC) memristors. We perform experiments to model the energy required to set the devices into various states, as well as assessing the stability of these states over time. Based on these results, we employ a generative modelling approach, using a conditional Generative Adversarial Network (cGAN) to characterise the storage conditional distribution, allowing us to estimate energy-information curves for a range of storage delays, showing the graceful trade-off between energy consumed and the effective capacity of the devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16236v1</guid>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <category>physics.app-ph</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Waleed El-Geresy, D\'aniel Hajt\'o, Gy\"orgy Cserey, Deniz G\"und\"uz</dc:creator>
    </item>
    <item>
      <title>VR Fire safety training application</title>
      <link>https://arxiv.org/abs/2508.15788</link>
      <description>arXiv:2508.15788v1 Announce Type: cross 
Abstract: Fire emergencies can happen without warning and knowing how to respond quickly can save lives Unfortunately traditional fire drills can be disruptive costly and often fail to recreate the pressure of a real emergency This project introduces a Virtual Reality VR Fire Safety Training Application that gives people a safe yet realistic way to practice life saving skills Using a VR headset and motion controllers trainees step into a 3D world where fire hazards smoke and evacuation routes are brought to life They can learn how to use a fire extinguisher find safe exits and make decisions under pressure without any real danger The training adapts to the users skill level and tracks progress making it useful for beginners and experienced personnel alike By turning fire safety into an interactive experience this VR approach boosts confidence improves retention and makes learning both safer and more engaging</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15788v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ujwal M R</dc:creator>
    </item>
    <item>
      <title>An Auditable Pipeline for Fuzzy Full-Text Screening in Systematic Reviews: Integrating Contrastive Semantic Highlighting and LLM Judgment</title>
      <link>https://arxiv.org/abs/2508.15822</link>
      <description>arXiv:2508.15822v1 Announce Type: cross 
Abstract: Full-text screening is the major bottleneck of systematic reviews (SRs), as decisive evidence is dispersed across long, heterogeneous documents and rarely admits static, binary rules. We present a scalable, auditable pipeline that reframes inclusion/exclusion as a fuzzy decision problem and benchmark it against statistical and crisp baselines in the context of the Population Health Modelling Consensus Reporting Network for noncommunicable diseases (POPCORN). Articles are parsed into overlapping chunks and embedded with a domain-adapted model; for each criterion (Population, Intervention, Outcome, Study Approach), we compute contrastive similarity (inclusion-exclusion cosine) and a vagueness margin, which a Mamdani fuzzy controller maps into graded inclusion degrees with dynamic thresholds in a multi-label setting. A large language model (LLM) judge adjudicates highlighted spans with tertiary labels, confidence scores, and criterion-referenced rationales; when evidence is insufficient, fuzzy membership is attenuated rather than excluded. In a pilot on an all-positive gold set (16 full texts; 3,208 chunks), the fuzzy system achieved recall of 81.3% (Population), 87.5% (Intervention), 87.5% (Outcome), and 75.0% (Study Approach), surpassing statistical (56.3-75.0%) and crisp baselines (43.8-81.3%). Strict "all-criteria" inclusion was reached for 50.0% of articles, compared to 25.0% and 12.5% under the baselines. Cross-model agreement on justifications was 98.3%, human-machine agreement 96.1%, and a pilot review showed 91% inter-rater agreement (kappa = 0.82), with screening time reduced from about 20 minutes to under 1 minute per article at significantly lower cost. These results show that fuzzy logic with contrastive highlighting and LLM adjudication yields high recall, stable rationale, and end-to-end traceability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15822v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pouria Mortezaagha, Arya Rahgozar</dc:creator>
    </item>
    <item>
      <title>EGaIn tube memristors offer reliable switching on a biological time scale</title>
      <link>https://arxiv.org/abs/2508.15950</link>
      <description>arXiv:2508.15950v1 Announce Type: cross 
Abstract: Memristive devices have been considered promising candidates for nature-inspired computing and in-memory information processing. However, experimental devices developed to date typically show significant variability and function at different time scales than biological neurons and synapses. This study presents a new kind of memristive device comprised of liquid-metal eutectic gallium indium (EGaIn) contained within a mm-scale tube that operates via a bulk, voltage-dependent switching mechanism and exhibits distinct unipolar resistive switching characteristics that occur on a biological time scale (tens of milliseconds). The switching mechanism involves voltage-controlled growth and dissolution of an oxide layer on the surface of the liquid metal in contact with an aqueous electrolyte. Through comprehensive measurements on many devices, we observed remarkably consistent cycle-to-cycle behavior and uniformity in the voltage-controlled memristance. We present our findings, which also include an experimental demonstration of logic gates utilizing EGaIn tube memristors. Furthermore, we observe both accelerated and decelerated switching behaviors and identify signatures indicative of a fractional dynamic response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15950v1</guid>
      <category>physics.app-ph</category>
      <category>cs.ET</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuriy V. Pershin, Liya Patel, Bapi Berra, Doug Aaron, Stephen A. Sarles</dc:creator>
    </item>
    <item>
      <title>Scalable Equilibrium Propagation via Intermediate Error Signals for Deep Convolutional CRNNs</title>
      <link>https://arxiv.org/abs/2508.15989</link>
      <description>arXiv:2508.15989v1 Announce Type: cross 
Abstract: Equilibrium Propagation (EP) is a biologically inspired local learning rule first proposed for convergent recurrent neural networks (CRNNs), in which synaptic updates depend only on neuron states from two distinct phases. EP estimates gradients that closely align with those computed by Backpropagation Through Time (BPTT) while significantly reducing computational demands, positioning it as a potential candidate for on-chip training in neuromorphic architectures. However, prior studies on EP have been constrained to shallow architectures, as deeper networks suffer from the vanishing gradient problem, leading to convergence difficulties in both energy minimization and gradient computation. To address the vanishing gradient problem in deep EP networks, we propose a novel EP framework that incorporates intermediate error signals to enhance information flow and convergence of neuron dynamics. This is the first work to integrate knowledge distillation and local error signals into EP, enabling the training of significantly deeper architectures. Our proposed approach achieves state-of-the-art performance on the CIFAR-10 and CIFAR-100 datasets, showcasing its scalability on deep VGG architectures. These results represent a significant advancement in the scalability of EP, paving the way for its application in real-world systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15989v1</guid>
      <category>cs.LG</category>
      <category>cs.ET</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqi Lin, Malyaban Bal, Abhronil Sengupta</dc:creator>
    </item>
    <item>
      <title>Machine Learning in Micromobility: A Systematic Review of Datasets, Techniques, and Applications</title>
      <link>https://arxiv.org/abs/2508.16135</link>
      <description>arXiv:2508.16135v1 Announce Type: cross 
Abstract: Micromobility systems, which include lightweight and low-speed vehicles such as bicycles, e-bikes, and e-scooters, have become an important part of urban transportation and are used to solve problems such as traffic congestion, air pollution, and high transportation costs. Successful utilisation of micromobilities requires optimisation of complex systems for efficiency, environmental impact mitigation, and overcoming technical challenges for user safety. Machine Learning (ML) methods have been crucial to support these advancements and to address their unique challenges. However, there is insufficient literature addressing the specific issues of ML applications in micromobilities. This survey paper addresses this gap by providing a comprehensive review of datasets, ML techniques, and their specific applications in micromobilities. Specifically, we collect and analyse various micromobility-related datasets and discuss them in terms of spatial, temporal, and feature-based characteristics. In addition, we provide a detailed overview of ML models applied in micromobilities, introducing their advantages, challenges, and specific use cases. Furthermore, we explore multiple ML applications, such as demand prediction, energy management, and safety, focusing on improving efficiency, accuracy, and user experience. Finally, we propose future research directions to address these issues, aiming to help future researchers better understand this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16135v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>eess.IV</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sen Yan, Chinmaya Kaundanya, Noel E. O'Connor, Suzanne Little, Mingming Liu</dc:creator>
    </item>
    <item>
      <title>Hybrid Classical-Quantum Supercomputing: A demonstration of a multi-user, multi-QPU and multi-GPU environment</title>
      <link>https://arxiv.org/abs/2508.16297</link>
      <description>arXiv:2508.16297v1 Announce Type: cross 
Abstract: Achieving a practical quantum advantage for near-term applications is widely expected to rely on hybrid classical-quantum algorithms. To deliver this practical advantage to users, high performance computing (HPC) centers need to provide a suitable software and hardware stack that supports algorithms of this type. In this paper, we describe the world's first implementation of a classical-quantum environment in an HPC center that allows multiple users to execute hybrid algorithms on multiple quantum processing units (QPUs) and GPUs. Our setup at the Poznan Supercomputing and Networking Center (PCSS) aligns with current HPC norms: the computing hardware including QPUs is installed in an active data center room with standard facilities; there are no special considerations for networking, power, and cooling; we use Slurm for workload management as well as the NVIDIA CUDA-Q extension API for classical-quantum interactions. We demonstrate applications of this environment for hybrid classical-quantum machine learning and optimisation. The aim of this work is to provide the community with an experimental example for further research and development on how quantum computing can practically enhance and extend HPC capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16297v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mateusz Slysz, Piotr Rydlichowski, Krzysztof Kurowski, Omar Bacarezza, Esperanza Cuenca Gomez, Zohim Chandani, Bettina Heim, Pradnya Khalate, William R. Clements, James Fletcher</dc:creator>
    </item>
    <item>
      <title>Cyber Physical Awareness via Intent-Driven Threat Assessment: Enhanced Space Networks with Intershell Links</title>
      <link>https://arxiv.org/abs/2508.16314</link>
      <description>arXiv:2508.16314v1 Announce Type: cross 
Abstract: This letter addresses essential aspects of threat assessment by proposing intent-driven threat models that incorporate both capabilities and intents. We propose a holistic framework for cyber physical awareness (CPA) in space networks, pointing out that analyzing reliability and security separately can lead to overfitting on system-specific criteria. We structure our proposed framework in three main steps. First, we suggest an algorithm that extracts characteristic properties of the received signal to facilitate an intuitive understanding of potential threats. Second, we develop a multitask learning architecture where one task evaluates reliability-related capabilities while the other deciphers the underlying intentions of the signal. Finally, we propose an adaptable threat assessment that aligns with varying security and reliability requirements. The proposed framework enhances the robustness of threat detection and assessment, outperforming conventional sequential methods, and enables space networks with emerging intershell links to effectively address complex threat scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16314v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/LWC.2025.3593066</arxiv:DOI>
      <dc:creator>Selen Gecgel Cetin, Tolga Ovatman, Gunes Karabulut Kurt</dc:creator>
    </item>
    <item>
      <title>TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine</title>
      <link>https://arxiv.org/abs/2508.16553</link>
      <description>arXiv:2508.16553v1 Announce Type: cross 
Abstract: In the context of industry 4.0, long-serving industrial machines can be retrofitted with process monitoring capabilities for future use in a smart factory. One possible approach is the deployment of wireless monitoring systems, which can benefit substantially from the TinyML paradigm. This work presents a complete TinyML flow from dataset generation, to machine learning model development, up to implementation and evaluation of a full preprocessing and classification pipeline on a microcontroller. After a short review on TinyML in industrial process monitoring, the creation of the novel MillingVibes dataset is described. The feasibility of a TinyML system for structure-integrated process quality monitoring could be shown by the development of an 8-bit-quantized convolutional neural network (CNN) model with 12.59kiB parameter storage. A test accuracy of 100.0% could be reached at 15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex M4F microcontroller, serving as a reference for future TinyML process monitoring solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16553v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tim Langer, Matthias Widra, Volkhard Beyer</dc:creator>
    </item>
  </channel>
</rss>
