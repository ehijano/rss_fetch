<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Mar 2025 04:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Conceptual Entity-Relationship Model: Underneath the Simplicity and Staticity</title>
      <link>https://arxiv.org/abs/2503.06155</link>
      <description>arXiv:2503.06155v1 Announce Type: new 
Abstract: This paper deals with the issue of conceptual models role in capturing semantics and aligning them to serve the remaining development phases of systems design. Specifically, the entity-relationship (ER) model is selected as an example of conceptual representation that serves this purpose in building relational database systems. It is claimed that ER diagrams provide a solid basis for subsequent technical implementation. The ER model appeal relies on its simplicity and its benefit in clarifying the requirements for databases. Nevertheless, some researchers have observed that this reduction of complexity is accompanied by oversimplification and overlooking dynamism. Accordingly, complaints have risen about the lack of direct compatibility between ER modeling and relational model. This paper is an attempt to explore what is beneath this static ER simplicity and its role as a base for subsequent technical implementation. In this undertaking, we use thinging machines (TMs), where modeling is constructed upon a single notion thimac (thing/machine). Thimac constituents are formed from the makeup of five actions, create, process, release, transfer, and receive that inject dynamism alongside with structure. The ER entities, attributes, and relationship are modeled as thimacs. Accordingly, in this paper, ER examples are remodeled in TM while identifying TM portions that correspond to ER components. The resulting TM model insets actions into entities, attributes and relationships. In this case, relationships are the products of creating linking thimacs plus the logic of constructing them. Based on such static/dynamic TM representation, the modeler can produce any level of simplification, including the original ER model. In conclusion, results indicated that the TM models facilitate multilevel simplicity and viable direct compatibility with the relational database model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06155v1</guid>
      <category>cs.DB</category>
      <category>cs.SE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sabah Al-Fedaghi</dc:creator>
    </item>
    <item>
      <title>VerIso: Verifiable Isolation Guarantees for Database Transactions</title>
      <link>https://arxiv.org/abs/2503.06284</link>
      <description>arXiv:2503.06284v1 Announce Type: new 
Abstract: Isolation bugs, stemming especially from design-level defects, have been repeatedly found in carefully designed and extensively tested production databases over decades. In parallel, various frameworks for modeling database transactions and reasoning about their isolation guarantees have been developed. What is missing however is a mathematically rigorous and systematic framework with tool support for formally verifying a wide range of such guarantees for all possible system behaviors. We present the first such framework, VerIso, developed within the theorem prover Isabelle/HOL. To showcase its use in verification, we model the strict two-phase locking concurrency control protocol and verify that it provides strict serializability isolation guarantee. Moreover, we show how VerIso helps identify isolation bugs during protocol design. We derive new counterexamples for the TAPIR protocol from failed attempts to prove its claimed strict serializability. In particular, we show that it violates a much weaker isolation level, namely, atomic visibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06284v1</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.LO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shabnam Ghasemirad, Si Liu, Christoph Sprenger, Luca Multazzu, David Basin</dc:creator>
    </item>
    <item>
      <title>Approximate Hausdorff Distance for Multi-Vector Databases</title>
      <link>https://arxiv.org/abs/2503.06833</link>
      <description>arXiv:2503.06833v1 Announce Type: new 
Abstract: The Hausdorff distance is a fundamental measure for comparing sets of vectors, widely used in database theory and geometric algorithms. However, its exact computation is computationally expensive, often making it impractical for large-scale applications such as multi-vector databases. In this paper, we introduce an approximation framework that efficiently estimates the Hausdorff distance while maintaining rigorous error bounds. Our approach leverages approximate nearest-neighbor (ANN) search to construct a surrogate function that preserves essential geometric properties while significantly reducing computational complexity. We provide a formal analysis of approximation accuracy, deriving both worst-case and expected error bounds. Additionally, we establish theoretical guarantees on the stability of our method under transformations, including translation, rotation, and scaling, and quantify the impact of non-uniform scaling on approximation quality. This work provides a principled foundation for integrating Hausdorff distance approximations into large-scale data retrieval and similarity search applications, ensuring both computational efficiency and theoretical correctness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06833v1</guid>
      <category>cs.DB</category>
      <category>cs.CG</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dongfang Zhao</dc:creator>
    </item>
    <item>
      <title>Maximum Inner Product is Query-Scaled Nearest Neighbor</title>
      <link>https://arxiv.org/abs/2503.06882</link>
      <description>arXiv:2503.06882v1 Announce Type: new 
Abstract: Maximum Inner Product Search (MIPS) for high-dimensional vectors is pivotal across databases, information retrieval, and artificial intelligence. Existing methods either reduce MIPS to Nearest Neighbor Search (NNS) while suffering from harmful vector space transformations, or attempt to tackle MIPS directly but struggle to mitigate redundant computations due to the absence of the triangle inequality. This paper presents a novel theoretical framework that equates MIPS with NNS without requiring space transformation, thereby allowing us to leverage advanced graph-based indices for NNS and efficient edge pruning strategies, significantly reducing unnecessary computations. Despite a strong baseline set by our theoretical analysis, we identify and address two persistent challenges to further refine our method: the introduction of the Proximity Graph with Spherical Pathway (PSP), designed to mitigate the issue of MIPS solutions clustering around large-norm vectors, and the implementation of Adaptive Early Termination (AET), which efficiently curtails the excessive exploration once an accuracy bottleneck is reached. Extensive experiments reveal the superiority of our method over existing state-of-the-art techniques in search efficiency, scalability, and practical applicability. Compared with state-of-the-art graph based methods, it achieves an average 35% speed-up in query processing and a 3x reduction in index size. Notably, our approach has been validated and deployed in the search engines of Shopee, a well-known online shopping platform. Our code and an industrial-scale dataset for offline evaluation will also be released to address the absence of e-commerce data in public benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06882v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tingyang Chen, Cong Fu, Kun Wang, Xiangyu Ke, Yunjun Gao, Wenchao Zhou, Yabo Ni, Anxiang Zeng</dc:creator>
    </item>
    <item>
      <title>A Query Optimization Method Utilizing Large Language Models</title>
      <link>https://arxiv.org/abs/2503.06902</link>
      <description>arXiv:2503.06902v1 Announce Type: new 
Abstract: Query optimization is a critical task in database systems, focused on determining the most efficient way to execute a query from an enormous set of possible strategies. Traditional approaches rely on heuristic search methods and cost predictions, but these often struggle with the complexity of the search space and inaccuracies in performance estimation, leading to suboptimal plan choices. This paper presents LLMOpt, a novel framework that leverages Large Language Models (LLMs) to address these challenges through two innovative components: (1) LLM for Plan Candidate Generation (LLMOpt(G)), which eliminates heuristic search by utilizing the reasoning abilities of LLMs to directly generate high-quality query plans, and (2) LLM for Plan Candidate Selection (LLMOpt(S)), a list-wise cost model that compares candidates globally to enhance selection accuracy. To adapt LLMs for query optimization, we propose fine-tuning pre-trained models using optimization data collected offline. Experimental results on the JOB, JOB-EXT, and Stack benchmarks show that LLMOpt(G) and LLMOpt(S) outperform state-of-the-art methods, including PostgreSQL, BAO, and HybridQO. Notably, LLMOpt(S) achieves the best practical performance, striking a balance between plan quality and inference efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06902v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiming Yao, Haoyang Li, Jing Zhang, Cuiping Li, Hong Chen</dc:creator>
    </item>
    <item>
      <title>Bag Semantics Query Containment: The CQ vs. UCQ Case and Other Stories</title>
      <link>https://arxiv.org/abs/2503.07219</link>
      <description>arXiv:2503.07219v1 Announce Type: new 
Abstract: Query Containment Problem (QCP) is a fundamental decision problem in query processing and optimization. While QCP has for a long time been completely understood for the case of set semantics, decidability of QCP for conjunctive queries under multi-set semantics (QCPbag(CQ)) remains one of the most intriguing open problems in database theory. Certain effort has been put, in last 30 years, to solve this problem and some decidable special cases of QCPbag(CQ) were identified, as well as some undecidable extensions, including QCPbag(UCQ). In this paper we introduce a new technique which produces, for a given UCQ {\Phi}, a CQ {\phi} such that the application of {\phi} to a database D is, in some sense, an approximation of the application of {\Phi} to D. Using this technique we could analyze the status of QCPbag when one of the queries in question is a CQ and the other is a UCQ, and we reached conclusions which surprised us a little bit. We also tried to use this technique to translate the known undecidability proof for QCPbag(UCQ) into a proof of undecidability of QCPbag(CQ). And, as you are going to see, we got stopped just one infinitely small {\epsilon} before reaching this ultimate goal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07219v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jerzy Marcinkowski, Piotr Ostropolski-Nalewaja</dc:creator>
    </item>
    <item>
      <title>GeoJEPA: Towards Eliminating Augmentation- and Sampling Bias in Multimodal Geospatial Learning</title>
      <link>https://arxiv.org/abs/2503.05774</link>
      <description>arXiv:2503.05774v1 Announce Type: cross 
Abstract: Existing methods for self-supervised representation learning of geospatial regions and map entities rely extensively on the design of pretext tasks, often involving augmentations or heuristic sampling of positive and negative pairs based on spatial proximity. This reliance introduces biases and limits the representations' expressiveness and generalisability. Consequently, the literature has expressed a pressing need to explore different methods for modelling geospatial data. To address the key difficulties of such methods, namely multimodality, heterogeneity, and the choice of pretext tasks, we present GeoJEPA, a versatile multimodal fusion model for geospatial data built on the self-supervised Joint-Embedding Predictive Architecture. With GeoJEPA, we aim to eliminate the widely accepted augmentation- and sampling biases found in self-supervised geospatial representation learning. GeoJEPA uses self-supervised pretraining on a large dataset of OpenStreetMap attributes, geometries and aerial images. The results are multimodal semantic representations of urban regions and map entities that we evaluate both quantitatively and qualitatively. Through this work, we uncover several key insights into JEPA's ability to handle multimodal data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05774v1</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theodor Lundqvist, Ludvig Delvret</dc:creator>
    </item>
    <item>
      <title>CtrTab: Tabular Data Synthesis with High-Dimensional and Limited Data</title>
      <link>https://arxiv.org/abs/2503.06444</link>
      <description>arXiv:2503.06444v1 Announce Type: cross 
Abstract: Diffusion-based tabular data synthesis models have yielded promising results. However, we observe that when the data dimensionality increases, existing models tend to degenerate and may perform even worse than simpler, non-diffusion-based models. This is because limited training samples in high-dimensional space often hinder generative models from capturing the distribution accurately. To address this issue, we propose CtrTab-a condition controlled diffusion model for tabular data synthesis-to improve the performance of diffusion-based generative models in high-dimensional, low-data scenarios. Through CtrTab, we inject samples with added Laplace noise as control signals to improve data diversity and show its resemblance to L2 regularization, which enhances model robustness. Experimental results across multiple datasets show that CtrTab outperforms state-of-the-art models, with performance gap in accuracy over 80% on average. Our source code will be released upon paper publication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06444v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zuqing Li, Jianzhong Qi, Junhao Gan</dc:creator>
    </item>
    <item>
      <title>Roq: Robust Query Optimization Based on a Risk-aware Learned Cost Model</title>
      <link>https://arxiv.org/abs/2401.15210</link>
      <description>arXiv:2401.15210v2 Announce Type: replace 
Abstract: Query optimizers in RDBMSs search for execution plans expected to be optimal for given queries. They use parameter estimates, often inaccurate, and make assumptions that may not hold in practice. Consequently, they may select plans that are suboptimal at runtime, if estimates and assumptions are not valid. Therefore, they do not sufficiently support robust query optimization. Using ML to improve data systems has shown promising results for query optimization. Inspired by this, we propose Robust Query Optimizer, (Roq), a holistic framework based on a risk-aware learning approach. Roq includes a novel formalization of the notion of robustness in the context of query optimization and a principled approach for its quantification and measurement based on approximate probabilistic ML. It also includes novel strategies and algorithms for query plan evaluation and selection. Roq includes a novel learned cost model that is designed to predict the cost of query execution and the associated risks and performs query optimization accordingly. We demonstrate that Roq provides significant improvements in robust query optimization compared with the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15210v2</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amin Kamali, Verena Kantere, Calisto Zuzarte, Vincent Corvinelli</dc:creator>
    </item>
    <item>
      <title>Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL</title>
      <link>https://arxiv.org/abs/2501.12372</link>
      <description>arXiv:2501.12372v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks. In particular, improvements in reasoning abilities and the expansion of context windows have opened new avenues for leveraging these powerful models. NL2SQL is challenging in that the natural language question is inherently ambiguous, while the SQL generation requires a precise understanding of complex data schema and semantics. One approach to this semantic ambiguous problem is to provide more and sufficient contextual information.
  In this work, we explore the performance and the latency trade-offs of the extended context window (a.k.a., long context) offered by Google's state-of-the-art LLM (\textit{gemini-1.5-pro}). We study the impact of various contextual information, including column example values, question and SQL query pairs, user-provided hints, SQL documentation, and schema. To the best of our knowledge, this is the first work to study how the extended context window and extra contextual information can help NL2SQL generation with respect to both accuracy and latency cost. We show that long context LLMs are robust and do not get lost in the extended contextual information. Additionally, our long-context NL2SQL pipeline based on Google's \textit{gemini-pro-1.5} achieve strong performances on various benchmark datasets without finetuning and expensive self-consistency based techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12372v4</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, Fatma Ozcan</dc:creator>
    </item>
    <item>
      <title>Principles for Open Data Curation: A Case Study with the New York City 311 Service Request Data</title>
      <link>https://arxiv.org/abs/2502.08649</link>
      <description>arXiv:2502.08649v2 Announce Type: replace 
Abstract: In the early 21st century, the open data movement began to transform societies and governments by promoting transparency, innovation, and public engagement. The City of New York (NYC) has been at the forefront of this movement since the enactment of the Open Data Law in 2012, creating the NYC Open Data portal. The portal currently hosts 2,700 datasets, serving as a crucial resource for research across various domains, including health, urban development, and transportation. However, the effective use of open data relies heavily on data quality and usability, challenges that remain insufficiently addressed in the literature. This paper examines these challenges via a case study of the NYC 311 Service Request dataset, identifying key issues in data validity, consistency, and curation efficiency. We propose a set of data curation principles, tailored for government-released open data, to address these challenges. Our findings highlight the importance of harmonized field definitions, streamlined storage, and automated quality checks, offering practical guidelines for improving the reliability and utility of open datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08649v2</guid>
      <category>cs.DB</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Tussey, Jun Yan</dc:creator>
    </item>
    <item>
      <title>From Commands to Prompts: LLM-based Semantic File System for AIOS</title>
      <link>https://arxiv.org/abs/2410.11843</link>
      <description>arXiv:2410.11843v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated significant potential in the development of intelligent applications and systems such as LLM-based agents and agent operating systems (AIOS). However, when these applications and systems interact with the underlying file system, the file system still remains the traditional paradigm: reliant on manual navigation through precise commands. This paradigm poses a bottleneck to the usability of these systems as users are required to navigate complex folder hierarchies and remember cryptic file names. To address this limitation, we propose an LLM-based semantic file system ( LSFS ) for prompt-driven file management. Unlike conventional approaches, LSFS incorporates LLMs to enable users or agents to interact with files through natural language prompts, facilitating semantic file management. At the macro-level, we develop a comprehensive API set to achieve semantic file management functionalities, such as semantic file retrieval, file update monitoring and summarization, and semantic file rollback). At the micro-level, we store files by constructing semantic indexes for them, design and implement syscalls of different semantic operations (e.g., CRUD, group by, join) powered by vector database. Our experiments show that LSFS offers significant improvements over traditional file systems in terms of user convenience, the diversity of supported functions, and the accuracy and efficiency of file operations. Additionally, with the integration of LLM, our system enables more intelligent file management tasks, such as content summarization and version comparison, further enhancing its capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11843v4</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ICLR2025</arxiv:journal_reference>
      <dc:creator>Zeru Shi, Kai Mei, Mingyu Jin, Yongye Su, Chaoji Zuo, Wenyue Hua, Wujiang Xu, Yujie Ren, Zirui Liu, Mengnan Du, Dong Deng, Yongfeng Zhang</dc:creator>
    </item>
    <item>
      <title>Parser Knows Best: Testing DBMS with Coverage-Guided Grammar-Rule Traversal</title>
      <link>https://arxiv.org/abs/2503.03893</link>
      <description>arXiv:2503.03893v2 Announce Type: replace-cross 
Abstract: Database Management System (DBMS) is the key component for data-intensive applications. Recently, researchers propose many tools to comprehensively test DBMS systems for finding various bugs. However, these tools only cover a small subset of diverse syntax elements defined in DBMS-specific SQL dialects, leaving a large number of features unexplored. In this paper, we propose ParserFuzz, a novel fuzzing framework that automatically extracts grammar rules from DBMSs' built-in syntax definition files for SQL query generation. Without any input corpus, ParserFuzz can generate diverse query statements to saturate the grammar features of the tested DBMSs, which grammar features could be missed by previous tools. Additionally, ParserFuzz utilizes code coverage as feedback to guide the query mutation, which combines different DBMS features extracted from the syntax rules to find more function and safety bugs. In our evaluation, ParserFuzz outperforms all state-of-the-art existing DBMS testing tools in terms of bug finding, grammar rule coverage and code coverage. ParserFuzz detects 81 previously unknown bugs in total across 5 popular DBMSs, where all bugs are confirmed and 34 have been fixed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03893v2</guid>
      <category>cs.CR</category>
      <category>cs.DB</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Liang, Hong Hu</dc:creator>
    </item>
  </channel>
</rss>
