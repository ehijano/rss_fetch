<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Mar 2024 04:03:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 11 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Shapes as Product Differentiation: Neural Network Embedding in the Analysis of Markets for Fonts</title>
      <link>https://arxiv.org/abs/2107.02739</link>
      <description>arXiv:2107.02739v2 Announce Type: replace 
Abstract: Many differentiated products have key attributes that are unstructured and thus high-dimensional (e.g., design, text). Instead of treating unstructured attributes as unobservables in economic models, quantifying them can be important to answer interesting economic questions. To propose an analytical framework for these types of products, this paper considers one of the simplest design products-fonts-and investigates merger and product differentiation using an original dataset from the world's largest online marketplace for fonts. We quantify font shapes by constructing embeddings from a deep convolutional neural network. Each embedding maps a font's shape onto a low-dimensional vector. In the resulting product space, designers are assumed to engage in Hotelling-type spatial competition. From the image embeddings, we construct two alternative measures that capture the degree of design differentiation. We then study the causal effects of a merger on the merging firm's creative decisions using the constructed measures in a synthetic control method. We find that the merger causes the merging firm to increase the visual variety of font design. Notably, such effects are not captured when using traditional measures for product offerings (e.g., specifications and the number of products) constructed from structured data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.02739v2</guid>
      <category>econ.EM</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sukjin Han, Eric H. Schulman, Kristen Grauman, Santhosh Ramakrishnan</dc:creator>
    </item>
    <item>
      <title>Automatic Debiased Machine Learning via Riesz Regression</title>
      <link>https://arxiv.org/abs/2104.14737</link>
      <description>arXiv:2104.14737v2 Announce Type: replace-cross 
Abstract: A variety of interesting parameters may depend on high dimensional regressions. Machine learning can be used to estimate such parameters. However estimators based on machine learners can be severely biased by regularization and/or model selection. Debiased machine learning uses Neyman orthogonal estimating equations to reduce such biases. Debiased machine learning generally requires estimation of unknown Riesz representers. A primary innovation of this paper is to provide Riesz regression estimators of Riesz representers that depend on the parameter of interest, rather than explicit formulae, and that can employ any machine learner, including neural nets and random forests. End-to-end algorithms emerge where the researcher chooses the parameter of interest and the machine learner and the debiasing follows automatically. Another innovation here is debiased machine learners of parameters depending on generalized regressions, including high-dimensional generalized linear models. An empirical example of automatic debiased machine learning using neural nets is given. We find in Monte Carlo examples that automatic debiasing sometimes performs better than debiasing via inverse propensity scores and never worse. Finite sample mean square error bounds for Riesz regression estimators and asymptotic theory are also given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.14737v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Whitney K. Newey, Victor Quintas-Martinez, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>An Optimal Transport Approach to Estimating Causal Effects via Nonlinear Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2108.05858</link>
      <description>arXiv:2108.05858v2 Announce Type: replace-cross 
Abstract: We propose a nonlinear difference-in-differences method to estimate multivariate counterfactual distributions in classical treatment and control study designs with observational data. Our approach sheds a new light on existing approaches like the changes-in-changes and the classical semiparametric difference-in-differences estimator and generalizes them to settings with multivariate heterogeneity in the outcomes. The main benefit of this extension is that it allows for arbitrary dependence and heterogeneity in the joint outcomes. We demonstrate its utility both on synthetic and real data. In particular, we revisit the classical Card \&amp; Krueger dataset, examining the effect of a minimum wage increase on employment in fast food restaurants; a reanalysis with our method reveals that restaurants tend to substitute full-time with part-time labor after a minimum wage increase at a faster pace. A previous version of this work was entitled "An optimal transport approach to causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.05858v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Torous, Florian Gunsilius, Philippe Rigollet</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Non-Parametric Sequential Tests and Confidence Sequences with Possibly Dependent Observations</title>
      <link>https://arxiv.org/abs/2212.14411</link>
      <description>arXiv:2212.14411v4 Announce Type: replace-cross 
Abstract: Sequential testing, always-valid $p$-values, and confidence sequences promise flexible statistical inference and on-the-fly decision making. However, unlike fixed-$n$ inference based on asymptotic normality, existing sequential tests either make parametric assumptions and end up under-covering/over-rejecting when these fail or use non-parametric but conservative concentration inequalities and end up over-covering/under-rejecting. To circumvent these issues, we sidestep exact at-least-$\alpha$ coverage and focus on asymptotic calibration and asymptotic optimality. That is, we seek sequential tests whose probability of \emph{ever} rejecting a true hypothesis approaches $\alpha$ and whose expected time to reject a false hypothesis approaches a lower bound on all such asymptotically calibrated tests, both "approaches" occurring under an appropriate limit. We permit observations to be both non-parametric and dependent and focus on testing whether the observations form a martingale difference sequence. We propose the universal sequential probability ratio test (uSPRT), a slight modification to the normal-mixture sequential probability ratio test, where we add a burn-in period and adjust thresholds accordingly. We show that even in this very general setting, the uSPRT is asymptotically optimal under mild generic conditions. We apply the results to stabilized estimating equations to test means, treatment effects, {\etc} Our results also provide corresponding guarantees for the implied confidence sequences. Numerical simulations verify our guarantees and the benefits of the uSPRT over alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.14411v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aurelien Bibaut, Nathan Kallus, Michael Lindon</dc:creator>
    </item>
  </channel>
</rss>
