<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Oct 2024 04:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Graphical models for inference: A model comparison approach for analyzing bacterial conjugation</title>
      <link>https://arxiv.org/abs/2410.03814</link>
      <description>arXiv:2410.03814v1 Announce Type: new 
Abstract: We present a proof-of-concept of a model comparison approach for analyzing spatio-temporal observations of interacting populations. Our model variants are a collection of structurally similar Bayesian networks. Their distinct Noisy-Or conditional probability distributions describe interactions within the population, with each distribution corresponding to a specific mechanism of interaction. To determine which distributions most accurately represent the underlying mechanisms, we examine the accuracy of each Bayesian network with respect to observational data. We implement such a system for observations of bacterial populations engaged in conjugation, a type of horizontal gene transfer that allows microbes to share genetic material with nearby cells through physical contact. Evaluating cell-specific factors that affect conjugation is generally difficult because of the stochastic nature of the process. Our approach provides a new method for gaining insight into this process. We compare eight model variations for each of three experimental trials and rank them using two different metrics</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03814v1</guid>
      <category>stat.ME</category>
      <category>q-bio.CB</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nat Kendal-Freedman, Joseph Victor Fiorillo Meleshko, Aaron Yip, Brian Ingalls</dc:creator>
    </item>
    <item>
      <title>Efficient Bayesian Additive Regression Models For Microbiome Studies</title>
      <link>https://arxiv.org/abs/2410.03911</link>
      <description>arXiv:2410.03911v1 Announce Type: new 
Abstract: Statistical analysis of microbiome data is challenging. Bayesian multinomial logistic-normal (MLN) models have gained popularity due to their ability to account for the count compositional nature of these data. However, these models are often computationally intractable to infer. Recently, we developed a computationally efficient and accurate approach to inferring MLN models with a Marginally Latent Matrix-T Process (MLTP) form: MLN-MLTPs. Our approach is based on a novel sampler with a marginal Laplace approximation -- called the \textit{Collapse-Uncollapse} (CU) sampler. However, existing work with MLTPs has been limited to linear models or models of a single non-linear process. Moreover, existing methods lack an efficient means of estimating model hyperparameters. This article addresses both deficiencies. We introduce a new class of MLN Additive Gaussian Process models (\textit{MultiAddGPs}) for deconvolution of overlapping linear and non-linear processes. We show that MultiAddGPs are examples of MLN-MLTPs and derive an efficient CU sampler for this model class. Moreover, we derive efficient Maximum Marginal Likelihood estimation for hyperparameters in MLTP models by taking advantage of Laplace approximation in the CU sampler. We demonstrate our approach using simulated and real data studies. Our models produce novel biological insights from a previously published artificial gut study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03911v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tinghua Chen, Michelle Pistner Nixon, Justin D. Silverman</dc:creator>
    </item>
    <item>
      <title>"6 choose 4": A framework to understand and facilitate discussion of strategies for overall survival safety monitoring</title>
      <link>https://arxiv.org/abs/2410.04020</link>
      <description>arXiv:2410.04020v1 Announce Type: new 
Abstract: Advances in anticancer therapies have significantly contributed to declining death rates in certain disease and clinical settings. However, they have also made it difficult to power a clinical trial in these settings with overall survival (OS) as the primary efficacy endpoint. A number of statistical approaches have therefore been recently proposed for the pre-specified analysis of OS as a safety endpoint (Shan, 2023; Fleming et al., 2024; Rodriguez et al., 2024). In this paper, we provide a simple, unifying framework that includes the aforementioned approaches (and a couple others) as special cases. By highlighting each approach's focus, priority, tolerance for risk, and strengths or challenges for practical implementation, this framework can help to facilitate discussions between stakeholders on "fit-for-purpose OS data collection and assessment of harm" (American Association for Cancer Research, 2024). We apply this framework to a real clinical trial in large B-cell lymphoma to illustrate its application and value. Several recommendations and open questions are also raised.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04020v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Godwin Yung, Kaspar Rufibach, Marcel Wolbers, Mark Yan, Jue Wang</dc:creator>
    </item>
    <item>
      <title>Penalized Sparse Covariance Regression with High Dimensional Covariates</title>
      <link>https://arxiv.org/abs/2410.04028</link>
      <description>arXiv:2410.04028v1 Announce Type: new 
Abstract: Covariance regression offers an effective way to model the large covariance matrix with the auxiliary similarity matrices. In this work, we propose a sparse covariance regression (SCR) approach to handle the potentially high-dimensional predictors (i.e., similarity matrices). Specifically, we use the penalization method to identify the informative predictors and estimate their associated coefficients simultaneously. We first investigate the Lasso estimator and subsequently consider the folded concave penalized estimation methods (e.g., SCAD and MCP). However, the theoretical analysis of the existing penalization methods is primarily based on i.i.d. data, which is not directly applicable to our scenario. To address this difficulty, we establish the non-asymptotic error bounds by exploiting the spectral properties of the covariance matrix and similarity matrices. Then, we derive the estimation error bound for the Lasso estimator and establish the desirable oracle property of the folded concave penalized estimator. Extensive simulation studies are conducted to corroborate our theoretical results. We also illustrate the usefulness of the proposed method by applying it to a Chinese stock market dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04028v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Gao, Zhiyuan Zhang, Zhanrui Cai, Xuening Zhu, Tao Zou, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Jackknife empirical likelihood ratio test for log symmetric distribution using probability weighted moments</title>
      <link>https://arxiv.org/abs/2410.04082</link>
      <description>arXiv:2410.04082v1 Announce Type: new 
Abstract: Log symmetric distributions are useful in modeling data which show high skewness and have found applications in various fields. Using a recent characterization for log symmetric distributions, we propose a goodness of fit test for testing log symmetry. The asymptotic distributions of the test statistics under both null and alternate distributions are obtained. As the normal-based test is difficult to implement, we also propose a jackknife empirical likelihood (JEL) ratio test for testing log symmetry. We conduct a Monte Carlo Simulation to evaluate the performance of the JEL ratio test. Finally, we illustrated our methodology using different data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04082v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjana S, Sudheesh Kattumannil</dc:creator>
    </item>
    <item>
      <title>A Novel Unit Distribution Named As Median Based Unit Rayleigh (MBUR): Properties and Estimations</title>
      <link>https://arxiv.org/abs/2410.04132</link>
      <description>arXiv:2410.04132v1 Announce Type: new 
Abstract: The importance of continuously emerging new distribution is a mandate to understand the world and environment surrounding us. In this paper, the author will discuss a new distribution defined on the interval (0,1) as regards the methodology of deducing its PDF, some of its properties and related functions. A simulation and real data analysis will be highlighted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04132v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohammed Attia</dc:creator>
    </item>
    <item>
      <title>How to Compare Copula Forecasts?</title>
      <link>https://arxiv.org/abs/2410.04165</link>
      <description>arXiv:2410.04165v1 Announce Type: new 
Abstract: This paper lays out a principled approach to compare copula forecasts via strictly consistent scores. We first establish the negative result that, in general, copulas fail to be elicitable, implying that copula predictions cannot sensibly be compared on their own. A notable exception is on Fr\'echet classes, that is, when the marginal distribution structure is given and fixed, in which case we give suitable scores for the copula forecast comparison. As a remedy for the general non-elicitability of copulas, we establish novel multi-objective scores for copula forecast along with marginal forecasts. They give rise to two-step tests of equal or superior predictive ability which admit attribution of the forecast ranking to the accuracy of the copulas or the marginals. Simulations show that our two-step tests work well in terms of size and power. We illustrate our new methodology via an empirical example using copula forecasts for international stock market indices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04165v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Fissler, Yannick Hoga</dc:creator>
    </item>
    <item>
      <title>Physics-encoded Spatio-temporal Regression</title>
      <link>https://arxiv.org/abs/2410.04170</link>
      <description>arXiv:2410.04170v1 Announce Type: new 
Abstract: Physics-informed methods have gained a great success in analyzing data with partial differential equation (PDE) constraints, which are ubiquitous when modeling dynamical systems. Different from the common penalty-based approach, this work promotes adherence to the underlying physical mechanism that facilitates statistical procedures. The motivating application concerns modeling fluorescence recovery after photobleaching, which is used for characterization of diffusion processes. We propose a physics-encoded regression model for handling spatio-temporally distributed data, which enables principled interpretability, parsimonious computation and efficient estimation by exploiting the structure of solutions of a governing evolution equation. The rate of convergence attaining the minimax optimality is theoretically demonstrated, generalizing the result obtained for the spatial regression. We conduct simulation studies to assess the performance of our proposed estimator and illustrate its usage in the aforementioned real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04170v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tongyu Li, Fang Yao</dc:creator>
    </item>
    <item>
      <title>Adjusting for Spatial Correlation in Machine and Deep Learning</title>
      <link>https://arxiv.org/abs/2410.04312</link>
      <description>arXiv:2410.04312v1 Announce Type: new 
Abstract: Spatial data display correlation between observations collected at neighboring locations. Generally, machine and deep learning methods either do not account for this correlation or do so indirectly through correlated features and thereby forfeit predictive accuracy. To remedy this shortcoming, we propose preprocessing the data using a spatial decorrelation transform derived from properties of a multivariate Gaussian distribution and Vecchia approximations. The transformed data can then be ported into a machine or deep learning tool. After model fitting on the transformed data, the output can be spatially re-correlated via the corresponding inverse transformation. We show that including this spatial adjustment results in higher predictive accuracy on simulated and real spatial datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04312v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew J. Heaton, Andrew Millane, Jake S. Rhodes</dc:creator>
    </item>
    <item>
      <title>Subspace decompositions for association structure learning in multivariate categorical response regression</title>
      <link>https://arxiv.org/abs/2410.04356</link>
      <description>arXiv:2410.04356v1 Announce Type: new 
Abstract: Modeling the complex relationships between multiple categorical response variables as a function of predictors is a fundamental task in the analysis of categorical data. However, existing methods can be difficult to interpret and may lack flexibility. To address these challenges, we introduce a penalized likelihood method for multivariate categorical response regression that relies on a novel subspace decomposition to parameterize interpretable association structures. Our approach models the relationships between categorical responses by identifying mutual, joint, and conditionally independent associations, which yields a linear problem within a tensor product space. We establish theoretical guarantees for our estimator, including error bounds in high-dimensional settings, and demonstrate the method's interpretability and prediction accuracy through comprehensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04356v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongru Zhao, Aaron J. Molstad, Adam J. Rothman</dc:creator>
    </item>
    <item>
      <title>Efficient, Cross-Fitting Estimation of Semiparametric Spatial Point Processes</title>
      <link>https://arxiv.org/abs/2410.04359</link>
      <description>arXiv:2410.04359v1 Announce Type: new 
Abstract: We study a broad class of models called semiparametric spatial point processes where the first-order intensity function contains both a parametric component and a nonparametric component. We propose a novel, spatial cross-fitting estimator of the parametric component based on random thinning, a common simulation technique in point processes. The proposed estimator is shown to be consistent and in many settings, asymptotically Normal. Also, we generalize the notion of semiparametric efficiency lower bound in i.i.d. settings to spatial point processes and show that the proposed estimator achieves the efficiency lower bound if the process is Poisson. Next, we present a new spatial kernel regression estimator that can estimate the nonparametric component of the intensity function at the desired rates for inference. Despite the dependence induced by the point process, we show that our estimator can be computed using existing software for generalized partial linear models in i.i.d. settings. We conclude with a small simulation study and a re-analysis of the spatial distribution of rainforest trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04359v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xindi Lin, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Four-Regime Segmented Regression Models</title>
      <link>https://arxiv.org/abs/2410.04384</link>
      <description>arXiv:2410.04384v1 Announce Type: new 
Abstract: Segmented regression models offer model flexibility and interpretability as compared to the global parametric and the nonparametric models, and yet are challenging in both estimation and inference. We consider a four-regime segmented model for temporally dependent data with segmenting boundaries depending on multivariate covariates with non-diminishing boundary effects. A mixed integer quadratic programming algorithm is formulated to facilitate the least square estimation of the regression and the boundary parameters. The rates of convergence and the asymptotic distributions of the least square estimators are obtained for the regression and the boundary coefficients, respectively. We propose a smoothed regression bootstrap to facilitate inference on the parameters and a model selection procedure to select the most suitable model within the model class with at most four segments. Numerical simulations and a case study on air pollution in Beijing are conducted to demonstrate the proposed approach, which shows that the segmented models with three or four regimes are suitable for the modeling of the meteorological effects on the PM2.5 concentration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04384v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Yan, Song Xi Chen</dc:creator>
    </item>
    <item>
      <title>Approximate Maximum Likelihood Inference for Acoustic Spatial Capture-Recapture with Unknown Identities, Using Monte Carlo Expectation Maximization</title>
      <link>https://arxiv.org/abs/2410.04390</link>
      <description>arXiv:2410.04390v1 Announce Type: new 
Abstract: Acoustic spatial capture-recapture (ASCR) surveys with an array of synchronized acoustic detectors can be an effective way of estimating animal density or call density. However, constructing the capture histories required for ASCR analysis is challenging, as recognizing which detections at different detectors are of which calls is not a trivial task. Because calls from different distances take different times to arrive at detectors, the order in which calls are detected is not necessarily the same as the order in which they are made, and without knowing which detections are of the same call, we do not know how many different calls are detected. We propose a Monte Carlo expectation-maximization (MCEM) estimation method to resolve this unknown call identity problem. To implement the MCEM method in this context, we sample the latent variables from a complete-data likelihood model in the expectation step and use a semi-complete-data likelihood or conditional likelihood in the maximization step. We use a parametric bootstrap to obtain confidence intervals. When we apply our method to a survey of moss frogs, it gives an estimate within 15% of the estimate obtained using data with call capture histories constructed by experts, and unlike this latter estimate, our confidence interval incorporates the uncertainty about call identities. Simulations show it to have a low bias (6%) and coverage probabilities close to the nominal 95% value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04390v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuheng Wang, Juan Ye, Weiye Li, David L. Borchers</dc:creator>
    </item>
    <item>
      <title>Transfer Learning with General Estimating Equations</title>
      <link>https://arxiv.org/abs/2410.04398</link>
      <description>arXiv:2410.04398v1 Announce Type: new 
Abstract: We consider statistical inference for parameters defined by general estimating equations under the covariate shift transfer learning. Different from the commonly used density ratio weighting approach, we undertake a set of formulations to make the statistical inference semiparametric efficient with simple inference. It starts with re-constructing the estimation equations to make them Neyman orthogonal, which facilitates more robustness against errors in the estimation of two key nuisance functions, the density ratio and the conditional mean of the moment function. We present a divergence-based method to estimate the density ratio function, which is amenable to machine learning algorithms including the deep learning. To address the challenge that the conditional mean is parametric-dependent, we adopt a nonparametric multiple-imputation strategy that avoids regression at all possible parameter values. With the estimated nuisance functions and the orthogonal estimation equation, the inference for the target parameter is formulated via the empirical likelihood without sample splittings. We show that the proposed estimator attains the semiparametric efficiency bound, and the inference can be conducted with the Wilks' theorem. The proposed method is further evaluated by simulations and an empirical study on a transfer learning inference for ground-level ozone pollution</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04398v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Yan, Song Xi Chen</dc:creator>
    </item>
    <item>
      <title>A Reflection on the Impact of Misspecifying Unidentifiable Causal Inference Models in Surrogate Endpoint Evaluation</title>
      <link>https://arxiv.org/abs/2410.04438</link>
      <description>arXiv:2410.04438v1 Announce Type: new 
Abstract: Surrogate endpoints are often used in place of expensive, delayed, or rare true endpoints in clinical trials. However, regulatory authorities require thorough evaluation to accept these surrogate endpoints as reliable substitutes. One evaluation approach is the information-theoretic causal inference framework, which quantifies surrogacy using the individual causal association (ICA). Like most causal inference methods, this approach relies on models that are only partially identifiable. For continuous outcomes, a normal model is often used. Based on theoretical elements and a Monte Carlo procedure we studied the impact of model misspecification across two scenarios: 1) the true model is based on a multivariate t-distribution, and 2) the true model is based on a multivariate log-normal distribution. In the first scenario, the misspecification has a negligible impact on the results, while in the second, it has a significant impact when the misspecification is detectable using the observed data. Finally, we analyzed two data sets using the normal model and several D-vine copula models that were indistinguishable from the normal model based on the data at hand. We observed that the results may vary when different models are used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04438v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gokce Deliorman, Florian Stijven, Wim Van der Elst, Maria del Carmen Pardo, Ariel Alonso</dc:creator>
    </item>
    <item>
      <title>Hybrid Monte Carlo for Failure Probability Estimation with Gaussian Process Surrogates</title>
      <link>https://arxiv.org/abs/2410.04496</link>
      <description>arXiv:2410.04496v1 Announce Type: new 
Abstract: We tackle the problem of quantifying failure probabilities for expensive computer experiments with stochastic inputs. The computational cost of evaluating the computer simulation prohibits direct Monte Carlo (MC) and necessitates a statistical surrogate model. Surrogate-informed importance sampling -- which leverages the surrogate to identify suspected failures, fits a bias distribution to these locations, then calculates failure probabilities using a weighted average -- is popular, but it is data hungry and can provide erroneous results when budgets are limited. Instead, we propose a hybrid MC scheme which first uses the uncertainty quantification (UQ) of a Gaussian process (GP) surrogate to identify areas of high classification uncertainty, then combines surrogate predictions in certain regions with true simulator evaluation in uncertain regions. We also develop a stopping criterion which informs the allocation of a fixed budget of simulator evaluations between surrogate training and failure probability estimation. Our method is agnostic to surrogate choice (as long as UQ is provided); we showcase functionality with both GPs and deep GPs. It is also agnostic to design choices; we deploy contour locating sequential designs throughout. With these tools, we are able to effectively estimate small failure probabilities with only hundreds of simulator evaluations. We validate our method on a variety of synthetic benchmarks before deploying it on an expensive computer experiment of fluid flow around an airfoil.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04496v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annie S. Booth, S. Ashwin Renganathan</dc:creator>
    </item>
    <item>
      <title>A Bayesian Method for Adverse Effects Estimation in Observational Studies with Truncation by Death</title>
      <link>https://arxiv.org/abs/2410.04561</link>
      <description>arXiv:2410.04561v1 Announce Type: new 
Abstract: Death among subjects is common in observational studies evaluating the causal effects of interventions among geriatric or severely ill patients. High mortality rates complicate the comparison of the prevalence of adverse events (AEs) between interventions. This problem is often referred to as outcome "truncation" by death. A possible solution is to estimate the survivor average causal effect (SACE), an estimand that evaluates the effects of interventions among those who would have survived under both treatment assignments. However, because the SACE does not include subjects who would have died under one or both arms, it does not consider the relationship between AEs and death. We propose a Bayesian method which imputes the unobserved mortality and AE outcomes for each participant under the intervention they did not receive. Using the imputed outcomes we define a composite ordinal outcome for each patient, combining the occurrence of death and the AE in an increasing scale of severity. This allows for the comparison of the effects of the interventions on death and the AE simultaneously among the entire sample. We implement the procedure to analyze the incidence of heart failure among geriatric patients being treated for Type II diabetes with sulfonylureas or dipeptidyl peptidase-4 inhibitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04561v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Sisti, Andrew Zullo, Roee Gutman</dc:creator>
    </item>
    <item>
      <title>A Finite Mixture Hidden Markov Model for Intermittently Observed Disease Process with Heterogeneity and Partially Known Disease Type</title>
      <link>https://arxiv.org/abs/2410.04667</link>
      <description>arXiv:2410.04667v1 Announce Type: new 
Abstract: Continuous-time multistate models are widely used for analyzing interval-censored data on disease progression over time. Sometimes, diseases manifest differently and what appears to be a coherent collection of symptoms is the expression of multiple distinct disease subtypes. To address this complexity, we propose a mixture hidden Markov model, where the observation process encompasses states representing common symptomatic stages across these diseases, and each underlying process corresponds to a distinct disease subtype. Our method models both the overall and the type-specific disease incidence/prevalence accounting for sampling conditions and exactly observed death times. Additionally, it can utilize partially available disease-type information, which offers insights into the pathway through specific hidden states in the disease process, to aid in the estimation. We present both a frequentist and a Bayesian way to obtain the estimates. The finite sample performance is evaluated through simulation studies. We demonstrate our method using the Nun Study and model the development and progression of dementia, encompassing both Alzheimer's disease (AD) and non-AD dementia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04667v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yidan Shi (Department of Population Health, New York University Grossman School of Medicine), Leilei Zeng (Department of Statistics and Actuarial Science, University of Waterloo), Mary E. Thompson (Department of Statistics and Actuarial Science, University of Waterloo), Suzanne L. Tyas (School of Public Health Sciences, University of Waterloo)</dc:creator>
    </item>
    <item>
      <title>Efficient Input Uncertainty Quantification for Ratio Estimator</title>
      <link>https://arxiv.org/abs/2410.04696</link>
      <description>arXiv:2410.04696v1 Announce Type: new 
Abstract: We study the construction of a confidence interval (CI) for a simulation output performance measure that accounts for input uncertainty when the input models are estimated from finite data. In particular, we focus on performance measures that can be expressed as a ratio of two dependent simulation outputs' means. We adopt the parametric bootstrap method to mimic input data sampling and construct the percentile bootstrap CI after estimating the ratio at each bootstrap sample. The standard estimator, which takes the ratio of two sample averages, tends to exhibit large finite-sample bias and variance, leading to overcoverage of the percentile bootstrap CI. To address this, we propose two new ratio estimators that replace the sample averages with pooled mean estimators via the $k$-nearest neighbor ($k$NN) regression: the $k$NN estimator and the $k$LR estimator. The $k$NN estimator performs well in low dimensions but its theoretical performance guarantee degrades as the dimension increases. The $k$LR estimator combines the likelihood ratio (LR) method with the $k$NN regression, leveraging the strengths of both while mitigating their weaknesses; the LR method removes dependence on dimension, while the variance inflation introduced by the LR is controlled by $k$NN. Based on asymptotic analyses and finite-sample heuristics, we propose an experiment design that maximizes the efficiency of the proposed estimators and demonstrate their empirical performances using three examples including one in the enterprise risk management application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04696v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Linyun He, Ben Feng, Eunhye Song</dc:creator>
    </item>
    <item>
      <title>Nonparametric tests for interaction in two-way ANOVA with balanced replications</title>
      <link>https://arxiv.org/abs/2410.04700</link>
      <description>arXiv:2410.04700v1 Announce Type: new 
Abstract: Nonparametric procedures are more powerful for detecting interaction in two-way ANOVA when the data are non-normal. In this paper, we compute null critical values for the aligned rank-based tests (APCSSA/APCSSM) where the levels of the factors are between 2 and 6. We compare the performance of these new procedures with the ANOVA F-test for interaction, the adjusted rank transform test (ART), Conover's rank transform procedure (RT), and a rank-based ANOVA test (raov) using Monte Carlo simulations. The new procedures APCSSA/APCSSM are comparable with existing competitors in all settings. Even though there is no single dominant test in detecting interaction effects for non-normal data, nonparametric procedure APCSSM is the most highly recommended procedure for Cauchy errors settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04700v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bao Khue Tran, Amy S. Wagaman, Andrew Nguyen, David Jacobson, Bradley Hartlaub</dc:creator>
    </item>
    <item>
      <title>Order of Addition in Mixture-Amount Experiments</title>
      <link>https://arxiv.org/abs/2410.04864</link>
      <description>arXiv:2410.04864v1 Announce Type: new 
Abstract: In a mixture experiment, we study the behavior and properties of $m$ mixture components, where the primary focus is on the proportions of the components that make up the mixture rather than the total amount. Mixture-amount experiments are specialized types of mixture experiments where both the proportions of the components in the mixture and the total amount of the mixture are of interest. Such experiments consider the total amount of the mixture as a variable. In this paper, we consider an Order-of-Addition (OofA) mixture-amount experiment in which the response depends on both the mixture amounts of components and their order of addition. Full Mixture OofA designs are constructed to maintain orthogonality between the mixture-amount model terms and the effects of the order of addition. These designs enable the estimation of mixture-component model parameters and the order-of-addition effects. The G-efficiency criterion assesses how well the design supports precise and unbiased estimation of the model parameters. The Fraction of Design Space (FDS) plot is used to provide a visual assessment of the prediction capabilities of a design across the entire design space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04864v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taha Hasan, Touqeer Ahmad</dc:creator>
    </item>
    <item>
      <title>Random-projection ensemble dimension reduction</title>
      <link>https://arxiv.org/abs/2410.04922</link>
      <description>arXiv:2410.04922v1 Announce Type: new 
Abstract: We introduce a new framework for dimension reduction in the context of high-dimensional regression. Our proposal is to aggregate an ensemble of random projections, which have been carefully chosen based on the empirical regression performance after being applied to the covariates. More precisely, we consider disjoint groups of independent random projections, apply a base regression method after each projection, and retain the projection in each group based on the empirical performance. We aggregate the selected projections by taking the singular value decomposition of their empirical average and then output the leading order singular vectors. A particularly appealing aspect of our approach is that the singular values provide a measure of the relative importance of the corresponding projection directions, which can be used to select the final projection dimension. We investigate in detail (and provide default recommendations for) various aspects of our general framework, including the projection distribution and the base regression method, as well as the number of random projections used. Additionally, we investigate the possibility of further reducing the dimension by applying our algorithm twice in cases where projection dimension recommended in the initial application is too large. Our theoretical results show that the error of our algorithm stabilises as the number of groups of projections increases. We demonstrate the excellent empirical performance of our proposal in a large numerical study using simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04922v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenxing Zhou, Timothy I. Cannings</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Post-Integrated Inference with Negative Control Outcomes</title>
      <link>https://arxiv.org/abs/2410.04996</link>
      <description>arXiv:2410.04996v1 Announce Type: new 
Abstract: Data integration has become increasingly common in aligning multiple heterogeneous datasets. With high-dimensional outcomes, data integration methods aim to extract low-dimensional embeddings of observations to remove unwanted variations, such as batch effects and unmeasured covariates, inherent in data collected from different sources. However, multiple hypothesis testing after data integration can be substantially biased due to the data-dependent integration processes. To address this challenge, we introduce a robust post-integrated inference (PII) method that adjusts for latent heterogeneity using negative control outcomes. By leveraging causal interpretations, we derive nonparametric identification conditions that form the basis of our PII approach.
  Our assumption-lean semiparametric inference method extends robustness and generality to projected direct effect estimands that account for mediators, confounders, and moderators. These estimands remain statistically meaningful under model misspecifications and with error-prone embeddings. We provide deterministic quantifications of the bias of target estimands induced by estimated embeddings and finite-sample linear expansions of the estimators with uniform concentration bounds on the residuals for all outcomes.
  The proposed doubly robust estimators are consistent and efficient under minimal assumptions, facilitating data-adaptive estimation with machine learning algorithms. Using random forests, we evaluate empirical statistical errors in simulations and analyze single-cell CRISPR perturbed datasets with potential unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04996v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Kathryn Roeder, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Testing procedures based on maximum likelihood estimation for Marked Hawkes processes</title>
      <link>https://arxiv.org/abs/2410.05008</link>
      <description>arXiv:2410.05008v1 Announce Type: new 
Abstract: The Hawkes model is a past-dependent point process, widely used in various fields for modeling temporal clustering of events. Extending this framework, the multidimensional marked Hawkes process incorporates multiple interacting event types and additional marks, enhancing its capability to model complex dependencies in multivariate time series data. However, increasing the complexity of the model also increases the computational cost of the associated estimation methods and may induce an overfitting of the model. Therefore, it is essential to find a trade-off between accuracy and artificial complexity of the model. In order to find the appropriate version of Hawkes processes, we address, in this paper, the tasks of model fit evaluation and parameter testing for marked Hawkes processes. This article focuses on parametric Hawkes processes with exponential memory kernels, a popular variant for its theoretical and practical advantages. Our work introduces robust testing methodologies for assessing model parameters and complexity, building upon and extending previous theoretical frameworks. We then validate the practical robustness of these tests through comprehensive numerical studies, especially in scenarios where theoretical guarantees remains incomplete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05008v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Bonnet, Charlotte Dion-Blanc, Maya Sadeler Perrin</dc:creator>
    </item>
    <item>
      <title>False Discovery Rate Control for Fast Screening of Large-Scale Genomics Biobanks</title>
      <link>https://arxiv.org/abs/2410.05169</link>
      <description>arXiv:2410.05169v1 Announce Type: new 
Abstract: Genomics biobanks are information treasure troves with thousands of phenotypes (e.g., diseases, traits) and millions of single nucleotide polymorphisms (SNPs). The development of methodologies that provide reproducible discoveries is essential for the understanding of complex diseases and precision drug development. Without statistical reproducibility guarantees, valuable efforts are spent on researching false positives. Therefore, scalable multivariate and high-dimensional false discovery rate (FDR)-controlling variable selection methods are urgently needed, especially, for complex polygenic diseases and traits. In this work, we propose the Screen-T-Rex selector, a fast FDR-controlling method based on the recently developed T-Rex selector. The method is tailored to screening large-scale biobanks and it does not require choosing additional parameters (sparsity parameter, target FDR level, etc). Numerical simulations and a real-world HIV-1 drug resistance example demonstrate that the performance of the Screen-T-Rex selector is superior, and its computation time is multiple orders of magnitude lower compared to current benchmark knockoff methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05169v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jasin Machkour, Michael Muma, Daniel P. Palomar</dc:creator>
    </item>
    <item>
      <title>The Informed Elastic Net for Fast Grouped Variable Selection and FDR Control in Genomics Research</title>
      <link>https://arxiv.org/abs/2410.05211</link>
      <description>arXiv:2410.05211v1 Announce Type: new 
Abstract: Modern genomics research relies on genome-wide association studies (GWAS) to identify the few genetic variants among potentially millions that are associated with diseases of interest. Only reproducible discoveries of groups of associations improve our understanding of complex polygenic diseases and enable the development of new drugs and personalized medicine. Thus, fast multivariate variable selection methods that have a high true positive rate (TPR) while controlling the false discovery rate (FDR) are crucial. Recently, the T-Rex+GVS selector, a version of the T-Rex selector that uses the elastic net (EN) as a base selector to perform grouped variable election, was proposed. Although it significantly increased the TPR in simulated GWAS compared to the original T-Rex, its comparably high computational cost limits scalability. Therefore, we propose the informed elastic net (IEN), a new base selector that significantly reduces computation time while retaining the grouped variable selection property. We quantify its grouping effect and derive its formulation as a Lasso-type optimization problem, which is solved efficiently within the T-Rex framework by the terminated LARS algorithm. Numerical simulations and a GWAS study demonstrate that the proposed T-Rex+GVS (IEN) exhibits the desired grouping effect, reduces computation time, and achieves the same TPR as T-Rex+GVS (EN) but with lower FDR, which makes it a promising method for large-scale GWAS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05211v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jasin Machkour, Michael Muma, Daniel P. Palomar</dc:creator>
    </item>
    <item>
      <title>Fast algorithm for sparse least trimmed squares via trimmed-regularized reformulation</title>
      <link>https://arxiv.org/abs/2410.04554</link>
      <description>arXiv:2410.04554v1 Announce Type: cross 
Abstract: The least trimmed squares (LTS) is a reasonable formulation of robust regression whereas it suffers from high computational cost due to the nonconvexity and nonsmoothness of its objective function. The most frequently used FAST-LTS algorithm is particularly slow when a sparsity-inducing penalty such as the $\ell_1$ norm is added. This paper proposes a computationally inexpensive algorithm for the sparse LTS, which is based on the proximal gradient method with a reformulation technique. Proposed method is equipped with theoretical convergence preferred over existing methods. Numerical experiments show that our method efficiently yields small objective value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04554v1</guid>
      <category>stat.CO</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shotaro Yagishita</dc:creator>
    </item>
    <item>
      <title>Graph Fourier Neural Kernels (G-FuNK): Learning Solutions of Nonlinear Diffusive Parametric PDEs on Multiple Domains</title>
      <link>https://arxiv.org/abs/2410.04655</link>
      <description>arXiv:2410.04655v1 Announce Type: cross 
Abstract: Predicting time-dependent dynamics of complex systems governed by non-linear partial differential equations (PDEs) with varying parameters and domains is a challenging task motivated by applications across various fields. We introduce a novel family of neural operators based on our Graph Fourier Neural Kernels, designed to learn solution generators for nonlinear PDEs in which the highest-order term is diffusive, across multiple domains and parameters. G-FuNK combines components that are parameter- and domain-adapted with others that are not. The domain-adapted components are constructed using a weighted graph on the discretized domain, where the graph Laplacian approximates the highest-order diffusive term, ensuring boundary condition compliance and capturing the parameter and domain-specific behavior. Meanwhile, the learned components transfer across domains and parameters via Fourier Neural Operators. This approach naturally embeds geometric and directional information, improving generalization to new test domains without need for retraining the network. To handle temporal dynamics, our method incorporates an integrated ODE solver to predict the evolution of the system. Experiments show G-FuNK's capability to accurately approximate heat, reaction diffusion, and cardiac electrophysiology equations across various geometries and anisotropic diffusivity fields. G-FuNK achieves low relative errors on unseen domains and fiber fields, significantly accelerating predictions compared to traditional finite-element solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04655v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.SP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shane E. Loeffler, Zan Ahmad, Syed Yusuf Ali, Carolyna Yamamoto, Dan M. Popescu, Alana Yee, Yash Lal, Natalia Trayanova, Mauro Maggioni</dc:creator>
    </item>
    <item>
      <title>Regression Conformal Prediction under Bias</title>
      <link>https://arxiv.org/abs/2410.05263</link>
      <description>arXiv:2410.05263v1 Announce Type: cross 
Abstract: Uncertainty quantification is crucial to account for the imperfect predictions of machine learning algorithms for high-impact applications. Conformal prediction (CP) is a powerful framework for uncertainty quantification that generates calibrated prediction intervals with valid coverage. In this work, we study how CP intervals are affected by bias - the systematic deviation of a prediction from ground truth values - a phenomenon prevalent in many real-world applications. We investigate the influence of bias on interval lengths of two different types of adjustments -- symmetric adjustments, the conventional method where both sides of the interval are adjusted equally, and asymmetric adjustments, a more flexible method where the interval can be adjusted unequally in positive or negative directions. We present theoretical and empirical analyses characterizing how symmetric and asymmetric adjustments impact the "tightness" of CP intervals for regression tasks. Specifically for absolute residual and quantile-based non-conformity scores, we prove: 1) the upper bound of symmetrically adjusted interval lengths increases by $2|b|$ where $b$ is a globally applied scalar value representing bias, 2) asymmetrically adjusted interval lengths are not affected by bias, and 3) conditions when asymmetrically adjusted interval lengths are guaranteed to be smaller than symmetric ones. Our analyses suggest that even if predictions exhibit significant drift from ground truth values, asymmetrically adjusted intervals are still able to maintain the same tightness and validity of intervals as if the drift had never happened, while symmetric ones significantly inflate the lengths. We demonstrate our theoretical results with two real-world prediction tasks: sparse-view computed tomography (CT) reconstruction and time-series weather forecasting. Our work paves the way for more bias-robust machine learning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05263v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matt Y. Cheung, Tucker J. Netherton, Laurence E. Court, Ashok Veeraraghavan, Guha Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Covariance Matrix Estimation for High-Throughput Biomedical Data with Interconnected Communities</title>
      <link>https://arxiv.org/abs/2302.01861</link>
      <description>arXiv:2302.01861v3 Announce Type: replace 
Abstract: Estimating a covariance matrix is central to high-dimensional data analysis. Empirical analyses of high-dimensional biomedical data, including genomics, proteomics, microbiome, and neuroimaging, among others, consistently reveal strong modularity in the dependence patterns. In these analyses, intercorrelated high-dimensional biomedical features often form communities or modules that can be interconnected with others. While the interconnected community structure has been extensively studied in biomedical research (e.g., gene co-expression networks), its potential to assist in the estimation of covariance matrices remains largely unexplored. To address this gap, we propose a procedure that leverages the commonly observed interconnected community structure in high-dimensional biomedical data to estimate large covariance and precision matrices. We derive the uniformly minimum-variance unbiased estimators for covariance and precision matrices in closed forms and provide theoretical results on their asymptotic properties. Our proposed method enhances the accuracy of covariance- and precision-matrix estimation and demonstrates superior performance compared to the competing methods in both simulations and real data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.01861v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yifan Yang, Chixiang Chen, Shuo Chen</dc:creator>
    </item>
    <item>
      <title>Confidence Sets for Causal Orderings</title>
      <link>https://arxiv.org/abs/2305.14506</link>
      <description>arXiv:2305.14506v2 Announce Type: replace 
Abstract: Causal discovery procedures aim to deduce causal relationships among variables in a multivariate dataset. While various methods have been proposed for estimating a single causal model or a single equivalence class of models, less attention has been given to quantifying uncertainty in causal discovery in terms of confidence statements. A primary challenge in causal discovery of directed acyclic graphs is determining a causal ordering among the variables, and our work offers a framework for constructing confidence sets of causal orderings that the data do not rule out. Our methodology specifically applies to identifiable structural equation models with additive errors and is based on a residual bootstrap procedure to test the goodness-of-fit of causal orderings. We demonstrate the asymptotic validity of the confidence set constructed using this goodness-of-fit test and explain how the confidence set may be used to form sub/supersets of ancestral relationships as well as confidence intervals for causal effects that incorporate model uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14506v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Y. Samuel Wang, Mladen Kolar, Mathias Drton</dc:creator>
    </item>
    <item>
      <title>Causal Message Passing for Experiments with Unknown and General Network Interference</title>
      <link>https://arxiv.org/abs/2311.08340</link>
      <description>arXiv:2311.08340v3 Announce Type: replace 
Abstract: Randomized experiments are a powerful methodology for data-driven evaluation of decisions or interventions. Yet, their validity may be undermined by network interference. This occurs when the treatment of one unit impacts not only its outcome but also that of connected units, biasing traditional treatment effect estimations. Our study introduces a new framework to accommodate complex and unknown network interference, moving beyond specialized models in the existing literature. Our framework, termed causal message-passing, is grounded in high-dimensional approximate message passing methodology. It is tailored for multi-period experiments and is particularly effective in settings with many units and prevalent network interference. The framework models causal effects as a dynamic process where a treated unit's impact propagates through the network via neighboring units until equilibrium is reached. This approach allows us to approximate the dynamics of potential outcomes over time, enabling the extraction of valuable information before treatment effects reach equilibrium. Utilizing causal message-passing, we introduce a practical algorithm to estimate the total treatment effect, defined as the impact observed when all units are treated compared to the scenario where no unit receives treatment. We demonstrate the effectiveness of this approach across five numerical scenarios, each characterized by a distinct interference structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08340v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sadegh Shirani, Mohsen Bayati</dc:creator>
    </item>
    <item>
      <title>Nonparametric Strategy Test</title>
      <link>https://arxiv.org/abs/2312.10695</link>
      <description>arXiv:2312.10695v5 Announce Type: replace 
Abstract: We present a nonparametric statistical test for determining whether an agent is following a given mixed strategy in a repeated strategic-form game given samples of the agent's play. This involves two components: determining whether the agent's frequencies of pure strategies are sufficiently close to the target frequencies, and determining whether the pure strategies selected are independent between different game iterations. Our integrated test involves applying a chi-squared goodness of fit test for the first component and a generalized Wald-Wolfowitz runs test for the second component. The results from both tests are combined using Bonferroni correction to produce a complete test for a given significance level $\alpha.$ We applied the test to publicly available data of human rock-paper-scissors play. The data consists of 50 iterations of play for 500 human players. We test with a null hypothesis that the players are following a uniform random strategy independently at each game iteration. Using a significance level of $\alpha = 0.05$, we conclude that 305 (61%) of the subjects are following the target strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10695v5</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sam Ganzfried</dc:creator>
    </item>
    <item>
      <title>Semi-Confirmatory Factor Analysis for High-Dimensional Data with Interconnected Community Structures</title>
      <link>https://arxiv.org/abs/2401.00624</link>
      <description>arXiv:2401.00624v3 Announce Type: replace 
Abstract: Confirmatory factor analysis (CFA) is a statistical method for identifying and confirming the presence of latent factors among observed variables through the analysis of their covariance structure. Compared to alternative factor models, CFA offers interpretable common factors with enhanced specificity and a more adaptable approach to covariance structure modeling. However, the application of CFA has been limited by the requirement for prior knowledge about "non-zero loadings" and by the lack of computational scalability (e.g., it can be computationally intractable for hundreds of observed variables). We propose a data-driven semi-confirmatory factor analysis (SCFA) model that attempts to alleviate these limitations. SCFA automatically specifies "non-zero loadings" by learning the network structure of the large covariance matrix of observed variables, and then offers closed-form estimators for factor loadings, factor scores, covariances between common factors, and variances between errors using the likelihood method. Therefore, SCFA is applicable to high-throughput datasets (e.g., hundreds of thousands of observed variables) without requiring prior knowledge about "non-zero loadings". Through an extensive simulation analysis benchmarking against standard packages, SCFA exhibits superior performance in estimating model parameters with a much-reduced computational time. We illustrate its practical application through factor analysis on two high-dimensional RNA-seq gene expression datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00624v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yifan Yang, Tianzhou Ma, Chuan Bi, Shuo Chen</dc:creator>
    </item>
    <item>
      <title>Optimal monotone conditional error functions</title>
      <link>https://arxiv.org/abs/2402.00814</link>
      <description>arXiv:2402.00814v3 Announce Type: replace 
Abstract: This note presents a method that provides optimal monotone conditional error functions for a large class of adaptive two stage designs. The presented method builds on a previously developed general theory for optimal adaptive two stage designs where sample sizes are reassessed for a specific conditional power and the goal is to minimize the expected sample size. The previous theory can easily lead to a non-monotonous conditional error function which is highly undesirable for logical reasons and can harm type I error rate control for composite null hypotheses. The here presented method extends the existing theory by introducing intermediate monotonising steps that can easily be implemented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00814v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Werner Brannath, Morten Dreher, Martin Scharpenberg</dc:creator>
    </item>
    <item>
      <title>Modeling local predictive ability using power-transformed Gaussian processes</title>
      <link>https://arxiv.org/abs/2402.02068</link>
      <description>arXiv:2402.02068v2 Announce Type: replace 
Abstract: A Gaussian process is proposed as a model for the posterior distribution of the local predictive ability of a model or expert, conditional on a vector of covariates, from historical predictions in the form of log predictive scores. Assuming Gaussian expert predictions and a Gaussian data generating process, a linear transformation of the predictive score follows a noncentral chi-squared distribution with one degree of freedom. Motivated by this we develop a noncentral chi-squared Gaussian process regression to flexibly model local predictive ability, with the posterior distribution of the latent GP function and kernel hyperparameters sampled by Hamiltonian Monte Carlo. We show that a cube-root transformation of the log scores is approximately Gaussian with homoscedastic variance, making it possible to estimate the model much faster by marginalizing the latent GP function analytically. A multi-output Gaussian process regression is also introduced to model the dependence in predictive ability between experts, both for inference and prediction purposes. Linear pools based on learned local predictive ability are applied to predict daily bike usage in Washington DC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02068v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oscar Oelrich, Mattias Villani</dc:creator>
    </item>
    <item>
      <title>A generalized ordinal quasi-symmetry model and its separability for analyzing multi-way tables</title>
      <link>https://arxiv.org/abs/2405.04193</link>
      <description>arXiv:2405.04193v2 Announce Type: replace 
Abstract: This paper addresses the challenge of modeling multi-way contingency tables for matched set data with ordinal categories. Although the complete symmetry and marginal homogeneity models are well established, they may not always provide a satisfactory fit to the data. To address this issue, we propose a generalized ordinal quasi-symmetry model that offers increased flexibility when the complete symmetry model fails to capture the underlying structure. We investigate the properties of this new model and provide an information-theoretic interpretation, elucidating its relationship to the ordinal quasi-symmetry model. Moreover, we revisit Agresti's findings and present a new necessary and sufficient condition for the complete symmetry model, proving that the proposed model and the marginal moment equality model are separable hypotheses. We demonstrate the practical application of our model through empirical studies on medical and public opinion datasets. Comprehensive simulation studies evaluate the proposed model under various scenarios, including model's performance for multivariate normal data and asymptotic behavior. It enables researchers to examine the symmetry structure in the data with greater precision, providing a more thorough understanding of the underlying patterns. This powerful framework equips researchers with the necessary tools to explore the complexities of ordinal variable relationships in matched data sets, paving the way for new discoveries and insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04193v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hisaya Okahara, Kouji Tahata</dc:creator>
    </item>
    <item>
      <title>Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift</title>
      <link>https://arxiv.org/abs/2405.10302</link>
      <description>arXiv:2405.10302v2 Announce Type: replace 
Abstract: As machine learning models are increasingly deployed in dynamic environments, it becomes paramount to assess and quantify uncertainties associated with distribution shifts. A distribution shift occurs when the underlying data-generating process changes, leading to a deviation in the model's performance. The prediction interval, which captures the range of likely outcomes for a given prediction, serves as a crucial tool for characterizing uncertainties induced by their underlying distribution. In this paper, we propose methodologies for aggregating prediction intervals to obtain one with minimal width and adequate coverage on the target domain under unsupervised domain shift, under which we have labeled samples from a related source domain and unlabeled covariates from the target domain. Our analysis encompasses scenarios where the source and the target domain are related via i) a bounded density ratio, and ii) a measure-preserving transformation. Our proposed methodologies are computationally efficient and easy to implement. Beyond illustrating the performance of our method through real-world datasets, we also delve into the theoretical details. This includes establishing rigorous theoretical guarantees, coupled with finite sample bounds, regarding the coverage and width of our prediction intervals. Our approach excels in practical applications and is underpinned by a solid theoretical framework, ensuring its reliability and effectiveness across diverse contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10302v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Ge, Debarghya Mukherjee, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>Adjusting for Selection Bias Due to Missing Eligibility Criteria in Emulated Target Trials</title>
      <link>https://arxiv.org/abs/2406.16830</link>
      <description>arXiv:2406.16830v2 Announce Type: replace 
Abstract: Target trial emulation (TTE) is a popular framework for observational studies based on electronic health records (EHR). A key component of this framework is determining the patient population eligible for inclusion in both a target trial of interest and its observational emulation. Missingness in variables that define eligibility criteria, however, presents a major challenge towards determining the eligible population when emulating a target trial with an observational study. In practice, patients with incomplete data are almost always excluded from analysis despite the possibility of selection bias, which can arise when subjects with observed eligibility data are fundamentally different than excluded subjects. Despite this, to the best of our knowledge, very little work has been done to mitigate this concern. In this paper, we propose a novel conceptual framework to address selection bias in TTE studies, tailored towards time-to-event endpoints, and describe estimation and inferential procedures via inverse probability weighting (IPW). Under an EHR-based simulation infrastructure, developed to reflect the complexity of EHR data, we characterize common settings under which missing eligibility data poses the threat of selection bias and investigate the ability of the proposed methods to address it. Finally, using EHR databases from Kaiser Permanente, we demonstrate the use of our method to evaluate the effect of bariatric surgery on microvascular outcomes among a cohort of severely obese patients with Type II diabetes mellitus (T2DM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16830v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Benz, Rajarshi Mukherjee, Rui Wang, David Arterburn, Catherine Lee, Heidi Fischer, Susan Shortreed, Sebastien Haneuse</dc:creator>
    </item>
    <item>
      <title>Bootstrap-based goodness-of-fit test for parametric families of conditional distributions</title>
      <link>https://arxiv.org/abs/2409.20262</link>
      <description>arXiv:2409.20262v2 Announce Type: replace 
Abstract: In various scientific fields, researchers are interested in exploring the relationship between some response variable Y and a vector of covariates X. In order to make use of a specific model for the dependence structure, it first has to be checked whether the conditional density function of Y given X fits into a given parametric family. We propose a consistent bootstrap-based goodness-of-fit test for this purpose. The test statistic traces the difference between a nonparametric and a semi-parametric estimate of the marginal distribution function of Y. As its asymptotic null distribution is not distribution-free, a parametric bootstrap method is used to determine the critical value. A simulation study shows that, in some cases, the new method is more sensitive to deviations from the parametric model than other tests found in the literature. We also apply our method to real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20262v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gitte Kremling, Gerhard Dikta</dc:creator>
    </item>
    <item>
      <title>Stability-Adjusted Cross-Validation for Sparse Linear Regression</title>
      <link>https://arxiv.org/abs/2306.14851</link>
      <description>arXiv:2306.14851v2 Announce Type: replace-cross 
Abstract: Given a high-dimensional covariate matrix and a response vector, ridge-regularized sparse linear regression selects a subset of features that explains the relationship between covariates and the response in an interpretable manner. To select the sparsity and robustness of linear regressors, techniques like k-fold cross-validation are commonly used for hyperparameter tuning. However, cross-validation substantially increases the computational cost of sparse regression as it requires solving many mixed-integer optimization problems (MIOs). Additionally, validation metrics often serve as noisy estimators of test set errors, with different hyperparameter combinations leading to models with different noise levels. Therefore, optimizing over these metrics is vulnerable to out-of-sample disappointment, especially in underdetermined settings. To improve upon this state of affairs, we make two key contributions. First, motivated by the generalization theory literature, we propose selecting hyperparameters that minimize a weighted sum of a cross-validation metric and a model's output stability, thus reducing the risk of poor out-of-sample performance. Second, we leverage ideas from the mixed-integer optimization literature to obtain computationally tractable relaxations of k-fold cross-validation metrics and the output stability of regressors, facilitating hyperparameter selection after solving fewer MIOs. These relaxations result in an efficient cyclic coordinate descent scheme, achieving lower validation errors than via traditional methods such as grid search. On synthetic datasets, our confidence adjustment procedure improves out-of-sample performance by 2%-5% compared to minimizing the k-fold error alone. On 13 real-world datasets, our confidence adjustment procedure reduces test set error by 2%, on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14851v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Cory-Wright, Andr\'es G\'omez</dc:creator>
    </item>
    <item>
      <title>Granger Causality in High-Dimensional Networks of Time Series</title>
      <link>https://arxiv.org/abs/2406.02360</link>
      <description>arXiv:2406.02360v2 Announce Type: replace-cross 
Abstract: A novel approach is developed for discovering directed connectivity between specified pairs of nodes in a high-dimensional network (HDN) of brain signals. To accurately identify causal connectivity for such specified objectives, it is necessary to properly address the influence of all other nodes within the network. The proposed procedure herein starts with the estimation of a low-dimensional representation of the other nodes in the network utilizing (frequency-domain-based) spectral dynamic principal component analysis (sDPCA). The resulting scores can then be removed from the nodes of interest, thus eliminating the confounding effect of other nodes within the HDN. Accordingly, causal interactions can be dissected between nodes that are isolated from the effects of the network. Extensive simulations have demonstrated the effectiveness of this approach as a tool for causality analysis in complex time series networks. The proposed methodology has also been shown to be applicable to multichannel EEG networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02360v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
  </channel>
</rss>
