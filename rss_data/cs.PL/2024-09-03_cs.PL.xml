<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Sep 2024 04:01:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Wasm-R3: Record-Reduce-Replay for Realistic and Standalone WebAssembly Benchmarks</title>
      <link>https://arxiv.org/abs/2409.00708</link>
      <description>arXiv:2409.00708v1 Announce Type: new 
Abstract: WebAssembly (Wasm for short) brings a new, powerful capability to the web as well as Edge, IoT, and embedded systems. Wasm is a portable, compact binary code format with high performance and robust sandboxing properties. As Wasm applications grow in size and importance, the complex performance characteristics of diverse Wasm engines demand robust, representative benchmarks for proper tuning. Stopgap benchmark suites, such as PolyBenchC and libsodium, continue to be used in the literature, though they are known to be unrepresentative. Porting of more complex suites remains difficult because Wasm lacks many system APIs and extracting real-world Wasm benchmarks from the web is difficult due to complex host interactions. To address this challenge, we introduce Wasm-R3, the first record and replay technique for Wasm. Wasm-R3 transparently injects instrumentation into Wasm modules to record an execution trace from inside the module, then reduces the execution trace via several optimizations, and finally produces a replay module that is executable sandalone without any host environment - on any engine. The benchmarks created by our approach are (i) realistic, because the approach records real-world web applications, (ii) faithful to the original execution, because the replay benchmark includes the unmodified original code, only adding emulation of host interactions, and (iii) standalone, because the replay benchmarks run on any engine. Applying Wasm-R3 to web-based Wasm applications in the wild demonstrates the correctness of our approach as well as the effectiveness of our optimizations, which reduce the recorded traces by 99.53 percent and the size of the replay benchmark by 9.98 percent. We release the resulting benchmark suite of 27 applications, called Wasm-R3-Bench, to the community, to inspire a new generation of realistic and standalone Wasm benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00708v1</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Doehyun Baek, Jakob Getz, Yusung Sim, Daniel Lehmann, Ben L. Titzer, Sukyoung Ryu, Michael Pradel</dc:creator>
    </item>
    <item>
      <title>Statically Contextualizing Large Language Models with Typed Holes</title>
      <link>https://arxiv.org/abs/2409.00921</link>
      <description>arXiv:2409.00921v1 Announce Type: new 
Abstract: Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate context, particularly when working with definitions not in the training data nor near the cursor. This paper demonstrates that tight integration with the type and binding structure of a language, as exposed by its language server, can address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server identifies the type and typing context of the hole being filled, even in the presence of errors, ensuring that a meaningful program sketch is always available. This allows prompting with codebase-wide contextual information not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications. These applications serve as challenge problems due to their reliance on application-specific data structures. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00921v1</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3689728</arxiv:DOI>
      <dc:creator>Andrew Blinn, Xiang Li, June Hyung Kim, Cyrus Omar</dc:creator>
    </item>
    <item>
      <title>Mix Testing: Specifying and Testing ABI Compatibility of C/C++ Atomics Implementations</title>
      <link>https://arxiv.org/abs/2409.01161</link>
      <description>arXiv:2409.01161v1 Announce Type: new 
Abstract: The correctness of complex software depends on the correctness of both the source code and the compilers that generate corresponding binary code. Compilers must do more than preserve the semantics of a single source file: they must ensure that generated binaries can be composed with other binaries to form a final executable. The compatibility of composition is ensured using an Application Binary Interface (ABI), which specifies details of calling conventions, exception handling, and so on. Unfortunately, there are no official ABIs for concurrent programs, so different atomics mappings, although correct in isolation, may induce bugs when composed. Indeed, today, mixing binaries generated by different compilers can lead to an erroneous resulting binary.
  We present mix testing: a new technique designed to find compiler bugs when the instructions of a C/C++ test are separately compiled for multiple compatible architectures and then mixed together. We define a class of compiler bugs, coined mixing bugs, that arise when parts of a program are compiled separately using different mappings from C/C++ atomic operations to assembly sequences. To demonstrate the generality of mix testing, we have designed and implemented a tool, atomic-mixer, which we have used: (a) to reproduce one existing non-mixing bug that state-of-the-art concurrency testing tools are limited to being able to find (showing that atomic-mixer at least meets the capabilities of these tools), and (b) to find four previously-unknown mixing bugs in LLVM and GCC, and one prospective mixing bug in mappings proposed for the Java Virtual Machine. Lastly, we have worked with engineers at Arm to specify, for the first time, an atomics ABI for Armv8, and have used atomic-mixer to validate the LLVM and GCC compilers against it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01161v1</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3689727</arxiv:DOI>
      <dc:creator>Luke Geeson, James Brotherston, Wilco Dijkstra, Alastair F. Donaldson, Lee Smith, Tyler Sorensen, John Wickerson</dc:creator>
    </item>
    <item>
      <title>BinSub: The Simple Essence of Polymorphic Type Inference for Machine Code</title>
      <link>https://arxiv.org/abs/2409.01841</link>
      <description>arXiv:2409.01841v1 Announce Type: new 
Abstract: Recovering high-level type information in binaries is a key task in reverse engineering and binary analysis. Binaries contain very little explicit type information. The structure of binary code is incredibly flexible allowing for ad-hoc subtyping and polymorphism. Prior work has shown that precise type inference on binary code requires expressive subtyping and polymorphism.
  Implementations of these type system features in a binary type inference algorithm have thus-far been too inefficient to achieve widespread adoption. Recent advances in traditional type inference have achieved simple and efficient principal type inference in an ML like language with subtyping and polymorphism through the framework of algebraic subtyping. BinSub, a new binary type inference algorithm, recognizes the connection between algebraic subtyping and the type system features required to analyze binaries effectively. Using this connection, BinSub achieves simple, precise, and efficient binary type inference. We show that BinSub maintains a similar precision to prior work, while achieving a 63x improvement in average runtime for 1568 functions. We also present a formalization of BinSub and show that BinSub's type system maintains the expressiveness of prior work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01841v1</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian Smith</dc:creator>
    </item>
    <item>
      <title>Type-Based Verification of Connectivity Constraints in Lattice Surgery</title>
      <link>https://arxiv.org/abs/2409.00529</link>
      <description>arXiv:2409.00529v1 Announce Type: cross 
Abstract: Fault-tolerant quantum computation using lattice surgery can be abstracted as operations on graphs, wherein each logical qubit corresponds to a vertex of the graph, and multi-qubit measurements are accomplished by connecting the vertices with paths between them. Operations attempting to connect vertices without a valid path will result in abnormal termination. As the permissible paths may evolve during execution, it is necessary to statically verify that the execution of a quantum program can be completed.
  This paper introduces a type-based method to statically verify that well-typed programs can be executed without encountering halts induced by surgery operations. Alongside, we present $\mathcal{Q}_{LS}$, a first-order quantum programming language to formalize the execution model of surgery operations. Furthermore, we provide a type checking algorithm by reducing the type checking problem to the offline dynamic connectivity problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00529v1</guid>
      <category>quant-ph</category>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryo Wakizaka, Yasunari Suzuki, Atsushi Igarashi</dc:creator>
    </item>
    <item>
      <title>Armadillo and Eigen: A Tale of Two Linear Algebra Libraries</title>
      <link>https://arxiv.org/abs/2409.00568</link>
      <description>arXiv:2409.00568v1 Announce Type: cross 
Abstract: This article introduces `cpp11eigen`, a new R package that integrates the powerful Eigen C++ library for linear algebra into the R programming environment. This article provides a detailed comparison between Armadillo and Eigen speed and syntax. The `cpp11eigen` package simplifies a part of the process of using C++ within R by offering additional ease of integration for those who require high-performance linear algebra operations in their R workflows. This work aims to discuss the tradeoff between computational efficiency and accessibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00568v1</guid>
      <category>cs.MS</category>
      <category>cs.PL</category>
      <category>stat.CO</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mauricio Vargas Sepulveda</dc:creator>
    </item>
    <item>
      <title>Benchmarking LLM Code Generation for Audio Programming with Visual Dataflow Languages</title>
      <link>https://arxiv.org/abs/2409.00856</link>
      <description>arXiv:2409.00856v1 Announce Type: cross 
Abstract: Node-based programming languages are increasingly popular in media arts coding domains. These languages are designed to be accessible to users with limited coding experience, allowing them to achieve creative output without an extensive programming background. Using LLM-based code generation to further lower the barrier to creative output is an exciting opportunity. However, the best strategy for code generation for visual node-based programming languages is still an open question. In particular, such languages have multiple levels of representation in text, each of which may be used for code generation. In this work, we explore the performance of LLM code generation in audio programming tasks in visual programming languages at multiple levels of representation. We explore code generation through metaprogramming code representations for these languages (i.e., coding the language using a different high-level text-based programming language), as well as through direct node generation with JSON. We evaluate code generated in this way for two visual languages for audio programming on a benchmark set of coding problems. We measure both correctness and complexity of the generated code. We find that metaprogramming results in more semantically correct generated code, given that the code is well-formed (i.e., is syntactically correct and runs). We also find that prompting for richer metaprogramming using randomness and loops led to more complex code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00856v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Zhang, Maria Leon, Ryan Xu, Adrian Cardenas, Amelia Wissink, Hanna Martin, Maya Srikanth, Kaya Dorogi, Christian Valadez, Pedro Perez, Citlalli Grijalva, Corey Zhang, Mark Santolucito</dc:creator>
    </item>
    <item>
      <title>An Array Intermediate Language for Mixed Cryptography</title>
      <link>https://arxiv.org/abs/2409.01587</link>
      <description>arXiv:2409.01587v1 Announce Type: cross 
Abstract: We introduce AIRduct, a new array-based intermediate representation designed to support generating efficient code for interactive programs employing multiple cryptographic mechanisms. AIRduct is intended as an IR for the Viaduct compiler, which can synthesize secure, distributed programs with an extensible suite of cryptography. Therefore, AIRduct supports an extensible variety of cryptographic mechanisms, including MPC and ZKP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01587v1</guid>
      <category>cs.CR</category>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivian Ding, Co\c{s}ku Acay, Andrew C. Myers</dc:creator>
    </item>
    <item>
      <title>Strengthening Solidity Invariant Generation: From Post- to Pre-Deployment</title>
      <link>https://arxiv.org/abs/2409.01804</link>
      <description>arXiv:2409.01804v1 Announce Type: cross 
Abstract: Invariants are essential for ensuring the security and correctness of Solidity smart contracts, particularly in the context of blockchain's immutability and decentralized execution. This paper introduces InvSol, a novel framework for pre-deployment invariant generation tailored specifically for Solidity smart contracts. Unlike existing solutions, namely InvCon, InvCon+, and Trace2Inv, that rely on post-deployment transaction histories on Ethereum mainnet, InvSol identifies invariants before deployment and offers comprehensive coverage of Solidity language constructs, including loops. Additionally, InvSol incorporates custom templates to effectively prevent critical issues such as reentrancy, out-of-gas errors, and exceptions during invariant generation. We rigorously evaluate InvSol using a benchmark set of smart contracts and compare its performance with state-of-the-art solutions. Our findings reveal that InvSol significantly outperforms these tools, demonstrating its effectiveness in handling new contracts with limited transaction histories. Notably, InvSol achieves a 15% improvement in identifying common vulnerabilities compared to InvCon+ and is able to address certain crucial vulnerabilities using specific invariant templates, better than Trace2Inv.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01804v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kartik Kaushik, Raju Halder, Samrat Mondal</dc:creator>
    </item>
    <item>
      <title>Software Verification with CPAchecker 3.0: Tutorial and User Guide (Extended Version)</title>
      <link>https://arxiv.org/abs/2409.02094</link>
      <description>arXiv:2409.02094v1 Announce Type: cross 
Abstract: This tutorial provides an introduction to CPAchecker for users. CPAchecker is a flexible and configurable framework for software verification and testing. The framework provides many abstract domains, such as BDDs, explicit values, intervals, memory graphs, and predicates, and many program-analysis and model-checking algorithms, such as abstract interpretation, bounded model checking, Impact, interpolation-based model checking, k -induction, PDR, predicate abstraction, and symbolic execution. This tutorial presents basic use cases for CPAchecker in formal software verification, focusing on its main verification techniques with their strengths and weaknesses. It also shows further use cases of CPAchecker for test-case generation and witness-based result validation. The envisioned readers are assumed to possess a background in automatic formal verification and program analysis, but prior knowledge of CPAchecker is not required. This tutorial and user guide is based on CPAchecker in version 3.0. This user guide's latest version and other documentation are available at https://cpachecker.sosy-lab.org/doc.php.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02094v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Baier, Dirk Beyer, Po-Chun Chien, Marie-Christine Jakobs, Marek Jankola, Matthias Kettl, Nian-Ze Lee, Thomas Lemberger, Marian Lingsch-Rosenfeld, Henrik Wachowitz, Philipp Wendler</dc:creator>
    </item>
    <item>
      <title>Fair Asynchronous Session Subtyping</title>
      <link>https://arxiv.org/abs/2101.08181</link>
      <description>arXiv:2101.08181v4 Announce Type: replace 
Abstract: Session types are widely used as abstractions of asynchronous message passing systems. Refinement for such abstractions is crucial as it allows improvements of a given component without compromising its compatibility with the rest of the system. In the context of session types, the most general notion of refinement is asynchronous session subtyping, which allows message emissions to be anticipated w.r.t. a bounded amount of message consumptions. In this paper we investigate the possibility to anticipate emissions w.r.t. an unbounded amount of consumptions: to this aim we propose to consider fair compliance over asynchronous session types and fair refinement as the relation that preserves it. This allows us to propose a novel variant of session subtyping that leverages the notion of controllability from service contract theory and that is a sound characterisation of fair refinement. In addition, we show that both fair refinement and our novel subtyping are undecidable. We also present a sound algorithm which deals with examples that feature potentially unbounded buffering. Finally, we present an implementation of our algorithm and an empirical evaluation of it on synthetic benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.08181v4</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mario Bravetti, Julien Lange, Gianluigi Zavattaro</dc:creator>
    </item>
    <item>
      <title>Minotaur: A SIMD-Oriented Synthesizing Superoptimizer</title>
      <link>https://arxiv.org/abs/2306.00229</link>
      <description>arXiv:2306.00229v3 Announce Type: replace 
Abstract: A superoptimizing compiler--one that performs a meaningful search of the program space as part of the optimization process--can find optimization opportunities that are missed by even the best existing optimizing compilers. We created Minotaur: a superoptimizer for LLVM that uses program synthesis to improve its code generation, focusing on integer and floating-point SIMD code. On an Intel Cascade Lake processor, Minotaur achieves an average speedup of 7.3\% on the GNU Multiple Precision library (GMP)'s benchmark suite, with a maximum speedup of 13\%. On SPEC CPU 2017, our superoptimizer produces an average speedup of 1.5\%, with a maximum speedup of 4.5\% for 638.imagick. Every optimization produced by Minotaur has been formally verified, and several optimizations that it has discovered have been implemented in LLVM as a result of our work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.00229v3</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengyang Liu, Stefan Mada, John Regehr</dc:creator>
    </item>
    <item>
      <title>Effects and Coeffects in Call-By-Push-Value (Extended Version)</title>
      <link>https://arxiv.org/abs/2311.11795</link>
      <description>arXiv:2311.11795v2 Announce Type: replace 
Abstract: Effect and coeffect tracking integrate many types of compile-time analysis, such as cost, liveness, or dataflow, directly into a language's type system. In this paper, we investigate the addition of effect and coeffect tracking to the type system of call-by-push-value (CBPV), a computational model useful in compilation for its isolation of effects and for its ability to cleanly express both call-by-name and call-by-value computations. Our main result is effect-and-coeffect soundness, which asserts that the type system accurately bounds the effects that the program may trigger during execution and accurately tracks the demands that the program may make on its environment. This result holds for two different dynamic semantics: a generic one that can be adapted for different coeffects and one that is adapted for reasoning about resource usage. In particular, the second semantics discards the evaluation of unused values and pure computations while ensuring that effectful computations are always evaluated, even if their results are not required. Our results have been mechanized using the Coq proof assistant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11795v2</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3689750</arxiv:DOI>
      <dc:creator>Cassia Torczon, Emmanuel Su\'arez Acevedo, Shubh Agrawal, Joey Velez-Ginorio, Stephanie Weirich</dc:creator>
    </item>
    <item>
      <title>Towards Neural Synthesis for SMT-Assisted Proof-Oriented Programming</title>
      <link>https://arxiv.org/abs/2405.01787</link>
      <description>arXiv:2405.01787v2 Announce Type: replace 
Abstract: Proof-oriented programs mix computational content with proofs of program correctness. However, the human effort involved in programming and proving is still substantial, despite the use of Satisfiability Modulo Theories (SMT) solvers to automate proofs in languages such as F*.
  Seeking to spur research on using AI to automate the construction of proof-oriented programs, we curate a dataset of 600K lines of open-source F* programs and proofs, including software used in production systems ranging from Windows and Linux, to Python and Firefox. Our dataset includes around 32K top-level F* definitions, each representing a type-directed program and proof synthesis problem -- producing a definition given a formal specification expressed as an F* type. We provide a program-fragment checker that queries F* to check the correctness of candidate solutions. We believe this is the largest corpus of SMT-assisted program proofs coupled with a reproducible program-fragment checker.
  Grounded in this dataset, we investigate the use of AI to synthesize programs and their proofs in F*, with promising results. Our main finding in that the performance of fine-tuned smaller language models (such as Phi-2 or StarCoder) compare favorably with large language models (such as GPT-4), at a much lower computational cost. We also identify various type-based retrieval augmentation techniques and find that they boost performance significantly. With detailed error analysis and case studies, we identify potential strengths and weaknesses of models and techniques and suggest directions for future improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01787v2</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saikat Chakraborty, Gabriel Ebner, Siddharth Bhat, Sarah Fakhoury, Sakina Fatima, Shuvendu Lahiri, Nikhil Swamy</dc:creator>
    </item>
    <item>
      <title>Guard Analysis and Safe Erasure Gradual Typing: a Type System for Elixir</title>
      <link>https://arxiv.org/abs/2408.14345</link>
      <description>arXiv:2408.14345v2 Announce Type: replace 
Abstract: We define several techniques to extend gradual typing with semantic subtyping, specifically targeting dynamic languages. Focusing on the Elixir programming language, we provide the theoretical foundations for its type system. Our approach demonstrates how to achieve type soundness for gradual typing in existing dynamic languages without modifying their compilation, while still maintaining high precision. This is accomplished through the static detection of "strong functions", which leverage runtime checks inserted by the programmer or performed by the virtual machine, and through a fine-grained type analysis of pattern-matching expressions with guards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14345v2</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Castagna, Guillaume Duboc</dc:creator>
    </item>
    <item>
      <title>Extending the C/C++ Memory Model with Inline Assembly</title>
      <link>https://arxiv.org/abs/2408.17208</link>
      <description>arXiv:2408.17208v2 Announce Type: replace 
Abstract: Programs written in C/C++ often include inline assembly: a snippet of architecture-specific assembly code used to access low-level functionalities that are impossible or expensive to simulate in the source language. Although inline assembly is widely used, its semantics has not yet been formally studied.
  In this paper, we overcome this deficiency by investigating the effect of inline assembly on the consistency semantics of C/C++ programs. We propose the first memory model of the C++ Programming Language with support for inline assembly for Intel's x86 including non-temporal stores and store fences. We argue that previous provably correct compiler optimizations and correct compiler mappings should remain correct under such an extended model and we prove that this requirement is met by our proposed model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17208v2</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3689749</arxiv:DOI>
      <dc:creator>Paulo Em\'ilio de Vilhena, Ori Lahav, Viktor Vafeiadis, Azalea Raad</dc:creator>
    </item>
    <item>
      <title>On Complexity Bounds and Confluence of Parallel Term Rewriting</title>
      <link>https://arxiv.org/abs/2305.18250</link>
      <description>arXiv:2305.18250v4 Announce Type: replace-cross 
Abstract: We revisit parallel-innermost term rewriting as a model of parallel computation on inductive data structures and provide a corresponding notion of runtime complexity parametric in the size of the start term. We propose automatic techniques to derive both upper and lower bounds on parallel complexity of rewriting that enable a direct reuse of existing techniques for sequential complexity. Our approach to find lower bounds requires confluence of the parallel-innermost rewrite relation, thus we also provide effective sufficient criteria for proving confluence. The applicability and the precision of the method are demonstrated by the relatively light effort in extending the program analysis tool AProVE and by experiments on numerous benchmarks from the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.18250v4</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tha\"is Baudon, Carsten Fuhs, Laure Gonnord</dc:creator>
    </item>
    <item>
      <title>Explaining Explanations in Probabilistic Logic Programming</title>
      <link>https://arxiv.org/abs/2401.17045</link>
      <description>arXiv:2401.17045v4 Announce Type: replace-cross 
Abstract: The emergence of tools based on artificial intelligence has also led to the need of producing explanations which are understandable by a human being. In most approaches, the system is considered a black box, making it difficult to generate appropriate explanations. In this work, though, we consider a setting where models are transparent: probabilistic logic programming (PLP), a paradigm that combines logic programming for knowledge representation and probability to model uncertainty. However, given a query, the usual notion of explanation is associated with a set of choices, one for each random variable of the model. Unfortunately, such a set does not explain why the query is true and, in fact, it may contain choices that are actually irrelevant for the considered query. To improve this situation, we present in this paper an approach to explaining explanations which is based on defining a new query-driven inference mechanism for PLP where proofs are labeled with "choice expressions", a compact and easy to manipulate representation for sets of choices. The combination of proof trees and choice expressions allows us to produce comprehensible query justifications with a causal structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17045v4</guid>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Germ\'an Vidal</dc:creator>
    </item>
    <item>
      <title>A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond</title>
      <link>https://arxiv.org/abs/2403.14734</link>
      <description>arXiv:2403.14734v3 Announce Type: replace-cross 
Abstract: Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we also observe a co-evolving shift. It spans from initial endeavors to tackling specific scenarios, through exploring a diverse array of tasks during its rapid expansion, to currently focusing on tackling increasingly complex and varied real-world challenges. Building on our examination of the developmental trajectories, we further investigate the emerging synergies between code intelligence and broader machine intelligence, uncovering new cross-domain opportunities and illustrating the substantial influence of code intelligence across various domains. Finally, we delve into both the opportunities and challenges associated with this field, alongside elucidating our insights on the most promising research directions. An ongoing, dynamically updated project and resources associated with this survey have been released at https://github.com/QiushiSun/NCISurvey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14734v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, Qipeng Guo, Xipeng Qiu, Pengcheng Yin, Xiaoli Li, Fei Yuan, Lingpeng Kong, Xiang Li, Zhiyong Wu</dc:creator>
    </item>
    <item>
      <title>FORAY: Towards Effective Attack Synthesis against Deep Logical Vulnerabilities in DeFi Protocols</title>
      <link>https://arxiv.org/abs/2407.06348</link>
      <description>arXiv:2407.06348v2 Announce Type: replace-cross 
Abstract: Blockchain adoption has surged with the rise of Decentralized Finance (DeFi) applications. However, the significant value of digital assets managed by DeFi protocols makes them prime targets for attacks. Current smart contract vulnerability detection tools struggle with DeFi protocols due to deep logical bugs arising from complex financial interactions between multiple smart contracts. These tools primarily analyze individual contracts and resort to brute-force methods for DeFi protocols crossing numerous smart contracts, leading to inefficiency. We introduce Foray, a highly effective attack synthesis framework against deep logical bugs in DeFi protocols. Foray proposes a novel attack sketch generation and completion framework. Specifically, instead of treating DeFis as regular programs, we design a domain-specific language (DSL) to lift the low-level smart contracts into their high-level financial operations. Based on our DSL, we first compile a given DeFi protocol into a token flow graph, our graphical representation of DeFi protocols. Then, we design an efficient sketch generation method to synthesize attack sketches for a certain attack goal (e.g., price manipulation, arbitrage, etc.). This algorithm strategically identifies candidate sketches by finding reachable paths in TFG, which is much more efficient than random enumeration. For each candidate sketch written in our DSL, Foray designs a domain-specific symbolic compilation to compile it into SMT constraints. Our compilation simplifies the constraints by removing redundant smart contract semantics. It maintains the usability of symbolic compilation, yet scales to problems orders of magnitude larger. Finally, the candidates are completed via existing solvers and are transformed into concrete attacks via direct syntax transformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06348v2</guid>
      <category>cs.CR</category>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3658644.3690293</arxiv:DOI>
      <dc:creator>Hongbo Wen, Hanzhi Liu, Jiaxin Song, Yanju Chen, Wenbo Guo, Yu Feng</dc:creator>
    </item>
    <item>
      <title>cpp11armadillo: An R Package to Use the Armadillo C++ Library</title>
      <link>https://arxiv.org/abs/2408.11074</link>
      <description>arXiv:2408.11074v3 Announce Type: replace-cross 
Abstract: This article introduces 'cpp11armadillo', a new R package that integrates the powerful Armadillo C++ library for linear algebra into the R programming environment. Targeted primarily at social scientists and other non-programmers, this article explains the computational benefits of moving code to C++ in terms of speed and syntax. We provide a comprehensive overview of Armadillo's capabilities, highlighting its user-friendly syntax akin to MATLAB and its efficiency for computationally intensive tasks. The 'cpp11armadillo' package simplifies a part of the process of using C++ within R by offering additional ease of integration for those who require high-performance linear algebra operations in their R workflows. This work aims to bridge the gap between computational efficiency and accessibility, making advanced linear algebra operations more approachable for R users without extensive programming backgrounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11074v3</guid>
      <category>cs.MS</category>
      <category>cs.PL</category>
      <category>stat.CO</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mauricio Vargas Sep\'ulveda, Jonathan Schneider Malamud</dc:creator>
    </item>
    <item>
      <title>Galley: Modern Query Optimization for Sparse Tensor Programs</title>
      <link>https://arxiv.org/abs/2408.14706</link>
      <description>arXiv:2408.14706v3 Announce Type: replace-cross 
Abstract: The tensor programming abstraction has become a foundational paradigm for modern computing. This framework allows users to write high performance programs for bulk computation via a high-level imperative interface. Recent work has extended this paradigm to sparse tensors (i.e. tensors where most entries are not explicitly represented) with the use of sparse tensor compilers. These systems excel at producing efficient code for computation over sparse tensors, which may be stored in a wide variety of formats. However, they require the user to manually choose the order of operations and the data formats at every step. Unfortunately, these decisions are both highly impactful and complicated, requiring significant effort to manually optimize. In this work, we present Galley, a system for declarative sparse tensor programming. Galley performs cost-based optimization to lower these programs to a logical plan then to a physical plan. It then leverages sparse tensor compilers to execute the physical plan efficiently. We show that Galley achieves high performance on a wide variety of problems including machine learning algorithms, subgraph counting, and iterative graph algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14706v3</guid>
      <category>cs.DB</category>
      <category>cs.PL</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Deeds, Willow Ahrens, Magda Balazinska, Dan Suciu</dc:creator>
    </item>
  </channel>
</rss>
