<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Sep 2025 01:33:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sparse Seemingly Unrelated Regression (SSUR) Copula Mixed Models for Multivariate Loss Reserving</title>
      <link>https://arxiv.org/abs/2509.05426</link>
      <description>arXiv:2509.05426v1 Announce Type: new 
Abstract: Insurance companies often operate across multiple interrelated lines of business (LOBs), and accounting for dependencies between them is essential for accurate reserve estimation and risk capital determination. In our previous work on the Extended Deep Triangle (EDT), we demonstrated that a more flexible model that uses multiple companies' data reduces reserve prediction error and increases diversification benefits. However, the EDT's limitation lies in its limited interpretability of the dependence structure, which is an important feature needed by insurers to guide strategic decisions. Motivated by the need for interpretability and flexibility, this paper proposes a Seemingly Unrelated Regression (SUR) copula mixed model to handle heterogeneous data across multiple companies. The model incorporates random effects to capture company-specific heterogeneity, uses flexible marginal distributions across LOBs, and treats development and accident year effects as fixed effects with shrinkage via LASSO to enhance robustness. We estimate the model using an iterative two-stage procedure and generate predictive reserve distributions via a modified bootstrap that accounts for systematic effects, dependence structure, and sparse fixed-effect coefficients. Through simulation studies and real data from the National Association of Insurance Commissioners, we show that the proposed model outperforms the SUR copula regression model in terms of reserve accuracy and generates larger risk capital gain. Overall, the SUR copula mixed model achieves better predictive performance, greater risk diversification, and retains interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05426v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pengfei Cai, Anas Abdallah, Pratheepa Jeganathan</dc:creator>
    </item>
    <item>
      <title>Multidimensional constructs and moderated linear and nonlinear factor analysis</title>
      <link>https://arxiv.org/abs/2509.05443</link>
      <description>arXiv:2509.05443v1 Announce Type: new 
Abstract: Multidimensional factor models with moderations on all model parameters have so far been limited to single-factor and two-factor models. This does not align well with existing psychological measures, which are commonly intended to assess 3-5 dimensions of a latent construct. In this paper, we introduce a penalized maximum likelihood approach for multidimensional MNLFA that permits moderation of item intercepts, loadings, residual variances, factor means, variances, and correlations across three or more latent factors. Our approach incorporates ridge, lasso, and alignment penalties to stabilize estimation and detect partial measurement non-invariance while preserving model interpretability. We derive closed-form analytic gradients of the likelihood, eliminating the need for costly numerical or MCMC-based approximations, and demonstrate how this dramatically improves computational efficiency. Through simulation and application, we illustrate that penalized MNLFA recovers complex moderation patterns in multidimensional constructs and provides a scalable alternative to Bayesian implementations. We conclude by discussing the theoretical implications of penalization for measurement invariance, computational considerations, and future directions for extending the framework to categorical indicators, longitudinal data, and applied research contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05443v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. Noah Padgett</dc:creator>
    </item>
    <item>
      <title>Dynamic Prediction of Joint Longitudinal-Survival Models Using a Similarity-Based Approach</title>
      <link>https://arxiv.org/abs/2509.05476</link>
      <description>arXiv:2509.05476v1 Announce Type: new 
Abstract: Longitudinal and time-to-event data are often analyzed in biomarker research to study the association between the longitudinal biomarker measurements and the event-time outcome, in which the longitudinal information contributes to the probability of the outcome of interest. An attractive nature of fitting a joint model on this type of data is that we can dynamically predict the survival probability as additional longitudinal information becomes available. We propose a new similarity-based method for the dynamic prediction of joint models where we consider training the model on only a targeted subset of the data to obtain an improved outcome prediction. Through comprehensive simulation study and an application to intensive care unit data, we demonstrate that the predictive performance of the dynamic prediction of joint models can be improved with our proposed similarity-based approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05476v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minzee Kim, Joel A. Dubin</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Confounding Variables and Limited Information</title>
      <link>https://arxiv.org/abs/2509.05520</link>
      <description>arXiv:2509.05520v1 Announce Type: new 
Abstract: A central challenge in statistical inference is the presence of confounding variables that may distort observed associations between treatment and outcome. Conventional "causal" methods, grounded in assumptions such as ignorability, exclude the possibility of unobserved confounders, leading to posterior inferences that overstate certainty. We develop a Bayesian framework that relaxes these assumptions by introducing entropy-favoring priors over hypothesis spaces that explicitly allow for latent confounding variables and partial information. Using the case of Simpson's paradox, we demonstrate how this approach produces logically consistent posterior distributions that widen credibly intervals in the presence of potential confounding. Our method provides a generalizable, information-theoretic foundation for more robust predictive inference in observational sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05520v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ellis Scharfenaker, Duncan K. Foley</dc:creator>
    </item>
    <item>
      <title>Optimal scheduling of interim analyses in group sequential trials</title>
      <link>https://arxiv.org/abs/2509.05537</link>
      <description>arXiv:2509.05537v1 Announce Type: new 
Abstract: Group sequential designs (GSDs) are well established and the most commonly used adaptive design in confirmatory clinical trials with interim analyses. However, they remain underutilised, and their implementation involves unique theoretical and practical decisions that demand careful consideration to optimise efficiency. A common practice is to schedule interim analyses at equal intervals based on calendar time or accumulated data. While straightforward, this approach does not completely exploit the potential sample size savings achievable with GSDs. To address this challenge, we develop OptimInterim, an R-based tool that can determine the optimal scheduling of interim analyses to minimise the expected sample size under the alternative hypothesis while controlling overall type I and type II errors. Our method accommodates trials with continuous or binary endpoints, allows multiple interim analyses and supports a range of stopping boundaries. Through extensive simulations, we demonstrate that optimally spaced interim analyses can yield substantial savings in expected sample size compared to equally spaced interim analyses, without compromising the maximum sample size, across various endpoint types, effect sizes, error rates and stopping rules. We illustrate its practical utility with two landmark trials evaluating steroid use in septic shock. Notably, for given type I and type II error rates, the optimal scheduling is independent of endpoint types and effect sizes, ensuring broad applicability across a wide range of trial contexts. To facilitate implementation, we offer a ready-to-use reference table of optimal schedules for up to eight interim analyses under commonly used error rates and stopping rules. Access OptimInterim at https://github.com/zhangyi-he/GSD_OptimInterim.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05537v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhangyi He, Suzie Cro, Laurent Billot</dc:creator>
    </item>
    <item>
      <title>Interpretable dimension reduction for compositional data</title>
      <link>https://arxiv.org/abs/2509.05563</link>
      <description>arXiv:2509.05563v1 Announce Type: new 
Abstract: High-dimensional compositional data, such as those from human microbiome studies, pose unique statistical challenges due to the simplex constraint and excess zeros. While dimension reduction is indispensable for analyzing such data, conventional approaches often rely on log-ratio transformations that compromise interpretability and distort the data through ad hoc zero replacements. We introduce a novel framework for interpretable dimension reduction of compositional data that avoids extra transformations and zero imputations. Our approach generalizes the concept of amalgamation by softening its operation, mapping high-dimensional compositions directly to a lower-dimensional simplex, which can be visualized in ternary plots. The framework further provides joint visualization of the reduction matrix, enabling intuitive, at-a-glance interpretation. To achieve optimal reduction within our framework, we incorporate sufficient dimension reduction, which defines a new identifiable objective: the central compositional subspace. For estimation, we propose a compositional kernel dimension reduction (CKDR) method. The estimator is provably consistent, exhibits sparsity that reveals underlying amalgamation structures, and comes with an intrinsic predictive model for downstream analyses. Applications to real microbiome datasets demonstrate that our approach provides a powerful graphical exploration tool for uncovering meaningful biological patterns, opening a new pathway for analyzing high-dimensional compositional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05563v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyoung Park, Cheolwoo Park, Jeongyoun Ahn</dc:creator>
    </item>
    <item>
      <title>WISE: A Weighted Similarity Aggregation Test for Serial Independence</title>
      <link>https://arxiv.org/abs/2509.05678</link>
      <description>arXiv:2509.05678v1 Announce Type: new 
Abstract: We propose a nonparametric test for serial independence that aggregates pairwise similarities of observations with lag-dependent weights. The resulting statistic is powerful to general forms of temporal dependence, including nonlinear and uncorrelated alternatives, and applies to ultra-high-dimensional and non-Euclidean data. We derive asymptotic normality under both permutation and population nulls, and establish consistency in classical large-sample and high-dimension-low-sample-size (HDLSS) regimes. The test therefore provides the first theoretical power guarantees for serial independence in the HDLSS setting. Simulations demonstrate accurate size and strong power against a wide range of alternatives, showing significant power improvement over existing methods under various high-dimensional time series models. An application to spatio-temporal data illustrates the method's utility for non-Euclidean observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05678v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qihua Zhu, Mingshuo Liu, Yuefeng Han, Doudou Zhou</dc:creator>
    </item>
    <item>
      <title>Cumulative/Dynamic Time-Dependent ROC Analysis for Left-Truncated and Right-Censored Data: Estimators and Comparison</title>
      <link>https://arxiv.org/abs/2509.05693</link>
      <description>arXiv:2509.05693v1 Announce Type: new 
Abstract: Time-dependent Receiver Operating Characteristics (ROC) analysis is a standard method to evaluate the discriminative performance of biomarkers or risk scores for time-to-event outcomes. Extensions of this useful method to left-truncated right-censored data have been understudied, with the exception of Li 2017. In this paper, we first extended the estimators in Li 2017 to several regression-type estimators that account for independent or covariate-induced dependent left truncation and right censoring. We further proposed novel inverse probability weighting estimators of cumulative sensitivity, dynamic specificity, and area under the ROC curve (AUC), where the weights simultaneously account for left truncation and right censoring, with or without adjusting for covariates. We demonstrated the proposed AUC estimators in simulation studies with different scenarios. We performed the proposed time-dependent ROC analysis to evaluate the predictive performance of two risk prediction models of heart failure by Chow et al. 2015 in five-year childhood cancer survivors using the St. Jude Lifetime Cohort Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05693v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kendrick Li, Mithun Kumar Acharjee</dc:creator>
    </item>
    <item>
      <title>LORDs: Locally Optimal Restricted Designs for Phase I/II Dose-Finding Studies</title>
      <link>https://arxiv.org/abs/2509.05825</link>
      <description>arXiv:2509.05825v1 Announce Type: new 
Abstract: We propose Locally Optimal Restricted Designs (LORDs) for phase I/II dose-finding studies that focus on both efficacy and toxicity outcomes. As an illustrative application, we find various LORDs for a 4-parameter continuation-ratio (CR) model defined on a user-specified dose range, where ethical constraints are imposed to prevent patients from receiving excessively toxic or ineffective doses. We study the structure and efficiency of LORDs across several experimental scenarios and assess the sensitivity of the results to changes in the design problem, such as adjusting the dose range or redefining target doses. Additionally, we compare LORDs with a more heuristic phase I/II design and show that LORDs offer more statistically efficient and ethical benchmark designs. A key innovation in our work is the use of a nature-inspired metaheuristic algorithm to determine dose-finding designs. This algorithm is free from assumptions, fast, and highly flexible. As a result, more realistic and adaptable designs for any model and design criterion with multiple practical constraints can be readily found and implemented. Our work also is the first to suggest how to modify and informatively select the next set of doses for the next study for enhanced statistical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05825v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oleksandr Sverdlov, Yevgen Ryeznik, Weng Kee Wong</dc:creator>
    </item>
    <item>
      <title>Decision Theoretic Subgroup Detection With Bayesian Machine Learning</title>
      <link>https://arxiv.org/abs/2509.05832</link>
      <description>arXiv:2509.05832v1 Announce Type: new 
Abstract: We consider the problem of identifying promising subpopulations in terms of treatment effectiveness or treatment effect heterogeneity, from a Bayesian decision theoretic perspective. We first show that a straight-forward application of Bayesian decision theory to subgroup detection leads to a counter-intuitive risk-seeking (RS) behavior. Motivated by this observation, we introduce the Bayesian Risk-Aware Inference and Detection of Subgroups (BRAIDS) utility and use it to perform subgroup selection and post selection inference. The BRAIDS utility interpolates between risk-seeking (RS) and risk-averse (RA) identifications of subgroups, with a variant of the virtual twins algorithm as its risk-neutral midpoint. We also argue that effective subgroup estimation and inference requires the use of regularization priors to safeguard inferences from the winner's curse. We provide empirical evidence that posterior credible intervals for subgroup effects can still obtain nominal coverage levels, provided that an appropriate prior distribution is chosen. The proposed framework is illustrated on data from clinical trial assessing the efficacy of canagliflozin as a treatment for type 2 diabetes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05832v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Entejar Alam, Poorbita Kundu, Antonio R. Linero</dc:creator>
    </item>
    <item>
      <title>Beyond ATE: Multi-Criteria Design for A/B Testing</title>
      <link>https://arxiv.org/abs/2509.05864</link>
      <description>arXiv:2509.05864v1 Announce Type: new 
Abstract: A/B testing is a widely adopted methodology for estimating conditional average treatment effects (CATEs) in both clinical trials and online platforms. While most existing research has focused primarily on maximizing estimation accuracy, practical applications must also account for additional objectives-most notably welfare or revenue loss. In many settings, it is critical to administer treatments that improve patient outcomes or to implement plans that generate greater revenue from customers. Within a machine learning framework, such objectives are naturally captured through the notion of cumulative regret. In this paper, we investigate the fundamental trade-off between social welfare loss and statistical accuracy in (adaptive) experiments with heterogeneous treatment effects. We establish matching upper and lower bounds for the resulting multi-objective optimization problem and employ the concept of Pareto optimality to characterize the necessary and sufficient conditions for optimal experimental designs. Beyond estimating CATEs, practitioners often aim to deploy treatment policies that maximize welfare across the entire population. We demonstrate that our Pareto-optimal adaptive design achieves optimal post-experiment welfare, irrespective of the in-experiment trade-off between accuracy and welfare. Furthermore, since clinical and commercial data are often highly sensitive, it is essential to incorporate robust privacy guarantees into any treatment-allocation mechanism. To this end, we develop differentially private algorithms that continue to achieve our established lower bounds, showing that privacy can be attained at negligible cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05864v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiachun Li, Kaining Shi, David Simchi-Levi</dc:creator>
    </item>
    <item>
      <title>Simplicial clustering using the $\alpha$--transformation</title>
      <link>https://arxiv.org/abs/2509.05945</link>
      <description>arXiv:2509.05945v1 Announce Type: new 
Abstract: We introduce two simplicial clustering approaches for compositional data, that are adaptations of the $K$--means and of the Gaussian mixture models algorithms, by employing the $\alpha$--transformation. By utilizing clustering validation indices we can decide on the number of clusters and choose the value of $\alpha$ for the $K$--means, while for the model-based clustering approach information criteria complete this task. extensive simulation studies compare the performance of these two approaches and a real data set illustrates their performance in real world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05945v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris, Nikolaos Kontemeniotis</dc:creator>
    </item>
    <item>
      <title>Joint modeling of low and high extremes using a multivariate extended generalized Pareto distribution</title>
      <link>https://arxiv.org/abs/2509.05982</link>
      <description>arXiv:2509.05982v1 Announce Type: new 
Abstract: In most risk assessment studies, it is important to accurately capture the entire distribution of the multivariate random vector of interest from low to high values. For example, in climate sciences, low precipitation events may lead to droughts, while heavy rainfall may generate large floods, and both of these extreme scenarios can have major impacts on the safety of people and infrastructure, as well as agricultural or other economic sectors. In the univariate case, the extended generalized Pareto distribution (eGPD) was specifically developed to accurately model low, moderate, and high precipitation intensities, while bypassing the threshold selection procedure usually conducted in extreme-value analyses. In this work, we extend this approach to the multivariate case. The proposed multivariate eGPD has the following appealing properties: (1) its marginal distributions behave like univariate eGPDs; (2) its lower and upper joint tails comply with multivariate extreme-value theory, with key parameters separately controlling dependence in each joint tail; and (3) the model allows for fast simulation and is thus amenable to simulation-based inference. We propose estimating model parameters by leveraging modern neural approaches, where a neural network, once trained, can provide point estimates, credible intervals, or full posterior approximations in a fraction of a second. Our new methodology is illustrated by application to daily rainfall times series data from the Netherlands. The proposed model is shown to provide satisfactory marginal and dependence fits from low to high quantiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05982v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noura Alotaibi, Matthew Sainsbury-Dale, Philippe Naveau, Carlo Gaetan, Rapha\"el Huser</dc:creator>
    </item>
    <item>
      <title>Spatial weights matrix selection and model averaging for multivariate spatial autoregressive models</title>
      <link>https://arxiv.org/abs/2509.06005</link>
      <description>arXiv:2509.06005v1 Announce Type: new 
Abstract: In this paper, we focus on the model specification problem in multivariate spatial econometric models when a candidate set for the spatial weights matrix is available. We propose a model selection method for the multivariate spatial autoregressive model, when the true spatial weights matrix may not be in the candidates. We show that the selected estimator is asymptotically optimal in the sense of minimizing the squared loss. If the candidate set contains the true spatial weights matrix, the method has selection consistency. We further propose a model averaging estimator that combines a set of candidate models and show its asymptotic optimality. Monte Carlo simulation results indicate that the proposed model selection and model averaging estimators perform quite well in finite samples. The proposed methods are applied to a Sina Weibo data to reveal how the user's posting behavior is influenced by the users that he follows. The analysis results indicate that the influence tends to be uniformly distributed among the user's followee, or linearly correlated with the number of followers of the followee.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06005v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Miao, Fang Fang, Xuening Zhu, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Treatment effect extrapolation in the presence of unmeasured confounding</title>
      <link>https://arxiv.org/abs/2509.06045</link>
      <description>arXiv:2509.06045v1 Announce Type: new 
Abstract: While randomised controlled trials (RCTs) are the gold standard for estimating causal treatment effects, their limited sample sizes and restrictive criteria make it difficult to extrapolate to a broader population. Observational data, while larger, suffer from unmeasured confounding. Therefore, we can combine the strengths of both data sources for more accurate results.
  This work extends existing methods that use RCTs to debias conditional average treatment effects (CATEs) estimated in observational data by defining a deconfounding function. Our proposed approach borrows information from RCTs of multiple related treatments to improve the extrapolation of CATEs.
  Simulation results showed that, for non-linear deconfounding functions, using only one RCT poorly estimates the CATE outside of the support of that RCT. This is emphasised for smaller RCTs. Borrowing information from a second RCT provided more accurate estimates of the CATE outside of the support of both RCTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06045v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephanie Riley, Ricardo Silva, Matthew Sperrin</dc:creator>
    </item>
    <item>
      <title>Measuring General Associations in Time Series: An Adaptation and Empirical Evaluation of the CODEC Coefficient in Determining Autoregressive Dynamics</title>
      <link>https://arxiv.org/abs/2509.06111</link>
      <description>arXiv:2509.06111v1 Announce Type: new 
Abstract: Identifying the number of lags to include in an autoregressive model remains an open research problem due to the computational burden of treating it as a hyperparameter, especially in complex models. This study explores model-agnostic association measures, including Pearson, Spearman, and an adaptation of the recently proposed conditional dependence coefficient (CODEC), for guiding lag selection in time series. We adapt and implement the CODEC-based Feature Ordering by Conditional Independence (CODEC-FOCI) algorithm and evaluate its performance through extensive simulations across linear, nonlinear, stationary, nonstationary, seasonal, and heteroskedastic processes. Results show that CODEC outperforms classical correlation-based measures in nonlinear and nonstationary settings, especially for large sample sizes. In contrast, Pearson performs better in purely linear models. Applications to benchmark datasets confirm that the CODEC approach identifies lag structures consistent with those reported in the literature. These findings highlight CODEC's potential as a practical, model-free tool for exploratory lag identification in time series analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06111v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Pablo Monta\~no, Mario E. Arrieta-Prieto</dc:creator>
    </item>
    <item>
      <title>A simulation-free extrapolation method for misspecified models with errors-in-variables in epidemiological studies</title>
      <link>https://arxiv.org/abs/2509.06118</link>
      <description>arXiv:2509.06118v1 Announce Type: new 
Abstract: In epidemiology studies, it is often of interest to consider a misspecified model that categorizes continuous variables, such as calorie and nutrient intake, to analyze disease risk and achieve better model interpretation. When the original continuous variable is contaminated with measurement errors, ignoring this issue and performing regular statistical analysis leads to severely biased point estimators and invalid confidence intervals. Though errors-in-variables is a well-known critical issue in many areas, most existing methods addressing measurement errors either do not consider model misspecification or have strong parametric distributional assumptions. To this end, we propose a flexible simulation-free extrapolation method, which provides valid and robust statistical inference under various models and has no distributional assumptions on the observed data. Through extensive numerical studies, we demonstrate that the proposed method can provide unbiased point estimation and valid confidence intervals under various regression models. Through the analysis of the Food Frequency Questionnaire in UK Biobank data, we show that ignoring measurement errors underestimates the impact of high fat intake on BMI and obesity by at least 30% and 60%, respectively, compared to the results of correcting measurement errors by the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06118v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huali Zhao, Tianying Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian field theory for the rate estimation</title>
      <link>https://arxiv.org/abs/2509.06129</link>
      <description>arXiv:2509.06129v1 Announce Type: new 
Abstract: We consider the statistical inference of a time-dependent rate of events in the framework of Bayesian field theory. By mapping the problem to a stochastic partial differential equation, as it is standard approach in field theory, we are able to numerically sample the distribution around the maximum likelihood path. We then analytically consider the perturbative expansion around the local and linear solution, and at the leading-order correction to the variance we find novel terms which depend on the local shape of the maximum likelihood path. We show that this shape correction is statistically most important in the variance expansion than the nonlinearity corrections. We then demonstrate the general applicability of the simulation method by extending it to the case of uncertain event times and by estimating the mortality rate in Venice during the 1348 Black Death epidemics from indirect evidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06129v1</guid>
      <category>stat.ME</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Auconi, Alessandro Codello, Raffaella Burioni, Guido Caldarelli</dc:creator>
    </item>
    <item>
      <title>Regression for spherical responses with linear and spherical covariates using a scaled link function</title>
      <link>https://arxiv.org/abs/2509.06204</link>
      <description>arXiv:2509.06204v1 Announce Type: new 
Abstract: We propose a regression model in which the responses are spherical variables and the covariates include linear and/or spherical variables. A novel link function is introduced by extending the M\"obius transformation on the sphere. This link function is an anisotropic mapping that enables scale control along each axis of the spherical covariates and for each linear covariate. It generalizes several well-known link functions for circular or linear covariates. Each parameter of the link function is clearly interpretable. For the error distribution, we consider a general class of elliptically symmetric distributions, which includes the Kent distribution, the elliptically symmetric angular Gaussian distribution, and the scaled von Mises-Fisher distribution. Axes of symmetry of the error distribution are determined using a method involving parallel transport. Maximum likelihood estimation is feasible via reparameterization of the proposed model. Moreover, the parameters of the link function and the shape/scale parameters of the error distribution are orthogonal in the sense of the Fisher information matrix. The proposed regression model is illustrated using two real datasets. An R software package accompanies this article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06204v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shogo Kato, Kassel L. Hingee, Janice L. Scealy, Andrew T. A. Wood</dc:creator>
    </item>
    <item>
      <title>Maximum-likelihood estimation of the Mat\'ern covariance structure of isotropic spatial random fields on finite, sampled grids</title>
      <link>https://arxiv.org/abs/2509.06223</link>
      <description>arXiv:2509.06223v1 Announce Type: new 
Abstract: We present a statistically and computationally efficient spectral-domain maximum-likelihood procedure to solve for the structure of Gaussian spatial random fields within the Matern covariance hyperclass. For univariate, stationary, and isotropic fields, the three controlling parameters are the process variance, smoothness, and range. The debiased Whittle likelihood maximization explicitly treats discretization and edge effects for finite sampled regions in parameter estimation and uncertainty quantification. As even the best parameter estimate may not be good enough, we provide a test for whether the model specification itself warrants rejection. Our results are practical and relevant for the study of a variety of geophysical fields, and for spatial interpolation, out-of-sample extension, kriging, machine learning, and feature detection of geological data. We present procedural details and high-level results on real-world examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06223v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederik J. Simons, Olivia L. Walbert, Arthur P. Guillaumin, Gabriel L. Eggers, Kevin W. Lewis, Sofia C. Olhede</dc:creator>
    </item>
    <item>
      <title>Generalized Tensor Completion with Non-Random Missingness</title>
      <link>https://arxiv.org/abs/2509.06225</link>
      <description>arXiv:2509.06225v2 Announce Type: new 
Abstract: Tensor completion plays a crucial role in applications such as recommender systems and medical imaging, where data are often highly incomplete. While extensive prior work has addressed tensor completion with data missingness, most assume that each entry of the tensor is available independently with probability $p$. However, real-world tensor data often exhibit missing-not-at-random (MNAR) patterns, where the probability of missingness depends on the underlying tensor values. This paper introduces a generalized tensor completion framework for noisy data with MNAR, where the observation probability is modeled as a function of underlying tensor values. Our flexible framework accommodates various tensor data types, such as continuous, binary and count data. For model estimation, we develop an alternating maximization algorithm and derive non-asymptotic error bounds for the estimator at each iteration, under considerably relaxed conditions on the observation probabilities. Additionally, we propose a statistical inference procedure to test whether observation probabilities depend on underlying tensor values, offering a formal assessment of the missingness assumption within our modeling framework. The utility and efficacy of our approach are demonstrated through comparative simulation studies and analyses of two real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06225v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maoyu Zhang, Biao Cai, Will Wei Sun, Jingfei Zhang</dc:creator>
    </item>
    <item>
      <title>Objective Bayesian inference for the Dhillon distribution</title>
      <link>https://arxiv.org/abs/2509.06344</link>
      <description>arXiv:2509.06344v1 Announce Type: new 
Abstract: In this work, we develop an objective Bayesian framework for the Dhillon probability distribution. We explicitly derive three objective priors: the Jeffreys prior, the overall reference prior, and the maximal data information prior. We show that both Jeffreys and reference priors yields a proper posterior distribution, whereas the maximal data information prior leads to an improper posterior. Moreover, we establish sufficient conditions for the existence of its respective posterior moments. Bayesian inference is carried out via Markov chain Monte Carlo, using the Metropolis-Hastings algorithm. A comprehensive simulation study compares the Bayesian estimators to maximum likelihood estimators in terms of bias, mean squared error, and coverage probability. Finally, a real-data application illustrates the practical utility of the proposed Bayesian approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06344v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Luiz Ramos, Enrique Achire Quispe, Ricardo Puziol de Oliveira, Jorge A. Achcar</dc:creator>
    </item>
    <item>
      <title>Noise-Robust Phase Connectivity Estimation via Bayesian Circular Functional Models</title>
      <link>https://arxiv.org/abs/2509.06418</link>
      <description>arXiv:2509.06418v1 Announce Type: new 
Abstract: The phase locking value (PLV) is a widely used measure to detect phase connectivity. Main drawbacks of the standard PLV are it can be sensitive to noisy observations and does not provide uncertainty measures under finite samples. To overcome the difficulty, we propose a model-based PLV through nonparametric statistical modeling. Specifically, since the discrete time series of phase can be regarded as a functional observation taking values on circle, we employ a Bayesian model for circular-variate functional data, which gives denoising and inference on the resulting PLV values. The proposed model is defined through "wrapping" functional Gaussian models on real line, for which we develop an efficient posterior computation algorithm using Gibbs sampler. The usefulness of the proposed method is demonstrated through simulation experiments based on real EEG data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06418v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shonosuke Sugasawa, Takeru Matsuda, Tomoyuki Nagakawa</dc:creator>
    </item>
    <item>
      <title>Specification Tests for the Error--Law in Vector Multiplicative Errors Models</title>
      <link>https://arxiv.org/abs/2509.06732</link>
      <description>arXiv:2509.06732v1 Announce Type: new 
Abstract: We suggest specification tests for the error distribution in vector multiplicative error models (vMEM). The test statistic is formulated as a weighted integrated distance between the parametric estimator of the Laplace transform of the null distribution and its empirical counterpart computed from the residuals. Asymptotic results are obtained under both the null and alternative hypotheses. If the Laplace transform of the null distribution is not available in closed form, we propose a test statistic that uses independent artificial samples generated from the distribution under test, possibly with estimated parameters. The test statistic compares the empirical Laplace transforms of the residuals and the artificial errors using a similar weighted integrated distance. Bootstrap resampling is used to approximate the critical values of the test. The finite-sample performance of the two testing procedures is compared in a Monte Carlo simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06732v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>\v{S}\'arka Hudecov\'a, Simos G. Meintanis</dc:creator>
    </item>
    <item>
      <title>A nutritionally informed model for Bayesian variable selection with metabolite response variables</title>
      <link>https://arxiv.org/abs/2509.06779</link>
      <description>arXiv:2509.06779v1 Announce Type: new 
Abstract: Understanding the pathways through which diet affects human metabolism is a central task in nutritional epidemiology. This article proposes novel methodology to identify food items associated with blood metabolites in two cohorts of healthcare professionals. We analyze 30 food intake variables that exhibit relationship structure through their correlations and nutritional attributes. The metabolic responses include 244 compounds measured by mass spectrometry, presenting substantial challenges that include missingness, left-censoring, and skewness. While existing methods can address such factors in low-dimensional settings, they are not designed for high-dimensional regression involving strongly correlated predictors and non-normal outcomes. To address these challenges, we propose a novel Bayesian variable selection framework for metabolite response variables based on a skew-normal censored mixture model. To exploit substantive information on the nutritional similarities among dietary factors, we employ a Markov random field prior that encourages joint selection of related predictors, while introducing a new, efficient strategy for its hyperparameter specification. Applying this methodology to the cohort data identifies multiple metabolite-diet associations that are consistent with previous research as well as several potentially novel associations that were not detected using standard methods. The proposed approach is implemented in the R package multimetab, facilitating its use in high-dimensional metabolomic analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06779v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dylan Clark-Boucher, Brent A Coull, Harrison T Reeder, Fenglei Wang, Qi Sun, Jacqueline R Starr, Kyu Ha Lee</dc:creator>
    </item>
    <item>
      <title>Parametric convergence rate of a non-parametric estimator in multivariate mixtures of power series distributions under conditional independence</title>
      <link>https://arxiv.org/abs/2509.05452</link>
      <description>arXiv:2509.05452v1 Announce Type: cross 
Abstract: The conditional independence assumption has recently appeared in a growing body of literature on the estimation of multivariate mixtures. We consider here conditionally independent multivariate mixtures of power series distributions with infinite support, to which belong Poisson, Geometric or Negative Binomial mixtures. We show that for all these mixtures, the non-parametric maximum likelihood estimator converges to the truth at the rate $(\log (nd))^{1+d/2} n^{-1/2}$ in the Hellinger distance, where $n$ denotes the size of the observed sample and $d$ represents the dimension of the mixture. Using this result, we then construct a new non-parametric estimator based on the maximum likelihood estimator that converges with the parametric rate $n^{-1/2}$ in all $\ell_p$-distances, for $p \ge 1$. These convergences rates are supported by simulations and the theory is illustrated using the famous V\'{e}lib dataset of the bike sharing system of Paris. We also introduce a testing procedure for whether the conditional independence assumption is satisfied for a given sample. This testing procedure is applied for several multivariate mixtures, with varying levels of dependence, and is thereby shown to distinguish well between conditionally independent and dependent mixtures. Finally, we use this testing procedure to investigate whether conditional independence holds for V\'{e}lib dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05452v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fadoua Balabdaoui, Harald Besdziek, Yong Wang</dc:creator>
    </item>
    <item>
      <title>Sparse Sensor Allocation for Inverse Problems of Detecting Sparse Leaking Emission Sources</title>
      <link>https://arxiv.org/abs/2509.05559</link>
      <description>arXiv:2509.05559v1 Announce Type: cross 
Abstract: This paper investigates the sparse optimal allocation of sensors for detecting sparse leaking emission sources. Because of the non-negativity of emission rates, uncertainty associated with parameters in the forward model, and sparsity of leaking emission sources, the classical linear Gaussian Bayesian inversion setup is limited and no closed-form solutions are available. By incorporating the non-negativity constraints on emission rates, relaxing the Gaussian distributional assumption, and considering the parameter uncertainties associated with the forward model, this paper provides comprehensive investigations, technical details, in-depth discussions and implementation of the optimal sensor allocation problem leveraging a bilevel optimization framework. The upper-level problem determines the optimal sensor locations by minimizing the Integrated Mean Squared Error (IMSE) of the estimated emission rates over uncertain wind conditions, while the lower-level problem solves an inverse problem that estimates the emission rates. Two algorithms, including the repeated Sample Average Approximation (rSAA) and the Stochastic Gradient Descent based bilevel approximation (SBA), are thoroughly investigated. It is shown that the proposed approach can further reduce the IMSE of the estimated emission rates starting from various initial sensor deployment generated by existing approaches. Convergence analysis is performed to obtain the performance guarantee, and numerical investigations show that the proposed approach can allocate sensors according to the parameters and output of the forward model. Computationally efficient code with GPU acceleration is available on GitHub so that the approach readily applicable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05559v1</guid>
      <category>stat.AP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinchao Liu, Youngdeok Hwang, Dzung Phan, Levente Klein, Xiao Liu, Kyongmin Yeo</dc:creator>
    </item>
    <item>
      <title>Robust Confidence Intervals for a Binomial Proportion: Local Optimality and Adaptivity</title>
      <link>https://arxiv.org/abs/2509.05568</link>
      <description>arXiv:2509.05568v1 Announce Type: cross 
Abstract: This paper revisits the classical problem of interval estimation of a binomial proportion under Huber contamination. Our main result derives the rate of optimal interval length when the contamination proportion is unknown under a local minimax framework, where the performance of an interval is evaluated at each point in the parameter space. By comparing the rate with the optimal length of a confidence interval that is allowed to use the knowledge of contamination proportion, we characterize the exact adaptation cost due to the ignorance of data quality. Our construction of the confidence interval to achieve local length optimality builds on robust hypothesis testing with a new monotonization step, which guarantees valid coverage, boundary-respecting intervals, and an efficient algorithm for computing the endpoints. The general strategy of interval construction can be applied beyond the binomial setting, and leads to optimal interval estimation for Poisson data with contamination as well. We also investigate a closely related Erd\H{o}s--R\'{e}nyi model with node contamination. Though its optimal rate of parameter estimation agrees with that of the binomial setting, we show that adaptation to unknown contamination proportion is provably impossible for interval estimation in that setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05568v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minjun Cho, Yuetian Luo, Chao Gao</dc:creator>
    </item>
    <item>
      <title>Network signflip parallel analysis for selecting the embedding dimension</title>
      <link>https://arxiv.org/abs/2509.05722</link>
      <description>arXiv:2509.05722v1 Announce Type: cross 
Abstract: This paper investigates the problem of selecting the embedding dimension for large heterogeneous networks that have weakly distinguishable community structure. For a broad family of embeddings based on normalized adjacency matrices, we introduce a novel spectral method that compares the eigenvalues of the normalized adjacency matrix to those obtained after randomly signflipping its entries. The proposed method, called network signflip parallel analysis (NetFlipPA), is interpretable, simple to implement, data driven, and does not require users to carefully tune parameters. For large random graphs arising from degree-corrected stochastic blockmodels with weakly distinguishable community structure (and consequently, non-diverging eigenvalues), NetFlipPA provably recovers the spectral noise floor (i.e., the upper-edge of the eigenvalues corresponding to the noise component of the normalized adjacency matrix). NetFlipPA thus provides a statistically rigorous randomization-based method for selecting the embedding dimension by keeping the eigenvalues that rise above the recovered spectral noise floor. Compared to traditional cutoff-based methods, the data-driven threshold used in NetFlipPA is provably effective under milder assumptions on the node degree heterogeneity and the number of node communities. Our main results rely on careful non-asymptotic perturbation analysis and leverage recent progress on local laws for nonhomogeneous Wigner-type random matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05722v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Hong, Joshua Cape</dc:creator>
    </item>
    <item>
      <title>Polynomial Log-Marginals and Tweedie's Formula : When Is Bayes Possible?</title>
      <link>https://arxiv.org/abs/2509.05823</link>
      <description>arXiv:2509.05823v1 Announce Type: cross 
Abstract: Motivated by Tweedie's formula for the Compound Decision problem, we examine the theoretical foundations of empirical Bayes estimators that directly model the marginal density $m(y)$. Our main result shows that polynomial log-marginals of degree $k \ge 3 $ cannot arise from any valid prior distribution in exponential family models, while quadratic forms correspond exactly to Gaussian priors. This provides theoretical justification for why certain empirical Bayes decision rules, while practically useful, do not correspond to any formal Bayes procedures. We also strengthen the diagnostic by showing that a marginal is a Gaussian convolution only if it extends to a bounded solution of the heat equation in a neighborhood of the smoothing parameter, beyond the convexity of $c(y)=\tfrac12 y^2+\log m(y)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05823v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jyotishka Datta, Nicholas G. Polson</dc:creator>
    </item>
    <item>
      <title>Additive Distributionally Robust Ranking and Selection</title>
      <link>https://arxiv.org/abs/2509.06147</link>
      <description>arXiv:2509.06147v1 Announce Type: cross 
Abstract: Ranking and selection (R&amp;S) aims to identify the alternative with the best mean performance among $k$ simulated alternatives. The practical value of R&amp;S depends on accurate simulation input modeling, which often suffers from the curse of input uncertainty due to limited data. Distributionally robust ranking and selection (DRR&amp;S) addresses this challenge by modeling input uncertainty via an ambiguity set of $m &gt; 1$ plausible input distributions, resulting in $km$ scenarios in total. Recent DRR&amp;S studies suggest a key structural insight: additivity in budget allocation is essential for efficiency. However, existing justifications are heuristic, and fundamental properties such as consistency and the precise allocation pattern induced by additivity remain poorly understood. In this paper, we propose a simple additive allocation (AA) procedure that aims to exclusively sample the $k + m - 1$ previously hypothesized critical scenarios. Leveraging boundary-crossing arguments, we establish a lower bound on the probability of correct selection and characterize the procedure's budget allocation behavior. We then prove that AA is consistent and, surprisingly, achieves additivity in the strongest sense: as the total budget increases, only $k + m - 1$ scenarios are sampled infinitely often. Notably, the worst-case scenarios of non-best alternatives may not be among them, challenging prior beliefs about their criticality. These results offer new and counterintuitive insights into the additive structure of DRR&amp;S. To improve practical performance while preserving this structure, we introduce a general additive allocation (GAA) framework that flexibly incorporates sampling rules from traditional R&amp;S procedures in a modular fashion. Numerical experiments support our theoretical findings and demonstrate the competitive performance of the proposed GAA procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06147v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zaile Li, Yuchen Wan, L. Jeff Hong</dc:creator>
    </item>
    <item>
      <title>Largevars: An R Package for Testing Large VARs for the Presence of Cointegration</title>
      <link>https://arxiv.org/abs/2509.06295</link>
      <description>arXiv:2509.06295v1 Announce Type: cross 
Abstract: Cointegration is a property of multivariate time series that determines whether its non-stationary, growing components have a stationary linear combination. Largevars R package conducts a cointegration test for high-dimensional vector autoregressions of order k based on the large N, T asymptotics of Bykhovskaya and Gorin (2022, 2025). The implemented test is a modification of the Johansen likelihood ratio test. In the absence of cointegration the test converges to the partial sum of the Airy_1 point process, an object arising in random matrix theory.
  The package and this article contain simulated quantiles of the first ten partial sums of the Airy_1 point process that are precise up to the first 3 digits. We also include two examples using Largevars: an empirical example on S&amp;P100 stocks and a simulated VAR(2) example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06295v1</guid>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Bykhovskaya, Vadim Gorin, Eszter Kiss</dc:creator>
    </item>
    <item>
      <title>MOSAIC: Minimax-Optimal Sparsity-Adaptive Inference for Change Points in Dynamic Networks</title>
      <link>https://arxiv.org/abs/2509.06303</link>
      <description>arXiv:2509.06303v1 Announce Type: cross 
Abstract: We propose a new inference framework, named MOSAIC, for change-point detection in dynamic networks with the simultaneous low-rank and sparse-change structure. We establish the minimax rate of detection boundary, which relies on the sparsity of changes. We then develop an eigen-decomposition-based test with screened signals that approaches the minimax rate in theory, with only a minor logarithmic loss. For practical implementation of MOSAIC, we adjust the theoretical test by a novel residual-based technique, resulting in a pivotal statistic that converges to a standard normal distribution via the martingale central limit theorem under the null hypothesis and achieves full power under the alternative hypothesis. We also analyze the minimax rate of testing boundary for dynamic networks without the low-rank structure, which almost aligns with the results in high-dimensional mean-vector change-point inference. We showcase the effectiveness of MOSAIC and verify our theoretical results with several simulation examples and a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06303v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingying Fan, Jingyuan Liu, Jinchi Lv, Ao Sun</dc:creator>
    </item>
    <item>
      <title>Robust and Adaptive Spectral Method for Representation Multi-Task Learning with Contamination</title>
      <link>https://arxiv.org/abs/2509.06575</link>
      <description>arXiv:2509.06575v1 Announce Type: cross 
Abstract: Representation-based multi-task learning (MTL) improves efficiency by learning a shared structure across tasks, but its practical application is often hindered by contamination, outliers, or adversarial tasks. Most existing methods and theories assume a clean or near-clean setting, failing when contamination is significant. This paper tackles representation MTL with an unknown and potentially large contamination proportion, while also allowing for heterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectral method (RAS) that can distill the shared inlier representation effectively and efficiently, while requiring no prior knowledge of the contamination level or the true representation dimension. Theoretically, we provide non-asymptotic error bounds for both the learned representation and the per-task parameters. These bounds adapt to inlier task similarity and outlier structure, and guarantee that RAS performs at least as well as single-task learning, thus preventing negative transfer. We also extend our framework to transfer learning with corresponding theoretical guarantees for the target task. Extensive experiments confirm our theory, showcasing the robustness and adaptivity of RAS, and its superior performance in regimes with up to 80\% task contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06575v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yian Huang, Yang Feng, Zhiliang Ying</dc:creator>
    </item>
    <item>
      <title>Sequential Gibbs Posteriors with Applications to Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2310.12882</link>
      <description>arXiv:2310.12882v2 Announce Type: replace 
Abstract: Gibbs posteriors are proportional to a prior distribution multiplied by an exponentiated loss function, with a key tuning parameter weighting information in the loss relative to the prior and providing a control of posterior uncertainty. Gibbs posteriors provide a principled framework for likelihood-free Bayesian inference, but in many situations, including a single tuning parameter inevitably leads to poor uncertainty quantification. In particular, regardless of the value of the parameter, credible regions have far from the nominal frequentist coverage even in large samples. We propose a sequential extension to Gibbs posteriors to address this problem. We prove the proposed sequential posterior exhibits concentration and a Bernstein-von Mises theorem, which holds under easy to verify conditions in Euclidean space and on manifolds. As a byproduct, we obtain the first Bernstein-von Mises theorem for traditional likelihood-based Bayesian posteriors on manifolds. All methods are illustrated with an application to principal component analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12882v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Winter, Omar Melikechi, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Robust Functional Data Analysis for Stochastic Evolution Equations in Infinite Dimensions</title>
      <link>https://arxiv.org/abs/2401.16286</link>
      <description>arXiv:2401.16286v3 Announce Type: replace 
Abstract: We develop an asymptotic theory for the jump robust measurement of covariations in the context of stochastic evolution equation in infinite dimensions. Namely, we identify scaling limits for realized covariations of solution processes with the quadratic covariation of the latent random process that drives the evolution equation which is assumed to be a Hilbert space-valued semimartingale. We discuss applications to dynamically consistent and outlier-robust dimension reduction in the spirit of functional principal components and the estimation of infinite-dimensional stochastic volatility models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16286v3</guid>
      <category>stat.ME</category>
      <category>q-fin.MF</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dennis Schroers</dc:creator>
    </item>
    <item>
      <title>Sequential Change-point Detection for Binomial Time Series</title>
      <link>https://arxiv.org/abs/2402.17274</link>
      <description>arXiv:2402.17274v2 Announce Type: replace 
Abstract: A binomial time series describes binary behaviors of individuals within a group, which depend on group behaviors in the past. Binomial time series data is widely applied in fields such as infection tracking and behavior analysis. In this paper, we introduce a generalized Binomial AR($p$) model with exogenous variables based on Generalized Linear Model (GLM), prove the statistical properties of the model when $p = 1$, and provide a parameter estimation method. Then, we propose a sequential change-point detection method for the generalized Binomial AR(1) model, facilitate real-time data monitoring and triggering alarms when a change point is detected. We apply the generalized Binomial AR(1) model to weekly pneumonia \&amp; influenza mortality data and successfully identify change points related to the COVID-19 outbreak using the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17274v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.11159/icsta25.181</arxiv:DOI>
      <dc:creator>Yajun Liu, Beth Andrews</dc:creator>
    </item>
    <item>
      <title>Conformalized Tensor Completion with Riemannian Optimization</title>
      <link>https://arxiv.org/abs/2405.00581</link>
      <description>arXiv:2405.00581v2 Announce Type: replace 
Abstract: Tensor data, or multi-dimensional arrays, is a data format popular in multiple fields such as social network analysis, recommender systems, and brain imaging. It is not uncommon to observe tensor data containing missing values, and tensor completion aims at estimating the missing values given the partially observed tensor. Sufficient efforts have been spared on devising scalable tensor completion algorithms, but few on quantifying the uncertainty of the estimator. In this paper, we nest the uncertainty quantification (UQ) of tensor completion under a split conformal prediction framework and establish the connection of the UQ problem to a problem of estimating the missing propensity of each tensor entry. We model the data missingness of the tensor with a tensor Ising model parameterized by a low-rank tensor parameter. We propose to estimate the tensor parameter by maximum pseudo-likelihood estimation (MPLE) with a Riemannian gradient descent algorithm. Extensive simulation studies have been conducted to justify the validity of the resulting conformal interval. We apply our method to the regional total electron content (TEC) reconstruction problem. Supplemental materials of the paper are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00581v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hu Sun, Yang Chen</dc:creator>
    </item>
    <item>
      <title>The ideal trial: defining causal estimands that balance relevance and feasibility in target trial emulations and actual randomized trials</title>
      <link>https://arxiv.org/abs/2405.10026</link>
      <description>arXiv:2405.10026v4 Announce Type: replace 
Abstract: Causal inference is the goal of randomized trials and many observational studies. The first step in a formal causal inference framework is to define the causal estimand, and in both types of study this can be intuitively defined as the effect in an ideal trial: a hypothetical perfect randomized experiment (with representative sample, perfect adherence, etc.). The target trial framework is increasingly used for causal inference in observational studies, but clarity is lacking in how a target trial should be specified and how it relates to an ideal trial. In this paper, we review the concept of the ideal trial and highlight the need to balance relevance for decision-making in the real world and feasibility of estimation when specifying it. We then consider the question of how a target trial should be specified, outlining the challenges of a recommended approach, commonly seen in applications, that puts the focus heavily on feasibility of estimation: to specify the target trial such that it is closely aligned with the observational data (e.g. uses the same eligibility criteria). We argue that with this "aligned" approach, biases may remain relative to the estimand of ultimate practical interest, defined by the ideal trial, which mirror the often-overlooked biases of actual trials. We conclude that consideration of the ideal trial and of how the target trial and its emulation or the actual trial differ from it is necessary to identify and manage all bias sources in both settings. An example from respiratory epidemiology is used for illustration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10026v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Margarita Moreno-Betancur, Rushani Wijesuriya, John B. Carlin</dc:creator>
    </item>
    <item>
      <title>Robust integration of external control data in randomized trials</title>
      <link>https://arxiv.org/abs/2406.17971</link>
      <description>arXiv:2406.17971v4 Announce Type: replace 
Abstract: One approach for increasing the efficiency of randomized trials is the use of "external controls" -- individuals who received the control treatment studied in the trial during routine practice or in prior experimental studies. Existing external control methods, however, can be biased if the populations underlying the trial and the external control data are not exchangeable. Here, we characterize a randomization-aware class of treatment effect estimators in the population underlying the trial that remain consistent and asymptotically normal when using external control data, even when exchangeability does not hold. We consider two members of this class of estimators: the well-known augmented inverse probability weighting trial-only estimator, which is the efficient estimator when only trial data are used; and a potentially more efficient member of the class when exchangeability holds and external control data are available, which we refer to as the optimized randomization-aware estimator. To achieve robust integration of external control data in trial analyses, we then propose a combined estimator based on the efficient trial-only estimator and the optimized randomization-aware estimator. We show that the combined estimator is consistent and no less efficient than the most efficient of the two component estimators, whether the exchangeability assumption holds or not. We examine the estimators' performance in simulations and we illustrate their use with data from two trials of paliperidone extended-release for schizophrenia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17971v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rickard Karlsson, Guanbo Wang, Piersilvio De Bartolomeis, Jesse H. Krijthe, Issa J. Dahabreh</dc:creator>
    </item>
    <item>
      <title>Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions</title>
      <link>https://arxiv.org/abs/2408.13414</link>
      <description>arXiv:2408.13414v4 Announce Type: replace 
Abstract: Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more -- and more complex -- models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is _replicability_: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is _reproducibly_ better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13414v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Ren\'e, Andr\'e Longtin</dc:creator>
    </item>
    <item>
      <title>POI-SIMEX for Conditionally Poisson Distributed Biomarkers from Tissue Histology</title>
      <link>https://arxiv.org/abs/2409.14256</link>
      <description>arXiv:2409.14256v3 Announce Type: replace 
Abstract: Covariate measurement error in regression analysis is an important issue that has been studied extensively under the classical additive and the Berkson error models. Here, we consider cases where covariates are derived from tumor tissue histology, and in particular tissue microarrays. In such settings, biomarkers are evaluated from tissue cores that are subsampled from a larger tissue section so that these biomarkers are only estimates of the true cell densities. The resulting measurement error is non-negligible but is seldom accounted for in the analysis of cancer studies involving tissue microarrays. To adjust for this type of measurement error, we assume that these discrete-valued biomarkers are conditionally Poisson distributed, based on a Poisson process model governing the spatial locations of marker-positive cells. Existing methods for addressing conditional Poisson surrogates, particularly in the absence of internal validation data, are limited. We extend the simulation extrapolation (SIMEX) algorithm to accommodate the conditional Poisson case (POI-SIMEX), where measurement errors are non-Gaussian with heteroscedastic variance. The proposed estimator is shown to be strongly consistent in a linear regression model under the assumption of a conditional Poisson distribution for the observed biomarker. Simulation studies evaluate the performance of POI-SIMEX, comparing it with the naive method and an alternative corrected likelihood approach in linear regression and survival analysis contexts. POI-SIMEX is then applied to a study of high-grade serous cancer, examining the association between survival and the presence of triple-positive biomarker (CD3+CD8+FOXP3+ cells)</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14256v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aijun Yang, Phineas T. Hamilton, Brad H. Nelson, Julian J. Lum, Mary Lesperance, Farouk S. Nathoo</dc:creator>
    </item>
    <item>
      <title>Modelling Extremal Dependence in high-dimensions: A Practical Perspective</title>
      <link>https://arxiv.org/abs/2412.13453</link>
      <description>arXiv:2412.13453v2 Announce Type: replace 
Abstract: From environmental sciences to finance, there are growing needs for assessing the risk of more extreme events than those observed. Extrapolating extreme events beyond the range of the data is not obvious and requires advanced tools based on extreme value theory. Furthermore, the complexity of risk assessments often requires the inclusion of multiple variables. Extreme value theory provides very important tools for the analysis of multivariate or spatial extreme events, but these are not easily accessible to professionals without appropriate expertise. This article provides a minimal background on multivariate and spatial extremes and gives simple yet thorough instructions to analyse high-dimensional extremes using the R package ExtremalDep. After briefly introducing the statistical methodologies, we focus on road testing the package's toolbox through several real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13453v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boris Beranger, Simone A. Padoan</dc:creator>
    </item>
    <item>
      <title>Statistical description and dimension reduction of categorical trajectories with multivariate functional principal components</title>
      <link>https://arxiv.org/abs/2502.09986</link>
      <description>arXiv:2502.09986v3 Announce Type: replace 
Abstract: There are many examples in which the statistical units of interest are samples of a continuous time categorical random process, that is to say a continuous time stochastic process taking values in a finite state space. Getting simple representations that allow comparisons of a set of trajectories is of major interest for statisticians. Without loosing any information, we associate to each state a binary random function, taking values in $\{0,1\}$, and turn the problem of statistical description of the categorical trajectories into a multivariate functional principal components analysis. The (multivariate) covariance operator has nice interpretations in terms of departure from independence of the joint probabilities and the multivariate functional principal components are simple to interpret. Under the weak hypothesis assuming only continuity in probability of the $0-1$ trajectories, it is simple to build consistent estimators of the covariance kernel and perform multivariate functional principal components analysis. The sample paths being piecewise constant, with a finite number of jumps, this a rare case in functional data analysis in which the trajectories are not supposed to be continuous and can be observed exhaustively. The approach is illustrated on a data set of sensory perceptions, considering different gustometer-controlled stimuli experiments. We also show how it can be easily extended to analyze experiments, such as temporal check-all-that-apply, in which two states or more can be observed at the same time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09986v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Herv\'e Cardot, Caroline Peltier</dc:creator>
    </item>
    <item>
      <title>Randomized Quasi-Monte Carlo Features for Kernel Approximation</title>
      <link>https://arxiv.org/abs/2503.06041</link>
      <description>arXiv:2503.06041v2 Announce Type: replace 
Abstract: We investigate the application of randomized quasi-Monte Carlo (RQMC) methods in random feature approximations for kernel-based learning. Compared to the classical Monte Carlo (MC) approach \citep{rahimi2007random}, RQMC improves the deterministic approximation error bound from $O_P(1/\sqrt{M})$ to $O(1/M)$ (up to logarithmic factors), matching the rate achieved by quasi-Monte Carlo (QMC) methods \citep{huangquasi}. Beyond the deterministic error bound guarantee, we further establish additional average error bounds for RQMC features: some requiring weaker assumptions and others significantly reducing the exponent of the logarithmic factor. In the context of kernel ridge regression, we show that RQMC features offer computational advantages over MC features while preserving the same statistical error rate. Empirical results further show that RQMC methods maintain stable performance in both low and moderately high-dimensional settings, unlike QMC methods, which suffer from significant performance degradation as dimension increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06041v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yian Huang, Zhen Huang</dc:creator>
    </item>
    <item>
      <title>Variational inference for hierarchical models with conditional scale and skewness corrections</title>
      <link>https://arxiv.org/abs/2503.18075</link>
      <description>arXiv:2503.18075v3 Announce Type: replace 
Abstract: Gaussian variational approximations are widely used for summarizing posterior distributions in Bayesian models, especially in high-dimensional settings. However, a drawback of such approximations is the inability to capture skewness or more complex features of the posterior. Recent work suggests applying skewness corrections to existing Gaussian or other symmetric approximations to address this limitation. We propose to incorporate the skewness correction into the definition of an approximating variational family. We consider approximating the posterior for hierarchical models, in which there are ``global'' and ``local'' parameters. A baseline variational approximation is defined as the product of a Gaussian marginal posterior for global parameters and a Gaussian conditional posterior for local parameters given the global ones. Skewness corrections are then considered. The adjustment of the conditional posterior term for local variables is adaptive to the global parameter value. Optimization of baseline variational parameters is performed jointly with the skewness correction. Our approach allows the location, scale and skewness to be captured separately, without using additional parameters for skewness adjustments. The proposed method substantially improves accuracy for only a modest increase in computational cost compared to state-of-the-art Gaussian approximations. Good performance is demonstrated in generalized linear mixed models and multinomial logit discrete choice models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18075v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Kock, Linda S. L. Tan, Prateek Bansal, David J. Nott</dc:creator>
    </item>
    <item>
      <title>Mitigating Eddington and Malmquist Biases in Latent-Inclination Inference of the Tully-Fisher Relation</title>
      <link>https://arxiv.org/abs/2504.10589</link>
      <description>arXiv:2504.10589v3 Announce Type: replace 
Abstract: The Tully-Fisher relation is a vital distance indicator, but its precise inference is challenged by selection bias, statistical bias, and uncertain inclination corrections. This study presents a Bayesian framework that simultaneously addresses these issues. To eliminate the need for individual inclination corrections, inclination is treated as a latent variable with a known probability distribution. To correct for the distance-dependent Malmquist bias arising from sample selection, the model incorporates Gaussian scatter in the dependent variable, the distribution of the independent variable, and the observational selection function into the data likelihood. To mitigate the statistical bias -- termed the ``general Eddington bias'' -- caused by Gaussian scatter and the non-uniform distribution of the independent variable, two methods are introduced: (1) analytical bias corrections applied to the dependent variable before likelihood computation, and (2) a dual-scatter model that accounts for Gaussian scatter in the independent variable within the likelihood function. The effectiveness of these methods is demonstrated using simulated datasets. By rigorously addressing selection and statistical biases in a latent-variable regression analysis, this work provides a robust approach for unbiased distance estimates from standardizable candles, which is critical for improving the accuracy of Hubble constant determinations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10589v3</guid>
      <category>stat.ME</category>
      <category>astro-ph.GA</category>
      <category>astro-ph.IM</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hai Fu</dc:creator>
    </item>
    <item>
      <title>Fair Conformal Prediction for Incomplete Covariate Data</title>
      <link>https://arxiv.org/abs/2504.12582</link>
      <description>arXiv:2504.12582v2 Announce Type: replace 
Abstract: Conformal prediction provides a distribution-free framework for uncertainty quantification. This study explores the application of conformal prediction in scenarios where covariates are missing, which introduces significant challenges for uncertainty quantification. We establish that marginal validity holds for imputed datasets across various mechanisms of missing data and most imputation methods. Building on the framework of nonexchangeable conformal prediction, we demonstrate that coverage guarantees depend on the mask. To address this, we propose a nonexchangeable conformal prediction method for missing covariates that satisfies both marginal and mask-conditional validity. However, as this method does not ensure asymptotic conditional validity, we further introduce a localized conformal prediction approach that employs a novel score function based on kernel smoothing. This method achieves marginal, mask-conditional, and asymptotic conditional validity under certain assumptions. Extensive simulation studies and real-data analysis demonstrate the advantages of these proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12582v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingsen Kong, YIming Liu, Guangren Yang</dc:creator>
    </item>
    <item>
      <title>Model-free High Dimensional Mediator Selection with False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2505.09105</link>
      <description>arXiv:2505.09105v2 Announce Type: replace 
Abstract: There is a challenge in selecting high-dimensional mediators when the mediators have complex correlation structures and interactions. In this work, we frame the high-dimensional mediator selection problem into a series of hypothesis tests with composite nulls, and develop a method to control the false discovery rate (FDR) which has mild assumptions on the mediation model. We show the theoretical guarantee that the proposed method and algorithm achieve FDR control. We present extensive simulation results to demonstrate the power and finite sample performance compared with existing methods. Lastly, we demonstrate the method for analyzing the Alzheimer's Disease Neuroimaging Initiative (ADNI) data, in which the proposed method selects the volume of the hippocampus and amygdala, as well as some other important MRI-derived measures as mediators for the relationship between gender and dementia progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09105v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runqiu Wang, Ran Dai, Jieqiong Wang, Kah Meng Soh, Ziyang Xu, Mohamed Azzam, Hongying Dai, Cheng Zheng</dc:creator>
    </item>
    <item>
      <title>One-dimensional quantile-stratified sampling and its application in statistical simulations</title>
      <link>https://arxiv.org/abs/2506.07437</link>
      <description>arXiv:2506.07437v2 Announce Type: replace 
Abstract: In this paper we examine quantile-stratified samples from a known univariate probability distribution, with stratification occurring over a partition of the quantile regions in the distribution. We examine some general properties of this sampling method and we contrast it with standard IID sampling to highlight its similarities and differences. We examine the applications of this sampling method to various statistical simulations including importance sampling. We conduct simulation analysis to compare the performance of standard importance sampling against the quantile-stratified importance sampling to see how they each perform on a range of functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07437v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ben O'Neill</dc:creator>
    </item>
    <item>
      <title>FLAT: Fused Lasso Regression with Adaptive Minimum Spanning Tree with Applications on Thermohaline Circulation</title>
      <link>https://arxiv.org/abs/2507.09800</link>
      <description>arXiv:2507.09800v2 Announce Type: replace 
Abstract: This article introduces a new methodology model both discrete and continuous spatial heterogeneity simultaneously with an application in detection of hyper-plain in thermohaline circulation. To enable the data-driven detection of spatial boundaries with heterogeneity, we constructs an adaptive minimum spanning tree guided by both spatial proximity and coefficient dissimilarity, and combines both a spatial fused regularization and LASSO-type regularization to estimate the spatial coefficients under the framework of spatial regression. Numerical simulations demonstrate the effectiveness of proposed method in both estimation and heterogeneity detection. The usefulness of the approach is further illustrated via an analysis of oceanic data that provides new empirical finds about Atlantic with detected surfaces in temperature-salinity relationship.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09800v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cuiwen Che, Yifan Chen, Zhaoyu Xing, Wei Zhong</dc:creator>
    </item>
    <item>
      <title>Simulation-based validation of Bayes factor computation</title>
      <link>https://arxiv.org/abs/2508.11814</link>
      <description>arXiv:2508.11814v2 Announce Type: replace 
Abstract: We propose and evaluate two methods that validate the computation of Bayes factors: one based on an improved variant of simulation-based calibration checking (SBC) and one based on calibration metrics for binary predictions. We show that in theory, binary prediction calibration is equivalent to a special case of SBC, but with finite resources, binary prediction calibration is typically more sensitive. With well-designed test quantities, SBC can however detect all possible problems in computation, including some that cannot be uncovered by binary prediction calibration.
  Previous work on Bayes factor validation includes checks based on the data-averaged posterior and the Good check method. We demonstrate that both checks miss many problems in Bayes factor computation detectable with SBC and binary prediction calibration. Moreover, we find that the Good check as originally described fails to control its error rates. Our proposed checks also typically use simulation results more efficiently than data-averaged posterior checks. Finally, we show that a special approach based on posterior SBC is necessary when checking Bayes factor computation under improper priors and we validate several models with such priors.
  We recommend that novel methods for Bayes factor computation be validated with SBC and binary prediction calibration with at least several hundred simulations. For all the models we tested, the bridgesampling and BayesFactor R packages satisfy all available checks and thus are likely safe to use in standard scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11814v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Martin Modr\'ak, Sebastian Stroppel, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>An information metric for comparing and assessing informative interim decisions in sequential clinical trials</title>
      <link>https://arxiv.org/abs/2509.04904</link>
      <description>arXiv:2509.04904v2 Announce Type: replace 
Abstract: Group sequential designs enable interim analyses and potential early stopping for efficacy or futility. While these adaptations improve trial efficiency and ethical considerations, they also introduce bias into the adapted analyses. We demonstrate how failing to account for informative interim decisions in the analysis can substantially affect posterior estimates of the treatment effect, often resulting in overly optimistic credible intervals aligned with the stopping decision. Drawing on information theory, we use the Kullback-Leibler divergence to quantify this distortion and highlight its use for post-hoc evaluation of informative interim decisions, with a focus on end-of-study inference. Unlike pointwise comparisons, this measure provides an integrated summary of this distortion on the whole parameter space. By comparing alternative decision boundaries and prior specifications, we illustrate how this measure can improve the understanding of trial results and inform the planning of future adaptive studies. We also introduce an expected version of this metric to support clinicians in choosing decision boundaries. This guidance complements traditional strategies based on type-I error rate control by offering insights into the distortion introduced to the treatment effect at each interim phase. The use of this pre-experimental measure is finally illustrated in a group sequential trial for evaluating a treatment for central nervous system disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04904v2</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G. Caruso, W. F. Rosenberger, P. Mozgunov, N. Flournoy</dc:creator>
    </item>
    <item>
      <title>Beyond Linearity and Time-homogeneity: Relational Hyper Event Models with Time-Varying Non-Linear Effects</title>
      <link>https://arxiv.org/abs/2509.05289</link>
      <description>arXiv:2509.05289v2 Announce Type: replace 
Abstract: Recent technological advances have made it easier to collect large and complex networks of time-stamped relational events connecting two or more entities. Relational hyper-event models (RHEMs) aim to explain the dynamics of these events by modeling the event rate as a function of statistics based on past history and external information.
  However, despite the complexity of the data, most current RHEM approaches still rely on a linearity assumption to model this relationship. In this work, we address this limitation by introducing a more flexible model that allows the effects of statistics to vary non-linearly and over time. While time-varying and non-linear effects have been used in relational event modeling, we take this further by modeling joint time-varying and non-linear effects using tensor product smooths.
  We validate our methodology on both synthetic and empirical data. In particular, we use RHEMs to study how patterns of scientific collaboration and impact evolve over time. Our approach provides deeper insights into the dynamic factors driving relational hyper-events, allowing us to evaluate potential non-monotonic patterns that cannot be identified using linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05289v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Boschi, J\"urgen Lerner, Ernst C. Wit</dc:creator>
    </item>
    <item>
      <title>Optimal Shrinkage Estimation of Fixed Effects in Linear Panel Data Models</title>
      <link>https://arxiv.org/abs/2308.12485</link>
      <description>arXiv:2308.12485v4 Announce Type: replace-cross 
Abstract: Shrinkage methods are frequently used to improve the precision of least squares estimators of fixed effects. However, widely used shrinkage estimators guarantee improved precision only under strong distributional assumptions. I develop an estimator for the fixed effects that obtains the best possible mean squared error within a class of shrinkage estimators. This class includes conventional shrinkage estimators and the optimality does not require distributional assumptions. The estimator has an intuitive form and is easy to implement. Moreover, the fixed effects are allowed to vary with time and to be serially correlated, in which case the shrinkage optimally incorporates the underlying correlation structure. I also provide a method to forecast fixed effects one period ahead in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12485v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soonwoo Kwon</dc:creator>
    </item>
    <item>
      <title>A nonparametric test of spherical symmetry applicable to high dimensional data</title>
      <link>https://arxiv.org/abs/2403.12491</link>
      <description>arXiv:2403.12491v2 Announce Type: replace-cross 
Abstract: We develop a test for spherical symmetry of a multivariate distribution $\Pr$ that works well even when the dimension of the data $d$ is larger than the sample size $n$. We propose a non-negative measure of spherical asymmetry $\zeta(\Pr)$ such that $\zeta(\Pr)=0$ if and only if $\Pr$ is spherically symmetric. We construct a consistent estimator of $\zeta(\Pr)$ using the data augmentation method and investigate its large sample properties. The proposed test based on this estimator is calibrated using a novel resampling algorithm. Our test controls the type I error, and it is consistent against general alternatives. We also study its behavior for a sequence of alternatives $(1-\delta_n) F+\delta_n G$, where $\zeta(G)=0$ but $\zeta(F)&gt;0$, and $\delta_n \in [0,1]$. When $\lim\sup\delta_n&lt;1$, for any $G$, the power of our test converges to unity as $n$ increases. However, if $\lim\sup\delta_n=1$, the asymptotic power of our test depends on $\lim n(1-\delta_n)^2$. We establish this by proving the minimax rate optimality of our test over a suitable class of alternatives and showing that it is Pitman efficient when $\lim n(1-\delta_n)^2&gt;0$. Moreover, our test is provably consistent for high-dimensional data even when $d$ grows with $n$. When the center of symmetry is not specified by the null hypothesis, most of the existing tests often fail to satisfy the level property. To take care of this problem, we propose a general recipe for constructing modified tests based on pairwise differences of the observations. Our numerical results amply demonstrate the superiority of the proposed test over some state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12491v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bilol Banerjee, Anil K. Ghosh</dc:creator>
    </item>
    <item>
      <title>FACEGroup: Feasible and Actionable Counterfactual Explanations for Group Fairness</title>
      <link>https://arxiv.org/abs/2410.22591</link>
      <description>arXiv:2410.22591v3 Announce Type: replace-cross 
Abstract: Counterfactual explanations assess unfairness by revealing how inputs must change to achieve a desired outcome. This paper introduces the first graph-based framework for generating group counterfactual explanations to audit group fairness, a key aspect of trustworthy machine learning. Our framework, FACEGroup (Feasible and Actionable Counterfactual Explanations for Group Fairness), models real-world feasibility constraints, identifies subgroups with similar counterfactuals, and captures key trade-offs in counterfactual generation, distinguishing it from existing methods. To evaluate fairness, we introduce novel metrics for both group and subgroup level analysis that explicitly account for these trade-offs. Experiments on benchmark datasets show that FACEGroup effectively generates feasible group counterfactuals while accounting for trade-offs, and that our metrics capture and quantify fairness disparities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22591v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christos Fragkathoulas, Vasiliki Papanikou, Evaggelia Pitoura, Evimaria Terzi</dc:creator>
    </item>
    <item>
      <title>Design-Based and Network Sampling-Based Uncertainties in Network Experiments</title>
      <link>https://arxiv.org/abs/2506.22989</link>
      <description>arXiv:2506.22989v3 Announce Type: replace-cross 
Abstract: Ordinary least squares (OLS) estimators are widely used in network experiments to estimate spillover effects. We study the causal interpretation of, and inference for the OLS estimator under both design-based uncertainty from random treatment assignment and sampling-based uncertainty in network links. We show that correlations among regressors that capture the exposure to neighbors' treatments can induce contamination bias, preventing OLS from aggregating heterogeneous spillover effects for a clear causal interpretation. We derive the OLS estimator's asymptotic distribution and propose a network-robust variance estimator. Simulations and an empirical application demonstrate that contamination bias can be substantial, leading to inflated spillover estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22989v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kensuke Sakamoto, Yuya Shimizu</dc:creator>
    </item>
    <item>
      <title>GenAI-Powered Inference</title>
      <link>https://arxiv.org/abs/2507.03897</link>
      <description>arXiv:2507.03897v2 Announce Type: replace-cross 
Abstract: We introduce GenAI-Powered Inference (GPI), a statistical framework for both causal and predictive inference using unstructured data, including text and images. GPI leverages open-source Generative Artificial Intelligence (GenAI) models -- such as large language models and diffusion models -- not only to generate unstructured data at scale but also to extract low-dimensional representations that are guaranteed to capture their underlying structure. Applying machine learning to these representations, GPI enables estimation of causal and predictive effects while quantifying associated estimation uncertainty. Unlike existing approaches to representation learning, GPI does not require fine-tuning of generative models, making it computationally efficient and broadly accessible. We illustrate the versatility of the GPI framework through three applications: (1) analyzing Chinese social media censorship, (2) estimating predictive effects of candidates' facial appearance on electoral outcomes, and (3) assessing the persuasiveness of political rhetoric. An open-source software package is available for implementing GPI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03897v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosuke Imai, Kentaro Nakamura</dc:creator>
    </item>
  </channel>
</rss>
