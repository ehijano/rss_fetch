<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Sep 2025 01:18:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On design, analysis, and hybrid manufacturing of microstructured blade-like geometries</title>
      <link>https://arxiv.org/abs/2509.07044</link>
      <description>arXiv:2509.07044v1 Announce Type: new 
Abstract: With the evolution of new manufacturing technologies such as multi-material 3D printing, one can think of new type of objects that consist of considerably less, yet heterogeneous, material, consequently being porous, lighter and cheaper, while having the very same functionality as the original object when manufactured from one single solid material. We aim at questioning five decades of traditional paradigms in geometric CAD and focus at new generation of CAD objects that are not solid, but contain heterogeneous free-form internal microstructures. We propose a unified manufacturing pipeline that involves all stages, namely design, optimization, manufacturing, and inspection of microstructured free-form geometries. We demonstrate our pipeline on an industrial test case of a blisk blade that sustains the desired pressure limits, yet requires significantly less material when compared to the solid counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07044v1</guid>
      <category>cs.GR</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pablo Antolin, Michael Barton, Georges-Pierre Bonneau, Annalisa Buffa, Amaia Calleja-Ochoa, Gershon Elber, Stefanie Elgeti, Gaizka G\'omez Escudero, Alicia Gonzalez, Haizea Gonz\'alez Barrio, Stefanie Hahmann, Thibaut Hirschler, Q Youn Honga, Konstantin Key, Myung-Soo Kim, Michael Kofler, Norberto Lopez de Lacalle, Silvia de la Maza, Kanika Rajain, Jacques Zwar</dc:creator>
    </item>
    <item>
      <title>SVGauge: Towards Human-Aligned Evaluation for SVG Generation</title>
      <link>https://arxiv.org/abs/2509.07127</link>
      <description>arXiv:2509.07127v1 Announce Type: new 
Abstract: Generated Scalable Vector Graphics (SVG) images demand evaluation criteria tuned to their symbolic and vectorial nature: criteria that existing metrics such as FID, LPIPS, or CLIPScore fail to satisfy. In this paper, we introduce SVGauge, the first human-aligned, reference based metric for text-to-SVG generation. SVGauge jointly measures (i) visual fidelity, obtained by extracting SigLIP image embeddings and refining them with PCA and whitening for domain alignment, and (ii) semantic consistency, captured by comparing BLIP-2-generated captions of the SVGs against the original prompts in the combined space of SBERT and TF-IDF. Evaluation on the proposed SHE benchmark shows that SVGauge attains the highest correlation with human judgments and reproduces system-level rankings of eight zero-shot LLM-based generators more faithfully than existing metrics. Our results highlight the necessity of vector-specific evaluation and provide a practical tool for benchmarking future text-to-SVG generation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07127v1</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonardo Zini, Elia Frigieri, Sebastiano Aloscari, Marcello Generali, Lorenzo Dodi, Robert Dosen, Lorenzo Baraldi</dc:creator>
    </item>
    <item>
      <title>Efficient Computation of Voronoi Diagrams Using Point-in-Cell Tests</title>
      <link>https://arxiv.org/abs/2509.07175</link>
      <description>arXiv:2509.07175v1 Announce Type: new 
Abstract: Since the Voronoi diagram appears in many applications, the topic of improving its computational efficiency remains attractive. We propose a novel yet efficient method to compute Voronoi diagrams bounded by a given domain, i.e., the clipped or restricted Voronoi diagrams. The intersection of the domain and a Voronoi cell (domain-cell intersection) is generated by removing the part outside the cell from the domain, which can be accomplished by several clippings. Different from the existing methods, we present an edge-based search scheme to find clipping planes (bisectors). A test called point-in-cell is first set up to tell whether a space point is in a target Voronoi cell or not. Then, for each edge of the intermediate domain-cell intersection, we will launch a clipping only if its two endpoints are respectively inside and outside the corresponding Voronoi cell, where the bisector for the clipping can be found by using a few times of point-in-cell tests. Therefore, our method only involves the clippings that contribute to the final results, which is a great advantage over the state-of-the-art methods. Additionally, because each domain-cell intersection can be generated independently, we extend the proposed method to the GPUs for computing Voronoi diagrams in parallel. The experimental results show the best performance of our method compared to state-of-the-art ones, regardless of site distribution. This paper was first submitted to SIGGRAPH Asia 2025.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07175v1</guid>
      <category>cs.GR</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanyang Xiao, Yao Li, Juan Cao, Zhonggui Chen</dc:creator>
    </item>
    <item>
      <title>Neural Cone Radiosity for Interactive Global Illumination with Glossy Materials</title>
      <link>https://arxiv.org/abs/2509.07522</link>
      <description>arXiv:2509.07522v1 Announce Type: new 
Abstract: Modeling of high-frequency outgoing radiance distributions has long been a key challenge in rendering, particularly for glossy material. Such distributions concentrate radiative energy within a narrow lobe and are highly sensitive to changes in view direction. However, existing neural radiosity methods, which primarily rely on positional feature encoding, exhibit notable limitations in capturing these high-frequency, strongly view-dependent radiance distributions. To address this, we propose a highly-efficient approach by reflectance-aware ray cone encoding based on the neural radiosity framework, named neural cone radiosity. The core idea is to employ a pre-filtered multi-resolution hash grid to accurately approximate the glossy BSDF lobe, embedding view-dependent reflectance characteristics directly into the encoding process through continuous spatial aggregation. Our design not only significantly improves the network's ability to model high-frequency reflection distributions but also effectively handles surfaces with a wide range of glossiness levels, from highly glossy to low-gloss finishes. Meanwhile, our method reduces the network's burden in fitting complex radiance distributions, allowing the overall architecture to remain compact and efficient. Comprehensive experimental results demonstrate that our method consistently produces high-quality, noise-free renderings in real time under various glossiness conditions, and delivers superior fidelity and realism compared to baseline approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07522v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jierui Ren, Haojie Jin, Bo Pang, Yisong Chen, Guoping Wang, Sheng Li</dc:creator>
    </item>
    <item>
      <title>Topology-Aware Optimization of Gaussian Primitives for Human-Centric Volumetric Videos</title>
      <link>https://arxiv.org/abs/2509.07653</link>
      <description>arXiv:2509.07653v1 Announce Type: new 
Abstract: Volumetric video is emerging as a key medium for digitizing the dynamic physical world, creating the virtual environments with six degrees of freedom to deliver immersive user experiences. However, robustly modeling general dynamic scenes, especially those involving topological changes while maintaining long-term tracking remains a fundamental challenge. In this paper, we present TaoGS, a novel topology-aware dynamic Gaussian representation that disentangles motion and appearance to support, both, long-range tracking and topological adaptation. We represent scene motion with a sparse set of motion Gaussians, which are continuously updated by a spatio-temporal tracker and photometric cues that detect structural variations across frames. To capture fine-grained texture, each motion Gaussian anchors and dynamically activates a set of local appearance Gaussians, which are non-rigidly warped to the current frame to provide strong initialization and significantly reduce training time. This activation mechanism enables efficient modeling of detailed textures and maintains temporal coherence, allowing high-fidelity rendering even under challenging scenarios such as changing clothes. To enable seamless integration into codec-based volumetric formats, we introduce a global Gaussian Lookup Table that records the lifespan of each Gaussian and organizes attributes into a lifespan-aware 2D layout. This structure aligns naturally with standard video codecs and supports up to 40 compression. TaoGS provides a unified, adaptive solution for scalable volumetric video under topological variation, capturing moments where "elegance in motion" and "Power in Stillness", delivering immersive experiences that harmonize with the physical world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07653v1</guid>
      <category>cs.GR</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuheng Jiang, Chengcheng Guo, Yize Wu, Yu Hong, Shengkun Zhu, Zhehao Shen, Yingliang Zhang, Shaohui Jiao, Zhuo Su, Lan Xu, Marc Habermann, Christian Theobalt</dc:creator>
    </item>
    <item>
      <title>ReShape: a Collaborative Art Experience</title>
      <link>https://arxiv.org/abs/2509.07643</link>
      <description>arXiv:2509.07643v1 Announce Type: cross 
Abstract: This article describes a project called ReShape in which we created and designed a crowdsourced art initiative, inspired and powered by mathematics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07643v1</guid>
      <category>math.HO</category>
      <category>cs.GR</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Parlier, Bruno Teheux</dc:creator>
    </item>
    <item>
      <title>dciWebMapper2: Enhancing the dciWebMapper framework toward integrated, interactive visualization of linked multi-type maps, charts, and spatial statistics and analysis</title>
      <link>https://arxiv.org/abs/2509.07897</link>
      <description>arXiv:2509.07897v1 Announce Type: cross 
Abstract: As interactive web-based geovisualization becomes increasingly vital across disciplines, there is a growing need for open-source frameworks that support dynamic, multi-attribute spatial analysis and accessible design. This paper introduces dciWebMapper2, a significant expansion of the original dciWebMapper framework, designed to enable exploratory analysis across domains such as climate justice, food access, and social vulnerability. The enhanced framework integrates multiple map types, including choropleth, proportional symbol, small multiples, and heatmaps, with linked statistical charts (e.g., scatter plots, boxplots) and time sliders, all within a coordinated-view environment. Dropdown-based controls allow flexible, high-dimensional comparisons while maintaining visual clarity. Grounded in cartographic and information visualization principles, dciWebMapper2 is fully open-source, self-contained, and server-free, supporting modularity, reproducibility, and long-term sustainability. Three applied use cases demonstrate its adaptability and potential to democratize interactive web cartography. This work offers a versatile foundation for inclusive spatial storytelling and transparent geospatial analysis in research, education, and civic engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07897v1</guid>
      <category>cs.HC</category>
      <category>cs.DB</category>
      <category>cs.GR</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarigai Sarigai, Liping Yang, Katie Slack, Carolyn Fish, Michaela Buenemann, Qiusheng Wu, Yan Lin, Joseph A. Cook, David Jacobs</dc:creator>
    </item>
    <item>
      <title>Don't Splat your Gaussians: Volumetric Ray-Traced Primitives for Modeling and Rendering Scattering and Emissive Media</title>
      <link>https://arxiv.org/abs/2405.15425</link>
      <description>arXiv:2405.15425v3 Announce Type: replace 
Abstract: Efficient scene representations are essential for many computer graphics applications. A general unified representation that can handle both surfaces and volumes simultaneously, remains a research challenge. Inspired by recent methods for scene reconstruction that leverage mixtures of 3D Gaussians to model radiance fields, we formalize and generalize the modeling of scattering and emissive media using mixtures of simple kernel-based volumetric primitives. We introduce closed-form solutions for transmittance and free-flight distance sampling for different kernels, and propose several optimizations to use our method efficiently within any off-the-shelf volumetric path tracer. We demonstrate our method as a compact and efficient alternative to other forms of volume modeling for forward and inverse rendering of scattering media. Furthermore, we adapt and showcase our method in radiance field optimization and rendering, providing additional flexibility compared to current state of the art given its ray-tracing formulation. We also introduce the Epanechnikov kernel and demonstrate its potential as an efficient alternative to the traditionally-used Gaussian kernel in scene reconstruction tasks. The versatility and physically-based nature of our approach allows us to go beyond radiance fields and bring to kernel-based modeling and rendering any path-tracing enabled functionality such as scattering, relighting and complex camera models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15425v3</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3711853</arxiv:DOI>
      <arxiv:journal_reference>ACM Trans. Graph. 44, 1, Article 10 (February 2025), 17 pages</arxiv:journal_reference>
      <dc:creator>Jorge Condor, Sebastien Speierer, Lukas Bode, Aljaz Bozic, Simon Green, Piotr Didyk, Adrian Jarabo</dc:creator>
    </item>
    <item>
      <title>BEAM: Bridging Physically-based Rendering and Gaussian Modeling for Relightable Volumetric Video</title>
      <link>https://arxiv.org/abs/2502.08297</link>
      <description>arXiv:2502.08297v2 Announce Type: replace 
Abstract: Volumetric video enables immersive experiences by capturing dynamic 3D scenes, enabling diverse applications for virtual reality, education, and telepresence. However, traditional methods struggle with fixed lighting conditions, while neural approaches face trade-offs in efficiency, quality, or adaptability for relightable scenarios. To address these limitations, we present BEAM, a novel pipeline that bridges 4D Gaussian representations with physically-based rendering (PBR) to produce high-quality, relightable volumetric videos from multi-view RGB footage. BEAM recovers detailed geometry and PBR properties via a series of available Gaussian-based techniques. It first combines Gaussian-based human performance tracking with geometry-aware rasterization in a coarse-to-fine optimization framework to recover spatially and temporally consistent geometries. We further enhance Gaussian attributes by incorporating PBR properties step by step. We generate roughness via a multi-view-conditioned diffusion model, and then derive AO and base color using a 2D-to-3D strategy, incorporating a tailored Gaussian-based ray tracer for efficient visibility computation. Once recovered, these dynamic, relightable assets integrate seamlessly into traditional CG pipelines, supporting real-time rendering with deferred shading and offline rendering with ray tracing. By offering realistic, lifelike visualizations under diverse lighting conditions, BEAM opens new possibilities for interactive entertainment, storytelling, and creative visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08297v2</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Hong, Yize Wu, Zhehao Shen, Chengcheng Guo, Yuheng Jiang, Yingliang Zhang, Jingyi Yu, Lan Xu</dc:creator>
    </item>
    <item>
      <title>IntuiTF: MLLM-Guided Transfer Function Optimization for Direct Volume Rendering</title>
      <link>https://arxiv.org/abs/2506.18407</link>
      <description>arXiv:2506.18407v2 Announce Type: replace 
Abstract: Direct volume rendering (DVR) is a fundamental technique for visualizing volumetric data, where transfer functions (TFs) play a crucial role in extracting meaningful structures. However, designing effective TFs remains unintuitive due to the semantic gap between user intent and TF parameter space. Although numerous TF optimization methods have been proposed to mitigate this issue, existing approaches still face two major challenges: the vast exploration space and limited generalizability. To address these issues, we propose IntuiTF, a novel framework that leverages Multimodal Large Language Models (MLLMs) to guide TF optimization in alignment with user intent. Specifically, our method consists of two key components: (1) an evolution-driven explorer for effective exploration of the TF space, and (2) an MLLM-guided human-aligned evaluator that provides generalizable visual feedback on rendering quality. The explorer and the evaluator together establish an efficient Trial-Insight-Replanning paradigm for TF space exploration. We further extend our framework with an interactive TF design system. We demonstrate the broad applicability of our framework through three case studies and validate the effectiveness of each component through extensive experiments. We strongly recommend readers check our cases, demo video, and source code at: https://github.com/wyysteelhead/IntuiTF</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18407v2</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiyao Wang, Bo Pan, Ke Wang, Han Liu, Jinyuan Mao, Yuxin Liu, Minfeng Zhu, Xiuqi Huang, Weifeng Chen, Bo Zhang, Wei Chen</dc:creator>
    </item>
    <item>
      <title>HodgeFormer: Transformers for Learnable Operators on Triangular Meshes through Data-Driven Hodge Matrices</title>
      <link>https://arxiv.org/abs/2509.01839</link>
      <description>arXiv:2509.01839v3 Announce Type: replace 
Abstract: Currently, prominent Transformer architectures applied on graphs and meshes for shape analysis tasks employ traditional attention layers that heavily utilize spectral features requiring costly eigenvalue decomposition-based methods. To encode the mesh structure, these methods derive positional embeddings, that heavily rely on eigenvalue decomposition based operations, e.g. on the Laplacian matrix, or on heat-kernel signatures, which are then concatenated to the input features. This paper proposes a novel approach inspired by the explicit construction of the Hodge Laplacian operator in Discrete Exterior Calculus as a product of discrete Hodge operators and exterior derivatives, i.e. $(L := \star_0^{-1} d_0^T \star_1 d_0)$. We adjust the Transformer architecture in a novel deep learning layer that utilizes the multi-head attention mechanism to approximate Hodge matrices $\star_0$, $\star_1$ and $\star_2$ and learn families of discrete operators $L$ that act on mesh vertices, edges and faces. Our approach results in a computationally-efficient architecture that achieves comparable performance in mesh segmentation and classification tasks, through a direct learning framework, while eliminating the need for costly eigenvalue decomposition operations or complex preprocessing operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01839v3</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Akis Nousias, Stavros Nousias</dc:creator>
    </item>
    <item>
      <title>PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based Implicit Neural Map</title>
      <link>https://arxiv.org/abs/2502.05752</link>
      <description>arXiv:2502.05752v2 Announce Type: replace-cross 
Abstract: Robots benefit from high-fidelity reconstructions of their environment, which should be geometrically accurate and photorealistic to support downstream tasks. While this can be achieved by building distance fields from range sensors and radiance fields from cameras, realising scalable incremental mapping of both fields consistently and at the same time with high quality is challenging. In this paper, we propose a novel map representation that unifies a continuous signed distance field and a Gaussian splatting radiance field within an elastic and compact point-based implicit neural map. By enforcing geometric consistency between these fields, we achieve mutual improvements by exploiting both modalities. We present a novel LiDAR-visual SLAM system called PINGS using the proposed map representation and evaluate it on several challenging large-scale datasets. Experimental results demonstrate that PINGS can incrementally build globally consistent distance and radiance fields encoded with a compact set of neural points. Compared to state-of-the-art methods, PINGS achieves superior photometric and geometric rendering at novel views by constraining the radiance field with the distance field. Furthermore, by utilizing dense photometric cues and multi-view consistency from the radiance field, PINGS produces more accurate distance fields, leading to improved odometry estimation and mesh reconstruction. We also provide an open-source implementation of PING at: https://github.com/PRBonn/PINGS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05752v2</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Pan, Xingguang Zhong, Liren Jin, Louis Wiesmann, Marija Popovi\'c, Jens Behley, Cyrill Stachniss</dc:creator>
    </item>
  </channel>
</rss>
