<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Dec 2024 05:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Asymptotic Extinction in Large Coordination Games</title>
      <link>https://arxiv.org/abs/2412.15461</link>
      <description>arXiv:2412.15461v1 Announce Type: new 
Abstract: We study the exploration-exploitation trade-off for large multiplayer coordination games where players strategise via Q-Learning, a common learning framework in multi-agent reinforcement learning. Q-Learning is known to have two shortcomings, namely non-convergence and potential equilibrium selection problems, when there are multiple fixed points, called Quantal Response Equilibria (QRE). Furthermore, whilst QRE have full support for finite games, it is not clear how Q-Learning behaves as the game becomes large. In this paper, we characterise the critical exploration rate that guarantees convergence to a unique fixed point, addressing the two shortcomings above. Using a generating-functional method, we show that this rate increases with the number of players and the alignment of their payoffs. For many-player coordination games with perfectly aligned payoffs, this exploration rate is roughly twice that of $p$-player zero-sum games. As for large games, we provide a structural result for QRE, which suggests that as the game size increases, Q-Learning converges to a QRE near the boundary of the simplex of the action space, a phenomenon we term asymptotic extinction, where a constant fraction of the actions are played with zero probability at a rate $o(1/N)$ for an $N$-action game.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15461v1</guid>
      <category>cs.GT</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Desmond Chan, Bart De Keijzer, Tobias Galla, Stefanos Leonardos, Carmine Ventre</dc:creator>
    </item>
    <item>
      <title>On the Fairness of Additive Welfarist Rules</title>
      <link>https://arxiv.org/abs/2412.15472</link>
      <description>arXiv:2412.15472v1 Announce Type: new 
Abstract: Allocating indivisible goods is a ubiquitous task in fair division. We study additive welfarist rules, an important class of rules which choose an allocation that maximizes the sum of some function of the agents' utilities. Prior work has shown that the maximum Nash welfare (MNW) rule is the unique additive welfarist rule that guarantees envy-freeness up to one good (EF1). We strengthen this result by showing that MNW remains the only additive welfarist rule that ensures EF1 for identical-good instances, two-value instances, as well as normalized instances with three or more agents. On the other hand, if the agents' utilities are integers, we demonstrate that several other rules offer the EF1 guarantee, and provide characterizations of these rules for various classes of instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15472v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karen Frilya Celine, Warut Suksompong, Sheung Man Yuen</dc:creator>
    </item>
    <item>
      <title>DualGFL: Federated Learning with a Dual-Level Coalition-Auction Game</title>
      <link>https://arxiv.org/abs/2412.15492</link>
      <description>arXiv:2412.15492v1 Announce Type: new 
Abstract: Despite some promising results in federated learning using game-theoretical methods, most existing studies mainly employ a one-level game in either a cooperative or competitive environment, failing to capture the complex dynamics among participants in practice. To address this issue, we propose DualGFL, a novel Federated Learning framework with a Dual-level Game in cooperative-competitive environments. DualGFL includes a lower-level hedonic game where clients form coalitions and an upper-level multi-attribute auction game where coalitions bid for training participation. At the lower-level DualGFL, we introduce a new auction-aware utility function and propose a Pareto-optimal partitioning algorithm to find a Pareto-optimal partition based on clients' preference profiles. At the upper-level DualGFL, we formulate a multi-attribute auction game with resource constraints and derive equilibrium bids to maximize coalitions' winning probabilities and profits. A greedy algorithm is proposed to maximize the utility of the central server. Extensive experiments on real-world datasets demonstrate DualGFL's effectiveness in improving both server utility and client utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15492v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xiaobing Chen, Xiangwei Zhou, Songyang Zhang, Mingxuan Sun</dc:creator>
    </item>
    <item>
      <title>Online Optimization Algorithms in Repeated Price Competition: Equilibrium Learning and Algorithmic Collusion</title>
      <link>https://arxiv.org/abs/2412.15707</link>
      <description>arXiv:2412.15707v1 Announce Type: new 
Abstract: This paper addresses the question of whether or not uncoupled online learning algorithms converge to the Nash equilibrium in pricing competition or whether they can learn to collude. Algorithmic collusion has been debated among competition regulators, and it is a highly relevant phenomenon for buyers and sellers on online retail platforms. We analyze formally if mean-based algorithms, a class of bandit algorithms relevant to algorithmic pricing, converge to the Nash equilibrium in repeated Bertrand oligopolies. Bandit algorithms only learn the profit of the agent for the price set in each step. In addition, we provide results of extensive experiments with different types of multi-armed bandit algorithms used for algorithmic pricing. In a mathematical proof, we show that mean-based algorithms converge to correlated rational strategy profiles, which coincide with the Nash equilibrium in versions of the Bertrand competition. Learning algorithms do not converge to a Nash equilibrium in general, and the fact that Bertrand pricing games are learnable with bandit algorithms is remarkable. Our numerical results suggest that wide-spread bandit algorithms that are not mean-based also converge to equilibrium and that algorithmic collusion only arises with symmetric implementations of UCB or Q-learning, but not if different algorithms are used by sellers. In addition, the level of supra-competitive prices decreases with increasing numbers of sellers. Supra-competitive prices decrease consumer welfare. If algorithms lead to algorithmic collusion, this is important for consumers, sellers, and regulators to understand. We show that for the important class of multi-armed bandit algorithms such fears are overrated unless all sellers agree on a symmetric implementation of certain collusive algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15707v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Bichler, Julius Durmann, Matthias Oberlechner</dc:creator>
    </item>
    <item>
      <title>Approximate State Abstraction for Markov Games</title>
      <link>https://arxiv.org/abs/2412.15877</link>
      <description>arXiv:2412.15877v1 Announce Type: new 
Abstract: This paper introduces state abstraction for two-player zero-sum Markov games (TZMGs), where the payoffs for the two players are determined by the state representing the environment and their respective actions, with state transitions following Markov decision processes. For example, in games like soccer, the value of actions changes according to the state of play, and thus such games should be described as Markov games. In TZMGs, as the number of states increases, computing equilibria becomes more difficult. Therefore, we consider state abstraction, which reduces the number of states by treating multiple different states as a single state. There is a substantial body of research on finding optimal policies for Markov decision processes using state abstraction. However, in the multi-player setting, the game with state abstraction may yield different equilibrium solutions from those of the ground game. To evaluate the equilibrium solutions of the game with state abstraction, we derived bounds on the duality gap, which represents the distance from the equilibrium solutions of the ground game. Finally, we demonstrate our state abstraction with Markov Soccer, compute equilibrium policies, and examine the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15877v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>AAAI 2025</arxiv:journal_reference>
      <dc:creator>Hiroki Ishibashi, Kenshi Abe, Atsushi Iwasaki</dc:creator>
    </item>
    <item>
      <title>On the Power of Strategic Corpus Enrichment in Content Creation Games</title>
      <link>https://arxiv.org/abs/2412.15878</link>
      <description>arXiv:2412.15878v1 Announce Type: new 
Abstract: Search and recommendation ecosystems exhibit competition among content creators. This competition has been tackled in a variety of game-theoretic frameworks. Content creators generate documents with the aim of being recommended by a content ranker for various information needs. In order for the ecosystem, modeled as a content ranking game, to be effective and maximize user welfare, it should guarantee stability, where stability is associated with the existence of pure Nash equilibrium in the corresponding game. Moreover, if the contents' ranking algorithm possesses a game in which any best-response learning dynamics of the content creators converge to equilibrium of high welfare, the system is considered highly attractive. However, as classical content ranking algorithms, employed by search and recommendation systems, rank documents by their distance to information needs, it has been shown that they fail to provide such stability properties. As a result, novel content ranking algorithms have been devised. In this work, we offer an alternative approach: corpus enrichment with a small set of fixed dummy documents. It turns out that, with the right design, such enrichment can lead to pure Nash equilibrium and even to the convergence of any best-response dynamics to a high welfare result, where we still employ the classical/current content ranking approach. We show two such corpus enrichment techniques with tight bounds on the number of documents needed to obtain the desired results. Interestingly, our study is a novel extension of Borel's Colonel Blotto game.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15878v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haya Nachimovsky, Moshe Tennenholtz</dc:creator>
    </item>
    <item>
      <title>Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game</title>
      <link>https://arxiv.org/abs/2412.16079</link>
      <description>arXiv:2412.16079v1 Announce Type: cross 
Abstract: Decentralised learning enables the training of deep learning algorithms without centralising data sets, resulting in benefits such as improved data privacy, operational efficiency and the fostering of data ownership policies. However, significant data imbalances pose a challenge in this framework. Participants with smaller datasets in distributed learning environments often achieve poorer results than participants with larger datasets. Data imbalances are particularly pronounced in medical fields and are caused by different patient populations, technological inequalities and divergent data collection practices.
  In this paper, we consider distributed learning as an Stackelberg evolutionary game. We present two algorithms for setting the weights of each node's contribution to the global model in each training round: the Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg Weighting Model (ASWM). We use three medical datasets to highlight the impact of dynamic weighting on underrepresented nodes in distributed learning. Our results show that the ASWM significantly favours underrepresented nodes by improving their performance by 2.713% in AUC. Meanwhile, nodes with larger datasets experience only a modest average performance decrease of 0.441%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16079v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.GT</category>
      <category>cs.NE</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Niehaus, Ingo Roeder, Nico Scherf</dc:creator>
    </item>
    <item>
      <title>Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information</title>
      <link>https://arxiv.org/abs/2412.16132</link>
      <description>arXiv:2412.16132v1 Announce Type: cross 
Abstract: We study mechanism design when agents hold private information about both their preferences and a common payoff-relevant state. We show that standard message-driven mechanisms cannot implement socially efficient allocations when agents have multidimensional types, even under favorable conditions. To overcome this limitation, we propose data-driven mechanisms that leverage additional post-allocation information, modeled as an estimator of the payoff-relevant state. Our data-driven mechanisms extend the classic Vickrey-Clarke-Groves class. We show that they achieve exact implementation in posterior equilibrium when the state is either fully revealed or the utility is linear in an unbiased estimator. We also show that they achieve approximate implementation with a consistent estimator, converging to exact implementation as the estimator converges, and present bounds on the convergence rate. We demonstrate applications to digital advertising auctions and large language model (LLM)-based mechanisms, where user engagement naturally reveals relevant information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16132v1</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dirk Bergemann, Marek Bojko, Paul D\"utting, Renato Paes Leme, Haifeng Xu, Song Zuo</dc:creator>
    </item>
    <item>
      <title>Sample Complexity of Linear Regression Models for Opinion Formation in Networks</title>
      <link>https://arxiv.org/abs/2311.02349</link>
      <description>arXiv:2311.02349v3 Announce Type: replace 
Abstract: Consider public health officials aiming to spread awareness about a new vaccine in a community interconnected by a social network. How can they distribute information with minimal resources, so as to avoid polarization and ensure community-wide convergence of opinion? To tackle such challenges, we initiate the study of sample complexity of opinion convergence in networks. Our framework is built on the recognized opinion formation game, where we regard the opinion of each agent as a data-derived model, unlike previous works that treat opinions as data-independent scalars. The opinion model for every agent is initially learned from its local samples and evolves game-theoretically as all agents communicate with neighbors and revise their models towards an equilibrium. Our focus is on the sample complexity needed to ensure that the opinions converge to an equilibrium such that the final model of every agent has low generalization error.
  Our paper has two main technical results. First, we present a novel polynomial time optimization framework to quantify the total sample complexity for arbitrary networks, when the underlying learning problem is (generalized) linear regression. Second, we leverage this optimization to study the network gain which measures the improvement of sample complexity when learning over a network compared to that in isolation. Towards this end, we derive network gain bounds for various network classes including cliques, star graphs, and random regular graphs. Additionally, our framework provides a method to study sample distribution within the network, suggesting that it is sufficient to allocate samples inversely to the degree. Empirical results on both synthetic and real-world networks strongly support our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02349v3</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haolin Liu, Rajmohan Rajaraman, Ravi Sundaram, Anil Vullikanti, Omer Wasim, Haifeng Xu</dc:creator>
    </item>
    <item>
      <title>Temporal Elections: Welfare, Strategyproofness, and Proportionality</title>
      <link>https://arxiv.org/abs/2408.13637</link>
      <description>arXiv:2408.13637v2 Announce Type: replace 
Abstract: We investigate a model of sequential decision-making where a single alternative is chosen at each round. We focus on two objectives -- utilitarian welfare (Util) and egalitarian welfare (Egal) -- and consider the computational complexity of maximizing these objectives, as well as their compatibility with strategyproofness and proportionality. We observe that maximizing Util is easy, but the corresponding decision problem for Egal is NP-complete even in restricted cases. We complement this hardness result for Egal with parameterized complexity analysis and an approximation algorithm. Additionally, we show that, while a mechanism that outputs an outcome that maximizes Util is strategyproof, all deterministic mechanisms for computing outcomes that maximize Egal fail a very weak variant of strategyproofness, called non-obvious manipulability (NOM). However, we show that when agents have non-empty approval sets at each timestep, choosing an Egal-maximizing outcome while breaking ties lexicographically satisfies NOM. Regarding proportionality, we prove that a proportional (PROP) outcome can be computed efficiently, but finding an outcome that maximizes Util while guaranteeing PROP is NP-hard. We also derive upper and lower bounds on the (strong) price of proportionality with respect to Util and Egal. Some of our results extend to $p$-mean welfare measures other than Egal and Util.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13637v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3233/FAIA240877</arxiv:DOI>
      <dc:creator>Edith Elkind, Tzeh Yuan Neoh, Nicholas Teh</dc:creator>
    </item>
    <item>
      <title>Agent-based Modelling of Quantum Prisoner's Dilemma</title>
      <link>https://arxiv.org/abs/2404.02216</link>
      <description>arXiv:2404.02216v2 Announce Type: replace-cross 
Abstract: What happens when an infinite number of players play a quantum game? In this tutorial, we will answer this question by looking at the emergence of cooperation, in the presence of noise, in a one-shot quantum Prisoner's dilemma (QuPD). We will use the numerical Agent-based model (ABM), and compare it with the analytical Nash equilibrium mapping (NEM) technique. To measure cooperation, we consider five indicators, i.e., game magnetization, entanglement susceptibility, correlation, player's payoff average and payoff capacity, respectively. In quantum social dilemmas, entanglement plays a non-trivial role in determining the behaviour of the quantum players (or, \textit{qubits}) in the thermodynamic limit, and for QuPD, we consider the existence of bipartite entanglement between neighbouring quantum players. For the five indicators in question, we observe \textit{first}-order phase transitions at two entanglement values, and these phase transition points depend on the payoffs associated with the QuPD game. We numerically analyze and study the properties of both the \textit{Quantum} and the \textit{Defect} phases of the QuPD via the five indicators. The results of this tutorial demonstrate that both ABM and NEM, in conjunction with the chosen five indicators, provide insightful information on cooperative behaviour in an infinite-player one-shot quantum Prisoner's dilemma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02216v2</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.GT</category>
      <category>quant-ph</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>APL Quantum (2024)</arxiv:journal_reference>
      <dc:creator>Colin Benjamin, Rajdeep Tah</dc:creator>
    </item>
    <item>
      <title>AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making</title>
      <link>https://arxiv.org/abs/2411.03865</link>
      <description>arXiv:2411.03865v3 Announce Type: replace-cross 
Abstract: Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03865v3</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yizhe Huang, Xingbo Wang, Hao Liu, Fanqi Kong, Aoyang Qin, Min Tang, Xiaoxi Wang, Song-Chun Zhu, Mingjie Bi, Siyuan Qi, Xue Feng</dc:creator>
    </item>
  </channel>
</rss>
