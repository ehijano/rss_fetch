<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Jul 2024 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 12 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Source Code Summarization in the Era of Large Language Models</title>
      <link>https://arxiv.org/abs/2407.07959</link>
      <description>arXiv:2407.07959v1 Announce Type: new 
Abstract: To support software developers in understanding and maintaining programs, various automatic (source) code summarization techniques have been proposed to generate a concise natural language summary (i.e., comment) for a given code snippet. Recently, the emergence of large language models (LLMs) has led to a great boost in the performance of code-related tasks. In this paper, we undertake a systematic and comprehensive study on code summarization in the era of LLMs, which covers multiple aspects involved in the workflow of LLM-based code summarization. Specifically, we begin by examining prevalent automated evaluation methods for assessing the quality of summaries generated by LLMs and find that the results of the GPT-4 evaluation method are most closely aligned with human evaluation. Then, we explore the effectiveness of five prompting techniques (zero-shot, few-shot, chain-of-thought, critique, and expert) in adapting LLMs to code summarization tasks. Contrary to expectations, advanced prompting techniques may not outperform simple zero-shot prompting. Next, we investigate the impact of LLMs' model settings (including top\_p and temperature parameters) on the quality of generated summaries. We find the impact of the two parameters on summary quality varies by the base LLM and programming language, but their impacts are similar. Moreover, we canvass LLMs' abilities to summarize code snippets in distinct types of programming languages. The results reveal that LLMs perform suboptimally when summarizing code written in logic programming languages compared to other language types. Finally, we unexpectedly find that CodeLlama-Instruct with 7B parameters can outperform advanced GPT-4 in generating summaries describing code implementation details and asserting code properties. We hope that our findings can provide a comprehensive understanding of code summarization in the era of LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07959v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weisong Sun, Yun Miao, Yuekang Li, Hongyu Zhang, Chunrong Fang, Yi Liu, Gelei Deng, Yang Liu, Zhenyu Chen</dc:creator>
    </item>
    <item>
      <title>How Do Developers Structure Unit Test Cases? An Empirical Study from the "AAA" Perspective</title>
      <link>https://arxiv.org/abs/2407.08138</link>
      <description>arXiv:2407.08138v1 Announce Type: new 
Abstract: The AAA pattern, i.e. arrange, act, and assert, provides a unified structure for unit test cases, which benefits comprehension and maintenance. However, there is little understanding regarding whether and how common real-life developers structure unit test cases following AAA in practice. In particular, are there recurring anti-patterns that deviate from the AAA structure and merit refactoring? And, if test cases follow the AAA structure, could they contain design flaws in the A blocks? If we propose refactoring to fix the design of test cases following the AAA, how do developers receive the proposals? Do they favor refactoring? If not, what are their considerations? This study presents an empirical study on 435 real-life unit test cases randomly selected from four open-source projects. Overall, the majority (71.5%) of test cases follow the AAA structure. And, we observed three recurring anti-patterns that deviate from the AAA structure, as well as four design flaws that may reside inside of the A blocks. Each issue type has its drawbacks and merits corresponding refactoring resolutions. We sent a total of 18 refactoring proposals as issue tickets for fixing these problems. We received 78% positive feedback favoring the refactoring. From the rejections, we learned that return-on-investment is a key consideration for developers. The findings provide insights for practitioners to structure unit test cases with AAA in mind, and for researchers to develop related techniques for enforcing AAA in test cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08138v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chenhao Wei, Lu Xiao, Tingting Yu, Sunny Wong, Abigail Clune</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Study of Disaster Support Mobile Apps</title>
      <link>https://arxiv.org/abs/2407.08145</link>
      <description>arXiv:2407.08145v1 Announce Type: new 
Abstract: Context: Disasters are a common global occurrence with climate change leading to increase both their frequency and intensity. To reduce the impact of these disasters on lives and livelihoods it is important to provide accurate warnings and information about recovery and mitigation. Today most emergency management agencies deliver this information via mobile apps.
  Objective: There is a large collection of disaster mobile apps available across the globe. But a detailed study is not yet conducted on these apps and their reviews to understand their key features and user feedback. In this paper we present a comprehensive analysis to address this research gap.
  Method: We conducted a detailed analysis of 45 disaster apps and 28,161 reviews on these apps. We manually analysed the features of these 45 apps and for review analysis employed topic modelling and sentiment analysis techniques.
  Results: We identified 13 key features in these apps and categorised them in to the 4 stages of disaster life cycle. Our analysis revealed 22 topics with highest discussions being on apps alert functionality, app satisfaction and use of maps. Sentiment analysis of reviews showed that while 22\% of users provided positive feedback, 9.5\% were negative and 6.8\% were neutral. It also showed that signup/signin issues, network issues and app configuration issues were the most frustrating to users. These impacted user safety as these prevented them from accessing the app when it mattered most.
  Conclusions: We provide a set of practical recommendations for future disaster app developers. Our findings will help emergency agencies develop better disaster apps by ensuring key features are supported in their apps, by understanding commonly discussed user issues. This will help to improve the disaster app eco-system and lead to more user friendly and supportive disaster support apps in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08145v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhamad Syukron, Anuradha Madugalla, Mojtaba Shahin, John Grundy</dc:creator>
    </item>
    <item>
      <title>Foundation Model Engineering: Engineering Foundation Models Just as Engineering Software</title>
      <link>https://arxiv.org/abs/2407.08176</link>
      <description>arXiv:2407.08176v1 Announce Type: new 
Abstract: By treating data and models as the source code, Foundation Models (FMs) become a new type of software. Mirroring the concept of software crisis, the increasing complexity of FMs making FM crisis a tangible concern in the coming decade, appealing for new theories and methodologies from the field of software engineering. In this paper, we outline our vision of introducing Foundation Model (FM) engineering, a strategic response to the anticipated FM crisis with principled engineering methodologies. FM engineering aims to mitigate potential issues in FM development and application through the introduction of declarative, automated, and unified programming interfaces for both data and model management, reducing the complexities involved in working with FMs by providing a more structured and intuitive process for developers. Through the establishment of FM engineering, we aim to provide a robust, automated, and extensible framework that addresses the imminent challenges, and discovering new research opportunities for the software engineering field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08176v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dezhi Ran, Mengzhou Wu, Wei Yang, Tao Xie</dc:creator>
    </item>
    <item>
      <title>Verificarlo CI: continuous integration for numerical optimization and debugging</title>
      <link>https://arxiv.org/abs/2407.08262</link>
      <description>arXiv:2407.08262v1 Announce Type: new 
Abstract: Floating-point accuracy is an important concern when developing numerical simulations or other compute-intensive codes. Tracking the introduction of numerical regression is often delayed until it provokes unexpected bug for the end-user.  In this paper, we introduce Verificarlo CI, a continuous integration workflow for the numerical optimization and debugging of a code over the course of its development. We demonstrate applicability of Verificarlo CI on two test-case applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08262v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aur\'elien Delval (LI-PaRAD), Fran\c{c}ois Coppens (LI-PaRAD), Eric Petit (LI-PaRAD, UVSQ), Roman Iakymchuk (LI-PaRAD, UVSQ), Pablo de Oliveira Castro (LI-PaRAD, UVSQ)</dc:creator>
    </item>
    <item>
      <title>eUDEVS: Executable UML with DEVS Theory of Modeling and Simulation</title>
      <link>https://arxiv.org/abs/2407.08281</link>
      <description>arXiv:2407.08281v1 Announce Type: new 
Abstract: Modeling and Simulation (M&amp;S) for system design and prototyping is practiced today both in the industry and academia. M&amp;S are two different areas altogether and have specific objectives. However, most of the times these two separate areas are taken together. The developed code is tightly woven around both the model and the underlying simulator that executes it. This constraints both the model development and the simulation engine that impacts scalability of the developed code. Furthermore, a lot of time is spent in development of a model because it needs both domain knowledge and simulation techniques, which also requires communication among users and developers. Unified Modeling Language (UML) is widely accepted in the industry, whereas Discrete Event Specification (DEVS) based modeling that separates the model and the simulator, provides a cleaner methodology to develop models and is much used in academia. DEVS today is used by engineers who understand discrete event modeling at a much detailed level and are able to translate requirements to DEVS modeling code. There have been earlier efforts to integrate UML and DEVS but they haven't succeeded in providing a transformation mechanism due to inherent differences in these two modeling paradigms. This paper presents an integrated approach towards crosstransformations between UML and DEVS using the proposed eUDEVS, which stands for executable UML based on DEVS. Further, we will also show that the obtained DEVS models belong to a specific class of DEVS models called Finite Deterministic DEVS (FD-DEVS) that is available as a W3C XML Schema in XFD-DEVS. We also put the proposed eUDEVS in a much larger unifying framework called DEVS Unified Process that allows bifurcated model-continuity based lifecycle methodology for systems M&amp;S. Finally, we demonstrate the laid concepts with a complete example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08281v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1177/0037549709104727</arxiv:DOI>
      <arxiv:journal_reference>SIMULATION: Transactions of the SCS, 85(11-12), pp. 750-777, 2009</arxiv:journal_reference>
      <dc:creator>Jos\'e L. Risco-Mart\'in, J. M. Cruz, Saurabh Mittal, Bernard P. Zeigler</dc:creator>
    </item>
    <item>
      <title>Scenario-Based Field Testing of Drone Missions</title>
      <link>https://arxiv.org/abs/2407.08359</link>
      <description>arXiv:2407.08359v1 Announce Type: new 
Abstract: Testing and validating Cyber-Physical Systems (CPSs) in the aerospace domain, such as field testing of drone rescue missions, poses challenges due to volatile mission environments, such as weather conditions. While testing processes and methodologies are well established, structured guidance and execution support for field tests are still weak. This paper identifies requirements for field testing of drone missions, and introduces the Field Testing Scenario Management (FiTS) approach for adaptive field testing guidance. FiTS aims to provide sufficient guidance for field testers as a foundation for efficient data collection to facilitate quality assurance and iterative improvement of field tests and CPSs. FiTS shall leverage concepts from scenario-based requirements engineering and Behavior-Driven Development to define structured and reusable test scenarios, with dedicated tasks and responsibilities for role-specific guidance. We evaluate FiTS by (i) applying it to three use cases for a search-and-rescue drone application to demonstrate feasibility and (ii) interviews with experienced drone developers to assess its usefulness and collect further requirements. The study results indicate FiTS to be feasible and useful to facilitate drone field testing and data analysis</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08359v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Michael Vierhauser, Kristof Meixner, Stefan Biffl</dc:creator>
    </item>
    <item>
      <title>Inductive Predicate Synthesis Modulo Programs (Extended)</title>
      <link>https://arxiv.org/abs/2407.08455</link>
      <description>arXiv:2407.08455v1 Announce Type: new 
Abstract: A growing trend in program analysis is to encode verification conditions within the language of the input program. This simplifies the design of analysis tools by utilizing off-the-shelf verifiers, but makes communication with the underlying solver more challenging. Essentially, the analyzer operates at the level of input programs, whereas the solver operates at the level of problem encodings. To bridge this gap, the verifier must pass along proof-rules from the analyzer to the solver. For example, an analyzer for concurrent programs built on an inductive program verifier might need to declare Owicki-Gries style proof-rules for the underlying solver. Each such proof-rule further specifies how a program should be verified, meaning that the problem of passing proof-rules is a form of invariant synthesis.
  Similarly, many program analysis tasks reduce to the synthesis of pure, loop-free Boolean functions (i.e., predicates), relative to a program. From this observation, we propose Inductive Predicate Synthesis Modulo Programs (IPS-MP) which extends high-level languages with minimal synthesis features to guide analysis. In IPS-MP, unknown predicates appear under assume and assert statements, acting as specifications modulo the program semantics. Existing synthesis solvers are inefficient at IPS-MP as they target more general problems. In this paper, we show that IPS-MP admits an efficient solution in the Boolean case, despite being generally undecidable. Moreover, we show that IPS-MP reduces to the satisfiability of constrained Horn clauses, which is less general than existing synthesis problems, yet expressive enough to encode verification tasks. We provide reductions from challenging verification tasks -- such as parameterized model checking -- to IPS-MP. We realize these reductions with an efficient IPS-MP-solver based on SeaHorn, and describe a application to smart-contract verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08455v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Scott Wesley, Maria Christakis, Jorge A. Navas, Richard Trefler, Valentin W\"ustholz, Arie Gurfinkel</dc:creator>
    </item>
    <item>
      <title>Learning Program Behavioral Models from Synthesized Input-Output Pairs</title>
      <link>https://arxiv.org/abs/2407.08597</link>
      <description>arXiv:2407.08597v1 Announce Type: new 
Abstract: We introduce Modelizer - a novel framework that, given a black-box program, learns a _model from its input/output behavior_ using _neural machine translation_. The resulting model _mocks_ the original program: Given an input, the model predicts the output that would have been produced by the program. However, the model is also _reversible_ - that is, the model can predict the input that would have produced a given output. Finally, the model is _differentiable_ and can be efficiently restricted to predict only a certain aspect of the program behavior.
  Modelizer uses _grammars_ to synthesize inputs and to parse the resulting outputs, allowing it to learn sequence-to-sequence associations between token streams. Other than input and output grammars, Modelizer only requires the ability to execute the program.
  The resulting models are _small_, requiring fewer than 6.3 million parameters for languages such as Markdown or HTML; and they are _accurate_, achieving up to 95.4% accuracy and a BLEU score of 0.98 with standard error 0.04 in mocking real-world applications. We foresee several _applications_ of these models, especially as the output of the program can be any aspect of program behavior. Besides mocking and predicting program behavior, the model can also synthesize inputs that are likely to produce a particular behavior, such as failures or coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08597v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tural Mammadov, Dietrich Klakow, Alexander Koller, Andreas Zeller</dc:creator>
    </item>
    <item>
      <title>Semantic GUI Scene Learning and Video Alignment for Detecting Duplicate Video-based Bug Reports</title>
      <link>https://arxiv.org/abs/2407.08610</link>
      <description>arXiv:2407.08610v1 Announce Type: new 
Abstract: Video-based bug reports are increasingly being used to document bugs for programs centered around a graphical user interface (GUI). However, developing automated techniques to manage video-based reports is challenging as it requires identifying and understanding often nuanced visual patterns that capture key information about a reported bug. In this paper, we aim to overcome these challenges by advancing the bug report management task of duplicate detection for video-based reports. To this end, we introduce a new approach, called JANUS, that adapts the scene-learning capabilities of vision transformers to capture subtle visual and textual patterns that manifest on app UI screens - which is key to differentiating between similar screens for accurate duplicate report detection. JANUS also makes use of a video alignment technique capable of adaptive weighting of video frames to account for typical bug manifestation patterns. In a comprehensive evaluation on a benchmark containing 7,290 duplicate detection tasks derived from 270 video-based bug reports from 90 Android app bugs, the best configuration of our approach achieves an overall mRR/mAP of 89.8%/84.7%, and for the large majority of duplicate detection tasks, outperforms prior work by around 9% to a statistically significant degree. Finally, we qualitatively illustrate how the scene-learning capabilities provided by Janus benefits its performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08610v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanfu Yan, Nathan Cooper, Oscar Chaparro, Kevin Moran, Denys Poshyvanyk</dc:creator>
    </item>
    <item>
      <title>Teaching Type Systems Implementation with Stella, an Extensible Statically Typed Programming Language</title>
      <link>https://arxiv.org/abs/2407.08089</link>
      <description>arXiv:2407.08089v1 Announce Type: cross 
Abstract: We report on a half-semester course focused around implementation of type systems in programming languages. The course assumes basics of classical compiler construction, in particular, the abstract syntax representation, the Visitor pattern, and parsing. The course is built around a language Stella with a minimalistic core and a set of small extensions, covering algebraic data types, references, exceptions, exhaustive pattern matching, subtyping, recursive types, universal polymorphism, and type reconstruction. Optionally, an implementation of an interpreter and a compiler is offered to the students. To facilitate fast development and variety of implementation languages we rely on the BNF Converter tool and provide templates for the students in multiple languages. Finally, we report some results of teaching based on students' achievements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08089v1</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.405.1</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 405, 2024, pp. 1-19</arxiv:journal_reference>
      <dc:creator>Abdelrahman Abounegm (Innopolis University), Nikolai Kudasov (Innopolis University), Alexey Stepanov (Innopolis University)</dc:creator>
    </item>
    <item>
      <title>DIDUP: Dynamic Iterative Development for UI Prototyping</title>
      <link>https://arxiv.org/abs/2407.08474</link>
      <description>arXiv:2407.08474v1 Announce Type: cross 
Abstract: Large language models (LLMs) are remarkably good at writing code. A particularly valuable case of human-LLM collaboration is code-based UI prototyping, a method for creating interactive prototypes that allows users to view and fully engage with a user interface. We conduct a formative study of GPT Pilot, a leading LLM-generated code-prototyping system, and find that its inflexibility towards change once development has started leads to weaknesses in failure prevention and dynamic planning; it closely resembles the linear workflow of the waterfall model. We introduce DIDUP, a system for code-based UI prototyping that follows an iterative spiral model, which takes changes and iterations that come up during the development process into account. We propose three novel mechanisms for LLM-generated code-prototyping systems: (1) adaptive planning, where plans should be dynamic and reflect changes during implementation, (2) code injection, where the system should write a minimal amount of code and inject it instead of rewriting code so users have a better mental model of the code evolution, and (3) lightweight state management, a simplified version of source control so users can quickly revert to different working states. Together, this enables users to rapidly develop and iterate on prototypes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08474v1</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenny Ma, Karthik Sreedhar, Vivian Liu, Sitong Wang, Pedro Alejandro Perez, Lydia B. Chilton</dc:creator>
    </item>
    <item>
      <title>Tactics, Techniques, and Procedures (TTPs) in Interpreted Malware: A Zero-Shot Generation with Large Language Models</title>
      <link>https://arxiv.org/abs/2407.08532</link>
      <description>arXiv:2407.08532v1 Announce Type: cross 
Abstract: Nowadays, the open-source software (OSS) ecosystem suffers from security threats of software supply chain (SSC) attacks. Interpreted OSS malware plays a vital role in SSC attacks, as criminals have an arsenal of attack vectors to deceive users into installing malware and executing malicious activities. In this paper, we introduce tactics, techniques, and procedures (TTPs) proposed by MITRE ATT\&amp;CK into the interpreted malware analysis to characterize different phases of an attack lifecycle. Specifically, we propose GENTTP, a zero-shot approach to extracting a TTP of an interpreted malware package. GENTTP leverages large language models (LLMs) to automatically generate a TTP, where the input is a malicious package, and the output is a deceptive tactic and an execution tactic of attack vectors. To validate the effectiveness of GENTTP, we collect two datasets for evaluation: a dataset with ground truth labels and a large dataset in the wild. Experimental results show that GENTTP can generate TTPs with high accuracy and efficiency. To demonstrate GENTTP's benefits, we build an LLM-based Chatbot from 3,700+ PyPI malware's TTPs. We further conduct a quantitative analysis of malware's TTPs at a large scale. Our main findings include: (1) many OSS malicious packages share a relatively stable TTP, even with the increasing emergence of malware and attack campaigns, (2) a TTP reflects characteristics of a malware-based attack, and (3) an attacker's intent behind the malware is linked to a TTP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08532v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Zhang, Xiaoyan Zhou, Hui Wen, Wenjia Niu, Jiqiang Liu, Haining Wang, Qiang Li</dc:creator>
    </item>
    <item>
      <title>The Upper Bound of Information Diffusion in Code Review</title>
      <link>https://arxiv.org/abs/2306.08980</link>
      <description>arXiv:2306.08980v4 Announce Type: replace 
Abstract: Background: Code review, the discussion around a code change among humans, forms a communication network that enables its participants to exchange and spread information. Although reported by qualitative studies, our understanding of the capability of code review as a communication network is still limited. Objective: In this article, we report on a first step towards evaluating the capability of code review as a communication network by quantifying how fast and how far information can spread through code review: the upper bound of information diffusion in code review. Method: In an in-silico experiment, we simulate an artificial information diffusion within large (Microsoft), mid-sized (Spotify), and small code review systems (Trivago) modelled as communication networks. We then measure the minimal topological and temporal distances between the participants to quantify how far and how fast information can spread in code review. Results: An average code review participants in the small and mid-sized code review systems can spread information to between 72% and 85% of all code review participants within four weeks independently of network size and tooling; for the large code review systems, we found an absolute boundary of about 11000 reachable participants. On average (median), information can spread between two participants in code review in less than five hops and less than five days. Conclusion: We found evidence that the communication network emerging from code review scales well and spreads information fast and broadly, corroborating the findings of prior qualitative work. The study lays the foundation for understanding and improving code review as a communication network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.08980v4</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Dorner, Daniel Mendez, Krzysztof Wnuk, Ehsan Zabardast, Jacek Czerwonka</dc:creator>
    </item>
    <item>
      <title>HITA: An Architecture for System-level Testing of Healthcare IoT Applications</title>
      <link>https://arxiv.org/abs/2309.04223</link>
      <description>arXiv:2309.04223v3 Announce Type: replace 
Abstract: System-level testing of healthcare Internet of Things (IoT) applications requires creating a test infrastructure with integrated medical devices and third-party applications. A significant challenge in creating such test infrastructure is that healthcare IoT applications evolve continuously with the addition of new medical devices from different vendors and new services offered by different third-party organizations following different architectures. Moreover, creating test infrastructure with a large number of different types of medical devices is time-consuming, financially expensive, and practically infeasible. Oslo City's healthcare department faced these challenges while working with various healthcare IoT applications. To address these challenges, this paper presents a real-world test infrastructure software architecture (HITA) designed for healthcare IoT applications. We evaluated HITA's digital twin (DT) generation component implemented using model-based and machine learning (ML) approaches in terms of DT fidelity, scalability, and time cost of generating DTs. Results show that the fidelity of DTs created using model-based and ML approaches reach 94% and 95%, respectively. Results from operating 100 DTs concurrently show that the DT generation component is scalable and ML-based DTs have a higher time cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04223v3</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hassan Sartaj, Shaukat Ali, Tao Yue, Julie Marie Gj{\o}by</dc:creator>
    </item>
    <item>
      <title>Mining Issue Trackers: Concepts and Techniques</title>
      <link>https://arxiv.org/abs/2403.05716</link>
      <description>arXiv:2403.05716v2 Announce Type: replace 
Abstract: An issue tracker is a software tool used by organisations to interact with users and manage various aspects of the software development lifecycle. With the rise of agile methodologies, issue trackers have become popular in open and closed-source settings alike. Internal and external stakeholders report, manage, and discuss "issues", which represent different information such as requirements and maintenance tasks. Issue trackers can quickly become complex ecosystems, with dozens of projects, hundreds of users, thousands of issues, and often millions of issue evolutions. Finding and understanding the relevant issues for the task at hand and keeping an overview becomes difficult with time. Moreover, managing issue workflows for diverse projects becomes more difficult as organisations grow, and more stakeholders get involved. To help address these difficulties, software and requirements engineering research have suggested automated techniques based on mining issue tracking data. Given the vast amount of textual data in issue trackers, many of these techniques leverage natural language processing. This chapter discusses four major use cases for algorithmically analysing issue data to assist stakeholders with the complexity and heterogeneity of information in issue trackers. The chapter is accompanied by a follow-along demonstration package with JupyterNotebooks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05716v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lloyd Montgomery, Clara L\"uders, Walid Maalej</dc:creator>
    </item>
    <item>
      <title>AbstractBeam: Enhancing Bottom-Up Program Synthesis using Library Learning</title>
      <link>https://arxiv.org/abs/2405.17514</link>
      <description>arXiv:2405.17514v2 Announce Type: replace 
Abstract: LambdaBeam is a state-of-the-art execution-guided algorithm for program synthesis that incorporates higher-order functions, lambda functions, and iterative loops into the Domain-Specific Language (DSL). LambdaBeam generates every program from the start. Yet, many program blocks or subprograms occur frequently in a given domain, e.g., loops to traverse a list. Thus, repeating programs can be used to enhance the synthesis algorithm. However, LambdaBeam fails to leverage this potential. For this purpose, we introduce AbstractBeam: A novel program synthesis framework that employs Library Learning to identify such program repetitions, integrates them into the DSL, and thus utilizes their potential to boost LambdaBeam's synthesis algorithm. Our experimental evaluations demonstrate that AbstractBeam significantly improves LambdaBeam's performance in the LambdaBeam integer list manipulation domain. Additionally, AbstractBeam's program generation is more efficient compared to LambdaBeam's synthesis. Finally, our findings indicate that Library Learning is effective in domains not specifically crafted to highlight its benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17514v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Janis Zenkner, Lukas Dierkes, Tobias Sesterhenn, Chrisitan Bartelt</dc:creator>
    </item>
    <item>
      <title>SpecTra: Enhancing the Code Translation Ability of Language Models by Generating Multi-Modal Specifications</title>
      <link>https://arxiv.org/abs/2405.18574</link>
      <description>arXiv:2405.18574v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly being used for the task of automated code translation, which has important real-world applications. However, most existing approaches use only the source code of a program as an input to an LLM, and do not consider the different kinds of specifications that can be extracted from a program. In this paper, we propose SpecTra, a multi-stage approach that uses a novel self-consistency filter to first generate high-quality static specifications, test cases, and natural language descriptions from a given program, and then uses these along with the source code to improve the quality of LLM-generated translations. We evaluate SpecTra on three code translation tasks - C to Rust, C to Go, and JavaScript to TypeScript - and show that it can enhance the performance of six popular LLMs on these tasks by up to 10 percentage points and a relative improvement of 26\%. Our research suggests that generating high-quality specifications could be a promising and efficient way to improve the performance of LLMs for code translation. We make our code and data available, anonymized for review.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18574v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vikram Nitin, Rahul Krishna, Baishakhi Ray</dc:creator>
    </item>
    <item>
      <title>A Comparison of Vulnerability Feature Extraction Methods from Textual Attack Patterns</title>
      <link>https://arxiv.org/abs/2407.06753</link>
      <description>arXiv:2407.06753v2 Announce Type: replace-cross 
Abstract: Nowadays, threat reports from cybersecurity vendors incorporate detailed descriptions of attacks within unstructured text. Knowing vulnerabilities that are related to these reports helps cybersecurity researchers and practitioners understand and adjust to evolving attacks and develop mitigation plans. This paper aims to aid cybersecurity researchers and practitioners in choosing attack extraction methods to enhance the monitoring and sharing of threat intelligence. In this work, we examine five feature extraction methods (TF-IDF, LSI, BERT, MiniLM, RoBERTa) and find that Term Frequency-Inverse Document Frequency (TF-IDF) outperforms the other four methods with a precision of 75\% and an F1 score of 64\%. The findings offer valuable insights to the cybersecurity community, and our research can aid cybersecurity researchers in evaluating and comparing the effectiveness of upcoming extraction methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06753v2</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Refat Othman, Bruno Rossi, Russo Barbara</dc:creator>
    </item>
    <item>
      <title>Cybersecurity Defenses: Exploration of CVE Types through Attack Descriptions</title>
      <link>https://arxiv.org/abs/2407.06759</link>
      <description>arXiv:2407.06759v2 Announce Type: replace-cross 
Abstract: Vulnerabilities in software security can remain undiscovered even after being exploited. Linking attacks to vulnerabilities helps experts identify and respond promptly to the incident. This paper introduces VULDAT, a classification tool using a sentence transformer MPNET to identify system vulnerabilities from attack descriptions. Our model was applied to 100 attack techniques from the ATT&amp;CK repository and 685 issues from the CVE repository. Then, we compare the performance of VULDAT against the other eight state-of-the-art classifiers based on sentence transformers. Our findings indicate that our model achieves the best performance with F1 score of 0.85, Precision of 0.86, and Recall of 0.83. Furthermore, we found 56% of CVE reports vulnerabilities associated with an attack were identified by VULDAT, and 61% of identified vulnerabilities were in the CVE repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06759v2</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Refat Othman, Bruno Rossi, Barbara Russo</dc:creator>
    </item>
  </channel>
</rss>
