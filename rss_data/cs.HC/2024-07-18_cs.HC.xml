<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Jul 2024 01:56:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An efficient machine learning approach for extracting eSports players distinguishing features and classifying their skill levels using symbolic transfer entropy and consensus nested cross validation</title>
      <link>https://arxiv.org/abs/2407.11972</link>
      <description>arXiv:2407.11972v1 Announce Type: new 
Abstract: Discovering features that set elite players apart is of great significance for eSports coaches as it enables them to arrange a more effective training program focused on improving those features. Moreover, finding such features results in a better evaluation of eSports players skills, which, besides coaches, is of interest for game developers to design games automatically adaptable to the players expertise. Sensor data combined with machine learning have already proved effective in classifying eSports players. However, the existing methods do not provide sufficient information about features that distinguish high-skilled players. In this paper, we propose an efficient method to find these features and then use them to classify players' skill levels. We first apply a time window to extract the players' sensor data, including heart rate, hand activities, etc., before and after game events in the League of Legends game. We use the extracted segments and symbolic transfer entropy to calculate connectivity features between sensors. The most relevant features are then selected using the newly developed consensus nested cross validation method. These features, representing the harmony between body parts, are finally used to find the optimum window size and classify players' skills. The classification results demonstrate a significant improvement by achieving 90.1% accuracy. Also, connectivity features between players gaze positions and keyboard, mouse, and hand activities were the most distinguishing features in classifying players' skills. The proposed method in this paper can be similarly applied to sportspeople data and potentially revolutionize the training programs in both eSports and sports industries</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11972v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s41060-024-00529-6</arxiv:DOI>
      <dc:creator>Amin Noroozi, Mohammad S. Hasan, Maryam Ravan, Elham Norouzi, Ying-Ying Law</dc:creator>
    </item>
    <item>
      <title>Preliminary Study of the Impact of AI-Based Interventions on Health and Behavioral Outcomes in Maternal Health Programs</title>
      <link>https://arxiv.org/abs/2407.11973</link>
      <description>arXiv:2407.11973v1 Announce Type: new 
Abstract: Automated voice calls are an effective method of delivering maternal and child health information to mothers in underserved communities. One method to fight dwindling listenership is through an intervention in which health workers make live service calls. Previous work has shown that we can use AI to identify beneficiaries whose listenership gets the greatest boost from an intervention. It has also been demonstrated that listening to the automated voice calls consistently leads to improved health outcomes for the beneficiaries of the program. These two observations combined suggest the positive effect of AI-based intervention scheduling on behavioral and health outcomes. This study analyzes the relationship between the two. Specifically, we are interested in mothers' health knowledge in the post-natal period, measured through survey questions. We present evidence that improved listenership through AI-scheduled interventions leads to a better understanding of key health issues during pregnancy and infancy. This improved understanding has the potential to benefit the health outcomes of mothers and their babies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11973v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arpan Dasgupta, Niclas Boehmer, Neha Madhiwalla, Aparna Hedge, Bryan Wilder, Milind Tambe, Aparna Taneja</dc:creator>
    </item>
    <item>
      <title>Explainable AI Enhances Glaucoma Referrals, Yet the Human-AI Team Still Falls Short of the AI Alone</title>
      <link>https://arxiv.org/abs/2407.11974</link>
      <description>arXiv:2407.11974v1 Announce Type: new 
Abstract: Primary care providers are vital for initial triage and referrals to specialty care. In glaucoma, asymptomatic and fast progression can lead to vision loss, necessitating timely referrals to specialists. However, primary eye care providers may not identify urgent cases, potentially delaying care. Artificial Intelligence (AI) offering explanations could enhance their referral decisions. We investigate how various AI explanations help providers distinguish between patients needing immediate or non-urgent specialist referrals. We built explainable AI algorithms to predict glaucoma surgery needs from routine eyecare data as a proxy for identifying high-risk patients. We incorporated intrinsic and post-hoc explainability and conducted an online study with optometrists to assess human-AI team performance, measuring referral accuracy and analyzing interactions with AI, including agreement rates, task time, and user experience perceptions. AI support enhanced referral accuracy among 87 participants (59.9%/50.8% with/without AI), though Human-AI teams underperformed compared to AI alone. Participants believed they included AI advice more when using the intrinsic model, and perceived it more useful and promising. Without explanations, deviations from AI recommendations increased. AI support did not increase workload, confidence, and trust, but reduced challenges. On a separate test set, our black-box and intrinsic models achieved an accuracy of 77% and 71%, respectively, in predicting surgical outcomes. We identify opportunities of human-AI teaming for glaucoma management in primary eye care, noting that while AI enhances referral accuracy, it also shows a performance gap compared to AI alone, even with explanations. Human involvement remains essential in medical decision making, underscoring the need for future research to optimize collaboration, ensuring positive experiences and safe AI use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11974v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Catalina Gomez, Ruolin Wang, Katharina Breininger, Corinne Casey, Chris Bradley, Mitchell Pavlak, Alex Pham, Jithin Yohannan, Mathias Unberath</dc:creator>
    </item>
    <item>
      <title>Comparing Visual Metaphors with Textual Code For Learning Basic Computer Science Concepts in Virtual Reality</title>
      <link>https://arxiv.org/abs/2407.11975</link>
      <description>arXiv:2407.11975v1 Announce Type: new 
Abstract: This paper represents a pilot study examining learners who are new to computer science (CS). Subjects are taught to program in one of two virtual reality (VR) applications developed by the researcher that use interactable objects representing programming concepts. The different versions are the basis for two experimental groups. One version of the app uses textual code for the interactable programming objects and the other version uses everyday objects as visual metaphors for the CS concepts the programming objects represent. For the two experimental groups, the study compares the results of self-efficacy surveys and CS knowledge tests taken before and after the VR activity intervention. An attitudinal survey taken after the intervention examines learners' sense of productivity and engagement with the VR activity. While further iterations of the study with a larger sample size would be needed to confirm any results, preliminary findings from the pilot study suggest that both methods of teaching basic programming concepts in VR can lead to increased levels of self-efficacy and knowledge regarding CS, and can contribute toward productive mental states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11975v1</guid>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin William Baron</dc:creator>
    </item>
    <item>
      <title>Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents</title>
      <link>https://arxiv.org/abs/2407.11977</link>
      <description>arXiv:2407.11977v1 Announce Type: new 
Abstract: The incorporation of Large Language Models (LLMs) such as the GPT series into diverse sectors including healthcare, education, and finance marks a significant evolution in the field of artificial intelligence (AI). The increasing demand for personalised applications motivated the design of conversational agents (CAs) to possess distinct personas. This paper commences by examining the rationale and implications of imbuing CAs with unique personas, smoothly transitioning into a broader discussion of the personalisation and anthropomorphism of CAs based on LLMs in the LLM era. We delve into the specific applications where the implementation of a persona is not just beneficial but critical for LLM-based CAs. The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise. Attention is directed towards the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11977v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangzhi Sun, Xiao Zhan, Jose Such</dc:creator>
    </item>
    <item>
      <title>"It depends": Configuring AI to Improve Clinical Usefulness Across Contexts</title>
      <link>https://arxiv.org/abs/2407.11978</link>
      <description>arXiv:2407.11978v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) repeatedly match or outperform radiologists in lab experiments. However, real-world implementations of radiological AI-based systems are found to provide little to no clinical value. This paper explores how to design AI for clinical usefulness in different contexts. We conducted 19 design sessions and design interventions with 13 radiologists from 7 clinical sites in Denmark and Kenya, based on three iterations of a functional AI-based prototype. Ten sociotechnical dependencies were identified as crucial for the design of AI in radiology. We conceptualised four technical dimensions that must be configured to the intended clinical context of use: AI functionality, AI medical focus, AI decision threshold, and AI Explainability. We present four design recommendations on how to address dependencies pertaining to the medical knowledge, clinic type, user expertise level, patient context, and user situation that condition the configuration of these technical dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11978v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3643834.3660707</arxiv:DOI>
      <dc:creator>Hubert D. Zaj\k{a}c, Jorge M. N. Ribeiro, Silvia Ingala, Simona Gentile, Ruth Wanjohi, Samuel N. Gitau, Jonathan F. Carlsen, Michael B. Nielsen, Tariq O. Andersen</dc:creator>
    </item>
    <item>
      <title>Interpret3C: Interpretable Student Clustering Through Individualized Feature Selection</title>
      <link>https://arxiv.org/abs/2407.11979</link>
      <description>arXiv:2407.11979v1 Announce Type: new 
Abstract: Clustering in education, particularly in large-scale online environments like MOOCs, is essential for understanding and adapting to diverse student needs. However, the effectiveness of clustering depends on its interpretability, which becomes challenging with high-dimensional data. Existing clustering approaches often neglect individual differences in feature importance and rely on a homogenized feature set. Addressing this gap, we introduce Interpret3C (Interpretable Conditional Computation Clustering), a novel clustering pipeline that incorporates interpretable neural networks (NNs) in an unsupervised learning context. This method leverages adaptive gating in NNs to select features for each student. Then, clustering is performed using the most relevant features per student, enhancing clusters' relevance and interpretability. We use Interpret3C to analyze the behavioral clusters considering individual feature importances in a MOOC with over 5,000 students. This research contributes to the field by offering a scalable, robust clustering methodology and an educational case study that respects individual student differences and improves interpretability for high-dimensional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11979v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isadora Salles, Paola Mejia-Domenzain, Vinitra Swamy, Julian Blackwell, Tanja K\"aser</dc:creator>
    </item>
    <item>
      <title>SERENE: The Semi-Automatic User Experience Detector</title>
      <link>https://arxiv.org/abs/2407.11980</link>
      <description>arXiv:2407.11980v1 Announce Type: new 
Abstract: SERENE (uSer ExpeRiENce dEtector), also known as UX-SAD (User eXperience-Smells Automatic Detector), is a research project born in 2020, which comprises different components. As its name suggests, its primary goal is to provide a way to quickly and (semi-) automatically detect problems in the user experience of websites and web-based systems. Through a set of Artificial Intelligence (AI) models, SERENE detects users' emotions in web pages while guaranteeing users' privacy. Its main strength over typical user experience and usability evaluation is in the generalizability of its detections. While traditional methods use samples (that may not be representative), SERENE allows to tap into data provided by the whole user population. The platform is available at https://serene.ddns.net.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11980v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Esposito</dc:creator>
    </item>
    <item>
      <title>What is Beautiful is Still Good: The Attractiveness Halo Effect in the era of Beauty Filters</title>
      <link>https://arxiv.org/abs/2407.11981</link>
      <description>arXiv:2407.11981v1 Announce Type: new 
Abstract: The impact of cognitive biases on decision-making in the digital world remains under-explored despite its well-documented effects in physical contexts. This study addresses this gap by investigating the attractiveness halo effect using AI-based beauty filters. We conduct a large-scale online user study involving 2,748 participants who rated facial images from a diverse set of 462 distinct individuals in two conditions: original and attractive after applying a beauty filter. Our study reveals that the same individuals receive statistically significantly higher ratings of attractiveness and other traits, such as intelligence and trustworthiness, in the attractive condition. We also study the impact of age, gender, and ethnicity and identify a weakening of the halo effect in the beautified condition, resolving conflicting findings from the literature and suggesting that filters could mitigate this cognitive bias. Finally, our findings raise ethical concerns regarding the use of beauty filters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11981v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Gulati, Marina Martinez-Garcia, Daniel Fernandez, Miguel Angel Lozano, Bruno Lepri, Nuria Oliver</dc:creator>
    </item>
    <item>
      <title>Generative AI as a Learning Buddy and Teaching Assistant: Pre-service Teachers' Uses and Attitudes</title>
      <link>https://arxiv.org/abs/2407.11983</link>
      <description>arXiv:2407.11983v1 Announce Type: new 
Abstract: To uncover pre-service teachers' (PSTs') user experience and perceptions of generative artificial intelligence (GenAI) applications, we surveyed 167 Ghana PSTs' specific uses of GenAI as a learning buddy and teaching assistant, and their attitudes towards these applications. Employing exploratory factor analysis (EFA), we identified three key factors shaping PSTs' attitudes towards GenAI: teaching, learning, and ethical and advocacy factors. The mean scores of these factors revealed a generally positive attitude towards GenAI, indicating high levels of agreement on its potential to enhance PSTs' content knowledge and access to learning and teaching resources, thereby reducing their need for assistance from colleagues. Specifically, PSTs use GenAI as a learning buddy to access reading materials, in-depth content explanations, and practical examples, and as a teaching assistant to enhance teaching resources, develop assessment strategies, and plan lessons. A regression analysis showed that background factors such as age, gender, and year of study do not predict PSTs' attitudes towards GenAI, but age and year of study significantly predict the frequency of their use of GenAI, while gender does not. These findings suggest that older PSTs and those further along in their teacher education programs may use GenAI more frequently, but their perceptions of the application remain unchanged. However, PSTs expressed concerns about the accuracy and trustworthiness of the information provided by GenAI applications. We, therefore, suggest addressing these concerns to ensure PSTs can confidently rely on these applications in their teacher preparation programs. Additionally, we recommend targeted strategies to integrate GenAI more effectively into both learning and teaching processes for PSTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11983v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Nyaaba, Lehong Shi, Macharious Nabang, Xiaoming Zhai, Patrick Kyeremeh, Samuel Arthur Ayoberd, Bismark Nyaaba Akanzire</dc:creator>
    </item>
    <item>
      <title>Mimetic Poet</title>
      <link>https://arxiv.org/abs/2407.11984</link>
      <description>arXiv:2407.11984v1 Announce Type: new 
Abstract: This paper presents the design and initial assessment of a novel device that uses generative AI to facilitate creative ideation, inspiration, and reflective thought. Inspired by magnetic poetry, which was originally designed to help overcome writer's block, the device allows participants to compose short poetic texts from a limited vocabulary by physically placing words on the device's surface. Upon composing the text, the system employs a large language model (LLM) to generate a response, displayed on an e-ink screen. We explored various strategies for internally sequencing prompts to foster creative thinking, including analogy, allegorical interpretations, and ideation. We installed the device in our research laboratory for two weeks and held a focus group at the conclusion to evaluate the design. The design choice to limit interactions with the LLM to poetic text, coupled with the tactile experience of assembling the poem, fostered a deeper and more enjoyable engagement with the LLM compared to traditional chatbot or screen-based interactions. This approach gives users the opportunity to reflect on the AI-generated responses in a manner conducive to creative thought.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11984v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jon McCormack, Elliott Wilson, Nina Rajcic, Maria Teresa Llano</dc:creator>
    </item>
    <item>
      <title>A Novel Implementation of Marksheet Parser Using PaddleOCR</title>
      <link>https://arxiv.org/abs/2407.11985</link>
      <description>arXiv:2407.11985v1 Announce Type: new 
Abstract: When an applicant files an online application, there is usually a requirement to fill the marks in the online form and also upload the marksheet in the portal for the verification. A system was built for reading the uploaded marksheet using OCR and automatically filling the rows/ columns in the online form. Though there are partial solutions to this problem - implemented using PyTesseract - the accuracy is low. Hence, the PaddleOCR was used to build the marksheet parser. Several pre-processing and post-processing steps were also performed. The system was tested and evaluated for seven states. Further work is being done and the system is being evaluated for more states and boards of India.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11985v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sankalp Bagaria, S Irene,  Harikrishnan, Elakia V M</dc:creator>
    </item>
    <item>
      <title>SlicerChat: Building a Local Chatbot for 3D Slicer</title>
      <link>https://arxiv.org/abs/2407.11987</link>
      <description>arXiv:2407.11987v1 Announce Type: new 
Abstract: 3D Slicer is a powerful platform for 3D data visualization and analysis, but has a significant learning curve for new users. Generative AI applications, such as ChatGPT, have emerged as a potential method of bridging the gap between various sources of documentation using natural language. The limited exposure of LLM services to 3D Slicer documentation, however, means that ChatGPT and related services tend to suffer from significant hallucination. The objective of this project is to build a chatbot architecture, called SlicerChat, that is optimized to answer 3D Slicer related questions and able to run locally using an open-source model. The core research questions explored in this work revolve around the answer quality and speed differences due to fine-tuning, model size, and the type of domain knowledge included in the prompt. A prototype SlicerChat system was built as a custom extension in 3D Slicer based on the Code-Llama Instruct architecture. Models of size 1.1B, 7B and 13B were fine-tuned using Low rank Adaptation, and various sources of 3D Slicer documentation were compiled for use in a Retrieval Augmented Generation paradigm. Testing combinations of fine-tuning and model sizes on a benchmark dataset of five 3D Slicer questions revealed that fine-tuning had no impact on model performance or speed compared to the base architecture, and that larger models performed better with a significant speed decrease. Experiments with adding 3D Slicer documentation to the prompt showed that Python sample code and Markdown documentation were the most useful information to include, but that adding 3D Slicer scene data and questions taken from Discourse also improved model performance. In conclusion, this project shows the potential for integrating a high quality, local chatbot directly into 3D Slicer to help new users and experienced developers alike to more efficiently use the software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11987v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Colton Barr</dc:creator>
    </item>
    <item>
      <title>Interconnected virtual space and Theater. Practice as research on theater stage in the era of the network</title>
      <link>https://arxiv.org/abs/2407.11989</link>
      <description>arXiv:2407.11989v1 Announce Type: new 
Abstract: Since 2014, we have been conducting experiments based on a multidisciplinary collaboration between specialists in theatrical staging and researchers in virtual reality, digital art, and video games. This team focused its work on the similarities and differencesthat exist between real physical actors (actor-performers) and virtual digital actors (avatars). From this multidisciplinary approach, experimental research-creation projects have emerged and rely on a physical actor playing with the image of an avatar, controlled by another physical actor via the intermediary of a low-cost motion-capture system. In the first part of the paper, we will introduce the scenographic design on which our presentation is based, and the modifications we have made in relation to our previous work. Next, in the second section, we will discuss in detail the impact of augmenting the player's game using an avatar, compared to the scenic limitations of the theatrical stage. In part three of the paper, we will discuss the software-related aspects of the project, focusing on exchanges between the different components of our design and describing the algorithms enabling us to utilize the real-time movement of a player via various capture devices. To conclude, we will examine in detail how our experimental system linking physical actors and avatars profoundly alters the nature of collaboration between directors, actors, and digital artists in terms of actor/avatar direction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11989v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/9781119549765.ch8</arxiv:DOI>
      <dc:creator>Georges Gagner\'e (INREV), C\'edric Plessiet (INREV), R\'emy Sohier (INREV)</dc:creator>
    </item>
    <item>
      <title>Digital twins in sport: Concepts, Taxonomies, Challenges and Practical Potentials</title>
      <link>https://arxiv.org/abs/2407.11990</link>
      <description>arXiv:2407.11990v1 Announce Type: new 
Abstract: Digital twins belong to ten of the strategic technology trends according to the Gartner list from 2019, and have encountered a big expansion, especially with the introduction of Industry 4.0. Sport, on the other hand, has become a constant companion of the modern human suffering a lack of a healthy way of life. The application of digital twins in sport has brought dramatic changes not only in the domain of sport training, but also in managing athletes during competitions, searching for strategical solutions before and tactical solutions during the games by coaches. In this paper, the domain of digital twins in sport is reviewed based on papers which have emerged in this area. At first, the concept of a digital twin is discussed in general. Then, taxonomies of digital twins are appointed. According to these taxonomies, the collection of relevant papers is analyzed, and some real examples of digital twins are exposed. The review finishes with a discussion about how the digital twins affect changes in the modern sport disciplines, and what challenges and opportunities await the digital twins in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11990v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tilen Hli\v{s}, Iztok Fister, Iztok Fister Jr</dc:creator>
    </item>
    <item>
      <title>Inspired by AI? A Novel Generative AI System To Assist Conceptual Automotive Design</title>
      <link>https://arxiv.org/abs/2407.11991</link>
      <description>arXiv:2407.11991v1 Announce Type: new 
Abstract: Design inspiration is crucial for establishing the direction of a design as well as evoking feelings and conveying meanings during the conceptual design process. Many practice designers use text-based searches on platforms like Pinterest to gather image ideas, followed by sketching on paper or using digital tools to develop concepts. Emerging generative AI techniques, such as diffusion models, offer a promising avenue to streamline these processes by swiftly generating design concepts based on text and image inspiration inputs, subsequently using the AI generated design concepts as fresh sources of inspiration for further concept development. However, applying these generative AI techniques directly within a design context has challenges. Firstly, generative AI tools may exhibit a bias towards particular styles, resulting in a lack of diversity of design outputs. Secondly, these tools may struggle to grasp the nuanced meanings of texts or images in a design context. Lastly, the lack of integration with established design processes within design teams can result in fragmented use scenarios. Focusing on these challenges, we conducted workshops, surveys, and data augmentation involving teams of experienced automotive designers to investigate their current practices in generating concepts inspired by texts and images, as well as their preferred interaction modes for generative AI systems to support the concept generation workflow. Finally, we developed a novel generative AI system based on diffusion models to assist conceptual automotive design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11991v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IDETC 2024</arxiv:journal_reference>
      <dc:creator>Ye Wang, Nicole B. Damen, Thomas Gale, Voho Seo, Hooman Shayani</dc:creator>
    </item>
    <item>
      <title>Flowers Revisited: A Preliminary Replication of Flowers et al. 1997</title>
      <link>https://arxiv.org/abs/2407.11992</link>
      <description>arXiv:2407.11992v1 Announce Type: new 
Abstract: In 1997, Flowers, Buhman, and Turnage published a paper titled ``Cross-Modal Equivalence of Visual and Auditory Scatterplots for Exploring Bivariate Data Samples.'' This paper examined our capacity to assess the relationship between two data variables when presented through visual or auditory scatterplots. Twenty-seven years later, we have replicated the first part of this influential study and present the preliminary findings of our replication, initially involving 21 participants. In addition to purely auditory and visual scatterplots, we introduced audiovisual scatterplots as a third condition in our experiment. Our initial findings mirror those of Flowers et al.'s original research. With this extended abstract, we also aim to spark a discussion about the significance of replication studies for our research community in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11992v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <category>cs.MM</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kajetan Enge, Liam Fabry, Robert H\"oldrich</dc:creator>
    </item>
    <item>
      <title>Evaluating Contextually Personalized Programming Exercises Created with Generative AI</title>
      <link>https://arxiv.org/abs/2407.11994</link>
      <description>arXiv:2407.11994v1 Announce Type: new 
Abstract: Programming skills are typically developed through completing various hands-on exercises. Such programming problems can be contextualized to students' interests and cultural backgrounds. Prior research in educational psychology has demonstrated that context personalization of exercises stimulates learners' situational interests and positively affects their engagement. However, creating a varied and comprehensive set of programming exercises for students to practice on is a time-consuming and laborious task for computer science educators. Previous studies have shown that large language models can generate conceptually and contextually relevant programming exercises. Thus, they offer a possibility to automatically produce personalized programming problems to fit students' interests and needs. This article reports on a user study conducted in an elective introductory programming course that included contextually personalized programming exercises created with GPT-4. The quality of the exercises was evaluated by both the students and the authors. Additionally, this work investigated student attitudes towards the created exercises and their engagement with the system. The results demonstrate that the quality of exercises generated with GPT-4 was generally high. What is more, the course participants found them engaging and useful. This suggests that AI-generated programming problems can be a worthwhile addition to introductory programming courses, as they provide students with a practically unlimited pool of practice material tailored to their personal interests and educational needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11994v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evanfiya Logacheva, Arto Hellas, James Prather, Sami Sarsa, Juho Leinonen</dc:creator>
    </item>
    <item>
      <title>Neuro-Symbolic Artificial Intelligence for Patient Monitoring</title>
      <link>https://arxiv.org/abs/2407.11995</link>
      <description>arXiv:2407.11995v1 Announce Type: new 
Abstract: In this paper we argue that Neuro-Symbolic AI (NeSy-AI) should be applied for patient monitoring. In this context, we introduce patient monitoring as a special case of Human Activity Recognition and derive concrete requirements for this application area. We then present a process architecture and discuss why NeSy-AI should be applied for patient monitoring. To further support our argumentation, we show how NeSy-AI can help to overcome certain technical challenges that arise from this application area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11995v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ole Fenske, Sebastian Bader, Thomas Kirste</dc:creator>
    </item>
    <item>
      <title>From Top-Right to User-Right: Perceptual Prioritization of Point-Feature Label Positions</title>
      <link>https://arxiv.org/abs/2407.11996</link>
      <description>arXiv:2407.11996v1 Announce Type: new 
Abstract: In cartography, Geographic Information Systems (GIS), and the entire field of visualization, the position of a label relative to its point feature is pivotal for ensuring visualization readability and improving the user experience. The label placement is governed by the Position Priority Order (PPO), a systematic raking of potential label positions around a point feature according to predetermined priorities. Traditional PPOs have relied heavily on typographic and cartographic conventions established decades ago, which may no longer align with the expectations of today's users. Our extensive user study introduces the Perceptual Position Priority Order (PerceptPPO), a user-validated PPO that significantly departed from traditional conventions. A key finding of our research is the identification of an exact order of label positions, with labels placed at the top of point features being significantly preferred by users, contrary to the conventional top-right position. Furthermore, we performed a supplemental user study to find users' preferred label density - an area scarcely explored in prior research - of a generic map. Finally, we conducted a comparative user study assessing the perceived quality of PerceptPPO compared to existing PPOs. The outcome confirmed PerceptPPO's superior perception among users, advocating its adoption not only in future cartographic and GIS applications but also across various types of visualizations. The effectiveness of PerceptPPO is supported by nearly 800 participants from 48 countries, who collectively contributed to over 45,500 pairwise comparisons across three studies. Our research not only proposes a novel PPO to the research community but also offers practical guidance for designers and application developers aiming to optimize user engagement and comprehension, paving the way for more intuitive and accessible visual solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11996v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petr Bob\'ak, Ladislav \v{C}mol\'ik, Martin \v{C}ad\'ik</dc:creator>
    </item>
    <item>
      <title>HydroTrack: Spectroscopic Analysis Prototype Enabling Real-Time Hydration Monitoring in Wearables</title>
      <link>https://arxiv.org/abs/2407.11997</link>
      <description>arXiv:2407.11997v1 Announce Type: new 
Abstract: In the rapidly growing field of wearable technology, optical devices are emerging as a significant innovation, offering non-invasive methods for analyzing skin and underlying tissue properties. Despite their promise, progress has been slowed by a lack of specialized prototypes and advanced analysis techniques. Addressing this gap, our study introduces, HydroTrack, an 18-channel spectroscopy sensor, ingeniously embedded in a smart-watch. Accompanying this hardware, we present signal processing and data analysis techniques implemented at the edge, designed to maximize the utility of our system in comprehensive health tracking. A pivotal application of our device is the real-time assessment of hydration levels in physically active individuals. We validated our prototype and analytical approach through experiments on six participants, focusing on hydration dynamics during physical exercises. Our findings reveal an accuracy of avg. 95% in determining hydration states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11997v1</guid>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) 2024</arxiv:journal_reference>
      <dc:creator>Nazim A. Belabbaci, Mohammad Arif Ul Alam</dc:creator>
    </item>
    <item>
      <title>Custom Cloth Creation and Virtual Try-on for Everyone</title>
      <link>https://arxiv.org/abs/2407.11998</link>
      <description>arXiv:2407.11998v1 Announce Type: new 
Abstract: This demo showcases a simple tool that utilizes AIGC technology, enabling both professional designers and regular users to easily customize clothing for their digital avatars. Customization options include changing clothing colors, textures, logos, and patterns. Compared with traditional 3D modeling processes, our approach significantly enhances efficiency and interactivity and reduces production costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11998v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pei Chen, Heng Wang, Sainan Sun, Zhiyuan Chen, Zhenkun Liu, Shuhua Cao, Li Yang, Minghui Yang</dc:creator>
    </item>
    <item>
      <title>Evaluation and Continual Improvement for an Enterprise AI Assistant</title>
      <link>https://arxiv.org/abs/2407.12003</link>
      <description>arXiv:2407.12003v1 Announce Type: new 
Abstract: The development of conversational AI assistants is an iterative process with multiple components. As such, the evaluation and continual improvement of these assistants is a complex and multifaceted problem. This paper introduces the challenges in evaluating and improving a generative AI assistant for enterprises, which is under active development, and how we address these challenges. We also share preliminary results and discuss lessons learned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12003v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Akash V. Maharaj, Kun Qian, Uttaran Bhattacharya, Sally Fang, Horia Galatanu, Manas Garg, Rachel Hanessian, Nishant Kapoor, Ken Russell, Shivakumar Vaithyanathan, Yunyao Li</dc:creator>
    </item>
    <item>
      <title>People will agree what I think: Investigating LLM's False Consensus Effect</title>
      <link>https://arxiv.org/abs/2407.12007</link>
      <description>arXiv:2407.12007v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently been widely adopted on interactive systems requiring communications. As the false belief in a model can harm the usability of such systems, LLMs should not have cognitive biases that humans have. Especially psychologists focused on the False Consensus Effect (FCE), which can distract smooth communication by posing false beliefs. However, previous studies have less examined FCE in LLMs thoroughly, which needs more consideration of confounding biases, general situations, and prompt changes. Therefore, in this paper, we conduct two studies to deeply examine the FCE phenomenon in LLMs. In Study 1, we investigate whether LLMs have FCE. In Study 2, we explore how various prompting styles affect the demonstration of FCE. As a result of these studies, we identified that popular LLMs have FCE. Also, the result specifies the conditions when the strength of FCE becomes larger or smaller compared to normal usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12007v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Junhyuk Choi, Yeseon Hong, Bugeun Kim</dc:creator>
    </item>
    <item>
      <title>Surprising Performances of Students with Autism in Classroom with NAO Robot</title>
      <link>https://arxiv.org/abs/2407.12014</link>
      <description>arXiv:2407.12014v1 Announce Type: new 
Abstract: Autism is a developmental disorder that manifests in early childhood and persists throughout life, profoundly affecting social behavior and hindering the acquisition of learning and social skills in those diagnosed. As technological advancements progress, an increasing array of technologies is being utilized to support the education of students with Autism Spectrum Disorder (ASD), aiming to improve their educational outcomes and social capabilities. Numerous studies on autism intervention have highlighted the effectiveness of social robots in behavioral treatments. However, research on the integration of social robots into classroom settings for children with autism remains sparse. This paper describes the design and implementation of a group experiment in a collective classroom setting mediated by the NAO robot. The experiment involved special education teachers and the NAO robot collaboratively conducting classroom activities, aiming to foster a dynamic learning environment through interactions among teachers, the robot, and students. Conducted in a special education school, this experiment served as a foundational study in anticipation of extended robot-assisted classroom sessions. Data from the experiment suggest that ASD students in classrooms equipped with the NAO robot exhibited notably better performance compared to those in regular classrooms. The humanoid features and body language of the NAO robot captivated the students' attention, particularly during talent shows and command tasks, where students demonstrated heightened engagement and a decrease in stereotypical repetitive behaviors and irrelevant minor movements commonly observed in regular settings. Our preliminary findings indicate that the NAO robot significantly enhances focus and classroom engagement among students with ASD, potentially improving educational performance and fostering better social behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12014v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qin Yang, Huan Lu, Dandan Liang, Shengrong Gong, Huanghao Feng</dc:creator>
    </item>
    <item>
      <title>Leveraging Large Language Models for enhanced personalised user experience in Smart Homes</title>
      <link>https://arxiv.org/abs/2407.12024</link>
      <description>arXiv:2407.12024v1 Announce Type: new 
Abstract: Smart home automation systems aim to improve the comfort and convenience of users in their living environment. However, adapting automation to user needs remains a challenge. Indeed, many systems still rely on hand-crafted routines for each smart object.This paper presents an original smart home architecture leveraging Large Language Models (LLMs) and user preferences to push the boundaries of personalisation and intuitiveness in the home environment.This article explores a human-centred approach that uses the general knowledge provided by LLMs to learn and facilitate interactions with the environment.The advantages of the proposed model are demonstrated on a set of scenarios, as well as a comparative analysis with various LLM implementations. Some metrics are assessed to determine the system's ability to maintain comfort, safety, and user preferences. The paper details the approach to real-world implementation and evaluation.The proposed approach of using preferences shows up to 52.3% increase in average grade, and with an average processing time reduced by 35.6% on Starling 7B Alpha LLM. In addition, performance is 26.4% better than the results of the larger models without preferences, with processing time almost 20 times faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12024v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Rey-Jouanchicot (IRIT-ELIPSE, LAAS), Andr\'e Bottaro (LAAS-S4M), Eric Campo (LAAS-S4M), Jean-L\'eon Bouraoui (IRIT-ELIPSE), Nadine Vigouroux (IRIT-ELIPSE), Fr\'ed\'eric Vella (IRIT-ELIPSE)</dc:creator>
    </item>
    <item>
      <title>LLM4DESIGN: An Automated Multi-Modal System for Architectural and Environmental Design</title>
      <link>https://arxiv.org/abs/2407.12025</link>
      <description>arXiv:2407.12025v1 Announce Type: new 
Abstract: This study introduces LLM4DESIGN, a highly automated system for generating architectural and environmental design proposals. LLM4DESIGN, relying solely on site conditions and design requirements, employs Multi-Agent systems to foster creativity, Retrieval Augmented Generation (RAG) to ground designs in realism, and Visual Language Models (VLM) to synchronize all information. This system resulting in coherent, multi-illustrated, and multi-textual design schemes. The system meets the dual needs of narrative storytelling and objective drawing presentation in generating architectural and environmental design proposals. Extensive comparative and ablation experiments confirm the innovativeness of LLM4DESIGN's narrative and the grounded applicability of its plans, demonstrating its superior performance in the field of urban renewal design. Lastly, we have created the first cross-modal design scheme dataset covering architecture, landscape, interior, and urban design, providing rich resources for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12025v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ran Chen, Xueqi Yao, Xuhui Jiang</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Behavioral Economics: Internal Validity and Elicitation of Mental Models</title>
      <link>https://arxiv.org/abs/2407.12032</link>
      <description>arXiv:2407.12032v1 Announce Type: new 
Abstract: In this article, we explore the transformative potential of integrating generative AI, particularly Large Language Models (LLMs), into behavioral and experimental economics to enhance internal validity. By leveraging AI tools, researchers can improve adherence to key exclusion restrictions and in particular ensure the internal validity measures of mental models, which often require human intervention in the incentive mechanism. We present a case study demonstrating how LLMs can enhance experimental design, participant engagement, and the validity of measuring mental models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12032v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Brian Jabarian</dc:creator>
    </item>
    <item>
      <title>Reporting Risks in AI-based Assistive Technology Research: A Systematic Review</title>
      <link>https://arxiv.org/abs/2407.12035</link>
      <description>arXiv:2407.12035v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) is increasingly employed to enhance assistive technologies, yet it can fail in various ways. We conducted a systematic literature review of research into AI-based assistive technology for persons with visual impairments. Our study shows that most proposed technologies with a testable prototype have not been evaluated in a human study with members of the sight-loss community. Furthermore, many studies did not consider or report failure cases or possible risks. These findings highlight the importance of inclusive system evaluations and the necessity of standardizing methods for presenting and analyzing failure cases and threats when developing AI-based assistive technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12035v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zahra Ahmadi, Peter R. Lewis, Mahadeo A. Sukhai</dc:creator>
    </item>
    <item>
      <title>GameDevDojo -- An Educational Game for Teaching Game Development Concepts</title>
      <link>https://arxiv.org/abs/2407.12050</link>
      <description>arXiv:2407.12050v1 Announce Type: new 
Abstract: Computer Science (CS) has experienced significant growth and diversification in recent decades. However, there is a lack of diversity in CS learning approaches. Traditional teaching methods and hands-on learning dominate this field, with limited use of playful and interactive learning methods such as educational games. This gap is particularly evident in game development as a subfield of CS. To address this problem, we present a game-based learning approach to teach foundational concepts for game development. The paper aims to expand the educational landscape within CSE, offering a unique and engaging platform for learners to explore the intricacies of game creation by integrating gamified learning strategies. In this paper, we investigate the user's learning experience and motivation, and the differences between traditional learning and game-based learning methods for teaching game development concepts. The study involves 57 participants in an AB test to assess learners' motivation, user experience, and learning outcomes. The results indicate a significantly increased learning outcome for the game-based learning approach, as well as higher motivation in learning game development concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12050v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Holly, Lisa Habich, Johanna Pirker</dc:creator>
    </item>
    <item>
      <title>Situated Instruction Following</title>
      <link>https://arxiv.org/abs/2407.12061</link>
      <description>arXiv:2407.12061v1 Announce Type: new 
Abstract: Language is never spoken in a vacuum. It is expressed, comprehended, and contextualized within the holistic backdrop of the speaker's history, actions, and environment. Since humans are used to communicating efficiently with situated language, the practicality of robotic assistants hinge on their ability to understand and act upon implicit and situated instructions. In traditional instruction following paradigms, the agent acts alone in an empty house, leading to language use that is both simplified and artificially "complete." In contrast, we propose situated instruction following, which embraces the inherent underspecification and ambiguity of real-world communication with the physical presence of a human speaker. The meaning of situated instructions naturally unfold through the past actions and the expected future behaviors of the human involved. Specifically, within our settings we have instructions that (1) are ambiguously specified, (2) have temporally evolving intent, (3) can be interpreted more precisely with the agent's dynamic actions. Our experiments indicate that state-of-the-art Embodied Instruction Following (EIF) models lack holistic understanding of situated human intention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12061v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>So Yeon Min, Xavi Puig, Devendra Singh Chaplot, Tsung-Yen Yang, Akshara Rai, Priyam Parashar, Ruslan Salakhutdinov, Yonatan Bisk, Roozbeh Mottaghi</dc:creator>
    </item>
    <item>
      <title>Towards Dataset-scale and Feature-oriented Evaluation of Text Summarization in Large Language Model Prompts</title>
      <link>https://arxiv.org/abs/2407.12192</link>
      <description>arXiv:2407.12192v2 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our workflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12192v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sam Yu-Te Lee, Aryaman Bahukhandi, Dongyu Liu, Kwan-Liu Ma</dc:creator>
    </item>
    <item>
      <title>HuBar: A Visual Analytics Tool to Explore Human Behaviour based on fNIRS in AR guidance systems</title>
      <link>https://arxiv.org/abs/2407.12260</link>
      <description>arXiv:2407.12260v1 Announce Type: new 
Abstract: The concept of an intelligent augmented reality (AR) assistant has significant, wide-ranging applications, with potential uses in medicine, military, and mechanics domains. Such an assistant must be able to perceive the environment and actions, reason about the environment state in relation to a given task, and seamlessly interact with the task performer. These interactions typically involve an AR headset equipped with sensors which capture video, audio, and haptic feedback. Previous works have sought to facilitate the development of intelligent AR assistants by visualizing these sensor data streams in conjunction with the assistant's perception and reasoning model outputs. However, existing visual analytics systems do not focus on user modeling or include biometric data, and are only capable of visualizing a single task session for a single performer at a time. Moreover, they typically assume a task involves linear progression from one step to the next. We propose a visual analytics system that allows users to compare performance during multiple task sessions, focusing on non-linear tasks where different step sequences can lead to success. In particular, we design visualizations for understanding user behavior through functional near-infrared spectroscopy (fNIRS) data as a proxy for perception, attention, and memory as well as corresponding motion data (acceleration, angular velocity, and gaze). We distill these insights into embedding representations that allow users to easily select groups of sessions with similar behaviors. We provide two case studies that demonstrate how to use these visualizations to gain insights about task performance using data collected during helicopter copilot training tasks. Finally, we evaluate our approach by conducting an in-depth examination of a think-aloud experiment with five domain experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12260v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sonia Castelo, Joao Rulff, Parikshit Solunke, Erin McGowan, Guande Wu, Iran Roman, Roque Lopez, Bea Steers, Qi Sun, Juan Bello, Bradley Feest, Michael Middleton, Ryan Mckendrick, Claudio Silva</dc:creator>
    </item>
    <item>
      <title>Assessing the Feasibility, and Efficacy of Virtual Reality Navigational Training for Older Adults</title>
      <link>https://arxiv.org/abs/2407.12272</link>
      <description>arXiv:2407.12272v1 Announce Type: new 
Abstract: Objective. Evaluate the feasibility of Virtual Reality (VR) wayfinding training with aging adults, and examine the impact of the training on wayfinding performance. Design. Design involved wayfinding tasks in a study with three groups: active VR training, passive video training, and no training, assigned randomly. The training featured 5 tasks in a digital version of a real building. Post-training assessments had 10 tasks in this building, half familiar from training and half new. The study was double-blinded, with each intervention lasting 10 minutes. Participants. A convenience sample of 49 participants; inclusion criteria: age &gt;58, unfamiliar with the building; exclusion criteria: mobility or vision impairments, history of motion sickness, or medical implants. Outcomes. Time spent and Distance traveled on each wayfinding task with a fixed 10-min limit. Results. Participants in VR group reported moderate usability (63.82, SD=14.55) with respect to the training intervention and high Self Location (3.71, SD=0.94). There were no differences in task performance among the groups in the similar tasks. In the new tasks, compared to the control condition, Time spent on tasks was marginally significantly reduced in the VR group; Distance traveled to finish tasks was also reduced in the VR group, and marginally significantly reduced in the Video training group. No differences were found between VR and Video conditions. No adverse effects were reported during or post intervention. Conclusions. This study provides preliminary evidence that VR training can effectively improve wayfinding performance in older adults with no reported adverse effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12272v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tong Bill Xu, Armin Mostafavi, Walter R. Boot, Sara Czaja, Saleh Kalantari</dc:creator>
    </item>
    <item>
      <title>Demonstrating PilotAR: A Tool to Assist Wizard-of-Oz Pilot Studies with OHMD</title>
      <link>https://arxiv.org/abs/2407.12388</link>
      <description>arXiv:2407.12388v1 Announce Type: new 
Abstract: While pilot studies help to identify potential interesting research directions, the additional requirements in AR/MR make it challenging to conduct quick and dirty pilot studies efficiently with Optical See-Through Head-Mounted Displays (OST HMDs, OHMDs). To overcome these challenges, including the inability to observe and record in-context user interactions, increased task load, and difficulties with in-context data analysis and discussion, we introduce PilotAR (https://github.com/Synteraction-Lab/PilotAR), a tool designed iteratively to enhance AR/MR pilot studies, allowing live first-person and third-person views, multi-modal annotations, flexible wizarding interfaces, and multi-experimenter support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12388v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3675094.3677554</arxiv:DOI>
      <arxiv:journal_reference>UbiComp Companion (2024)</arxiv:journal_reference>
      <dc:creator>Nuwan Janaka, Runze Cai, Shengdong Zhao, David Hsu</dc:creator>
    </item>
    <item>
      <title>Sphere Window: Challenges and Opportunities of 360{\deg} Video in Collaborative Design Workshops</title>
      <link>https://arxiv.org/abs/2407.12407</link>
      <description>arXiv:2407.12407v1 Announce Type: new 
Abstract: The increased ubiquity of 360{\deg} video presents a unique opportunity for designers to deeply engage with the world of users by capturing the complete visual context. However, the opportunities and challenges 360{\deg} video introduces for video design ethnography is unclear. This study investigates this gap through 16 workshops in which experienced designers engaged with 360{\deg} video. Our analysis shows that while 360{\deg} video enhances designers' ability to explore and understand user contexts, it also complicates the process of sharing insights. To address this challenge, we present two opportunities to support the use of 360{\deg} video by designers - the creation of designerly 360{\deg} video annotation tools, and 360{\deg} ``screenshots'' - in order to enable designers to leverage the complete context of 360{\deg} video for user research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12407v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wo Meijer, Jacky Bourgeois, Wilhelm Frederik van der Vegte, Gerd Kortuem</dc:creator>
    </item>
    <item>
      <title>StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT Interactions</title>
      <link>https://arxiv.org/abs/2407.12423</link>
      <description>arXiv:2407.12423v1 Announce Type: new 
Abstract: The integration of Large Language Models (LLMs), especially ChatGPT, into education is poised to revolutionize students' learning experiences by introducing innovative conversational learning methodologies. To empower students to fully leverage the capabilities of ChatGPT in educational scenarios, understanding students' interaction patterns with ChatGPT is crucial for instructors. However, this endeavor is challenging due to the absence of datasets focused on student-ChatGPT conversations and the complexities in identifying and analyzing the evolutional interaction patterns within conversations. To address these challenges, we collected conversational data from 48 students interacting with ChatGPT in a master's level data visualization course over one semester. We then developed a coding scheme, grounded in the literature on cognitive levels and thematic analysis, to categorize students' interaction patterns with ChatGPT. Furthermore, we present a visual analytics system, StuGPTViz, that tracks and compares temporal patterns in student prompts and the quality of ChatGPT's responses at multiple scales, revealing significant pedagogical insights for instructors. We validated the system's effectiveness through expert interviews with six data visualization instructors and three case studies. The results confirmed StuGPTViz's capacity to enhance educators' insights into the pedagogical value of ChatGPT. We also discussed the potential research opportunities of applying visual analytics in education and developing AI-driven personalized learning solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12423v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zixin Chen, Jiachen Wang, Meng Xia, Kento Shigyo, Dingdong Liu, Rong Zhang, Huamin Qu</dc:creator>
    </item>
    <item>
      <title>ExploreGen: Large Language Models for Envisioning the Uses and Risks of AI Technologies</title>
      <link>https://arxiv.org/abs/2407.12454</link>
      <description>arXiv:2407.12454v1 Announce Type: new 
Abstract: Responsible AI design is increasingly seen as an imperative by both AI developers and AI compliance experts. One of the key tasks is envisioning AI technology uses and risks. Recent studies on the model and data cards reveal that AI practitioners struggle with this task due to its inherently challenging nature. Here, we demonstrate that leveraging a Large Language Model (LLM) can support AI practitioners in this task by enabling reflexivity, brainstorming, and deliberation, especially in the early design stages of the AI development process. We developed an LLM framework, ExploreGen, which generates realistic and varied uses of AI technology, including those overlooked by research, and classifies their risk level based on the EU AI Act regulation. We evaluated our framework using the case of Facial Recognition and Analysis technology in nine user studies with 25 AI practitioners. Our findings show that ExploreGen is helpful to both developers and compliance experts. They rated the uses as realistic and their risk classification as accurate (94.5%). Moreover, while unfamiliar with many of the uses, they rated them as having high adoption potential and transformational impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12454v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Viviane Herdel, Sanja \v{S}\'cepanovi\'c, Edyta Bogucka, Daniele Quercia</dc:creator>
    </item>
    <item>
      <title>Decoupled Edge Physics algorithms for collaborative XR simulations</title>
      <link>https://arxiv.org/abs/2407.12486</link>
      <description>arXiv:2407.12486v1 Announce Type: new 
Abstract: This work proposes a novel approach to transform any modern game engine pipeline, for optimized performance and enhanced user experiences in Extended Reality (XR) environments. Decoupling the physics engine from the game engine pipeline and using a client-server N-1 architecture creates a scalable solution, efficiently serving multiple graphics clients on Head-Mounted Displays (HMDs) with a single physics engine on edge-cloud infrastructure. This approach ensures better synchronization in multiplayer scenarios without introducing overhead in single-player experiences, maintaining session continuity despite changes in user participation. Relocating the Physics Engine to an edge or cloud node reduces strain on local hardware, dedicating more resources to high-quality rendering and unlocking the full potential of untethered HMDs. We present four algorithms that decouple the physics engine, increasing frame rates and Quality of Experience (QoE) in VR simulations, supporting advanced interactions, numerous physics objects, and multi-user sessions with over 100 concurrent users. Incorporating a Geometric Algebra interpolator reduces inter-calls between dissected parts, maintaining QoE and easing network stress. Experimental validation, with more than 100 concurrent users, 10,000 physics objects, and softbody simulations, confirms the technical viability of the proposed architecture, showcasing transformative capabilities for more immersive and collaborative XR applications without compromising performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12486v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George Kokiadis, Antonis Protopsaltis, Michalis Morfiadakis, Nick Lydatakis, George Papagiannakis</dc:creator>
    </item>
    <item>
      <title>Application of Prompt Learning Models in Identifying the Collaborative Problem Solving Skills in an Online Task</title>
      <link>https://arxiv.org/abs/2407.12487</link>
      <description>arXiv:2407.12487v1 Announce Type: new 
Abstract: Collaborative problem solving (CPS) competence is considered one of the essential 21st-century skills. To facilitate the assessment and learning of CPS competence, researchers have proposed a series of frameworks to conceptualize CPS and explored ways to make sense of the complex processes involved in collaborative problem solving. However, encoding explicit behaviors into subskills within the frameworks of CPS skills is still a challenging task. Traditional studies have relied on manual coding to decipher behavioral data for CPS, but such coding methods can be very time-consuming and cannot support real-time analyses. Scholars have begun to explore approaches for constructing automatic coding models. Nevertheless, the existing models built using machine learning or deep learning techniques depend on a large amount of training data and have relatively low accuracy. To address these problems, this paper proposes a prompt-based learning pre-trained model. The model can achieve high performance even with limited training data. In this study, three experiments were conducted, and the results showed that our model not only produced the highest accuracy, macro F1 score, and kappa values on large training sets, but also performed the best on small training sets of the CPS behavioral data. The application of the proposed prompt-based learning pre-trained model contributes to the CPS skills coding task and can also be used for other CSCW coding tasks to replace manual coding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12487v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mengxiao Zhu, Xin Wang, Xiantao Wang, Zihang Chen, Wei Huang</dc:creator>
    </item>
    <item>
      <title>AudienceView: AI-Assisted Interpretation of Audience Feedback in Journalism</title>
      <link>https://arxiv.org/abs/2407.12613</link>
      <description>arXiv:2407.12613v1 Announce Type: new 
Abstract: Understanding and making use of audience feedback is important but difficult for journalists, who now face an impractically large volume of audience comments online. We introduce AudienceView, an online tool to help journalists categorize and interpret this feedback by leveraging large language models (LLMs). AudienceView identifies themes and topics, connects them back to specific comments, provides ways to visualize the sentiment and distribution of the comments, and helps users develop ideas for subsequent reporting projects. We consider how such tools can be useful in a journalist's workflow, and emphasize the importance of contextual awareness and human judgment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12613v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Brannon, Doug Beeferman, Hang Jiang, Andrew Heyward, Deb Roy</dc:creator>
    </item>
    <item>
      <title>The Future of Learning: Large Language Models through the Lens of Students</title>
      <link>https://arxiv.org/abs/2407.12723</link>
      <description>arXiv:2407.12723v1 Announce Type: new 
Abstract: As Large-Scale Language Models (LLMs) continue to evolve, they demonstrate significant enhancements in performance and an expansion of functionalities, impacting various domains, including education. In this study, we conducted interviews with 14 students to explore their everyday interactions with ChatGPT. Our preliminary findings reveal that students grapple with the dilemma of utilizing ChatGPT's efficiency for learning and information seeking, while simultaneously experiencing a crisis of trust and ethical concerns regarding the outcomes and broader impacts of ChatGPT. The students perceive ChatGPT as being more "human-like" compared to traditional AI. This dilemma, characterized by mixed emotions, inconsistent behaviors, and an overall positive attitude towards ChatGPT, underscores its potential for beneficial applications in education and learning. However, we argue that despite its human-like qualities, the advanced capabilities of such intelligence might lead to adverse consequences. Therefore, it's imperative to approach its application cautiously and strive to mitigate potential harms in future developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12723v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>He Zhang, Jingyi Xie, Chuhao Wu, Jie Cai, ChanMin Kim, John M. Carroll</dc:creator>
    </item>
    <item>
      <title>An investigation into the scientific landscape of the conversational and generative artificial intelligence, and human-chatbot interaction in education and research</title>
      <link>https://arxiv.org/abs/2407.12004</link>
      <description>arXiv:2407.12004v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) as a disruptive technology is not new. However, its recent evolution, engineered by technological transformation, big data analytics, and quantum computing, produces conversational and generative AI (CGAI/GenAI) and human-like chatbots that disrupt conventional operations and methods in different fields. This study investigates the scientific landscape of CGAI and human-chatbot interaction/collaboration and evaluates use cases, benefits, challenges, and policy implications for multidisciplinary education and allied industry operations. The publications trend showed that just 4% (n=75) occurred during 2006-2018, while 2019-2023 experienced astronomical growth (n=1763 or 96%). The prominent use cases of CGAI (e.g., ChatGPT) for teaching, learning, and research activities occurred in computer science [multidisciplinary and AI] (32%), medical/healthcare (17%), engineering (7%), and business fields (6%). The intellectual structure shows strong collaboration among eminent multidisciplinary sources in business, Information Systems, and other areas. The thematic structure of SLP highlights prominent CGAI use cases, including improved user experience in human-computer interaction, computer programs/code generation, and systems creation. Widespread CGAI usefulness for teachers, researchers, and learners includes syllabi/course content generation, testing aids, and academic writing. The concerns about abuse and misuse (plagiarism, academic integrity, privacy violations) and issues about misinformation, danger of self-diagnoses, and patient privacy in medical/healthcare applications are prominent. Formulating strategies and policies to address potential CGAI challenges in teaching/learning and practice are priorities. Developing discipline-based automatic detection of GenAI contents to check abuse is proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12004v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ikpe Justice Akpan, Yawo M. Kobara, Josiah Owolabi, Asuama Akpam, Onyebuchi Felix Offodile</dc:creator>
    </item>
    <item>
      <title>The Great AI Witch Hunt: Reviewers Perception and (Mis)Conception of Generative AI in Research Writing</title>
      <link>https://arxiv.org/abs/2407.12015</link>
      <description>arXiv:2407.12015v1 Announce Type: cross 
Abstract: Generative AI (GenAI) use in research writing is growing fast. However, it is unclear how peer reviewers recognize or misjudge AI-augmented manuscripts. To investigate the impact of AI-augmented writing on peer reviews, we conducted a snippet-based online survey with 17 peer reviewers from top-tier HCI conferences. Our findings indicate that while AI-augmented writing improves readability, language diversity, and informativeness, it often lacks research details and reflective insights from authors. Reviewers consistently struggled to distinguish between human and AI-augmented writing but their judgements remained consistent. They noted the loss of a "human touch" and subjective expressions in AI-augmented writing. Based on our findings, we advocate for reviewer guidelines that promote impartial evaluations of submissions, regardless of any personal biases towards GenAI. The quality of the research itself should remain a priority in reviews, regardless of any preconceived notions about the tools used to create it. We emphasize that researchers must maintain their authorship and control over the writing process, even when using GenAI's assistance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12015v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hilda Hadan, Derrick Wang, Reza Hadi Mogavi, Joseph Tu, Leah Zhang-Kennedy, Lennart E. Nacke</dc:creator>
    </item>
    <item>
      <title>The Art of Saying No: Contextual Noncompliance in Language Models</title>
      <link>https://arxiv.org/abs/2407.12043</link>
      <description>arXiv:2407.12043v1 Announce Type: cross 
Abstract: Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of "unsafe" queries, we posit that the scope of noncompliance should be broadened. We introduce a comprehensive taxonomy of contextual noncompliance describing when and how models should not comply with user requests. Our taxonomy spans a wide range of categories including incomplete, unsupported, indeterminate, and humanizing requests (in addition to unsafe requests). To test noncompliance capabilities of language models, we use this taxonomy to develop a new evaluation suite of 1000 noncompliance prompts. We find that most existing models show significantly high compliance rates in certain previously understudied categories with models like GPT-4 incorrectly complying with as many as 30% of requests. To address these gaps, we explore different training strategies using a synthetically-generated training set of requests and expected noncompliant responses. Our experiments demonstrate that while direct finetuning of instruction-tuned models can lead to both over-refusal and a decline in general capabilities, using parameter efficient methods like low rank adapters helps to strike a good balance between appropriate noncompliance and other capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12043v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faeze Brahman, Sachin Kumar, Vidhisha Balachandran, Pradeep Dasigi, Valentina Pyatkin, Abhilasha Ravichander, Sarah Wiegreffe, Nouha Dziri, Khyathi Chandu, Jack Hessel, Yulia Tsvetkov, Noah A. Smith, Yejin Choi, Hannaneh Hajishirzi</dc:creator>
    </item>
    <item>
      <title>AeroHaptix: A Wearable Vibrotactile Feedback System for Enhancing Collision Avoidance in UAV Teleoperation</title>
      <link>https://arxiv.org/abs/2407.12105</link>
      <description>arXiv:2407.12105v1 Announce Type: cross 
Abstract: Haptic feedback enhances collision avoidance by providing directional obstacle information to operators in unmanned aerial vehicle (UAV) teleoperation. However, such feedback is often rendered via haptic joysticks, which are unfamiliar to UAV operators and limited to single-directional force feedback. Additionally, the direct coupling of the input device and the feedback method diminishes the operators' control authority and causes oscillatory movements. To overcome these limitations, we propose AeroHaptix, a wearable haptic feedback system that uses high-resolution vibrations to communicate multiple obstacle directions simultaneously. The vibrotactile actuators' layout was optimized based on a perceptual study to eliminate perceptual biases and achieve uniform spatial coverage. A novel rendering algorithm, MultiCBF, was adapted from control barrier functions to support multi-directional feedback. System evaluation showed that AeroHaptix effectively reduced collisions in complex environment, and operators reported significantly lower physical workload, improved situational awareness, and increased control authority.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12105v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bingjian Huang, Zhecheng Wang, Qilong Cheng, Siyi Ren, Hanfeng Cai, Antonio Alvarez Valdivia, Karthik Mahadevan, Daniel Wigdor</dc:creator>
    </item>
    <item>
      <title>ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via Modal Fusion Map</title>
      <link>https://arxiv.org/abs/2407.12315</link>
      <description>arXiv:2407.12315v1 Announce Type: cross 
Abstract: Multi-modal embeddings form the foundation for vision-language models, such as CLIP embeddings, the most widely used text-image embeddings. However, these embeddings are vulnerable to subtle misalignment of cross-modal features, resulting in decreased model performance and diminished generalization. To address this problem, we design ModalChorus, an interactive system for visual probing and alignment of multi-modal embeddings. ModalChorus primarily offers a two-stage process: 1) embedding probing with Modal Fusion Map (MFM), a novel parametric dimensionality reduction method that integrates both metric and nonmetric objectives to enhance modality fusion; and 2) embedding alignment that allows users to interactively articulate intentions for both point-set and set-set alignments. Quantitative and qualitative comparisons for CLIP embeddings with existing dimensionality reduction (e.g., t-SNE and MDS) and data fusion (e.g., data context map) methods demonstrate the advantages of MFM in showcasing cross-modal features over common vision-language datasets. Case studies reveal that ModalChorus can facilitate intuitive discovery of misalignment and efficient re-alignment in scenarios ranging from zero-shot classification to cross-modal retrieval and generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12315v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yilin Ye, Shishi Xiao, Xingchen Zeng, Wei Zeng</dc:creator>
    </item>
    <item>
      <title>Are Educational Escape Rooms More Effective Than Traditional Lectures for Teaching Software Engineering? A Randomized Controlled Trial</title>
      <link>https://arxiv.org/abs/2407.12355</link>
      <description>arXiv:2407.12355v1 Announce Type: cross 
Abstract: Contribution: This article analyzes the learning effectiveness of a virtual educational escape room for teaching software engineering and compares this activity with traditional teaching through a randomized controlled trial. Background: Educational escape rooms have been used across a wide variety of disciplines at all levels of education and they are becoming increasingly popular among teachers. Nevertheless, there is a clear general need for more robust empirical evidence on the learning effectiveness of these novel activities and, particularly, on their application in software engineering education. Research Questions: Is game-based learning using educational escape rooms more effective than traditional lectures for teaching software engineering? What are the perceptions of software engineering students toward game-based learning using educational escape rooms? Methodology: The study presented in this article is a randomized controlled trial with a pre-and post-test design that was completed by a total of 326 software engineering students. The 164 students belonging to the experimental group learned software modeling by playing an educational escape room whereas the 162 students belonging to the control group learned the same subject matter through a traditional lecture. Findings: The results of the randomized controlled trial show that the students who learned software modeling through the educational escape room had very positive perceptions toward this activity, significantly increased their knowledge, and outperformed those students who learned through a traditional lecture in terms of knowledge acquisition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12355v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TE.2024.3403913</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Education, 2024</arxiv:journal_reference>
      <dc:creator>Aldo Gordillo, Daniel L\'opez-Fern\'andez</dc:creator>
    </item>
    <item>
      <title>Evaluating graph-based explanations for AI-based recommender systems</title>
      <link>https://arxiv.org/abs/2407.12357</link>
      <description>arXiv:2407.12357v1 Announce Type: cross 
Abstract: Recent years have witnessed a rapid growth of recommender systems, providing suggestions in numerous applications with potentially high social impact, such as health or justice. Meanwhile, in Europe, the upcoming AI Act mentions \emph{transparency} as a requirement for critical AI systems in order to ``mitigate the risks to fundamental rights''. Post-hoc explanations seamlessly align with this goal and extensive literature on the subject produced several forms of such objects, graphs being one of them. Early studies in visualization demonstrated the graphs' ability to improve user understanding, positioning them as potentially ideal explanations. However, it remains unclear how graph-based explanations compare to other explanation designs. In this work, we aim to determine the effectiveness of graph-based explanations in improving users' perception of AI-based recommendations using a mixed-methods approach. We first conduct a qualitative study to collect users' requirements for graph explanations. We then run a larger quantitative study in which we evaluate the influence of various explanation designs, including enhanced graph-based ones, on aspects such as understanding, usability and curiosity toward the AI system. We find that users perceive graph-based explanations as more usable than designs involving feature importance. However, we also reveal that textual explanations lead to higher objective understanding than graph-based designs. Most importantly, we highlight the strong contrast between participants' expressed preferences for graph design and their actual ratings using it, which are lower compared to textual design. These findings imply that meeting stakeholders' expressed preferences might not alone guarantee ``good'' explanations. Therefore, crafting hybrid designs successfully balancing social expectations with downstream performance emerges as a significant challenge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12357v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Delarue, Astrid Bertrand, Tiphaine Viard</dc:creator>
    </item>
    <item>
      <title>Abstraction Alignment: Comparing Model and Human Conceptual Relationships</title>
      <link>https://arxiv.org/abs/2407.12543</link>
      <description>arXiv:2407.12543v1 Announce Type: cross 
Abstract: Abstraction -- the process of generalizing specific examples into broad reusable patterns -- is central to how people efficiently process and store information and apply their knowledge to new data. Promisingly, research has shown that ML models learn representations that span levels of abstraction, from specific concepts like "bolo tie" and "car tire" to more general concepts like "CEO" and "model". However, existing techniques analyze these representations in isolation, treating learned concepts as independent artifacts rather than an interconnected web of abstraction. As a result, although we can identify the concepts a model uses to produce its output, it is difficult to assess if it has learned a human-aligned abstraction of the concepts that will generalize to new data. To address this gap, we introduce abstraction alignment, a methodology to measure the agreement between a model's learned abstraction and the expected human abstraction. We quantify abstraction alignment by comparing model outputs against a human abstraction graph, such as linguistic relationships or medical disease hierarchies. In evaluation tasks interpreting image models, benchmarking language models, and analyzing medical datasets, abstraction alignment provides a deeper understanding of model behavior and dataset content, differentiating errors based on their agreement with human knowledge, expanding the verbosity of current model quality metrics, and revealing ways to improve existing human abstractions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12543v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angie Boggust, Hyemin Bang, Hendrik Strobelt, Arvind Satyanarayan</dc:creator>
    </item>
    <item>
      <title>GroundUp: Rapid Sketch-Based 3D City Massing</title>
      <link>https://arxiv.org/abs/2407.12739</link>
      <description>arXiv:2407.12739v1 Announce Type: cross 
Abstract: We propose GroundUp, the first sketch-based ideation tool for 3D city massing of urban areas. We focus on early-stage urban design, where sketching is a common tool and the design starts from balancing building volumes (masses) and open spaces. With Human-Centered AI in mind, we aim to help architects quickly revise their ideas by easily switching between 2D sketches and 3D models, allowing for smoother iteration and sharing of ideas. Inspired by feedback from architects and existing workflows, our system takes as a first input a user sketch of multiple buildings in a top-down view. The user then draws a perspective sketch of the envisioned site. Our method is designed to exploit the complementarity of information in the two sketches and allows users to quickly preview and adjust the inferred 3D shapes. Our model has two main components. First, we propose a novel sketch-to-depth prediction network for perspective sketches that exploits top-down sketch shapes. Second, we use depth cues derived from the perspective sketch as a condition to our diffusion model, which ultimately completes the geometry in a top-down view. Thus, our final 3D geometry is represented as a heightfield, allowing users to construct the city `from the ground up'.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12739v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Gizem Esra Unlu, Mohamed Sayed, Yulia Gryaditskaya, Gabriel Brostow</dc:creator>
    </item>
    <item>
      <title>Enabling Waypoint Generation for Collaborative Robots using LLMs and Mixed Reality</title>
      <link>https://arxiv.org/abs/2403.09308</link>
      <description>arXiv:2403.09308v2 Announce Type: replace 
Abstract: Programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot's physical constraints. We propose a framework that simplifies robot deployment by allowing direct communication using natural language. It uses large language models (LLM) for prompt processing, workspace understanding, and waypoint generation. It also employs Augmented Reality (AR) to provide visual feedback of the planned outcome. We showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot. Moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09308v2</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cathy Mengying Fang, Krzysztof Zieli\'nski, Pattie Maes, Joe Paradiso, Bruce Blumberg, Mikkel Baun Kj{\ae}rgaard</dc:creator>
    </item>
    <item>
      <title>Good Intentions, Risky Inventions: A Method for Assessing the Risks and Benefits of AI in Mobile and Wearable Uses</title>
      <link>https://arxiv.org/abs/2407.09322</link>
      <description>arXiv:2407.09322v2 Announce Type: replace 
Abstract: Integrating Artificial Intelligence (AI) into mobile and wearables offers numerous benefits at individual, societal, and environmental levels. Yet, it also spotlights concerns over emerging risks. Traditional assessments of risks and benefits have been sporadic, and often require costly expert analysis. We developed a semi-automatic method that leverages Large Language Models (LLMs) to identify AI uses in mobile and wearables, classify their risks based on the EU AI Act, and determine their benefits that align with globally recognized long-term sustainable development goals; a manual validation of our method by two experts in mobile and wearable technologies, a legal and compliance expert, and a cohort of nine individuals with legal backgrounds who were recruited from Prolific, confirmed its accuracy to be over 85\%. We uncovered that specific applications of mobile computing hold significant potential in improving well-being, safety, and social equality. However, these promising uses are linked to risks involving sensitive data, vulnerable groups, and automated decision-making. To avoid rejecting these risky yet impactful mobile and wearable uses, we propose a risk assessment checklist for the Mobile HCI community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09322v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Constantinides, Edyta Bogucka, Sanja Scepanovic, Daniele Quercia</dc:creator>
    </item>
    <item>
      <title>Walk along: An Experiment on Controlling the Mobile Robot 'Spot' with Voice and Gestures</title>
      <link>https://arxiv.org/abs/2407.11218</link>
      <description>arXiv:2407.11218v2 Announce Type: replace 
Abstract: Robots are becoming increasingly intelligent and can autonomously perform tasks such as navigating between locations. However, human oversight remains crucial. This study compared two hands-free methods for directing mobile robots: voice control and gesture control. These methods were tested with the human stationary and walking freely. We hypothesized that walking with the robot would lead to higher intuitiveness ratings and better task performance due to increased stimulus-response compatibility, assuming humans align themselves with the robot. In a 2x2 within-subject design, 218 participants guided the quadrupedal robot Spot using 90 degrees rotation and walk-forward commands. After each trial, participants rated the intuitiveness of the command mapping, while post-experiment interviews were used to gather the participants' preferences. Results showed that voice control combined with walking with Spot was the most favored and intuitive, while gesture control while standing caused confusion for left/right commands. Despite this, 29% of participants preferred gesture control, citing task engagement and visual congruence as reasons. An odometry-based analysis revealed that participants aligned behind Spot, particularly in the gesture control condition, when allowed to walk. In conclusion, voice control with walking produced the best outcomes. Improving physical ergonomics and adjusting gesture types could improve the effectiveness of gesture control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11218v2</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Renchi Zhang, Jesse van der Linden, Dimitra Dodou, Harleigh Seyffert, Yke Bauke Eisma, Joost C. F. de Winter</dc:creator>
    </item>
    <item>
      <title>VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots</title>
      <link>https://arxiv.org/abs/2404.04066</link>
      <description>arXiv:2404.04066v2 Announce Type: replace-cross 
Abstract: Physically assistive robots present an opportunity to significantly increase the well-being and independence of individuals with motor impairments or other forms of disability who are unable to complete activities of daily living. Speech interfaces, especially ones that utilize Large Language Models (LLMs), can enable individuals to effectively and naturally communicate high-level commands and nuanced preferences to robots. Frameworks for integrating LLMs as interfaces to robots for high level task planning and code generation have been proposed, but fail to incorporate human-centric considerations which are essential while developing assistive interfaces. In this work, we present a framework for incorporating LLMs as speech interfaces for physically assistive robots, constructed iteratively with 3 stages of testing involving a feeding robot, culminating in an evaluation with 11 older adults at an independent living facility. We use both quantitative and qualitative data from the final study to validate our framework and additionally provide design guidelines for using LLMs as speech interfaces for assistive robots. Videos and supporting files are located on our project website: https://sites.google.com/andrew.cmu.edu/voicepilot/</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04066v2</guid>
      <category>cs.RO</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3654777.3676401</arxiv:DOI>
      <dc:creator>Akhil Padmanabha, Jessie Yuan, Janavi Gupta, Zulekha Karachiwalla, Carmel Majidi, Henny Admoni, Zackory Erickson</dc:creator>
    </item>
    <item>
      <title>Beyond Words: On Large Language Models Actionability in Mission-Critical Risk Analysis</title>
      <link>https://arxiv.org/abs/2406.10273</link>
      <description>arXiv:2406.10273v4 Announce Type: replace-cross 
Abstract: Context. Risk analysis assesses potential risks in specific scenarios. Risk analysis principles are context-less; the same methodology can be applied to a risk connected to health and information technology security. Risk analysis requires a vast knowledge of national and international regulations and standards and is time and effort-intensive. A large language model can quickly summarize information in less time than a human and can be fine-tuned to specific tasks.
  Aim. Our empirical study aims to investigate the effectiveness of Retrieval-Augmented Generation and fine-tuned LLM in risk analysis. To our knowledge, no prior study has explored its capabilities in risk analysis.
  Method. We manually curated 193 unique scenarios leading to 1283 representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years. We compared the base GPT-3.5 and GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned counterparts. We employ two human experts as competitors of the models and three other human experts to review the models and the former human experts' analysis. The reviewers analyzed 5,000 scenario analyses.
  Results and Conclusions. Human experts demonstrated higher accuracy, but LLMs are quicker and more actionable. Moreover, our findings show that RAG-assisted LLMs have the lowest hallucination rates, effectively uncovering hidden risks and complementing human expertise. Thus, the choice of model depends on specific needs, with FTMs for accuracy, RAG for hidden risks discovery, and base models for comprehensiveness and actionability. Therefore, experts can leverage LLMs as an effective complementing companion in risk analysis within a condensed timeframe. They can also save costs by averting unnecessary expenses associated with implementing unwarranted countermeasures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10273v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Esposito, Francesco Palagiano, Valentina Lenarduzzi, Davide Taibi</dc:creator>
    </item>
  </channel>
</rss>
