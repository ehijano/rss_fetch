<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Jul 2024 04:03:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Revisiting Randomization with the Cube Method</title>
      <link>https://arxiv.org/abs/2407.13613</link>
      <description>arXiv:2407.13613v1 Announce Type: new 
Abstract: We propose a novel randomization approach for randomized controlled trials (RCTs), named the cube method. The cube method allows for the selection of balanced samples across various covariate types, ensuring consistent adherence to balance tests and, whence, substantial precision gains when estimating treatment effects. We establish several statistical properties for the population and sample average treatment effects (PATE and SATE, respectively) under randomization using the cube method. The relevance of the cube method is particularly striking when comparing the behavior of prevailing methods employed for treatment allocation when the number of covariates to balance is increasing. We formally derive and compare bounds of balancing adjustments depending on the number of units $n$ and the number of covariates $p$ and show that our randomization approach outperforms methods proposed in the literature when $p$ is large and $p/n$ tends to 0. We run simulation studies to illustrate the substantial gains from the cube method for a large set of covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13613v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laurent Davezies, Guillaume Hollard, Pedro Vergara Merino</dc:creator>
    </item>
    <item>
      <title>Forward Orthogonal Deviations GMM and the Absence of Large Sample Bias</title>
      <link>https://arxiv.org/abs/2212.14075</link>
      <description>arXiv:2212.14075v2 Announce Type: replace 
Abstract: It is well known that generalized method of moments (GMM) estimators of dynamic panel data regressions can have significant bias when the number of time periods ($T$) is not small compared to the number of cross-sectional units ($n$). The bias is attributed to the use of many instrumental variables. This paper shows that if the maximum number of instrumental variables used in a period increases with $T$ at a rate slower than $T^{1/2}$, then GMM estimators that exploit the forward orthogonal deviations (FOD) transformation do not have asymptotic bias, regardless of how fast $T$ increases relative to $n$. This conclusion is specific to using the FOD transformation. A similar conclusion does not necessarily apply when other transformations are used to remove fixed effects. Monte Carlo evidence illustrating the analytical results is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.14075v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert F. Phillips</dc:creator>
    </item>
    <item>
      <title>Identification with possibly invalid IVs</title>
      <link>https://arxiv.org/abs/2401.03990</link>
      <description>arXiv:2401.03990v3 Announce Type: replace 
Abstract: This paper proposes a novel identification strategy relying on quasi-instrumental variables (quasi-IVs). A quasi-IV is a relevant but possibly invalid IV because it is not exogenous or not excluded. We show that a variety of models with discrete or continuous endogenous treatment which are usually identified with an IV - quantile models with rank invariance, additive models with homogenous treatment effects, and local average treatment effect models - can be identified under the joint relevance of two complementary quasi-IVs instead. To achieve identification, we complement one excluded but possibly endogenous quasi-IV (e.g., "relevant proxies" such as lagged treatment choice) with one exogenous (conditional on the excluded quasi-IV) but possibly included quasi-IV (e.g., random assignment or exogenous market shocks). Our approach also holds if any of the two quasi-IVs turns out to be a valid IV. In practice, being able to address endogeneity with complementary quasi-IVs instead of IVs is convenient since there are many applications where quasi-IVs are more readily available. Difference-in-differences is a notable example: time is an exogenous quasi-IV while the group assignment acts as a complementary excluded quasi-IV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03990v3</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christophe Bruneel-Zupanc, Jad Beyhum</dc:creator>
    </item>
    <item>
      <title>Tensor Factor Model Estimation by Iterative Projection</title>
      <link>https://arxiv.org/abs/2006.02611</link>
      <description>arXiv:2006.02611v3 Announce Type: replace-cross 
Abstract: Tensor time series, which is a time series consisting of tensorial observations, has become ubiquitous. It typically exhibits high dimensionality. One approach for dimension reduction is to use a factor model structure, in a form similar to Tucker tensor decomposition, except that the time dimension is treated as a dynamic process with a time dependent structure. In this paper we introduce two approaches to estimate such a tensor factor model by using iterative orthogonal projections of the original tensor time series. These approaches extend the existing estimation procedures and improve the estimation accuracy and convergence rate significantly as proven in our theoretical investigation. Our algorithms are similar to the higher order orthogonal projection method for tensor decomposition, but with significant differences due to the need to unfold tensors in the iterations and the use of autocorrelation. Consequently, our analysis is significantly different from the existing ones. Computational and statistical lower bounds are derived to prove the optimality of the sample size requirement and convergence rate for the proposed methods. Simulation study is conducted to further illustrate the statistical properties of these estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.02611v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuefeng Han, Rong Chen, Dan Yang, Cun-Hui Zhang</dc:creator>
    </item>
    <item>
      <title>Latent Gaussian dynamic factor modeling and forecasting for multivariate count time series</title>
      <link>https://arxiv.org/abs/2307.10454</link>
      <description>arXiv:2307.10454v2 Announce Type: replace-cross 
Abstract: This work considers estimation and forecasting in a multivariate, possibly high-dimensional count time series model constructed from a transformation of a latent Gaussian dynamic factor series. The estimation of the latent model parameters is based on second-order properties of the count and underlying Gaussian time series, yielding estimators of the underlying covariance matrices for which standard principal component analysis applies. Theoretical consistency results are established for the proposed estimation, building on certain concentration results for the models of the type considered. They also involve the memory of the latent Gaussian process, quantified through a spectral gap, shown to be suitably bounded as the model dimension increases, which is of independent interest. In addition, novel cross-validation schemes are suggested for model selection. The forecasting is carried out through a particle-based sequential Monte Carlo, leveraging Kalman filtering techniques. A simulation study and an application are also considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10454v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younghoon Kim, Marie-Christine D\"uker, Zachary F. Fisher, Vladas Pipiras</dc:creator>
    </item>
  </channel>
</rss>
