<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Mar 2025 01:58:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Retrieval Augmented Generation with Multi-Modal LLM Framework for Wireless Environments</title>
      <link>https://arxiv.org/abs/2503.07670</link>
      <description>arXiv:2503.07670v1 Announce Type: new 
Abstract: Future wireless networks aim to deliver high data rates and lower power consumption while ensuring seamless connectivity, necessitating robust optimization. Large language models (LLMs) have been deployed for generalized optimization scenarios. To take advantage of generative AI (GAI) models, we propose retrieval augmented generation (RAG) for multi-sensor wireless environment perception. Utilizing domain-specific prompt engineering, we apply RAG to efficiently harness multimodal data inputs from sensors in a wireless environment. Key pre-processing pipelines including image-to-text conversion, object detection, and distance calculations for multimodal RAG input from multi-sensor data are proposed to obtain a unified vector database crucial for optimizing LLMs in global wireless tasks. Our evaluation, conducted with OpenAI's GPT and Google's Gemini models, demonstrates an 8%, 8%, 10%, 7%, and 12% improvement in relevancy, faithfulness, completeness, similarity, and accuracy, respectively, compared to conventional LLM-based designs. Furthermore, our RAG-based LLM framework with vectorized databases is computationally efficient, providing real-time convergence under latency constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07670v1</guid>
      <category>cs.NI</category>
      <category>eess.IV</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Ahmed Mohsin, Ahsan Bilal, Sagnik Bhattacharya, John M. Cioffi</dc:creator>
    </item>
    <item>
      <title>Learning-Based Traffic Classification for Mixed-Critical Flows in Time-Sensitive Networking</title>
      <link>https://arxiv.org/abs/2503.07893</link>
      <description>arXiv:2503.07893v2 Announce Type: new 
Abstract: Time-Sensitive Networking (TSN) supports multiple traffic types with diverse timing requirements, such as hard real-time (HRT), soft real-time (SRT), and Best Effort (BE) within a single network. To provide varying Quality of Service (QoS) for these traffic types, TSN incorporates different scheduling and shaping mechanisms. However, assigning traffic types to the proper scheduler or shaper, known as Traffic-Type Assignment (TTA), is a known NP-hard problem. Relying solely on domain expertise to make these design decisions can be inefficient, especially in complex network scenarios. In this paper, we present a proof-of-concept highlighting the advantages of a learning-based approach to the TTA problem. We formulate an optimization model for TTA in TSN and develop a Proximal Policy Optimization (PPO) based Deep Reinforcement Learning (DRL) model, called ``TTASelector'', to assign traffic types to TSN flows efficiently. Using synthetic and realistic test cases, our evaluation shows that TTASelector assigns a higher number of traffic types to HRT and SRT flows compared to the state-of-the-art Tabu Search-based metaheuristic method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07893v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rubi Debnath, Luxi Zhao, Sebastian Steinhorst</dc:creator>
    </item>
    <item>
      <title>Accelerating Development in UAV Network Digital Twins with a Flexible Simulation Framework</title>
      <link>https://arxiv.org/abs/2503.07935</link>
      <description>arXiv:2503.07935v1 Announce Type: new 
Abstract: Unmanned aerial vehicles (UAVs) enhance coverage and provide flexible deployment in 5G and next-generation wireless networks. The performance of such wireless networks can be improved by developing new navigation and wireless adaptation approaches in digital twins (DTs). However, challenges such as complex propagation conditions and hardware complexities in real-world scenarios introduce a realism gap with the DTs. Moreover, while using real-time full-stack protocols in DTs enables subsequent deployment and testing in a real-world environment, development in DTs requires high computational complexity and involves a long development time. In this paper, to accelerate the development cycle, we develop a measurement-calibrated Matlab-based simulation framework to replicate performance in a full-stack UAV wireless network DT. In particular, we use the DT from the NSF AERPAW platform and compare its reports with those generated by our developed simulation framework in wireless networks with similar settings. In both environments, we observe comparable results in terms of RSRP measurement, hence motivating iterative use of the developed simulation environment with the DT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07935v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Sharif Hossen, Anil Gurses, Mihail Sichitiu, Ismail Guvenc</dc:creator>
    </item>
    <item>
      <title>ALCS: An Adaptive Latency Compensation Scheduler for Multipath TCP in Satellite-Terrestrial Integrated Networks</title>
      <link>https://arxiv.org/abs/2503.07973</link>
      <description>arXiv:2503.07973v1 Announce Type: new 
Abstract: The Satellite-Terrestrial Integrated Network (STIN) enhances end-to-end transmission by simultaneously utilizing terrestrial and satellite networks, offering significant benefits in scenarios like emergency response and cross-continental communication. Low Earth Orbit (LEO) satellite networks offer reduced Round Trip Time (RTT) for long-distance data transmission and serve as a crucial backup during terrestrial network failures. Meanwhile, terrestrial networks are characterized by ample bandwidth resources and generally more stable link conditions. Therefore, integrating Multipath TCP (MPTCP) into STIN is vital for optimizing resource utilization and ensuring efficient data transfer by exploiting the complementary strengths of both networks. However, the inherent challenges of STIN, such as heterogeneity, instability, and handovers, pose difficulties for traditional multipath schedulers, which are typically designed for terrestrial networks. We propose a novel multipath data scheduling approach for STIN, Adaptive Latency Compensation Scheduler (ALCS), to address these issues. ALCS refines transmission latency estimates by incorporating RTT, congestion window size, inflight and queuing packets, and satellite trajectory information. It further employs adaptive mechanisms for latency compensation and proactive handover management. It further employs adaptive mechanisms for latency compensation and proactive handover management. Implemented in the MPTCP Linux Kernel and evaluated in a simulated STIN testbed, ALCS outperforms existing multipath schedulers, delivering faster data transmission and achieving throughput gains of 9.8% to 44.0% compared to benchmark algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07973v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Wang, Ze Wang, Zeyi Deng, Jingjing Zhang, Yue Gao</dc:creator>
    </item>
    <item>
      <title>Interference Graph Estimation for Resource Allocation in Multi-Cell Multi-Numerology Networks: A Power-Domain Approach</title>
      <link>https://arxiv.org/abs/2503.08082</link>
      <description>arXiv:2503.08082v1 Announce Type: new 
Abstract: The interference graph, depicting the intra- and inter-cell interference channel gains, is indispensable for resource allocation in multi-cell networks.However, there lacks viable methods of interference graph estimation (IGE) for multi-cell multi-numerology (MN) networks. To fill this gap, we propose an efficient power-domain approach to IGE for the resource allocation in multi-cell MN networks. Unlike traditional reference signal-based approaches that consume frequency-time resources, our approach uses power as a new dimension for the estimation of channel gains. By carefully controlling the transmit powers of base stations, our approach is capable of estimating both intra- and inter-cell interference channel gains. As a power-domain approach, it can be seamlessly integrated with the resource allocation such that IGE and resource allocation can be conducted simultaneously using the same frequency-time resources. We derive the necessary conditions for the power-domain IGE and design a practical power control scheme. We formulate a multi-objective joint optimization problem of IGE and resource allocation, propose iterative solutions with proven convergence, and analyze the computational complexity. Our simulation results show that power-domain IGE can accurately estimate strong interference channel gains with low power overhead and is robust to carrier frequency and timing offsets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08082v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daqian Ding, Haorui Li, Yibo Pi, Xudong Wang</dc:creator>
    </item>
    <item>
      <title>LLM4MAC: An LLM-Driven Reinforcement Learning Framework for MAC Protocol Emergence</title>
      <link>https://arxiv.org/abs/2503.08123</link>
      <description>arXiv:2503.08123v1 Announce Type: new 
Abstract: With the advent of 6G systems, emerging hyper-connected ecosystems necessitate agile and adaptive medium access control (MAC) protocols to contend with network dynamics and diverse service requirements. We propose LLM4MAC, a novel framework that harnesses large language models (LLMs) within a reinforcement learning paradigm to drive MAC protocol emergence. By reformulating uplink data transmission scheduling as a semantics-generalized partially observable Markov game (POMG), LLM4MAC encodes network operations in natural language, while proximal policy optimization (PPO) ensures continuous alignment with the evolving network dynamics. A structured identity embedding (SIE) mechanism further enables robust coordination among heterogeneous agents. Extensive simulations demonstrate that on top of a compact LLM, which is purposefully selected to balance performance with resource efficiency, the protocol emerging from LLM4MAC outperforms comparative baselines in throughput and generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08123v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Renxuan Tan, Rongpeng Li, Zhifeng Zhao</dc:creator>
    </item>
    <item>
      <title>Explainable Autoencoder Design for RSSI-Based Multi-User Beam Probing and Hybrid Precoding</title>
      <link>https://arxiv.org/abs/2503.08267</link>
      <description>arXiv:2503.08267v1 Announce Type: new 
Abstract: This paper introduces a novel neural network (NN) structure referred to as an ``Auto-hybrid precoder'' (Auto-HP) and an unsupervised deep learning (DL) approach that jointly designs \ac{mmWave} probing beams and hybrid precoding matrix design for mmWave multi-user communication system with minimal training pilots. Our learning-based model capitalizes on prior channel observations to achieve two primary goals: designing a limited set of probing beams and predicting off-grid \ac{RF} beamforming vectors. The Auto-HP framework optimizes the probing beams in an unsupervised manner, concentrating the sensing power on the most promising spatial directions based on the surrounding environment. This is achieved through an innovative neural network architecture that respects \ac{RF} chain constraints and models received signal strength power measurements using complex-valued convolutional layers. Then, the autoencoder is trained to directly produce RF beamforming vectors for hybrid architectures, unconstrained by a predefined codebook, based on few projected received signal strength indicators (RSSIs). Finally, once the RF beamforming vectors for the multi-users are predicted, the baseband (BB) digital precoders are designed accounting for the multi-user interference. The Auto-HP neural network is trained end-to-end (E2E) in an unsupervised learning manner with a customized loss function that aims to maximizes the received signal strength. The adequacy of the Auto-HP NN's bottleneck layer dimension is evaluated from an information theory perspective, ensuring maximum data compression and reliable RF beam predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08267v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asmaa Abdallah, Abdulkadir Celik, Ahmed Alkhateeb, Ahmed M. Eltawil</dc:creator>
    </item>
    <item>
      <title>Towards Sustainability in 6G and beyond: Challenges and Opportunities of Open RAN</title>
      <link>https://arxiv.org/abs/2503.08353</link>
      <description>arXiv:2503.08353v1 Announce Type: new 
Abstract: The transition to 6G is expected to bring significant advancements, including much higher data rates, enhanced reliability and ultra-low latency compared to previous generations. Although 6G is anticipated to be 100 times more energy efficient, this increased efficiency does not necessarily mean reduced energy consumption or enhanced sustainability. Network sustainability encompasses a broader scope, integrating business viability, environmental sustainability, and social responsibility. This paper explores the sustainability requirements for 6G and proposes Open RAN as a key architectural solution. By enabling network diversification, fostering open and continuous innovation, and integrating AI/ML, Open RAN can promote sustainability in 6G. The paper identifies high energy consumption and e-waste generation as critical sustainability challenges and discusses how Open RAN can address these issues through softwarisation, edge computing, and AI integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08353v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamed Ahmadi, Mostafa Rahmani, Swarna Bindu Chetty, Eirini Eleni Tsiropoulou, Huseyin Arslan, Merouane Debbah, Tony Quek</dc:creator>
    </item>
    <item>
      <title>Efficient Resource Allocation in 5G Massive MIMO-NOMA Networks: Comparative Analysis of SINR-Aware Power Allocation and Spatial Correlation-Based Clustering</title>
      <link>https://arxiv.org/abs/2503.08466</link>
      <description>arXiv:2503.08466v1 Announce Type: new 
Abstract: With the evolution of 5G networks, optimizing resource allocation has become crucial to meeting the increasing demand for massive connectivity and high throughput. Combining Non-Orthogonal Multiple Access (NOMA) and massive Multi-Input Multi-Output (MIMO) enhances spectral efficiency, power efficiency, and device connectivity. However, deploying MIMO-NOMA in dense networks poses challenges in managing interference and optimizing power allocation while ensuring that the Signal-to-Interference-plus-Noise Ratio (SINR) meets required thresholds. Unlike previous studies that analyze user clustering and power allocation techniques under simplified assumptions, this work provides a comparative evaluation of multiple clustering and allocation strategies under identical spatially correlated network conditions. We focus on maximizing the number of served users under a given Quality of Service (QoS) constraint rather than the conventional sum-rate maximization approach. Additionally, we consider spatial correlation in user grouping, a factor often overlooked despite its importance in mitigating intra-cluster interference. We evaluate clustering algorithms, including user pairing, random clustering, Correlation Iterative Clustering Algorithm (CIA), K-means++-based User Clustering (KUC), and Grey Wolf Optimizer-based clustering (GWO), in a downlink spatially correlated MIMO-NOMA environment. Numerical results demonstrate that the GWO-based clustering algorithm achieves superior energy efficiency while maintaining scalability, whereas CIA effectively maximizes the number of served users. These findings provide valuable insights for designing MIMO-NOMA systems that optimize resource allocation in next-generation wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08466v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samar Chebbi, Oussama Habachi, Jean-Pierre Cances, Vahid Meghdadi Essaid Sabir</dc:creator>
    </item>
    <item>
      <title>Hierarchical Multi Agent DRL for Soft Handovers Between Edge Clouds in Open RAN</title>
      <link>https://arxiv.org/abs/2503.08493</link>
      <description>arXiv:2503.08493v1 Announce Type: new 
Abstract: Multi-connectivity (MC) for aerial users via a set of ground access points offers the potential for highly reliable communication. Within an open radio access network (O-RAN) architecture, edge clouds (ECs) enable MC with low latency for users within their coverage area. However, ensuring seamless service continuity for transitional users-those moving between the coverage areas of neighboring ECs-poses challenges due to centralized processing demands. To address this, we formulate a problem facilitating soft handovers between ECs, ensuring seamless transitions while maintaining service continuity for all users. We propose a hierarchical multi-agent reinforcement learning (HMARL) algorithm to dynamically determine the optimal functional split configuration for transitional and non-transitional users. Simulation results show that the proposed approach outperforms the conventional functional split in terms of the percentage of users maintaining service continuity, with at most 4% optimality gap. Additionally, HMARL achieves better scalability compared to the static baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08493v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F. Giarr\`e, I. A. Meer, M. Masoudi, M. Ozger, C. Cavdar</dc:creator>
    </item>
    <item>
      <title>MAREA: A Delay-Aware Multi-time-Scale Radio Resource Orchestrator for 6G O-RAN</title>
      <link>https://arxiv.org/abs/2503.08599</link>
      <description>arXiv:2503.08599v1 Announce Type: new 
Abstract: The Open Radio Access Network (O-RAN)-compliant solutions often lack crucial details for implementing effective control loops at various time scales. To overcome this, we introduce MAREA, an O-RAN-compliant mathematical framework designed for the allocation of radio resources to multiple ultra-Reliable Low Latency Communication (uRLLC) services. In the near-real-time (RT) control loop, MAREA employs a novel Martingales-based model to determine the guaranteed radio resources for each uRLLC service. Unlike traditional queueing theory approaches, this model ensures that the probability of packet transmission delays exceeding a predefined threshold -- the violation probability -- remains below a target tolerance.
  Additionally, MAREA uses a real-time control loop to monitor transmission queues and dynamically adjust guaranteed radio resources in response to traffic anomalies. To the best of our knowledge, MAREA is the first O-RAN-compliant solution that leverages Martingales for both near-RT and RT control loops. Simulations demonstrate that MAREA significantly outperforms reference solutions, achieving an average violation probability that is x10 lower.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08599v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oscar Adamuz-Hinojosa, Lanfranco Zanzi, Vincenzo Sciancalepore, Xavier Costa-P\'erez</dc:creator>
    </item>
    <item>
      <title>Cost-driven prunings for iterative solving of constrained routing problem with SRLG-disjoint protection</title>
      <link>https://arxiv.org/abs/2503.08262</link>
      <description>arXiv:2503.08262v1 Announce Type: cross 
Abstract: The search for the optimal pair of active and protection paths in a network with Shared Risk Link Groups (SRLG) is a challenging but high-value problem in the industry that is inevitable in ensuring reliable connections on the modern Internet. We propose a new approach to solving this problem, with a novel use of statistical analysis of the distribution of paths with respect to their cost, which is an integral part of our innovation. The key idea in our algorithm is to employ iterative updates of cost bounds, allowing efficient pruning of suboptimal paths. This idea drives an efficacious exploration of the search space. We benchmark our algorithms against the state-of-the-art algorithms that exploit the alternative strategy of conflicting links exclusion, showing that our approach has the advantage of finding more feasible connections within a set time limit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08262v1</guid>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P. A. Mosharev, Choon-Meng Lee, Xu Shu, Xiaoshan Zhang, Man-Hong Yung</dc:creator>
    </item>
    <item>
      <title>A systematic literature review of unsupervised learning algorithms for anomalous traffic detection based on flows</title>
      <link>https://arxiv.org/abs/2503.08293</link>
      <description>arXiv:2503.08293v1 Announce Type: cross 
Abstract: The constant increase of devices connected to the Internet, and therefore of cyber-attacks, makes it necessary to analyze network traffic in order to recognize malicious activity. Traditional packet-based analysis methods are insufficient because in large networks the amount of traffic is so high that it is unfeasible to review all communications. For this reason, flows is a suitable approach for this situation, which in future 5G networks will have to be used, as the number of packets will increase dramatically. If this is also combined with unsupervised learning models, it can detect new threats for which it has not been trained. This paper presents a systematic review of the literature on unsupervised learning algorithms for detecting anomalies in network flows, following the PRISMA guideline. A total of 63 scientific articles have been reviewed, analyzing 13 of them in depth. The results obtained show that autoencoder is the most used option, followed by SVM, ALAD, or SOM. On the other hand, all the datasets used for anomaly detection have been collected, including some specialised in IoT or with real data collected from honeypots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08293v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alberto Miguel-Diez, Adri\'an Campazas-Vega, Claudia \'Alvarez-Aparicio, Gonzalo Esteban-Costales, \'Angel Manuel Guerrero-Higueras</dc:creator>
    </item>
    <item>
      <title>Uni-Gaussians: Unifying Camera and Lidar Simulation with Gaussians for Dynamic Driving Scenarios</title>
      <link>https://arxiv.org/abs/2503.08317</link>
      <description>arXiv:2503.08317v1 Announce Type: cross 
Abstract: Ensuring the safety of autonomous vehicles necessitates comprehensive simulation of multi-sensor data, encompassing inputs from both cameras and LiDAR sensors, across various dynamic driving scenarios. Neural rendering techniques, which utilize collected raw sensor data to simulate these dynamic environments, have emerged as a leading methodology. While NeRF-based approaches can uniformly represent scenes for rendering data from both camera and LiDAR, they are hindered by slow rendering speeds due to dense sampling. Conversely, Gaussian Splatting-based methods employ Gaussian primitives for scene representation and achieve rapid rendering through rasterization. However, these rasterization-based techniques struggle to accurately model non-linear optical sensors. This limitation restricts their applicability to sensors beyond pinhole cameras. To address these challenges and enable unified representation of dynamic driving scenarios using Gaussian primitives, this study proposes a novel hybrid approach. Our method utilizes rasterization for rendering image data while employing Gaussian ray-tracing for LiDAR data rendering. Experimental results on public datasets demonstrate that our approach outperforms current state-of-the-art methods. This work presents a unified and efficient solution for realistic simulation of camera and LiDAR data in autonomous driving scenarios using Gaussian primitives, offering significant advancements in both rendering quality and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08317v1</guid>
      <category>cs.RO</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zikang Yuan, Yuechuan Pu, Hongcheng Luo, Fengtian Lang, Cheng Chi, Teng Li, Yingying Shen, Haiyang Sun, Bing Wang, Xin Yang</dc:creator>
    </item>
    <item>
      <title>Integrating Captive Portal Technology into Computer Science Education: A Modular, Hands-On Approach to Infrastructure</title>
      <link>https://arxiv.org/abs/2503.08426</link>
      <description>arXiv:2503.08426v1 Announce Type: cross 
Abstract: In this paper, we present an educational project aimed to introduce students to the technology behind Captive Portals infrastructures. For doing this, we developed a series of modules to emphasize each of the different aspects and features of this technology. The project is based on an open source implementation which is widely used in many computer network courses, making it well-suited and very appealing for instructors and practitioners in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08426v1</guid>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.22369/issn.2153-4136/16/1/8</arxiv:DOI>
      <arxiv:journal_reference>Journal of Computational Science Education, Volume 16, Issue 1 (March 2025), pp.35-42</arxiv:journal_reference>
      <dc:creator>Lianting Wang, Marcelo Ponce</dc:creator>
    </item>
    <item>
      <title>Efficient Neuro-enhanced Video Analytics</title>
      <link>https://arxiv.org/abs/2407.16990</link>
      <description>arXiv:2407.16990v4 Announce Type: replace 
Abstract: Video analytics is widespread in various applications serving our society. Recent advances of content enhancement in video analytics offer significant benefits for the bandwidth saving and accuracy improvement. However, existing content-enhanced video analytics systems are excessively computationally expensive and provide extremely low throughput. In this paper, we present region-based content enhancement, that enhances only the important regions in videos, to improve analytical accuracy. Our system, RegenHance, enables high-accuracy and high-throughput video analytics at the edge by 1) a macroblock-based region importance predictor that identifies the important regions fast and precisely, 2) a region-aware enhancer that stitches sparsely distributed regions into dense tensors and enhances them efficiently, and 3) a profile-based execution planer that allocates appropriate resources for enhancement and analytics components. We prototype RegenHance on five heterogeneous edge devices. Experiments on two analytical tasks reveal that region-based enhancement improves the overall accuracy of 10-19% and achieves 2-3x throughput compared to the state-of-the-art frame-based enhancement methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16990v4</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weijun Wang, Liang Mi, Shaowei Cen, Haipeng Dai, Yuanchun Li, Xiaoming Fu, Yunxin Liu</dc:creator>
    </item>
    <item>
      <title>Mobility-Aware Decentralized Federated Learning with Joint Optimization of Local Iteration and Leader Selection for Vehicular Networks</title>
      <link>https://arxiv.org/abs/2503.06443</link>
      <description>arXiv:2503.06443v2 Announce Type: replace 
Abstract: Federated learning (FL) emerges as a promising approach to empower vehicular networks, composed by intelligent connected vehicles equipped with advanced sensing, computing, and communication capabilities. While previous studies have explored the application of FL in vehicular networks, they have largely overlooked the intricate challenges arising from the mobility of vehicles and resource constraints. In this paper, we propose a framework of mobility-aware decentralized federated learning (MDFL) for vehicular networks. In this framework, nearby vehicles train an FL model collaboratively, yet in a decentralized manner. We formulate a local iteration and leader selection joint optimization problem (LSOP) to improve the training efficiency of MDFL. For problem solving, we first reformulate LSOP as a decentralized partially observable Markov decision process (Dec-POMDP), and then develop an effective optimization algorithm based on multi-agent proximal policy optimization (MAPPO) to solve Dec-POMDP. Finally, we verify the performance of the proposed algorithm by comparing it with other algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06443v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongyu Chen, Tao Deng, Juncheng Jia, Siwei Feng, Di Yuan</dc:creator>
    </item>
    <item>
      <title>From Classification to Optimization: Slicing and Resource Management with TRACTOR</title>
      <link>https://arxiv.org/abs/2312.07896</link>
      <description>arXiv:2312.07896v2 Announce Type: replace-cross 
Abstract: 5G and beyond networks promise advancements in bandwidth, latency, and connectivity. The Open Radio Access Network (O-RAN) framework enhances flexibility through network slicing and closed-loop RAN control. Central to this evolution is integrating machine learning (ML) for dynamic network control. This paper presents a framework to optimize O-RAN operation. First, we build and share a robust O-RAN dataset from real-world traffic captured across diverse locations and mobility scenarios, replicated within a full-stack srsRAN-based O-RAN system using the Colosseum RF emulator. This dataset supports ML training and deployment. We then introduce a traffic classification approach leveraging various ML models, demonstrating rapid training, testing, and refinement to improve accuracy. With up to 99% offline accuracy and 92% online accuracy for specific slices, our framework adapts efficiently to different models and network conditions. Finally, we present a physical resource block (PRB) assignment optimization strategy using reinforcement learning to refine resource allocation. Our learned policy achieves a mean performance score (0.631), surpassing a manually configured expert policy (0.609) and a random baseline (0.588), demonstrating improved PRB utilization. More importantly, our approach exhibits lower variability, with the Coefficient of Variation (CV) reduced by up to an order of magnitude in three out of four cases, ensuring more consistent performance. Our contributions, including open-source tools and datasets, accelerate O-RAN and ML-driven network control research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07896v2</guid>
      <category>eess.SY</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Groen, Zixian Yang, Divyadharshini Muruganandham, Mauro Belgiovine, Lei Ying, Kaushik Chowdhury</dc:creator>
    </item>
  </channel>
</rss>
