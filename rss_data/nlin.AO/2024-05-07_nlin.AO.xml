<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>nlin.AO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/nlin.AO</link>
    <description>nlin.AO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/nlin.AO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 May 2024 04:02:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A simple theory for training response of deep neural networks</title>
      <link>https://arxiv.org/abs/2405.04074</link>
      <description>arXiv:2405.04074v1 Announce Type: cross 
Abstract: Deep neural networks give us a powerful method to model the training dataset's relationship between input and output. We can regard that as a complex adaptive system consisting of many artificial neurons that work as an adaptive memory as a whole. The network's behavior is training dynamics with a feedback loop from the evaluation of the loss function. We already know the training response can be constant or shows power law-like aging in some ideal situations. However, we still have gaps between those findings and other complex phenomena, like network fragility. To fill the gap, we introduce a very simple network and analyze it. We show the training response consists of some different factors based on training stages, activation functions, or training methods. In addition, we show feature space reduction as an effect of stochastic training dynamics, which can result in network fragility. Finally, we discuss some complex phenomena of deep networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04074v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>nlin.AO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenichi Nakazato</dc:creator>
    </item>
    <item>
      <title>Two-dimensional hydrodynamic simulation for synchronized oscillatory flows in two collapsible channels connected in parallel</title>
      <link>https://arxiv.org/abs/2311.17570</link>
      <description>arXiv:2311.17570v2 Announce Type: replace 
Abstract: We investigated self-sustained oscillation in a collapsible channel, in which a part of one rigid wall is replaced by a thin elastic wall, and synchronization phenomena in the two channels connected in parallel. We performed a two-dimensional hydrodynamic simulation in a pair of collapsible channels which merged into a single channel downstream. The stable synchronization modes depended on the distance between the deformable region and the merging point; only an in-phase mode was stable for the large distance, in-phase and antiphase modes were bistable for the middle distance, and again only an in-phase mode was stable for the small distance. An antiphase mode became stable through the subcritical pitchfork bifurcation by decreasing the distance. Further decreasing the distance, the antiphase mode became unstable through the subcritical Neimark-Sacker bifurcation. We also clarified the distance dependences of the amplitude and frequency for each stable synchronization mode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17570v2</guid>
      <category>nlin.AO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevE.109.054201</arxiv:DOI>
      <dc:creator>Yuki Araya, Hiroaki Ito, Hiroyuki Kitahata</dc:creator>
    </item>
  </channel>
</rss>
