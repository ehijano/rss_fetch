<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Oct 2024 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Wasserstein distance in terms of the comonotonicity Copula</title>
      <link>https://arxiv.org/abs/2410.19914</link>
      <description>arXiv:2410.19914v1 Announce Type: new 
Abstract: The aim of this article is to write the $p$-Wasserstein metric $W_p$ with the $p$-norm, $p\in [1,\infty)$, on $\R^d$ in terms of copula. In particular for the case of one-dimensional distributions, we get that the copula employed to get the optimal coupling of the Wasserstein distances is the comotonicity copula. We obtain the equivalent result also for $d$-dimensional distributions under the sufficient and necessary condition that these have the same dependence structure of their one-dimensional marginals, i.e that the $d$-dimensional distributions share the same copula. Assuming $p\neq q$, $p,q$ $\in [1,\infty)$ and that the probability measures $\mu$ and $\nu$ are sharing the same copula, we also analyze the Wasserstein distance $W_{p,q}$ discussed in \cite{Alfonsi} and get an upper and lower bounds of $W_{p,q}$ in terms of $W_p$, written in terms of comonotonicity copula. We show that as a consequence the lower and upper bound of $W_{p,q}$ can be written in terms of generalized inverse functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19914v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mariem Abdellatif, Peter Kuching, Barbara R\"udiger, Irene Ventura</dc:creator>
    </item>
    <item>
      <title>L\'evy graphical models</title>
      <link>https://arxiv.org/abs/2410.19952</link>
      <description>arXiv:2410.19952v1 Announce Type: new 
Abstract: Conditional independence and graphical models are crucial concepts for sparsity and statistical modeling in higher dimensions. For L\'evy processes, a widely applied class of stochastic processes, these notions have not been studied. By the L\'evy-It\^o decomposition, a multivariate L\'evy process can be decomposed into the sum of a Brownian motion part and an independent jump process. We show that conditional independence statements between the marginal processes can be studied separately for these two parts. While the Brownian part is well-understood, we derive a novel characterization of conditional independence between the sample paths of the jump process in terms of the L\'evy measure. We define L\'evy graphical models as L\'evy processes that satisfy undirected or directed Markov properties. We prove that the graph structure is invariant under changes of the univariate marginal processes. L\'evy graphical models allow the construction of flexible, sparse dependence models for L\'evy processes in large dimensions, which are interpretable thanks to the underlying graph. For trees, we develop statistical methodology to learn the underlying structure from low- or high-frequency observations of the L\'evy process and show consistent graph recovery. We apply our method to model stock returns from U.S. companies to illustrate the advantages of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19952v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Engelke, Jevgenijs Ivanovs, Jakob D. Th{\o}stesen</dc:creator>
    </item>
    <item>
      <title>Interaction Order Estimation in Tensor Curie-Weiss Models</title>
      <link>https://arxiv.org/abs/2410.20213</link>
      <description>arXiv:2410.20213v1 Announce Type: new 
Abstract: In this paper, we consider the problem of estimating the interaction parameter $p$ of a $p$-spin Curie-Weiss model at inverse temperature $\beta$, given a single observation from this model. We show, by a contiguity argument, that joint estimation of the parameters $\beta$ and $p$ is impossible, which implies that estimation of $p$ is impossible if $\beta$ is unknown. These impossibility results are also extended to the more general $p$-spin Erd\H{o}s-R\'enyi Ising model. The situation is more delicate when $\beta$ is known. In this case, we show that there exists an increasing threshold function $\beta^*(p)$, such that for all $\beta$, consistent estimation of $p$ is impossible when $\beta^*(p) &gt; \beta$, and for almost all $\beta$, consistent estimation of $p$ is possible for $\beta^*(p)&lt;\beta$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20213v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Somabha Mukherjee</dc:creator>
    </item>
    <item>
      <title>Centrality and topology properties in a tree-structured Markov random field</title>
      <link>https://arxiv.org/abs/2410.20240</link>
      <description>arXiv:2410.20240v1 Announce Type: new 
Abstract: The topology of the tree underlying a tree-structured Markov random field (MRF) is central to the understanding of its stochastic dynamics: it is, after all, what synthesizes the rich dependence relations within the MRF. In this paper, we shed light on the influence of the tree's topology, through an extensive comparison-based analysis, on the aggregate distribution of the MRF. This is done within the framework of a recently introduced family of tree-structured MRFs with the uncommon property of having fixed Poisson marginal distributions unaffected by the dependence scheme. We establish convex orderings of sums of MRFs encrypted on trees having different topologies, leading to the devising of a new poset of trees. Hasse diagrams, cataloguing trees of dimension up to 9, and methods for the comparison of higher-dimension trees are provided to offer an exhaustive investigation of the new poset. We also briefly discuss its relation to other existing posets of trees and to invariants from spectral graph theory. Such an analysis requires, beforehand, to study the joint distribution of a MRF's component and its sum, a random vector we refer to as a synecdochic pair. To assess if a component is less or more contributing than another to the sum, we employ stochastic orders to compare synecdochic pairs within a MRF. The resulting orderings are reflected through allocation-related quantities, which thus act as centrality indices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20240v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin C\^ot\'e, H\'el\`ene Cossette, Etienne Marceau</dc:creator>
    </item>
    <item>
      <title>Understanding the Effect of GCN Convolutions in Regression Tasks</title>
      <link>https://arxiv.org/abs/2410.20068</link>
      <description>arXiv:2410.20068v1 Announce Type: cross 
Abstract: Graph Convolutional Networks (GCNs) have become a pivotal method in machine learning for modeling functions over graphs. Despite their widespread success across various applications, their statistical properties (e.g. consistency, convergence rates) remain ill-characterized. To begin addressing this knowledge gap, in this paper, we provide a formal analysis of the impact of convolution operators on regression tasks over homophilic networks. Focusing on estimators based solely on neighborhood aggregation, we examine how two common convolutions - the original GCN and GraphSage convolutions - affect the learning error as a function of the neighborhood topology and the number of convolutional layers. We explicitly characterize the bias-variance trade-off incurred by GCNs as a function of the neighborhood size and identify specific graph topologies where convolution operators are less effective. Our theoretical findings are corroborated by synthetic experiments, and provide a start to a deeper quantitative understanding of convolutional effects in GCNs for offering rigorous guidelines for practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20068v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juntong Chen, Johannes Schmidt-Hieber, Claire Donnat, Olga Klopp</dc:creator>
    </item>
    <item>
      <title>Causal Modeling in Multi-Context Systems: Distinguishing Multiple Context-Specific Causal Graphs which Account for Observational Support</title>
      <link>https://arxiv.org/abs/2410.20405</link>
      <description>arXiv:2410.20405v1 Announce Type: cross 
Abstract: Causal structure learning with data from multiple contexts carries both opportunities and challenges. Opportunities arise from considering shared and context-specific causal graphs enabling to generalize and transfer causal knowledge across contexts. However, a challenge that is currently understudied in the literature is the impact of differing observational support between contexts on the identifiability of causal graphs. Here we study in detail recently introduced [6] causal graph objects that capture both causal mechanisms and data support, allowing for the analysis of a larger class of context-specific changes, characterizing distribution shifts more precisely. We thereby extend results on the identifiability of context-specific causal structures and propose a framework to model context-specific independence (CSI) within structural causal models (SCMs) in a refined way that allows to explore scenarios where these graph objects differ. We demonstrate how this framework can help explaining phenomena like anomalies or extreme events, where causal mechanisms change or appear to change under different conditions. Our results contribute to the theoretical foundations for understanding causal relations in multi-context systems, with implications for generalization, transfer learning, and anomaly detection. Future work may extend this approach to more complex data types, such as time-series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20405v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Martin Rabel, Wiebke G\"unther, Jakob Runge, Andreas Gerhardus</dc:creator>
    </item>
    <item>
      <title>A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data</title>
      <link>https://arxiv.org/abs/2410.20659</link>
      <description>arXiv:2410.20659v1 Announce Type: cross 
Abstract: Federated Learning (FL) has emerged as a groundbreaking paradigm in collaborative machine learning, emphasizing decentralized model training to address data privacy concerns. While significant progress has been made in optimizing federated learning, the exploration of generalization error, particularly in heterogeneous settings, has been limited, focusing mainly on parametric cases. This paper investigates the generalization properties of deep federated regression within a two-stage sampling model. Our findings highlight that the intrinsic dimension, defined by the entropic dimension, is crucial for determining convergence rates when appropriate network sizes are used. Specifically, if the true relationship between response and explanatory variables is charecterized by a $\beta$-H\"older function and there are $n$ independent and identically distributed (i.i.d.) samples from $m$ participating clients, the error rate for participating clients scales at most as $\tilde{O}\left((mn)^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))}\right)$, and for non-participating clients, it scales as $\tilde{O}\left(\Delta \cdot m^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))} + (mn)^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))}\right)$. Here, $\bar{d}_{2\beta}(\lambda)$ represents the $2\beta$-entropic dimension of $\lambda$, the marginal distribution of the explanatory variables, and $\Delta$ characterizes the dependence between the sampling stages. Our results explicitly account for the "closeness" of clients, demonstrating that the convergence rates of deep federated learners depend on intrinsic rather than nominal high-dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20659v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saptarshi Chakraborty, Peter L. Bartlett</dc:creator>
    </item>
    <item>
      <title>Statistical Inference in High-dimensional Poisson Regression with Applications to Mediation Analysis</title>
      <link>https://arxiv.org/abs/2410.20671</link>
      <description>arXiv:2410.20671v1 Announce Type: cross 
Abstract: Large-scale datasets with count outcome variables are widely present in various applications, and the Poisson regression model is among the most popular models for handling count outcomes. This paper considers the high-dimensional sparse Poisson regression model and proposes bias-corrected estimators for both linear and quadratic transformations of high-dimensional regression vectors. We establish the asymptotic normality of the estimators, construct asymptotically valid confidence intervals, and conduct related hypothesis testing. We apply the devised methodology to high-dimensional mediation analysis with count outcome, with particular application of testing for the existence of interaction between the treatment variable and high-dimensional mediators. We demonstrate the proposed methods through extensive simulation studies and application to real-world epigenetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20671v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prabrisha Rakshit, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>Almost goodness-of-fit tests</title>
      <link>https://arxiv.org/abs/2410.20918</link>
      <description>arXiv:2410.20918v1 Announce Type: cross 
Abstract: We introduce the almost goodness-of-fit test, a procedure to decide if a (parametric) model provides a good representation of the probability distribution generating the observed sample. We consider the approximate model determined by an M-estimator of the parameters as the best representative of the unknown distribution within the parametric class. The objective is the approximate validation of a distribution or an entire parametric family up to a pre-specified threshold value, the margin of error. The methodology also allows quantifying the percentage improvement of the proposed model compared to a non-informative (constant) one. The test statistic is the $\mathrm{L}^p$-distance between the empirical distribution function and the corresponding one of the estimated (parametric) model. The value of the parameter $p$ allows modulating the impact of the tails of the distribution in the validation of the model. By deriving the asymptotic distribution of the test statistic, as well as proving the consistency of its bootstrap approximation, we present an easy-to-implement and flexible method. The performance of the proposal is illustrated with a simulation study and the analysis of a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20918v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amparo Ba\'illo, Javier C\'arcamo</dc:creator>
    </item>
    <item>
      <title>A Stein Gradient Descent Approach for Doubly Intractable Distributions</title>
      <link>https://arxiv.org/abs/2410.21021</link>
      <description>arXiv:2410.21021v1 Announce Type: cross 
Abstract: Bayesian inference for doubly intractable distributions is challenging because they include intractable terms, which are functions of parameters of interest. Although several alternatives have been developed for such models, they are computationally intensive due to repeated auxiliary variable simulations. We propose a novel Monte Carlo Stein variational gradient descent (MC-SVGD) approach for inference for doubly intractable distributions. Through an efficient gradient approximation, our MC-SVGD approach rapidly transforms an arbitrary reference distribution to approximate the posterior distribution of interest, without necessitating any predefined variational distribution class for the posterior. Such a transport map is obtained by minimizing Kullback-Leibler divergence between the transformed and posterior distributions in a reproducing kernel Hilbert space (RKHS). We also investigate the convergence rate of the proposed method. We illustrate the application of the method to challenging examples, including a Potts model, an exponential random graph model, and a Conway--Maxwell--Poisson regression model. The proposed method achieves substantial computational gains over existing algorithms, while providing comparable inferential performance for the posterior distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21021v1</guid>
      <category>stat.ML</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heesang Lee, Songhee Kim, Bokgyeong Kang, Jaewoo Park</dc:creator>
    </item>
    <item>
      <title>SoS Certifiability of Subgaussian Distributions and its Algorithmic Applications</title>
      <link>https://arxiv.org/abs/2410.21194</link>
      <description>arXiv:2410.21194v1 Announce Type: cross 
Abstract: We prove that there is a universal constant $C&gt;0$ so that for every $d \in \mathbb N$, every centered subgaussian distribution $\mathcal D$ on $\mathbb R^d$, and every even $p \in \mathbb N$, the $d$-variate polynomial $(Cp)^{p/2} \cdot \|v\|_{2}^p - \mathbb E_{X \sim \mathcal D} \langle v,X\rangle^p$ is a sum of square polynomials. This establishes that every subgaussian distribution is \emph{SoS-certifiably subgaussian} -- a condition that yields efficient learning algorithms for a wide variety of high-dimensional statistical tasks. As a direct corollary, we obtain computationally efficient algorithms with near-optimal guarantees for the following tasks, when given samples from an arbitrary subgaussian distribution: robust mean estimation, list-decodable mean estimation, clustering mean-separated mixture models, robust covariance-aware mean estimation, robust covariance estimation, and robust linear regression. Our proof makes essential use of Talagrand's generic chaining/majorizing measures theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21194v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Samuel B. Hopkins, Ankit Pensia, Stefan Tiegel</dc:creator>
    </item>
    <item>
      <title>Adaptive Transfer Clustering: A Unified Framework</title>
      <link>https://arxiv.org/abs/2410.21263</link>
      <description>arXiv:2410.21263v1 Announce Type: cross 
Abstract: We propose a general transfer learning framework for clustering given a main dataset and an auxiliary one about the same subjects. The two datasets may reflect similar but different latent grouping structures of the subjects. We propose an adaptive transfer clustering (ATC) algorithm that automatically leverages the commonality in the presence of unknown discrepancy, by optimizing an estimated bias-variance decomposition. It applies to a broad class of statistical models including Gaussian mixture models, stochastic block models, and latent class models. A theoretical analysis proves the optimality of ATC under the Gaussian mixture model and explicitly quantifies the benefit of transfer. Extensive simulations and real data experiments confirm our method's effectiveness in various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21263v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuqi Gu, Zhongyuan Lyu, Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>Large sample behavior of the least trimmed squares estimator</title>
      <link>https://arxiv.org/abs/2210.06460</link>
      <description>arXiv:2210.06460v4 Announce Type: replace 
Abstract: The least trimmed squares (LTS) estimator is popular in location, regression, machine learning, and AI literature. Despite the empirical version of least trimmed squares (LTS) being repeatedly studied in the literature, the population version of the LTS has never been introduced and studied. The lack of the population version hinders the study of the large sample properties of the LTS utilizing the empirical process theory. Novel properties of the objective function in both empirical and population settings of the LTS and other properties are established for the first time in this article. The primary properties of the objective function facilitate the establishment of other original results, including the influence function and Fisher consistency. The strong consistency is established with the help of a generalized Glivenko-Cantelli Theorem over a class of functions for the first time. Differentiability and stochastic equicontinuity promote the establishment of asymptotic normality with a concise and novel approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.06460v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijun Zuo</dc:creator>
    </item>
    <item>
      <title>Theoretical guarantees for neural control variates in MCMC</title>
      <link>https://arxiv.org/abs/2304.01111</link>
      <description>arXiv:2304.01111v2 Announce Type: replace 
Abstract: In this paper, we propose a variance reduction approach for Markov chains based on additive control variates and the minimization of an appropriate estimate for the asymptotic variance. We focus on the particular case when control variates are represented as deep neural networks. We derive the optimal convergence rate of the asymptotic variance under various ergodicity assumptions on the underlying Markov chain. The proposed approach relies upon recent results on the stochastic errors of variance reduction algorithms and function approximation theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01111v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Denis Belomestny, Artur Goldman, Alexey Naumov, Sergey Samsonov</dc:creator>
    </item>
    <item>
      <title>Change point estimation for a stochastic heat equation</title>
      <link>https://arxiv.org/abs/2307.10960</link>
      <description>arXiv:2307.10960v2 Announce Type: replace 
Abstract: We study a change point model based on a stochastic partial differential equation (SPDE) corresponding to the heat equation governed by the weighted Laplacian $\Delta_\vartheta = \nabla\vartheta\nabla$, where $\vartheta=\vartheta(x)$ is a space-dependent diffusivity. As a basic problem the domain $(0,1)$ is considered with a piecewise constant diffusivity with a jump at an unknown point $\tau$. Based on local measurements of the solution in space with resolution $\delta$ over a finite time horizon, we construct a simultaneous M-estimator for the diffusivity values and the change point. The change point estimator converges at rate $\delta$, while the diffusivity constants can be recovered with convergence rate $\delta^{3/2}$. Moreover, when the diffusivity parameters are known and the jump height vanishes with the spatial resolution tending to zero, we derive a limit theorem for the change point estimator and identify the limiting distribution. For the mathematical analysis, a precise understanding of the SPDE with discontinuous $\vartheta$, tight concentration bounds for quadratic functionals in the solution, and a generalisation of classical M-estimators are developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10960v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Markus Rei{\ss}, Claudia Strauch, Lukas Trottner</dc:creator>
    </item>
    <item>
      <title>Dyson Equation for Correlated Linearizations and Test Error of Random Features Regression</title>
      <link>https://arxiv.org/abs/2312.09194</link>
      <description>arXiv:2312.09194v2 Announce Type: replace 
Abstract: This paper develops some theory of the Dyson equation for correlated linearizations and uses it to solve a problem on asymptotic deterministic equivalent for the test error in random features regression. The theory developed for the correlated Dyson equation includes existence-uniqueness, spectral support bounds, and stability properties. This theory is new for constructing deterministic equivalents for pseudo-resolvents of a class of correlated linear pencils. In the application, this theory is used to give a deterministic equivalent of the test error in random features ridge regression, in a proportional scaling regime, wherein we have conditioned on both training and test datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09194v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Latourelle-Vigeant, Elliot Paquette</dc:creator>
    </item>
    <item>
      <title>Matrix Denoising with Doubly Heteroscedastic Noise: Fundamental Limits and Optimal Spectral Methods</title>
      <link>https://arxiv.org/abs/2405.13912</link>
      <description>arXiv:2405.13912v2 Announce Type: replace 
Abstract: We study the matrix denoising problem of estimating the singular vectors of a rank-$1$ signal corrupted by noise with both column and row correlations. Existing works are either unable to pinpoint the exact asymptotic estimation error or, when they do so, the resulting approaches (e.g., based on whitening or singular value shrinkage) remain vastly suboptimal. On top of this, most of the literature has focused on the special case of estimating the left singular vector of the signal when the noise only possesses row correlation (one-sided heteroscedasticity). In contrast, our work establishes the information-theoretic and algorithmic limits of matrix denoising with doubly heteroscedastic noise. We characterize the exact asymptotic minimum mean square error, and design a novel spectral estimator with rigorous optimality guarantees: under a technical condition, it attains positive correlation with the signals whenever information-theoretically possible and, for one-sided heteroscedasticity, it also achieves the Bayes-optimal error. Numerical experiments demonstrate the significant advantage of our theoretically principled method with the state of the art. The proofs draw connections with statistical physics and approximate message passing, departing drastically from standard random matrix theory techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13912v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yihan Zhang, Marco Mondelli</dc:creator>
    </item>
    <item>
      <title>Bivariate dynamic conditional failure extropy</title>
      <link>https://arxiv.org/abs/2410.09882</link>
      <description>arXiv:2410.09882v5 Announce Type: replace 
Abstract: Nair and Sathar (2020) introduced a new metric for uncertainty known as dynamic failure extropy, focusing on the analysis of past lifetimes. In this study, we extend this concept to a bivariate context, exploring various properties associated with the proposed bivariate measure. We show that bivariate conditional failure extropy can uniquely determine the joint distribution function. Additionally, we derive characterizations for certain bivariate lifetime models using this measure. A new stochastic ordering, based on bivariate conditional failure extropy, is also proposed, along with some established bounds. We further develop an estimator for the bivariate conditional failure extropy using a smoothed kernel and empirical approach. The performance of the proposed estimator is evaluated through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09882v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aman Pandey, Chanchal Kundu</dc:creator>
    </item>
    <item>
      <title>A Short Note on the Efficiency of Markov Chains for Bayesian Linear Regression Models with Heavy-Tailed Errors</title>
      <link>https://arxiv.org/abs/2410.17070</link>
      <description>arXiv:2410.17070v3 Announce Type: replace 
Abstract: In this short note, we consider posterior simulation for a linear regression model when the error distribution is given by a scale mixture of multivariate normals. We first show that the sampler of Backlund and Hobert (2020) for the case of the conditionally conjugate normal-inverse Wishart prior continues to be geometrically ergodic even when the error density is heavier-tailed. Moreover, we prove that the ergodicity is uniform by verifying the minorization condition. In the second half of this note, we treat an improper case and show that the sampler of Section 4 of Roy and Hobert (2010) is geometrically ergodic under significantly milder conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17070v3</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Hamura</dc:creator>
    </item>
    <item>
      <title>A General Framework for Cutting Feedback within Modularised Bayesian Inference</title>
      <link>https://arxiv.org/abs/2211.03274</link>
      <description>arXiv:2211.03274v3 Announce Type: replace-cross 
Abstract: Standard Bayesian inference can build models that combine information from various sources, but this inference may not be reliable if components of a model are misspecified. Cut inference, as a particular type of modularized Bayesian inference, is an alternative which splits a model into modules and cuts the feedback from the suspect module. Previous studies have focused on a two-module case, but a more general definition of a "module" remains unclear. We present a formal definition of a "module" and discuss its properties. We formulate methods for identifying modules; determining the order of modules; and building the cut distribution that should be used for cut inference within an arbitrary directed acyclic graph structure. We justify the cut distribution by showing that it not only cuts the feedback but also is the best approximation satisfying this condition to the joint distribution in the Kullback-Leibler divergence. We also extend cut inference for the two-module case to a general multiple-module case via a sequential splitting technique and demonstrate this via illustrative applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.03274v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Liu, Robert J. B. Goudie</dc:creator>
    </item>
    <item>
      <title>Fused Extended Two-Way Fixed Effects for Difference-in-Differences With Staggered Adoptions</title>
      <link>https://arxiv.org/abs/2312.05985</link>
      <description>arXiv:2312.05985v3 Announce Type: replace-cross 
Abstract: To address the bias of the canonical two-way fixed effects estimator for difference-in-differences under staggered adoptions, Wooldridge (2021) proposed the extended two-way fixed effects estimator, which adds many parameters. However, this reduces efficiency. Restricting some of these parameters to be equal (for example, subsequent treatment effects within a cohort) helps, but ad hoc restrictions may reintroduce bias. We propose a machine learning estimator with a single tuning parameter, fused extended two-way fixed effects (FETWFE), that enables automatic data-driven selection of these restrictions. We prove that under an appropriate sparsity assumption FETWFE identifies the correct restrictions with probability tending to one, which improves efficiency. We also prove the consistency, oracle property, and asymptotic normality of FETWFE for several classes of heterogeneous marginal treatment effect estimators under either conditional or marginal parallel trends, and we prove the same results for conditional average treatment effects under conditional parallel trends. We demonstrate FETWFE in simulation studies and an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05985v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Faletto</dc:creator>
    </item>
    <item>
      <title>The ESPRIT algorithm under high noise: Optimal error scaling and noisy super-resolution</title>
      <link>https://arxiv.org/abs/2404.03885</link>
      <description>arXiv:2404.03885v3 Announce Type: replace-cross 
Abstract: Subspace-based signal processing techniques, such as the Estimation of Signal Parameters via Rotational Invariant Techniques (ESPRIT) algorithm, are popular methods for spectral estimation. These algorithms can achieve the so-called super-resolution scaling under low noise conditions, surpassing the well-known Nyquist limit. However, the performance of these algorithms under high-noise conditions is not as well understood. Existing state-of-the-art analysis indicates that ESPRIT and related algorithms can be resilient even for signals where each observation is corrupted by statistically independent, mean-zero noise of size $\mathcal{O}(1)$, but these analyses only show that the error $\epsilon$ decays at a slow rate $\epsilon=\mathcal{\tilde{O}}(n^{-1/2})$ with respect to the cutoff frequency $n$ (i.e., the maximum frequency of the measurements). In this work, we prove that under certain assumptions, the ESPRIT algorithm can attain a significantly improved error scaling $\epsilon = \mathcal{\tilde{O}}(n^{-3/2})$, exhibiting noisy super-resolution scaling beyond the Nyquist limit $\epsilon = \mathcal{O}(n^{-1})$ given by the Nyquist-Shannon sampling theorem. We further establish a theoretical lower bound and show that this scaling is optimal. Our analysis introduces novel matrix perturbation results, which could be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03885v3</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyan Ding, Ethan N. Epperly, Lin Lin, Ruizhe Zhang</dc:creator>
    </item>
    <item>
      <title>A Fast Coordinate Descent Method for High-Dimensional Non-Negative Least Squares using a Unified Sparse Regression Framework</title>
      <link>https://arxiv.org/abs/2410.03014</link>
      <description>arXiv:2410.03014v2 Announce Type: replace-cross 
Abstract: We develop theoretical results that establish a connection across various regression methods such as the non-negative least squares, bounded variable least squares, simplex constrained least squares, and lasso. In particular, we show in general that a polyhedron constrained least squares problem admits a locally unique sparse solution in high dimensions. We demonstrate the power of our result by concretely quantifying the sparsity level for the aforementioned methods. Furthermore, we propose a novel coordinate descent based solver for NNLS in high dimensions using our theoretical result as motivation. We show through simulated data and a real data example that our solver achieves at least a 5x speed-up from the state-of-the-art solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03014v2</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Yang, Trevor Hastie</dc:creator>
    </item>
    <item>
      <title>On the Lower Confidence Band for the Optimal Welfare</title>
      <link>https://arxiv.org/abs/2410.07443</link>
      <description>arXiv:2410.07443v2 Announce Type: replace-cross 
Abstract: This article addresses the question of reporting a lower confidence band (LCB) for optimal welfare in a policy learning problem. A straightforward procedure inverts a one-sided t-test based on an efficient estimator of the optimal welfare. We show that under empirically relevant data-generating processes, this procedure can be dominated by an LCB corresponding to suboptimal welfare, with the average difference of the order N-1/2. We relate the first-order dominance result to a lack of uniformity in the margin assumption, a standard sufficient condition for debiased inference on the optimal welfare ensuring that the first-best policy is well-separated from the suboptimal ones. Finally, we show that inverting the existing tests from the moment inequality literature produces LCBs that are robust to the non-uniqueness of the optimal policy and easy to compute. We find that this approach performs well empirically in the context of the National JTPA study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07443v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kirill Ponomarev, Vira Semenova</dc:creator>
    </item>
  </channel>
</rss>
