<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>nlin.AO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/nlin.AO</link>
    <description>nlin.AO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/nlin.AO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Oct 2024 01:59:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Cooperation in Public Goods Games: Leveraging Other-Regarding Reinforcement Learning on Hypergraphs</title>
      <link>https://arxiv.org/abs/2410.10921</link>
      <description>arXiv:2410.10921v1 Announce Type: cross 
Abstract: Cooperation as a self-organized collective behavior plays a significant role in the evolution of ecosystems and human society. Reinforcement learning (RL) offers a new perspective, distinct from imitation learning in evolutionary games, for exploring the mechanisms underlying its emergence. However, most existing studies with the public good game (PGG) employ a self-regarding setup or are on pairwise interaction networks. Players in the real world, however, optimize their policies based not only on their histories but also on the histories of their co-players, and the game is played in a group manner. In the work, we investigate the evolution of cooperation in the PGG under the other-regarding reinforcement learning evolutionary game (OR-RLEG) on hypergraph by combining the Q-learning algorithm and evolutionary game framework, where other players' action history is incorporated and the game is played on hypergraphs. Our results show that as the synergy factor increases, the parameter interval is divided into three distinct regions, the absence of cooperation (AC), medium cooperation (MC), and high cooperation (HC), accompanied by two abrupt transitions in the cooperation level near two transition points, respectively. Interestingly, we identify regular and anti-coordinated chessboard structures in the spatial pattern that positively contribute to the first cooperation transition but adversely affect the second. Furthermore, we provide a theoretical treatment for the first transition with an approximated first transition point and reveal that players with a long-sighted perspective and low exploration rate are more likely to reciprocate kindness with each other, thus facilitating the emergence of cooperation. Our findings contribute to understanding the evolution of human cooperation, where other-regarding information and group interactions are commonplace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10921v1</guid>
      <category>physics.soc-ph</category>
      <category>nlin.AO</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Bo-Ying Li, Zhen-Na Zhang, Guo-Zhong Zheng, Chao-Ran Cai, Ji-Qiang Zhang, Chen Li</dc:creator>
    </item>
    <item>
      <title>Probabilistic Principles for Biophysics and Neuroscience: Entropy Production, Bayesian Mechanics &amp; the Free-Energy Principle</title>
      <link>https://arxiv.org/abs/2410.11735</link>
      <description>arXiv:2410.11735v1 Announce Type: cross 
Abstract: This thesis focuses on three fundamental aspects of biological systems; namely, entropy production, Bayesian mechanics, and the free-energy principle. The contributions are threefold: 1) We compute the entropy production for a greater class of systems than before, including almost any stationary diffusion process, such as degenerate diffusions where the driving noise does not act on all coordinates of the system. Importantly, this class of systems encompasses Markovian approximations of stochastic differential equations driven by colored noise, which is significant since biological systems at the macro- and meso-scale are generally subject to colored fluctuations. 2) We develop a Bayesian mechanics for biological and physical entities that interact with their environment in which we give sufficient and necessary conditions for the internal states of something to infer its external states, consistently with variational Bayesian inference in statistics and theoretical neuroscience. 3) We refine the constraints on Bayesian mechanics to obtain a description that is more specific to biological systems, called the free-energy principle. This says that active and internal states of biological systems unfold as minimising a quantity known as free energy. The mathematical foundation to the free-energy principle, presented here, unlocks a first principles approach to modeling and simulating behavior in neurobiology and artificial intelligence, by minimising free energy given a generative model of external and sensory states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11735v1</guid>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>nlin.AO</category>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>PhD Thesis Imperial College London 2024</arxiv:journal_reference>
      <dc:creator>Lancelot Da Costa</dc:creator>
    </item>
    <item>
      <title>How social reinforcement learning can lead to metastable polarisation and the voter model</title>
      <link>https://arxiv.org/abs/2406.07993</link>
      <description>arXiv:2406.07993v2 Announce Type: replace-cross 
Abstract: Previous explanations for the persistence of polarization of opinions have typically included modelling assumptions that predispose the possibility of polarization (i.e., assumptions allowing a pair of agents to drift apart in their opinion such as repulsive interactions or bounded confidence). An exception is a recent simulation study showing that polarization is persistent when agents form their opinions using social reinforcement learning.
  Our goal is to highlight the usefulness of reinforcement learning in the context of modeling opinion dynamics, but that caution is required when selecting the tools used to study such a model. We show that the polarization observed in the model of the simulation study cannot persist indefinitely, and exhibits consensus asymptotically with probability one. By constructing a link between the reinforcement learning model and the voter model, we argue that the observed polarization is metastable. Finally, we show that a slight modification in the learning process of the agents changes the model from being non-ergodic to being ergodic.
  Our results show that reinforcement learning may be a powerful method for modelling polarization in opinion dynamics, but that the tools (objects to study such as the stationary distribution, or time to absorption for example) appropriate for analysing such models crucially depend on their properties (such as ergodicity, or transience). These properties are determined by the details of the learning process and may be difficult to identify based solely on simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07993v2</guid>
      <category>physics.soc-ph</category>
      <category>nlin.AO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Benedikt V. Meylahn, Janusz M. Meylahn</dc:creator>
    </item>
  </channel>
</rss>
