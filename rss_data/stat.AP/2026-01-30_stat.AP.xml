<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Jan 2026 05:01:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Analyzing the Temporal Factors for Anxiety and Depression Symptoms with the Rashomon Perspective</title>
      <link>https://arxiv.org/abs/2601.20874</link>
      <description>arXiv:2601.20874v1 Announce Type: new 
Abstract: This paper introduces a new modeling perspective in the public mental health domain to provide a robust interpretation of the relations between anxiety and depression, and the demographic and temporal factors. This perspective particularly leverages the Rashomon Effect, where multiple models exhibit similar predictive performance but rely on diverse internal structures. Instead of considering these multiple models, choosing a single best model risks masking alternative narratives embedded in the data. To address this, we employed this perspective in the interpretation of a large-scale psychological dataset, specifically focusing on the Patient Health Questionnaire-4. We use a random forest model combined with partial dependence profiles to rigorously assess the robustness and stability of predictive relationships across the resulting Rashomon set, which consists of multiple models that exhibit similar predictive performance. Our findings confirm that demographic variables \texttt{age}, \texttt{sex}, and \texttt{education} lead to consistent structural shifts in anxiety and depression risk. Crucially, we identify significant temporal effects: risk probability demonstrates clear diurnal and circaseptan fluctuations, peaking during early morning hours. This work demonstrates the necessity of moving beyond the best model to analyze the entire Rashomon set. Our results highlight that the observed variability, particularly due to circadian and circaseptan rhythms, must be meticulously considered for robust interpretation in psychological screening. We advocate for a multiplicity-aware approach to enhance the stability and generalizability of ML-based conclusions in mental health research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20874v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mustafa Cavus, Przemys{\l}aw Biecek, Julian Tejada, Fernando Marmolejo-Ramos, Andre Faro</dc:creator>
    </item>
    <item>
      <title>Distributed Causality in the SDG Network: Evidence from Panel VAR and Conditional Independence Analysis</title>
      <link>https://arxiv.org/abs/2601.20875</link>
      <description>arXiv:2601.20875v1 Announce Type: new 
Abstract: The achievement of the 2030 Sustainable Development Goals (SDGs) is dependent upon strategic resource distribution. We propose a causal discovery framework using Panel Vector Autoregression, along with both country-specific fixed effects and PCMCI+ conditional independence testing on 168 countries (2000-2025) to develop the first complete causal architecture of SDG dependencies. Utilizing 8 strategically chosen SDGs, we identify a distributed causal network (i.e., no single 'hub' SDG), with 10 statistically significant Granger-causal relationships identified as 11 unique direct effects. Education to Inequality is identified as the most statistically significant direct relationship (r = -0.599; p &lt; 0.05), while effect magnitude significantly varies depending on income levels (e.g., high-income: r = -0.65; lower-middle-income: r = -0.06; non-significant). We also reject the idea that there exists a single 'keystone' SDG. Additionally, we offer a proposed tiered priority framework for the SDGs namely, identifying upstream drivers (Education, Growth), enabling goals (Institutions, Energy), and downstream outcomes (Poverty, Health). Therefore, we conclude that effective SDG acceleration can be accomplished through coordinated multi-dimensional intervention(s), and that single-goal sequential strategies are insufficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20875v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Muhtasim Munif Fahim, Md Jahid Hasan Imran, Luknath Debnath, Tonmoy Shill, Md. Naim Molla, Ehsanul Bashar Pranto, Md Shafin Sanyan Saad, Md Rezaul Karim</dc:creator>
    </item>
    <item>
      <title>Spatial Heterogeneity in Climate Risk and Human Flourishing: An Exploration with Generative AI</title>
      <link>https://arxiv.org/abs/2601.20880</link>
      <description>arXiv:2601.20880v1 Announce Type: new 
Abstract: Recent advances in Generative Artificial Intelligence (AI), particularly Large Language Models (LLMs), enable scalable extraction of spatial information from unstructured text and offer new methodological opportunities for studying climate geography. This study develops a spatial framework to examine how cumulative climate risk relates to multidimensional human flourishing across U.S. counties. High-resolution climate hazard indicators are integrated with a Human Flourishing Geographic Index (HFGI), an index derived from classification of 2.6 billion geotagged tweets using fine-tuned open-source Large Language Models (LLMs). These indicators are aggregated to the US county-level and mapped to a structural equation model to infer overall climate risk and human flourishing dimensions, including expressed well-being, meaning and purpose, social connectedness, psychological distress, physical condition, economic stability, religiosity, character and virtue, and institutional trust. The results reveal spatially heterogeneous associations between greater cumulative climate risk and lower levels of expressed human flourishing, with coherent spatial patterns corresponding to recurrent exposure to heat, flooding, wind, drought, and wildfire hazards. The study demonstrates how Generative AI can be combined with latent construct modeling for geographical analysis and for spatial knowledge extraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20880v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Maria Iacus, Haodong Qi, Devika Jain</dc:creator>
    </item>
    <item>
      <title>A new strategy for finite-sample valid prediction of future insurance claims in the regression setting</title>
      <link>https://arxiv.org/abs/2601.21153</link>
      <description>arXiv:2601.21153v1 Announce Type: new 
Abstract: The extant insurance literature demonstrates a paucity of finite-sample valid prediction intervals of future insurance claims in the regression setting. To address this challenge, this article proposes a new strategy that converts a predictive method in the unsupervised iid (independent identically distributed) setting to a predictive method in the regression setting. In particular, it enables an actuary to obtain infinitely many finite-sample valid prediction intervals in the regression setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21153v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liang Hong</dc:creator>
    </item>
    <item>
      <title>Bivariate Postprocessing of Wind Vectors</title>
      <link>https://arxiv.org/abs/2601.21401</link>
      <description>arXiv:2601.21401v1 Announce Type: new 
Abstract: To quantify the uncertainty in numerical weather prediction (NWP) forecasts, ensemble prediction systems are utilized. Although NWP forecasts continuously improve, they suffer from systematic bias and dispersion errors. To obtain well calibrated and sharp predictive probability distributions, statistical postprocessing methods are applied to NWP output. Recent developments focus on multivariate postprocessing models incorporating dependencies directly into the model. We introduce three novel bivariate postprocessing approaches, and analyze their performance for joint postprocessing of bivariate wind vector components for 60 stations in Germany. Bivariate vine copula based models, a bivariate gradient boosted version of ensemble model output statistics (EMOS), and a bivariate distributional regression network (DRN) are compared to bivariate EMOS. The case study indicates that the novel bivariate methods improve over the bivariate EMOS approaches. The bivariate DRN and the most flexible version of the bivariate vine copula approach exhibit the best performance in terms of verification scores and calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21401v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ferdinand Buchner, David Jobst, Annette M\"oller, Claudia Czado</dc:creator>
    </item>
    <item>
      <title>Clustering Methods for Identifying and Modelling Areas with Similar Temperature Variations</title>
      <link>https://arxiv.org/abs/2601.21495</link>
      <description>arXiv:2601.21495v1 Announce Type: new 
Abstract: This paper proposes a novel data-driven approach for identifying and modelling areas with similar temperature variations throufigureh clustering and Space-Time AutoRegressive (STAR) models. Using annual temperature data from 168 countries (1901-2022), we apply three clustering methods based on (i) warming rates, (ii) annual temperature variations, and (iii) persistence of variation signs, using Euclidean and Hamming distances. These clusters are then employed to construct alternative spatial weight matrices for STAR models. Empirical results show that distance-based STAR models outperform classical contiguity-based ones, both in-sample and out-of-sample, with the Hamming distance-based STAR model achieving the best predictive accuracy. The study demonstrates that using statistical similarity rather than geographical proximity improves the modelling of global temperature dynamics, suggesting broader applicability to other environmental and socioeconomic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21495v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Otranto</dc:creator>
    </item>
    <item>
      <title>Social Media Data for Population Mapping: A Bayesian Approach to Address Representativeness and Privacy Challenges</title>
      <link>https://arxiv.org/abs/2601.22104</link>
      <description>arXiv:2601.22104v1 Announce Type: new 
Abstract: Accurate and timely population data are essential for disaster response and humanitarian planning, but traditional censuses often cannot capture rapid demographic changes. Social media data offer a promising alternative for dynamic population monitoring, but their representativeness remains poorly understood and stringent privacy requirements limit their reliability. Here, we address these limitations in the context of the Philippines by calibrating Facebook user counts with the country's 2020 census figures. First, we find that differential privacy techniques commonly applied to social media-based population datasets disproportionately mask low-population areas. To address this, we propose a Bayesian imputation approach to recover missing values, restoring data coverage for $5.5\%$ of rural areas. Further, using the imputed social media data and leveraging predictors such as urbanisation level, demographic composition, and socio-economic status, we develop a statistical model for the proportion of Facebook users in each municipality, which links observed Facebook user numbers to the true population levels. Out-of-sample validation demonstrates strong result generalisability, with errors as low as ${\approx}18\%$ and ${\approx}24\%$ for urban and rural Facebook user proportions, respectively. We further demonstrate that accounting for overdispersion and spatial correlations in the data is crucial to obtain accurate estimates and appropriate credible intervals. Crucially, as predictors change over time, the models can be used to regularly update the population predictions, providing a dynamic complement to census-based estimates. These results have direct implications for humanitarian response in disaster-prone regions and offer a general framework for using biased social media signals to generate reliable and timely population data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22104v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paolo Andrich, Shengjie Lai, Halim Jun, Qianwen Duan, Zhifeng Cheng, Seth R. Flaxman, Andrew J. Tatem</dc:creator>
    </item>
    <item>
      <title>A generative machine learning model for designing metal hydrides applied to hydrogen storage</title>
      <link>https://arxiv.org/abs/2601.20892</link>
      <description>arXiv:2601.20892v1 Announce Type: cross 
Abstract: Developing new metal hydrides is a critical step toward efficient hydrogen storage in carbon-neutral energy systems. However, existing materials databases, such as the Materials Project, contain a limited number of well-characterized hydrides, which constrains the discovery of optimal candidates. This work presents a framework that integrates causal discovery with a lightweight generative machine learning model to generate novel metal hydride candidates that may not exist in current databases. Using a dataset of 450 samples (270 training, 90 validation, and 90 testing), the model generates 1,000 candidates. After ranking and filtering, six previously unreported chemical formulas and crystal structures are identified, four of which are validated by density functional theory simulations and show strong potential for future experimental investigation. Overall, the proposed framework provides a scalable and time-efficient approach for expanding hydrogen storage datasets and accelerating materials discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20892v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.mtrl-sci</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijhydene.2026.153744</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Hydrogen Energy,Volume 211,2026,153744,ISSN 0360-3199,</arxiv:journal_reference>
      <dc:creator>Xiyuan Liu, Christian Hacker, Shengnian Wang, Yuhua Duan</dc:creator>
    </item>
    <item>
      <title>Efficient Causal Structure Learning via Modular Subgraph Integration</title>
      <link>https://arxiv.org/abs/2601.21014</link>
      <description>arXiv:2601.21014v1 Announce Type: cross 
Abstract: Learning causal structures from observational data remains a fundamental yet computationally intensive task, particularly in high-dimensional settings where existing methods face challenges such as the super-exponential growth of the search space and increasing computational demands. To address this, we introduce VISTA (Voting-based Integration of Subgraph Topologies for Acyclicity), a modular framework that decomposes the global causal structure learning problem into local subgraphs based on Markov Blankets. The global integration is achieved through a weighted voting mechanism that penalizes low-support edges via exponential decay, filters unreliable ones with an adaptive threshold, and ensures acyclicity using a Feedback Arc Set (FAS) algorithm. The framework is model-agnostic, imposing no assumptions on the inductive biases of base learners, is compatible with arbitrary data settings without requiring specific structural forms, and fully supports parallelization. We also theoretically establish finite-sample error bounds for VISTA, and prove its asymptotic consistency under mild conditions. Extensive experiments on both synthetic and real datasets consistently demonstrate the effectiveness of VISTA, yielding notable improvements in both accuracy and efficiency over a wide range of base learners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21014v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haixiang Sun, Pengchao Tian, Zihan Zhou, Jielei Zhang, Peiyi Li, Andrew L. Liu</dc:creator>
    </item>
    <item>
      <title>Collective Noise Filtering in Complex Networks</title>
      <link>https://arxiv.org/abs/2601.21299</link>
      <description>arXiv:2601.21299v1 Announce Type: cross 
Abstract: Complex networks are powerful representations of complex systems across scales and domains, and the field is experiencing unprecedented growth in data availability. However, real-world network data often suffer from noise, biases, and missing data in the edge weights, which undermine the reliability of downstream network analyses. Standard noise filtering approaches, whether treating individual edges one-by-one or assuming a uniform global noise level, are suboptimal, because in reality both signal and noise can be heterogeneous and correlated across multiple edges. As a solution, we introduce the Network Wiener Filter, a principled method for collective edge-level noise filtering that leverages both network topology and noise characteristics, to reduce error in the observed edge weights and to infer missing edge weights. We demonstrate the broad practical efficacy of the Network Wiener Filter in two distinct settings, the genetic interaction network of the yeast S. cerevisiae and the Enron Corpus email network, noting compelling evidence of successful noise suppression in both applications. With the Network Wiener Filter, we advocate for a shift toward error-aware network science, one that embraces data imperfection as an inherent feature and learns to navigate it effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21299v1</guid>
      <category>cs.CE</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tingyu Zhao, Istv\'an A. Kov\'acs</dc:creator>
    </item>
    <item>
      <title>Differential Dynamic Causal Nets: Model Construction, Identification and Group Comparisons</title>
      <link>https://arxiv.org/abs/2601.21478</link>
      <description>arXiv:2601.21478v1 Announce Type: cross 
Abstract: Pathophysiolpgical modelling of brain systems from microscale to macroscale remains difficult in group comparisons partly because of the infeasibility of modelling the interactions of thousands of neurons at the scales involved. Here, to address the challenge, we present a novel approach to construct differential causal networks directly from electroencephalogram (EEG) data. The proposed network is based on conditionally coupled neuronal circuits which describe the average behaviour of interacting neuron populations that contribute to observed EEG data. In the network, each node represents a parameterised local neural system while directed edges stand for node-wise connections with transmission parameters. The network is hierarchically structured in the sense that node and edge parameters are varying in subjects but follow a mixed-effects model. A novel evolutionary optimisation algorithm for parameter inference in the proposed method is developed using a loss function derived from Chen-Fliess expansions of stochastic differential equations. The method is demonstrated by application to the fitting of coupled Jansen-Rit local models. The performance of the proposed method is evaluated on both synthetic and real EEG data. In the real EEG data analysis, we track changes in the parameters that characterise dynamic causality within brains that demonstrate epileptic activity. We show evidence of network functional disruptions, due to imbalance of excitatory-inhibitory interneurons and altered epileptic brain connectivity, before and during seizure periods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21478v1</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kang You, Gary Green, Jian Zhang</dc:creator>
    </item>
    <item>
      <title>A Poisson Factor Mixture Model for the Analysis of Linguistic Competence in Italian University Students' Writing</title>
      <link>https://arxiv.org/abs/2601.21493</link>
      <description>arXiv:2601.21493v1 Announce Type: cross 
Abstract: Public debate on the alleged decline of language skills among younger generations often focuses on university students, the most highly educated segment of the population. Rather than addressing the ill posed question of linguistic decline, this paper examines how formal written Italian is currently used by university students and whether systematic patterns of competence and heterogeneity can be identified. The analysis is based on data from the UniversITA project, which collected formal texts written by a large and nationally representative sample of Italian university students. Texts were annotated for linguistically motivated features covering orthography, lexicon, syntax, morphosyntax, coherence, register, and sentence structure, yielding low frequency multivariate count data. To analyse these data, we propose a novel model-based clustering approach based on a Poisson factor mixture model that accounts for dependence among linguistic features and unobserved population heterogeneity. The results identify two correlated dimensions of writing competence, interpretable as communicative competence and linguistic grammatical competence. When educational and socio demographic information is incorporated, distinct student profiles emerge that are associated with field of study and educational background. These findings provide quantitative evidence on contemporary writing and offer insights relevant for language education and higher education policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21493v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Silvia Dallari, Laura Anderlucci, Nicola Grandi, Angela Montanari</dc:creator>
    </item>
    <item>
      <title>Investigating symptom duration using current status data: a case study of post-acute COVID-19 syndrome</title>
      <link>https://arxiv.org/abs/2407.04214</link>
      <description>arXiv:2407.04214v4 Announce Type: replace 
Abstract: For infectious diseases, characterizing symptom duration is of clinical and public health importance. Symptom duration may be assessed by surveying infected individuals and querying symptom status at the time of survey response. For example, in a SARS-CoV-2 testing program at the University of Washington, participants were surveyed at least $28$ days after testing positive and asked to report current symptom status. This study design yielded current status data: outcome measurements for each respondent consisted only of the time of survey response and a binary indicator of whether symptoms had resolved by that time. Such study design benefits from limited risk of recall bias, but analyzing the resulting data necessitates tailored statistical tools. Here, we review methods for current status data and describe a novel application of modern nonparametric techniques to this setting. The proposed approach is valid under weaker assumptions compared to existing methods, allows use of flexible machine learning tools, and handles potential survey nonresponse. From the university study, under an assumption that the survey response time is conditionally independent of symptom resolution time within strata of measured covariates, we estimate that 19% of participants experienced ongoing symptoms 30 days after testing positive, decreasing to 7% at 90 days. We assess the sensitivity of these results to deviations from conditional independence, finding the estimates to be more sensitive to assumption violations at 30 days compared to 90 days. Female sex, fatigue during acute infection, and higher viral load were associated with slower symptom resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04214v4</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1097/EDE.0000000000001882</arxiv:DOI>
      <arxiv:journal_reference>Epidemiology 36(5) (2025)</arxiv:journal_reference>
      <dc:creator>Charles J. Wolock, Susan Jacob, Julia C. Bennett, Anna Elias-Warren, Jessica O'Hanlon, Avi Kenny, Nicholas P. Jewell, Andrea Rotnitzky, Stephen R. Cole, Ana A. Weil, Helen Y. Chu, Marco Carone</dc:creator>
    </item>
    <item>
      <title>When Context Is Not Enough: Modeling Unexplained Variability in Car-Following Behavior</title>
      <link>https://arxiv.org/abs/2507.07012</link>
      <description>arXiv:2507.07012v2 Announce Type: replace 
Abstract: Modeling car-following behavior is fundamental to microscopic traffic simulation, yet traditional deterministic models often fail to capture the full extent of variability and unpredictability in human driving. While many modern approaches incorporate context-aware inputs (e.g., spacing, speed, relative speed), they frequently overlook structured stochasticity that arises from latent driver intentions, perception errors, and memory effects -- factors that are not directly observable from context alone. To fill the gap, this study introduces an interpretable stochastic modeling framework that captures not only context-dependent dynamics but also residual variability beyond what context can explain. Leveraging deep neural networks integrated with nonstationary Gaussian processes (GPs), our model employs a scenario-adaptive Gibbs kernel to learn dynamic temporal correlations in acceleration decisions, where the strength and duration of correlations between acceleration decisions evolve with the driving context. This formulation enables a principled, data-driven quantification of uncertainty in acceleration, speed, and spacing, grounded in both observable context and latent behavioral variability. Comprehensive experiments on the naturalistic vehicle trajectory dataset collected from the German highway, i.e., the HighD dataset, demonstrate that the proposed stochastic simulation method within this framework surpasses conventional methods in both predictive performance and interpretable uncertainty quantification. The integration of interpretability and accuracy makes this framework a promising tool for traffic analysis and safety-critical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07012v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chengyuan Zhang, Zhengbing He, Cathy Wu, Lijun Sun</dc:creator>
    </item>
    <item>
      <title>Estimating shots and variance on noisy quantum circuits</title>
      <link>https://arxiv.org/abs/2501.03194</link>
      <description>arXiv:2501.03194v2 Announce Type: replace-cross 
Abstract: We present a method for estimating the number of shots required to achieve a desired variance in the results of a quantum circuit. First, we establish a baseline for single-qubit characterisation of individual noise sources. We then move on to multi-qubit circuits, focusing on expectation-value circuits. We decompose the variance of the estimator into a sum of a statistical term and a bias floor. These are independently estimated with one additional run of the circuit. We test our method on a Variational Quantum Eigensolver for $H_2$ and show that we can predict the variance to within known error bounds. We go on to show that for IBM Pittsburgh's noise characteristics, at that instant, 7000 shots for the given circuit would have achieved a $\sigma^2 \approx 0.01$</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03194v2</guid>
      <category>quant-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manav Seksaria, Anil Prabhakar</dc:creator>
    </item>
    <item>
      <title>Data-Driven Probabilistic Air-Sea Flux Parameterization</title>
      <link>https://arxiv.org/abs/2503.03990</link>
      <description>arXiv:2503.03990v3 Announce Type: replace-cross 
Abstract: Accurately quantifying air-sea fluxes is important for understanding air-sea interactions and improving coupled weather and climate systems. This study introduces a probabilistic framework to represent the highly variable nature of air-sea fluxes, which is missing in deterministic bulk algorithms. Assuming Gaussian distributions conditioned on the input variables, we use artificial neural networks and eddy-covariance measurement data to estimate the mean and variance by minimizing negative log-likelihood loss. The trained neural networks provide alternative mean flux estimates to existing bulk algorithms, and quantify the uncertainty around the mean estimates. Stochastic parameterization of air-sea turbulent fluxes can be constructed by sampling from the predicted distributions. Tests in a single-column forced upper-ocean model suggest that changes in flux algorithms influence sea surface temperature and mixed layer depth seasonally. The ensemble spread in stochastic runs is most pronounced during spring restratification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03990v3</guid>
      <category>physics.ao-ph</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiarong Wu, Pavel Perezhogin, David John Gagne, Brandon Reichl, Aneesh C. Subramanian, Elizabeth Thompson, Laure Zanna</dc:creator>
    </item>
    <item>
      <title>Bridging Performance Gaps for ECG Foundation Models: A Post-Training Strategy</title>
      <link>https://arxiv.org/abs/2509.12991</link>
      <description>arXiv:2509.12991v2 Announce Type: replace-cross 
Abstract: ECG foundation models are increasingly popular due to their adaptability across various tasks. However, their clinical applicability is often limited by performance gaps compared to task-specific models, even after pre-training on large ECG datasets and fine-tuning on target data. This limitation is likely due to the lack of an effective post-training strategy. In this paper, we propose a simple yet effective post-training approach to enhance ECG foundation models. We evaluate it on a publicly available Transformer-based foundation model. Experiments across multiple ECG tasks show that our method consistently outperforms baseline fine-tuning. On the PTB-XL benchmarks, it improves macro AUROC by 0.7%-8.9% and macro AUPRC by 23.3%-77.9%, also outperforming several recent state-of-the-art approaches, including task-specific and advanced architectures. Further analyses demonstrate improved training dynamics and data efficiency, with only 30% of the training data outperforming the baseline trained on the full dataset. Ablation studies highlight the importance of stochastic depth and preview linear probing. These findings underscore the potential of post-training strategies to improve ECG foundation models, and we hope this work will contribute to the continued development of foundation models in the ECG domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12991v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ya Zhou, Yujie Yang, Xiaohan Fan, Wei Zhao</dc:creator>
    </item>
    <item>
      <title>A thermoinformational formulation for the description of neuropsychological systems</title>
      <link>https://arxiv.org/abs/2511.09506</link>
      <description>arXiv:2511.09506v3 Announce Type: replace-cross 
Abstract: Complex systems produce high-dimensional signals that lack macroscopic variables analogous to entropy, temperature, or free energy. This work introduces a thermoinformational formulation that derives entropy, internal energy, temperature, and Helmholtz free energy directly from empirical microstate distributions of arbitrary datasets. The approach provides a data-driven description of how a system reorganizes, exchanges information, and moves between stable and unstable states. Applied to dual-EEG recordings from mother-infant dyads performing the A-not-B task, the formulation captures increases in informational heat during switches and errors, and reveals that correct choices arise from more stable, low-temperature states. In an independent optogenetic dam-pup experiment, the same variables separate stimulation conditions and trace coherent trajectories in thermodynamic state space. Across both human and rodent systems, this thermoinformational formulation yields compact and physically interpretable macroscopic variables that generalize across species, modalities, and experimental paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09506v3</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George-Rafael Domenikos, Victoria Leong</dc:creator>
    </item>
    <item>
      <title>A General Mixture Loss Function to Optimize a Personalized Predictive Model</title>
      <link>https://arxiv.org/abs/2601.20788</link>
      <description>arXiv:2601.20788v2 Announce Type: replace-cross 
Abstract: Advances in precision medicine increasingly drive methodological innovation in health research. A key development is the use of personalized prediction models (PPMs), which are fit using a similar subpopulation tailored to a specific index patient, and have been shown to outperform one-size-fits-all models, particularly in terms of model discrimination performance. We propose a generalized loss function that enables tuning of the subpopulation size used to fit a PPM. This loss function allows joint optimization of discrimination and calibration, allowing both the performance measures and their relative weights to be specified by the user. To reduce computational burden, we conducted extensive simulation studies to identify practical bounds for the grid of subpopulation sizes. Based on these results, we recommend using a lower bound of 20\% and an upper bound of 70\% of the entire training dataset. We apply the proposed method to both simulated and real-world datasets and demonstrate that previously observed relationships between subpopulation size and model performance are robust. Furthermore, we show that the choice of performance measures in the loss function influences the optimal subpopulation size selected. These findings support the flexible and computationally efficient implementation of PPMs in precision health research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20788v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatiana Krikella, Joel A. Dubin</dc:creator>
    </item>
  </channel>
</rss>
