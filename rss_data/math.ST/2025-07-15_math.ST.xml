<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Jul 2025 04:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Causal Discovery for Linear Non-Gaussian Models with Disjoint Cycles</title>
      <link>https://arxiv.org/abs/2507.10767</link>
      <description>arXiv:2507.10767v1 Announce Type: new 
Abstract: The paradigm of linear structural equation modeling readily allows one to incorporate causal feedback loops in the model specification. These appear as directed cycles in the common graphical representation of the models. However, the presence of cycles entails difficulties such as the fact that models need no longer be characterized by conditional independence relations. As a result, learning cyclic causal structures remains a challenging problem. In this paper, we offer new insights on this problem in the context of linear non-Gaussian models. First, we precisely characterize when two directed graphs determine the same linear non-Gaussian model. Next, we take up a setting of cycle-disjoint graphs, for which we are able to show that simple quadratic and cubic polynomial relations among low-order moments of a non-Gaussian distribution allow one to locate source cycles. Complementing this with a strategy of decorrelating cycles and multivariate regression allows one to infer a block-topological order among the directed cycles, which leads to a {consistent and computationally efficient algorithm} for learning causal structures with disjoint cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10767v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias Drton, Marina Garrote-L\'opez, Niko Nikov, Elina Robeva, Y. Samuel Wang</dc:creator>
    </item>
    <item>
      <title>Debiased Prediction Inference with Non-sparse Loadings in Misspecified High-dimensional Regression Models</title>
      <link>https://arxiv.org/abs/2507.10944</link>
      <description>arXiv:2507.10944v1 Announce Type: new 
Abstract: High-dimensional regression models with regularized sparse estimation are widely applied. For statistical inferences, debiased methods are available about single coefficients or predictions with sparse new covariate vectors (also called loadings), in the presence of possible model misspecification. However, statistical inferences about predictions with non-sparse loadings are studied only under the assumption of correctly specified models. In this work, we develop debiased estimation and associated Wald confidence intervals for predictions with general loadings, allowed to be non-sparse, from possibly misspecified high-dimensional regression models. Our debiased estimator involves estimation of a debiasing vector, which is the general loading left-multiplied by the non-centered precision matrix in the linear model (LM) setting or the inverse Hessian of the objective function at the target coefficient vector in the generalized linear model (GLM) setting. We propose suitable estimators of the precision matrix or the inverse Hessian respectively in the LM or GLM settings and, for the first time, establish a root-n asymptotic expansion for the debiased prediction and justify associated Wald confidence intervals under sparsity conditions on the precision matrix or the inverse Hessian which are comparable to the conjunction of sparsity conditions required for inferences about all single coefficients in existing works. We also provide numerical results which further demonstrate the validity of our proposed confidence intervals for predictions with general loadings from possibly misspecified regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10944v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Libin Liang, Zhiqiang Tan</dc:creator>
    </item>
    <item>
      <title>Tube formula for spherically contoured random fields with subexponential marginals</title>
      <link>https://arxiv.org/abs/2507.11154</link>
      <description>arXiv:2507.11154v1 Announce Type: cross 
Abstract: It is widely known that the tube method, or equivalently the Euler characteristic heuristic, provides a very accurate approximation for the tail probability that the supremum of a smooth Gaussian random field exceeds a threshold value $c$. The relative approximation error $\Delta(c)$ is exponentially small as a function of $c$ when $c$ tends to infinity. On the other hand, little is known about non-Gaussian random fields.
  In this paper, we obtain the approximation error of the tube method applied to the canonical isotropic random fields on a unit sphere defined by $u\mapsto\langle u,\xi\rangle$, $u\in M\subset\mathbb{S}^{n-1}$, where $\xi$ is a spherically contoured random vector. These random fields have statistical applications in multiple testing and simultaneous regression inference when the unknown variance is estimated. The decay rate of the relative error $\Delta(c)$ depends on the tail of the distribution of $\|\xi\|^2$ and the critical radius of the index set $M$. If this distribution is subexponential but not regularly varying, $\Delta(c)\to 0$ as $c\to\infty$. However, in the regularly varying case, $\Delta(c)$ does not vanish and hence is not negligible. To address this limitation, we provide simple upper and lower bounds for $\Delta(c)$ and for the tube formula itself. Numerical studies are conducted to assess the accuracy of the asymptotic approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11154v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satoshi Kuriki, Evgeny Spodarev</dc:creator>
    </item>
    <item>
      <title>Projection Estimators of the Stationary Density of a Differential Equation Driven by the Fractional Brownian Motion</title>
      <link>https://arxiv.org/abs/2104.01144</link>
      <description>arXiv:2104.01144v3 Announce Type: replace 
Abstract: The paper deals with projection estimators of the density of the stationary solution $X$ to a differential equation driven by the fractional Brownian motion under a dissipativity condition on the drift function. A model selection method is provided and, thanks to the concentration inequality for Lipschitz functionals of discrete samples of $X$ proved in Bertin et al. (2020), an oracle inequality is established for the adaptive estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.01144v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spl.2021.109244</arxiv:DOI>
      <arxiv:journal_reference>Statistics and Probability Letters 180, 9 pages, 2022</arxiv:journal_reference>
      <dc:creator>Nicolas Marie</dc:creator>
    </item>
    <item>
      <title>Frontiers to the learning of nonparametric hidden Markov models</title>
      <link>https://arxiv.org/abs/2306.16293</link>
      <description>arXiv:2306.16293v2 Announce Type: replace 
Abstract: Hidden Markov models (HMMs) are flexible tools for clustering dependent data coming from unknown populations, allowing nonparametric modelling of the population densities. Identifiability fails when the data is in fact independent and identically distributed (i.i.d.), and we study the frontier between learnable and unlearnable two-state nonparametric HMMs. Learning the parameters of the HMM requires solving a nonlinear inverse problem whose difficulty depends not only on the smoothnesses of the populations but also on the distance to the i.i.d. boundary of the parameter set. The latter difficulty is mostly ignored in the literature in favour of assumptions precluding nearly independent data. This is the first work conducting a precise nonasymptotic, nonparametric analysis of the minimax risk taking into account all aspects of the hardness of the problem, in the case of two populations. Our analysis reveals an unexpected interplay between the distance to the i.i.d. boundary and the relative smoothnesses of the two populations: a surprising and intriguing transition occurs in the rate when the two densities have differing smoothnesses. We obtain upper and lower bounds revealing that, close to the i.i.d. boundary, it is possible to "borrow strength" from the estimator of the smoother density to improve the risk of the other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16293v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kweku Abraham, Elisabeth Gassiat, Zacharie Naulet</dc:creator>
    </item>
    <item>
      <title>A multivariate spatial regression model using signatures</title>
      <link>https://arxiv.org/abs/2410.07899</link>
      <description>arXiv:2410.07899v2 Announce Type: replace 
Abstract: We propose a spatial autoregressive model for a multivariate response variable and functional covariates. The approach is based on the notion of signature, which represents a function as an infinite series of its iterated integrals and presents the advantage of being applicable to a wide range of processes. We have provided theoretical guarantees for the choice of the signature truncation order, and we have shown in a simulation study and an application to pollution data that this approach outperforms existing approaches in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07899v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Camille Fr\'event, Issa-Mbenard Dabo</dc:creator>
    </item>
    <item>
      <title>Coverage errors for Student's t confidence intervals comparable to those in Hall (1988)</title>
      <link>https://arxiv.org/abs/2501.07645</link>
      <description>arXiv:2501.07645v2 Announce Type: replace 
Abstract: Table 1 of Hall (1988) contains asymptotic coverage error formulas for some nonparametric approximate 95\% confidence intervals for the mean based on $n$ IID samples. The table includes an entry for an interval based on the central limit theorem using Gaussian quantiles and the Gaussian maximum likelihood variance estimate. It is missing an entry for the very widely used Student's $t$ confidence intervals. This note develops such a formula. The impetus to revisit this issue arose from the surprisingly robust performance of confidence intervals based on Student's t statistic in randomized quasi-Monte Carlo sampling. Hall's table had $0.14\kappa -2.12\gamma^2-3.35$ for normal theory intervals; the corresponding entry for Student's $t$ is $0.14\kappa -2.12\gamma^2$.
  An earlier version of this note reported that it corrected some coverage error formulas in Hall (1988). Two-sided errors take the form $2\Phi^{-1}(0.975)(A\kappa + \gamma^2+C)\varphi(1.96)/n +O(1/n^{3/2})$ where the error may well be $O(n^{-2})$. Hall's table showed $\Phi^{-1}(0.975)(A\kappa + B\gamma^2+C)$. The version intended as a correction had $2(A\kappa + B\gamma^2+C)$, wider by about $2/1.96\doteq1.02$. So, Hall's table really is proportional to the two-sided coverage errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07645v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Art B. Owen</dc:creator>
    </item>
    <item>
      <title>Nadaraya-Watson Type Estimator of the Transition Density Function for Diffusion Processes</title>
      <link>https://arxiv.org/abs/2502.14498</link>
      <description>arXiv:2502.14498v2 Announce Type: replace 
Abstract: This paper deals with a nonparametric Nadaraya-Watson (NW) estimator of the transition density function computed from independent continuous observations of a diffusion process. A risk bound is established on this estimator. The paper also deals with an extension of the penalized comparison to overfitting bandwidths selection method for our NW estimator. Finally, numerical experiments are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14498v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Marie, Ousmane Sacko</dc:creator>
    </item>
    <item>
      <title>TorchCP: A Python Library for Conformal Prediction</title>
      <link>https://arxiv.org/abs/2402.12683</link>
      <description>arXiv:2402.12683v3 Announce Type: replace-cross 
Abstract: Conformal prediction (CP) is a robust statistical framework that generates prediction intervals or sets with guaranteed coverage probability, addressing the challenge of quantifying predictive uncertainty in deep learning. Despite advancements in deep learning architectures and datasets, reliable uncertainty estimation remains elusive, making CP increasingly vital. This paper introduces TorchCP, a PyTorch-native library designed to integrate state-of-the-art CP algorithms into deep learning tasks, including classification, regression, graph neural networks, and large language models. TorchCP offers a comprehensive suite of advanced methodologies, a modular design for easy customization, and full GPU-accelerated scalability. Released under the LGPL-3.0 license, TorchCP has gained widespread adoption with over 12,582 PyPi downloads. It is supported by approximately 16,132 lines of code, 564 unit tests achieving 100\% coverage, and comprehensive documentation. By bridging statistics and computer science, TorchCP empowers researchers and practitioners to advance conformal prediction in diverse deep learning applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12683v3</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianguo Huang, Jianqing Song, Xuanning Zhou, Bingyi Jing, Hongxin Wei</dc:creator>
    </item>
  </channel>
</rss>
