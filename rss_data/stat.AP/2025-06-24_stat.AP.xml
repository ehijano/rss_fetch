<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Jun 2025 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Identifying Causally-Robust Mediators of Health Disparities: A Review and Simulation Studies With Directed Acyclic Graphs</title>
      <link>https://arxiv.org/abs/2506.19047</link>
      <description>arXiv:2506.19047v1 Announce Type: new 
Abstract: Background Traditionally researchers have used linear approaches such as difference in coefficients DIC and Kitagawa Oaxaca Blinder KOB decomposition to identify risk factors or resources referred to as mediators underlying health disparities Recently causal decomposition analysis CDA has gained popularity by defining clear causal effects of interest and estimating them without modeling restrictions Methods We begin with a brief review of each method under the assumption of no unmeasured confounders followed by two realistic scenarios where unmeasured confounders affect first the relationship between intermediate confounders and the mediator and second the relationship between the mediator and the outcome For each scenario we generate simulated data apply all three methods compare estimates and interpret results using directed acyclic graphs Results The DIC approach performs well only when no intermediate confounders are present a condition unlikely in real world health disparities that arise from many factors over the life course The KOB decomposition is appropriate only when baseline covariates such as age need not be controlled When unmeasured confounding exists DIC yields biased estimates in both scenarios while both KOB and CDA yield biased estimates in the second scenario however CDA supplemented by sensitivity analysis can reveal how robust its estimates are to unmeasured confounding Conclusions We recommend against using DIC for investigating drivers of health disparities and instead advise applying CDA combined with sensitivity analysis as a robust strategy for identifying mediators of health disparities</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19047v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Soojin Park, Su Yeon Kim, Chioun Lee</dc:creator>
    </item>
    <item>
      <title>Investigating Resiliency of Transportation Network Under Targeted and Potential Climate Change Disruptions</title>
      <link>https://arxiv.org/abs/2506.19102</link>
      <description>arXiv:2506.19102v1 Announce Type: new 
Abstract: Ensuring robustness and resilience in intermodal transportation systems is essential for the continuity and reliability of global logistics. These systems are vulnerable to various disruptions, including natural disasters and technical failures. Despite significant research on freight transportation resilience, investigating the robustness of the system after targeted and climate-change driven disruption remains a crucial challenge. Drawing on network science methodologies, this study models the interdependencies within the rail and water transport networks and simulates different disruption scenarios to evaluate system responses. We use the data from the US Department of Energy Volpe Center for network topology and tonnage projections. The proposed framework quantifies deliberate, stochastic, and climate driven infrastructure failure, using higher resolution downscaled multiple Earth System Models simulations from Coupled Model Intercomparison Project Phase version 6. We show that the disruptions of a few nodes could have a larger impact on the total tonnage of freight transport than on network topology. For example, the removal of targeted 20 nodes can bring the total tonnage carrying capacity to 30 percent with about 75 percent of the rail freight network intact. This research advances the theoretical understanding of transportation resilience and provides practical applications for infrastructure managers and policymakers. By implementing these strategies, stakeholders and policymakers can better prepare for and respond to unexpected disruptions, ensuring sustained operational efficiency in the transportation networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19102v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maedeh Rahimitouranposhti, Bharat Sharma, Mustafa Can Camur, Olufemi A. Omitaomu, Xueping Li</dc:creator>
    </item>
    <item>
      <title>Carbon Cycle Extremes Accelerate Weakening of the Land Carbon Sink in the Late 21st Century</title>
      <link>https://arxiv.org/abs/2506.19119</link>
      <description>arXiv:2506.19119v1 Announce Type: new 
Abstract: Increasing surface temperature could lead to enhanced evaporation, reduced soil moisture availability, and more frequent droughts and heat waves. The spatiotemporal co-occurrence of such effects further drives extreme anomalies in vegetation productivity and net land carbon storage. However, the impacts of climate change on extremes in net biospheric production (NBP) over longer time periods are unknown. Using the percentile threshold on the probability distribution curve of NBP anomalies, we computed negative and positive extremes in NBP. Here we show that due to climate warming, about 88% of global regions will experience a larger magnitude of negative NBP extremes than positive NBP extremes toward the end of 2100, which accelerate the weakening of the land carbon sink. Our analysis indicates the frequency of negative extremes associated with declines in biospheric productivity was larger than positive extremes, especially in the tropics. While the overall impact of warming at high latitudes is expected to increase plant productivity and carbon uptake, high-temperature anomalies increasingly induce negative NBP extremes toward the end of the 21st century. Using regression analysis, we found soil moisture anomalies to be the most dominant individual driver of NBP extremes. The compound effect of hot, dry, and fire caused extremes at more than 50% of the total grid cells. The larger proportion of negative NBP extremes raises a concern about whether the Earth is capable of increasing vegetation production with growing human population and rising demand for plant material for food, fiber, fuel, and building materials. The increasing proportion of negative NBP extremes highlights the consequences of not only reduction in total carbon uptake capacity but also of conversion of land to a carbon source.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19119v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bharat Sharma, Jitendra Kumar, Auroop R. Ganguly, Forrest M. Hoffman</dc:creator>
    </item>
    <item>
      <title>Covariance Supervised Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2506.19247</link>
      <description>arXiv:2506.19247v1 Announce Type: new 
Abstract: Principal component analysis (PCA) is a widely used unsupervised dimensionality reduction technique in machine learning, applied across various fields such as bioinformatics, computer vision and finance. However, when the response variables are available, PCA does not guarantee that the derived principal components are informative to the response variables. Supervised PCA (SPCA) methods address this limitation by incorporating response variables into the learning process, typically through an objective function similar to PCA. Existing SPCA methods do not adequately address the challenge of deriving projections that are both interpretable and informative with respect to the response variable. The only existing approach attempting to overcome this, relies on a mathematically complicated manifold optimization scheme, sensitive to hyperparameter tuning. We propose covariance-supervised principal component analysis (CSPCA), a novel SPCA method that projects data into a lower-dimensional space by balancing (1) covariance between projections and responses and (2) explained variance, controlled via a regularization parameter. The projection matrix is derived through a closed-form solution in the form of a simple eigenvalue decomposition. To enhance computational efficiency for high-dimensional datasets, we extend CSPCA using the standard Nystr\"om method. Simulations and real-world applications demonstrate that CSPCA achieves strong performance across numerous performance metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19247v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theodosios Papazoglou, Guosheng Yin</dc:creator>
    </item>
    <item>
      <title>Nonlinear Rank Scaling and Hidden Structure in NHS Expenditure Transparency Data</title>
      <link>https://arxiv.org/abs/2506.19520</link>
      <description>arXiv:2506.19520v1 Announce Type: new 
Abstract: A variety of transparency initiatives have been introduced by governments to reduce corruption and allow citizens to independently evaluate effectiveness and efficiency of spending. In 2010, the UK government mandated transparency for many expenditures exceeding {\pounds}25,000. The resulting data is dispersed across a range of governmental organizations and presents an opportunity to understand expenditure at scale, interrogate organizational structures and develop transparency measures. Here, we focus on data from the top two layers of the National Health Service (NHS) within England, including NHS England (NHSE) and Integrated Care Boards (ICBs). As the one of the largest government run healthcare organizations in the world and potentially the sixth largest employer globally, the NHS provides a distinctive case for studying healthcare delivery, contractor dynamics, and organizational self-organization. We find that limiting transparency to larger transactions conceals a substantial share of spending from scrutiny, including most transactions. The rank-frequency distributions of suppliers, expense types, and spending categories exhibit multiple scaling regimes and these are similar to patterns observed in word frequency and urban scaling studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19520v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Animotu Mohammed, Golnaz Shahtahmassebi, Haroldo V. Ribeiro, Jack Sutton, Quentin S. Hanley</dc:creator>
    </item>
    <item>
      <title>Programming Geotechnical Reliability Algorithms using Generative AI</title>
      <link>https://arxiv.org/abs/2506.19536</link>
      <description>arXiv:2506.19536v1 Announce Type: new 
Abstract: Programming reliability algorithms is crucial for risk assessment in geotechnical engineering. This study explores the possibility of automating and accelerating this task using Generative AI based on Large Language Models (LLMs). Specifically, the most popular LLM, i.e., ChatGPT, is used to test the ability to generate MATLAB codes for four classical reliability algorithms. The four specific examples considered in this study are: (1) First Order Reliability Method (FORM); (2) Subset simulation; (3) Random field simulation; and (4) Bayesian update using Gibbs sampling. The results obtained using the generated codes are compared with benchmark methods. It is found that the use of LLMs can be promising for generating reliability codes. Failure, limitations, and challenges of adopting LLMs are also discussed. Overall, this study demonstrates that existing LLMs can be leveraged powerfully and can contribute toward accelerating the adoption of reliability techniques in routine geotechnical engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19536v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Atma Sharma, Jie Zhang, Meng Lu, Shuangyi Wu, Baoxiang Li</dc:creator>
    </item>
    <item>
      <title>A comparative analysis of machine learning algorithms for predicting probabilities of default</title>
      <link>https://arxiv.org/abs/2506.19789</link>
      <description>arXiv:2506.19789v1 Announce Type: new 
Abstract: Predicting the probability of default (PD) of prospective loans is a critical objective for financial institutions. In recent years, machine learning (ML) algorithms have achieved remarkable success across a wide variety of prediction tasks; yet, they remain relatively underutilised in credit risk analysis. This paper highlights the opportunities that ML algorithms offer to this field by comparing the performance of five predictive models-Random Forests, Decision Trees, XGBoost, Gradient Boosting and AdaBoost-to the predominantly used logistic regression, over a benchmark dataset from Scheule et al. (Credit Risk Analytics: The R Companion). Our findings underscore the strengths and weaknesses of each method, providing valuable insights into the most effective ML algorithms for PD prediction in the context of loan portfolios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19789v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrian Iulian Cristescu, Matteo Giordano</dc:creator>
    </item>
    <item>
      <title>Statistical Geometry and Information Dynamics on Hyperspherical Surfaces</title>
      <link>https://arxiv.org/abs/2506.19251</link>
      <description>arXiv:2506.19251v1 Announce Type: cross 
Abstract: We study the statistical geometry of random chords on n-dimensional spheres by deriving explicit analytical expressions for the chord length distribution and its associated structural properties. A critical threshold emerges at dimension 19, marking the transition from curvature-dominated variability to high-dimensional concentration, where interpoint distances become nearly deterministic and probabilistic diversity collapses into geometric uniformity. We further derive a closed-form expression for the Fisher information, showing that it is inversely proportional to the square of the radius and varies non-monotonically with dimension. Notably, it attains a minimum at dimension 7, coinciding with the dimension at which the volume of the unit sphere is maximized. This reflects a unique regime of maximal spatial diffuseness and minimal inferential sensitivity, arising from the interplay between weakening curvature and still-latent concentration. The alignment between volumetric and statistical extrema reveals a deeper duality between geometry and information. We also analyze the characteristic function, which exhibits a dichotomy: in even dimensions, it takes rational-exponential form, while in odd dimensions it involves Bessel and Struve functions. This distinction reflects differences in harmonic structure and boundary regularity across dimensions. Together, these findings show how curvature and dimension jointly regulate statistical efficiency on hyperspherical domains, with implications for geometric inference and high-dimensional learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19251v1</guid>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masoud Ataei</dc:creator>
    </item>
    <item>
      <title>The Shape of Consumer Behavior: A Symbolic and Topological Analysis of Time Series</title>
      <link>https://arxiv.org/abs/2506.19759</link>
      <description>arXiv:2506.19759v1 Announce Type: cross 
Abstract: Understanding temporal patterns in online search behavior is crucial for real-time marketing and trend forecasting. Google Trends offers a rich proxy for public interest, yet the high dimensionality and noise of its time-series data present challenges for effective clustering. This study evaluates three unsupervised clustering approaches, Symbolic Aggregate approXimation (SAX), enhanced SAX (eSAX), and Topological Data Analysis (TDA), applied to 20 Google Trends keywords representing major consumer categories. Our results show that while SAX and eSAX offer fast and interpretable clustering for stable time series, they struggle with volatility and complexity, often producing ambiguous ``catch-all'' clusters. TDA, by contrast, captures global structural features through persistent homology and achieves more balanced and meaningful groupings.
  We conclude with practical guidance for using symbolic and topological methods in consumer analytics and suggest that hybrid approaches combining both perspectives hold strong potential for future applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19759v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pola Bereta, Ioannis Diamantis</dc:creator>
    </item>
    <item>
      <title>A Practical Introduction to Regression-based Causal Inference in Meteorology (I): All confounders measured</title>
      <link>https://arxiv.org/abs/2506.18808</link>
      <description>arXiv:2506.18808v2 Announce Type: replace 
Abstract: Whether a variable is the cause of another, or simply associated with it, is often an important scientific question. Causal Inference is the name associated with the body of techniques for addressing that question in a statistical setting. Although assessing causality is relatively straightforward in the presence of temporal information, outside of that setting - the situation considered here - it is more difficult to assess causal effects. The development of the field of causal inference has involved concepts from a wide range of topics, thereby limiting its adoption across some fields, including meteorology. However, at its core, the requisite knowledge for causal inference involves little more than basic probability theory and regression, topics familiar to most meteorologists. By focusing on these core areas, this and a companion article provide a steppingstone for the meteorology community into the field of (non-temporal) causal inference. Although some theoretical foundations are presented, the main goal is the application of a specific method, called matching, to a problem in meteorology. The data for the application are in public domain, and R code is provided as well, forming an easy path for meteorology students and researchers to enter the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18808v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caren Marzban, Yikun Zhang, Nicholas Bond, Michael Richman</dc:creator>
    </item>
    <item>
      <title>Degree of Interference: A General Framework For Causal Inference Under Interference</title>
      <link>https://arxiv.org/abs/2210.17516</link>
      <description>arXiv:2210.17516v3 Announce Type: replace-cross 
Abstract: One core assumption typically adopted for valid causal inference is that of no interference between experimental units, i.e., the outcome of an experimental unit is unaffected by the treatments assigned to other experimental units. This assumption can be violated in real-life experiments, which significantly complicates the task of causal inference. As the number of potential outcomes increases, it becomes challenging to disentangle direct treatment effects from ``spillover'' effects. Current methodologies are lacking, as they cannot handle arbitrary, unknown interference structures to permit inference on causal estimands. We present a general framework to address the limitations of existing approaches. Our framework is based on the new concept of the ``degree of interference'' (DoI). The DoI is a unit-level latent variable that captures the latent structure of interference. We also develop a data augmentation algorithm that adopts a blocked Gibbs sampler and Bayesian nonparametric methodology to perform inferences on the estimands under our framework. We illustrate the DoI concept and properties of our Bayesian methodology via extensive simulation studies and an analysis of a randomized experiment investigating the impact of a cash transfer program for which interference is a critical concern. Ultimately, our framework enables us to infer causal effects without strong structural assumptions on interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.17516v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuki Ohnishi, Bikram Karmakar, Arman Sabbaghi</dc:creator>
    </item>
    <item>
      <title>The conditional saddlepoint approximation for fast and accurate large-scale hypothesis testing</title>
      <link>https://arxiv.org/abs/2407.08911</link>
      <description>arXiv:2407.08911v3 Announce Type: replace-cross 
Abstract: Saddlepoint approximations (SPAs) for resampling-based procedures offer statistically accurate and computationally efficient inference, which is particularly critical in the analysis of large-scale, high-multiplicity data. Despite being introduced 70 years ago, SPAs for resampling-based procedures lack rigorous justification and have been underutilized in modern applications. We establish a theoretical foundation for the SPA in this context by developing a general result on its approximation accuracy for conditional tail probabilities of averages of conditionally independent summands. This result both justifies existing SPAs for classical procedures like the sign-flipping test and enables new SPAs for modern resampling methods, including those using black-box machine learning. Capitalizing on this result, we introduce the saddlepoint approximation-based conditional randomization test (spaCRT), a resampling-free conditional independence test that is both statistically accurate and computationally efficient. The method is especially well-suited for sparse, large-scale datasets such as single-cell CRISPR screens and genome-wide association studies involving rare diseases. We prove the validity of the spaCRT when paired with modern regression tools such as lasso and kernel ridge regression. Extensive analyses of simulated and real data show that the spaCRT controls Type-I error, achieves high power, and outperforms existing asymptotic and resampling-based alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08911v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Niu, Jyotishka Ray Choudhury, Eugene Katsevich</dc:creator>
    </item>
    <item>
      <title>Statistical Multicriteria Evaluation of LLM-Generated Text</title>
      <link>https://arxiv.org/abs/2506.18082</link>
      <description>arXiv:2506.18082v2 Announce Type: replace-cross 
Abstract: Assessing the quality of LLM-generated text remains a fundamental challenge in natural language processing. Current evaluation approaches often rely on isolated metrics or simplistic aggregations that fail to capture the nuanced trade-offs between coherence, diversity, fluency, and other relevant indicators of text quality. In this work, we adapt a recently proposed framework for statistical inference based on Generalized Stochastic Dominance (GSD) that addresses three critical limitations in existing benchmarking methodologies: the inadequacy of single-metric evaluation, the incompatibility between cardinal automatic metrics and ordinal human judgments, and the lack of inferential statistical guarantees. The GSD-front approach enables simultaneous evaluation across multiple quality dimensions while respecting their different measurement scales, building upon partial orders of decoding strategies, thus avoiding arbitrary weighting of the involved metrics. By applying this framework to evaluate common decoding strategies against human-generated text, we demonstrate its ability to identify statistically significant performance differences while accounting for potential deviations from the i.i.d. assumption of the sampling design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18082v2</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Esteban Garces Arias, Hannah Blocher, Julian Rodemann, Matthias A{\ss}enmacher, Christoph Jansen</dc:creator>
    </item>
  </channel>
</rss>
