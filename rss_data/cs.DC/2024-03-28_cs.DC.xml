<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Mar 2024 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 28 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Navigating the Docker Ecosystem: A Comprehensive Taxonomy and Survey</title>
      <link>https://arxiv.org/abs/2403.17940</link>
      <description>arXiv:2403.17940v1 Announce Type: new 
Abstract: The cloud computing landscape is rapidly expanding and growing in complexity. It has witnessed the emergence of Cloud Computing as a widely adopted model for efficiently processing large volumes of data by harnessing clusters of commodity computers. This evolution enables the handling of massive data through on-demand services, relying on numerous microservices with diverse dependencies. The technology of containers ensures secure storage, allowing for largescale data processing with high scalability and portability. Container technology, particularly exemplified by Docker in the last decade, plays a pivotal role in this scenario. It empowers microservices to process data swiftly, enabling developers to dynamically scale these services in real-time. This paper initiates by establishing a comprehensive taxonomy for delineating container architecture. Focusing specifically on Docker containers, we scrutinize various existing container related literature. Through this taxonomy and survey, we not only discern similarities and disparities in the architectural approaches of Docker container technology but also pinpoint areas necessitating further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17940v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.9734/AJRCOS/2024/v17i1411</arxiv:DOI>
      <dc:creator>Prathamesh Muzumdar, Amol Bhosale, Ganga Prasad Basyal, George Kurian</dc:creator>
    </item>
    <item>
      <title>Workflow Mini-Apps: Portable, Scalable, Tunable &amp; Faithful Representations of Scientific Workflows</title>
      <link>https://arxiv.org/abs/2403.18073</link>
      <description>arXiv:2403.18073v1 Announce Type: new 
Abstract: Workflows are critical for scientific discovery. However, the sophistication, heterogeneity, and scale of workflows make building, testing, and optimizing them increasingly challenging. Furthermore, their complexity and heterogeneity make performance reproducibility hard. In this paper, we propose workflow mini-apps as a tool to address the challenges in building and testing workflows while controlling the fidelity of representing realworld workflows. Workflow mini-apps are deployed and run on various HPC systems and architectures without workflow-specific constraints. We offer insight into their design and implementation, providing an analysis of their performance and reproducibility. Workflow mini-apps thus advance the science of workflows by providing simple, portable, and managed (fidelity) representations of otherwise complex and difficult-to-control real workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18073v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ozgur Ozan Kilic, Tianle Wang, Matteo Turilli, Mikhail Titov, Andre Merzky, Line Pouchard, Shantenu Jha</dc:creator>
    </item>
    <item>
      <title>Optimizing Communication for Latency Sensitive HPC Applications on up to 48 FPGAs Using ACCL</title>
      <link>https://arxiv.org/abs/2403.18374</link>
      <description>arXiv:2403.18374v1 Announce Type: new 
Abstract: Most FPGA boards in the HPC domain are well-suited for parallel scaling because of the direct integration of versatile and high-throughput network ports. However, the utilization of their network capabilities is often challenging and error-prone because the whole network stack and communication patterns have to be implemented and managed on the FPGAs. Also, this approach conceptually involves a trade-off between the performance potential of improved communication and the impact of resource consumption for communication infrastructure, since the utilized resources on the FPGAs could otherwise be used for computations. In this work, we investigate this trade-off, firstly, by using synthetic benchmarks to evaluate the different configuration options of the communication framework ACCL and their impact on communication latency and throughput. Finally, we use our findings to implement a shallow water simulation whose scalability heavily depends on low-latency communication. With a suitable configuration of ACCL, good scaling behavior can be shown to all 48 FPGAs installed in the system. Overall, the results show that the availability of inter-FPGA communication frameworks as well as the configurability of framework and network stack are crucial to achieve the best application performance with low latency communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18374v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Meyer, Tobias Kenter, Lucian Petrica, Kenneth O'Brien, Michaela Blott, Christian Pessl</dc:creator>
    </item>
    <item>
      <title>Distributed Maximum Consensus over Noisy Links</title>
      <link>https://arxiv.org/abs/2403.18509</link>
      <description>arXiv:2403.18509v1 Announce Type: new 
Abstract: We introduce a distributed algorithm, termed noise-robust distributed maximum consensus (RD-MC), for estimating the maximum value within a multi-agent network in the presence of noisy communication links. Our approach entails redefining the maximum consensus problem as a distributed optimization problem, allowing a solution using the alternating direction method of multipliers. Unlike existing algorithms that rely on multiple sets of noise-corrupted estimates, RD-MC employs a single set, enhancing both robustness and efficiency. To further mitigate the effects of link noise and improve robustness, we apply moving averaging to the local estimates. Through extensive simulations, we demonstrate that RD-MC is significantly more robust to communication link noise compared to existing maximum-consensus algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18509v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsan Lari, Reza Arablouei, Naveen K. D. Venkategowda, Stefan Werner</dc:creator>
    </item>
    <item>
      <title>Optimal Resource Efficiency with Fairness in Heterogeneous GPU Clusters</title>
      <link>https://arxiv.org/abs/2403.18545</link>
      <description>arXiv:2403.18545v1 Announce Type: new 
Abstract: Ensuring the highest training throughput to maximize resource efficiency, while maintaining fairness among users, is critical for deep learning (DL) training in heterogeneous GPU clusters. However, current DL schedulers provide only limited fairness properties and suboptimal training throughput, impeding tenants from effectively leveraging heterogeneous resources. The underlying design challenge stems from inherent conflicts between efficiency and fairness properties.
  In this paper, we introduce OEF, a new resource allocation framework specifically developed for achieving optimal resource efficiency and ensuring diverse fairness properties in heterogeneous GPU clusters. By integrating resource efficiency and fairness within a global optimization framework, OEF is capable of providing users with maximized overall efficiency, as well as various guarantees of fairness, in both cooperative and non-cooperative environments. We have implemented OEF in a cluster resource manager and conducted large-scale experiments, showing that OEF can improve the overall training throughput by up to 32% while improving fairness compared to state-of-the-art heterogeneity-aware schedulers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18545v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zizhao Mo, Huanle Xu, Wing Cheong Lau</dc:creator>
    </item>
    <item>
      <title>Enhanced OpenMP Algorithm to Compute All-Pairs Shortest Path on x86 Architectures</title>
      <link>https://arxiv.org/abs/2403.18619</link>
      <description>arXiv:2403.18619v1 Announce Type: new 
Abstract: Graphs have become a key tool when modeling and solving problems in different areas. The Floyd-Warshall (FW) algorithm computes the shortest path between all pairs of vertices in a graph and is employed in areas like communication networking, traffic routing, bioinformatics, among others. However, FW is computationally and spatially expensive since it requires O(n^3) operations and O(n^2) memory space. As the graph gets larger, parallel computing becomes necessary to provide a solution in an acceptable time range. In this paper, we studied a FW code developed for Xeon Phi KNL processors and adapted it to run on any Intel x86 processors, losing the specificity of the former. To do so, we verified one by one the optimizations proposed by the original code, making adjustments to the base code where necessary, and analyzing its performance on two Intel servers under different test scenarios. In addition, a new optimization was proposed to increase the concurrency degree of the parallel algorithm, which was implemented using two different synchronization mechanisms. The experimental results show that all optimizations were beneficial on the two x86 platforms selected. Last, the new optimization proposal improved performance by up to 23%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18619v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sergio Calder\'on, Enzo Rucci, Franco Chichizola</dc:creator>
    </item>
    <item>
      <title>Dependency Aware Incident Linking in Large Cloud Systems</title>
      <link>https://arxiv.org/abs/2403.18639</link>
      <description>arXiv:2403.18639v1 Announce Type: new 
Abstract: Despite significant reliability efforts, large-scale cloud services inevitably experience production incidents that can significantly impact service availability and customer's satisfaction. Worse, in many cases one incident can lead to multiple downstream failures due to cascading effects that creates several related incidents across different dependent services. Often time On-call Engineers (OCEs) examine these incidents in silos that lead to significant amount of manual toil and increase the overall time-to-mitigate incidents. Therefore, developing efficient incident linking models is of paramount importance for grouping related incidents into clusters so as to quickly resolve major outages and reduce on-call fatigue. Existing incident linking methods mostly leverages textual and contextual information of incidents (e.g., title, description, severity, impacted components), thus failing to leverage the inter-dependencies between services. In this paper, we propose the dependency-aware incident linking (DiLink) framework which leverages both textual and service dependency graph information to improve the accuracy and coverage of incident links not only coming from same service, but also from different services and workloads. Furthermore, we propose a novel method to align the embeddings of multi-modal (i.e., textual and graphical) data using Orthogonal Procrustes. Extensive experimental results on real-world incidents from 5 workloads of Microsoft demonstrate that our alignment method has an F1-score of 0.96 (14% gain over current state-of-the-art methods). We are also in the process of deploying this solution across 610 services from these 5 workloads for continuously supporting OCEs improving incident management and reducing manual toil.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18639v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Supriyo Ghosh, Karish Grover, Jimmy Wong, Chetan Bansal, Rakesh Namineni, Mohit Verma, Saravan Rajmohan</dc:creator>
    </item>
    <item>
      <title>HotStuff-2 vs. HotStuff: The Difference and Advantage</title>
      <link>https://arxiv.org/abs/2403.18300</link>
      <description>arXiv:2403.18300v1 Announce Type: cross 
Abstract: Byzantine consensus protocols are essential in blockchain technology. The widely recognized HotStuff protocol uses cryptographic measures for efficient view changes and reduced communication complexity. Recently, the main authors of HotStuff introduced an advanced iteration named HotStuff-2. This paper aims to compare the principles and analyze the effectiveness of both protocols, hoping to depict their key differences and assess the potential enhancements offered by HotStuff-2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18300v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyuan Zhao, Yanqi Wu, Zheng Wang</dc:creator>
    </item>
    <item>
      <title>Privacy-Preserving Distributed Nonnegative Matrix Factorization</title>
      <link>https://arxiv.org/abs/2403.18326</link>
      <description>arXiv:2403.18326v1 Announce Type: cross 
Abstract: Nonnegative matrix factorization (NMF) is an effective data representation tool with numerous applications in signal processing and machine learning. However, deploying NMF in a decentralized manner over ad-hoc networks introduces privacy concerns due to the conventional approach of sharing raw data among network agents. To address this, we propose a privacy-preserving algorithm for fully-distributed NMF that decomposes a distributed large data matrix into left and right matrix factors while safeguarding each agent's local data privacy. It facilitates collaborative estimation of the left matrix factor among agents and enables them to estimate their respective right factors without exposing raw data. To ensure data privacy, we secure information exchanges between neighboring agents utilizing the Paillier cryptosystem, a probabilistic asymmetric algorithm for public-key cryptography that allows computations on encrypted data without decryption. Simulation results conducted on synthetic and real-world datasets demonstrate the effectiveness of the proposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18326v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsan Lari, Reza Arablouei, Stefan Werner</dc:creator>
    </item>
    <item>
      <title>Improving Efficiency of Parallel Across the Method Spectral Deferred Corrections</title>
      <link>https://arxiv.org/abs/2403.18641</link>
      <description>arXiv:2403.18641v1 Announce Type: cross 
Abstract: Parallel-across-the method time integration can provide small scale parallelism when solving initial value problems. Spectral deferred corrections (SDC) with a diagonal sweeper, which is closely related to iterated Runge-Kutta methods proposed by Van der Houwen and Sommeijer, can use a number of threads equal to the number of quadrature nodes in the underlying collocation method. However, convergence speed, efficiency and stability depends critically on the used coefficients. Previous approaches have used numerical optimization to find good parameters. Instead, we propose an ansatz that allows to find optimal parameters analytically. We show that the resulting parallel SDC methods provide stability domains and convergence order very similar to those of well established serial SDC variants. Using a model for computational cost that assumes 80% efficiency of an implementation of parallel SDC we show that our variants are competitive with serial SDC, previously published parallel SDC coefficients as well as Picard iteration, explicit RKM-4 and an implicit fourth-order diagonally implicit Runge-Kutta method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18641v1</guid>
      <category>math.NA</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gayatri \v{C}aklovi\'c, Thibaut Lunet, Sebastian G\"otschel, Daniel Ruprecht</dc:creator>
    </item>
    <item>
      <title>JumpBackHash: Say Goodbye to the Modulo Operation to Distribute Keys Uniformly to Buckets</title>
      <link>https://arxiv.org/abs/2403.18682</link>
      <description>arXiv:2403.18682v1 Announce Type: cross 
Abstract: The distribution of keys to a given number of buckets is a fundamental task in distributed data processing and storage. A simple, fast, and therefore popular approach is to map the hash values of keys to buckets based on the remainder after dividing by the number of buckets. Unfortunately, these mappings are not stable when the number of buckets changes, which can lead to severe spikes in system resource utilization, such as network or database requests. Consistent hash algorithms can minimize remappings, but are either significantly slower than the modulo-based approach, require floating-point arithmetic, or are based on a family of hash functions rarely available in standard libraries. This paper introduces JumpBackHash, which uses only integer arithmetic and a standard pseudorandom generator. Due to its speed and simple implementation, it can safely replace the modulo-based approach to improve assignment and system stability. A production-ready Java implementation of JumpBackHash has been released as part of the Hash4j open source library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18682v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Otmar Ertl</dc:creator>
    </item>
    <item>
      <title>Superior Parallel Big Data Clustering through Competitive Stochastic Sample Size Optimization in Big-means</title>
      <link>https://arxiv.org/abs/2403.18766</link>
      <description>arXiv:2403.18766v1 Announce Type: cross 
Abstract: This paper introduces a novel K-means clustering algorithm, an advancement on the conventional Big-means methodology. The proposed method efficiently integrates parallel processing, stochastic sampling, and competitive optimization to create a scalable variant designed for big data applications. It addresses scalability and computation time challenges typically faced with traditional techniques. The algorithm adjusts sample sizes dynamically for each worker during execution, optimizing performance. Data from these sample sizes are continually analyzed, facilitating the identification of the most efficient configuration. By incorporating a competitive element among workers using different sample sizes, efficiency within the Big-means algorithm is further stimulated. In essence, the algorithm balances computational time and clustering quality by employing a stochastic, competitive sampling strategy in a parallel computing setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18766v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.IR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rustam Mussabayev, Ravil Mussabayev</dc:creator>
    </item>
    <item>
      <title>Space and Move-optimal Arbitrary Pattern Formation on Infinite Rectangular Grid by Oblivious Robot Swarm</title>
      <link>https://arxiv.org/abs/2309.11190</link>
      <description>arXiv:2309.11190v2 Announce Type: replace 
Abstract: Arbitrary Pattern Formation (APF) is a fundamental coordination problem in swarm robotics. It requires a set of autonomous robots (mobile computing units) to form an arbitrary pattern (given as input) starting from any initial pattern. This problem has been extensively investigated in continuous and discrete scenarios, with this study focusing on the discrete variant. A set of robots is placed on the nodes of an infinite rectangular grid graph embedded in the euclidean plane. The movements of each robot is restricted to one of the four neighboring grid nodes from its current position. The robots are autonomous, anonymous, identical, and homogeneous, and operate Look-Compute-Move cycles. In this work, we adopt the classical $\mathcal{OBLOT}$ robot model, meaning the robots have no persistent memory or explicit communication methods, yet they possess full and unobstructed visibility. This work proposes an algorithm that solves the APF problem in a fully asynchronous scheduler assuming the initial configuration is asymmetric. The considered performance measures of the algorithm are space and number of moves required for the robots. The algorithm is asymptotically move-optimal. Here, we provide a definition of space complexity that takes the visibility issue into consideration. We observe an obvious lower bound $\mathcal{D}$ of the space complexity and show that the proposed algorithm has the space complexity $\mathcal{D}+4$. On comparing with previous related works, we show that this is the first proposed algorithm considering $\mathcal{OBLOT}$ robot model that is asymptotically move-optimal and has the least space complexity which is almost optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.11190v2</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avisek Sharma, Satakshi Ghosh, Pritam Goswami, Buddhadeb Sau</dc:creator>
    </item>
    <item>
      <title>Automated MPI code generation for scalable finite-difference solvers</title>
      <link>https://arxiv.org/abs/2312.13094</link>
      <description>arXiv:2312.13094v2 Announce Type: replace 
Abstract: Partial differential equations (PDEs) are crucial in modelling diverse phenomena across scientific disciplines, including seismic and medical imaging, computational fluid dynamics, image processing, and neural networks. Solving these PDEs on a large scale is an intricate and time-intensive process that demands careful tuning. This paper introduces automated code-generation techniques specifically tailored for distributed memory parallelism (DMP) to solve explicit finite-difference (FD) stencils at scale, a fundamental challenge in numerous scientific applications. These techniques are implemented and integrated into the Devito DSL and compiler framework, a well-established solution for automating the generation of FD solvers based on a high-level symbolic math input. Users benefit from modelling simulations at a high-level symbolic abstraction and effortlessly harnessing HPC-ready distributed-memory parallelism without altering their source code. This results in drastic reductions both in execution time and developer effort. While the contributions of this work are implemented and integrated within the Devito framework, the DMP concepts and the techniques applied are generally applicable to any FD solvers. A comprehensive performance evaluation of Devito's DMP via MPI demonstrates highly competitive weak and strong scaling on the Archer2 supercomputer, demonstrating the effectiveness of the proposed approach in meeting the demands of large-scale scientific simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13094v2</guid>
      <category>cs.DC</category>
      <category>cs.MS</category>
      <category>cs.PF</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George Bisbas, Rhodri Nelson, Mathias Louboutin, Paul H. J. Kelly, Fabio Luporini, Gerard Gorman</dc:creator>
    </item>
    <item>
      <title>Taking GPU Programming Models to Task for Performance Portability</title>
      <link>https://arxiv.org/abs/2402.08950</link>
      <description>arXiv:2402.08950v2 Announce Type: replace 
Abstract: Ensuring high productivity in scientific software development necessitates developing and maintaining a single codebase that can run efficiently on a range of accelerator-based supercomputing platforms. While prior work has investigated the performance portability of a few selected proxy applications or programming models, this paper provides a comprehensive study of a range of proxy applications implemented in the major programming models suitable for GPU-based platforms. We present and analyze performance results across NVIDIA and AMD GPU hardware currently deployed in leadership-class computing facilities using a representative range of scientific codes and several programming models -- CUDA, HIP, Kokkos, RAJA, OpenMP, OpenACC, and SYCL. Based on the specific characteristics of applications tested, we include recommendations to developers on how to choose the right programming model for their code. We find that Kokkos, RAJA, and SYCL in particular offer the most promise empirically as performance portable programming models. These results provide a comprehensive evaluation of the extent to which each programming model for heterogeneous systems provides true performance portability in real-world usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08950v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua H. Davis, Pranav Sivaraman, Joy Kitson, Konstantinos Parasyris, Harshitha Menon, Isaac Minn, Giorgis Georgakoudis, Abhinav Bhatele</dc:creator>
    </item>
    <item>
      <title>Design and Implementation of an Analysis Pipeline for Heterogeneous Data</title>
      <link>https://arxiv.org/abs/2403.15721</link>
      <description>arXiv:2403.15721v2 Announce Type: replace 
Abstract: Managing and preparing complex data for deep learning, a prevalent approach in large-scale data science can be challenging. Data transfer for model training also presents difficulties, impacting scientific fields like genomics, climate modeling, and astronomy. A large-scale solution like Google Pathways with a distributed execution environment for deep learning models exists but is proprietary. Integrating existing open-source, scalable runtime tools and data frameworks on high-performance computing (HPC) platforms is crucial to address these challenges. Our objective is to establish a smooth and unified method of combining data engineering and deep learning frameworks with diverse execution capabilities that can be deployed on various high-performance computing platforms, including cloud and supercomputers. We aim to support heterogeneous systems with accelerators, where Cylon and other data engineering and deep learning frameworks can utilize heterogeneous execution. To achieve this, we propose Radical-Cylon, a heterogeneous runtime system with a parallel and distributed data framework to execute Cylon as a task of Radical Pilot. We thoroughly explain Radical-Cylon's design and development and the execution process of Cylon tasks using Radical Pilot. This approach enables the use of heterogeneous MPI-communicators across multiple nodes. Radical-Cylon achieves better performance than Bare-Metal Cylon with minimal and constant overhead. Radical-Cylon achieves (4~15)% faster execution time than batch execution while performing similar join and sort operations with 35 million and 3.5 billion rows with the same resources. The approach aims to excel in both scientific and engineering research HPC systems while demonstrating robust performance on cloud infrastructures. This dual capability fosters collaboration and innovation within the open-source scientific research community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15721v2</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arup Kumar Sarker, Aymen Alsaadi, Niranda Perera, Mills Staylor, Gregor von Laszewski, Matteo Turilli, Ozgur Ozan Kilic, Mikhail Titov, Andre Merzky, Shantenu Jha, Geoffrey Fox</dc:creator>
    </item>
    <item>
      <title>Emerging Trends in Federated Learning: From Model Fusion to Federated X Learning</title>
      <link>https://arxiv.org/abs/2102.12920</link>
      <description>arXiv:2102.12920v5 Announce Type: replace-cross 
Abstract: Federated learning is a new learning paradigm that decouples data collection and model training via multi-party computation and model aggregation. As a flexible learning setting, federated learning has the potential to integrate with other learning frameworks. We conduct a focused survey of federated learning in conjunction with other learning algorithms. Specifically, we explore various learning algorithms to improve the vanilla federated averaging algorithm and review model fusion methods such as adaptive aggregation, regularization, clustered methods, and Bayesian methods. Following the emerging trends, we also discuss federated learning in the intersection with other learning paradigms, termed federated X learning, where X includes multitask learning, meta-learning, transfer learning, unsupervised learning, and reinforcement learning. In addition to reviewing state-of-the-art studies, this paper also identifies key challenges and applications in this field, while also highlighting promising future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.12920v5</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaoxiong Ji, Yue Tan, Teemu Saravirta, Zhiqin Yang, Yixin Liu, Lauri Vasankari, Shirui Pan, Guodong Long, Anwar Walid</dc:creator>
    </item>
    <item>
      <title>A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs</title>
      <link>https://arxiv.org/abs/2305.13525</link>
      <description>arXiv:2305.13525v2 Announce Type: replace-cross 
Abstract: Large communication costs are a critical bottleneck in training state-of-the-art neural networks on distributed systems. This paper introduces AxoNN, a novel four-dimensional (4D) parallelization approach, inspired by Agarwal's algorithm for matrix multiplication, for parallelizing tensor computations in deep learning, AxoNN employs two key strategies to minimize communication overhead. First, we optimize communication by overlapping expensive collective operations (reduce-scatter, all-gather, all-reduce) with computations. Our experiments with a 20-billion parameter transformer model demonstrate that these optimizations deliver nearly 53\% improvement. Second, we present an analytical model to assist users in identifying communication-minimizing configurations within the vast search space defined by our 4D algorithm. This model empowers practitioners by simplifying the tuning process for their specific training workloads. When training an 80-billion parameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a state-of-the-art framework, by a significant 26%. Additionally, it achieves 57% of the theoretical peak FLOP/s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.13525v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddharth Singh, Prajwal Singhania, Aditya K. Ranjan, Zack Sating, Abhinav Bhatele</dc:creator>
    </item>
    <item>
      <title>Complete Multiparty Session Type Projection with Automata</title>
      <link>https://arxiv.org/abs/2305.17079</link>
      <description>arXiv:2305.17079v3 Announce Type: replace-cross 
Abstract: Multiparty session types (MSTs) are a type-based approach to verifying communication protocols. Central to MSTs is a projection operator: a partial function that maps protocols represented as global types to correct-by-construction implementations for each participant, represented as a communicating state machine. Existing projection operators are syntactic in nature, and trade efficiency for completeness. We present the first projection operator that is sound, complete, and efficient. Our projection separates synthesis from checking implementability. For synthesis, we use a simple automata-theoretic construction; for checking implementability, we present succinct conditions that summarize insights into the property of implementability. We use these conditions to show that MST implementability is in PSPACE. This improves upon a previous decision procedure that is in EXPSPACE and applies to a smaller class of MSTs. We demonstrate the effectiveness of our approach using a prototype implementation, which handles global types not supported by previous work without sacrificing performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17079v3</guid>
      <category>cs.FL</category>
      <category>cs.DC</category>
      <category>cs.PL</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elaine Li, Felix Stutz, Thomas Wies, Damien Zufferey</dc:creator>
    </item>
    <item>
      <title>High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers</title>
      <link>https://arxiv.org/abs/2307.13352</link>
      <description>arXiv:2307.13352v2 Announce Type: replace-cross 
Abstract: Robust distributed learning with Byzantine failures has attracted extensive research interests in recent years. However, most of existing methods suffer from curse of dimensionality, which is increasingly serious with the growing complexity of modern machine learning models. In this paper, we design a new method that is suitable for high dimensional problems, under arbitrary number of Byzantine attackers. The core of our design is a direct high dimensional semi-verified mean estimation method. Our idea is to identify a subspace first. The components of mean value perpendicular to this subspace can be estimated via gradient vectors uploaded from worker machines, while the components within this subspace are estimated using auxiliary dataset. We then use our new method as the aggregator of distributed learning problems. Our theoretical analysis shows that the new method has minimax optimal statistical rates. In particular, the dependence on dimensionality is significantly improved compared with previous works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.13352v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Puning Zhao, Zhiguo Wan</dc:creator>
    </item>
    <item>
      <title>FedSN: A Novel Federated Learning Framework over LEO Satellite Networks</title>
      <link>https://arxiv.org/abs/2311.01483</link>
      <description>arXiv:2311.01483v3 Announce Type: replace-cross 
Abstract: Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, and fully explore data diversity on LEO satellites. Specifically, we first present a novel sub-structure scheme to enable heterogeneous local model training considering different computing, memory, and communication constraints on LEO satellites. Additionally, we propose a pseudo-synchronous model aggregation strategy to dynamically schedule model aggregation for compensating model staleness. To further demonstrate the effectiveness of the FedSN, we evaluate it using space modulation recognition and remote sensing image classification tasks by leveraging the data from real-world satellite networks. Extensive experimental results demonstrate that FedSN framework achieves higher accuracy, lower computing, and communication overhead than the state-of-the-art benchmarks and the effectiveness of each components in FedSN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01483v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, Yue Gao</dc:creator>
    </item>
    <item>
      <title>OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models</title>
      <link>https://arxiv.org/abs/2402.01739</link>
      <description>arXiv:2402.01739v2 Announce Type: replace-cross 
Abstract: To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped. Finally, we rethink our design based on the above-mentioned observations and analysis. To facilitate future MoE LLM development, we propose potential strategies for mitigating the issues we found and further improving off-the-shelf MoE LLM designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01739v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You</dc:creator>
    </item>
    <item>
      <title>Empowering Data Mesh with Federated Learning</title>
      <link>https://arxiv.org/abs/2403.17878</link>
      <description>arXiv:2403.17878v2 Announce Type: replace-cross 
Abstract: The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making. However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing. A new data paradigm, Data Mesh, is proposed to overcome these challenges. Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the federated governance to monitor domains and their data products. Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture. In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains, especially for security-sensitive organizations. To this end, we introduce a pioneering approach that incorporates Federated Learning into Data Mesh. To the best of our knowledge, this is the first open-source applied work that represents a critical advancement toward the integration of federated learning methods into the Data Mesh paradigm, underscoring the promising prospects for privacy-preserving and decentralized data analysis strategies within Data Mesh architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17878v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyuan Li, Salman Toor</dc:creator>
    </item>
  </channel>
</rss>
