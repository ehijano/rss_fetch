<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Sep 2025 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems</title>
      <link>https://arxiv.org/abs/2509.08969</link>
      <description>arXiv:2509.08969v1 Announce Type: new 
Abstract: Distributed systems require robust, scalable identifier schemes to ensure data uniqueness and efficient indexing across multiple nodes. This paper presents a comprehensive analysis of the evolution of distributed identifiers, comparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We combine mathematical calculation of collision probabilities with empirical experiments measuring generation speed and network transmission overhead in a simulated distributed environment. Results demonstrate that ULIDs significantly outperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing generation speed by 97.32%. statistical analysis further shows ULIDs offer a 98.42% lower collision risk compared to UUIDv7, while maintaining negligible collision probabilities even at high generation rates. These findings highlight ULIDs as an optimal choice for high-performance distributed systems, providing efficient, time-ordered, and lexicographically sortable identifiers suitable for scalable applications. All source code, datasets, and analysis scripts utilized in this research are publicly available in our dedicated repository at https://github.com/nimakarimiank/uids-comparison. This repository contains comprehensive documentation of the experimental setup, including configuration files for the distributed environment, producer and consumer implementations, and message broker integration. Additionally, it provides the data scripts and datasets. Researchers and practitioners are encouraged to explore the repository for full reproducibility of the experiments and to facilitate further investigation or extension of the presented work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08969v1</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nima Karimian Kakolaki</dc:creator>
    </item>
    <item>
      <title>Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines</title>
      <link>https://arxiv.org/abs/2509.09058</link>
      <description>arXiv:2509.09058v1 Announce Type: new 
Abstract: Variant calling is the first step in analyzing a human genome and aims to detect variants in an individual's genome compared to a reference genome. Due to the computationally-intensive nature of variant calling, genomic data are increasingly processed in cloud environments as large amounts of compute and storage resources can be acquired with the pay-as-you-go pricing model. In this paper, we address the problem of efficiently executing a variant calling pipeline for a workload of human genomes on graphics processing unit (GPU)-enabled machines. We propose a novel machine learning (ML)-based approach for optimizing the workload execution to minimize the total execution time. Our approach encompasses two key techniques: The first technique employs ML to predict the execution times of different stages in a variant calling pipeline based on the characteristics of a genome sequence. Using the predicted times, the second technique generates optimal execution plans for the machines by drawing inspiration from the flexible job shop scheduling problem. The plans are executed via careful synchronization across different machines. We evaluated our approach on a workload of publicly available genome sequences using a testbed with different types of GPU hardware. We observed that our approach was effective in predicting the execution times of variant calling pipeline stages using ML on features such as sequence size, read quality, percentage of duplicate reads, and average read length. In addition, our approach achieved 2X speedup (on an average) over a greedy approach that also used ML for predicting the execution times on the tested workload of sequences. Finally, our approach achieved 1.6X speedup (on an average) over a dynamic approach that executed the workload based on availability of resources without using any ML-based time predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09058v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajay Kumar, Praveen Rao, Peter Sanders</dc:creator>
    </item>
    <item>
      <title>Coherence-Aware Task Graph Modeling for Realistic Application</title>
      <link>https://arxiv.org/abs/2509.09094</link>
      <description>arXiv:2509.09094v1 Announce Type: new 
Abstract: As multicore systems continue to scale, cache coherence has emerged as a critical determinant of system performance, with coherence behavior and task execution closely intertwined, reshaping inter-task dependencies. Task graph modeling provides a structured way to capture such dependencies and serves as the foundation for many system-level design strategies. However, these strategies typically rely on predefined task graphs, while many real-world applications lack explicit graphs and exhibit dynamic, data-dependent behavior, limiting the effectiveness of static approaches. To address this, several task graph modeling methods for realistic workloads have been developed. Yet, they either rely on implicit techniques that use application-specific features without producing explicit graphs, or they generate graphs tailored to fixed scheduling models, which limits generality. More importantly, they often overlook coherence interactions, creating a gap between design assumptions and actual runtime behavior. To overcome these limitations, we propose CoTAM, a Coherence-Aware Task Graph Modeling framework for realistic workloads that constructs a unified task graph reflecting runtime behavior. CoTAM analyzes the impact of coherence by decoupling its effects from overall execution, quantifies its influence through a learned weighting scheme, and infers inter-task dependencies for coherence-aware graph generation. Extensive experiments show that CoTAM outperforms implicit methods, bridging the gap between dynamic workload behavior and existing designs while demonstrating the importance of incorporating cache coherence into task graph modeling for accurate and generalizable system-level analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09094v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3742875.3754678</arxiv:DOI>
      <arxiv:journal_reference>International Symposium on Formal Methods and Models for System Design (MEMOCODE '25), September 28-October 3, 2025, Taipei, Taiwan</arxiv:journal_reference>
      <dc:creator>Guochu Xiong, Xiangzhong Luo, Weichen Liu</dc:creator>
    </item>
    <item>
      <title>WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge</title>
      <link>https://arxiv.org/abs/2509.09400</link>
      <description>arXiv:2509.09400v1 Announce Type: new 
Abstract: Serverless computing at the edge requires lightweight execution environments to minimize cold start latency, especially in Urgent Edge Computing (UEC). This paper compares WebAssembly and unikernel-based MicroVMs for serverless workloads. We present Limes, a WebAssembly runtime built on Wasmtime, and evaluate it against the Firecracker-based environment used in SPARE. Results show that WebAssembly offers lower cold start times for lightweight functions but suffers with complex workloads, while Firecracker provides higher, but stable, cold starts and better execution performance, particularly for I/O-heavy tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09400v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valerio Besozzi, Enrico Fiasco, Marco Danelutto, Patrizio Dazzi</dc:creator>
    </item>
    <item>
      <title>Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing</title>
      <link>https://arxiv.org/abs/2509.09435</link>
      <description>arXiv:2509.09435v1 Announce Type: new 
Abstract: Collaborative mobile edge computing (MEC) has emerged as a promising paradigm to enable low-capability edge nodes to cooperatively execute computation-intensive tasks. However, straggling edge nodes (stragglers) significantly degrade the performance of MEC systems by prolonging computation latency. While coded distributed computing (CDC) as an effective technique is widely adopted to mitigate straggler effects, existing CDC schemes exhibit two critical limitations: (i) They cannot successfully decode the final result unless the number of received results reaches a fixed recovery threshold, which seriously restricts their flexibility; (ii) They suffer from inherent poles in their encoding/decoding functions, leading to decoding inaccuracies and numerical instability in the computational results. To address these limitations, this paper proposes an approximated CDC scheme based on barycentric rational interpolation. The proposed CDC scheme offers several outstanding advantages. Firstly, it can decode the final result leveraging any returned results from workers. Secondly, it supports computations over both finite and real fields while ensuring numerical stability. Thirdly, its encoding/decoding functions are free of poles, which not only enhances approximation accuracy but also achieves flexible accuracy tuning. Fourthly, it integrates a novel BRI-based gradient coding algorithm accelerating the training process while providing robustness against stragglers. Finally, experimental results reveal that the proposed scheme is superior to existing CDC schemes in both waiting time and approximate accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09435v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Houming Qiu, Kun Zhu, Dusit Niyato, Nguyen Cong Luong, Changyan Yi, Chen Dai</dc:creator>
    </item>
    <item>
      <title>Weaker Assumptions for Asymmetric Trust</title>
      <link>https://arxiv.org/abs/2509.09493</link>
      <description>arXiv:2509.09493v1 Announce Type: new 
Abstract: In distributed systems with asymmetric trust, each participant is free to make its own trust assumptions about others, captured by an asymmetric quorum system. This contrasts with ordinary, symmetric quorum systems and threshold models, where trust assumptions are uniformly shared among participants. Fundamental problems like reliable broadcast and consensus are unsolvable in the asymmetric model if quorum systems satisfy only the classical properties of consistency and availability. Existing approaches overcome this by introducing stronger assumptions. We show that some of these assumptions are overly restrictive, so much so that they effectively eliminate the benefits of asymmetric trust. To address this, we propose a new approach to characterize asymmetric problems and, building upon it, present algorithms for reliable broadcast and consensus that require weaker assumptions than previous solutions. Our methods are general and can be extended to other core problems in systems with asymmetric trust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09493v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ignacio Amores-Sesar, Christian Cachin, Juan Villacis</dc:creator>
    </item>
    <item>
      <title>TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes</title>
      <link>https://arxiv.org/abs/2509.09525</link>
      <description>arXiv:2509.09525v1 Announce Type: new 
Abstract: Serverless computing provides dynamic scalability, but its infrastructure overhead becomes a bottleneck for emerging workloads such as LLM agents, which exhibit unpredictable invocation patterns and variable resource demands. Our analysis shows that for these agents, the cost of running on serverless platforms can reach up to 70% of the cost of LLM API calls. This finding motivates the need for a more efficient, high-density serverless platform. We present TrEnv, a co-designed serverless platform that supports both container- and VM-based environments, optimized for the unique demands of LLM agents. TrEnv reduces startup latency and memory usage through repurposable sandboxes and memory templates, which enable fast reuse and restoration of execution environments. To further reduce overhead in VM-based agent workloads, TrEnv leverages browser sharing and a page cache bypassing mechanism. Evaluations show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in container-based settings, and achieves up to 58% lower P99 latency and 61% memory savings for VM-based agents compared to state-of-the-art systems like E2B.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09525v1</guid>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jialiang Huang, Teng Ma, Zheng Liu, Sixing Lin, Kang Chen, Jinlei Jiang, Xia Liao, Yingdi Shan, Yongwei Wu, Ning Zhang, Mengting Lu, Tao Ma, Haifeng Gong, Mingxing Zhang</dc:creator>
    </item>
    <item>
      <title>HARD: A Performance Portable Radiation Hydrodynamics Code based on FleCSI Framework</title>
      <link>https://arxiv.org/abs/2509.08971</link>
      <description>arXiv:2509.08971v1 Announce Type: cross 
Abstract: Hydrodynamics And Radiation Diffusion} (HARD) is an open-source application for high-performance simulations of compressible hydrodynamics with radiation-diffusion coupling. Built on the FleCSI (Flexible Computational Science Infrastructure) framework, HARD expresses its computational units as tasks whose execution can be orchestrated by multiple back-end runtimes, including Legion, MPI, and HPX. Node-level parallelism is delegated to Kokkos, providing a single, portable code base that runs efficiently on laptops, small homogeneous clusters, and the largest heterogeneous supercomputers currently available. To ensure scientific reliability, HARD includes a regression-test suite that automatically reproduces canonical verification problems such as the Sod and LeBlanc shock tubes and the Sedov blast wave, comparing numerical solutions against known analytical results. The project is distributed under an OSI-approved license, hosted on GitHub, and accompanied by reproducible build scripts and continuous integration workflows. This combination of performance portability, verification infrastructure, and community-focused development makes HARD a sustainable platform for advancing radiation hydrodynamics research across multiple domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08971v1</guid>
      <category>physics.comp-ph</category>
      <category>astro-ph.IM</category>
      <category>cs.DC</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Loiseau, Hyun Lim, Andr\'es Yag\"ue L\'opez, Mammadbaghir Baghirzade, Shihab Shahriar Khan, Yoonsoo Kim, Sudarshan Neopane, Alexander Strack, Farhana Taiyebah, Benjamin K. Bergen</dc:creator>
    </item>
    <item>
      <title>ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning</title>
      <link>https://arxiv.org/abs/2509.09534</link>
      <description>arXiv:2509.09534v1 Announce Type: cross 
Abstract: Federated Learning (FL) emerged as a widely studied paradigm for distributed learning. Despite its many advantages, FL remains vulnerable to adversarial attacks, especially under data heterogeneity. We propose a new Byzantine-robust FL algorithm called ProDiGy. The key novelty lies in evaluating the client gradients using a joint dual scoring system based on the gradients' proximity and dissimilarity. We demonstrate through extensive numerical experiments that ProDiGy outperforms existing defenses in various scenarios. In particular, when the clients' data do not follow an IID distribution, while other defense mechanisms fail, ProDiGy maintains strong defense capabilities and model accuracy. These findings highlight the effectiveness of a dual perspective approach that promotes natural similarity among honest clients while detecting suspicious uniformity as a potential indicator of an attack.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09534v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sena Ergisi, Luis Ma{\ss}ny, Rawad Bitar</dc:creator>
    </item>
    <item>
      <title>Towards A High-Performance Quantum Data Center Network Architecture</title>
      <link>https://arxiv.org/abs/2509.09653</link>
      <description>arXiv:2509.09653v1 Announce Type: cross 
Abstract: Quantum Data Centers (QDCs) are needed to support large-scale quantum processing for both academic and commercial applications. While large-scale quantum computers are constrained by technological and financial barriers, a modular approach that clusters small quantum computers offers an alternative. This approach, however, introduces new challenges in network scalability, entanglement generation, and quantum memory management. In this paper, we propose a three-layer fat-tree network architecture for QDCs, designed to address these challenges. Our architecture features a unique leaf switch and an advanced swapping spine switch design, optimized to handle high volumes of entanglement requests as well as a queue scheduling mechanism that efficiently manages quantum memory to prevent decoherence. Through queuing-theoretical models and simulations in NetSquid, we demonstrate the proposed architecture's scalability and effectiveness in maintaining high entanglement fidelity, offering a practical path forward for modular QDC networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09653v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yufeng Xin, Liang Zhang</dc:creator>
    </item>
    <item>
      <title>DistTrain: Addressing Model and Data Heterogeneity with Disaggregated Training for Multimodal Large Language Models</title>
      <link>https://arxiv.org/abs/2408.04275</link>
      <description>arXiv:2408.04275v3 Announce Type: replace 
Abstract: Multimodal large language models (LLMs) empower LLMs to ingest inputs and generate outputs in multiple forms, such as text, image, and audio. However, the integration of multiple modalities introduces heterogeneity in both the model and training data, creating unique systems challenges.
  We propose DistTrain, a disaggregated training system for multimodal LLMs. DistTrain incorporates two novel disaggregation techniques to address model and data heterogeneity, respectively. The first is disaggregated model orchestration, which separates the training for modality encoder, LLM backbone, and modality generator. This allows the three components to adaptively and independently orchestrate their resources and parallelism configurations. The second is disaggregated data preprocessing, which decouples data preprocessing from training. This eliminates resource contention between preprocessing and training, and enables efficient data reordering to mitigate stragglers within and between microbatches caused by data heterogeneity. We evaluate DistTrain across different sizes of multimodal LLMs on a large-scale production cluster. The experimental results show that DistTrain achieves 54.7% Model FLOPs Utilization (MFU) when training a 72B multimodal LLM on 1172 GPUs and outperforms Megatron-LM by up to 2.2x on training throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04275v3</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3718958.3750472</arxiv:DOI>
      <arxiv:journal_reference>SIGCOMM'25: Proceedings of the ACM SIGCOMM 2025 Conference Pages 24-38</arxiv:journal_reference>
      <dc:creator>Zili Zhang, Yinmin Zhong, Yimin Jiang, Hanpeng Hu, Jianjian Sun, Zheng Ge, Yibo Zhu, Daxin Jiang, Xin Jin</dc:creator>
    </item>
    <item>
      <title>Asynchronous-Many-Task Systems: Challenges and Opportunities -- Scaling an AMR Astrophysics Code on Exascale machines using Kokkos and HPX</title>
      <link>https://arxiv.org/abs/2412.15518</link>
      <description>arXiv:2412.15518v2 Announce Type: replace 
Abstract: Dynamic and adaptive mesh refinement is pivotal in high-resolution, multi-physics, multi-model simulations, necessitating precise physics resolution in localized areas across expansive domains. Today's supercomputers' extreme heterogeneity presents a significant challenge for dynamically adaptive codes, highlighting the importance of achieving performance portability at scale. Our research focuses on astrophysical simulations, particularly stellar mergers, to elucidate early universe dynamics. We present Octo-Tiger, leveraging Kokkos, HPX, and SIMD for portable performance at scale in complex, massively parallel adaptive multi-physics simulations. Octo-Tiger supports diverse processors, accelerators, and network backends. Experiments demonstrate exceptional scalability across several heterogeneous supercomputers including Perlmutter, Frontier, and Fugaku, encompassing major GPU architectures and x86, ARM, and RISC-V CPUs. Parallel efficiency of 47.59% (110,080 cores and 6880 hybrid A100 GPUs) on a full-system run on Perlmutter (26% HPCG peak performance) and 51.37% (using 32,768 cores and 2,048 MI250X) on Frontier are achieved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15518v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregor Dai\ss, Patrick Diehl, Jiakun Yan, John K. Holmen, Rahulkumar Gayatri, Christoph Junghans, Alexander Straub, Jeff R. Hammond, Dominic Marcello, Miwako Tsuji, Dirk Pfl\"uger, Hartmut Kaiser</dc:creator>
    </item>
    <item>
      <title>An Autonomy Loop for Dynamic HPC Job Time Limit Adjustment</title>
      <link>https://arxiv.org/abs/2505.05927</link>
      <description>arXiv:2505.05927v2 Announce Type: replace 
Abstract: High Performance Computing (HPC) systems rely on fixed user-provided estimates of job time limits. These estimates are often inaccurate, resulting in inefficient resource use and the loss of unsaved work if a job times out shortly before reaching its next checkpoint. This work proposes a novel feedback-driven autonomy loop that dynamically adjusts HPC job time limits based on checkpoint progress reported by applications. Our approach monitors checkpoint intervals and queued jobs, enabling informed decisions to either early cancel a job after its last completed checkpoint or extend the time limit sufficiently to accommodate the next checkpoint. The objective is to minimize tail waste, that is, the computation that occurs between the last checkpoint and the termination of a job, which is not saved and hence wasted. Through experiments conducted on a subset of a production workload trace, we show a 95% reduction of tail waste, which equates to saving approximately 1.3% of the total CPU time that would otherwise be wasted. We propose various policies that combine early cancellation and time limit extension, achieving tail waste reduction while improving scheduling metrics such as weighted average job wait time. This work contributes an autonomy loop for improved scheduling in HPC environments, where system job schedulers and applications collaborate to significantly reduce resource waste and improve scheduling performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05927v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Jakobsche, Osman Seckin Simsek, Jim Brandt, Ann Gentile, Florina M. Ciorba</dc:creator>
    </item>
    <item>
      <title>Universal Workers: A Vision for Eliminating Cold Starts in Serverless Computing</title>
      <link>https://arxiv.org/abs/2505.19880</link>
      <description>arXiv:2505.19880v2 Announce Type: replace 
Abstract: Serverless computing enables developers to deploy code without managing infrastructure, but suffers from cold start overhead when initializing new function instances. Existing solutions such as "keep-alive" or "pre-warming" are costly and unreliable under bursty workloads. We propose universal workers, which are computational units capable of executing any function with minimal initialization overhead. Based on an analysis of production workload traces, our key insight is that requests in Function-as-a-Service (FaaS) platforms show a highly skewed distribution, with most requests invoking a small subset of functions. We exploit this observation to approximate universal workers through locality groups and three-tier caching (handler, install, import). With this work, we aim to enable more efficient and scalable FaaS platforms capable of handling diverse workloads with minimal initialization overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19880v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/CLOUD67622.2025.00051</arxiv:DOI>
      <dc:creator>Saman Akbari, Manfred Hauswirth</dc:creator>
    </item>
    <item>
      <title>Beyond Pairwise Comparisons: Unveiling Structural Landscape of Mobile Robot Models</title>
      <link>https://arxiv.org/abs/2508.19805</link>
      <description>arXiv:2508.19805v3 Announce Type: replace 
Abstract: Understanding the computational power of mobile robot systems is a fundamental challenge in distributed computing. While prior work has focused on pairwise separations between models, we explore how robot capabilities, light observability, and scheduler synchrony interact in more complex ways.
  We first show that the Exponential Times Expansion (ETE) problem is solvable only in the strongest model -- fully-synchronous robots with full mutual lights ($\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and TAR(d)* problems to demonstrate how internal memory and lights interact with synchrony: under weak synchrony, internal memory alone is insufficient, while full synchrony can substitute for both lights and memory.
  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and ZCC to show fine-grained separations between $\mathcal{FSTA}$ and $\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and Leave Place Convergence (LP-Cv), illustrating the limitations of internal memory in symmetric settings.
  These results extend the known separation map of 14 canonical robot models, revealing structural phenomena only visible through higher-order comparisons. Our work provides new impossibility criteria and deepens the understanding of how observability, memory, and synchrony collectively shape the computational power of mobile robots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19805v3</guid>
      <category>cs.DC</category>
      <category>cs.RO</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shota Naito, Tsukasa Ninomiya, Koichi Wada</dc:creator>
    </item>
    <item>
      <title>Harmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems</title>
      <link>https://arxiv.org/abs/2503.20507</link>
      <description>arXiv:2503.20507v3 Announce Type: replace-cross 
Abstract: Hybrid storage systems (HSS) integrate multiple storage devices with diverse characteristics to deliver high performance and capacity at low cost. The performance of an HSS highly depends on the effectiveness of two key policies: (1) the data-placement policy, which determines the best-fit storage device for incoming data, and (2) the data-migration policy, which dynamically rearranges stored data (i.e., prefetches hot data and evicts cold data) across the devices to sustain high HSS performance. Prior works optimize either data placement or data migration in isolation, which leads to suboptimal HSS performance. Unfortunately, no prior work tries to optimize both policies together.
  Our goal is to design a holistic data-management technique that optimizes both data-placement and data-migration policies to fully exploit the potential of an HSS, and thus significantly improve system performance. We propose Harmonia, a multi-agent reinforcement learning (RL)-based data-management technique that employs two lightweight autonomous RL agents, a data-placement agent and a data-migration agent, that adapt their policies for the current workload and HSS configuration while coordinating with each other to improve overall HSS performance.
  We evaluate Harmonia on real HSS configurations with up to four heterogeneous storage devices and seventeen data-intensive workloads. On performance-optimized (cost-optimized) HSS with two storage devices, Harmonia outperforms the best-performing prior approach by 49.5% (31.7%) on average. On an HSS with three (four) devices, Harmonia outperforms the best-performing prior work by 37.0% (42.0%) on average. Harmonia's performance benefits come with low latency (240ns for inference) and storage overheads (206 KiB in DRAM for both RL agents combined). We will open-source Harmonia's implementation to aid future research on HSS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20507v3</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rakesh Nadig, Vamanan Arulchelvan, Rahul Bera, Taha Shahroodi, Gagandeep Singh, Andreas Kakolyris, Mohammad Sadrosadati, Jisung Park, Onur Mutlu</dc:creator>
    </item>
    <item>
      <title>Byzantine-Robust Federated Learning Using Generative Adversarial Networks</title>
      <link>https://arxiv.org/abs/2503.20884</link>
      <description>arXiv:2503.20884v3 Announce Type: replace-cross 
Abstract: Federated learning (FL) enables collaborative model training across distributed clients without sharing raw data, but its robustness is threatened by Byzantine behaviors such as data and model poisoning. Existing defenses face fundamental limitations: robust aggregation rules incur error lower bounds that grow with client heterogeneity, while detection-based methods often rely on heuristics (e.g., a fixed number of malicious clients) or require trusted external datasets for validation. We present a defense framework that addresses these challenges by leveraging a conditional generative adversarial network (cGAN) at the server to synthesize representative data for validating client updates. This approach eliminates reliance on external datasets, adapts to diverse attack strategies, and integrates seamlessly into standard FL workflows. Extensive experiments on benchmark datasets demonstrate that our framework accurately distinguishes malicious from benign clients while maintaining overall model accuracy. Beyond Byzantine robustness, we also examine the representativeness of synthesized data, computational costs of cGAN training, and the transparency and scalability of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20884v3</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Usama Zafar, Andr\'e M. H. Teixeira, Salman Toor</dc:creator>
    </item>
    <item>
      <title>Efficient Optimization Accelerator Framework for Multistate Ising Problems</title>
      <link>https://arxiv.org/abs/2505.20250</link>
      <description>arXiv:2505.20250v2 Announce Type: replace-cross 
Abstract: Ising Machines are emerging hardware architectures that efficiently solve NP-Hard combinatorial optimization problems. Generally, combinatorial problems are transformed into quadratic unconstrained binary optimization (QUBO) form, but this transformation often complicates the solution landscape, degrading performance, especially for multi-state problems. To address this challenge, we model spin interactions as generalized boolean logic function to significantly reduce the exploration space. We demonstrate the effectiveness of our approach on graph coloring problem using probabilistic Ising solvers, achieving similar accuracy compared to state-of-the-art heuristics and machine learning algorithms. It also shows significant improvement over state-of-the-art QUBO-based Ising solvers, including probabilistic Ising and simulated bifurcation machines. We also design 1024-neuron all-to-all connected probabilistic Ising accelerator on FPGA with the proposed approach that shows ~10000x performance acceleration compared to GPU-based Tabucol heuristics and reducing physical neurons by 1.5-4x over baseline Ising frameworks. Thus, this work establishes superior efficiency, scalability and solution quality for multi-state optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20250v2</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chirag Garg, Sayeef Salahuddin</dc:creator>
    </item>
  </channel>
</rss>
