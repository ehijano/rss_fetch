<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Jan 2026 05:00:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A penalized heteroskedastic ordered probit model for DIF (measurement invariance) testing of single-item assessments in cross-cultural research</title>
      <link>https://arxiv.org/abs/2601.18889</link>
      <description>arXiv:2601.18889v1 Announce Type: new 
Abstract: Differential item functioning (DIF) or measurement invariance (MI) testing for single-item assessments has previously been impossible. Part of the issue is that there are no conditioning variables to serve as a proxy for the latent variable--regression-based DIF methods. Another reason is that factor-analytic approaches require multiple items to estimate parameters. In this technical working paper, I propose an approach for evaluating DIF/MI in a single-item assessment of a construct. The current methods should NOT replace using multiple-indicator MG-CFA/IRT analyses of DIF/MI or regression mased methods when possible. More items generally provide significantly better construct coverage and provide more rigorous DIF/MI evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18889v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R Noah Padgett</dc:creator>
    </item>
    <item>
      <title>Embedding Birth-Death Processes within a Dynamic Stochastic Block Model</title>
      <link>https://arxiv.org/abs/2601.19277</link>
      <description>arXiv:2601.19277v1 Announce Type: new 
Abstract: Statistical clustering in dynamic networks aims to identify groups of nodes with similar or distinct internal connectivity patterns as the network evolves over time. While early research primarily focused on static Stochastic Block Models (SBMs), recent advancements have extended these models to handle dynamic and weighted networks, allowing for a more accurate representation of temporal variations in structure. Additional developments have introduced methods for detecting structural changes, such as shifts in community membership. However, limited attention has been paid to dynamic networks with variable population sizes, where nodes may enter or exit the network. To address this gap, we propose an extension of dynamic SBMs (dSBMs) that incorporates a birth-death process, enabling the statistical clustering of nodes in dynamic networks with evolving population sizes. This work makes three main contributions: (1) the introduction of a novel model for dSBMs with birth-death processes, (2) a framework for parameter inference and prediction of latent communities in this model, and (3) the development of an adapted Variational Expectation-Maximization (VEM) algorithm for efficient inference within this extended framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19277v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriela Bayolo Soler (UTC), Miraine D\'avila Felipe (UTC), Ghislaine Gayraud (UTC)</dc:creator>
    </item>
    <item>
      <title>Adaptive L-tests for high dimensional independence</title>
      <link>https://arxiv.org/abs/2601.19688</link>
      <description>arXiv:2601.19688v1 Announce Type: new 
Abstract: Testing mutual independence among multiple random variables is a fundamental problem in statistics, with wide applications in genomics, finance, and neuroscience. In this paper, we propose a new class of tests for high-dimensional mutual independence based on $L$-statistics. We establish the asymptotic distribution of the proposed test when the order parameter $k$ is fixed, and prove asymptotic normality when $k$ diverges with the dimension. Moreover, we show the asymptotic independence of the fixed-$k$ and diverging-$k$ statistics, enabling their combination through the Cauchy method. The resulting adaptive test is both theoretically justified and practically powerful across a wide range of alternatives. Simulation studies demonstrate the advantages of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19688v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ping Zhao, Huifang Ma</dc:creator>
    </item>
    <item>
      <title>Is gelation a singularity or a flow induced instability?</title>
      <link>https://arxiv.org/abs/2601.18806</link>
      <description>arXiv:2601.18806v1 Announce Type: cross 
Abstract: Gelation in the Smoluchowski coagulation equation is commonly interpreted as a finite-time singularity marked by mass loss or moment divergence. We instead characterize gelation as a loss of dynamical stability of the Smoluchowski flow, quantified through the time-dependent spectrum of the Jacobian along the evolving aggregation dynamics. Studying homogeneous kernels $K(i,j)=(ij)^{\alpha}$ together with the classical Smoluchowski, we show that gelation is consistently preceded by the appearance of positive real eigenvalues, indicating a loss of local dynamical stability. While non-gelling kernels exhibit only transient finite-size effects, gelling kernels display persistent spectral destabilization associated with macroscopic gel formation. Our results identify gelation as a genuine dynamical instability of the Smoluchowski flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18806v1</guid>
      <category>cond-mat.soft</category>
      <category>nlin.AO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Manuel Dedola, Ludovico Cademartiri</dc:creator>
    </item>
    <item>
      <title>Local Variable and Neighborhood Selection for Firearm Fatality in the Southeast USA</title>
      <link>https://arxiv.org/abs/2601.19044</link>
      <description>arXiv:2601.19044v1 Announce Type: cross 
Abstract: A major public health concern in the United States (US) is gun-related deaths. The number of gun injuries largely varies spatially because of county-wise heterogeneity of race, sex, age, and income distributions. But still, a major challenge is to locally identify the influential socio-economic factors behind these firearm fatality incidents. For a diverging number of predictors, a rich literature exists regarding SCAD under the independence framework; however, a vacuum remains when discussing local variable selection for spatially correlated, over-dispersed data. This research presents a two-step localized variable selection and inference framework for spatially indexed gunshot fatality data. In the first step, we select variables locally using the SCAD penalty for specific locations where the number of gunshot incidents exceeds a threshold. For these locations, after selecting the predictors, we proceed to the next step, which involves examining the directional variation in the latent spatial neighborhood structure. We further discuss the theoretical properties of this county-specific local variable selection under infill asymptotics. This method has threefold advantages: (i) this method selects the variables locally, (ii) this method provides inference about directional variation of a selected predictor, and (iii) instead of assuming the spatial neighborhood structure in an ad hoc manner, this method identifies the specific type of spatial neighborhood structure that is most appropriate for modeling the random effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19044v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debjoy Thakur, Lingyuan Zhao, Soutir Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Optimized $k$-means color quantization of digital images in machine-based and human perception-based colorspaces</title>
      <link>https://arxiv.org/abs/2601.19117</link>
      <description>arXiv:2601.19117v1 Announce Type: cross 
Abstract: Color quantization represents an image using a fraction of its original number of colors while only minimally losing its visual quality. The $k$-means algorithm is commonly used in this context, but has mostly been applied in the machine-based RGB colorspace composed of the three primary colors. However, some recent studies have indicated its improved performance in human perception-based colorspaces. We investigated the performance of $k$-means color quantization at four quantization levels in the RGB, CIE-XYZ, and CIE-LUV/CIE-HCL colorspaces, on 148 varied digital images spanning a wide range of scenes, subjects and settings. The Visual Information Fidelity (VIF) measure numerically assessed the quality of the quantized images, and showed that in about half of the cases, $k$-means color quantization is best in the RGB space, while at other times, and especially for higher quantization levels ($k$), the CIE-XYZ colorspace is where it usually does better. There are also some cases, especially at lower $k$, where the best performance is obtained in the CIE-LUV colorspace. Further analysis of the performances in terms of the distributions of the hue, chromaticity and luminance in an image presents a nuanced perspective and characterization of the images for which each colorspace is better for $k$-means color quantization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19117v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ranjan Maitra</dc:creator>
    </item>
    <item>
      <title>Latent characterisation of the complete BATSE gamma ray bursts catalogue using Gaussian mixture of factor analysers and model-estimated overlap-based syncytial clustering</title>
      <link>https://arxiv.org/abs/2601.19140</link>
      <description>arXiv:2601.19140v1 Announce Type: cross 
Abstract: Characterising and distinguishing gamma-ray bursts (GRBs) has interested astronomers for many decades. While some authors have found two or three groups of GRBs by analyzing only a few parameters, recent work identified five ellipsoidally-shaped groups upon considering nine parameters $T_{50}, T_{90}, F_1, F_2, F_3, F_4, P_{64}, P_{256}, P_{1024}$. Yet others suggest sub-classes within the two or three groups found earlier. Using a mixture model of Gaussian factor analysers, we analysed 1150 GRBs, that had nine parameters observed, from the current Burst and Transient Source Experiment (BATSE) catalogue, and again established five ellipsoidal-shaped groups to describe the GRBs. These five groups are characterised in terms of their average duration, fluence and spectrum as shorter-faint-hard, long-intermediate-soft, long-intermediate-intermediate, long-bright-intermediate and short-faint-hard. The use of factor analysers in describing individual group densities allows for a more thorough group-wise characterisation of the parameters in terms of a few latent features. However, given the discrepancy with many other existing studies that advocated for two or three groups, we also performed model-estimated overlap-based syncytial clustering (MOBSynC) that successively merges poorer-separated groups. The five ellipsoidal groups merge into three and then into two groups, one with GRBs of low durations and the other having longer duration GRBs. These groups are also characterised in terms of a few latent factors made up of the nine parameters. Our analysis provides context for all three sets of results, and in doing so, details a multi-layered characterisation of the BATSE GRBs, while also explaining the structure in their variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19140v1</guid>
      <category>astro-ph.HE</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/mnras/stae2548</arxiv:DOI>
      <arxiv:journal_reference>Monthly Notices of the Royal Astronomical Society 535 (2024) 3396-3409</arxiv:journal_reference>
      <dc:creator>Fan Dai, Ranjan Maitra</dc:creator>
    </item>
    <item>
      <title>Almanac: HMC sampling with bounded velocity</title>
      <link>https://arxiv.org/abs/2601.19390</link>
      <description>arXiv:2601.19390v1 Announce Type: cross 
Abstract: In Hamiltonian Monte Carlo sampling, the shape of the potential and the choice of the momentum distribution jointly give rise to the Hamiltonian dynamics of the sampler. An efficient sampler propagates quickly in all regions of the parameter space, so that the chain has a low autocorrelation length and the sampler has a high acceptance rate, with the goal of optimising the number of near-independent samples for given computational cost. Standard Gaussian momentum distributions allow arbitrarily large velocities, which can lead to inefficient exploration in posteriors with ridges or funnel-like geometries. We investigate alternative momentum distributions based on relativistic and Student's t kinetic energies, which naturally limit particle velocities and may improve robustness. Using Almanac, a sampler for cosmological posterior distributions of sky maps and power spectra on the sphere, we test these alternatives in both low- and high-dimensional settings. We find that the choice of parameterization and momentum distribution can improve convergence and effective sample rate, though the achievable gains are generally modest and strongly problem-dependent, reaching up to an order of magnitude in favorable cases. Among the momentum distributions that we tested, those with moderately heavy tails achieved the best balance between efficiency and stability. These results highlight the importance of sampler design and encourage future work on adaptive and self-tuning strategies for kinetic energy parameter optimization in high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19390v1</guid>
      <category>astro-ph.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javier Silva Lafaurie, Lorne Whiteway, Elena Sellentin, Kutay Nazli, Andrew H. Jaffe, Alan F. Heavens, Arthur Loureiro</dc:creator>
    </item>
    <item>
      <title>Comparing how Large Language Models perform against keyword-based searches for social science research data discovery</title>
      <link>https://arxiv.org/abs/2601.19559</link>
      <description>arXiv:2601.19559v1 Announce Type: cross 
Abstract: This paper evaluates the performance of a large language model (LLM) based semantic search tool relative to a traditional keyword-based search for data discovery. Using real-world search behaviour, we compare outputs from a bespoke semantic search system applied to UKRI data services with the Consumer Data Research Centre (CDRC) keyword search. Analysis is based on 131 of the most frequently used search terms extracted from CDRC search logs between December 2023 and October 2024. We assess differences in the volume, overlap, ranking, and relevance of returned datasets using descriptive statistics, qualitative inspection, and quantitative similarity measures, including exact dataset overlap, Jaccard similarity, and cosine similarity derived from BERT embeddings. Results show that the semantic search consistently returns a larger number of results than the keyword search and performs particularly well for place based, misspelled, obscure, or complex queries. While the semantic search does not capture all keyword based results, the datasets returned are overwhelmingly semantically similar, with high cosine similarity scores despite lower exact overlap. Rankings of the most relevant results differ substantially between tools, reflecting contrasting prioritisation strategies. Case studies demonstrate that the LLM based tool is robust to spelling errors, interprets geographic and contextual relevance effectively, and supports natural-language queries that keyword search fails to resolve. Overall, the findings suggest that LLM driven semantic search offers a substantial improvement for data discovery, complementing rather than fully replacing traditional keyword-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19559v1</guid>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Green, Maura Halstead, Caroline Jay, Richard Kingston, Alex Singleton, David Topping</dc:creator>
    </item>
    <item>
      <title>Cross-Domain Offshore Wind Power Forecasting: Transfer Learning Through Meteorological Clusters</title>
      <link>https://arxiv.org/abs/2601.19674</link>
      <description>arXiv:2601.19674v1 Announce Type: cross 
Abstract: Ambitious decarbonisation targets are catalysing growth in orders of new offshore wind farms. For these newly commissioned plants to run, accurate power forecasts are needed from the onset. These allow grid stability, good reserve management and efficient energy trading. Despite machine learning models having strong performances, they tend to require large volumes of site-specific data that new farms do not yet have. To overcome this data scarcity, we propose a novel transfer learning framework that clusters power output according to covariate meteorological features. Rather than training a single, general-purpose model, we thus forecast with an ensemble of expert models, each trained on a cluster. As these pre-trained models each specialise in a distinct weather pattern, they adapt efficiently to new sites and capture transferable, climate-dependent dynamics. Through the expert models' built-in calibration to seasonal and meteorological variability, we remove the industry-standard requirement of local measurements over a year. Our contributions are two-fold - we propose this novel framework and comprehensively evaluate it on eight offshore wind farms, achieving accurate cross-domain forecasting with under five months of site-specific data. Our experiments achieve a MAE of 3.52\%, providing empirical verification that reliable forecasts do not require a full annual cycle. Beyond power forecasting, this climate-aware transfer learning method opens new opportunities for offshore wind applications such as early-stage wind resource assessment, where reducing data requirements can significantly accelerate project development whilst effectively mitigating its inherent risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19674v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dominic Weisser, Chlo\'e Hashimoto-Cullen, Benjamin Guedj</dc:creator>
    </item>
    <item>
      <title>Coarsened data in small area estimation: a Bayesian two-part model for mapping smoking behaviour</title>
      <link>https://arxiv.org/abs/2601.19729</link>
      <description>arXiv:2601.19729v1 Announce Type: cross 
Abstract: Estimating health indicators for restricted sub-populations is a recurring challenge in epidemiology and public health. When survey data are used, Small Area Estimation (SAE) methods can improve precision by borrowing strength across domains. In many applications, however, outcomes are self-reported and affected by coarsening mechanisms, such as rounding and digit preference, that reduce data resolution and may bias inference. This paper addresses both issues by developing a Bayesian unit-level SAE framework for semi-continuous, coarsened responses. Motivated by the 2019 Italian European Health Interview Survey, we estimate smoking indicators for domains defined by the cross-classification of Italian regions and age groups, capturing both smoking prevalence and intensity. The model adopts a two-part structure: a logistic component for smoking prevalence and a flexible mixture of Lognormal distributions for average cigarette consumption, coupled with an explicit model for coarsening and topcoding. Simulation studies show that ignoring coarsening can yield biased and unstable domain estimates with poor interval coverage, whereas the proposed model improves accuracy and achieves near-nominal coverage. The empirical application provides a detailed picture of smoking patterns across region-age domains, helping to characterize the dynamics of the phenomenon and inform targeted public health policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19729v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aldo Gardini, Lorenzo Mori</dc:creator>
    </item>
    <item>
      <title>Abundance and Economic diversity as a descriptor of cities' economic complexity</title>
      <link>https://arxiv.org/abs/2601.19814</link>
      <description>arXiv:2601.19814v1 Announce Type: cross 
Abstract: Intricate interactions among firms, institutions, and spatial structures shape urban economic systems. In this study, we propose a framework based on three structural dimensions -- abundance, diversity, and longevity (ADL) of economic units -- as proxies of urban economic complexity and resilience. Using a decade of georeferenced firm-level data from Mexico City, we analyze the relationships among ADL variables using regression, spatial correlation, and time-series clustering. Our results reveal nonlinear dynamics across urban space, with powerlaw behavior in central zones and logarithmic saturation in peripheral areas, suggesting differentiated growth regimes. Notably, firm longevity modulates the relationship between abundance and diversity, particularly in periurban transition zones. These spatial patterns point to an emerging polycentric restructuring within a traditionally monocentric metropolis. By integrating economic complexity theory with spatial analysis, our approach provides a scalable method to assess the adaptive capacity of urban economies. This has implications for understanding informality, designing inclusive urban policies, and navigating structural transitions in rapidly urbanizing regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19814v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco A. Rosas Pulido, Roberto Murcio, Omar R. V\'azquez, Carlos Gershenson</dc:creator>
    </item>
    <item>
      <title>Personalized Treatment Hierarchies in Bayesian Network Meta-Analysis</title>
      <link>https://arxiv.org/abs/2601.19836</link>
      <description>arXiv:2601.19836v1 Announce Type: cross 
Abstract: Network Meta-Analysis (NMA) is an increasingly popular evidence synthesis tool that can provide a ranking of competing treatments, also known as a treatment hierarchy. Treatment-Covariate Interactions (TCIs) can be included in NMA models to allow relative treatment effects to vary with covariate values. We show that in an NMA model that includes TCIs, treatment hierarchies should be created with a particular covariate profile in mind. We outline the typical approach for creating a treatment hierarchy in standard Bayesian NMA and show how a treatment hierarchy for a particular covariate profile can be created from an NMA model that estimates TCIs. We demonstrate our methods using a real network of studies for treatments of major depressive disorder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19836v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Augustine Wigle, Erica E. M. Moodie</dc:creator>
    </item>
    <item>
      <title>Modeling Two-Scale Rank Distributions via Redistribution Dynamics or an Analytic Derivation of the Beta Rank Function</title>
      <link>https://arxiv.org/abs/2601.19859</link>
      <description>arXiv:2601.19859v1 Announce Type: cross 
Abstract: Beta Rank Function (BRF) is a two-sided distribution characterized by a smooth peak and double powerlaw decay, widely used to model empirical data exhibiting deviations from pure power laws. In this paper, we introduce a novel two-step generative process that produces data exactly following the BRF distribution. The first step involves any mechanism generating a power-law distribution, while the second step applies a regressive redistribution process that reallocates resources from poorer to richer entities, thereby amplifying inequality. This approach represents the first analytic derivation of an exact BRF distribution from a generative mechanism. We validate the model through applications to income and urban population distributions. Beyond exact generation, this framework offers new insights into the systemic origins of deviations from power laws frequently observed in complex systems, linking rank distributions to underlying feedback and redistribution dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19859v1</guid>
      <category>physics.soc-ph</category>
      <category>nlin.AO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Fontanelli, Wentian Li</dc:creator>
    </item>
    <item>
      <title>On the PM2.5 -- Mortality Association: A Bayesian Model for Spatio-Temporal Confounding</title>
      <link>https://arxiv.org/abs/2405.16106</link>
      <description>arXiv:2405.16106v3 Announce Type: replace-cross 
Abstract: In epidemiological studies of air pollution and public health, estimating the health impact of exposure to air pollution may be hindered by the unknown functional form of the exposure-outcome association and by unmeasured confounding factors that are linked to both exposure and outcome. These challenges are especially relevant in spatio-temporal analyses, where their joint exploration remains limited. To study the effects of fine particulate matter on mortality among elderly people in Italy, we propose a Bayesian spatial dynamic generalized linear model that captures the non-linear exposure-outcome association and decomposes the exposure effect across fine and coarse spatio-temporal scales of variation. Together, these features allow reducing the spatio-temporal confounding bias and recovering the shape of the association, as demonstrated through simulation studies. The real-data analysis reveals a clear temporal pattern in the exposure effect, with peaks during summer months. We argue that this finding may be due to interactions of particulate matter with air temperature and unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16106v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlo Zaccardi, Pasquale Valentini, Luigi Ippoliti, Alexandra M. Schmidt</dc:creator>
    </item>
    <item>
      <title>Difference-in-Discontinuities: Estimation, Inference and Validity Tests</title>
      <link>https://arxiv.org/abs/2405.18531</link>
      <description>arXiv:2405.18531v2 Announce Type: replace-cross 
Abstract: This paper provides a formal econometric framework behind the newly developed difference-in-discontinuities design (DiDC). Despite its increasing use in applied research, there are currently limited studies of its properties. We formalize the theory behind the difference-in-discontinuity approach by stating the identification assumptions, proposing a nonparametric estimator, and deriving its asymptotic properties. We also provide comprehensive tests for one of the identification assumption of the DiDC and sensitivity analysis methods that allow researchers to evaluate the robustness of DiDC estimates under violations of the identifying assumptions. Monte Carlo simulation studies show that the estimators have desirable finite-sample properties. Finally, we revisit Grembi et al. (2016), which studies the effects of relaxing fiscal rules on public finance outcomes. Our results show that most of the qualitative takeaways of the original work are robust to time-varying confounding effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18531v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Picchetti, Cristine C. X. Pinto, Stephanie T. Shinoki</dc:creator>
    </item>
    <item>
      <title>Classical JAK2V617F+ Myeloproliferative Neoplasms emergence and development based on real life incidence and mathematical modeling</title>
      <link>https://arxiv.org/abs/2406.06765</link>
      <description>arXiv:2406.06765v3 Announce Type: replace-cross 
Abstract: Mathematical modeling allows us to better understand myeloproliferative neoplasms (MPN), a group of blood cancers, emergence and development. We test different mathematical models on an initial cohort to determine the emergence and evolution times before diagnosis of JAK2V617F+ classical MPN (Polycythemia Vera (PV) and Essential Thrombocythemia (ET)). We consider the time before diagnosis as the sum of two independent periods: the time (from embryonic development) for the JAK2V617F mutation to occur, not disappear and enter proliferation, and a second time corresponding to the expansion of the clonal population until diagnosis. We prove that the rate of active mutation occurrence increases exponentially with age following the Gompertz model rather than being constant. We find that the first tumorous cell takes an average time of $63.1 \pm 13$ years to appear and start proliferation. On the other hand, the expansion time is constant: $8.8$ years once the mutation has emerged. These results are validated in an external cohort. Using this model, we analyze JAK2V617F ET versus PV, and obtain that the time of active mutation occurrence for PV takes approximately $1.5$ years more than for ET to develop, while the expansion time was similar. In conclusion, our age-dependent approach for the emergence and development of MPN demonstrates that the emergence of a JAKV617F mutation should be linked to an aging mechanism, and indicates a $8-9$ years period of time to develop a full MPN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06765v3</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ana Fern\'andez Baranda, Vincent Bansaye, Evelyne Lauret, Morgane Mounier, Val\'erie Ugo, Sylvie M\'el\'eard, St\'ephane Giraudier</dc:creator>
    </item>
    <item>
      <title>Diffusion models for multivariate subsurface generation and efficient probabilistic inversion</title>
      <link>https://arxiv.org/abs/2507.15809</link>
      <description>arXiv:2507.15809v4 Announce Type: replace-cross 
Abstract: Diffusion models offer stable training and state-of-the-art performance for deep generative modeling tasks. Here, we consider their use in the context of multivariate subsurface modeling and probabilistic inversion. We first demonstrate that diffusion models enhance multivariate modeling capabilities compared to variational autoencoders and generative adversarial networks. In diffusion modeling, the generative process involves a comparatively large number of time steps with update rules that can be modified to account for conditioning data. We propose different corrections to the popular Diffusion Posterior Sampling approach by Chung et al. (2023). In particular, we introduce a likelihood approximation accounting for the noise-contamination that is inherent in diffusion modeling. We assess performance in a multivariate geological scenario involving facies and correlated acoustic impedance. Conditional modeling is demonstrated using both local hard data (well logs) and nonlinear geophysics (fullstack seismic data). Our tests show significantly improved statistical robustness, enhanced sampling of the posterior probability density function and reduced computational costs, compared to the original approach. The method can be used with both hard and indirect conditioning data, individually or simultaneously. As the inversion is included within the diffusion process, it is faster than other methods requiring an outer-loop around the generative model, such as Markov chain Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15809v4</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cageo.2025.106076</arxiv:DOI>
      <arxiv:journal_reference>Comput. Geosci. 207 (2026) 106076</arxiv:journal_reference>
      <dc:creator>Roberto Miele, Niklas Linde</dc:creator>
    </item>
    <item>
      <title>Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems</title>
      <link>https://arxiv.org/abs/2508.09156</link>
      <description>arXiv:2508.09156v2 Announce Type: replace-cross 
Abstract: We present a framework for fine-tuning flow-matching generative models to enforce physical constraints and solve inverse problems in scientific systems. Starting from a model trained on low-fidelity or observational data, we apply a differentiable post-training procedure that minimizes weak-form residuals of governing partial differential equations (PDEs), promoting physical consistency and adherence to boundary conditions without distorting the underlying learned distribution. To infer unknown physical inputs, such as source terms, material parameters, or boundary data, we augment the generative process with a learnable latent parameter predictor and propose a joint optimization strategy. The resulting model produces physically valid field solutions alongside plausible estimates of hidden parameters, effectively addressing ill-posed inverse problems in a data-driven yet physicsaware manner. We validate our method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE constraints and accurate recovery of latent coefficients. Our approach bridges generative modelling and scientific inference, opening new avenues for simulation-augmented discovery and data-efficient modelling of physical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09156v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Tauberschmidt, Sophie Fellenz, Sebastian J. Vollmer, Andrew B. Duncan</dc:creator>
    </item>
    <item>
      <title>Power Analysis is Essential: High-Powered Tests Suggest Minimal to No Effect of Rounded Shapes on Click-Through Rates</title>
      <link>https://arxiv.org/abs/2512.24521</link>
      <description>arXiv:2512.24521v2 Announce Type: replace-cross 
Abstract: Underpowered studies (below 50%) suffer from the winner's curse: A statistically significant result must exaggerate the true treatment effect to meet the significance threshold. A study by Dipayan Biswas, Annika Abell, and Roger Chacko published in the Journal of Consumer Research (2023) reported that in an A/B test simply rounding the corners of square buttons increased the online click-through rate by 55% (p-value 0.037)$\unicode{x2014}$a striking finding with potentially wide-ranging implications for a digital industry that is seeking to enhance consumer engagement. Drawing on our experience with tens of thousands of A/B tests, many involving similar user interface modifications, we found this dramatic claim implausibly large. To evaluate the claim, and provide a more accurate estimate of the treatment effect, we conducted three high-powered A/B tests, each involving over two thousand times more users than the original study. All three experiments yielded effect size estimates that were approximately two orders of magnitude smaller than initially reported, with 95% confidence intervals that include zero, that is, not statistically significant at the 0.05 level. Two additional independent replications by Evidoo found similarly small effects. These findings underscore the critical importance of power analysis and experimental design in increasing trust and reproducibility of results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24521v2</guid>
      <category>stat.ME</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ron Kohavi, Jakub Linowski, Lukas Vermeer, Fabrice Boisseranc, Joachim Furuseth, Andrew Gelman, Guido Imbens, Ravikiran Rajagopal</dc:creator>
    </item>
  </channel>
</rss>
