<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Jul 2024 04:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>What do we study when studying politics and democracy? A semantic analysis of how politics and democracy are treated in SIGCHI conference papers</title>
      <link>https://arxiv.org/abs/2407.02579</link>
      <description>arXiv:2407.02579v1 Announce Type: new 
Abstract: Human-computer interaction scholars are increasingly touching on topics related to politics or democracy. As these concepts are ambiguous, an examination of concepts' invoked meanings aids in the self-reflection of our research efforts. We conduct a thematic analysis of all papers with the word `politics' in abstract, title or keywords ($n$=378) and likewise 152 papers with the word `democracy.' We observe that these words are increasingly being used in human-computer interaction, both in absolute and relative terms. At the same time, we show that researchers invoke these words with diverse levels of analysis in mind: the early research focused on mezzo-level (i.e., small groups), but more recently the work has begun to include macro-level analysis (i.e., society and politics as played in the public sphere). After the increasing focus on the macro-level, we see a transition towards more normative and activist research, in some areas it replaces observational and empirical research. These differences indicate semantic differences, which -- in the worst case -- may limit scientific progress. We bring these differences visible to help in further exchanges of ideas and human-computer interaction community to explore how it orients itself to politics and democracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02579v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matti Nelimarkka, Ville Vuorenmaa</dc:creator>
    </item>
    <item>
      <title>An AI-Based System Utilizing IoT-Enabled Ambient Sensors and LLMs for Complex Activity Tracking</title>
      <link>https://arxiv.org/abs/2407.02606</link>
      <description>arXiv:2407.02606v1 Announce Type: new 
Abstract: Complex activity recognition plays an important role in elderly care assistance. However, the reasoning ability of edge devices is constrained by the classic machine learning model capacity. In this paper, we present a non-invasive ambient sensing system that can detect multiple activities and apply large language models (LLMs) to reason the activity sequences. This method effectively combines edge devices and LLMs to help elderly people in their daily activities, such as reminding them to take pills or handling emergencies like falls. The LLM-based edge device can also serve as an interface to interact with elderly people, especially with memory issue, assisting them in their daily lives. By deploying such a system, we believe that the smart sensing system can improve the quality of life for older people and provide more efficient protection</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02606v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Sun, Jorge Ortiz</dc:creator>
    </item>
    <item>
      <title>Co-Designing Unstructured Text Data Visualization Systems</title>
      <link>https://arxiv.org/abs/2407.02611</link>
      <description>arXiv:2407.02611v1 Announce Type: new 
Abstract: We present our in-progress work on co-designing a visualization tool for presenting unstructured text. We have conducted a focus group with a variety of professionals who regularly analyze large corpora of unstructured text. Our preliminary insights indicate there is an unmet need to visually explore the dynamics between entities and actors extracted from unstructured text. Additionally, large corpora contain multiple perspectives on the same series of events. There is a need to disentangle these perspectives and visually show the multiple narratives present in the data. In our future work, we will co-design low-fidelity prototypes to create a broad consideration space of possible solutions for visualizing unstructured text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02611v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beck Langstone, Fateme Rajabiyazdi</dc:creator>
    </item>
    <item>
      <title>AcuVR: Enhancing Acupuncture Training Workflow with Virtual Reality</title>
      <link>https://arxiv.org/abs/2407.02614</link>
      <description>arXiv:2407.02614v1 Announce Type: new 
Abstract: Acupuncture is a widely adopted medical practice that involves inserting thin needles into specific points on the body to alleviate pain and treat various health conditions. Current learning practices heavily rely on 2D atlases and practice on peers, which are notably less intuitive and pose risks, particularly in sensitive areas such as the eyes. To address these challenges, we introduce AcuVR, a Virtual Reality (VR) based system designed to add a layer of interactivity and realism. This innovation aims to reduce the risks associated with practicing acupuncture techniques while offering more effective learning strategies. Furthermore, AcuVR incorporates medical imaging and standardized anatomy models, enabling the simulation of customized acupuncture scenarios. This feature represents a significant advancement beyond the limitations of conventional resources such as atlases and textbooks, facilitating a more immersive and personalized learning experience. The evaluation study with eight acupuncture students and practitioners revealed high participant satisfaction and pointed to the effectiveness and potential of AcuVR as a valuable addition to acupuncture training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02614v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Menghe Zhang, Chen Chen, Matin Yarmand, Anish Rajeshkumar, Nadir Weibel</dc:creator>
    </item>
    <item>
      <title>STL: Still Tricky Logic (for System Validation, Even When Showing Your Work)</title>
      <link>https://arxiv.org/abs/2407.02632</link>
      <description>arXiv:2407.02632v1 Announce Type: new 
Abstract: As learned control policies become increasingly common in autonomous systems, there is increasing need to ensure that they are interpretable and can be checked by human stakeholders. Formal specifications have been proposed as ways to produce human-interpretable policies for autonomous systems that can still be learned from examples. Previous work showed that despite claims of interpretability, humans are unable to use formal specifications presented in a variety of ways to validate even simple robot behaviors. This work uses active learning, a standard pedagogical method, to attempt to improve humans' ability to validate policies in signal temporal logic (STL). Results show that overall validation accuracy is not high, at $65\% \pm 15\%$ (mean $\pm$ standard deviation), and that the three conditions of no active learning, active learning, and active learning with feedback do not significantly differ from each other. Our results suggest that the utility of formal specifications for human interpretability is still unsupported but point to other avenues of development which may enable improvements in system validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02632v1</guid>
      <category>cs.HC</category>
      <category>cs.FL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Isabelle Hurley, Rohan Paleja, Ashley Suh, Jaime D. Pe\~na, Ho Chit Siu</dc:creator>
    </item>
    <item>
      <title>Improving Steering and Verification in AI-Assisted Data Analysis with Interactive Task Decomposition</title>
      <link>https://arxiv.org/abs/2407.02651</link>
      <description>arXiv:2407.02651v1 Announce Type: new 
Abstract: LLM-powered tools like ChatGPT Data Analysis, have the potential to help users tackle the challenging task of data analysis programming, which requires expertise in data processing, programming, and statistics. However, our formative study (n=15) uncovered serious challenges in verifying AI-generated results and steering the AI (i.e., guiding the AI system to produce the desired output). We developed two contrasting approaches to address these challenges. The first (Stepwise) decomposes the problem into step-by-step subgoals with pairs of editable assumptions and code until task completion, while the second (Phasewise) decomposes the entire problem into three editable, logical phases: structured input/output assumptions, execution plan, and code. A controlled, within-subjects experiment (n=18) compared these systems against a conversational baseline. Users reported significantly greater control with the Stepwise and Phasewise systems, and found intervention, correction, and verification easier, compared to the baseline. The results suggest design guidelines and trade-offs for AI-assisted data analysis tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02651v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Majeed Kazemitabaar, Jack Williams, Ian Drosos, Tovi Grossman, Austin Henley, Carina Negreanu, Advait Sarkar</dc:creator>
    </item>
    <item>
      <title>Design and Development of PainBit: a Portable Device for Supporting Patients with Chronic Pain to Log their Pain</title>
      <link>https://arxiv.org/abs/2407.02697</link>
      <description>arXiv:2407.02697v1 Announce Type: new 
Abstract: Recently, we have seen growing interest among patients with chronic conditions to track their health-related data. There are many wearable devices available to track different health data. However, tracking pain is mostly done by using pen and paper or mobile apps. In collaboration with a healthcare professional we designed a portable pain tracker, PainBit. To gain an understanding of patients' perspectives on our tracker, we conducted two case studies with patients living with chronic pain. We asked patients to use PainBit for two weeks and later conducted semi-structured interviews with them. Patients found PainBit useful for tracking their pain and they preferred using a physical device, PainBit, to track their pain over using a mobile phone. Patients suggested reducing the size and weight of PainBit in the next iterations. We report on the lessons learnt through our design process and the evaluation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02697v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arsh Saleem, Beck Langstone, Alicia Ouskine, Fateme Rajabiyazdi</dc:creator>
    </item>
    <item>
      <title>WARNING This Contains Misinformation: The Effect of Cognitive Factors, Beliefs, and Personality on Misinformation Warning Tag Attitudes</title>
      <link>https://arxiv.org/abs/2407.02710</link>
      <description>arXiv:2407.02710v1 Announce Type: new 
Abstract: Social media platforms enhance the propagation of online misinformation by providing large user bases with a quick means to share content. One way to disrupt the rapid dissemination of misinformation at scale is through warning tags, which label content as potentially false or misleading. Past warning tag mitigation studies yield mixed results for diverse audiences, however. We hypothesize that personalizing warning tags to the individual characteristics of their diverse users may enhance mitigation effectiveness. To reach the goal of personalization, we need to understand how people differ and how those differences predict a person's attitudes and self-described behaviors toward tags and tagged content. In this study, we leverage Amazon Mechanical Turk (n = 132) and undergraduate students (n = 112) to provide this foundational understanding. Specifically, we find attitudes towards warning tags and self-described behaviors are positively influenced by factors such as Personality Openness and Agreeableness, Need for Cognitive Closure (NFCC), Cognitive Reflection Test (CRT) score, and Trust in Medical Scientists. Conversely, Trust in Religious Leaders, Conscientiousness, and political conservatism were negatively correlated with these attitudes and behaviors. We synthesize our results into design insights and a future research agenda for more effective and personalized misinformation warning tags and misinformation mitigation strategies more generally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02710v1</guid>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Kaufman, Aaron Broukhim, Michael Haupt</dc:creator>
    </item>
    <item>
      <title>Game-Based Discovery: Harnessing Mini-Games within Primary Games for Scientific Data Collection and Problem Solving</title>
      <link>https://arxiv.org/abs/2407.02798</link>
      <description>arXiv:2407.02798v1 Announce Type: new 
Abstract: In the popular video game Batman: Arkham Knight, produced by Rocksteady Studios and released in 2015, the primary protagonist of the game is Batman, a vigilante dressed as a bat, fighting crime from the shadows in the fictitious city of Gotham. The game involves a real-world player who takes up the role of Batman to solve a peculiar side mission wherein they have to reconstruct the clean DNA sequence of a human and separate it from mutant DNA to manufacture an antidote to cure the villain. Although this is undoubtedly a fascinating part of the game, one that was absent in previous Batman games, it showcases an interesting notion of using mini-games embedded within primary games to achieve a particular real-world research objective. Although the DNA data used in this case was not real, there are multiple such instances in video games where mini-games have been used for an underlying motive besides entertainment. Based on popular case studies incorporating a similar method, this study characterizes the methodology of designing mini-games within primary games for research purposes into a descriptive framework, highlighting the process's advantages and limitations. It is concluded that these mini-games not only facilitate a deeper understanding of complex scientific concepts but also accelerate data processing and analysis by leveraging crowd-sourced human intuition and pattern recognition capabilities. This paper argues for strategically incorporating miniaturized, gamified elements into established video games that are mainly intended for recreational purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02798v1</guid>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Abhishek Phadke, Mamta Yadav, Stanislav Ustymenko</dc:creator>
    </item>
    <item>
      <title>Understanding the Resilience of Caste: A Critical Discourse Analysis of Community Profiles on X</title>
      <link>https://arxiv.org/abs/2407.02810</link>
      <description>arXiv:2407.02810v1 Announce Type: new 
Abstract: Despite decades of anti-caste efforts, sociocultural practices that marginalize lower-caste groups in India remain resilient and have even proliferated with the use of social media. This paper examines how groups engaged in caste-based discrimination leverage platform affordances of the social media site X (formerly Twitter) to circulate and reinforce caste ideologies. Our analysis builds upon previous HCI conceptualizations of online harms and safety to inform how to address caste-based othering. We offer theoretical and methodological suggestions for critical HCI research focused on studying the mechanisms of power along other social categories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02810v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nayana Kirasur, Shagun Jhaver</dc:creator>
    </item>
    <item>
      <title>CogErgLLM: Exploring Large Language Model Systems Design Perspective Using Cognitive Ergonomics</title>
      <link>https://arxiv.org/abs/2407.02885</link>
      <description>arXiv:2407.02885v1 Announce Type: new 
Abstract: Integrating cognitive ergonomics with LLMs is essential for enhancing safety, reliability, and user satisfaction in human-AI interactions. Current LLM design often lacks this integration, leading to systems that may not fully align with human cognitive capabilities and limitations. Insufficient focus on incorporating cognitive science methods exacerbates biases in LLM outputs, while inconsistent application of user-centered design principles results in sub-optimal user experiences. To address these challenges, our position paper explores the critical integration of cognitive ergonomics principles into LLM design, aiming to provide a comprehensive framework and practical guidelines for ethical LLM development. Through our contributions, we seek to advance understanding and practice in integrating cognitive ergonomics into LLM systems, fostering safer, more reliable, and ethically sound human-AI interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02885v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Azmine Toushik Wasi</dc:creator>
    </item>
    <item>
      <title>Predicting and Understanding Turn-Taking Behavior in Open-Ended Group Activities in Virtual Reality</title>
      <link>https://arxiv.org/abs/2407.02896</link>
      <description>arXiv:2407.02896v1 Announce Type: new 
Abstract: In networked virtual reality (VR), user behaviors, individual differences, and group dynamics can serve as important signals into future speech behaviors, such as who the next speaker will be and the timing of turn-taking behaviors. The ability to predict and understand these behaviors offers opportunities to provide adaptive and personalized assistance, for example helping users with varying sensory abilities navigate complex social scenes and instantiating virtual moderators with natural behaviors. In this work, we predict turn-taking behaviors using features extracted based on social dynamics literature. We discuss results from a large-scale VR classroom dataset consisting of 77 sessions and 1660 minutes of small-group social interactions collected over four weeks. In our evaluation, gradient boosting classifiers achieved the best performance, with accuracies of 0.71--0.78 AUC (area under the ROC curve) across three tasks concerning the "what", "who", and "when" of turn-taking behaviors. In interpreting these models, we found that group size, listener personality, speech-related behavior (e.g., time elapsed since the listener's last speech event), group gaze (e.g., how much the group looks at the speaker), as well as the listener's and previous speaker's head pitch, head y-axis position, and left hand y-axis position more saliently influenced predictions. Results suggested that these features remain reliable indicators in novel social VR settings, as prediction performance is robust over time and with groups and activities not used in the training dataset. We discuss theoretical and practical implications of the work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02896v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Portia Wang, Eugy Han, Anna C. M. Queiroz, Cyan DeVeaux, Jeremy N. Bailenson</dc:creator>
    </item>
    <item>
      <title>"It's like a rubber duck that talks back": Understanding Generative AI-Assisted Data Analysis Workflows through a Participatory Prompting Study</title>
      <link>https://arxiv.org/abs/2407.02903</link>
      <description>arXiv:2407.02903v1 Announce Type: new 
Abstract: Generative AI tools can help users with many tasks. One such task is data analysis, which is notoriously challenging for non-expert end-users due to its expertise requirements, and where AI holds much potential, such as finding relevant data sources, proposing analysis strategies, and writing analysis code. To understand how data analysis workflows can be assisted or impaired by generative AI, we conducted a study (n=15) using Bing Chat via participatory prompting. Participatory prompting is a recently developed methodology in which users and researchers reflect together on tasks through co-engagement with generative AI. In this paper we demonstrate the value of the participatory prompting method. We found that generative AI benefits the information foraging and sensemaking loops of data analysis in specific ways, but also introduces its own barriers and challenges, arising from the difficulties of query formulation, specifying context, and verifying results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02903v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3663384.3663389</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for Work (CHIWORK 2024)</arxiv:journal_reference>
      <dc:creator>Ian Drosos, Advait Sarkar, Xiaotong Xu, Carina Negreanu, Sean Rintel, Lev Tankelevitch</dc:creator>
    </item>
    <item>
      <title>Project Beyond: An Escape Room Game in Virtual Reality to Teach Building Energy Simulations</title>
      <link>https://arxiv.org/abs/2407.02981</link>
      <description>arXiv:2407.02981v1 Announce Type: new 
Abstract: In recent years, Virtual Reality (VR) has found its way into different fields besides pure entertainment. One of the topics that can benefit from the immersive experience of VR is education. Furthermore, using game-based approaches in education can increase user motivation and engagement. Accordingly, in this paper, we designed and developed an immersive escape room game in VR to teach building energy simulation topics. In the game, players must solve puzzles like, for instance, assembling walls using different materials. We use a player guidance system that combines educational content, puzzles, and different types of hints to educate the players about parameters that influence energy efficiency, structural resistance, and costs. To improve user onboarding, we implemented a tutorial level to teach players general interactions and locomotion. To assess the user experience, we evaluate both the tutorial and the game with an expert study with gaming and VR experts (n=11). The participants were asked to play both the tutorial level and the escape room level and complete two sets of post-questionnaires, one after the tutorial and one after the puzzle level. The one after the tutorial level consisted of NASA-TLX and SUS questionnaires, while after the escape room level we asked users to complete the NASA-TLX, UESSF, and PXI questionnaires. The results indicate that the onboarding level successfully provided good usability while maintaining a low task load. On the other hand, the escape room level can provide an engaging, visually appealing, and usable learning environment by arousing players' curiosity through the gameplay. This environment can be extended in future development stages with different educational contents from various fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02981v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georg Arbesser-Rastburg, Saeed Safikhani, Matej Gustin, Christina Hopfe, Gerald Schweiger, Johanna Pirker</dc:creator>
    </item>
    <item>
      <title>JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets</title>
      <link>https://arxiv.org/abs/2407.03045</link>
      <description>arXiv:2407.03045v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have gained significant attention but also raised concerns due to the risk of misuse. Jailbreak prompts, a popular type of adversarial attack towards LLMs, have appeared and constantly evolved to breach the safety protocols of LLMs. To address this issue, LLMs are regularly updated with safety patches based on reported jailbreak prompts. However, malicious users often keep their successful jailbreak prompts private to exploit LLMs. To uncover these private jailbreak prompts, extensive analysis of large-scale conversational datasets is necessary to identify prompts that still manage to bypass the system's defenses. This task is highly challenging due to the immense volume of conversation data, diverse characteristics of jailbreak prompts, and their presence in complex multi-turn conversations. To tackle these challenges, we introduce JailbreakHunter, a visual analytics approach for identifying jailbreak prompts in large-scale human-LLM conversational datasets. We have designed a workflow with three analysis levels: group-level, conversation-level, and turn-level. Group-level analysis enables users to grasp the distribution of conversations and identify suspicious conversations using multiple criteria, such as similarity with reported jailbreak prompts in previous research and attack success rates. Conversation-level analysis facilitates the understanding of the progress of conversations and helps discover jailbreak prompts within their conversation contexts. Turn-level analysis allows users to explore the semantic similarity and token overlap between a singleturn prompt and the reported jailbreak prompts, aiding in the identification of new jailbreak strategies. The effectiveness and usability of the system were verified through multiple case studies and expert interviews.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03045v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihua Jin, Shiyi Liu, Haotian Li, Xun Zhao, Huamin Qu</dc:creator>
    </item>
    <item>
      <title>ScreenTK: Seamless Detection of Time-Killing Moments Using Continuous Mobile Screen Text Monitoring</title>
      <link>https://arxiv.org/abs/2407.03063</link>
      <description>arXiv:2407.03063v1 Announce Type: new 
Abstract: Smartphones have become essential to people's digital lives, providing a continuous stream of information and connectivity. However, this constant flow can lead to moments where users are simply passing time rather than engaging meaningfully. This underscores the importance of developing methods to identify these "time-killing" moments, enabling the delivery of important notifications in a way that minimizes interruptions and enhances user engagement. Recent work has utilized screenshots taken every 5 seconds to detect time-killing activities on smartphones. However, this method often misses to capture phone usage between intervals. We demonstrate that up to 50% of time-killing instances go undetected using screenshots, leading to substantial gaps in understanding user behavior. To address this limitation, we propose a method called ScreenTK that detects time-killing moments by leveraging continuous screen text monitoring and on-device large language models (LLMs). Screen text contains more comprehensive information than screenshots and allows LLMs to summarize detailed phone usage. To verify our framework, we conducted experiments with six participants, capturing 1,034 records of different time-killing moments. Initial results show that our framework outperforms state-of-the-art solutions by 38% in our case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03063v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Le Fang, Shiquan Zhang, Hong Jia, Jorge Goncalves, Vassilis Kostakos</dc:creator>
    </item>
    <item>
      <title>Design of a UE5-based digital twin platform</title>
      <link>https://arxiv.org/abs/2407.03107</link>
      <description>arXiv:2407.03107v1 Announce Type: new 
Abstract: Aiming at the current mainstream 3D scene engine learning and building cost is too high, this thesis proposes a digital twin platform design program based on Unreal Engine 5 (UE5). It aims to provide a universal platform construction design process to effectively reduce the learning cost of large-scale scene construction. Taking an actual project of a unit as an example, the overall cycle work of platform building is explained, and the digital twin and data visualization technologies and applications based on UE5 are analyzed. By summarizing the project implementation into a process approach, the standardization and operability of the process pathway is improved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03107v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <category>cs.MM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shaoqiu Lyu, Muzhi Wang, Sunrui Zhang, Shengzhi Wang</dc:creator>
    </item>
    <item>
      <title>EDPNet: An Efficient Dual Prototype Network for Motor Imagery EEG Decoding</title>
      <link>https://arxiv.org/abs/2407.03177</link>
      <description>arXiv:2407.03177v1 Announce Type: new 
Abstract: Motor imagery electroencephalograph (MI-EEG) decoding plays a crucial role in developing motor imagery brain-computer interfaces (MI-BCIs). However, decoding intentions from MI remains challenging due to the inherent complexity of EEG signals relative to the small-sample size. In this paper, we propose an Efficient Dual Prototype Network (EDPNet) to enable accurate and fast MI decoding. EDPNet employs a lightweight adaptive spatial-spectral fusion module, which promotes more efficient information fusion between multiple EEG electrodes. Subsequently, a parameter-free multi-scale variance pooling module extracts more comprehensive temporal features. Furthermore, we introduce dual prototypical learning to optimize the feature space distribution and training process, thereby improving the model's generalization ability on small-sample MI datasets. Our experimental results show that the EDPNet outperforms state-of-the-art models with superior classification accuracy and kappa values (84.11% and 0.7881 for dataset BCI competition IV 2a, 86.65% and 0.7330 for dataset BCI competition IV 2b). Additionally, we use the BCI competition III IVa dataset with fewer training data to further validate the generalization ability of the proposed EDPNet. We also achieve superior performance with 82.03% classification accuracy. Benefiting from the lightweight parameters and superior decoding accuracy, our EDPNet shows great potential for MI-BCI applications. The code is publicly available at https://github.com/hancan16/EDPNet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03177v1</guid>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Can Han, Chen Liu, Crystal Cai, Jun Wang, Dahong Qian</dc:creator>
    </item>
    <item>
      <title>Eyes on the Game: Deciphering Implicit Human Signals to Infer Human Proficiency, Trust, and Intent</title>
      <link>https://arxiv.org/abs/2407.03298</link>
      <description>arXiv:2407.03298v1 Announce Type: new 
Abstract: Effective collaboration between humans and AIs hinges on transparent communication and alignment of mental models. However, explicit, verbal communication is not always feasible. Under such circumstances, human-human teams often depend on implicit, nonverbal cues to glean important information about their teammates such as intent and expertise, thereby bolstering team alignment and adaptability. Among these implicit cues, two of the most salient and fundamental are a human's actions in the environment and their visual attention. In this paper, we present a novel method to combine eye gaze data and behavioral data, and evaluate their respective predictive power for human proficiency, trust, and intent. We first collect a dataset of paired eye gaze and gameplay data in the fast-paced collaborative "Overcooked" environment. We then train models on this dataset to compare how the predictive powers differ between gaze data, gameplay data, and their combination. We additionally compare our method to prior works that aggregate eye gaze data and demonstrate how these aggregation methods can substantially reduce the predictive ability of eye gaze. Our results indicate that, while eye gaze data and gameplay data excel in different situations, a model that integrates both types consistently outperforms all baselines. This work paves the way for developing intuitive and responsive agents that can efficiently adapt to new teammates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03298v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nikhil Hulle, St\'ephane Aroca-Ouellette, Anthony J. Ries, Jake Brawer, Katharina von der Wense, Alessandro Roncone</dc:creator>
    </item>
    <item>
      <title>Human-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition</title>
      <link>https://arxiv.org/abs/2407.00299</link>
      <description>arXiv:2407.00299v2 Announce Type: cross 
Abstract: Employing a teleoperation system for gathering demonstrations offers the potential for more efficient learning of robot manipulation. However, teleoperating a robot arm equipped with a dexterous hand or gripper, via a teleoperation system poses significant challenges due to its high dimensionality, complex motions, and differences in physiological structure.
  In this study, we introduce a novel system for joint learning between human operators and robots, that enables human operators to share control of a robot end-effector with a learned assistive agent, facilitating simultaneous human demonstration collection and robot manipulation teaching. In this setup, as data accumulates, the assistive agent gradually learns. Consequently, less human effort and attention are required, enhancing the efficiency of the data collection process. It also allows the human operator to adjust the control ratio to achieve a trade-off between manual and automated control.
  We conducted experiments in both simulated environments and physical real-world settings. Through user studies and quantitative evaluations, it is evident that the proposed system could enhance data collection efficiency and reduce the need for human adaptation while ensuring the collected data is of sufficient quality for downstream tasks. Videos are available at https://norweig1an.github.io/human-agent-joint-learning.github.io/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00299v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengcheng Luo, Quanquan Peng, Jun Lv, Kaiwen Hong, Katherine Rose Driggs-Campbell, Cewu Lu, Yong-Lu Li</dc:creator>
    </item>
    <item>
      <title>Enabling Student Innovation through Virtual Reality Development</title>
      <link>https://arxiv.org/abs/2407.02591</link>
      <description>arXiv:2407.02591v1 Announce Type: cross 
Abstract: It is clear, from the major press coverage that Virtual Reality (VR) development is garnering, that there is a huge amount of development interest in VR across multiple industries, including video streaming, gaming and simulated learning. Even though PC, web, and mobile are still the top platforms for software development, it is important for university computer science (CS) programs to expose students to VR as a development platform. Additionally, it is important for CS students to learn how to learn about new technologies, since change is constant in the CS field. CS curriculum changes happen much slower than the pace of technology adoption. As new technologies are introduced, CS faculty and students often learn together, especially in smaller CS programs. This paper describes how student-led VR projects are used, across the CS curriculum, as basic CS concepts are covered. The student-led VR projects are engaging, and promote learning and creativity. Additionally, each student project inspires more students to try their hand at VR development as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02591v1</guid>
      <category>cs.GL</category>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Harms, S. K. (2016). Enabling Student Innovation through Virtual Reality Development. 2016 Midwest Instructional Computing Symposium Proceedings, Cedar Rapids, IA</arxiv:journal_reference>
      <dc:creator>Sherri Harms</dc:creator>
    </item>
    <item>
      <title>GMM-ResNext: Combining Generative and Discriminative Models for Speaker Verification</title>
      <link>https://arxiv.org/abs/2407.03135</link>
      <description>arXiv:2407.03135v1 Announce Type: cross 
Abstract: With the development of deep learning, many different network architectures have been explored in speaker verification. However, most network architectures rely on a single deep learning architecture, and hybrid networks combining different architectures have been little studied in ASV tasks. In this paper, we propose the GMM-ResNext model for speaker verification. Conventional GMM does not consider the score distribution of each frame feature over all Gaussian components and ignores the relationship between neighboring speech frames. So, we extract the log Gaussian probability features based on the raw acoustic features and use ResNext-based network as the backbone to extract the speaker embedding. GMM-ResNext combines Generative and Discriminative Models to improve the generalization ability of deep learning models and allows one to more easily specify meaningful priors on model parameters. A two-path GMM-ResNext model based on two gender-related GMMs has also been proposed. The Experimental results show that the proposed GMM-ResNext achieves relative improvements of 48.1\% and 11.3\% in EER compared with ResNet34 and ECAPA-TDNN on VoxCeleb1-O test set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03135v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICASSP48485.2024.10447141</arxiv:DOI>
      <dc:creator>Hui Yan, Zhenchun Lei, Changhong Liu, Yong Zhou</dc:creator>
    </item>
    <item>
      <title>VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation</title>
      <link>https://arxiv.org/abs/2407.03291</link>
      <description>arXiv:2407.03291v1 Announce Type: cross 
Abstract: Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in the context of smart environments. Existing studies typically require meticulous labeling of both atomic and complex activities, a task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets. Most prior research has focused on datasets that either precisely label atomic activities or, at minimum, their sequence approaches that are often impractical in real world settings.In response, we introduce VCHAR (Variance-Driven Complex Human Activity Recognition), a novel framework that treats the outputs of atomic activities as a distribution over specified intervals. Leveraging generative methodologies, VCHAR elucidates the reasoning behind complex activity classifications through video-based explanations, accessible to users without prior machine learning expertise. Our evaluation across three publicly available datasets demonstrates that VCHAR enhances the accuracy of complex activity recognition without necessitating precise temporal or sequential labeling of atomic activities. Furthermore, user studies confirm that VCHAR's explanations are more intelligible compared to existing methods, facilitating a broader understanding of complex activity recognition among non-experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03291v1</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Sun, Navid Salami Pargoo, Taqiya Ehsan, Zhao Zhang Jorge Ortiz</dc:creator>
    </item>
    <item>
      <title>Wearable Device-Based Real-Time Monitoring of Physiological Signals: Evaluating Cognitive Load Across Different Tasks</title>
      <link>https://arxiv.org/abs/2406.07147</link>
      <description>arXiv:2406.07147v2 Announce Type: replace 
Abstract: This study employs cutting-edge wearable monitoring technology to conduct high-precision, high-temporal-resolution (1-second interval) cognitive load assessment on electroencephalogram (EEG) data from the FP1 channel and heart rate variability (HRV) data of secondary vocational students. By jointly analyzing these two critical physiological indicators, the research delves into their application value in assessing cognitive load among secondary vocational students and their utility across various tasks. The study designed two experiments to validate the efficacy of the proposed approach: Initially, a random forest classification model, developed using the N-BACK task, enabled the precise decoding of physiological signal characteristics in secondary vocational students under different levels of cognitive load, achieving a classification accuracy of 97%. Subsequently, this classification model was applied in a cross-task experiment involving the National Computer Rank Examination (Level-1), demonstrating the method's significant applicability and cross-task transferability in diverse learning contexts. Conducted with high portability, this research holds substantial theoretical and practical significance for optimizing teaching resource allocation in secondary vocational education, as well as for cognitive load assessment methods and monitoring. Currently, the research findings are undergoing trial implementation in the school.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07147v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ling He, Yanxin Chen, Wenqi Wang, Shuting He, Xiaoqiang Hu</dc:creator>
    </item>
    <item>
      <title>What Should Be Considered to Support well-being with AI: Considerations Based on Responsible Research and Innovation</title>
      <link>https://arxiv.org/abs/2407.02381</link>
      <description>arXiv:2407.02381v2 Announce Type: replace 
Abstract: Achieving people's well-being with AI systems requires that each user is guided to a healthier lifestyle in a way that is appropriate for her or him. Although well-being has diverse definitions~\cite{calvo2014positive}, leading a healthy lifestyle is one of the most representative aspects of well-being. A healthy lifestyle often varies from individual to individual and cannot be defined in a top-down manner. For example, while moderate exercise is important for almost everyone, how much exercise is needed and at what time of day varies from person to person. A habit that is easy for one person may be very difficult for another. Habits that are too difficult do not lead to a mentally healthy lifestyle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02381v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuri Nakao</dc:creator>
    </item>
    <item>
      <title>Design and Evaluation of Crowd-sourcing Platforms Based on Users Confidence Judgments</title>
      <link>https://arxiv.org/abs/2212.05777</link>
      <description>arXiv:2212.05777v3 Announce Type: replace-cross 
Abstract: Crowd-sourcing deals with solving problems by assigning them to a large number of non-experts called crowd using their spare time. In these systems, the final answer to the question is determined by summing up the votes obtained from the community. The popularity of using these systems has increased by facilitation of access to community members through mobile phones and the Internet. One of the issues raised in crowd-sourcing is how to choose people and how to collect answers. Usually, the separation of users is done based on their performance in a pre-test. Designing the pre-test for performance calculation is challenging; The pre-test questions should be chosen in a way that they test the characteristics in people related to the main questions. One of the ways to increase the accuracy of crowd-sourcing systems is to pay attention to people's cognitive characteristics and decision-making model to form a crowd and improve the estimation of the accuracy of their answers to questions. People can estimate the correctness of their responses while making a decision. The accuracy of this estimate is determined by a quantity called metacognition ability. Metacoginition is referred to the case where the confidence level is considered along with the answer to increase the accuracy of the solution. In this paper, by both mathematical and experimental analysis, we would answer the following question: Is it possible to improve the performance of the crowd-sourcing system by knowing the metacognition of individuals and recording and using the users' confidence in their answers?</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05777v3</guid>
      <category>cs.CE</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samin Nili Ahmadabadi, Maryam Haghifam, Vahid Shah-Mansouri, Sara Ershadmanesh</dc:creator>
    </item>
    <item>
      <title>Visual Analytics of Multivariate Networks with Representation Learning and Composite Variable Construction</title>
      <link>https://arxiv.org/abs/2303.09590</link>
      <description>arXiv:2303.09590v3 Announce Type: replace-cross 
Abstract: Multivariate networks are commonly found in real-world data-driven applications. Uncovering and understanding the relations of interest in multivariate networks is not a trivial task. This paper presents a visual analytics workflow for studying multivariate networks to extract associations between different structural and semantic characteristics of the networks (e.g., what are the combinations of attributes largely relating to the density of a social network?). The workflow consists of a neural-network-based learning phase to classify the data based on the chosen input and output attributes, a dimensionality reduction and optimization phase to produce a simplified set of results for examination, and finally an interpreting phase conducted by the user through an interactive visualization interface. A key part of our design is a composite variable construction step that remodels nonlinear features obtained by neural networks into linear features that are intuitive to interpret. We demonstrate the capabilities of this workflow with multiple case studies on networks derived from social media usage and also evaluate the workflow with qualitative feedback from experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.09590v3</guid>
      <category>cs.SI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hsiao-Ying Lu, Takanori Fujiwara, Ming-Yi Chang, Yang-chih Fu, Anders Ynnerman, Kwan-Liu Ma</dc:creator>
    </item>
    <item>
      <title>Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review</title>
      <link>https://arxiv.org/abs/2402.10086</link>
      <description>arXiv:2402.10086v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) shows promising applications for the perception and planning tasks in autonomous driving (AD) due to its superior performance compared to conventional methods. However, inscrutable AI systems exacerbate the existing challenge of safety assurance of AD. One way to mitigate this challenge is to utilize explainable AI (XAI) techniques. To this end, we present the first comprehensive systematic literature review of explainable methods for safe and trustworthy AD. We begin by analyzing the requirements for AI in the context of AD, focusing on three key aspects: data, model, and agency. We find that XAI is fundamental to meeting these requirements. Based on this, we explain the sources of explanations in AI and describe a taxonomy of XAI. We then identify five key contributions of XAI for safe and trustworthy AI in AD, which are interpretable design, interpretable surrogate models, interpretable monitoring, auxiliary explanations, and interpretable validation. Finally, we propose a modular framework called SafeX to integrate these contributions, enabling explanation delivery to users while simultaneously ensuring the safety of AI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10086v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Kuznietsov, Balint Gyevnar, Cheng Wang, Steven Peters, Stefano V. Albrecht</dc:creator>
    </item>
    <item>
      <title>Towards Human-AI Collaboration in Healthcare: Guided Deferral Systems with Large Language Models</title>
      <link>https://arxiv.org/abs/2406.07212</link>
      <description>arXiv:2406.07212v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) present a valuable technology for various applications in healthcare, but their tendency to hallucinate introduces unacceptable uncertainty in critical decision-making situations. Human-AI collaboration (HAIC) can mitigate this uncertainty by combining human and AI strengths for better outcomes. This paper presents a novel guided deferral system that provides intelligent guidance when AI defers cases to human decision-makers. We leverage LLMs' verbalisation capabilities and internal states to create this system, demonstrating that fine-tuning small-scale LLMs with data from large-scale LLMs greatly enhances performance while maintaining computational efficiency and data privacy. A pilot study showcases the effectiveness of our proposed deferral system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07212v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Strong, Qianhui Men, Alison Noble</dc:creator>
    </item>
  </channel>
</rss>
