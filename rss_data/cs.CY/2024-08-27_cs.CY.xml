<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Aug 2024 01:43:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Reconciling Different Theories of Learning with an Agent-based Model of Procedural Learning</title>
      <link>https://arxiv.org/abs/2408.13364</link>
      <description>arXiv:2408.13364v1 Announce Type: new 
Abstract: Computational models of human learning can play a significant role in enhancing our knowledge about nuances in theoretical and qualitative learning theories and frameworks. There are many existing frameworks in educational settings that have shown to be verified using empirical studies, but at times we find these theories make conflicting claims or recommendations for instruction. In this study, we propose a new computational model of human learning, Procedural ABICAP, that reconciles the ICAP, Knowledge-Learning-Instruction (KLI), and cognitive load theory (CLT) frameworks for learning procedural knowledge. ICAP assumes that constructive learning generally yields better learning outcomes, while theories such as KLI and CLT claim that this is not always true. We suppose that one reason for this may be that ICAP is primarily used for conceptual learning and is underspecified as a framework for thinking about procedural learning. We show how our computational model, both by design and through simulations, can be used to reconcile different results in the literature. More generally, we position our computational model as an executable theory of learning that can be used to simulate various educational settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13364v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sina Rismanchian, Shayan Doroudi</dc:creator>
    </item>
    <item>
      <title>Progress: A Post-AI Manifesto</title>
      <link>https://arxiv.org/abs/2408.13775</link>
      <description>arXiv:2408.13775v1 Announce Type: new 
Abstract: This manifesto outlines key principles for progress in the post-AI era, emphasizing non-linear yet cumulative advancement, deep understanding of purpose and context, multi-stakeholder collaboration, and system-level experimentation. It redefines progress as substantial, durable, and replicable advancement, highlighting the importance of balancing technological innovation with human-centric values. It acknowledges AI's potential to accelerate progress across industries while recognizing its limitations, such as creating illusions of understanding and potentially narrowing problem-solving approaches. It concludes that true progress in the AI age requires a symbiosis of artificial intelligence capabilities and human ingenuity, calling for a holistic, interdisciplinary approach to shape a future that serves all of humanity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13775v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Christoforus Yoga Haryanto</dc:creator>
    </item>
    <item>
      <title>Including Non-Autistic Peers in Games Designed for Autistic Socialization</title>
      <link>https://arxiv.org/abs/2408.13967</link>
      <description>arXiv:2408.13967v1 Announce Type: new 
Abstract: Through a review of current game practices, the author highlights concerns regarding the safety of public social games and the singular medical approach to serious game design for autism. The paper identifies a disconnect between the needs of autistic children and the existing solutions. To fill this gap, a neurodiversity approach to serious game design is proposed. This approach aims to address the social needs of autistic children, enabling them to interact with their neurotypical peers directly, confidently, and safely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13967v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiqi Xiao</dc:creator>
    </item>
    <item>
      <title>Overcoming the Barriers of Using Linked Open Data in Smart City Applications</title>
      <link>https://arxiv.org/abs/2408.14315</link>
      <description>arXiv:2408.14315v1 Announce Type: new 
Abstract: We study the benefits and challenges of using Linked Open Data in smart city applications and propose a set of open source, highly scalable tools within the case of a public-rental bicycle system, which can act as a reference guide for other smart city applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14315v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/MC.2022.3206144</arxiv:DOI>
      <arxiv:journal_reference>Computer ( Volume: 55, Issue: 12, December 2022)</arxiv:journal_reference>
      <dc:creator>Javier Conde, Andres Munoz-Arcentales, Johnny Choque, Gabriel Huecas, \'Alvaro Alonso</dc:creator>
    </item>
    <item>
      <title>As Biased as You Measure: Methodological Pitfalls of Bias Evaluations in Speaker Verification Research</title>
      <link>https://arxiv.org/abs/2408.13614</link>
      <description>arXiv:2408.13614v1 Announce Type: cross 
Abstract: Detecting and mitigating bias in speaker verification systems is important, as datasets, processing choices and algorithms can lead to performance differences that systematically favour some groups of people while disadvantaging others. Prior studies have thus measured performance differences across groups to evaluate bias. However, when comparing results across studies, it becomes apparent that they draw contradictory conclusions, hindering progress in this area. In this paper we investigate how measurement impacts the outcomes of bias evaluations. We show empirically that bias evaluations are strongly influenced by base metrics that measure performance, by the choice of ratio or difference-based bias measure, and by the aggregation of bias measures into meta-measures. Based on our findings, we recommend the use of ratio-based bias measures, in particular when the values of base metrics are small, or when base metrics with different orders of magnitude need to be compared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13614v1</guid>
      <category>eess.AS</category>
      <category>cs.CY</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wiebke Hutiri, Tanvina Patel, Aaron Yi Ding, Odette Scharenborg</dc:creator>
    </item>
    <item>
      <title>Evaluating Alternative Training Interventions Using Personalized Computational Models of Learning</title>
      <link>https://arxiv.org/abs/2408.13684</link>
      <description>arXiv:2408.13684v1 Announce Type: cross 
Abstract: Evaluating different training interventions to determine which produce the best learning outcomes is one of the main challenges faced by instructional designers. Typically, these designers use A/B experiments to evaluate each intervention; however, it is costly and time consuming to run such studies. To address this issue, we explore how computational models of learning might support designers in reasoning causally about alternative interventions within a fractions tutor. We present an approach for automatically tuning models to specific individuals and show that personalized models make better predictions of students' behavior than generic ones. Next, we conduct simulations to generate counterfactual predictions of performance and learning for two students (high and low performing) in different versions of the fractions tutor. Our approach makes predictions that align with previous human findings, as well as testable predictions that might be evaluated with future human experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13684v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Advances in Cognitive Systems, 10, 35-52 (2023)</arxiv:journal_reference>
      <dc:creator>Christopher James MacLellan, Kimberly Stowers, Lisa Brady</dc:creator>
    </item>
    <item>
      <title>Time Series Analysis for Education: Methods, Applications, and Future Directions</title>
      <link>https://arxiv.org/abs/2408.13960</link>
      <description>arXiv:2408.13960v2 Announce Type: cross 
Abstract: Recent advancements in the collection and analysis of sequential educational data have brought time series analysis to a pivotal position in educational research, highlighting its essential role in facilitating data-driven decision-making. However, there is a lack of comprehensive summaries that consolidate these advancements. To the best of our knowledge, this paper is the first to provide a comprehensive review of time series analysis techniques specifically within the educational context. We begin by exploring the landscape of educational data analytics, categorizing various data sources and types relevant to education. We then review four prominent time series methods-forecasting, classification, clustering, and anomaly detection-illustrating their specific application points in educational settings. Subsequently, we present a range of educational scenarios and applications, focusing on how these methods are employed to address diverse educational tasks, which highlights the practical integration of multiple time series methods to solve complex educational problems. Finally, we conclude with a discussion on future directions, including personalized learning analytics, multimodal data fusion, and the role of large language models (LLMs) in educational time series. The contributions of this paper include a detailed taxonomy of educational data, a synthesis of time series techniques with specific educational applications, and a forward-looking perspective on emerging trends and future research opportunities in educational analysis. The related papers and resources are available and regularly updated at the project page.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13960v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shengzhong Mao, Chaoli Zhang, Yichi Song, Jindong Wang, Xiao-Jun Zeng, Zenglin Xu, Qingsong Wen</dc:creator>
    </item>
    <item>
      <title>HyperSBINN: A Hypernetwork-Enhanced Systems Biology-Informed Neural Network for Efficient Drug Cardiosafety Assessment</title>
      <link>https://arxiv.org/abs/2408.14266</link>
      <description>arXiv:2408.14266v1 Announce Type: cross 
Abstract: Mathematical modeling in systems toxicology enables a comprehensive understanding of the effects of pharmaceutical substances on cardiac health. However, the complexity of these models limits their widespread application in early drug discovery. In this paper, we introduce a novel approach to solving parameterized models of cardiac action potentials by combining meta-learning techniques with Systems Biology-Informed Neural Networks (SBINNs). The proposed method, HyperSBINN, effectively addresses the challenge of predicting the effects of various compounds at different concentrations on cardiac action potentials, outperforming traditional differential equation solvers in speed. Our model efficiently handles scenarios with limited data and complex parameterized differential equations. The HyperSBINN model demonstrates robust performance in predicting APD90 values, indicating its potential as a reliable tool for modeling cardiac electrophysiology and aiding in preclinical drug development. This framework represents an advancement in computational modeling, offering a scalable and efficient solution for simulating and understanding complex biological systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14266v1</guid>
      <category>stat.ML</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Inass Soukarieh, Gerhard Hessler, Herv\'e Minoux, Marcel Mohr, Friedemann Schmidt, Jan Wenzel, Pierre Barbillon, Hugo Gangloff, Pierre Gloaguen</dc:creator>
    </item>
    <item>
      <title>Social perception of faces in a vision-language model</title>
      <link>https://arxiv.org/abs/2408.14435</link>
      <description>arXiv:2408.14435v1 Announce Type: cross 
Abstract: We explore social perception of human faces in CLIP, a widely used open-source vision-language model. To this end, we compare the similarity in CLIP embeddings between different textual prompts and a set of face images. Our textual prompts are constructed from well-validated social psychology terms denoting social perception. The face images are synthetic and are systematically and independently varied along six dimensions: the legally protected attributes of age, gender, and race, as well as facial expression, lighting, and pose. Independently and systematically manipulating face attributes allows us to study the effect of each on social perception and avoids confounds that can occur in wild-collected data due to uncontrolled systematic correlations between attributes. Thus, our findings are experimental rather than observational. Our main findings are three. First, while CLIP is trained on the widest variety of images and texts, it is able to make fine-grained human-like social judgments on face images. Second, age, gender, and race do systematically impact CLIP's social perception of faces, suggesting an undesirable bias in CLIP vis-a-vis legally protected attributes. Most strikingly, we find a strong pattern of bias concerning the faces of Black women, where CLIP produces extreme values of social perception across different ages and facial expressions. Third, facial expression impacts social perception more than age and lighting as much as age. The last finding predicts that studies that do not control for unprotected visual attributes may reach the wrong conclusions on bias. Our novel method of investigation, which is founded on the social psychology literature and on the experiments involving the manipulation of individual attributes, yields sharper and more reliable observations than previous observational methods and may be applied to study biases in any vision-language model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14435v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carina I. Hausladen, Manuel Knott, Colin F. Camerer, Pietro Perona</dc:creator>
    </item>
    <item>
      <title>Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study</title>
      <link>https://arxiv.org/abs/2408.14438</link>
      <description>arXiv:2408.14438v1 Announce Type: cross 
Abstract: The advent of large language models such as ChatGPT, Gemini, and others has underscored the importance of evaluating their diverse capabilities, ranging from natural language understanding to code generation. However, their performance on spatial tasks has not been comprehensively assessed. This study addresses this gap by introducing a novel multi-task spatial evaluation dataset, designed to systematically explore and compare the performance of several advanced models on spatial tasks. The dataset encompasses twelve distinct task types, including spatial understanding and path planning, each with verified, accurate answers. We evaluated multiple models, including OpenAI's gpt-3.5-turbo, gpt-4o, and ZhipuAI's glm-4, through a two-phase testing approach. Initially, we conducted zero-shot testing, followed by categorizing the dataset by difficulty and performing prompt tuning tests. Results indicate that gpt-4o achieved the highest overall accuracy in the first phase, with an average of 71.3%. Although moonshot-v1-8k slightly underperformed overall, it surpassed gpt-4o in place name recognition tasks. The study also highlights the impact of prompt strategies on model performance in specific tasks. For example, the Chain-of-Thought (COT) strategy increased gpt-4o's accuracy in path planning from 12.4% to 87.5%, while a one-shot strategy enhanced moonshot-v1-8k's accuracy in mapping tasks from 10.1% to 76.3%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14438v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liuchang Xu Shuo Zhao, Qingming Lin, Luyao Chen, Qianqian Luo, Sensen Wu, Xinyue Ye, Hailin Feng, Zhenhong Du</dc:creator>
    </item>
    <item>
      <title>Golden Eye: The Theory of Havana Syndrome</title>
      <link>https://arxiv.org/abs/2408.12041</link>
      <description>arXiv:2408.12041v2 Announce Type: replace 
Abstract: Beginning around 2016, US Diplomats reported unusual injuries while serving abroad. Personnel suffered from symptoms such as nausea, vertigo, and disorientation. The collective set of ailments was subbed "Havana Syndrome". This whitepaper delves into an analysis of competing hypotheses with respect to potential origins of these symptoms. Whitepaper cleared for release on 18 JUN 2024. The views expressed by this whitepaper are those of the author and do not reflect the official policy or position of Dakota State University, the N.H. Army National Guard, the U.S. Army, the Department of Defense, or the U.S. Government.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12041v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Dorian Wong</dc:creator>
    </item>
    <item>
      <title>Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population</title>
      <link>https://arxiv.org/abs/2401.14512</link>
      <description>arXiv:2401.14512v4 Announce Type: replace-cross 
Abstract: Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our paper addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We apply our methodology to extend inferences from the Starting Treatment with Agonist Replacement Therapies (START) trial -- investigating the effectiveness of medication for opioid use disorder -- to the real-world population represented by the Treatment Episode Dataset: Admissions (TEDS-A). By refining target populations using ROOT, our framework offers a systematic approach to enhance decision-making accuracy and inform future trials in diverse populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14512v4</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Harsh Parikh, Rachael Ross, Elizabeth Stuart, Kara Rudolph</dc:creator>
    </item>
    <item>
      <title>Hypothesis Generation with Large Language Models</title>
      <link>https://arxiv.org/abs/2404.04326</link>
      <description>arXiv:2404.04326v2 Announce Type: replace-cross 
Abstract: Effective generation of novel hypotheses is instrumental to scientific progress. So far, researchers have been the main powerhouse behind hypothesis generation by painstaking data analysis and thinking (also known as the Eureka moment). In this paper, we examine the potential of large language models (LLMs) to generate hypotheses. We focus on hypothesis generation based on data (i.e., labeled examples). To enable LLMs to handle arbitrarily long contexts, we generate initial hypotheses from a small number of examples and then update them iteratively to improve the quality of hypotheses. Inspired by multi-armed bandits, we design a reward function to inform the exploitation-exploration tradeoff in the update process. Our algorithm is able to generate hypotheses that enable much better predictive performance than few-shot prompting in classification tasks, improving accuracy by 31.7% on a synthetic dataset and by 13.9%, 3.3% and, 24.9% on three real-world datasets. We also outperform supervised learning by 12.8% and 11.2% on two challenging real-world datasets. Furthermore, we find that the generated hypotheses not only corroborate human-verified theories but also uncover new insights for the tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04326v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan</dc:creator>
    </item>
  </channel>
</rss>
