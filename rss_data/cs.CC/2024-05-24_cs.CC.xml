<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2024 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Pseudorandomness, symmetry, smoothing: I</title>
      <link>https://arxiv.org/abs/2405.13143</link>
      <description>arXiv:2405.13143v1 Announce Type: new 
Abstract: We prove several new results about bounded uniform and small-bias distributions. A main message is that, small-bias, even perturbed with noise, does not fool several classes of tests better than bounded uniformity. We prove this for threshold tests, small-space algorithms, and small-depth circuits. In particular, we obtain small-bias distributions that
  1) achieve an optimal lower bound on their statistical distance to any bounded-uniform distribution. This closes a line of research initiated by Alon, Goldreich, and Mansour in 2003, and improves on a result by O'Donnell and Zhao.
  2) have heavier tail mass than the uniform distribution. This answers a question posed by several researchers including Bun and Steinke.
  3) rule out a popular paradigm for constructing pseudorandom generators, originating in a 1989 work by Ajtai and Wigderson. This again answers a question raised by several researchers. For branching programs, our result matches a bound by Forbes and Kelley.
  Our small-bias distributions above are symmetric. We show that the xor of any two symmetric small-bias distributions fools any bounded function. Hence our examples cannot be extended to the xor of two small-bias distributions, another popular paradigm whose power remains unknown. We also generalize and simplify the proof of a result of Bazzi.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13143v1</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harm Derksen, Peter Ivanov, Chin Ho Lee, Emanuele Viola</dc:creator>
    </item>
    <item>
      <title>On the Inapproximability of Finding Minimum Monitoring Edge-Geodetic Sets</title>
      <link>https://arxiv.org/abs/2405.13875</link>
      <description>arXiv:2405.13875v1 Announce Type: new 
Abstract: Given an undirected connected graph $G = (V(G), E(G))$ on $n$ vertices, the minimum Monitoring Edge-Geodetic Set (MEG-set) problem asks to find a subset $M \subseteq V(G)$ of minimum cardinality such that, for every edge $e \in E(G)$, there exist $x,y \in M$ for which all shortest paths between $x$ and $y$ in $G$ traverse $e$.
  We show that, for any constant $c &lt; \frac{1}{2}$, no polynomial-time $(c \log n)$-approximation algorithm for the minimum MEG-set problem exists, unless $\mathsf{P} = \mathsf{NP}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13875v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Bil\`o, Giordano Colli, Luca Forlizzi, Stefano Leucci</dc:creator>
    </item>
    <item>
      <title>Dequantizability from inputs</title>
      <link>https://arxiv.org/abs/2405.13273</link>
      <description>arXiv:2405.13273v1 Announce Type: cross 
Abstract: By comparing constructions of block encoding given by [1-4], we propose a way to extract dequantizability from advancements in dequantization techniques that have been led by Tang, as in [5]. Then we apply this notion to the sparse-access input model that is known to be BQP-complete in general, thereby conceived to be un-dequantizable. Our goal is to break down this belief by examining the sparse-access input model's instances, particularly their input matrices. In conclusion, this paper forms a dequantizability-verifying scheme that can be applied whenever an input is given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13273v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tae-Won Kim, Byung-Soo Choi</dc:creator>
    </item>
    <item>
      <title>On connections between k-coloring and Euclidean k-means</title>
      <link>https://arxiv.org/abs/2405.13877</link>
      <description>arXiv:2405.13877v1 Announce Type: cross 
Abstract: In the Euclidean $k$-means problems we are given as input a set of $n$ points in $\mathbb{R}^d$ and the goal is to find a set of $k$ points $C\subseteq \mathbb{R}^d$, so as to minimize the sum of the squared Euclidean distances from each point in $P$ to its closest center in $C$. In this paper, we formally explore connections between the $k$-coloring problem on graphs and the Euclidean $k$-means problem. Our results are as follows:
  $\bullet$ For all $k\ge 3$, we provide a simple reduction from the $k$-coloring problem on regular graphs to the Euclidean $k$-means problem. Moreover, our technique extends to enable a reduction from a structured max-cut problem (which may be considered as a partial 2-coloring problem) to the Euclidean $2$-means problem. Thus, we have a simple and alternate proof of the NP-hardness of Euclidean 2-means problem.
  $\bullet$ In the other direction, we mimic the $O(1.7297^n)$ time algorithm of Williams [TCS'05] for the max-cut of problem on $n$ vertices to obtain an algorithm for the Euclidean 2-means problem with the same runtime, improving on the naive exhaustive search running in $2^n\cdot \text{poly}(n,d)$ time.
  $\bullet$ We prove similar results and connections as above for the Euclidean $k$-min-sum problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13877v1</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enver Aman, Karthik C. S., Sharath Punna</dc:creator>
    </item>
    <item>
      <title>Polynomial Pass Semi-Streaming Lower Bounds for K-Cores and Degeneracy</title>
      <link>https://arxiv.org/abs/2405.14835</link>
      <description>arXiv:2405.14835v1 Announce Type: cross 
Abstract: The following question arises naturally in the study of graph streaming algorithms:
  "Is there any graph problem which is "not too hard", in that it can be solved efficiently with total communication (nearly) linear in the number $n$ of vertices, and for which, nonetheless, any streaming algorithm with $\tilde{O}(n)$ space (i.e., a semi-streaming algorithm) needs a polynomial $n^{\Omega(1)}$ number of passes?"
  Assadi, Chen, and Khanna [STOC 2019] were the first to prove that this is indeed the case. However, the lower bounds that they obtained are for rather non-standard graph problems.
  Our first main contribution is to present the first polynomial-pass lower bounds for natural "not too hard" graph problems studied previously in the streaming model: $k$-cores and degeneracy. We devise a novel communication protocol for both problems with near-linear communication, thus showing that $k$-cores and degeneracy are natural examples of "not too hard" problems. Indeed, previous work have developed single-pass semi-streaming algorithms for approximating these problems. In contrast, we prove that any semi-streaming algorithm for exactly solving these problems requires (almost) $\Omega(n^{1/3})$ passes.
  Our second main contribution is improved round-communication lower bounds for the underlying communication problems at the basis of these reductions:
  * We improve the previous lower bound of Assadi, Chen, and Khanna for hidden pointer chasing (HPC) to achieve optimal bounds.
  * We observe that all current reductions from HPC can also work with a generalized version of this problem that we call MultiHPC, and prove an even stronger and optimal lower bound for this generalization.
  These two results collectively allow us to improve the resulting pass lower bounds for semi-streaming algorithms by a polynomial factor, namely, from $n^{1/5}$ to $n^{1/3}$ passes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14835v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepehr Assadi, Prantar Ghosh, Bruno Loff, Parth Mittal, Sagnik Mukhopadhyay</dc:creator>
    </item>
    <item>
      <title>Nearest Neighbor Complexity and Boolean Circuits</title>
      <link>https://arxiv.org/abs/2402.06740</link>
      <description>arXiv:2402.06740v2 Announce Type: replace 
Abstract: A nearest neighbor representation of a Boolean function $f$ is a set of vectors (anchors) labeled by $0$ or $1$ such that $f(\vec{x}) = 1$ if and only if the closest anchor to $\vec{x}$ is labeled by $1$. This model was introduced by Hajnal, Liu, and Tur\'an (2022), who studied bounds on the number of anchors required to represent Boolean functions under different choices of anchors (real vs. Boolean vectors) as well as the more expressive model of $k$-nearest neighbors.
  We initiate the study of the representational power of nearest and $k$-nearest neighbors through Boolean circuit complexity. To this end, we establish a connection between Boolean functions with polynomial nearest neighbor complexity and those that can be efficiently represented by classes based on linear inequalities -- min-plus polynomial threshold functions -- previously studied in relation to threshold circuits. This extends an observation of Hajnal et al. (2022). We obtain exponential lower bounds on the $k$-nearest neighbors complexity of explicit $n$-variate functions, assuming $k \leq n^{1-\epsilon}$. Previously, no superlinear lower bound was known for any $k&gt;1$.
  Next, we further extend the connection between nearest neighbor representations and circuits to the $k$-nearest neighbors case. As a result, we show that proving superpolynomial lower bounds for the $k$-nearest neighbors complexity of an explicit function for arbitrary $k$ would require a breakthrough in circuit complexity. In addition, we prove an exponential separation between the nearest neighbor and $k$-nearest neighbors complexity (for unrestricted $k$) of an explicit function. These results address questions raised by Hajnal et al. (2022) of proving strong lower bounds for $k$-nearest neighbors and understanding the role of the parameter $k$. Finally, we devise new bounds on the nearest neighbor complexity for several explicit functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06740v2</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mason DiCicco, Vladimir Podolskii, Daniel Reichman</dc:creator>
    </item>
    <item>
      <title>Bounded Depth Frege Lower Bounds for Random 3-CNFs via Deterministic Restrictions</title>
      <link>https://arxiv.org/abs/2403.02275</link>
      <description>arXiv:2403.02275v2 Announce Type: replace 
Abstract: A major open problem in proof complexity is to show that random 3-CNFs with linear number of clauses require super-polynomial size refutations in bounded depth Frege. We make a first step towards this question by showing a super-linear lower bound: for every $k$, there exists $\epsilon &gt; 0$ such that any depth-$k$ Frege refutation of a random $n$-variable 3-CNF with $\Theta(n)$ clauses has $\Omega(n^{1 + \epsilon})$ steps w.h.p. Our proof involves a novel adaptation of the deterministic restriction technique of Chaudhuri and Radhakrishnan (STOC'96).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02275v2</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Svyatoslav Gryaznov, Navid Talebanfard</dc:creator>
    </item>
    <item>
      <title>Local Enumeration and Majority Lower Bounds</title>
      <link>https://arxiv.org/abs/2403.09134</link>
      <description>arXiv:2403.09134v3 Announce Type: replace 
Abstract: Depth-3 circuit lower bounds and $k$-SAT algorithms are intimately related; the state-of-the-art $\Sigma^k_3$-circuit lower bound and the $k$-SAT algorithm are based on the same combinatorial theorem. In this paper we define a problem which reveals new interactions between the two. Define Enum($k$, $t$) problem as: given an $n$-variable $k$-CNF and an initial assignment $\alpha$, output all satisfying assignments at Hamming distance $t$ from $\alpha$, assuming that there are no satisfying assignments of Hamming distance less than $t$ from $\alpha$. Observe that: an upper bound $b(n, k, t)$ on the complexity of Enum($k$, $t$) implies:
  - Depth-3 circuits: Any $\Sigma^k_3$ circuit computing the Majority function has size at least $\binom{n}{\frac{n}{2}}/b(n, k, \frac{n}{2})$.
  - $k$-SAT: There exists an algorithm solving $k$-SAT in time $O(\sum_{t = 1}^{n/2}b(n, k, t))$.
  A simple construction shows that $b(n, k, \frac{n}{2}) \ge 2^{(1 - O(\log(k)/k))n}$. Thus, matching upper bounds would imply a $\Sigma^k_3$-circuit lower bound of $2^{\Omega(\log(k)n/k)}$ and a $k$-SAT upper bound of $2^{(1 - \Omega(\log(k)/k))n}$. The former yields an unrestricted depth-3 lower bound of $2^{\omega(\sqrt{n})}$ solving a long standing open problem, and the latter breaks the Super Strong Exponential Time Hypothesis.
  In this paper, we propose a randomized algorithm for Enum($k$, $t$) and introduce new ideas to analyze it. We demonstrate the power of our ideas by considering the first non-trivial instance of the problem, i.e., Enum($3$, $\frac{n}{2}$). We show that the expected running time of our algorithm is $1.598^n$, substantially improving on the trivial bound of $3^{n/2} \simeq 1.732^n$. This already improves $\Sigma^3_3$ lower bounds for Majority function to $1.251^n$. The previous bound was $1.154^n$ which follows from the work of H{\aa}stad, Jukna, and Pudl\'ak (Comput. Complex.'95).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09134v3</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohit Gurumukhani, Ramamohan Paturi, Pavel Pudl\'ak, Michael Saks, Navid Talebanfard</dc:creator>
    </item>
    <item>
      <title>A Strongly Polynomial-Time Algorithm for Weighted General Factors with Three Feasible Degrees</title>
      <link>https://arxiv.org/abs/2301.11761</link>
      <description>arXiv:2301.11761v3 Announce Type: replace-cross 
Abstract: General factors are a generalization of matchings. Given a graph $G$ with a set $\pi(v)$ of feasible degrees, called a degree constraint, for each vertex $v$ of $G$, the general factor problem is to find a (spanning) subgraph $F$ of $G$ such that $\text{deg}_F(x) \in \pi(v)$ for every $v$ of $G$. When all degree constraints are symmetric $\Delta$-matroids, the problem is solvable in polynomial time. The weighted general factor problem is to find a general factor of the maximum total weight in an edge-weighted graph. In this paper, we present the first strongly polynomial-time algorithm for a type of weighted general factor problems with real-valued edge weights that is provably not reducible to the weighted matching problem by gadget constructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.11761v3</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuai Shao, Stanislav \v{Z}ivn\'y</dc:creator>
    </item>
    <item>
      <title>Quantum Advantage from One-Way Functions</title>
      <link>https://arxiv.org/abs/2302.04749</link>
      <description>arXiv:2302.04749v2 Announce Type: replace-cross 
Abstract: We demonstrate quantum advantage with several basic assumptions, specifically based on only the existence of OWFs. We introduce inefficient-verifier proofs of quantumness (IV-PoQ), and construct it from classical bit commitments. IV-PoQ is an interactive protocol between a verifier and a quantum prover consisting of two phases. In the first phase, the verifier is probabilistic polynomial-time, and it interacts with the prover. In the second phase, the verifier becomes inefficient, and makes its decision based on the transcript of the first phase. If the prover is honest, the inefficient verifier accepts with high probability, but any classical malicious prover only has a small probability of being accepted by the inefficient verifier. Our construction demonstrates the following results: (1)If one-way functions exist, then IV-PoQ exist. (2)If distributional collision-resistant hash functions exist (which exist if hard-on-average problems in $\mathbf{SZK}$ exist), then constant-round IV-PoQ exist. We also demonstrate quantum advantage based on worst-case-hard assumptions. We define auxiliary-input IV-PoQ (AI-IV-PoQ) that only require that for any malicious prover, there exist infinitely many auxiliary inputs under which the prover cannot cheat. We construct AI-IV-PoQ from an auxiliary-input version of commitments in a similar way, showing that (1)If auxiliary-input one-way functions exist (which exist if $\mathbf{CZK}\not\subseteq\mathbf{BPP}$), then AI-IV-PoQ exist. (2)If auxiliary-input collision-resistant hash functions exist (which is equivalent to $\mathbf{PWPP}\nsubseteq \mathbf{FBPP}$) or $\mathbf{SZK}\nsubseteq \mathbf{BPP}$, then constant-round AI-IV-PoQ exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.04749v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Crypto 2024</arxiv:journal_reference>
      <dc:creator>Tomoyuki Morimae, Takashi Yamakawa</dc:creator>
    </item>
    <item>
      <title>Logics with probabilistic team semantics and the Boolean negation</title>
      <link>https://arxiv.org/abs/2306.00420</link>
      <description>arXiv:2306.00420v2 Announce Type: replace-cross 
Abstract: We study the expressivity and the complexity of various logics in probabilistic team semantics with the Boolean negation. In particular, we study the extension of probabilistic independence logic with the Boolean negation, and a recently introduced logic FOPT. We give a comprehensive picture of the relative expressivity of these logics together with the most studied logics in probabilistic team semantics setting, as well as relating their expressivity to a numerical variant of second-order logic. In addition, we introduce novel entropy atoms and show that the extension of first-order logic by entropy atoms subsumes probabilistic independence logic. Finally, we obtain some results on the complexity of model checking, validity, and satisfiability of our logics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.00420v2</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miika Hannula, Minna Hirvonen, Juha Kontinen, Yasir Mahmood, Arne Meier, Jonni Virtema</dc:creator>
    </item>
    <item>
      <title>Space-bounded quantum state testing via space-efficient quantum singular value transformation</title>
      <link>https://arxiv.org/abs/2308.05079</link>
      <description>arXiv:2308.05079v2 Announce Type: replace-cross 
Abstract: Driven by exploring the power of quantum computation with a limited number of qubits, we present a novel complete characterization for space-bounded quantum computation, which encompasses settings with one-sided error (unitary coRQL) and two-sided error (BQL), approached from a quantum state testing perspective:
  - The first family of natural complete problems for unitary coRQL, i.e., space-bounded quantum state certification for trace distance and Hilbert-Schmidt distance;
  - A new family of natural complete problems for BQL, i.e., space-bounded quantum state testing for trace distance, Hilbert-Schmidt distance, and quantum entropy difference.
  In the space-bounded quantum state testing problem, we consider two logarithmic-qubit quantum circuits (devices) denoted as $Q_0$ and $Q_1$, which prepare quantum states $\rho_0$ and $\rho_1$, respectively, with access to their ``source code''. Our goal is to decide whether $\rho_0$ is $\epsilon_1$-close to or $\epsilon_2$-far from $\rho_1$ with respect to a specified distance-like measure. Interestingly, unlike time-bounded state testing problems, our results reveal that the space-bounded state testing problems all correspond to the same class. Moreover, our algorithms on the trace distance inspire an algorithmic Holevo-Helstrom measurement, implying QSZK is in QIP(2) with a quantum linear-space honest prover.
  Our results primarily build upon a space-efficient variant of the quantum singular value transformation (QSVT) introduced by Gily\'en, Su, Low, and Wiebe (STOC 2019), which is of independent interest. Our technique provides a unified approach for designing space-bounded quantum algorithms. Specifically, we show that implementing QSVT for any bounded polynomial that approximates a piecewise-smooth function incurs only a constant overhead in terms of the space required for special forms of the projected unitary encoding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05079v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Le Gall, Yupan Liu, Qisheng Wang</dc:creator>
    </item>
    <item>
      <title>Causal Discovery under Latent Class Confounding</title>
      <link>https://arxiv.org/abs/2311.07454</link>
      <description>arXiv:2311.07454v4 Announce Type: replace-cross 
Abstract: An acyclic causal structure can be described using a directed acyclic graph (DAG) with arrows indicating causation. The task of learning this structure from data is known as "causal discovery." Diverse populations or changing environments can sometimes give rise to heterogeneous data. This heterogeneity can be thought of as a mixture model with multiple "sources," each exerting their own distinct signature on the observed variables. From this perspective, the source is a latent common cause for every observed variable. While some methods for causal discovery are able to work around unobserved confounding in special cases, the only known ways to deal with a global confounder (such as a latent class) involve parametric assumptions. Focusing on discrete observables, we demonstrate that globally confounded causal structures can still be identifiable without parametric assumptions, so long as the number of latent classes remains small relative to the size and sparsity of the underlying DAG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07454v4</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bijan Mazaheri, Spencer Gordon, Yuval Rabani, Leonard Schulman</dc:creator>
    </item>
    <item>
      <title>Chain of Thought Empowers Transformers to Solve Inherently Serial Problems</title>
      <link>https://arxiv.org/abs/2402.12875</link>
      <description>arXiv:2402.12875v3 Announce Type: replace-cross 
Abstract: Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\mathsf{poly}(n)$ embedding size can only solve problems in $\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\mathsf{AC}^0$, a proper subset of $ \mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12875v3</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma</dc:creator>
    </item>
    <item>
      <title>Real Stability and Log Concavity are coNP-Hard</title>
      <link>https://arxiv.org/abs/2405.00162</link>
      <description>arXiv:2405.00162v2 Announce Type: replace-cross 
Abstract: Real-stable, Lorentzian, and log-concave polynomials are well-studied classes of polynomials, and have been powerful tools in resolving several conjectures. We show that the problems of deciding whether a polynomial of fixed degree is real stable or log concave are coNP-hard. On the other hand, while all homogeneous real-stable polynomials are Lorentzian and all Lorentzian polynomials are log concave on the positive orthant, the problem of deciding whether a polynomial of fixed degree is Lorentzian can be solved in polynomial time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00162v2</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tracy Chin</dc:creator>
    </item>
  </channel>
</rss>
