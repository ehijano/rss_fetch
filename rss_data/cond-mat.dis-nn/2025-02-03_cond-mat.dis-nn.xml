<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Feb 2025 05:00:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Electrical conductivity of conductive films based on random metallic nanowire networks</title>
      <link>https://arxiv.org/abs/2501.18925</link>
      <description>arXiv:2501.18925v1 Announce Type: new 
Abstract: Using computer simulation, we investigated the dependence of the electrical conductivity of random two-dimensional systems of straight nanowires on the main parameters. Both the resistance of the conductors and the resistance of the contacts between them were taken into account. The dependence of the resistance, $R$, between network nodes on the distance between nodes, $r$, is $R(r) = R_\Box/\pi \ln r + \mathrm{const}$, where $R_\Box$ is the sheet resistance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18925v1</guid>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuri Yu. Tarasevich, Andrei V. Eserkepov, Irina V. Vodolazskaya</dc:creator>
    </item>
    <item>
      <title>Statistical Physics of Deep Neural Networks: Generalization Capability, Beyond the Infinite Width, and Feature Learning</title>
      <link>https://arxiv.org/abs/2501.19281</link>
      <description>arXiv:2501.19281v1 Announce Type: new 
Abstract: Deep Neural Networks (DNNs) excel at many tasks, often rivaling or surpassing human performance. Yet their internal processes remain elusive, frequently described as "black boxes." While performance can be refined experimentally, achieving a fundamental grasp of their inner workings is still a challenge.
  Statistical Mechanics has long tackled computational problems, and this thesis applies physics-based insights to understand DNNs via three complementary approaches.
  First, by averaging over data, we derive an asymptotic bound on generalization that depends solely on the size of the last layer, rather than on the total number of parameters -- revealing how deep architectures process information differently across layers.
  Second, adopting a data-dependent viewpoint, we explore a finite-width thermodynamic limit beyond the infinite-width regime. This leads to: (i) a closed-form expression for the generalization error in a finite-width one-hidden-layer network (regression task); (ii) an approximate partition function for deeper architectures; and (iii) a link between deep networks in this thermodynamic limit and Student's t-processes.
  Finally, from a task-explicit perspective, we present a preliminary analysis of how DNNs interact with a controlled dataset, investigating whether they truly internalize its structure -- collapsing to the teacher -- or merely memorize it. By understanding when a network must learn data structure rather than just memorize, it sheds light on fostering meaningful internal representations.
  In essence, this thesis leverages the synergy between Statistical Physics and Machine Learning to illuminate the inner behavior of DNNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19281v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastiano Ariosto</dc:creator>
    </item>
    <item>
      <title>Subharmonic spin correlations and spectral pairing in Floquet time crystals</title>
      <link>https://arxiv.org/abs/2501.18760</link>
      <description>arXiv:2501.18760v1 Announce Type: cross 
Abstract: Floquet time crystals are characterized by subharmonic behavior of temporal correlation functions. Studying the paradigmatic time crystal based on the disordered Floquet quantum Ising model, we show that its temporal spin correlations are directly related to spectral characteristics and that this relation provides analytical expressions for the correlation function of finite chains, which compare favorably with numerical simulations. Specifically, we show that the disorder-averaged temporal spin correlations are proportional to the Fourier transform of the splitting distribution of the pairs of eigenvalues of the Floquet operator, which differ by $\pi$ to exponential accuracy in the chain length. We find that the splittings are well described by a log-normal distribution, implying that the temporal spin correlations are characterized by two parameters. We discuss possible implications for the phase diagram of the Floquet time crystals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18760v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>quant-ph</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander-Georg Penner, Harald Schmid, Leonid I. Glazman, Felix von Oppen</dc:creator>
    </item>
    <item>
      <title>Proportional asymptotics of piecewise exponential proportional hazards models</title>
      <link>https://arxiv.org/abs/2501.18995</link>
      <description>arXiv:2501.18995v1 Announce Type: cross 
Abstract: We study the flexible piecewise exponential model in a high dimensional setting where the number of covariates $p$ grows proportionally to the number of observations $n$ and under the hypothesis of random uncorrelated Gaussian designs. We prove rigorously that the optimal ridge penalized log-likelihood of the model converges in probability to the saddle point of a surrogate objective function. The technique of proof is the Convex Gaussian Min-Max theorem of Thrampoulidis, Oymak and Hassibi. An important consequence of this result, is that we can study the impact of the ridge regularization on the estimates of the parameter of the model and the prediction error as a function of the ratio $p/n &gt; 0$. Furthermore, these results represent a first step toward rigorously proving the (conjectured) correctness of several results obtained with the heuristic replica method for the Cox semi-parametric model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18995v1</guid>
      <category>math.ST</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Massa</dc:creator>
    </item>
    <item>
      <title>Learning the Hamiltonian Matrix of Large Atomic Systems</title>
      <link>https://arxiv.org/abs/2501.19110</link>
      <description>arXiv:2501.19110v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) have shown promise in learning the ground-state electronic properties of materials, subverting ab initio density functional theory (DFT) calculations when the underlying lattices can be represented as small and/or repeatable unit cells (i.e., molecules and periodic crystals). Realistic systems are, however, non-ideal and generally characterized by higher structural complexity. As such, they require large (10+ Angstroms) unit cells and thousands of atoms to be accurately described. At these scales, DFT becomes computationally prohibitive, making GNNs especially attractive. In this work, we present a strictly local equivariant GNN capable of learning the electronic Hamiltonian (H) of realistically extended materials. It incorporates an augmented partitioning approach that enables training on arbitrarily large structures while preserving local atomic environments beyond boundaries. We demonstrate its capabilities by predicting the electronic Hamiltonian of various systems with up to 3,000 nodes (atoms), 500,000+ edges, ~28 million orbital interactions (nonzero entries of H), and $\leq$0.55% error in the eigenvalue spectra. Our work expands the applicability of current electronic property prediction methods to some of the most challenging cases encountered in computational materials science, namely systems with disorder, interfaces, and defects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19110v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Hao Xia, Manasa Kaniselvan, Alexandros Nikolaos Ziogas, Marko Mladenovi\'c, Rayen Mahjoub, Alexander Maeder, Mathieu Luisier</dc:creator>
    </item>
    <item>
      <title>A theoretical framework for overfitting in energy-based modeling</title>
      <link>https://arxiv.org/abs/2501.19158</link>
      <description>arXiv:2501.19158v1 Announce Type: cross 
Abstract: We investigate the impact of limited data on training pairwise energy-based models for inverse problems aimed at identifying interaction networks. Utilizing the Gaussian model as testbed, we dissect training trajectories across the eigenbasis of the coupling matrix, exploiting the independent evolution of eigenmodes and revealing that the learning timescales are tied to the spectral decomposition of the empirical covariance matrix. We see that optimal points for early stopping arise from the interplay between these timescales and the initial conditions of training. Moreover, we show that finite data corrections can be accurately modeled through asymptotic random matrix theory calculations and provide the counterpart of generalized cross-validation in the energy based model context. Our analytical framework extends to binary-variable maximum-entropy pairwise models with minimal variations. These findings offer strategies to control overfitting in discrete-variable models through empirical shrinkage corrections, improving the management of overfitting in energy-based generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19158v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giovanni Catania, Aur\'elien Decelle, Cyril Furtlehner, Beatriz Seoane</dc:creator>
    </item>
    <item>
      <title>How Random Are Ergodic Eigenstates of the Ultrametric Random Matrices and the Quantum Sun Model?</title>
      <link>https://arxiv.org/abs/2501.19244</link>
      <description>arXiv:2501.19244v1 Announce Type: cross 
Abstract: We numerically study the extreme-value statistics of the Schmidt eigenvalues of reduced density matrices obtained from the ergodic eigenstates. We start by exploring the extreme value statistics of the ultrametric random matrices and then the related Quantum Sun Model, which is also a toy model of avalanche theory. It is expected that these ergodic eigenstates are purely random and thus possess random matrix theory-like features, and the corresponding eigenvalue density should follow the universal Marchenko-Pastur law. Nonetheless, we find deviations, specifically near the tail in both cases. Similarly, the distribution of maximum eigenvalue, after appropriate centering and scaling, should follow the Tracy-Widom distribution. However, our results show that, for both the ultrametric random matrix and the Quantum Sun model, it can be better described using the extreme value distribution. As the extreme value distribution is associated with uncorrelated or weakly correlated random variables, the results hence indicate that the Schmidt eigenvalues exhibit much weaker correlations compared to the strong correlations typically observed in Wishart matrices. Similar deviations are observed for the case of minimum Schmidt eigenvalues as well . Despite the spectral statistics, such as nearest neighbor spacing ratios, aligning with the random matrix theory predictions, our findings reveal that randomness is still not fully achieved. This suggests that deviations in extreme-value statistics offer a stringent test to probe the randomness of ergodic eigenstates and can provide deeper insights into the underlying structure and correlations in ergodic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19244v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanay Pathak</dc:creator>
    </item>
    <item>
      <title>Financial instability transition under heterogeneous investments and portfolio diversification</title>
      <link>https://arxiv.org/abs/2501.19260</link>
      <description>arXiv:2501.19260v1 Announce Type: cross 
Abstract: We analyze the stability of financial investment networks, where financial institutions hold overlapping portfolios of assets. We consider the effect of portfolio diversification and heterogeneous investments using a random matrix dynamical model driven by portfolio rebalancing. While heterogeneity generally correlates with heightened volatility, increasing diversification may have a stabilizing or destabilizing effect depending on the connectivity level of the network. The stability/instability transition is dictated by the largest eigenvalue of the random matrix governing the time evolution of the endogenous components of the returns, for which different approximation schemes are proposed and tested against numerical diagonalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19260v1</guid>
      <category>q-fin.RM</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Preben Forer, Barak Budnick, Pierpaolo Vivo, Sabrina Aufiero, Silvia Bartolucci, Fabio Caccioli</dc:creator>
    </item>
    <item>
      <title>Top eigenvalue statistics of diluted Wishart matrices</title>
      <link>https://arxiv.org/abs/2501.19280</link>
      <description>arXiv:2501.19280v1 Announce Type: cross 
Abstract: Using the replica method, we compute analytically the average largest eigenvalue of diluted covariance matrices of the form $\mathbf{J} = \mathbf{X}^T \mathbf{X}$, where $\mathbf{X}$ is a $N\times M$ sparse data matrix, in the limit of large $N,M$ with fixed ratio. We allow for random non-zero weights, provided they lead to an isolated largest eigenvalue. By formulating the problem as the optimisation of a quadratic Hamiltonian constrained to the $N$-sphere at low temperatures, we derive a set of recursive distributional equations for auxiliary probability density functions, which can be efficiently solved using a population dynamics algorithm. The average largest eigenvalue is identified with a Lagrange parameter that governs the convergence of the algorithm. We find excellent agreement between our analytical results and numerical results obtained from direct diagonalisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19280v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Barak Budnick, Preben Forer, Pierpaolo Vivo, Sabrina Aufiero, Silvia Bartolucci, Fabio Caccioli</dc:creator>
    </item>
    <item>
      <title>Random features and polynomial rules</title>
      <link>https://arxiv.org/abs/2402.10164</link>
      <description>arXiv:2402.10164v2 Announce Type: replace 
Abstract: Random features models play a distinguished role in the theory of deep learning, describing the behavior of neural networks close to their infinite-width limit. In this work, we present a thorough analysis of the generalization performance of random features models for generic supervised learning problems with Gaussian data. Our approach, built with tools from the statistical mechanics of disordered systems, maps the random features model to an equivalent polynomial model, and allows us to plot average generalization curves as functions of the two main control parameters of the problem: the number of random features $N$ and the size $P$ of the training set, both assumed to scale as powers in the input dimension $D$. Our results extend the case of proportional scaling between $N$, $P$ and $D$. They are in accordance with rigorous bounds known for certain particular learning tasks and are in quantitative agreement with numerical experiments performed over many order of magnitudes of $N$ and $P$. We find good agreement also far from the asymptotic limits where $D\to \infty$ and at least one between $P/D^K$, $N/D^L$ remains finite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10164v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.21468/SciPostPhys.18.1.039</arxiv:DOI>
      <arxiv:journal_reference>SciPost Phys. 18, 039 (2025)</arxiv:journal_reference>
      <dc:creator>Fabi\'an Aguirre-L\'opez, Silvio Franz, Mauro Pastore</dc:creator>
    </item>
    <item>
      <title>Restoring balance: principled under/oversampling of data for optimal classification</title>
      <link>https://arxiv.org/abs/2405.09535</link>
      <description>arXiv:2405.09535v2 Announce Type: replace 
Abstract: Class imbalance in real-world data poses a common bottleneck for machine learning tasks, since achieving good generalization on under-represented examples is often challenging. Mitigation strategies, such as under or oversampling the data depending on their abundances, are routinely proposed and tested empirically, but how they should adapt to the data statistics remains poorly understood. In this work, we determine exact analytical expressions of the generalization curves in the high-dimensional regime for linear classifiers (Support Vector Machines). We also provide a sharp prediction of the effects of under/oversampling strategies depending on class imbalance, first and second moments of the data, and the metrics of performance considered. We show that mixed strategies involving under and oversampling of data lead to performance improvement. Through numerical experiments, we show the relevance of our theoretical predictions on real datasets, on deeper architectures and with sampling strategies based on unsupervised probabilistic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09535v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 41st International Conference on Machine Learning (ICML'24), PMLR 235:32643-32670, 2024</arxiv:journal_reference>
      <dc:creator>Emanuele Loffredo, Mauro Pastore, Simona Cocco, R\'emi Monasson</dc:creator>
    </item>
    <item>
      <title>Asymptotic Dynamics of Alternating Minimization for Bilinear Regression</title>
      <link>https://arxiv.org/abs/2402.04751</link>
      <description>arXiv:2402.04751v3 Announce Type: replace-cross 
Abstract: This study investigates the dynamics of alternating minimization applied to a bilinear regression task with normally distributed covariates, under the asymptotic system size limit where the number of parameters and observations diverge at the same rate. This is achieved by employing the replica method to a multi-temperature glassy system which unfolds the algorithm's time evolution. Our results show that the dynamics can be described effectively by a two-dimensional discrete stochastic process, where each step depends on all previous time steps, revealing the structure of the memory dependence in the evolution of alternating minimization. The theoretical framework developed in this work can be applied to the analysis of various iterative algorithms, extending beyond the scope of alternating minimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04751v3</guid>
      <category>math.OC</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koki Okajima, Takashi Takahashi</dc:creator>
    </item>
    <item>
      <title>A novel perspective on denoising using quantum localization with application to medical imaging</title>
      <link>https://arxiv.org/abs/2405.12226</link>
      <description>arXiv:2405.12226v3 Announce Type: replace-cross 
Abstract: Background noise in many fields such as medical imaging poses significant challenges for accurate diagnosis, prompting the development of denoising algorithms. Traditional methodologies, however, often struggle to address the complexities of noisy environments in high dimensional imaging systems. This paper introduces a novel quantum-inspired approach for image denoising, drawing upon principles of quantum and condensed matter physics. Our approach views medical images as amorphous structures akin to those found in condensed matter physics and we propose an algorithm that incorporates the concept of mode resolved localization directly into the denoising process. Notably, unlike previous studies that considered localization as a hindrance, our approach considers quantum localization as a fundamental component of image reconstruction which is used to differentiate between noisy and non-noisy modes based on diffusivity and localization measurements. This perspective eliminates the need for hyperparameter tuning, making the proposed method a standalone algorithm which can be implemented with minimal manual intervention and can perform automatic filtering of noise regardless of noise level. Through numerical validation, we showcase the effectiveness of our approach in addressing noise-related challenges in imaging and especially medical imaging, underscoring its relevance for possible quantum computing applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12226v3</guid>
      <category>eess.IV</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.med-ph</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amirreza Hashemi, Sayantan Dutta, Bertrand Georgeot, Denis Kouame, Hamid Sabet</dc:creator>
    </item>
  </channel>
</rss>
