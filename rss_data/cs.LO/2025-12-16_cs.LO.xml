<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Dec 2025 02:40:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Layered Monoidal Theories</title>
      <link>https://arxiv.org/abs/2512.12139</link>
      <description>arXiv:2512.12139v1 Announce Type: new 
Abstract: In the first part, we develop layered monoidal theories - a generalisation of monoidal theories combining descriptions of a system at several levels. Via their representation as string diagrams, monoidal theories provide a graphical syntax with a visually intuitive notions of information flow and composition. Layered monoidal theories allow mixing several monoidal theories (together with translations between them) within the same string diagram, while retaining mathematical precision and semantic interpretability. We define three flavours of layered monoidal theories, provide a recursively generated syntax for each, and construct a free-forgetful adjunction with respect to three closely related semantics: opfibrations, fibrations and deflations. We motivate the general theory by providing several examples from existing literature.
  In the second part, we develop a formal approach to retrosynthesis - the process of backwards reaction search in synthetic chemistry. Chemical processes are treated at three levels of abstraction: (1) (formal) reactions encode all chemically feasible combinatorial rearrangements of molecules, (2) reaction schemes encode transformations applicable to 'patches' of molecules (including the functional groups), and (3) disconnection rules encode local chemical rewrite rules applicable to a single bond or atom at a time. We show that the three levels are tightly linked: the reactions are generated by the reaction schemes, while there is a functorial translation from the disconnection rules to the reactions. Moreover, the translation from the disconnection rules to the reactions is shown to be sound, complete and universal - allowing one to treat the disconnection rules as a formal syntax with the semantics provided by the reactions. We tie together the two parts by providing a formalisation of retrosynthesis within a certain layered monoidal theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12139v1</guid>
      <category>cs.LO</category>
      <category>math.CT</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leo Lobski</dc:creator>
    </item>
    <item>
      <title>An STREL-based Formulation of Spatial Resilience in Cyber-Physical Systems</title>
      <link>https://arxiv.org/abs/2512.12511</link>
      <description>arXiv:2512.12511v1 Announce Type: new 
Abstract: Resiliency is the ability of a system to quickly recover from a violation (recoverability) and avoid future violations for as long as possible (durability). In the spatial setting, recoverability and durability (now known as persistency) are measured in units of distance. Like its temporal counterpart, spatial resiliency is of fundamental importance for Cyber-Physical Systems (CPS) and yet, to date, there is no widely agreed-upon formal treatment of spatial resiliency. We present a formal framework for reasoning about spatial resiliency in CPS. Our framework is based on the spatial fragment of STREL, which we refer to as SREL. In this framework, spatial resiliency is given a syntactic characterization in the form of a Spatial Resiliency Specification (SpaRS). An atomic predicate of SpaRS is called an S-atom. Given an arbitrary SREL formula $\varphi$, distance bounds $d_1, d_2$, the S-atom of $\varphi$, $S_{d_1, d_2} (\varphi)$, is the SREL formula $\neg\varphi R_{[0,d_1]} (\varphi R_{[d_2, +\infty)}\varphi)$, specifying that recovery from a violation of $\varphi$ occurs within distance $d_1$ (recoverability), and subsequently that $\varphi$ be maintained along a route for a distance greater than $d_2$ (persistency). S-atoms can be combined using spatial STREL operators, allowing one to express composite resiliency specifications. We define a quantitative semantics for SpaRS in the form of a Spatial Resilience Value (SpaRV) function $\sigma$ and prove its soundness and completeness w.r.t. SREL's Boolean semantics. The $\sigma$-value for $S_{d_1,d_2}(\varphi)$ is a set of non-dominated (rec, per) pairs, quantifying recoverability and persistency, given that some routes may offer better recoverability while others better persistency. In addition, we design algorithms to evaluate SpaRV for SpaRS formulas. Finally, two case studies demonstrate the practical utility of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12511v1</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeyu Zhang, Hongkai Chen, Nicola Paoletti, Shan Lin, Scott A. Smolka</dc:creator>
    </item>
    <item>
      <title>Cargo Sherlock: An SMT-Based Checker for Software Trust Costs</title>
      <link>https://arxiv.org/abs/2512.12553</link>
      <description>arXiv:2512.12553v1 Announce Type: new 
Abstract: Supply chain attacks threaten open-source software ecosystems. This paper proposes a formal framework for quantifying trust in third-party software dependencies that is both formally checkable - formalized in satisfiability modulo theories (SMT) - while at the same time incorporating human factors, like the number of downloads, authors, and other metadata that are commonly used to identify trustworthy software in practice. We use data from both software analysis tools and metadata to build a first-order relational model of software dependencies; to obtain an overall "trust cost" combining these factors, we propose a formalization based on the minimum trust problem which asks for the minimum cost of a set of assumptions which can be used to prove that the code is safe. We implement these ideas in Cargo Sherlock, targeted for Rust libraries (crates), incorporating a list of candidate assumptions motivated by quantifiable trust metrics identified in prior work. Our evaluation shows that Cargo Sherlock can be used to identify synthetically generated supply chain attacks and known incidents involving typosquatted and poorly AI-maintained crates, and that its performance scales to Rust crates with many dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12553v1</guid>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Hassnain, Anirudh Basu, Ethan Ng, Caleb Stanford</dc:creator>
    </item>
    <item>
      <title>Argumentative Reasoning with Language Models on Non-factorized Case Bases</title>
      <link>https://arxiv.org/abs/2512.12656</link>
      <description>arXiv:2512.12656v1 Announce Type: new 
Abstract: In this paper, we investigate how language models can perform case-based reasoning (CBR) on non-factorized case bases. We introduce a novel framework, argumentative agentic models for case-based reasoning (AAM-CBR), which extends abstract argumentation for case-based reasoning (AA-CBR). Unlike traditional approaches that require factorization of previous cases, AAM-CBR leverages language models to determine case coverage and extract factors based on new cases. This enables factor-based reasoning without exposing or preprocessing previous cases, thus improving both flexibility and privacy. We also present initial experiments to assess AAM-CBR performance by comparing the proposed framework with a baseline that uses a single-prompt approach to incorporate both new and previous cases. The experiments are conducted based on a synthetic credit card application dataset. The result shows that AAM-CBR surpasses the baseline only when the new case contains a richer set of factors. The finding indicates that language models can handle case-based reasoning with a limited number of factors, but face challenges as the number of factors increase. Consequently, integrating symbolic reasoning with language models, as implemented in AAM-CBR, is crucial for effectively handling cases involving many factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12656v1</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wachara Fungwacharakorn, May Myo Zin, Ha-Thanh Nguyen, Yuntao Kong, Ken Satoh</dc:creator>
    </item>
    <item>
      <title>A neuro-symbolic framework for accountability in public-sector AI</title>
      <link>https://arxiv.org/abs/2512.12109</link>
      <description>arXiv:2512.12109v1 Announce Type: cross 
Abstract: Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12109v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Allen Daniel Sunny</dc:creator>
    </item>
    <item>
      <title>Semitopological Barycentric Algebras</title>
      <link>https://arxiv.org/abs/2512.12865</link>
      <description>arXiv:2512.12865v1 Announce Type: cross 
Abstract: Barycentric algebras are an abstraction of the notion of convex sets, defined by a set of equations. We study semitopological and topological barycentric algebras, in the spirit of a previous study by Klaus Keimel on semitopological and topological cones (2008), which are special cases of semitopological and topological barycentric algebras.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12865v1</guid>
      <category>math.FA</category>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean Goubault-Larrecq</dc:creator>
    </item>
    <item>
      <title>Database Research needs an Abstract Relational Query Language</title>
      <link>https://arxiv.org/abs/2512.12957</link>
      <description>arXiv:2512.12957v1 Announce Type: cross 
Abstract: For decades, SQL has been the default language for composing queries, but it is increasingly used as an artifact to be read and verified rather than authored. With Large Language Models (LLMs), queries are increasingly machine-generated, while humans read, validate, and debug them. This shift turns relational query languages into interfaces for back-and-forth communication about intent, which will lead to a rethinking of relational language design, and more broadly, relational interface design.
  We argue that this rethinking needs support from an Abstract Relational Query Language (ARQL): a semantics-first reference metalanguage that separates query intent from user-facing syntax and makes underlying relational patterns explicit and comparable across user-facing languages. An ARQL separates a query into (i) a relational core (the compositional structure that determines intent), (ii) modalities (alternative representations of that core tailored to different audiences), and (iii) conventions (orthogonal environment-level semantic parameters under which the core is interpreted, e.g., set vs. bag semantics, or treatment of null values). Usability for humans or machines then depends less on choosing a particular language and more on choosing an appropriate modality. Comparing languages becomes a question of which relational patterns they support and what conventions they choose.
  We introduce Abstract Relational Calculus (ARC), a strict generalization of Tuple Relational Calculus (TRC), as a concrete instance of ARQL. ARC comes in three modalities: (i) a comprehension-style textual notation, (ii) an Abstract Language Tree (ALT) for machine reasoning about meaning, and (iii) a diagrammatic hierarchical graph (higraph) representation for humans. ARC provides the missing vocabulary and acts as a Rosetta Stone for relational querying.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12957v1</guid>
      <category>cs.DB</category>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wolfgang Gatterbauer, Diandre Miguel Sabale</dc:creator>
    </item>
    <item>
      <title>Homomorphism Indistinguishability, Multiplicity Automata Equivalence, and Polynomial Identity Testing</title>
      <link>https://arxiv.org/abs/2512.13058</link>
      <description>arXiv:2512.13058v1 Announce Type: cross 
Abstract: Two graphs $G$ and $H$ are homomorphism indistinguishable over a graph class $\mathcal{F}$ if they admit the same number of homomorphisms from every graph $F \in \mathcal{F}$. Many graph isomorphism relaxations such as (quantum) isomorphism and cospectrality can be characterised as homomorphism indistinguishability over specific graph classes. Thereby, the problems $\textrm{HomInd}(\mathcal{F})$ of deciding homomorphism indistinguishability over $\mathcal{F}$ subsume diverse graph isomorphism relaxations whose complexities range from logspace to undecidable. Establishing the first general result on the complexity of $\textrm{HomInd}(\mathcal{F})$, Seppelt (MFCS 2024) showed that $\textrm{HomInd}(\mathcal{F})$ is in randomised polynomial time for every graph class $\mathcal{F}$ of bounded treewidth that can be defined in counting monadic second-order logic $\mathsf{CMSO}_2$.
  We show that this algorithm is conditionally optimal, i.e. it cannot be derandomised unless polynomial identity testing is in $\mathsf{PTIME}$. For $\mathsf{CMSO}_2$-definable graph classes $\mathcal{F}$ of bounded pathwidth, we improve the previous complexity upper bound for $\textrm{HomInd}(\mathcal{F})$ from $\mathsf{PTIME}$ to $\mathsf{C}_=\mathsf{L}$ and show that this is tight. Secondarily, we establish a connection between homomorphism indistinguishability and multiplicity automata equivalence which allows us to pinpoint the complexity of the latter problem as $\mathsf{C}_=\mathsf{L}$-complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13058v1</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Marek \v{C}ern\'y, Tim Seppelt</dc:creator>
    </item>
    <item>
      <title>Fine-tuned LLM-based Code Migration Framework</title>
      <link>https://arxiv.org/abs/2512.13515</link>
      <description>arXiv:2512.13515v1 Announce Type: cross 
Abstract: The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13515v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oleg Grynets, Vasyl Lyashkevych, Dmytro Baran, Maksym Orliansky, Taras Zelenyy, Markiian Leshchyshyn</dc:creator>
    </item>
    <item>
      <title>Towards the type safety of Pure Subtype Systems (Full version)</title>
      <link>https://arxiv.org/abs/2407.13882</link>
      <description>arXiv:2407.13882v2 Announce Type: replace 
Abstract: Hutchins' Pure Subtype Systems (PSS) offer a unified framework for types and terms, promising significant advancements in language design for features like dependent types and higher-order subtyping. However, the theory has been hampered by a critical gap: a proof of type safety has remained an open problem for over a decade. The original attempt to prove this property relied on the conjectured commutativity of two fundamental reduction relations, equivalence and subtyping. Proving transitivity elimination, however, requires this commutativity, a property that is notoriously difficult to establish for higher-order subtyping systems.
  In this paper, we address this issue by introducing Machine-Based PSS (MPSS), a novel reformulation of the original system. MPSS integrates a continuation stack mechanism, reminiscent of the Krivine Abstract Machine, to keep track of arguments that are passed during function application, enabling more fine-grained reductions. This architectural change exposes crucial intermediate reduction steps that were absent in the original PSS. The primary contribution of our work is a direct proof that the equivalence and subtyping reductions in MPSS commute. This result formally establishes transitivity elimination, which is the cornerstone of the inversion lemma required for type safety. We conclude by outlining a pathway from our foundational result to a complete, type-safe system, thereby paving the way for the practical realization of PSS-based languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13882v2</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.CSL.2026.9</arxiv:DOI>
      <dc:creator>Valentin Pasquale, \'Alvaro Garc\'ia-P\'erez</dc:creator>
    </item>
    <item>
      <title>Computation and Concurrency</title>
      <link>https://arxiv.org/abs/2409.02595</link>
      <description>arXiv:2409.02595v4 Announce Type: replace 
Abstract: We try to clarify the relationship between computation and concurrency. Base on the so-called pomsetc automata, we introduce communication and more operators, and establish the algebras modulo language equivalence and truly concurrent bisimilarities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02595v4</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong Wang</dc:creator>
    </item>
    <item>
      <title>A Dichotomy Theorem for Ordinal Ranks in MSO</title>
      <link>https://arxiv.org/abs/2501.05385</link>
      <description>arXiv:2501.05385v2 Announce Type: replace 
Abstract: We focus on formulae $\exists X.\, \varphi(\vec{Y}, X)$ of monadic second-order logic over the full binary tree, such that the witness $X$ is a well-founded set. The ordinal rank $\mathrm{rank}(X) &lt; \omega_1$ of such a set $X$ measures its depth and branching structure. We search for the least upper bound for these ranks, and discover the following dichotomy depending on the formula $\varphi$. Let $\mathrm{rank}(\varphi)$ be the minimal ordinal such that, whenever an instance $\vec{Y}$ satisfies the formula, there is a witness $X$ with $\mathrm{rank}(X) \leq \mathrm{rank}(\varphi)$. Then $\mathrm{rank}(\varphi)$ is either strictly smaller than $\omega^2$ or it reaches the maximal possible value $\omega_1$. Moreover, it is decidable which of the cases holds. The result has potential for applications in a variety of ordinal-related problems, in particular it entails a result about the closure ordinal of a fixed-point formula.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05385v2</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damian Niwi\'nski, Pawe{\l} Parys, Micha{\l} Skrzypczak</dc:creator>
    </item>
    <item>
      <title>Kimina Lean Server: A High-Performance Lean Server for Large-Scale Verification</title>
      <link>https://arxiv.org/abs/2504.21230</link>
      <description>arXiv:2504.21230v3 Announce Type: replace 
Abstract: We introduce the Kimina Lean Server, an open-source project designed as a high-performance verifier for reinforcement learning pipelines. Built on top of the Lean REPL (Read-Eval-Print Loop) maintained by the Lean FRO, our server combines server-side parallelism by managing multiple Lean processes in parallel with a Least Recently Used (LRU) caching mechanism that reuses Lean imports across requests. On the client side, a lightweight Python package enables submitting proof batches and receiving Lean feedback, including extracted tactics and tactic states.
  Together, these features enable a scalable workflow for large-scale verification and data extraction. In our experiments, the Kimina Lean Server outperforms previous Lean interaction tools, achieving a 1.5 to 2 times speedup in verification time. Moreover, its improved efficiency has enabled its use in the large-scale training of state-of-the-art models such as Kimina-Prover.
  We hope that our open-source project will support the neural theorem proving community and accelerate future progress by enabling efficient large-scale verification and proof data extraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21230v3</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marco Dos Santos, Hugues de Saxc\'e, Haiming Wang, Ran Wang, Mantas Baksys, Mert Unsal, Junqi Liu, Zhengying Liu, Jia Li</dc:creator>
    </item>
    <item>
      <title>Characterizing Equivalence of Logically Constrained Terms via Existentially Constrained Terms (Full Version)</title>
      <link>https://arxiv.org/abs/2505.21986</link>
      <description>arXiv:2505.21986v3 Announce Type: replace 
Abstract: Logically constrained term rewriting is a rewriting framework that supports built-in data structures such as integers and bit vectors. Recently, constrained terms play a key role in various analyses and applications of logically constrained term rewriting. A fundamental question on constrained terms arising there is how to characterize equivalence between them. However, in the current literature only limited progress has been made on this. In this paper, we provide several sound and complete solutions to tackle this problem. Our key idea is the introduction of a novel concept, namely existentially constrained terms, into which the original form of constrained terms can be embedded. We present several syntactic characterizations of equivalence between existentially constrained terms. In particular, we provide two different kinds of complete characterizations: one is designed to facilitate equivalence checking, while the other is intended for theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21986v3</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-04848-6_12</arxiv:DOI>
      <dc:creator>Kanta Takahata, Jonas Sch\"opf, Naoki Nishida, Takahito Aoto</dc:creator>
    </item>
    <item>
      <title>Recovering Commutation of Logically Constrained Rewriting and Equivalence Transformations (Full Version)</title>
      <link>https://arxiv.org/abs/2507.09326</link>
      <description>arXiv:2507.09326v3 Announce Type: replace 
Abstract: Logically constrained term rewriting is a relatively new rewriting formalism that naturally supports built-in data structures, such as integers and bit vectors. In the analysis of logically constrained term rewrite systems (LCTRSs), rewriting constrained terms plays a crucial role. However, this combines rewrite rule applications and equivalence transformations in a closely intertwined way. This intertwining makes it difficult to establish useful theoretical properties for this kind of rewriting and causes problems in implementations -- namely, that impractically large search spaces are often required. To address this issue, we propose in this paper a novel notion of most general constrained rewriting, which operates on existentially constrained terms, a concept recently introduced by the authors. We define a class of left-linear, left-value-free LCTRSs that are general enough to simulate all left-linear LCTRSs and exhibit the desired key property: most general constrained rewriting commutes with equivalence. This property ensures that equivalence transformations can be deferred until after the application of rewrite rules, which helps mitigate the issue of large search spaces in implementations. In addition to that, we show that the original rewriting formalism on constrained terms can be embedded into our new rewriting formalism on existentially constrained terms. Thus, our results are expected to have significant implications for achieving correct and efficient implementations in tools operating on LCTRSs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09326v3</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3756907.3756916</arxiv:DOI>
      <dc:creator>Kanta Takahata, Jonas Sch\"opf, Naoki Nishida, Takahito Aoto</dc:creator>
    </item>
    <item>
      <title>Slightly Non-Linear Higher-Order Tree Transducers</title>
      <link>https://arxiv.org/abs/2402.05854</link>
      <description>arXiv:2402.05854v3 Announce Type: replace-cross 
Abstract: We investigate the tree-to-tree functions computed by "affine $\lambda$-transducers": tree automata whose memory consists of an affine $\lambda$-term instead of a finite state. They can be seen as variations on Gallot, Lemay and Salvati's Linear High-Order Deterministic Tree Transducers.
  When the memory is almost purely affine ($\textit{\`a la}$ Kanazawa), we show that these machines can be translated to tree-walking transducers (and with a purely affine memory, we get a reversible tree-walking transducer). This leads to a proof of an inexpressivity conjecture of Nguy\^en and Pradic on "implicit automata" in an affine $\lambda$-calculus. We also prove that a more powerful variant, extended with preprocessing by an MSO relabeling and allowing a limited amount of non-linearity, is equivalent in expressive power to Engelfriet, Hoogeboom and Samwel's invisible pebble tree transducers.
  The key technical tool in our proofs is the Interaction Abstract Machine (IAM), an operational avatar of Girard's geometry of interaction, a semantics of linear logic. We work with ad-hoc specializations to $\lambda$-terms of low exponential depth of a tree-generating version of the IAM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05854v3</guid>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L\^e Th\`anh D\~ung Nguy\^en, Gabriele Vanoni</dc:creator>
    </item>
    <item>
      <title>Taint Analysis for Graph APIs Focusing on Broken Access Control</title>
      <link>https://arxiv.org/abs/2501.08947</link>
      <description>arXiv:2501.08947v4 Announce Type: replace-cross 
Abstract: We present the first systematic approach to static and dynamic taint analysis for Graph APIs focusing on broken access control. The approach comprises the following. We taint nodes of the Graph API if they represent data requiring specific privileges in order to be retrieved or manipulated, and identify API calls which are related to sources and sinks. Then, we statically analyze whether a tainted information flow between API source and sink calls occurs. To this end, we model the API calls using graph transformation rules. We subsequently use Critical Pair Analysis to automatically analyze potential dependencies between rules representing source calls and rules representing sink calls. We distinguish direct from indirect tainted information flow and argue under which conditions the Critical Pair Analysis is able to detect not only direct, but also indirect tainted flow. The static taint analysis (i) identifies flows that need to be further reviewed, since tainted nodes may be created by an API call and used or manipulated by another API call later without having the necessary privileges, and (ii) can be used to systematically design dynamic security tests for broken access control. The dynamic taint analysis checks if potential broken access control risks detected during the static taint analysis really occur. We apply the approach to a part of the GitHub GraphQL API. The application illustrates that our analysis supports the detection of two types of broken access control systematically: the case where users of the API may not be able to access or manipulate information, although they should be able to do so; and the case where users (or attackers) of the API may be able to access/manipulate information that they should not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08947v4</guid>
      <category>cs.CR</category>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Leen Lambers, Lucas Sakizloglou, Taisiya Khakharova, Fernando Orejas</dc:creator>
    </item>
    <item>
      <title>Adversarial Barrier in Uniform Class Separation</title>
      <link>https://arxiv.org/abs/2512.08149</link>
      <description>arXiv:2512.08149v3 Announce Type: replace-cross 
Abstract: We identify a strong structural obstruction to Uniform Separation in constructive arithmetic. The mechanism is independent of semantic content; it emerges whenever two distinct evaluator predicates are sustained in parallel and inference remains uniformly representable in an extension of HA. Under these conditions, any putative Uniform Class Separation principle becomes a distinguished instance of a fixed point construction. The resulting limitation is stricter in scope than classical separation barriers (Baker; Rudich; Aaronson et al.) insofar as it constrains the logical form of uniform separation within HA, rather than limiting particular relativizing, naturalizing, or algebrizing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08149v3</guid>
      <category>math.LO</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milan Rosko</dc:creator>
    </item>
  </channel>
</rss>
