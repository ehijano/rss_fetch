<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Jun 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Impact of Social Isolation on Subjective Cognitive Decline in Older Adults: A Study Based on Network Analysis and Longitudinal Mode</title>
      <link>https://arxiv.org/abs/2506.13914</link>
      <description>arXiv:2506.13914v1 Announce Type: new 
Abstract: Social isolation (SI) in older adults has emerged as a critical mental health concern, with established links to cognitive decline. While depression is hypothesized to mediate this relationship, the longitudinal mechanisms remain unclear. This study employed network analysis and cross-lagged modeling to examine these relationships during pandemic-related social restrictions. We collected data from 1,230 older adults (Mage=64.49, SD=3.84) across three timepoints (during lockdown, immediately post-lockdown, and 6 months later). Network analysis identified depressive symptoms (particularly PHQ-9 item 9) as central nodes bridging SI and subjective cognitive decline (SCD). Longitudinal analyses revealed: 1) T1 SI predicted T2 depression; 2) T2 depression predicted T3 SCD. These patterns held for both online and offline SI, with excellent model fit. Our findings demonstrate depression's mediating role in the "SI-depression-SCD" pathway, highlighting how reduced social connections may gradually impair cognitive self-assessment through depressive symptoms. The results underscore the universal mental health impact of SI, regardless of interaction modality (virtual/in-person). These insights suggest that combined interventions targeting both social connection and depression management may be particularly effective for mitigating age-related cognitive decline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13914v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingchen Liu, Haixin Jiang</dc:creator>
    </item>
    <item>
      <title>Monitoring of Drift Patterns in Image Data</title>
      <link>https://arxiv.org/abs/2506.14260</link>
      <description>arXiv:2506.14260v1 Announce Type: new 
Abstract: Sequential monitoring of images has broad applications across various domains, including climate science, ecosystem monitoring, medical diagnostics, and so forth. In many such applications, images acquired over time exhibit gradual changes, referred to as drifts, which pose significant challenges for monitoring. Rather than detecting only abrupt step changes, it is crucial to monitor and characterize these drift patterns. Despite its practical importance, the problem of drift monitoring in image sequences has received limited attention. This paper addresses this gap by proposing a novel drift monitoring method based on an oblique-axis regression tree. It is particularly effective for monitoring drift patterns in the jump location curves present in the image intensity functions. By leveraging a decision tree framework, the method captures discontinuities both in spatial image intensity and temporal progression. A key advantage of this method lies in its flexibility: in the absence of drift, it remains capable of detecting abrupt step changes. Theoretical properties and numerical performance in diverse types of simulation settings indicate its broad applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14260v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Subhasish Basak, Anik Roy, Partha Sarathi Mukherjee</dc:creator>
    </item>
    <item>
      <title>A statistical framework for dynamic cognitive diagnosis in digital learning environments</title>
      <link>https://arxiv.org/abs/2506.14531</link>
      <description>arXiv:2506.14531v1 Announce Type: new 
Abstract: Reading is foundational for educational, employment, and economic outcomes, but a persistent proportion of students globally struggle to develop adequate reading skills. Some countries promote digital tools to support reading development, alongside regular classroom instruction. Such tools generate rich log data capturing students' behaviour and performance. This study proposes a dynamic cognitive diagnostic modeling (CDM) framework based on restricted latent class models to trace students' time-varying skills mastery using log files from digital tools. Unlike traditional CDMs that require expert-defined skill-item mappings (Q-matrix), our approach jointly estimates the Q-matrix and latent skill profiles, integrates log-derived covariates (e.g., reattempts, response times, counts of mastered items) and individual characteristics, and models transitions in mastery using a Bayesian estimation approach. Applied to real-world data, the model demonstrates practical value in educational settings by effectively uncovering individual skill profiles and the skill-item mappings. Simulation studies confirm robust recovery of Q-matrix structures and latent profiles with high accuracy under varied sample sizes, item counts and different sparsity of Q-matrices. The framework offers a data-driven, time-dependent restricted latent class modeling approach to understanding early reading development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14531v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yawen Ma, Anastasia Ushakova, Kate Cain, Gabriel Wallin</dc:creator>
    </item>
    <item>
      <title>Bayesian Hybrid Machine Learning of Gallstone Risk</title>
      <link>https://arxiv.org/abs/2506.14561</link>
      <description>arXiv:2506.14561v1 Announce Type: new 
Abstract: Gallstone disease is a complex, multifactorial condition with significant global health burdens. Identifying underlying risk factors and their interactions is crucial for early diagnosis, targeted prevention, and effective clinical management. Although logistic regression remains a standard tool for assessing associations between predictors and gallstone status, it often underperforms in high-dimensional settings and may fail to capture intricate relationships among variables. To address these limitations, we propose a hybrid machine learning framework that integrates robust variable selection with advanced interaction detection. Specifically, Adaptive LASSO is employed to identify a sparse and interpretable subset of influential features, followed by Bayesian Additive Regression Trees (BART) to model nonlinear effects and uncover key interactions. Selected interactions are further characterized by physiological knowledge through differential equation-informed interaction terms, grounding the model in biologically plausible mechanisms. The insights gained from these steps are then integrated into a final logistic regression model within a Bayesian framework, providing a balance between predictive accuracy and clinical interpretability. This proposed framework not only enhances prediction but also yields actionable insights, offering a valuable support tool for medical research and decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14561v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chitradipa Chakraborty, Nayana Mukherjee</dc:creator>
    </item>
    <item>
      <title>The use of cross validation in the analysis of designed experiments</title>
      <link>https://arxiv.org/abs/2506.14593</link>
      <description>arXiv:2506.14593v1 Announce Type: new 
Abstract: Cross-validation (CV) is a common method to tune machine learning methods and can be used for model selection in regression as well. Because of the structured nature of small, traditional experimental designs, the literature has warned against using CV in their analysis. The striking increase in the use of machine learning, and thus CV, in the analysis of experimental designs, has led us to empirically study the effectiveness of CV compared to other methods of selecting models in designed experiments, including the little bootstrap. We consider both response surface settings where prediction is of primary interest, as well as screening where factor selection is most important. Overall, we provide evidence that the use of leave-one-out cross-validation (LOOCV) in the analysis of small, structured is often useful. More general $k$-fold CV may also be competitive but its performance is uneven.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14593v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria L. Weese, Byran J. Smucker, David J. Edwards</dc:creator>
    </item>
    <item>
      <title>Markov Regime-Switching Intelligent Driver Model for Interpretable Car-Following Behavior</title>
      <link>https://arxiv.org/abs/2506.14762</link>
      <description>arXiv:2506.14762v1 Announce Type: new 
Abstract: Accurate and interpretable car-following models are essential for traffic simulation and autonomous vehicle development. However, classical models like the Intelligent Driver Model (IDM) are fundamentally limited by their parsimonious and single-regime structure. They fail to capture the multi-modal nature of human driving, where a single driving state (e.g., speed, relative speed, and gap) can elicit many different driver actions. This forces the model to average across distinct behaviors, reducing its fidelity and making its parameters difficult to interpret. To overcome this, we introduce a regime-switching framework that allows driving behavior to be governed by different IDM parameter sets, each corresponding to an interpretable behavioral mode. This design enables the model to dynamically switch between interpretable behavioral modes, rather than averaging across diverse driving contexts. We instantiate the framework using a Factorial Hidden Markov Model with IDM dynamics (FHMM-IDM), which explicitly separates intrinsic driving regimes (e.g., aggressive acceleration, steady-state following) from external traffic scenarios (e.g., free-flow, congestion, stop-and-go) through two independent latent Markov processes. Bayesian inference via Markov chain Monte Carlo (MCMC) is used to jointly estimate the regime-specific parameters, transition dynamics, and latent state trajectories. Experiments on the HighD dataset demonstrate that FHMM-IDM uncovers interpretable structure in human driving, effectively disentangling internal driver actions from contextual traffic conditions and revealing dynamic regime-switching patterns. This framework provides a tractable and principled solution to modeling context-dependent driving behavior under uncertainty, offering improvements in the fidelity of traffic simulations, the efficacy of safety analyses, and the development of more human-centric ADAS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14762v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chengyuan Zhang, Cathy Wu, Lijun Sun</dc:creator>
    </item>
    <item>
      <title>Ball path curvature and in-game free throw shooting proficiency in the National Basketball Association</title>
      <link>https://arxiv.org/abs/2506.13779</link>
      <description>arXiv:2506.13779v1 Announce Type: cross 
Abstract: Basketball shooting coaches agree that smoother shooting motions are better, but there is less agreement about what "smooth" means quantitatively or what part of the shooting motion needs to be smooth. Using ball tracking data from the 2023-2024 National Basketball Association regular season, we explore the relationship between ball path curvature and free throw shooting performance. We fit B\'ezier curves to the ball tracking data in the sagittal plane and test different methods of calculating path curvature. We find that both max curvature and terminal curvature are negatively associated with shooting performance, but terminal curvature explains much more of the between-player variance in free throw shooting performance. This suggests that shooting coaches would be better off focusing on the smoothness at the end of the shot rather than at the beginning of the forward motion of the ball.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13779v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruoqian Zhu, Dave Love, Scott Powers</dc:creator>
    </item>
    <item>
      <title>ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \&amp; a ML Ensemble on Longitudinal Identity Resolution</title>
      <link>https://arxiv.org/abs/2506.13792</link>
      <description>arXiv:2506.13792v1 Announce Type: cross 
Abstract: We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13792v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gon\c{c}alo Hora de Carvalho, Lazar S. Popov, Sander Kaatee, Kristinn R. Th\'orisson, Tangrui Li, P\'etur H\'uni Bj\"ornsson, Jilles S. Dibangoye</dc:creator>
    </item>
    <item>
      <title>High Socioeconomic Status is Associated with Diverse Consumption across Brands and Price Levels</title>
      <link>https://arxiv.org/abs/2506.13840</link>
      <description>arXiv:2506.13840v1 Announce Type: cross 
Abstract: Consumption practices are determined by a combination of economic, social, and cultural forces. We posit that lower economic constraints leave more room to diversify consumption along cultural and social aspects in the form of omnivorous or lifestyle-based niche consumption. We provide empirical evidence for this diversity hypothesis by analysing millions of mobile-tracked visits from thousands of Census Block Groups to thousands of stores in New York State. The results show that high income is significantly associated with diverse consumption across brands and price levels. The associations between diversity and income persist but are less prominent for necessity-based consumption and for the densely populated and demographically diverse New York City. The associations replicate for education as an alternative measure of socioeconomic status and for the state of Texas. We further illustrate that the associations cannot be explained by simple geographic constraints, including the neighbourhoods' demographic diversity, the residents' geographic mobility and the stores' local availability, so deeper social and cultural factors must be at play.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13840v1</guid>
      <category>econ.GN</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanmo He, Milena Tsvetkova</dc:creator>
    </item>
    <item>
      <title>Functional data decomposition reveals unexpectedly strong soil moisture-precipitation coupling over the Great Plains</title>
      <link>https://arxiv.org/abs/2506.13939</link>
      <description>arXiv:2506.13939v1 Announce Type: cross 
Abstract: Soil moisture-precipitation coupling (SMPC) plays a critical role in Earth's water and energy cycles but remains difficult to quantify due to synoptic-scale variability and the complex interplay of land-atmosphere processes. Here, we apply high-dimensional model representation (HDMR) to functionally decompose the structural, correlative, and cooperative contributions of key land-atmosphere variables to precipitation. Benchmark tests confirm that HDMR overcomes limitations of commonly used correlation and regression approaches in isolating direct versus indirect effects. For example, analysis of gross primary productivity using a light-use-efficiency model shows that linear regression underestimates the temperature effect, while HDMR captures it accurately. Applying HDMR to CONUS404 reanalysis data reveals that morning soil moisture explains up to 40 percent of the variance in summertime afternoon precipitation over the Great Plains, more than double prior estimates. On days with afternoon rainfall (12-hour totals of 4.7-8.2 mm), first-order SM effects can boost precipitation by up to 8 mm under wet conditions, with an additional 3 mm from second-order interactions involving temperature and moisture. By capturing real-world co-variability and higher-order effects, HDMR provides a physically grounded, data-driven framework for diagnosing land-atmosphere coupling. These results underscore the need for more nuanced, interaction-aware data analysis methods in climate modeling and prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13939v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifu Gao, Runze Li, Efi Foufoula-Georgiou, Jasper A. Vrugt</dc:creator>
    </item>
    <item>
      <title>Bridging Unsupervised and Semi-Supervised Anomaly Detection: A Theoretically-Grounded and Practical Framework with Synthetic Anomalies</title>
      <link>https://arxiv.org/abs/2506.13955</link>
      <description>arXiv:2506.13955v1 Announce Type: cross 
Abstract: Anomaly detection (AD) is a critical task across domains such as cybersecurity and healthcare. In the unsupervised setting, an effective and theoretically-grounded principle is to train classifiers to distinguish normal data from (synthetic) anomalies. We extend this principle to semi-supervised AD, where training data also include a limited labeled subset of anomalies possibly present in test time. We propose a theoretically-grounded and empirically effective framework for semi-supervised AD that combines known and synthetic anomalies during training. To analyze semi-supervised AD, we introduce the first mathematical formulation of semi-supervised AD, which generalizes unsupervised AD. Here, we show that synthetic anomalies enable (i) better anomaly modeling in low-density regions and (ii) optimal convergence guarantees for neural network classifiers -- the first theoretical result for semi-supervised AD. We empirically validate our framework on five diverse benchmarks, observing consistent performance gains. These improvements also extend beyond our theoretical framework to other classification-based AD methods, validating the generalizability of the synthetic anomaly principle in AD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13955v1</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Lau, Tian-Yi Zhou, Xiangchi Yuan, Jizhou Chen, Wenke Lee, Xiaoming Huo</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis of Priors in the Bayesian Dirichlet Auto-Regressive Moving Average Model</title>
      <link>https://arxiv.org/abs/2506.13973</link>
      <description>arXiv:2506.13973v1 Announce Type: cross 
Abstract: Prior choice can strongly influence Bayesian Dirichlet ARMA (B-DARMA) inference for compositional time-series. Using simulations with (i) correct lag order, (ii) overfitting, and (iii) underfitting, we assess five priors: weakly-informative, horseshoe, Laplace, mixture-of-normals, and hierarchical. With the true lag order, all priors achieve comparable RMSE, though horseshoe and hierarchical slightly reduce bias. Under overfitting, aggressive shrinkage-especially the horseshoe-suppresses noise and improves forecasts, yet no prior rescues a model that omits essential VAR or VMA terms.
  We then fit B-DARMA to daily SP 500 sector weights using an intentionally large lag structure. Shrinkage priors curb spurious dynamics, whereas weakly-informative priors magnify errors in volatile sectors. Two lessons emerge: (1) match shrinkage strength to the degree of overparameterization, and (2) prioritize correct lag selection, because no prior repairs structural misspecification. These insights guide prior selection and model complexity management in high-dimensional compositional time-series applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13973v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Liz Medina, Robert E. Weiss</dc:creator>
    </item>
    <item>
      <title>Causal Mediation Analysis with Multiple Mediators: A Simulation Approach</title>
      <link>https://arxiv.org/abs/2506.14019</link>
      <description>arXiv:2506.14019v1 Announce Type: cross 
Abstract: Analyses of causal mediation often involve exposure-induced confounders or, relatedly, multiple mediators. In such applications, researchers aim to estimate a variety of different quantities, including interventional direct and indirect effects, multivariate natural direct and indirect effects, and/or path-specific effects. This study introduces a general approach to estimating all these quantities by simulating potential outcomes from a series of distribution models for each mediator and the outcome. Building on similar methods developed for analyses with only a single mediator (Imai et al. 2010), we first outline how to implement this approach with parametric models. The parametric implementation can accommodate linear and nonlinear relationships, both continuous and discrete mediators, and many different types of outcomes. However, it depends on correct specification of each model used to simulate the potential outcomes. To address the risk of misspecification, we also introduce an alternative implementation using a novel class of nonparametric models, which leverage deep neural networks to approximate the relevant distributions without relying on strict assumptions about functional form. We illustrate both methods by reanalyzing the effects of media framing on attitudes toward immigration (Brader et al. 2008) and the effects of prenatal care on preterm birth (VanderWeele et al. 2014).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14019v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Zhou, Geoffrey T. Wodtke</dc:creator>
    </item>
    <item>
      <title>The Bones and Shapes of the Phillips Curve</title>
      <link>https://arxiv.org/abs/2506.14030</link>
      <description>arXiv:2506.14030v1 Announce Type: cross 
Abstract: The COVID-19 pandemic reignited debate on the U.S. Phillips curve. Using MSA-level panel data (2001-2024), we employ a Two-Stage Least Squares (2SLS) instrumental variable strategy with a shift-share instrument to estimate core non-tradable inflation's response to a v/u-based slack measure. We distinguish structural slope stability from state-dependent non-linearities via a threshold model. Our analysis addresses whether the slope of the Phillips Curve changed during and after the Pandemic in the United States by evaluating if recent inflation dynamics reflect an altered structural trade-off ("bones") or the activation of non-linear "shapes" in response to extreme labor market tightness. This distinction offers critical insights into the unemployment cost of disinflation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14030v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanyuan Jiang</dc:creator>
    </item>
    <item>
      <title>Smooth surface reconstruction of earthquake faults from distributed potency beachballs</title>
      <link>https://arxiv.org/abs/2506.14082</link>
      <description>arXiv:2506.14082v1 Announce Type: cross 
Abstract: Earthquake faults approximate smooth surfaces of displacement discontinuity in a coarse-grained linear continuum, and the beachball nodal planes of source inelastic strain (potency) indicate the normal vector (n-vector) of the crack face. This theoretical relation has been used to interpret estimated potency as geometrical information of earthquake faults. We formulate an inverse analysis for reconstructing a smooth, three-dimensional fault surface from a given potency field. The analysis is grounded on the definition of the n-vector that is orthogonality to a surface. It is shown from this definition that, whereas the estimated n-vector field is always explained by a unique curve in two dimensions, the surface reconstruction from the estimated n-vector field is an overdetermined problem with no solution in three dimensions. We resolve this overdetermination by adopting the original definition of a fault surface as a map that approximates a thin sheet of potency onto a displacement discontinuity across a literal face. An analytical expression for this fault reconstruction is derived and confirmed to reproduce the original three-dimensional smooth surface from a synthesized noisy n-vector field. By incorporating this result into surface reconstruction based on potency density tensor inversion, we estimate the three-dimensional source fault geometry of the 2013 Balochistan earthquake. The estimated geometry is more consistent with the fault trace than the quasi-two-dimensional shape reconstruction as anticipated by our numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14082v1</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dye SK Sato, Yuji Yagi, Ryo Okuwaki, Yukitoshi Fukahata</dc:creator>
    </item>
    <item>
      <title>hyperFA*IR: A hypergeometric approach to fair rankings with finite candidate pool</title>
      <link>https://arxiv.org/abs/2506.14349</link>
      <description>arXiv:2506.14349v1 Announce Type: cross 
Abstract: Ranking algorithms play a pivotal role in decision-making processes across diverse domains, from search engines to job applications. When rankings directly impact individuals, ensuring fairness becomes essential, particularly for groups that are marginalised or misrepresented in the data. Most of the existing group fairness frameworks often rely on ensuring proportional representation of protected groups. However, these approaches face limitations in accounting for the stochastic nature of ranking processes or the finite size of candidate pools. To this end, we present hyperFA*IR, a framework for assessing and enforcing fairness in rankings drawn from a finite set of candidates. It relies on a generative process based on the hypergeometric distribution, which models real-world scenarios by sampling without replacement from fixed group sizes. This approach improves fairness assessment when top-$k$ selections are large relative to the pool or when protected groups are small. We compare our approach to the widely used binomial model, which treats each draw as independent with fixed probability, and demonstrate$-$both analytically and empirically$-$that our method more accurately reproduces the statistical properties of sampling from a finite population. To operationalise this framework, we propose a Monte Carlo-based algorithm that efficiently detects unfair rankings by avoiding computationally expensive parameter tuning. Finally, we adapt our generative approach to define affirmative action policies by introducing weights into the sampling process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14349v1</guid>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3715275.3732143</arxiv:DOI>
      <dc:creator>Mauritz N. Cartier van Dissel, Samuel Martin-Gutierrez, Lisette Esp\'in-Noboa, Ana Mar\'ia Jaramillo, Fariba Karimi</dc:creator>
    </item>
    <item>
      <title>Assessing the Impact of Vaccination on Rotavirus Transmission Dynamics Using Bayesian Inference</title>
      <link>https://arxiv.org/abs/2506.14536</link>
      <description>arXiv:2506.14536v1 Announce Type: cross 
Abstract: The introduction of the rotavirus vaccine in the United Kingdom (UK) in 2013 led to a noticeable decline in laboratory reports in subsequent years. To assess the impact of vaccination on rotavirus transmissibility we calibrated a stochastic compartmental epidemiological model using Sequential Monte Carlo (SMC) methods. Our analysis focuses on estimating the time-varying transmissibility parameter and documenting its evolution before and after vaccine rollout. We observe distinct periods of increasing and decreasing transmissibility, reflecting the dynamic response of rotavirus spread to immunization efforts. These findings improve our understanding of vaccination-driven shifts in disease transmission and provide a quantitative framework for evaluating long-term epidemiological trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14536v1</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Conor Rosato, Joshua Murphy, Simon Maskell, John Harris</dc:creator>
    </item>
    <item>
      <title>Environmental extreme risk modeling via sub-sampling block maxima</title>
      <link>https://arxiv.org/abs/2506.14556</link>
      <description>arXiv:2506.14556v1 Announce Type: cross 
Abstract: This paper introduces a novel sub-sampling block maxima technique to model and characterize environmental extreme risks. We examine the relationships between block size and block maxima statistics derived from the Gaussian and generalized Pareto distributions. We introduce a weighted least square estimator for extreme value index (EVI) and evaluate its performance using simulated auto-correlated data. We employ the second moment of block maxima for plateau finding in EVI and extremal index (EI) estimation, and present the effect of EI on Kullback-Leibler divergence. The applicability of this approach is demonstrated across diverse environmental datasets, including meteorite landing mass, earthquake energy release, solar activity, and variations in Greenland's land snow cover and sea ice extent. Our method provides a sample-efficient framework, robust to temporal dependencies, that delivers actionable environmental extreme risk measures across different timescales. With its flexibility, sample efficiency, and limited reliance on subjective tuning, this approach emerges as a useful tool for environmental extreme risk assessment and management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14556v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tuoyuan Cheng, Xiao Peng, Achmad Choiruddin, Xiaogang He, Kan Chen</dc:creator>
    </item>
    <item>
      <title>Deep Learning Surrogates for Real-Time Gas Emission Inversion</title>
      <link>https://arxiv.org/abs/2506.14597</link>
      <description>arXiv:2506.14597v1 Announce Type: cross 
Abstract: Real-time identification and quantification of greenhouse-gas emissions under transient atmospheric conditions is a critical challenge in environmental monitoring. We introduce a spatio-temporal inversion framework that embeds a deep-learning surrogate of computational fluid dynamics (CFD) within a sequential Monte Carlo algorithm to perform Bayesian inference of both emission rate and source location in dynamic flow fields. By substituting costly numerical solvers with a multilayer perceptron trained on high-fidelity CFD outputs, our surrogate captures spatial heterogeneity and temporal evolution of gas dispersion, while delivering near-real-time predictions. Validation on the Chilbolton methane release dataset demonstrates comparable accuracy to full CFD solvers and Gaussian plume models, yet achieves orders-of-magnitude faster runtimes. Further experiments under simulated obstructed-flow scenarios confirm robustness in complex environments. This work reconciles physical fidelity with computational feasibility, offering a scalable solution for industrial emissions monitoring and other time-sensitive spatio-temporal inversion tasks in environmental and scientific modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14597v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Newman, Christopher Nemeth, Matthew Jones, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>Addressing Phase Discrepancies in Functional Data: A Bayesian Approach for Accurate Alignment and Smoothing</title>
      <link>https://arxiv.org/abs/2506.14650</link>
      <description>arXiv:2506.14650v1 Announce Type: cross 
Abstract: In many real-world applications, functional data exhibit considerable variability in both amplitude and phase. This is especially true in biomechanical data such as the knee flexion angle dataset motivating our work, where timing differences across curves can obscure meaningful comparisons. Curves of this study also exhibit substantial variability from one another. These pronounced differences make the dataset particularly challenging to align properly without distorting or losing some of the individual curves characteristics. Our alignment model addresses these challenges by eliminating phase discrepancies while preserving the individual characteristics of each curve and avoiding distortion, thanks to its flexible smoothing component. Additionally, the model accommodates group structures through a dedicated parameter. By leveraging the Bayesian approach, the new prior on the warping parameters ensures that the resulting warping functions automatically satisfy all necessary validity conditions. We applied our model to the knee flexion dataset, demonstrating excellent performance in both smoothing and alignment, particularly in the presence of high inter-curve variability and complex group structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14650v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacopo Gardella, Raffaele Argiento, Alessandro Casa, Alessia Pini</dc:creator>
    </item>
    <item>
      <title>Leveraging population information in brain connectivity via Bayesian ICA with a novel informative prior for correlation matrices</title>
      <link>https://arxiv.org/abs/2311.03791</link>
      <description>arXiv:2311.03791v3 Announce Type: replace 
Abstract: Brain functional connectivity (FC), the temporal synchrony between brain networks, is essential to understand the functional organization of the brain and to identify changes due to neurological disorders, development, treatment, and other phenomena. Independent component analysis (ICA) is a matrix decomposition method used extensively for simultaneous estimation of functional brain topography and connectivity. However, estimation of FC via ICA is often sub-optimal due to the use of ad-hoc estimation methods or temporal dimension reduction prior to ICA. Bayesian ICA can avoid dimension reduction, estimate latent variables and model parameters more accurately, and facilitate posterior inference. In this paper, we develop a novel, computationally feasible Bayesian ICA method with population-derived priors on both the spatial ICs and their temporal correlation, i.e. FC. For the latter we consider two priors: the inverse-Wishart, which is conjugate but is not ideally suited for modeling correlation matrices; and a novel informative prior for correlation matrices. For each prior, we derive a variational Bayes algorithm to estimate the model variables and facilitate posterior inference. Through extensive simulation studies, we evaluate the performance of the proposed methods and benchmark against existing approaches. We also analyze fMRI data from over 400 healthy adults in the Human Connectome Project. We find that our Bayesian ICA model and algorithms result in more accurate measures of functional connectivity and spatial brain features. Our novel prior for correlation matrices is more computationally intensive than the inverse-Wishart but provides improved accuracy and inference. The proposed framework is applicable to single-subject analysis, making it potentially clinically viable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03791v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amanda Mejia, David Bolin, Daniel Spencer, Ani Eloyan</dc:creator>
    </item>
    <item>
      <title>Navigating Challenges in Spatio-temporal Modelling of Antarctic Krill Abundance: Addressing Zero-inflated Data and Misaligned Covariates</title>
      <link>https://arxiv.org/abs/2412.01399</link>
      <description>arXiv:2412.01399v2 Announce Type: replace 
Abstract: Antarctic krill (Euphausia superba) are among the most abundant species on our planet and serve as a vital food source for many marine predators in the Southern Ocean. In this paper, we utilise statistical spatio-temporal methods to combine data from various sources and resolutions, aiming to model krill abundance. Our focus lies in fitting the model to a dataset comprising acoustic measurements of krill biomass. To achieve this, we integrate climate covariates obtained from satellite imagery and from drifting surface buoys (also known as drifters). Additionally, we use sparsely collected krill biomass data obtained from net fishing efforts (KRILLBASE) for validation. However, integrating these multiple heterogeneous data sources presents significant modelling challenges, including spatio-temporal misalignment and inflated zeros in the observed data. To address these challenges, we fit a Hurdle-Gamma model to jointly describe the occurrence of zeros and the krill biomass for the non-zero observations, while also accounting for misaligned and heterogeneous data sources, including drifters. Therefore, our work presents a comprehensive framework for analysing and predicting krill abundance in the Southern Ocean, leveraging information from various sources and formats. This is crucial due to the impact of krill fishing, as understanding their distribution is essential for informed management decisions and fishing regulations aimed at protecting the species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01399v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andr\'e Victor Ribeiro Amaral, Adam M. Sykulski, Sophie Fielding, Emma Cavan</dc:creator>
    </item>
    <item>
      <title>A Criterion for Extending Continuous-Mixture Identifiability Results</title>
      <link>https://arxiv.org/abs/2503.03536</link>
      <description>arXiv:2503.03536v2 Announce Type: replace-cross 
Abstract: Mixture distributions provide a versatile and widely used framework for modeling random phenomena, and are particularly well-suited to the analysis of geoscientific processes and their attendant risks to society. For continuous mixtures of random variables, we specify a simple criterion - generating-function accessibility - to extend previously known kernel-based identifiability (or unidentifiability) results to new kernel distributions. This criterion, based on functional relationships between the relevant kernels' moment-generating functions or Laplace transforms, may be applied to continuous mixtures of both discrete and continuous random variables. To illustrate the proposed approach, we present results for several specific kernels, in each case briefly noting its relevance to research in the geosciences and/or related risk analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03536v2</guid>
      <category>stat.ML</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Powers, Jiaxin Xu</dc:creator>
    </item>
    <item>
      <title>Assessing Risk Heterogeneity through Heavy-Tailed Frequency and Severity Mixtures</title>
      <link>https://arxiv.org/abs/2505.04795</link>
      <description>arXiv:2505.04795v2 Announce Type: replace-cross 
Abstract: In operational risk management and actuarial finance, the analysis of risk often begins by dividing a random damage-generation process into its separate frequency and severity components. In the present article, we construct canonical families of mixture distributions for each of these components, based on a Negative Binomial kernel for frequency and a Gamma kernel for severity. The mixtures are employed to assess the heterogeneity of risk factors underlying an empirical distribution through the shape of the implied mixing distribution. From the duality of the Negative Binomial and Gamma distributions, we first derive necessary and sufficient conditions for heavy-tailed (i.e., inverse power-law) canonical mixtures. We then formulate flexible 4-parameter families of mixing distributions for Geometric and Exponential kernels to generate heavy-tailed 4-parameter mixture models, and extend these mixtures to arbitrary Negative Binomial and Gamma kernels, respectively, yielding 5-parameter mixtures for detecting and measuring risk heterogeneity. To check the robustness of such heterogeneity inferences, we show how a fitted 5-parameter model may be re-expressed in terms of alternative Negative Binomial or Gamma kernels whose associated mixing distributions form a "calibrated" family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04795v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Powers, Jiaxin Xu</dc:creator>
    </item>
  </channel>
</rss>
