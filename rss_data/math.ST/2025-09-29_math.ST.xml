<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Sep 2025 04:00:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Generalization Analysis for Classification on Korobov Space</title>
      <link>https://arxiv.org/abs/2509.22748</link>
      <description>arXiv:2509.22748v1 Announce Type: new 
Abstract: In this paper, the classification algorithm arising from Tikhonov regularization is discussed. The main intention is to derive learning rates for the excess misclassification error according to the convex $\eta$-norm loss function $\phi(v)=(1 - v)_{+}^{\eta}$, $\eta\geq1$. Following the argument, the estimation of error under Tsybakov noise conditions is studied. In addition, we propose the rate of $L_p$ approximation of functions from Korobov space $X^{2, p}([-1,1]^{d})$, $1\leq p \leq \infty$, by the shallow ReLU neural network. This result consists of a novel Fourier analysis</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22748v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuqing Liu</dc:creator>
    </item>
    <item>
      <title>Misspecified Maximum Likelihood Estimation for Non-Uniform Group Orbit Recovery</title>
      <link>https://arxiv.org/abs/2509.22945</link>
      <description>arXiv:2509.22945v1 Announce Type: new 
Abstract: We study maximum likelihood estimation (MLE) in the generalized group orbit recovery model, where each observation is generated by applying a random group action and a known, fixed linear operator to an unknown signal, followed by additive noise. This model is motivated by single-particle cryo-electron microscopy (cryo-EM) and can be viewed primarily as a structured continuous Gaussian mixture model. In practice, signal estimation is often performed by marginalizing over the group using a uniform distribution--an assumption that typically does not hold and renders the MLE misspecified. This raises a fundamental question: how does the misspecified MLE perform? We address this question from several angles. First, we show that in the absence of projection, the misspecified population log-likelihood has desired optimization landscape that leads to correct signal recovery. In contrast, when projections are present, the global optimizers of the misspecified likelihood deviate from the true signal, with the magnitude of the bias depending on the noise level. To address this issue, we propose a joint estimation approach tailored to the cryo-EM setting, which parameterizes the unknown distribution of the group elements and estimates both the signal and distribution parameters simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22945v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sheng Xu, Anderson Ye Zhang, Amit Singer</dc:creator>
    </item>
    <item>
      <title>Learning single index model with gradient descent: spectral initialization and precise asymptotics</title>
      <link>https://arxiv.org/abs/2509.23527</link>
      <description>arXiv:2509.23527v1 Announce Type: new 
Abstract: Non-convex optimization plays a central role in many statistics and machine learning problems. Despite the landscape irregularities for general non-convex functions, some recent work showed that for many learning problems with random data and large enough sample size, there exists a region around the true signal with benign landscape. Motivated by this observation, a widely used strategy is a two-stage algorithm, where we first apply a spectral initialization to plunge into the region, and then run gradient descent for further refinement. While this two-stage algorithm has been extensively analyzed for many non-convex problems, the precise distributional property of both its transient and long-time behavior remains to be understood. In this work, we study this two-stage algorithm in the context of single index models under the proportional asymptotics regime. We derive a set of dynamical mean field equations, which describe the precise behavior of the trajectory of spectral initialized gradient descent in the large system limit. We further show that when the spectral initialization successfully lands in a region of benign landscape, the above equation system is asymptotically time translation invariant and exponential converging, and thus admits a set of long-time fixed points that represents the mean field characterization of the limiting point of the gradient descent dynamic. As a proof of concept, we demonstrate our general theory in the example of regularized Wirtinger flow for phase retrieval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23527v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Chen, Yandi Shen</dc:creator>
    </item>
    <item>
      <title>Nonparametric hazard rate estimation with associated kernels and minimax bandwidth choice</title>
      <link>https://arxiv.org/abs/2509.24535</link>
      <description>arXiv:2509.24535v1 Announce Type: new 
Abstract: In this paper, we consider the general theory of nonparametric hazard rate estimation with associated kernels, for which the shape of the kernel depends on the point of estimation. We prove MISE convergence results and a central limit theorem for such estimators. We then prove an oracle type inequality for both a local and global minimax bandwidth choice. The results are then illustrated by showing that they apply to the Gamma and providing numerical simulations and an application to biological data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24535v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luce Breuil, Sarah Kaaka\"i</dc:creator>
    </item>
    <item>
      <title>Resolution of the Borel-Kolmogorov Paradox via the Maximum Entropy Principle</title>
      <link>https://arxiv.org/abs/2509.24735</link>
      <description>arXiv:2509.24735v1 Announce Type: new 
Abstract: This paper presents a rigorous resolution of the Borel-Kolmogorov paradox using the Maximum Entropy Principle. We construct a metric-based framework for Bayesian inference that uniquely extends conditional probability to events of null measure. The results unify classical Bayes' rules and provide a robust foundation for Bayesian inference in metric spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24735v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rapha\"el Tr\'esor, Mykola Lukashchuk</dc:creator>
    </item>
    <item>
      <title>Autoregressive Processes on Stiefel and Grassmann Manifolds</title>
      <link>https://arxiv.org/abs/2509.24767</link>
      <description>arXiv:2509.24767v1 Announce Type: new 
Abstract: System identification of autoregressive processes on Stiefel and Grassmann manifolds are presented and studied. We define the system parameters as elements in the orthogonal group and we show that the system can be estimated by averaging over observations. Then we propose an algorithm on how to compute these system parameters using conjugate gradient descent on Stiefel and Grassmann manifolds, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24767v1</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordi-Llu\'is Figueras, Aron Persson</dc:creator>
    </item>
    <item>
      <title>Differentially Private Two-Stage Gradient Descent for Instrumental Variable Regression</title>
      <link>https://arxiv.org/abs/2509.22794</link>
      <description>arXiv:2509.22794v1 Announce Type: cross 
Abstract: We study instrumental variable regression (IVaR) under differential privacy constraints. Classical IVaR methods (like two-stage least squares regression) rely on solving moment equations that directly use sensitive covariates and instruments, creating significant risks of privacy leakage and posing challenges in designing algorithms that are both statistically efficient and differentially private. We propose a noisy two-state gradient descent algorithm that ensures $\rho$-zero-concentrated differential privacy by injecting carefully calibrated noise into the gradient updates. Our analysis establishes finite-sample convergence rates for the proposed method, showing that the algorithm achieves consistency while preserving privacy. In particular, we derive precise bounds quantifying the trade-off among privacy parameters, sample size, and iteration-complexity. To the best of our knowledge, this is the first work to provide both privacy guarantees and provable convergence rates for instrumental variable regression in linear models. We further validate our theoretical findings with experiments on both synthetic and real datasets, demonstrating that our method offers practical accuracy-privacy trade-offs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22794v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haodong Liang, Yanhao Jin, Krishnakumar Balasubramanian, Lifeng Lai</dc:creator>
    </item>
    <item>
      <title>An exploration of sequential Bayesian variable selection - A comment on Garc\'{i}a-Donato et al. (2025). "Model uncertainty and missing data: An objective Bayesian perspective"</title>
      <link>https://arxiv.org/abs/2509.22901</link>
      <description>arXiv:2509.22901v1 Announce Type: cross 
Abstract: Our comment on Garc\'ia-Donato et al. (2025). "Model uncertainty and missing data: An objective Bayesian perspective" explores a further extension of the proposed methodology. Specifically, we consider the sequential setting where (potentially missing) data accumulate over time, with the goal of continuously monitoring statistical evidence, as opposed to assessing it only once data collection terminates. We explore a new variable selection method based on sequential model confidence sets, as proposed by Arnold et al. (2024), and show that it can help stabilise the inference of Garc\'ia-Donato et al. (2025). To be published as "Invited discussion" in Bayesian Analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22901v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian Arnold, Alexander Ly</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Gradient Boosting Regression</title>
      <link>https://arxiv.org/abs/2509.23127</link>
      <description>arXiv:2509.23127v1 Announce Type: cross 
Abstract: Gradient boosting is widely popular due to its flexibility and predictive accuracy. However, statistical inference and uncertainty quantification for gradient boosting remain challenging and under-explored. We propose a unified framework for statistical inference in gradient boosting regression. Our framework integrates dropout or parallel training with a recently proposed regularization procedure that allows for a central limit theorem (CLT) for boosting. With these enhancements, we surprisingly find that increasing the dropout rate and the number of trees grown in parallel at each iteration substantially enhances signal recovery and overall performance. Our resulting algorithms enjoy similar CLTs, which we use to construct built-in confidence intervals, prediction intervals, and rigorous hypothesis tests for assessing variable importance. Numerical experiments demonstrate that our algorithms perform well, interpolate between regularized boosting and random forests, and confirm the validity of their built-in statistical inference procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23127v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haimo Fang, Kevin Tan, Giles Hooker</dc:creator>
    </item>
    <item>
      <title>Dense associative memory on the Bures-Wasserstein space</title>
      <link>https://arxiv.org/abs/2509.23162</link>
      <description>arXiv:2509.23162v1 Announce Type: cross 
Abstract: Dense associative memories (DAMs) store and retrieve patterns via energy-functional fixed points, but existing models are limited to vector representations. We extend DAMs to probability distributions equipped with the 2-Wasserstein distance, focusing mainly on the Bures-Wasserstein class of Gaussian densities. Our framework defines a log-sum-exp energy over stored distributions and a retrieval dynamics aggregating optimal transport maps in a Gibbs-weighted manner. Stationary points correspond to self-consistent Wasserstein barycenters, generalizing classical DAM fixed points. We prove exponential storage capacity, provide quantitative retrieval guarantees under Wasserstein perturbations, and validate the model on synthetic and real-world distributional tasks. This work elevates associative memory from vectors to full distributions, bridging classical DAMs with modern generative modeling and enabling distributional storage and retrieval in memory-augmented learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23162v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chandan Tankala, Krishnakumar Balasubramanian</dc:creator>
    </item>
    <item>
      <title>Statistical Learning Guarantees for Group-Invariant Barron Functions</title>
      <link>https://arxiv.org/abs/2509.23474</link>
      <description>arXiv:2509.23474v1 Announce Type: cross 
Abstract: We investigate the generalization error of group-invariant neural networks within the Barron framework. Our analysis shows that incorporating group-invariant structures introduces a group-dependent factor $\delta_{G,\Gamma,\sigma} \le 1$ into the approximation rate. When this factor is small, group invariance yields substantial improvements in approximation accuracy. On the estimation side, we establish that the Rademacher complexity of the group-invariant class is no larger than that of the non-invariant counterpart, implying that the estimation error remains unaffected by the incorporation of symmetry. Consequently, the generalization error can improve significantly when learning functions with inherent group symmetries. We further provide illustrative examples demonstrating both favorable cases, where $\delta_{G,\Gamma,\sigma}\approx |G|^{-1}$, and unfavorable ones, where $\delta_{G,\Gamma,\sigma}\approx 1$. Overall, our results offer a rigorous theoretical foundation showing that encoding group-invariant structures in neural networks leads to clear statistical advantages for symmetric target functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23474v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yahong Yang, Wei Zhu</dc:creator>
    </item>
    <item>
      <title>Bayesian Mixture-of-Experts: Towards Making LLMs Know What They Don't Know</title>
      <link>https://arxiv.org/abs/2509.23830</link>
      <description>arXiv:2509.23830v1 Announce Type: cross 
Abstract: The Mixture-of-Experts (MoE) architecture has enabled the creation of massive yet efficient Large Language Models (LLMs). However, the standard deterministic routing mechanism presents a significant limitation: its inherent brittleness is a key contributor to model miscalibration and overconfidence, resulting in systems that often do not know what they don't know.
  This thesis confronts this challenge by proposing a structured \textbf{Bayesian MoE routing framework}. Instead of forcing a single, deterministic expert selection, our approach models a probability distribution over the routing decision itself. We systematically investigate three families of methods that introduce this principled uncertainty at different stages of the routing pipeline: in the \textbf{weight-space}, the \textbf{logit-space}, and the final \textbf{selection-space}.
  Through a series of controlled experiments on a 3-billion parameter MoE model, we demonstrate that this framework significantly improves routing stability, in-distribution calibration, and out-of-distribution (OoD) detection. The results show that by targeting this core architectural component, we can create a more reliable internal uncertainty signal. This work provides a practical and computationally tractable pathway towards building more robust and self-aware LLMs, taking a crucial step towards making them know what they don't know.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23830v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Albus Yizhuo Li</dc:creator>
    </item>
    <item>
      <title>Sign and signed rank tests for paired functions</title>
      <link>https://arxiv.org/abs/2509.24170</link>
      <description>arXiv:2509.24170v1 Announce Type: cross 
Abstract: Simple nonparametric tests for paired functional data are an understudied area, despite recent advances in similar tests for other types of functional data. While the sign test has received limited treatment, the signed rank-type test has not previously been examined. The aim of the present work is to develop and evaluate these types of tests for functional data. We derive a simple, theoretical framework for both sign and signed rank tests for pairs of functions. In particular, we demonstrate that doubly ranked testing -- a newly developed framework for testing hypotheses involving functional data -- is a useful conduit for examining hypotheses regarding pairs o,f functions. We briefly examine the operating characteristics of all derived tests. We also use the described approaches to re-analyze pairs of functions from a randomized crossover study of heart health during simulated flight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24170v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark J. Meyer</dc:creator>
    </item>
    <item>
      <title>Equivalence Test for Mean Functions from Multi-population Functional Data</title>
      <link>https://arxiv.org/abs/2509.24242</link>
      <description>arXiv:2509.24242v1 Announce Type: cross 
Abstract: Most existing methods for testing equality of means of functional data from multiple populations rely on assumptions of equal covariance and/or Gaussianity. In this work we provide a new testing method based on a statistic that is distribution-free under the null hypothesis (i.e. the statistic is pivotal), and allows different covariance structures across populations, while Gaussianity is not required. In contrast to classical methods of functional mean testing, where either observations of the full curves or projections are applied, our method allows the projection dimension to increase with the sample size to allow asymptotic recovery of full information as the sample size increases. We obtain a unified theory for the asymptotic distribution of the test statistic under local alternatives, in both the sample and bootstrap cases. The finite sample performance for both size and power have been studied via simulations and the approach has also been applied to two real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24242v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuang Xu, Andrew T. A. Wood, Yanrong Yang</dc:creator>
    </item>
    <item>
      <title>Nonequilibrium statistics of barrier crossings with competing pathways</title>
      <link>https://arxiv.org/abs/2509.24533</link>
      <description>arXiv:2509.24533v1 Announce Type: cross 
Abstract: Many biological, chemical, and physical systems are underpinned by stochastic transitions between equilibrium states in a potential energy. Here, we consider such transitions in a minimal model with two possible competing pathways, both starting from a local potential energy minimum and eventually finding the global minimum. There is competition between the distance to travel in state space and the height of the potential energy barriers to be surmounted, for the transition to occur. One pathway has a higher energy barrier to go over, but requires traversing a shorter distance, whereas the other pathway has a lower potential barrier but it is substantially further away in configuration space. The most likely pathway taken depends on the available time for the transition process; when only a relatively short time is available, the most likely path is the one over the higher barrier. We find that upon varying temperature the overall most likely pathway can switch from one to the other. We calculate the statistics of where the barrier crossing occurs and the distribution of times taken to reach the potential minimum. Interestingly, while the configuration space statistics is complex, the time of arrival statistics is rather simple, having an exponential probability density over most of the time range. Taken together, our results show that empirically observed rates in nonequilibrium systems should not be used to infer barrier heights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24533v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>math.ST</category>
      <category>physics.chem-ph</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gulzar Ahmad, Sergey Saveliev, Steven P Fitzgerald, Marco G Mazza, Andrew J Archer</dc:creator>
    </item>
    <item>
      <title>Robust Semiparametric Inference for Bayesian Additive Regression Trees</title>
      <link>https://arxiv.org/abs/2509.24634</link>
      <description>arXiv:2509.24634v1 Announce Type: cross 
Abstract: We develop a semiparametric framework for inference on the mean response in missing-data settings using a corrected posterior distribution. Our approach is tailored to Bayesian Additive Regression Trees (BART), which is a powerful predictive method but whose nonsmoothness complicate asymptotic theory with multi-dimensional covariates. When using BART combined with Bayesian bootstrap weights, we establish a new Bernstein-von Mises theorem and show that the limit distribution generally contains a bias term. To address this, we introduce RoBART, a posterior bias-correction that robustifies BART for valid inference on the mean response. Monte Carlo studies support our theory, demonstrating reduced bias and improved coverage relative to existing procedures using BART.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24634v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Breunig, Ruixuan Liu, Zhengfei Yu</dc:creator>
    </item>
    <item>
      <title>Efficient Difference-in-Differences Estimation when Outcomes are Missing at Random</title>
      <link>https://arxiv.org/abs/2509.25009</link>
      <description>arXiv:2509.25009v1 Announce Type: cross 
Abstract: The Difference-in-Differences (DiD) method is a fundamental tool for causal inference, yet its application is often complicated by missing data. Although recent work has developed robust DiD estimators for complex settings like staggered treatment adoption, these methods typically assume complete data and fail to address the critical challenge of outcomes that are missing at random (MAR) -- a common problem that invalidates standard estimators. We develop a rigorous framework, rooted in semiparametric theory, for identifying and efficiently estimating the Average Treatment Effect on the Treated (ATT) when either pre- or post-treatment (or both) outcomes are missing at random. We first establish nonparametric identification of the ATT under two minimal sets of sufficient conditions. For each, we derive the semiparametric efficiency bound, which provides a formal benchmark for asymptotic optimality. We then propose novel estimators that are asymptotically efficient, achieving this theoretical bound. A key feature of our estimators is their multiple robustness, which ensures consistency even if some nuisance function models are misspecified. We validate the properties of our estimators and showcase their broad applicability through an extensive simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25009v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Testa, Edward H. Kennedy, Matthew Reimherr</dc:creator>
    </item>
    <item>
      <title>On Spectral Learning for Odeco Tensors: Perturbation, Initialization, and Algorithms</title>
      <link>https://arxiv.org/abs/2509.25126</link>
      <description>arXiv:2509.25126v1 Announce Type: cross 
Abstract: We study spectral learning for orthogonally decomposable (odeco) tensors, emphasizing the interplay between statistical limits, optimization geometry, and initialization. Unlike matrices, recovery for odeco tensors does not hinge on eigengaps, yielding improved robustness under noise. While iterative methods such as tensor power iterations can be statistically efficient, initialization emerges as the main computational bottleneck. We investigate perturbation bounds, non-convex optimization analysis, and initialization strategies, clarifying when efficient algorithms attain statistical limits and when fundamental barriers remain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25126v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnab Auddy, Ming Yuan</dc:creator>
    </item>
    <item>
      <title>Maximum likelihood estimation in the ergodic Volterra Ornstein-Uhlenbeck process</title>
      <link>https://arxiv.org/abs/2404.05554</link>
      <description>arXiv:2404.05554v3 Announce Type: replace 
Abstract: We study statistical inference of the drift parameters for the Volterra Ornstein-Uhlenbeck process on R in the ergodic regime. For continuous-time observations, we derive the corresponding maximum likelihood estimators and show that they are strongly consistent and asymptotically normal locally uniformly in the parameters. For the case of discrete high-frequency observations, we prove similar results by discretization of the continuous-time maximum likelihood estimator. Finally, for discrete low-frequency observations, we show that the method of moments is consistent. Our proofs are crucially based on the law of large numbers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05554v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Ben Alaya, Martin Friesen, Jonas Kremer</dc:creator>
    </item>
    <item>
      <title>On the Frequentist Coverage of Bayes Posteriors in Nonlinear Inverse Problems</title>
      <link>https://arxiv.org/abs/2407.13970</link>
      <description>arXiv:2407.13970v3 Announce Type: replace 
Abstract: We study asymptotic frequentist coverage and approximately Gaussian properties of Bayes posterior credible sets in nonlinear inverse problems when a Gaussian prior is placed on the parameter of the PDE. The aim is to ensure valid frequentist coverage of Bayes credible intervals when estimating continuous linear functionals of the parameter. Our results show that Bayes credible intervals have conservative coverage under certain smoothness assumptions on the parameter and a compatibility condition between the likelihood and the prior, regardless of whether an efficient limit exists or Bernstein von-Mises (BvM) theorem holds. In the latter case, our results yield a corollary with more relaxed sufficient conditions than previous works. We illustrate the practical utility of the results through the example of estimating the conductivity coefficient of a second order elliptic PDE, where a near-$N^{-1/2}$ contraction rate and conservative coverage results are obtained for linear functionals that were shown not to be estimable efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13970v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youngsoo Baek, Katerina Papagiannouli</dc:creator>
    </item>
    <item>
      <title>Sampling effects on Lasso estimation of drift functions in high-dimensional diffusion processes</title>
      <link>https://arxiv.org/abs/2408.08638</link>
      <description>arXiv:2408.08638v3 Announce Type: replace 
Abstract: In this paper, we address high-dimensional parametric estimation of the drift function in diffusion models, specifically focusing on a $d$-dimensional ergodic diffusion process observed at discrete time points. We consider both a general linear form for the drift function and the particular case of the Ornstein-Uhlenbeck (OU) process. Assuming sparsity of the parameter vector, we examine the statistical behavior of the Lasso estimator for the unknown parameter. Our primary contribution is the proof of an oracle inequality for the Lasso estimator, which holds on the intersection of three specific sets defined for our analysis. We carefully control the probability of these sets, tackling the central challenge of our study. This approach allows us to derive error bounds for the $l_1$ and $l_2$ norms, assessing the performance of the proposed Lasso estimator. Our results demonstrate that, under certain conditions, the discretization error becomes negligible, enabling us to achieve the same optimal rate of convergence as if the continuous trajectory of the process were observed. We validate our theoretical findings through numerical experiments, which show that the Lasso estimator significantly outperforms the maximum likelihood estimator (MLE) in terms of support recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08638v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Amorino, Francisco Pina, Mark Podolskij</dc:creator>
    </item>
    <item>
      <title>Convergence Rates for the Maximum A Posteriori Estimator in PDE-Regression Models with Random Design</title>
      <link>https://arxiv.org/abs/2409.03417</link>
      <description>arXiv:2409.03417v3 Announce Type: replace 
Abstract: We consider the statistical inverse problem of recovering a parameter $\theta\in H^\alpha$ from data arising from the Gaussian regression problem \begin{equation*}
  Y = \mathscr{G}(\theta)(Z)+\varepsilon \end{equation*} with nonlinear forward map $\mathscr{G}:\mathbb{L}^2\to\mathbb{L}^2$, random design points $Z$ and Gaussian noise $\varepsilon$. The estimation strategy is based on a least squares approach under $\Vert\cdot\Vert_{H^\alpha}$-constraints. We establish the existence of a least squares estimator $\hat{\theta}$ as a maximizer for a given functional under Lipschitz-type assumptions on the forward map $\mathscr{G}$. A general concentration result is shown, which is used to prove consistency and upper bounds for the prediction error. The corresponding rates of convergence reflect not only the smoothness of the parameter of interest but also the ill-posedness of the underlying inverse problem. We apply the general model to the Darcy problem, where the recovery of an unknown coefficient function $f$ of a PDE is of interest. For this example, we also provide corresponding rates of convergence for the prediction and estimation errors. Additionally, we briefly discuss the applicability of the general model to other problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03417v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Siebel</dc:creator>
    </item>
    <item>
      <title>Precise Asymptotics of Bagging Regularized M-estimators</title>
      <link>https://arxiv.org/abs/2409.15252</link>
      <description>arXiv:2409.15252v3 Announce Type: replace 
Abstract: We characterize the squared prediction risk of ensemble estimators obtained through subagging (subsample bootstrap aggregating) regularized M-estimators and construct a consistent estimator for the risk. Specifically, we consider a heterogeneous collection of $M \ge 1$ regularized M-estimators, each trained with (possibly different) subsample sizes, convex differentiable losses, and convex regularizers. We operate under the proportional asymptotics regime, where the sample size $n$, feature size $p$, and subsample sizes $k_m$ for $m \in [M]$ all diverge with fixed limiting ratios $n/p$ and $k_m/n$. Key to our analysis is a new result on the joint asymptotic behavior of correlations between the estimator and residual errors on overlapping subsamples, governed through a (provably) contractive nonlinear system of equations. Of independent interest, we also establish convergence of trace functionals related to degrees of freedom in the non-ensemble setting (with $M = 1$) along the way, extending previously known cases for squared loss with ridge and lasso regularizers.
  When specialized to homogeneous ensembles trained with a common loss, regularizer, and subsample size, the risk characterization sheds some light on the implicit regularization effect due to the ensemble and subsample sizes $(M,k)$. For any ensemble size $M$, optimally tuning subsample size yields sample-wise monotonic risk. For the full-ensemble estimator (when $M \to \infty$), the optimal subsample size $k^\star$ tends to be in the overparameterized regime $(k^\star \le \min\{n,p\})$, when explicit regularization is vanishing. Finally, joint optimization of subsample size, ensemble size, and regularization can significantly outperform regularizer optimization alone on the full data (without any subagging).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15252v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Koriyama, Pratik Patil, Jin-Hong Du, Kai Tan, Pierre C. Bellec</dc:creator>
    </item>
    <item>
      <title>Optimal and Provable Calibration in High-Dimensional Binary Classification: Angular Calibration and Platt Scaling</title>
      <link>https://arxiv.org/abs/2502.15131</link>
      <description>arXiv:2502.15131v2 Announce Type: replace 
Abstract: We study the fundamental problem of calibrating a linear binary classifier of the form $\sigma(\hat{w}^\top x)$, where the feature vector $x$ is Gaussian, $\sigma$ is a link function, and $\hat{w}$ is an estimator of the true linear weight $w^\star$. By interpolating with a noninformative $\textit{chance classifier}$, we construct a well-calibrated predictor whose interpolation weight depends on the angle $\angle(\hat{w}, w_\star)$ between the estimator $\hat{w}$ and the true linear weight $w_\star$. We establish that this angular calibration approach is provably well-calibrated in a high-dimensional regime where the number of samples and features both diverge, at a comparable rate. The angle $\angle(\hat{w}, w_\star)$ can be consistently estimated. Furthermore, the resulting predictor is uniquely $\textit{Bregman-optimal}$, minimizing the Bregman divergence to the true label distribution within a suitable class of calibrated predictors. Our work is the first to provide a calibration strategy that satisfies both calibration and optimality properties provably in high dimensions. Additionally, we identify conditions under which a classical Platt-scaling predictor converges to our Bregman-optimal calibrated solution. Thus, Platt-scaling also inherits these desirable properties provably in high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15131v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufan Li, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>Bayesian Predictive Inference Beyond Martingales</title>
      <link>https://arxiv.org/abs/2507.21874</link>
      <description>arXiv:2507.21874v2 Announce Type: replace 
Abstract: There is a growing interest in the so-called Bayesian Predictive Inference approach, which allows to perform Bayesian inference without specifying the likelihood and prior of the model, or the need of any MCMC. Instead, only a sequence of predictive distributions for the observations is required, and inference on the unknown estimand can be performed, cheaply in parallel, using bootstrap-type schemes. Understanding which classes of predictive distributions can be used within this framework, is still a key open question. We relax commonly used probabilistic assumptions on the observations, namely exchangeability and conditional identical distribution, and on their predictive distributions, being measure-valued martingales, by introducing the new class of Almost Conditional Identically Distributed (a.c.i.d.) random variables. This class assumes that the predictive distributions are measure-valued almost supermartingales, and is parametrized by a sequence of parameters $(\xi_n)_{n&gt;0}$, which regulate the decay of conditional dependence among future observations. Under mild summability assumptions on $(\xi_n)_{n&gt;0}$, the resulting sequence of observations is shown to be asymptotically exchangeable, hence amenable to Bayesian Predictive Inference techniques. A.c.i.d. random variables arise naturally in recursive algorithms, and include classic approaches in Statistics and Learning Theory, such as kernel estimators, and more novel ones, such as the parametric Bayesian bootstraps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21874v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Battiston, Lorenzo Cappello</dc:creator>
    </item>
    <item>
      <title>A note on the relation between one-step, outcome regression and IPW-type estimators of parameters with the mixed bias property</title>
      <link>https://arxiv.org/abs/2509.22452</link>
      <description>arXiv:2509.22452v2 Announce Type: replace 
Abstract: Bruns-Smith et al. (2025) established an algebraic identity between the one-step estimator and a specific outcome regression-type estimator for a class of parameters that forms a strict subset of the class introduced in Chernozhukov et al. (2022), assuming both nuisance functions are estimated as linear combinations of given features. They conjectured that this identity extends to the broader mixed bias class introduced in Rotnitzky et al. (2021). In this note, we prove their conjecture and further extend the result to allow one of the nuisance estimators to be non-linear. We also relate these findings to the work of Robins et al. (2007), who established other identities linking one-step estimators to outcome regression-type and IPW-type estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22452v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Rotnitzky, Ezequiel Smucler, James M. Robins</dc:creator>
    </item>
    <item>
      <title>Combining Experimental and Observational Data for Identification and Estimation of Long-Term Causal Effects</title>
      <link>https://arxiv.org/abs/2201.10743</link>
      <description>arXiv:2201.10743v4 Announce Type: replace-cross 
Abstract: We study identifying and estimating the causal effect of a treatment variable on a long-term outcome using data from an observational and an experimental domain. The observational data are subject to unobserved confounding. Furthermore, subjects in the experiment are only followed for a short period; thus, long-term effects are unobserved, though short-term effects are available. Consequently, neither data source alone suffices for causal inference on the long-term outcome, necessitating a principled fusion of the two. We propose three approaches for data fusion for the purpose of identifying and estimating the causal effect. The first assumes equal confounding bias for short-term and long-term outcomes. The second weakens this assumption by leveraging an observed confounder for which the short-term and long-term potential outcomes share the same partial additive association with this confounder. The third approach employs proxy variables of the latent confounder of the treatment-outcome relationship, extending the proximal causal inference framework to the data fusion setting. For each approach, we develop influence function-based estimators and analyze their robustness properties. We illustrate our methods by estimating the effect of class size on 8th-grade SAT scores using data from the Project STAR experiment combined with observational data from the Early Childhood Longitudinal Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.10743v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>AmirEmad Ghassami, Chang Liu, Alan Yang, David Richardson, Ilya Shpitser, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Cell Type Deconvolution</title>
      <link>https://arxiv.org/abs/2202.06420</link>
      <description>arXiv:2202.06420v5 Announce Type: replace-cross 
Abstract: Integrating heterogeneous datasets across different measurement platforms is a fundamental challenge in many scientific applications. A common example arises in deconvolution problems, such as cell type deconvolution, where one aims to estimate the composition of latent subpopulations using reference data from a different source. However, this task is complicated by systematic platform-specific scaling effects, measurement noise, and differences between data sources. For the problem of cell type deconvolution, existing methods often neglect the correlation and uncertainty in cell type proportion estimates, possibly leading to an additional concern of false positives in downstream comparisons across multiple individuals. We introduce MEAD, a statistical framework that provides both accurate estimation and valid statistical inference on the estimates. One of our key contributions is the identifiability result, which establishes the conditions under which cell type compositions are identifiable under arbitrary gene-specific scaling differences across platforms. MEAD also supports the comparison of cell type proportions across individuals after deconvolution, accounting for gene-gene correlations and biological variability. Through simulations and real-data analysis, MEAD demonstrates superior reliability for inferring cell type compositions in complex biological systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.06420v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongyue Xie, Lin Gui, Jingshu Wang</dc:creator>
    </item>
    <item>
      <title>Causal inference for the expected number of recurrent events in the presence of a terminal event</title>
      <link>https://arxiv.org/abs/2306.16571</link>
      <description>arXiv:2306.16571v3 Announce Type: replace-cross 
Abstract: While recurrent event analyses have been extensively studied, limited attention has been given to causal inference within the framework of recurrent event analysis. We develop a multiply robust estimation framework for causal inference in recurrent event data with a terminal failure event. We define our estimand as the vector comprising both the expected number of recurrent events and the failure survival function evaluated along a sequence of landmark times. We show that the estimand can be identified under a weaker condition than conditionally independent censoring and derive the associated class of influence functions under general censoring and failure distributions (i.e., without assuming absolute continuity). We propose a particular estimator within this class for further study, conduct comprehensive simulation studies to evaluate the small-sample performance of our estimator, and illustrate the proposed estimator using a large Medicare dataset to assess the causal effect of PM$_{2.5}$ on recurrent cardiovascular hospitalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16571v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin R. Baer, Trang Bui, Daniel Mork, Robert L. Strawderman, Ashkan Ertefaie</dc:creator>
    </item>
    <item>
      <title>Minimax Regret Learning for Data with Heterogeneous Subgroups</title>
      <link>https://arxiv.org/abs/2405.01709</link>
      <description>arXiv:2405.01709v2 Announce Type: replace-cross 
Abstract: Modern complex datasets often consist of various sub-populations with known group information. In the presence of sub-population heterogeneity, it is crucial to develop robust and generalizable learning methods that (1) can enjoy robust performance on each of the training populations, and (2) is generalizable to an unseen testing population. While various min-max formulations have been proposed to achieve (1) in the robust learning literature, their generalization to an unseen testing is less explored. Moreover, a general min-max formulation can be sensitive to the noise heterogeneity, and, in the extreme case, be degenerate such that a single high-noise population dominates. The min-max-regret (MMR) can mitigate these challenges. In this work, we consider a distribution-free robust hierarchical model for the generalization from multiple training populations to an unseen testing population. Under the robust hierarchical model, the empirical MMR can enjoy the regret guarantees on each of the training populations as well as the unseen testing population. We further specialize the general MMR framework to linear regression and generalized linear model, where we characterize the geometry of MMR and its distinction from other robust methods. We demonstrate the effectiveness of MMR through extensive simulation studies and an application to image recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01709v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weibin Mo, Weijing Tang, Songkai Xue, Yufeng Liu, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference for CP Tensor Factor Models</title>
      <link>https://arxiv.org/abs/2406.17278</link>
      <description>arXiv:2406.17278v2 Announce Type: replace-cross 
Abstract: High-dimensional tensor-valued data have recently gained attention from researchers in economics and finance. We consider the estimation and inference of high-dimensional tensor factor models, where each dimension of the tensor diverges. Our focus is on a factor model that admits CP-type tensor decomposition, which allows for non-orthogonal loading vectors. Based on the contemporary covariance matrix, we propose an iterative simultaneous projection estimation method. Our estimator is robust to weak dependence among factors and weak correlation across different dimensions in the idiosyncratic shocks. We establish an inferential theory, demonstrating both consistency and asymptotic normality under relaxed assumptions. Within a unified framework, we consider two eigenvalue ratio-based estimators for the number of factors in a tensor factor model and justify their consistency. Simulation studies confirm the theoretical results and an empirical application to sorted portfolios reveals three important factors: a market factor, a long-short factor, and a volatility factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17278v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bin Chen, Yuefeng Han, Qiyang Yu</dc:creator>
    </item>
    <item>
      <title>Sharp Concentration of Simple Random Tensors</title>
      <link>https://arxiv.org/abs/2502.16916</link>
      <description>arXiv:2502.16916v2 Announce Type: replace-cross 
Abstract: This paper establishes sharp dimension-free concentration inequalities and expectation bounds for the deviation of the sum of simple random tensors from its expectation. As part of our analysis, we use generic chaining techniques to obtain a sharp high-probability upper bound on the suprema of $L_p$ empirical processes. In so doing, we generalize classical results for quadratic and product empirical processes to higher-order settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16916v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Al-Ghattas, Jiaheng Chen, Daniel Sanz-Alonso</dc:creator>
    </item>
    <item>
      <title>Alignment-Sensitive Minimax Rates for Spectral Algorithms with Learned Kernels</title>
      <link>https://arxiv.org/abs/2509.20294</link>
      <description>arXiv:2509.20294v2 Announce Type: replace-cross 
Abstract: We study spectral algorithms in the setting where kernels are learned from data. We introduce the effective span dimension (ESD), an alignment-sensitive complexity measure that depends jointly on the signal, spectrum, and noise level $\sigma^2$. The ESD is well-defined for arbitrary kernels and signals without requiring eigen-decay conditions or source conditions. We prove that for sequence models whose ESD is at most $K$, the minimax excess risk scales as $\sigma^2 K$. Furthermore, we analyze over-parameterized gradient flow and prove that it can reduce the ESD. This finding establishes a connection between adaptive feature learning and provable improvements in generalization of spectral algorithms. We demonstrate the generality of the ESD framework by extending it to linear models and RKHS regression, and we support the theory with numerical experiments. This framework provides a novel perspective on generalization beyond traditional fixed-kernel theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20294v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongming Huang, Zhifan Li, Yicheng Li, Qian Lin</dc:creator>
    </item>
  </channel>
</rss>
