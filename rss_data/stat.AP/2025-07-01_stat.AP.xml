<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Jul 2025 04:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A General Simulation-Based Optimisation Framework for Multipoint Constant-Stress Accelerated Life Tests</title>
      <link>https://arxiv.org/abs/2507.00722</link>
      <description>arXiv:2507.00722v1 Announce Type: new 
Abstract: Accelerated life testing (ALT) is a method of reducing the lifetime of components through exposure to extreme stress. This method of obtaining lifetime information involves the design of a testing experiment, i.e., an accelerated test plan. In this work, we adopt a simulation-based approach to obtaining optimal test plans for constant-stress accelerated life tests with multiple design points. Within this simulation framework we can easily assess a variety of test plans by modifying the number of test stresses (and their levels) and evaluating the allocation of test units. We obtain optimal test plans by utilising the differential evolution (DE) optimisation algorithm, where the inputs to the objective function are the test plan parameters, and the output is the RMSE (root mean squared error) of out-of-sample (extrapolated) model predictions. When the life-stress distribution is correctly specified, we show that the optimal number of stress levels is related to the number of model parameters. In terms of test unit allocation, we show that the proportion of test units is inversely related to the stress level. Our general simulation framework provides an alternative approach to theoretical optimisation, and is particularly favourable for large/complex multipoint test plans where analytical optimisation could prove intractable. Our procedure can be applied to a broad range of experimental scenarios, and serves as a useful tool to aid practitioners seeking to maximise component lifetime information through accelerated life testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00722v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Owen McGrath, Kevin Burke</dc:creator>
    </item>
    <item>
      <title>Stochastic highway capacity: Unsuitable Kaplan-Meier estimator, revised maximum likelihood estimator, and impact of speed harmonisation</title>
      <link>https://arxiv.org/abs/2507.00893</link>
      <description>arXiv:2507.00893v1 Announce Type: new 
Abstract: The Kaplan-Meier estimate, also known as the product-limit method (PLM), is a widely used non-parametric maximum likelihood estimator (MLE) in survival analysis. In the context of highway engineering, it has been repeatedly applied to estimate stochastic traffic flow capacity. However, this paper demonstrates that PLM is fundamentally unsuitable for this purpose. The method implicitly assumes continuous exposure to failure risk over time - a premise invalid for traffic flow, where intensity does not increase linearly, and capacity is not even directly observable. Although parametric MLE approach offers a viable alternative, earlier derivation suffers from flawed likelihood formulation, likely due to attempt to preserve consistency with PLM. This study derives a corrected likelihood formula for stochastic capacity MLE and validates it using two empirical datasets. The proposed method is then applied in a case study examining the effect of a variable speed limit (VSL) system used for traffic flow speed harmonisation at a 2 to 1 lane drop. Results show that the VSL improved capacity by approximately 10% or reduced breakdown probability at the same flow intensity by up to 50%. The findings underscore the methodological importance of correct model formulation and highlight the practical relevance of stochastic capacity estimation for evaluating traffic control strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00893v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor Mikol\'a\v{s}ek</dc:creator>
    </item>
    <item>
      <title>Online Meal Detection Based on CGM Data Dynamics</title>
      <link>https://arxiv.org/abs/2507.00080</link>
      <description>arXiv:2507.00080v1 Announce Type: cross 
Abstract: We utilize dynamical modes as features derived from Continuous Glucose Monitoring (CGM) data to detect meal events. By leveraging the inherent properties of underlying dynamics, these modes capture key aspects of glucose variability, enabling the identification of patterns and anomalies associated with meal consumption. This approach not only improves the accuracy of meal detection but also enhances the interpretability of the underlying glucose dynamics. By focusing on dynamical features, our method provides a robust framework for feature extraction, facilitating generalization across diverse datasets and ensuring reliable performance in real-world applications. The proposed technique offers significant advantages over traditional approaches, improving detection accuracy,</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00080v1</guid>
      <category>cs.LG</category>
      <category>nlin.AO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>IFAC Diabetes Technology Conference 2025</arxiv:journal_reference>
      <dc:creator>Ali Tavasoli, Heman Shakeri</dc:creator>
    </item>
    <item>
      <title>Hybrid methods for missing categorical covariates in Cox model</title>
      <link>https://arxiv.org/abs/2507.00151</link>
      <description>arXiv:2507.00151v1 Announce Type: cross 
Abstract: Survival analysis aims to explore the relationship between covariates and the time until the occurrence of an event. The Cox proportional hazards model is commonly used for right-censored data, but it is not strictly limited to this type of data. However, the presence of missing values among the covariates, particularly categorical ones, can compromise the validity of the estimates. To address this issue, various classical methods for handling missing data have been proposed within the Cox model framework, including parametric imputation, nonparametric imputation, and semiparametric methods. It is well-documented that none of these methods is universally ideal or optimal, making the choice of the preferred method often complex and challenging. To overcome these limitations, we propose hybrid methods that combine the advantages of classical methods to enhance the robustness of the analyses. Through a simulation study, we demonstrate that these hybrid methods provide increased flexibility, simplified implementation, and improved robustness compared to classical methods. The results from the simulation study highlight that hybrid methods offer increased flexibility, simplified implementation, and greater robustness compared to classical approaches. In particular, they allow for a reduction in estimation bias; however, this improvement comes at the cost of reduced precision, due to increased variability. This observation reflects a well-known methodological trade-off between bias and variance, inherent to the combination of complementary imputation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00151v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdoulaye Dioni, Lynne Moore, Aida Eslami</dc:creator>
    </item>
    <item>
      <title>Penalized FCI for Causal Structure Learning in a Sparse DAG for Biomarker Discovery in Parkinson's Disease</title>
      <link>https://arxiv.org/abs/2507.00173</link>
      <description>arXiv:2507.00173v1 Announce Type: cross 
Abstract: Parkinson's disease (PD) is a progressive neurodegenerative disorder that lacks reliable early-stage biomarkers for diagnosis, prognosis, and therapeutic monitoring. While cerebrospinal fluid (CSF) biomarkers, such as alpha-synuclein seed amplification assays (alphaSyn-SAA), offer diagnostic potential, their clinical utility is limited by invasiveness and incomplete specificity. Plasma biomarkers provide a minimally invasive alternative, but their mechanistic role in PD remains unclear. A major challenge is distinguishing whether plasma biomarkers causally reflect primary neurodegenerative processes or are downstream consequences of disease progression. To address this, we leverage the Parkinson's Progression Markers Initiative (PPMI) Project 9000, containing 2,924 plasma and CSF biomarkers, to systematically infer causal relationships with disease status. However, only a sparse subset of these biomarkers and their interconnections are actually relevant for the disease. Existing causal discovery algorithms, such as Fast Causal Inference (FCI) and its variants, struggle with the high dimensionality of biomarker datasets under sparsity, limiting their scalability. We propose Penalized Fast Causal Inference (PFCI), a novel approach that incorporates sparsity constraints to efficiently infer causal structures in large-scale biological datasets. By applying PFCI to PPMI data, we aim to identify biomarkers that are causally linked to PD pathology, enabling early diagnosis and patient stratification. Our findings will facilitate biomarker-driven clinical trials and contribute to the development of neuroprotective therapies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00173v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samhita Pal, Dhrubajyoti Ghosh, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Toward a Data Processing Pipeline for Mobile-Phone Tracking Data</title>
      <link>https://arxiv.org/abs/2507.00952</link>
      <description>arXiv:2507.00952v1 Announce Type: cross 
Abstract: As mobile phones become ubiquitous, high-frequency smartphone positioning data are increasingly being used by researchers studying the mobility patterns of individuals as they go about their daily routines and the consequences of these patterns for health, behavioral, and other outcomes. A complex data pipeline underlies empirical research leveraging mobile phone tracking data. A key component of this pipeline is transforming raw, time-stamped positions into analysis-ready data objects, typically space-time "trajectories." In this paper, we break down a key portion of the data analysis pipeline underlying the Adolescent Health and Development in Context (AHDC) Study, a large-scale, longitudinal study of youth residing in the Columbus, OH metropolitan area. Recognizing that the bespoke "binning algorithm" used by AHDC researchers resembles a time-series filtering algorithm, we propose a statistical framework - a formal probability model and computational approach to inference - inspired by the binning algorithm for transforming noisy, time-stamped geographic positioning observations into mobility trajectories that capture periods of travel and stability. Our framework, unlike the binning algorithm, allows for formal smoothing via a particle Gibbs algorithm, improving estimation of trajectories as compared to the original binning algorithm. We argue that our framework can be used as a default data processing tool for future mobile-phone tracking studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00952v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcin Jurek, Catherine A. Calder, Corwin Zigler, Bethany Boettner, Christopher R. Browning</dc:creator>
    </item>
    <item>
      <title>clustra: A multi-platform k-means clustering algorithm for analysis of longitudinal trajectories in large electronic health records data</title>
      <link>https://arxiv.org/abs/2507.00962</link>
      <description>arXiv:2507.00962v1 Announce Type: cross 
Abstract: Background and Objective: Variables collected over time, or longitudinally, such as biologic measurements in electronic health records data, are not simple to summarize with a single time-point, and thus can be more holistically conceptualized as trajectories over time. Cluster analysis with longitudinal data further allows for clinical representation of groups of subjects with similar trajectories and identification of unique characteristics, or phenotypes, that can be investigated as risk factors or disease outcomes. Some of the challenges in estimating these clustered trajectories lie in the handling of observations at inconsistent time intervals and the usability of algorithms across programming languages.
  Methods: We propose longitudinal trajectory clustering using a k-means algorithm with thin-plate regression splines, implemented across multiple platforms, the R package clustra and corresponding \SAS macros. The \SAS macros accommodate flexible clustering approaches, and also include visualization of the clusters, and silhouette plots for diagnostic evaluation of the appropriate cluster number. The R package, designed in parallel, has similar functionality, with additional multi-core processing and Rand-index-based diagnostics.
  Results: The package and macros achieve comparable results when applied to an example of simulated blood pressure measurements based on real data from Veterans Affairs Healthcare recipients who were initiated on anti-hypertensive medication.
  Conclusion: The R package clustra and the SAS macros integrate a K-means clustering algorithm for longitudinal trajectories that operates with large electronic health record data. The implementations provide comparable results in both platforms, satisfying the needs of investigators familiar with, or constrained by access to, one or the other platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00962v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nimish Adhikari, Hanna Gerlovin, George Ostrouchov, Rachel Ehrbar, Alyssa B. Dufour, Brian R. Ferolito, Serkalem Demissie, Lauren Costa, Yuk-Lam Ho, Laura Tarko, Edmon Begoli, Kelly Cho, David R. Gagnon</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal smoothing, interpolation and prediction of income distributions based on grouped data</title>
      <link>https://arxiv.org/abs/2207.08384</link>
      <description>arXiv:2207.08384v3 Announce Type: replace 
Abstract: The Housing and Land Survey (HLS) of Japan provides municipality-level grouped data on household incomes. Although these data can be used for effective local policymaking, their analyses are hindered by several challenges, such as limited information attributed to grouping, the presence of non-sampled areas, and the very low frequency of implementing surveys. To address these challenges, we propose a novel grouped-data-based spatio-temporal finite mixture model for estimating the income distributions of multiple spatial units at multiple time points. A unique feature of the proposed method is that all the areas share common latent distributions and that the mixing proportions, including spatial and temporal effects, capture the potential area-wise heterogeneity. Thus, incorporating these effects can smooth out the quantities of interest over time and space, impute missing values, and predict future values. By treating the HLS data with the proposed method, we obtain complete maps of the income and inequality measures at an arbitrary time, which can facilitate rapid and efficient policymaking with fine granularity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.08384v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Genya Kobayashi, Shonosuke Sugasawa, Yuki Kawakubo</dc:creator>
    </item>
    <item>
      <title>Markov Processes for Enhanced Deepfake Generation and Detection</title>
      <link>https://arxiv.org/abs/2411.07993</link>
      <description>arXiv:2411.07993v2 Announce Type: replace 
Abstract: New and existing methods for generating, and especially detecting, deepfakes are investigated and compared on the simple problem of authenticating coin flip data. Importantly, an alternative approach to deepfake generation and detection, which uses a Markov Observation Model (MOM) is introduced and compared on detection ability to the traditional Generative Adversarial Network (GAN) approach as well as Support Vector Machine (SVM), Branching Particle Filtering (BPF) and human alternatives. MOM was also compared on generative and discrimination ability to GAN, filtering and humans (as SVM does not have generative ability). Humans are shown to perform the worst, followed in order by GAN, SVM, BPF and MOM, which was the best at the detection of deepfakes. Unsurprisingly, the order was maintained on the generation problem with removal of SVM as it does not have generation ability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07993v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael A. Kouritzin, Ian Zhang, Jyoti Bhadana, Seoyeon Park</dc:creator>
    </item>
    <item>
      <title>Equipoise calibration of clinical trial design</title>
      <link>https://arxiv.org/abs/2501.03009</link>
      <description>arXiv:2501.03009v2 Announce Type: replace 
Abstract: Clinical trial design ensures that primary analysis outcomes have strong statistical properties. However, mainstream methodology for randomised study design does not establish a formal link between statistical and clinical significance. This paper contributes to bridging this gap by calibrating the operational characteristics of primary trial outcomes to establishing clinical equipoise imbalance. Common late phase designs are shown to provide at least 90% evidence of equipoise imbalance. Designs carrying 95% power at 5% false positive rate are shown to demonstrate 95% evidence of equipoise imbalance, providing an operational definition of a robustly powered study. Equipoise calibration is applied to design of clinical development plans comprising phase 2 and phase 3 studies using standard oncology endpoints. Commonly used power and false positive error rates are shown to provide strong equipoise imbalance when positive outcomes are observed in both phase 2 and phase 3. Establishing strong equipoise imbalance based on inconsistent outcomes of phase 2 and phase 3 studies is shown to require large sample sizes unlikely to be associated with clinically meaningful effect sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03009v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fabio Rigat</dc:creator>
    </item>
    <item>
      <title>Coarse Personalization</title>
      <link>https://arxiv.org/abs/2204.05793</link>
      <description>arXiv:2204.05793v4 Announce Type: replace-cross 
Abstract: With advances in estimating heterogeneous treatment effects, firms can personalize and target individuals at a granular level. However, feasibility constraints limit full personalization. In practice, firms choose segments of individuals and assign a treatment to each segment to maximize profits: We call this the coarse personalization problem. We propose a two-step solution that simultaneously makes segmentation and targeting decisions. First, the firm personalizes by estimating conditional average treatment effects. Second, the firm discretizes using treatment effects to choose which treatments to offer and their segments. We show that a combination of available machine learning tools for estimating heterogeneous treatment effects and a novel application of optimal transport methods provides a viable and efficient solution. With data from a large-scale field experiment in promotions management, we find our methodology outperforms extant approaches that segment on consumer characteristics, consumer preferences, or those that only search over a prespecified grid. Using our procedure, the firm recoups over $99.5\%$ of its expected incremental profits under full personalization while offering only five segments. We conclude by discussing how coarse personalization arises in other domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.05793v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Walter W. Zhang, Sanjog Misra</dc:creator>
    </item>
    <item>
      <title>Nonparametric causal inference for optogenetics: sequential excursion effects for dynamic regimes</title>
      <link>https://arxiv.org/abs/2405.18597</link>
      <description>arXiv:2405.18597v3 Announce Type: replace-cross 
Abstract: Optogenetics is a powerful neuroscience technique for studying how neural circuit manipulation affects behavior. Standard analysis conventions discard information and severely limit the scope of the causal questions that can be probed. To address this gap, we 1) draw connections to the causal inference literature on sequentially randomized experiments, 2) propose a non-parametric framework for analyzing "open-loop" (static regime) optogenetics behavioral experiments, 3) derive extensions of history-restricted marginal structural models for dynamic treatment regimes with positivity violations for "closed-loop" designs, and 4) propose a taxonomy of identifiable causal effects that encompass a far richer collection of scientific questions compared to standard methods. From another view, our work extends "excursion effect" methods, popularized recently in the mobile health literature, to enable estimation of causal contrasts for treatment sequences in the presence of positivity violations. We describe sufficient conditions for identifiability of the proposed causal estimands, and provide asymptotic statistical guarantees for a proposed inverse probability-weighted estimator, a multiply-robust estimator (for two intervention timepoints), a framework for hypothesis testing, and a computationally scalable implementation. Finally, we apply our framework to data from a recent neuroscience study and show how it provides insight into causal effects of optogenetics on behavior that are obscured by standard analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18597v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Loewinger, Alexander W. Levis, Francisco Pereira</dc:creator>
    </item>
    <item>
      <title>Comparing Lasso and Adaptive Lasso in High-Dimensional Data: A Genetic Survival Analysis in Triple-Negative Breast Cancer</title>
      <link>https://arxiv.org/abs/2406.19213</link>
      <description>arXiv:2406.19213v2 Announce Type: replace-cross 
Abstract: In high-dimensional survival analysis, effective variable selection is crucial for both model interpretation and predictive performance. This paper investigates Cox regression with lasso and adaptive lasso penalties in genomic datasets where covariates far outnumber observations. We propose and evaluate four weight calculation strategies for adaptive lasso specifically designed for high-dimensional settings: ridge regression, principal component analysis (PCA), univariate Cox regression, and random survival forest (RSF) based weights. To address the inherent variability in high dimensional model selection, we develop a robust procedure that evaluates performance across multiple data partitions and selects variables based on a novel importance index. Extensive simulation studies demonstrate that adaptive lasso with ridge and PCA weights significantly outperforms standard lasso in variable selection accuracy while maintaining similar or better predictive performance across various correlation structures, censoring proportions (0-80%), and dimensionality settings. These improvements are particularly pronounced in highly-censored scenarios, making our approach valuable for real-world genetic studies with limited observed events. We apply our methodology to triple-negative breast cancer data with 234 patients, over 19500 variables and 82% censoring, identifying key genetic and clinical prognostic factors. Our findings demonstrate that adaptive lasso with appropriate weight calculation provides more stable and interpretable models for high-dimensional survival analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19213v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pilar Gonz\'alez-Barquero (uc3m-Santander Big Data Institute, Universidad Carlos III de Madrid), Rosa E. Lillo (uc3m-Santander Big Data Institute, Universidad Carlos III de Madrid, Department of Statistics, Universidad Carlos III de Madrid), \'Alvaro M\'endez-Civieta (uc3m-Santander Big Data Institute, Universidad Carlos III de Madrid, Department of Biostatistics, Columbia University, New York)</dc:creator>
    </item>
    <item>
      <title>Generative AI-based data augmentation for improved bioacoustic classification in noisy environments</title>
      <link>https://arxiv.org/abs/2412.01530</link>
      <description>arXiv:2412.01530v2 Announce Type: replace-cross 
Abstract: 1. Obtaining data to train robust artificial intelligence (AI)-based models for species classification can be challenging, particularly for rare species. Data augmentation can boost classification accuracy by increasing the diversity of training data and is cheaper to obtain than expert-labelled data. However, many classic image-based augmentation techniques are not suitable for audio spectrograms. 2. We investigate two generative AI models as data augmentation tools to synthesise spectrograms and supplement audio data: Auxiliary Classifier Generative Adversarial Networks (ACGAN) and Denoising Diffusion Probabilistic Models (DDPMs). The latter performed particularly well in terms of both realism of generated spectrograms and accuracy in a resulting classification task. 3. Alongside these new approaches, we present a new audio data set of 640 hours of bird calls from wind farm sites in Ireland, approximately 800 samples of which have been labelled by experts. Wind farm data are particularly challenging for classification models given the background wind and turbine noise. 4. Training an ensemble of classification models on real and synthetic data combined gave 92.6% accuracy (and 90.5% with just the real data) when compared with highly confident BirdNET predictions. 5. Our approach can be used to augment acoustic signals for more species and other land-use types, and has the potential to bring about a step-change in our capacity to develop reliable AI-based detection of rare species. Our code is available at https://github.com/gibbona1/SpectrogramGenAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01530v2</guid>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Gibbons, Emma King, Ian Donohue, Andrew Parnell</dc:creator>
    </item>
    <item>
      <title>Inherited or produced? Inferring protein production kinetics when protein counts are shaped by a cell's division history</title>
      <link>https://arxiv.org/abs/2506.09374</link>
      <description>arXiv:2506.09374v2 Announce Type: replace-cross 
Abstract: Inferring protein production kinetics for dividing cells is complicated protein inheritance from the mother cell. For instance, fluorescence measurements -- commonly used to assess gene activation -- may reflect not only newly produced proteins but also those inherited through successive cell divisions. In such cases, observed protein levels in any given cell are shaped by its division history. As a case study, we examine activation of the glc3 gene in yeast involved in glycogen synthesis and expressed under nutrient-limiting conditions. We monitor this activity using snapshot fluorescence measurements via flow cytometry, where GFP expression reflects glc3 promoter activity. A na\"ive analysis of flow cytometry data ignoring cell division suggests many cells are active with low expression. Explicitly accounting for the (non-Markovian) effects of cell division and protein inheritance makes it impossible to write down a tractable likelihood -- a key ingredient in physics-inspired inference, defining the probability of observing data given a model. The dependence on a cell's division history breaks the assumptions of standard (Markovian) master equations, rendering traditional likelihood-based approaches inapplicable. Instead, we adapt conditional normalizing flows (a class of neural network models designed to learn probability distributions) to approximate otherwise intractable likelihoods from simulated data. In doing so, we find that glc3 is mostly inactive under stress, showing that while cells occasionally activate the gene, expression is brief and transient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09374v2</guid>
      <category>q-bio.QM</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pedro Pessoa, Juan Andres Martinez, Vincent Vandenbroucke, Frank Delvigne, Steve Press\'e</dc:creator>
    </item>
  </channel>
</rss>
