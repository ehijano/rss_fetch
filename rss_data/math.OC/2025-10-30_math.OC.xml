<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Oct 2025 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Decarbonization of Steam Cracking for Clean Olefins Production: Optimal Microgrid Scheduling</title>
      <link>https://arxiv.org/abs/2510.25880</link>
      <description>arXiv:2510.25880v1 Announce Type: new 
Abstract: Ethylene is one of the most ubiquitous chemicals and is predominantly produced through steam cracking. However, steam cracking is highly energy- and carbon-intensive, making its decarbonization a priority. Electrifying the steam cracking process is a promising pathway to reduce carbon emissions. However, this is challenged by the intrinsic conflict between the continuous operational nature of ethylene plants and the intermittent nature of renewable energy sources in modern power systems. A viable solution is to pursue a gradual electrification pathway and operate an ethylene plant as a microgrid that adopts diverse energy sources. To optimize the operational strategy of such a microgrid considering uncertainties in renewable energy generation and market prices, in this work, we propose a novel superstructure for electrified steam cracking systems and introduce a stochastic optimization framework for minimizing the operating costs. Results from a case study show that, given the current status of the power grid and renewable energy generation technologies, the process economics and sustainability of electrified steam cracking do not always favor higher decarbonization levels. To overcome this barrier, electricity from the main grid must be cleaner and cheaper, and energy storage costs per unit stored must decrease. Furthermore, it is important for both chemical and power systems stakeholders must seamlessly coordinate with each other to pursue joint optimization in operation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25880v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saba Ghasemi Naraghi, Tylee Kareck, Lingyun Xiao, Richard Reed, Paritosh Ramanan, Zheyu Jiang</dc:creator>
    </item>
    <item>
      <title>Lifting and partial smoothing for stationary HJB equations and related control problems in infinite dimensions</title>
      <link>https://arxiv.org/abs/2510.25894</link>
      <description>arXiv:2510.25894v1 Announce Type: new 
Abstract: We study a family of stationary Hamilton-Jacobi-Bellman (HJB) equations in Hilbert spaces arising from stochastic optimal control problems. The main difficulties to treat such problems are: the lack of smoothing properties of the linear part of the HJB equation; the presence of unbounded control operators; the presence of state-dependent costs. This features, combined together, prevent the use of the classical mild solution theory of HJB equation (see e.g., Chapter 4 of G. Fabbri, F. Gozzi, A. Swiech, Stochastic Optimal Control in Infinite Dimensions: Dynamic Programming and HJB Equations, Springer, 2017). The problem has been studied in the evolutionary case in F. Gozzi, F. Masiero, Lifting Partial Smoothing to Solve HJB Equations and Stochastic Control Problems, SIAM Journal on Control and Optimization, 63(3), (2025), pp. 1515-1559 using a "lifting technique" (i.e. working in a suitable space of trajectories where a "partial smoothing" property of the linear part of the HJB equations holds. In this paper we extend such a theory to the case of infinite horizon optimal control problems, which are very common, in particular in economic applications. The main results are: the existence and uniqueness of a regular mild solution to the HJB equation; a verification theorem, and the synthesis of optimal feedback controls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25894v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Bolli, Fausto Gozzi</dc:creator>
    </item>
    <item>
      <title>Gradient Flow Sampler-based Distributionally Robust Optimization</title>
      <link>https://arxiv.org/abs/2510.25956</link>
      <description>arXiv:2510.25956v1 Announce Type: new 
Abstract: We propose a mathematically principled PDE gradient flow framework for distributionally robust optimization (DRO). Exploiting the recent advances in the intersection of Markov Chain Monte Carlo sampling and gradient flow theory, we show that our theoretical framework can be implemented as practical algorithms for sampling from worst-case distributions and, consequently, DRO. While numerous previous works have proposed various reformulation techniques and iterative algorithms, we contribute a sound gradient flow view of the distributional optimization that can be used to construct new algorithms. As an example of applications, we solve a class of Wasserstein and Sinkhorn DRO problems using the recently-discovered Wasserstein Fisher-Rao and Stein variational gradient flows. Notably, we also show some simple reductions of our framework recover exactly previously proposed popular DRO methods, and provide new insights into their theoretical limit and optimization dynamics. Numerical studies based on stochastic gradient descent provide empirical backing for our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25956v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zusen Xu, Jia-Jie Zhu</dc:creator>
    </item>
    <item>
      <title>A Parallelized Cutting-Plane Algorithm for Computationally Efficient Modelling to Generate Alternatives</title>
      <link>https://arxiv.org/abs/2510.26029</link>
      <description>arXiv:2510.26029v1 Announce Type: new 
Abstract: Contemporary macro energy systems modelling is characterized by the need to represent strategic and operational decisions with high temporal and spatial resolution and represent discrete investment and retirement decisions. This drive towards greater fidelity, however, conflicts with a simultaneous push towards greater model representation of inherent complexity in decision making, including methods like Modelling to Generate Alternatives (MGA). MGA aims to map the feasible space of a model within a cost slack by varying investment parameters without changing the operational constraints, a process which frequently requires hundreds of solutions. For large, detailed energy system models this is impossible with traditional methods, leading researchers to reduce complexity with linearized investments and zonal or temporal aggregation. This research presents a new solution method for MGA type problems using cutting-plane methods based on a tailored reformulation of Benders Decomposition. We accelerate the algorithm by sharing cuts between MGA master problems and grouping MGA objectives. We find that our new solution method consistently solves MGA problems times faster and requires less memory than existing monolithic Modelling to Generate Alternatives solution methods on linear problems, enabling rapid computation of a greater number of solutions to highly resolved models. We also show that our novel cutting-plane algorithm enables the solution of very large MGA problems with integer investment decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26029v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Lau, Filippo Pecci, Jesse D. Jenkins</dc:creator>
    </item>
    <item>
      <title>An Inventory System with Two Supply Modes and L\'evy Demand</title>
      <link>https://arxiv.org/abs/2510.26039</link>
      <description>arXiv:2510.26039v1 Announce Type: new 
Abstract: This study considers a continuous-review inventory model for a single item with two replenishment modes. Replenishments may occur continuously at any time with a higher unit cost, or at discrete times governed by Poisson arrivals with a lower cost. From a practical standpoint, the model represents an inventory system with random deal offerings. Demand is modeled by a spectrally positive L\'evy process (i.e., a L\'evy process with only positive jumps), which greatly generalizes existing studies. Replenishment quantities are continuous and backorders are allowed, while lead times, perishability, and lost sales are excluded. Using fluctuation theory for spectrally one-sided L\'evy processes, the optimality of a hybrid barrier policy incorporating both kinds of replenishments is established, and a semi-explicit expression for the associated value function is computed. Numerical analysis is provided to support the optimality result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26039v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e Luis P\'erez, Kazutoshi Yamazaki, Qingyuan Zhang</dc:creator>
    </item>
    <item>
      <title>Accelerated Dinkelbach method</title>
      <link>https://arxiv.org/abs/2510.26257</link>
      <description>arXiv:2510.26257v1 Announce Type: new 
Abstract: The classical Dinkelbach method (1967) solves fractional programming via a parametric approach, generating a decreasing upper bound sequence that converges to the optimum. Its important variant, the interval Dinkelbach method (1991), constructs convergent upper and lower bound sequences that bracket the solution and achieve quadratic and superlinear convergence, respectively, under the assumption that the parametric function is twice continuously differentiable. However, this paper demonstrates that a minimal correction, applied solely to the upper bound iterate, is sufficient to boost the convergence of the method, achieving superquadratic and cubic rates for the upper and lower bound sequences, respectively. By strategically integrating this correction, we develop a globally convergent, non-monotone, and accelerated Dinkelbach algorithm-the first of its kind, to our knowledge. Under sufficient differentiability, the new method achieves an asymptotic average convergence order of at least the square root of 5 per iteration, surpassing the quadratic order of the original algorithm. Crucially, this acceleration is achieved while maintaining the key practicality of solving only a single subproblem per iteration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26257v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanzhi Chen, Chuyue Zheng, Yong Xia</dc:creator>
    </item>
    <item>
      <title>Point Convergence Analysis of the Accelerated Gradient Method for Multiobjective Optimization: Continuous and Discrete</title>
      <link>https://arxiv.org/abs/2510.26382</link>
      <description>arXiv:2510.26382v1 Announce Type: new 
Abstract: This paper studies the point convergence of accelerated gradient methods for unconstrained convex smooth multiobjective optimization problems, covering both continuous-time gradient flows and discrete-time algorithms. In single-objective optimization, the point convergence problem of Nesterov's accelerated gradient method at the critical damping parameter $\alpha = 3$ has recently been resolved. This paper extends this theoretical framework to the multiobjective setting, focusing on the multiobjective inertial gradient system with asymptotically vanishing damping (MAVD) with $\alpha =3 $ and the multiobjective accelerated proximal gradient algorithm (MAPG). For the continuous system, we construct a suitable Lyapunov function for the multiobjective setting and prove that, under appropriate assumptions, the trajectory $x(t)$ converges to a weakly Pareto optimal solution. For the discrete algorithm, we construct a corresponding discrete Lyapunov function and prove that the sequence $\{x_k\}$ generated by the algorithm converges to a weakly Pareto optimal solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26382v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingdong Yin</dc:creator>
    </item>
    <item>
      <title>A flexible block-coordinate forward-backward algorithm for non-smooth and non-convex optimization</title>
      <link>https://arxiv.org/abs/2510.26477</link>
      <description>arXiv:2510.26477v1 Announce Type: new 
Abstract: Block coordinate descent (BCD) methods are prevalent in large scale optimization problems due to the low memory and computational costs per iteration, the predisposition to parallelization, and the ability to exploit the structure of the problem. The theoretical and practical performance of BCD relies heavily on the rules defining the choice of the blocks to be updated at each iteration. We propose a new deterministic BCD framework that allows for very flexible updates, while guaranteeing state-of-the-art convergence guarantees on non-smooth nonconvex optimization problems. While encompassing several update rules from the literature, this framework allows for priority on updates of particular blocks and correlations in the block selection between iterations, which is not permitted under the classical convergent stochastic framework. This flexibility is leveraged in the context of multilevel optimization algorithms and, in particular, in multilevel image restoration problems, where the efficiency of the approach is illustrated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26477v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Brice\~no-Arias (DANTE, OCKHAM), Paulo Gon\c{c}alves (DANTE, OCKHAM), Guillaume Lauga (OCKHAM), Nelly Pustelnik (OCKHAM), Elisa Riccietti (OCKHAM)</dc:creator>
    </item>
    <item>
      <title>Discontinuous Behavior of Time-of-Flight Distribution for Bi-impulsive Earth-Moon Transfers in the Three-Body Model</title>
      <link>https://arxiv.org/abs/2510.26567</link>
      <description>arXiv:2510.26567v1 Announce Type: new 
Abstract: As interest in the Earth-Moon transfers renewed around the world, understanding the solution space of transfer trajectories facilitates the construction of transfers. This paper is devoted to reporting a novel or less-reported phenomenon about the solution space of bi-impulsive Earth-Moon transfers in the Earth-Moon planar circular restricted three-body problem. Differing from the previous works focusing on the transfer characteristics of the solution space, we focus on the distribution of the construction parameters, i.e., departure phase angle at the Earth parking orbit, initial-to-circular velocity ratio, and time of flight. Firstly, the construction method of bi-impulsive transfers is described, and the solutions satisfying the given constraints are obtained from the grid search method and trajectory correction. Then, the distribution of the obtained solutions is analyzed, and an interesting phenomenon about the discontinuous behavior of the time-of-flight distribution for each departure phase angle is observed and briefly reported. This phenomenon can further provide useful insight into the construction of bi-impulsive transfers, deepening the understanding of the corresponding solution space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26567v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuyue Fu, Di Wu, Shengping Gong</dc:creator>
    </item>
    <item>
      <title>Accelerating mathematical research with language models: A case study of an interaction with GPT-5-Pro on a convex analysis problem</title>
      <link>https://arxiv.org/abs/2510.26647</link>
      <description>arXiv:2510.26647v1 Announce Type: new 
Abstract: Recent progress in large language models has made them increasingly capable research assistants in mathematics. Yet, as their reasoning abilities improve, evaluating their mathematical competence becomes increasingly challenging. The problems used for assessment must be neither too easy nor too difficult, their performance can no longer be summarized by a single numerical score, and meaningful evaluation requires expert oversight.
  In this work, we study an interaction between the author and a large language model in proving a lemma from convex optimization. Specifically, we establish a Taylor expansion for the gradient of the biconjugation operator--that is, the operator obtained by applying the Fenchel transform twice--around a strictly convex function, with assistance from GPT-5-pro, OpenAI's latest model.
  Beyond the mathematical result itself, whose novelty we do not claim with certainty, our main contribution lies in documenting the collaborative reasoning process. GPT-5-pro accelerated our progress by suggesting, relevant research directions and by proving some intermediate results. However, its reasoning still required careful supervision, particularly to correct subtle mistakes. While limited to a single mathematical problem and a single language model, this experiment illustrates both the promise and the current limitations of large language models as mathematical collaborators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26647v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adil Salim</dc:creator>
    </item>
    <item>
      <title>Minimax Theorems for Possibly Nonconvex Functions</title>
      <link>https://arxiv.org/abs/2510.26710</link>
      <description>arXiv:2510.26710v1 Announce Type: new 
Abstract: This paper establishes three minimax theorems for possibly nonconvex functions on Euclidean spaces or on infinite-dimensional Hilbert spaces. The theorems also guarantee the existence of saddle points. As a by-product, a complete solution to an interesting open problem related to continuously differentiable functions is obtained. The obtained results are analyzed via a concrete example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26710v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nguyen Nang Thieu, Nguyen Dong Yen</dc:creator>
    </item>
    <item>
      <title>Quantum Stochastic Gradient Descent in its continuous-time limit based on the Wigner formulation of Open Quantum Systems</title>
      <link>https://arxiv.org/abs/2510.25910</link>
      <description>arXiv:2510.25910v1 Announce Type: cross 
Abstract: The main ideas behind a research plan to use the Wigner formulation as a bridge between classical and quantum probabilistic algorithms are presented, focusing on a particular case: the Quantum analog of Stochastic Gradient Descent in its continuous-time limit based on the Wigner formulation of Open Quantum Systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25910v1</guid>
      <category>quant-ph</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose A. Morales Escalante</dc:creator>
    </item>
    <item>
      <title>A General and Streamlined Differentiable Optimization Framework</title>
      <link>https://arxiv.org/abs/2510.25986</link>
      <description>arXiv:2510.25986v1 Announce Type: cross 
Abstract: Differentiating through constrained optimization problems is increasingly central to learning, control, and large-scale decision-making systems, yet practical integration remains challenging due to solver specialization and interface mismatches. This paper presents a general and streamlined framework-an updated DiffOpt.jl-that unifies modeling and differentiation within the Julia optimization stack. The framework computes forward - and reverse-mode solution and objective sensitivities for smooth, potentially nonconvex programs by differentiating the KKT system under standard regularity assumptions. A first-class, JuMP-native parameter-centric API allows users to declare named parameters and obtain derivatives directly with respect to them - even when a parameter appears in multiple constraints and objectives - eliminating brittle bookkeeping from coefficient-level interfaces. We illustrate these capabilities on convex and nonconvex models, including economic dispatch, mean-variance portfolio selection with conic risk constraints, and nonlinear robot inverse kinematics. Two companion studies further demonstrate impact at scale: gradient-based iterative methods for strategic bidding in energy markets and Sobolev-style training of end-to-end optimization proxies using solver-accurate sensitivities. Together, these results demonstrate that differentiable optimization can be deployed as a routine tool for experimentation, learning, calibration, and design-without deviating from standard JuMP modeling practices and while retaining access to a broad ecosystem of solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25986v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrew W. Rosemberg, Joaquim Dias Garcia, Fran\c{c}ois Pacaud, Robert B. Parker, Beno\^it Legat, Kaarthik Sundar, Russell Bent, Pascal Van Hentenryck</dc:creator>
    </item>
    <item>
      <title>Data-driven Projection Generation for Efficiently Solving Heterogeneous Quadratic Programming Problems</title>
      <link>https://arxiv.org/abs/2510.26061</link>
      <description>arXiv:2510.26061v1 Announce Type: cross 
Abstract: We propose a data-driven framework for efficiently solving quadratic programming (QP) problems by reducing the number of variables in high-dimensional QPs using instance-specific projection. A graph neural network-based model is designed to generate projections tailored to each QP instance, enabling us to produce high-quality solutions even for previously unseen problems. The model is trained on heterogeneous QPs to minimize the expected objective value evaluated on the projected solutions. This is formulated as a bilevel optimization problem; the inner optimization solves the QP under a given projection using a QP solver, while the outer optimization updates the model parameters. We develop an efficient algorithm to solve this bilevel optimization problem, which computes parameter gradients without backpropagating through the solver. We provide a theoretical analysis of the generalization ability of solving QPs with projection matrices generated by neural networks. Experimental results demonstrate that our method produces high-quality feasible solutions with reduced computation time, outperforming existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26061v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoharu Iwata, Futoshi Futami</dc:creator>
    </item>
    <item>
      <title>Price Levels in Heterogeneous-Agent Models</title>
      <link>https://arxiv.org/abs/2510.26065</link>
      <description>arXiv:2510.26065v1 Announce Type: cross 
Abstract: We study a model of the Fiscal Theory of the Price Level (FTPL) in a Bewley-Huggett-Aiyagari framework with heterogeneous agents. The model is set in continuous time, and ex post heterogeneity arises due to idiosyncratic, uninsurable income shocks. Such models have a natural interpretation as mean-field games, introduced by Huang, Caines, and Malham\'e and by Lasry and Lions. We highlight this connection and discuss the existence and multiplicity of stationary equilibria in models with and without capital. Our focus is on the mathematical analysis, and we prove the existence of two equilibria in which the government runs constant primary deficits, which in turn implies the existence of multiple price levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26065v1</guid>
      <category>econ.TH</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix H\"ofer</dc:creator>
    </item>
    <item>
      <title>Duality-Based Fixed Point Iteration Algorithm for Beamforming Design in ISAC Systems</title>
      <link>https://arxiv.org/abs/2510.26147</link>
      <description>arXiv:2510.26147v1 Announce Type: cross 
Abstract: In this paper, we investigate the beamforming design problem in an integrated sensing and communication (ISAC) system, where a multi-antenna base station simultaneously serves multiple communication users while performing radar sensing. We formulate the problem as the minimization of the total transmit power, subject to signal-to-interference-plus-noise ratio (SINR) constraints for communication users and mean-squared-error (MSE) constraints for radar sensing. The core challenge arises from the complex coupling between communication SINR requirements and sensing performance metrics. To efficiently address this challenge, we first establish the equivalence between the original ISAC beamforming problem and its semidefinite relaxation (SDR), derive its Lagrangian dual formulation, and further reformulate it as a generalized downlink beamforming (GDB) problem with potentially indefinite weighting matrices. Compared to the classical DB problem, the presence of indefinite weighting matrices in the GDB problem introduces substantial analytical and computational challenges. Our key technical contributions include (i) a necessary and sufficient condition for the boundedness of the GDB problem, and (ii) a tailored efficient fixed point iteration (FPI) algorithm with a provable convergence guarantee for solving the GDB problem. Building upon these results, we develop a duality-based fixed point iteration (Dual-FPI) algorithm, which integrates an outer subgradient ascent loop with an inner FPI loop. Simulation results demonstrate that the proposed Dual-FPI algorithm achieves globally optimal solutions while significantly reducing computational complexity compared with existing baseline approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26147v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xilai Fan, Ya-Feng Liu</dc:creator>
    </item>
    <item>
      <title>Hybrid LLM and Higher-Order Quantum Approximate Optimization for CSA Collateral Management</title>
      <link>https://arxiv.org/abs/2510.26217</link>
      <description>arXiv:2510.26217v1 Announce Type: cross 
Abstract: We address finance-native collateral optimization under ISDA Credit Support Annexes (CSAs), where integer lots, Schedule A haircuts, RA/MTA gating, and issuer/currency/class caps create rugged, legally bounded search spaces. We introduce a certifiable hybrid pipeline purpose-built for this domain: (i) an evidence-gated LLM that extracts CSA terms to a normalized JSON (abstain-by-default, span-cited); (ii) a quantum-inspired explorer that interleaves simulated annealing with micro higher order QAOA (HO-QAOA) on binding sub-QUBOs (subset size n &lt;= 16, order k &lt;= 4) to coordinate multi-asset moves across caps and RA-induced discreteness; (iii) a weighted risk-aware objective (Movement, CVaR, funding-priced overshoot) with an explicit coverage window U &lt;= Reff+B; and (iv) CP-SAT as single arbiter to certify feasibility and gaps, including a U-cap pre-check that reports the minimal feasible buffer B*. Encoding caps/rounding as higher-order terms lets HO-QAOA target the domain couplings that defeat local swaps. On government bond datasets and multi-CSA inputs, the hybrid improves a strong classical baseline (BL-3) by 9.1%, 9.6%, and 10.7% across representative harnesses, delivering better cost-movement-tail frontiers under governance settings. We release governance grade artifacts-span citations, valuation matrix audit, weight provenance, QUBO manifests, and CP-SAT traces-to make results auditable and reproducible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26217v1</guid>
      <category>q-fin.CP</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tao Jin (Andrew), Stuart Florescu (Andrew),  Heyu (Andrew),  Jin</dc:creator>
    </item>
    <item>
      <title>Normal Curves in Sub-Finsler Lie Groups: Branching for Strongly Convex Norms and Face Stability for Polyhedral Norms</title>
      <link>https://arxiv.org/abs/2510.26261</link>
      <description>arXiv:2510.26261v1 Announce Type: cross 
Abstract: We consider Lie groups equipped with left-invariant subbundles of their tangent bundles and norms on them. On these sub-Finsler structures, we study the normal curves in the sense of control theory. We revisit the Pontryagin Maximum Principle using tools from convex analysis, expressing the normal equation as a differential inclusion involving the subdifferential of the dual norm. In addition to several properties of normal curves, we discuss their existence, the possibility of branching, and local optimality. Finally, we focus on polyhedral norms and show that normal curves have controls that locally take values in a single face of a sphere with respect to the norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26261v1</guid>
      <category>math.DG</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Enrico Le Donne, Sebastiano Nicolussi Golo, Nicola Paddeu</dc:creator>
    </item>
    <item>
      <title>Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch Regime</title>
      <link>https://arxiv.org/abs/2510.26303</link>
      <description>arXiv:2510.26303v1 Announce Type: cross 
Abstract: Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet its theoretical understanding remains limited. Prior analyses show that Adam favors solutions aligned with $\ell_\infty$-geometry, but these results are restricted to the full-batch regime. In this work, we study the implicit bias of incremental Adam (using one sample per step) for logistic regression on linearly separable data, and we show that its bias can deviate from the full-batch behavior. To illustrate this, we construct a class of structured datasets where incremental Adam provably converges to the $\ell_2$-max-margin classifier, in contrast to the $\ell_\infty$-max-margin bias of full-batch Adam. For general datasets, we develop a proxy algorithm that captures the limiting behavior of incremental Adam as $\beta_2 \to 1$ and we characterize its convergence direction via a data-dependent dual fixed-point formulation. Finally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges to the $\ell_\infty$-max-margin classifier for any batch size by taking $\beta$ close enough to 1. Overall, our results highlight that the implicit bias of Adam crucially depends on both the batching scheme and the dataset, while Signum remains invariant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26303v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beomhan Baek, Minhak Song, Chulhee Yun</dc:creator>
    </item>
    <item>
      <title>Multi-Task Learning Based on Support Vector Machines and Twin Support Vector Machines: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2510.26392</link>
      <description>arXiv:2510.26392v1 Announce Type: cross 
Abstract: Multi-task learning (MTL) enables simultaneous training across related tasks, leveraging shared information to improve generalization, efficiency, and robustness, especially in data-scarce or high-dimensional scenarios. While deep learning dominates recent MTL research, Support Vector Machines (SVMs) and Twin SVMs (TWSVMs) remain relevant due to their interpretability, theoretical rigor, and effectiveness with small datasets.
  This chapter surveys MTL approaches based on SVM and TWSVM, highlighting shared representations, task regularization, and structural coupling strategies. Special attention is given to emerging TWSVM extensions for multi-task settings, which show promise but remain underexplored. We compare these models in terms of theoretical properties, optimization strategies, and empirical performance, and discuss applications in fields such as computer vision, natural language processing, and bioinformatics.
  Finally, we identify research gaps and outline future directions for building scalable, interpretable, and reliable margin-based MTL frameworks. This work provides a comprehensive resource for researchers and practitioners interested in SVM- and TWSVM-based multi-task learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26392v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fatemeh Bazikar, Hossein Moosaei, Atefeh Hemmati, Panos M. Pardalos</dc:creator>
    </item>
    <item>
      <title>Reducing base drag on road vehicles using pulsed jets optimized by hybrid genetic algorithms</title>
      <link>https://arxiv.org/abs/2510.26718</link>
      <description>arXiv:2510.26718v1 Announce Type: cross 
Abstract: Aerodynamic drag on flat-backed vehicles like vans and trucks is dominated by a low-pressure wake, whose control is critical for reducing fuel consumption. This paper presents an experimental study at $Re_W\approx 78,300$ on active flow control using four pulsed jets at the rear edges of a bluff body model. A hybrid genetic algorithm, combining a global search with a local gradient-based optimizer, was used to determine the optimal jet actuation parameters in an experiment-in-the-loop setup. The cost function was designed to achieve a net energy saving by simultaneously minimizing aerodynamic drag and penalizing the actuation's energy consumption. The optimization campaign successfully identified a control strategy that yields a drag reduction of approximately 10%. The optimal control law features a strong, low-frequency actuation from the bottom jet, which targets the main vortex shedding, while the top and lateral jets address higher-frequency, less energetic phenomena. Particle Image Velocimetry analysis reveals a significant upward shift and stabilization of the wake, leading to substantial pressure recovery on the model's lower base. Ultimately, this work demonstrates that a model-free optimization approach can successfully identify non-intuitive, multi-faceted actuation strategies that yield significant and energetically efficient drag reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26718v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Isaac Robledo, Juan Alfaro, V\'ictor Duro, Alberto Solera-Rico, Rodrigo Castellanos, Carlos Sanmiguel Vila</dc:creator>
    </item>
    <item>
      <title>ripALM: A Relative-Type Inexact Proximal Augmented Lagrangian Method for Linearly Constrained Convex Optimization</title>
      <link>https://arxiv.org/abs/2411.13267</link>
      <description>arXiv:2411.13267v2 Announce Type: replace 
Abstract: Inexact proximal augmented Lagrangian methods (pALMs) are particularly appealing for tackling convex constrained optimization problems because of their elegant convergence properties and strong practical performance. To solve the associated pALM subproblems, efficient methods such as Newton-type methods are essential. Consequently, the effectiveness of the inexact pALM hinges on the error criteria used to control the inexactness when solving these subproblems. However, existing inexact pALMs either rely on \textit{absolute-type} error criteria (which may complicate implementation by necessitating the pre-specification of an infinite sequence of error tolerance parameters) or require an additional correction step to guarantee convergence when using \textit{relative-type} error criteria (which can potentially degrade the practical performance of pALM). To address these deficiencies, this paper proposes ripALM, a relative-type inexact pALM, for linearly constrained convex optimization. This method can simplify practical implementation while preserving the appealing convergence properties of the classical absolute-type inexact pALM. To the best of our knowledge, ripALM is the first relative-type inexact version of the vanilla pALM with provable convergence guarantees. Numerical experiments on quadratically regularized optimal transport problems and basis pursuit denoising problems demonstrate the competitive efficiency of the proposed method compared to existing state-of-the-art methods. As our model encompasses a wide range of application problems, the proposed ripALM offers broad applicability and has the potential to serve as a basic optimization toolbox.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13267v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayi Zhu, Ling Liang, Lei Yang, Kim-Chuan Toh</dc:creator>
    </item>
    <item>
      <title>Optimizing Flexible Complex Systems with Coupled and Co-Evolving Subsystems under Operational Uncertainties</title>
      <link>https://arxiv.org/abs/2505.23611</link>
      <description>arXiv:2505.23611v2 Announce Type: replace 
Abstract: The paper develops a novel design optimization framework and associated computational techniques for staged deployment optimization of complex systems under operational uncertainties. It proposes a local scenario discretization method that offers a computationally efficient approach to optimize staged co-deployment of multiple coupled subsystems by decoupling weak dynamic interaction among subsystems. The proposed method is applied to case studies and is demonstrated to provide an effective and scalable strategy to determine the optimal and flexible systems design under uncertainty. The developed optimization framework is expected to improve the staged deployment design of various complex engineering systems, such as water, energy, food, and other infrastructure systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23611v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koki Ho, Masafumi Isaji, Malav Patel, Kayla Garoust</dc:creator>
    </item>
    <item>
      <title>Partially-Supervised Neural Network Model For Quadratic Multiparametric Programming</title>
      <link>https://arxiv.org/abs/2506.05567</link>
      <description>arXiv:2506.05567v2 Announce Type: replace 
Abstract: Neural Networks (NN) with ReLU activation functions are used to model multiparametric quadratic optimization problems (mp-QP) in diverse engineering applications. Researchers have suggested leveraging the piecewise affine property of deep NN models to solve mp-QP with linear constraints, which also exhibit piecewise affine behaviour. However, traditional deep NN applications to mp-QP fall short of providing optimal and feasible predictions, even when trained on large datasets. This study proposes a partially-supervised NN (PSNN) architecture that directly represents the mathematical structure of the global solution function. In contrast to generic NN training approaches, the proposed PSNN method derives a large proportion of model weights directly from the mathematical properties of the optimization problem, producing more accurate solutions despite significantly smaller training data sets. Many energy management problems are formulated as QP, so we apply the proposed approach to energy systems (specifically DC optimal power flow) to demonstrate proof of concept. Model performance in terms of solution accuracy and speed of predictions was compared against a commercial solver and a generic Deep NN model based on classical training. Results show KKT sufficient conditions for PSNN consistently outperform generic NN architectures with classical training using far less data, including when tested on extreme, out-of-training distribution test data. Given its speed advantages over traditional solvers, the PSNN model can quickly produce optimal and feasible solutions within a second for millions of input parameters sampled from a distribution of stochastic demands and renewable generator dispatches, which can be used for simulations and long term planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05567v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fuat Can Beylunioglu, Mehrdad Pirnia, P. Robert Duimering</dc:creator>
    </item>
    <item>
      <title>Data-Driven Performance Guarantees for Parametric Optimization Problems</title>
      <link>https://arxiv.org/abs/2506.23819</link>
      <description>arXiv:2506.23819v4 Announce Type: replace 
Abstract: We propose a data-driven method to establish probabilistic performance guarantees for parametric optimization problems solved via iterative algorithms. Our approach addresses two key challenges: providing convergence guarantees to characterize the worst-case number of iterations required to achieve a predefined tolerance, and upper bounding a performance metric after a fixed number of iterations. These guarantees are particularly useful for online optimization problems with limited computational time, where existing performance guarantees are often unavailable or unduly conservative. We formulate the convergence analysis problem as a scenario optimization program based on a finite set of sampled parameter instances. Leveraging tools from scenario optimization theory enables us to derive probabilistic guarantees on the number of iterations needed to meet a given tolerance level. Using recent advancements in scenario optimization, we further introduce a relaxation approach to trade the number of iterations against the risk of violating convergence criteria thresholds. Additionally, we analyze the trade-off between solution accuracy and time efficiency for fixed-iteration optimization problems by casting them into scenario optimization programs. Numerical simulations demonstrate the efficacy of our approach in providing reliable probabilistic convergence guarantees and evaluating the trade-off between solution accuracy and computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23819v4</guid>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyi Huang, Paul Goulart, Kostas Margellos</dc:creator>
    </item>
    <item>
      <title>GPU-Accelerated Primal Heuristics for Mixed Integer Programming</title>
      <link>https://arxiv.org/abs/2510.20499</link>
      <description>arXiv:2510.20499v2 Announce Type: replace 
Abstract: We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer Programming. Leveraging GPU acceleration enables exploration of larger search regions and faster iterations. A GPU-accelerated PDLP serves as an approximate LP solver, while a new probing cache facilitates rapid roundings and early infeasibility detection. Several state-of-the-art heuristics, including Feasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further accelerated and enhanced. The combined approach of these GPU-driven algorithms yields significant improvements over existing methods, both in the number of feasible solutions and the quality of objectives by achieving 221 feasible solutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20499v2</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akif \c{C}\"ord\"uk, Piotr Sielski, Alice Boucher, Kumar Aatish</dc:creator>
    </item>
    <item>
      <title>Optimal Control Strategies for Multi-Agent Sheep Herding</title>
      <link>https://arxiv.org/abs/2510.25115</link>
      <description>arXiv:2510.25115v2 Announce Type: replace 
Abstract: We develop a cost functional and state-space equations to model the problem of herding m sheep to the origin using n dogs. Our initial approach uses solve_bvp to approximate optimal control trajectories. But this method often fails to converge due to the system's high dimensionality and nonlinearity. However, with a well-chosen initial guess and carefully selected hyperparameters, we succeed in getting solve_bvp to converge. We also explore alternatives including the shooting method and linearization with the iterative Linear Quadratic Regulator (iLQR). While the shooting method also suffers from poor convergence, the linearized iLQR approach proves more scalable and successfully handles scenarios with more agents. However, it struggles in regions where dogs and sheep are in close proximity, due to strong nonlinearities that violate the assumptions of local linearization. This leads to jagged, oscillatory paths and slow convergence, particularly when the number of sheep exceeds the number of dogs. These challenges reveal key limitations of standard numerical techniques in multi-agent control and underscore the need for more robust, nonlinear strategies for coordinating interacting agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25115v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Drake Brown, Trevor Garrity, Daniel Perkins, Davis Hunter, Wyatt Pochman</dc:creator>
    </item>
    <item>
      <title>Data-Driven Stabilization Using Prior Knowledge on Stabilizability and Controllability</title>
      <link>https://arxiv.org/abs/2510.25452</link>
      <description>arXiv:2510.25452v2 Announce Type: replace 
Abstract: In this work, we study data-driven stabilization of linear time-invariant systems using prior knowledge of system-theoretic properties, specifically stabilizability and controllability. To formalize this, we extend the concept of data informativity by requiring the existence of a controller that stabilizes all systems consistent with the data and the prior knowledge. We show that if the system is controllable, then incorporating this as prior knowledge does not relax the conditions required for data-driven stabilization. Remarkably, however, we show that if the system is stabilizable, then using this as prior knowledge leads to necessary and sufficient conditions that are weaker than those for data-driven stabilization without prior knowledge. In other words, data-driven stabilization is easier if one knows that the underlying system is stabilizable. We also provide new data-driven control design methods in terms of linear matrix inequalities that complement the conditions for informativity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25452v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Shakouri, Henk J. van Waarde, Tren M. J. T. Baltussen, W. P. M. H. Heemels</dc:creator>
    </item>
    <item>
      <title>Reward Collapse in Aligning Large Language Models</title>
      <link>https://arxiv.org/abs/2305.17608</link>
      <description>arXiv:2305.17608v2 Announce Type: replace-cross 
Abstract: The extraordinary capabilities of large language models (LLMs) such as ChatGPT and GPT-4 are in part unleashed by aligning them with reward models that are trained on human preferences, which are often represented as rankings of responses to prompts. In this paper, we document the phenomenon of \textit{reward collapse}, an empirical observation where the prevailing ranking-based approach results in an \textit{identical} reward distribution \textit{regardless} of the prompts during the terminal phase of training. This outcome is undesirable as open-ended prompts like ``write a short story about your best friend'' should yield a continuous range of rewards for their completions, while specific prompts like ``what is the capital of New Zealand'' should generate either high or low rewards. Our theoretical investigation reveals that reward collapse is primarily due to the insufficiency of the ranking-based objective function to incorporate prompt-related information during optimization. This insight allows us to derive closed-form expressions for the reward distribution associated with a set of utility functions in an asymptotic regime. To overcome reward collapse, we introduce a prompt-aware optimization scheme that provably admits a prompt-dependent reward distribution within the interpolating regime. Our experimental results suggest that our proposed prompt-aware utility functions significantly alleviate reward collapse during the training of reward models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17608v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ziang Song, Tianle Cai, Jason D. Lee, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>GSE: Group-wise Sparse and Explainable Adversarial Attacks</title>
      <link>https://arxiv.org/abs/2311.17434</link>
      <description>arXiv:2311.17434v5 Announce Type: replace-cross 
Abstract: Sparse adversarial attacks fool deep neural networks (DNNs) through minimal pixel perturbations, often regularized by the $\ell_0$ norm. Recent efforts have replaced this norm with a structural sparsity regularizer, such as the nuclear group norm, to craft group-wise sparse adversarial attacks. The resulting perturbations are thus explainable and hold significant practical relevance, shedding light on an even greater vulnerability of DNNs. However, crafting such attacks poses an optimization challenge, as it involves computing norms for groups of pixels within a non-convex objective. We address this by presenting a two-phase algorithm that generates group-wise sparse attacks within semantically meaningful areas of an image. Initially, we optimize a quasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored for non-convex programming. Subsequently, the algorithm transitions to a projected Nesterov's accelerated gradient descent with $2-$norm regularization applied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and ImageNet datasets demonstrate a remarkable increase in group-wise sparsity, e.g., $50.9\%$ on CIFAR-10 and $38.4\%$ on ImageNet (average case, targeted attack). This performance improvement is accompanied by significantly faster computation times, improved explainability, and a $100\%$ attack success rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17434v5</guid>
      <category>cs.CV</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shpresim Sadiku, Moritz Wagner, Sebastian Pokutta</dc:creator>
    </item>
    <item>
      <title>SafEDMD: A Koopman-based data-driven controller design framework for nonlinear dynamical systems</title>
      <link>https://arxiv.org/abs/2402.03145</link>
      <description>arXiv:2402.03145v4 Announce Type: replace-cross 
Abstract: The Koopman operator serves as the theoretical backbone for machine learning of dynamical control systems, where the operator is heuristically approximated by extended dynamic mode decomposition (EDMD). In this paper, we propose SafEDMD, a novel stability- and feedback-oriented EDMD-based controller design framework. Our approach leverages a reliable surrogate model generated in a data-driven fashion in order to provide closed-loop guarantees. In particular, we establish a controller design based on semi-definite programming with guaranteed stabilization of the underlying nonlinear system. As central ingredient, we derive proportional error bounds that vanish at the origin and are tailored to control tasks. We illustrate the developed method by means of several benchmark examples and highlight the advantages over state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03145v4</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robin Str\"asser, Manuel Schaller, Karl Worthmann, Julian Berberich, Frank Allg\"ower</dc:creator>
    </item>
    <item>
      <title>End-to-end guarantees for indirect data-driven control of bilinear systems with finite stochastic data</title>
      <link>https://arxiv.org/abs/2409.18010</link>
      <description>arXiv:2409.18010v3 Announce Type: replace-cross 
Abstract: In this paper we propose an end-to-end algorithm for indirect data-driven control for bilinear systems with stability guarantees. We consider the case where the collected i.i.d. data is affected by probabilistic noise with possibly unbounded support and leverage tools from statistical learning theory to derive finite sample identification error bounds. To this end, we solve the bilinear identification problem by solving a set of linear and affine identification problems, by a particular choice of a control input during the data collection phase. We provide a priori as well as data-dependent finite sample identification error bounds on the individual matrices as well as ellipsoidal bounds, both of which are structurally suitable for control. Further, we integrate the structure of the derived identification error bounds in a robust controller design to obtain an exponentially stable closed-loop. By means of an extensive numerical study we showcase the interplay between the controller design and the derived identification error bounds. Moreover, we note appealing connections of our results to indirect data-driven control of general nonlinear systems through Koopman operator theory and discuss how our results may be applied in this setup.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18010v3</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Chatzikiriakos, Robin Str\"asser, Frank Allg\"ower, Andrea Iannelli</dc:creator>
    </item>
    <item>
      <title>Differentiation Through Black-Box Quadratic Programming Solvers</title>
      <link>https://arxiv.org/abs/2410.06324</link>
      <description>arXiv:2410.06324v4 Announce Type: replace-cross 
Abstract: Differentiable optimization has attracted significant research interest, particularly for quadratic programming (QP). Existing approaches for differentiating the solution of a QP with respect to its defining parameters often rely on specific integrated solvers. This integration limits their applicability, including their use in neural network architectures and bi-level optimization tasks, restricting users to a narrow selection of solver choices. To address this limitation, we introduce dQP, a modular and solver-agnostic framework for plug-and-play differentiation of virtually any QP solver. A key insight we leverage to achieve modularity is that, once the active set of inequality constraints is known, both the solution and its derivative can be expressed using simplified linear systems that share the same matrix. This formulation fully decouples the computation of the QP solution from its differentiation. Building on this result, we provide a minimal-overhead, open-source implementation ( https://github.com/cwmagoon/dQP ) that seamlessly integrates with over 15 state-of-the-art solvers. Comprehensive benchmark experiments demonstrate dQP's robustness and scalability, particularly highlighting its advantages in large-scale sparse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06324v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Connor W. Magoon, Fengyu Yang, Noam Aigerman, Shahar Z. Kovalsky</dc:creator>
    </item>
    <item>
      <title>Beyond likelihood ratio bias: Nested multi-time-scale stochastic approximation for likelihood-free parameter estimation</title>
      <link>https://arxiv.org/abs/2411.12995</link>
      <description>arXiv:2411.12995v2 Announce Type: replace-cross 
Abstract: We study parameter inference in simulation-based stochastic models where the analytical form of the likelihood is unknown. The main difficulty is that score evaluation as a ratio of noisy Monte Carlo estimators induces bias and instability, which we overcome with a ratio-free nested multi-time-scale (NMTS) stochastic approximation (SA) method that simultaneously tracks the score and drives the parameter update. We provide a comprehensive theoretical analysis of the proposed NMTS algorithm for solving likelihood-free inference problems, including strong convergence, asymptotic normality, and convergence rates. We show that our algorithm can eliminate the original asymptotic bias $O\big(\sqrt{\frac{1}{N}}\big)$ and accelerate the convergence rate from $O\big(\beta_k+\sqrt{\frac{1}{N}}\big)$ to $O\big(\frac{\beta_k}{\alpha_k}+\sqrt{\frac{\alpha_k}{N}}\big)$, where $N$ is the fixed batch size, $\alpha_k$ and $\beta_k$ are decreasing step sizes with $\alpha_k$, $\beta_k$, $\beta_k/\alpha_k\rightarrow 0$. With proper choice of $\alpha_k$ and $\beta_k$, our convergence rates can match the optimal rate in the multi-time-scale SA literature. Numerical experiments demonstrate that our algorithm can improve the estimation accuracy by one to two orders of magnitude at the same computational cost, making it efficient for parameter estimation in stochastic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12995v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zehao Li, Zhouchen Lin, Yijie Peng</dc:creator>
    </item>
    <item>
      <title>The Learning Approach to Games</title>
      <link>https://arxiv.org/abs/2503.00227</link>
      <description>arXiv:2503.00227v3 Announce Type: replace-cross 
Abstract: This work introduces a unified framework for analyzing games in greater depth. In the existing literature, players' strategies are typically assigned scalar values, and equilibrium concepts are used to identify compatible choices. However, this approach neglects the internal structure of players, thereby failing to accurately model observed behaviors.
  To address this limitation, we propose an abstract definition of a player, consistent with constructions in reinforcement learning. Instead of defining games as external settings, our framework defines them in terms of the players themselves. This offers a language that enables a deeper connection between games and learning. To illustrate the need for this generality, we study a simple two-player game and show that even in basic settings, a sophisticated player may adopt dynamic strategies that cannot be captured by simpler models or compatibility analysis.
  For a general definition of a player, we discuss natural conditions on its components and define competition through their behavior. In the discrete setting, we consider players whose estimates largely follow the standard framework from the literature. We explore connections to correlated equilibrium and highlight that dynamic programming naturally applies to all estimates. In the mean-field setting, we exploit symmetry to construct explicit examples of equilibria. Finally, we conclude by examining relations to reinforcement learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00227v3</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melih \.I\c{s}eri, Erhan Bayraktar</dc:creator>
    </item>
    <item>
      <title>A Constrained Saddle Search Approach for Constructing Singular and Flexible Bar Frameworks</title>
      <link>https://arxiv.org/abs/2503.14807</link>
      <description>arXiv:2503.14807v2 Announce Type: replace-cross 
Abstract: Singularity analysis is essential in robot kinematics, as singular configurations cause loss of control and kinematic indeterminacy. This paper models singularities in bar frameworks as saddle points on constrained manifolds. Given an under-constrained, non-singular bar framework, by allowing one edge to vary its length while fixing lengths of others, we define the squared length of the free edge as an energy functional and show that its local saddle points correspond to singular and flexible frameworks. Using our constrained saddle search approach, we identify previously unknown singular and flexible bar frameworks, providing new insights into singular robotics design and analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14807v2</guid>
      <category>cs.RO</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xuenan Li, Mihnea Leonte, Christian D. Santangelo, Miranda Holmes-Cerfon</dc:creator>
    </item>
    <item>
      <title>Learning to Insert for Constructive Neural Vehicle Routing Solver</title>
      <link>https://arxiv.org/abs/2505.13904</link>
      <description>arXiv:2505.13904v3 Announce Type: replace-cross 
Abstract: Neural Combinatorial Optimisation (NCO) is a promising learning-based approach for solving Vehicle Routing Problems (VRPs) without extensive manual design. While existing constructive NCO methods typically follow an appending-based paradigm that sequentially adds unvisited nodes to partial solutions, this rigid approach often leads to suboptimal results. To overcome this limitation, we explore the idea of insertion-based paradigm and propose Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel learning-based method for constructive NCO. Unlike traditional approaches, L2C-Insert builds solutions by strategically inserting unvisited nodes at any valid position in the current partial solution, which can significantly enhance the flexibility and solution quality. The proposed framework introduces three key components: a novel model architecture for precise insertion position prediction, an efficient training scheme for model optimization, and an advanced inference technique that fully exploits the insertion paradigm's flexibility. Extensive experiments on both synthetic and real-world instances of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior performance across various problem sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13904v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fu Luo, Xi Lin, Mengyuan Zhong, Fei Liu, Zhenkun Wang, Jianyong Sun, Qingfu Zhang</dc:creator>
    </item>
    <item>
      <title>Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training</title>
      <link>https://arxiv.org/abs/2507.09846</link>
      <description>arXiv:2507.09846v3 Announce Type: replace-cross 
Abstract: As both model and dataset sizes continue to scale rapidly, conventional pretraining strategies with fixed compute budgets-such as cosine learning rate schedules-are increasingly inadequate for large-scale training. Recent alternatives, including warmup-stable-decay (WSD) schedules and weight averaging, offer greater flexibility. However, WSD relies on explicit decay phases to track progress, while weight averaging addresses this limitation at the cost of additional memory. In search of a more principled and scalable alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024], which has shown strong empirical performance across diverse settings. We show that SF-AdamW effectively navigates the "river" structure of the loss landscape without decay phases or auxiliary averaging, making it particularly suitable for continuously scaling training workloads. To understand this behavior, we conduct a theoretical and empirical analysis of SF dynamics, revealing that it implicitly performs weight averaging without memory overhead. Guided by this analysis, we propose a refined variant of SF that improves robustness to momentum and performs better under large batch sizes, addressing key limitations of the original method. Together, these results establish SF as a practical, scalable, and theoretically grounded approach for language model training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09846v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minhak Song, Beomhan Baek, Kwangjun Ahn, Chulhee Yun</dc:creator>
    </item>
    <item>
      <title>Convex computation of regions of attraction from data using Sums-of-Squares programming</title>
      <link>https://arxiv.org/abs/2507.14073</link>
      <description>arXiv:2507.14073v2 Announce Type: replace-cross 
Abstract: The paper concentrates on the analysis of the Region of Attraction (RoA) for unknown autonomous dynamical systems. The aim is to explore a data-driven approach based on moment Sum-of-Squares (SoS) hierarchy, which enables novel RoA outer approximations despite the reduced information on the structure of the dynamics. The main contribution of this work is bypassing the system model and, consequently, the recurring constraint on its polynomial structure. Numerical experimentation showcases the influence of data on learned approximating sets, offering a promising outlook on the potential of this method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14073v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Oumayma Khattabi, Matteo Tacchi, Sorin Olaru</dc:creator>
    </item>
    <item>
      <title>Distributed optimization: designed for federated learning</title>
      <link>https://arxiv.org/abs/2508.08606</link>
      <description>arXiv:2508.08606v3 Announce Type: replace-cross 
Abstract: Federated learning (FL), as a distributed collaborative machine learning (ML) framework under privacy-preserving constraints, has garnered increasing research attention in cross-organizational data collaboration scenarios. This paper proposes a class of distributed optimization algorithms based on the augmented Lagrangian technique, designed to accommodate diverse communication topologies in both centralized and decentralized FL settings. Furthermore, we develop multiple termination criteria and parameter update mechanisms to enhance computational efficiency, accompanied by rigorous theoretical guarantees of convergence. By generalizing the augmented Lagrangian relaxation through the incorporation of proximal relaxation and quadratic approximation, our framework systematically recovers a broad of classical unconstrained optimization methods, including proximal algorithm, classic gradient descent, and stochastic gradient descent, among others. Notably, the convergence properties of these methods can be naturally derived within the proposed theoretical framework. Numerical experiments demonstrate that the proposed algorithm exhibits strong performance in large-scale settings with significant statistical heterogeneity across clients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08606v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenyou Guo, Ting Qu, Chunrong Pan, George Q. Huang</dc:creator>
    </item>
    <item>
      <title>A Constrained Optimization Approach for Constructing Rigid Bar Frameworks with Higher-order Rigidity</title>
      <link>https://arxiv.org/abs/2509.23072</link>
      <description>arXiv:2509.23072v2 Announce Type: replace-cross 
Abstract: We present a systematic approach for constructing bar frameworks that are rigid but not first-order rigid, using constrained optimization. We show that prestress stable (but not first-order rigid) frameworks arise as the solution to a simple optimization problem, which asks to maximize or minimize the length of one edge while keep the other edge lengths fixed. By starting with a random first-order rigid framework, we can thus design a wide variety of prestress stable frameworks, which, unlike many examples known in the literature, have no special symmetries. We then show how to incorporate a bifurcation method to design frameworks that are third-order rigid. Our results highlight connections between concepts in rigidity theory and constrained optimization, offering new insights into the construction and analysis of bar frameworks with higher-order rigidity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23072v2</guid>
      <category>math.MG</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xuenan Li, Christian D. Santangelo, Miranda Holmes-Cerfon</dc:creator>
    </item>
    <item>
      <title>Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures</title>
      <link>https://arxiv.org/abs/2510.24607</link>
      <description>arXiv:2510.24607v2 Announce Type: replace-cross 
Abstract: We introduce Entropy-Guided Multiplicative Updates (EGMU), a convex optimization framework for constructing multi-factor target-exposure portfolios by minimizing Kullback-Leibler divergence from a benchmark under linear factor constraints. We establish feasibility and uniqueness of strictly positive solutions when the benchmark and targets satisfy convex-hull conditions. We derive the dual concave formulation with explicit gradient, Hessian, and sensitivity expressions, and provide two provably convergent solvers: a damped dual Newton method with global convergence and local quadratic rate, and a KL-projection scheme based on iterative proportional fitting and Bregman-Dykstra projections. We further generalize EGMU to handle elastic targets and robust target sets, and introduce a path-following ordinary differential equation for tracing solution trajectories. Stable and scalable implementations are provided using LogSumExp stabilization, covariance regularization, and half-space KL projections. Our focus is on theory and reproducible algorithms; empirical benchmarking is optional.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24607v2</guid>
      <category>q-fin.PM</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yimeng Qiu</dc:creator>
    </item>
    <item>
      <title>A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2510.25366</link>
      <description>arXiv:2510.25366v2 Announce Type: replace-cross 
Abstract: The key task of machine learning is to minimize the loss function that measures the model fit to the training data. The numerical methods to do this efficiently depend on the properties of the loss function. The most decisive among these properties is the convexity or non-convexity of the loss function. The fact that the loss function can have, and frequently has, non-convex regions has led to a widespread commitment to non-convex methods such as Adam. However, a local minimum implies that, in some environment around it, the function is convex. In this environment, second-order minimizing methods such as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We propose a novel framework grounded in the hypothesis that loss functions in real-world tasks swap from initial non-convexity to convexity towards the optimum. This is a property we leverage to design an innovative two-phase optimization algorithm. The presented algorithm detects the swap point by observing the gradient norm dependence on the loss. In these regions, non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing experiments confirm the hypothesis that this simple convexity structure is frequent enough to be practically exploited to substantially improve convergence and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25366v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.5220/0013696100004000</arxiv:DOI>
      <dc:creator>Tomas Hrycej, Bernhard Bermeitinger, Massimo Pavone, G\"otz-Henrik Wiegand, Siegfried Handschuh</dc:creator>
    </item>
  </channel>
</rss>
