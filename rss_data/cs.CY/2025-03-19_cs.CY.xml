<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Mar 2025 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Content ARCs: Decentralized Content Rights in the Age of Generative AI</title>
      <link>https://arxiv.org/abs/2503.14519</link>
      <description>arXiv:2503.14519v1 Announce Type: new 
Abstract: The rise of Generative AI (GenAI) has sparked significant debate over balancing the interests of creative rightsholders and AI developers. As GenAI models are trained on vast datasets that often include copyrighted material, questions around fair compensation and proper attribution have become increasingly urgent. To address these challenges, this paper proposes a framework called \emph{Content ARCs} (Authenticity, Rights, Compensation). By combining open standards for provenance and dynamic licensing with data attribution, and decentralized technologies, Content ARCs create a mechanism for managing rights and compensating creators for using their work in AI training. We characterize several nascent works in the AI data licensing space within Content ARCs and identify where challenges remain to fully implement the end-to-end framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14519v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <category>eess.IV</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kar Balan, Andrew Gilbert, John Collomosse</dc:creator>
    </item>
    <item>
      <title>Policy Frameworks for Transparent Chain-of-Thought Reasoning in Large Language Models</title>
      <link>https://arxiv.org/abs/2503.14521</link>
      <description>arXiv:2503.14521v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by decomposing complex problems into step-by-step solutions, improving performance on reasoning tasks. However, current CoT disclosure policies vary widely across different models in frontend visibility, API access, and pricing strategies, lacking a unified policy framework. This paper analyzes the dual-edged implications of full CoT disclosure: while it empowers small-model distillation, fosters trust, and enables error diagnosis, it also risks violating intellectual property, enabling misuse, and incurring operational costs. We propose a tiered-access policy framework that balances transparency, accountability, and security by tailoring CoT availability to academic, business, and general users through ethical licensing, structured reasoning outputs, and cross-tier safeguards. By harmonizing accessibility with ethical and operational considerations, this framework aims to advance responsible AI deployment while mitigating risks of misuse or misinterpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14521v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yihang Chen, Haikang Deng, Kaiqiao Han, Qingyue Zhao</dc:creator>
    </item>
    <item>
      <title>Accessibility Considerations in the Development of an AI Action Plan</title>
      <link>https://arxiv.org/abs/2503.14522</link>
      <description>arXiv:2503.14522v1 Announce Type: new 
Abstract: We argue that there is a need for Accessibility to be represented in several important domains:
  - Capitalize on the new capabilities AI provides - Support for open source development of AI, which can allow disabled and disability focused professionals to contribute, including
  - Development of Accessibility Apps which help realise the promise of AI in accessibility domains
  - Open Source Model Development and Validation to ensure that accessibility concerns are addressed in these algorithms
  - Data Augmentation to include accessibility in data sets used to train models
  - Accessible Interfaces that allow disabled people to use any AI app, and to validate its outputs
  - Dedicated Functionality and Libraries that can make it easy to integrate AI support into a variety of settings and apps. - Data security and privacy and privacy risks including data collected by AI based accessibility technologies; and the possibility of disability disclosure. - Disability-specific AI risks and biases including both direct bias (during AI use by the disabled person) and indirect bias (when AI is used by someone else on data relating to a disabled person).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14522v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jennifer Mankoff, Janice Light, James Coughlan, Christian Vogler, Abraham Glasser, Gregg Vanderheiden, Laura Rice</dc:creator>
    </item>
    <item>
      <title>Threefold model for AI Readiness: A Case Study with Finnish Healthcare SMEs</title>
      <link>https://arxiv.org/abs/2503.14527</link>
      <description>arXiv:2503.14527v1 Announce Type: new 
Abstract: This study examines AI adoption among Finnish healthcare SMEs through semi-structured interviews with six health-tech companies. We identify three AI engagement categories: AI-curious (exploring AI), AI-embracing (integrating AI), and AI-catering (providing AI solutions). Our proposed threefold model highlights key adoption barriers, including regulatory complexities, technical expertise gaps, and financial constraints. While SMEs recognize AI's potential, most remain in early adoption stages. We provide actionable recommendations to accelerate AI integration, focusing on regulatory reforms, talent development, and inter-company collaboration, offering valuable insights for healthcare organizations, policymakers, and researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14527v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mohammed Alnajjar, Khalid Alnajjar, Mika H\"am\"al\"ainen</dc:creator>
    </item>
    <item>
      <title>Ethical Implications of AI in Data Collection: Balancing Innovation with Privacy</title>
      <link>https://arxiv.org/abs/2503.14539</link>
      <description>arXiv:2503.14539v1 Announce Type: new 
Abstract: This article examines the ethical and legal implications of artificial intelligence (AI) driven data collection, focusing on developments from 2023 to 2024. It analyzes recent advancements in AI technologies and their impact on data collection practices across various sectors. The study compares regulatory approaches in the European Union, the United States, and China, highlighting the challenges in creating a globally harmonized framework for AI governance. Key ethical issues, including informed consent, algorithmic bias, and privacy protection, are critically assessed in the context of increasingly sophisticated AI systems. The research explores case studies in healthcare, finance, and smart cities to illustrate the practical challenges of AI implementation. It evaluates the effectiveness of current legal frameworks and proposes solutions encompassing legal and policy recommendations, technical safeguards, and ethical frameworks. The article emphasizes the need for adaptive governance and international cooperation to address the global nature of AI development while balancing innovation with the protection of individual rights and societal values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14539v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.36719/2706-6185/38/40-55</arxiv:DOI>
      <dc:creator>Shahmar Mirishli</dc:creator>
    </item>
    <item>
      <title>The Role of Legal Frameworks in Shaping Ethical Artificial Intelligence Use in Corporate Governance</title>
      <link>https://arxiv.org/abs/2503.14540</link>
      <description>arXiv:2503.14540v1 Announce Type: new 
Abstract: This article examines the evolving role of legal frameworks in shaping ethical artificial intelligence (AI) use in corporate governance. As AI systems become increasingly prevalent in business operations and decision-making, there is a growing need for robust governance structures to ensure their responsible development and deployment. Through analysis of recent legislative initiatives, industry standards, and scholarly perspectives, this paper explores key legal and regulatory approaches aimed at promoting transparency, accountability, and fairness in corporate AI applications. It evaluates the strengths and limitations of current frameworks, identifies emerging best practices, and offers recommendations for developing more comprehensive and effective AI governance regimes. The findings highlight the importance of adaptable, principle-based regulations coupled with sector-specific guidance to address the unique challenges posed by AI technologies in the corporate sphere.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14540v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shahmar Mirishli</dc:creator>
    </item>
    <item>
      <title>Regulating Ai In Financial Services: Legal Frameworks And Compliance Challenges</title>
      <link>https://arxiv.org/abs/2503.14541</link>
      <description>arXiv:2503.14541v1 Announce Type: new 
Abstract: This article examines the evolving landscape of artificial intelligence (AI) regulation in financial services, detailing the legal frameworks and compliance challenges posed by rapid technological adoption. By reviewing current legislation, industry guidelines, and real-world use cases, it highlights how AI-driven processes, from fraud detection to algorithmic trading, offer efficiency gains yet introduce significant risks, including algorithmic bias, data privacy breaches, and lack of transparency in automated decision-making. The study compares regulatory approaches across major jurisdictions such as the European Union, United States, and United Kingdom, identifying both universal concerns, like the need for explainability and robust data protection, and region-specific compliance requirements that impact the implementation of high-risk AI applications. Additionally, it underscores emerging areas of focus, such as liability for AI-driven errors, systemic risks posed by interlinked AI systems, and the ethical considerations of technology-driven financial exclusion. The findings reveal gaps in existing rules and emphasize the necessity for adaptive, technology-neutral policies capable of fostering innovation while safeguarding consumer rights and market integrity. The article concludes by proposing a principled regulatory model that balances flexibility with enforceable standards, advocating closer collaboration between policymakers, financial institutions, and AI developers to ensure a secure, fair, and forward-looking framework for AI in finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14541v1</guid>
      <category>cs.CY</category>
      <category>q-fin.GN</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shahmar Mirishli</dc:creator>
    </item>
    <item>
      <title>Inteligencia Artificial para la conservaci\'on y uso sostenible de la biodiversidad, una visi\'on desde Colombia (Artificial Intelligence for conservation and sustainable use of biodiversity, a view from Colombia)</title>
      <link>https://arxiv.org/abs/2503.14543</link>
      <description>arXiv:2503.14543v1 Announce Type: new 
Abstract: The rise of artificial intelligence (AI) and the aggravating biodiversity crisis have resulted in a research area where AI-based computational methods are being developed to act as allies in conservation, and the sustainable use and management of natural resources. While important general guidelines have been established globally regarding the opportunities and challenges that this interdisciplinary research offers, it is essential to generate local reflections from the specific contexts and realities of each region. Hence, this document aims to analyze the scope of this research area from a perspective focused on Colombia and the Neotropics. In this paper, we summarize the main experiences and debates that took place at the Humboldt Institute between 2023 and 2024 in Colombia. To illustrate the variety of promising opportunities, we present current uses such as automatic species identification from images and recordings, species modeling, and in silico bioprospecting, among others. From the experiences described above, we highlight limitations, challenges, and opportunities for in order to successfully implementate AI in conservation efforts and sustainable management of biological resources in the Neotropics. The result aims to be a guide for researchers, decision makers, and biodiversity managers, facilitating the understanding of how artificial intelligence can be effectively integrated into conservation and sustainable use strategies. Furthermore, it also seeks to open a space for dialogue on the development of policies that promote the responsible and ethical adoption of AI in local contexts, ensuring that its benefits are harnessed without compromising biodiversity or the cultural and ecosystemic values inherent in Colombia and the Neotropics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14543v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sebasti\'an Ca\~nas, Camila Parra-Guevara, Manuela Montoya-Castrill\'on, Julieta M Ram\'irez-Mej\'ia, Gabriel-Alejandro Perilla, Esteban Marentes, Nerieth Leuro, Jose Vladimir Sandoval-Sierra, Sindy Martinez-Callejas, Ang\'elica D\'iaz, Mario Murcia, Elkin A. Noguera-Urbano, Jose Manuel Ochoa-Quintero, Susana Rodr\'iguez Buritic\'a, Juan Sebasti\'an Ulloa</dc:creator>
    </item>
    <item>
      <title>3+ Seat Risk-Limiting Audits for Single Transferable Vote Elections</title>
      <link>https://arxiv.org/abs/2503.14803</link>
      <description>arXiv:2503.14803v1 Announce Type: new 
Abstract: Constructing efficient risk-limiting audits (RLAs) for multiwinner single transferable vote (STV) elections is a challenging problem. An STV RLA is designed to statistically verify that the reported winners of an election did indeed win according to the voters' expressed preferences and not due to mistabulation or interference, while limiting the risk of accepting an incorrect outcome to a desired threshold (the risk limit). Existing methods have shown that it is possible to form RLAs for two-seat STV elections in the context where the first seat has been awarded to a candidate in the first round of tabulation. This is called the first winner criterion. We present an assertion-based approach to conducting full or partial RLAs for STV elections with three or more seats, in which the first winner criterion is satisfied. Although the chance of forming a full audit that verifies all winners drops substantially as the number of seats increases, we show that we can quite often form partial audits that verify most, and sometimes all, of the reported winners. We evaluate our method on a dataset of over 500 three- and four-seat STV elections from the 2017 and 2022 local council elections in Scotland.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14803v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.GT</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michelle Blom, Alexander Ek, Peter J. Stuckey, Vanessa Teague, Damjan Vukcevic</dc:creator>
    </item>
    <item>
      <title>Foundation models may exhibit staged progression in novel CBRN threat disclosure</title>
      <link>https://arxiv.org/abs/2503.15182</link>
      <description>arXiv:2503.15182v1 Announce Type: new 
Abstract: The extent to which foundation models can disclose novel chemical, biological, radiation, and nuclear (CBRN) threats to expert users is unclear due to a lack of test cases. I leveraged the unique opportunity presented by an upcoming publication describing a novel catastrophic biothreat - "Technical Report on Mirror Bacteria: Feasibility and Risks" - to conduct a small controlled study before it became public. Graduate-trained biologists tasked with predicting the consequences of releasing mirror E. coli showed no significant differences in rubric-graded accuracy using Claude Sonnet 3.5 new (n=10) or web search only (n=2); both groups scored comparably to a web baseline (28 and 43 versus 36). However, Sonnet reasoned correctly when prompted by a report author, but a smaller model, Haiku 3.5, failed even with author guidance (80 versus 5). These results suggest distinct stages of model capability: Haiku is unable to reason about mirror life even with threat-aware expert guidance (Stage 1), while Sonnet correctly reasons only with threat-aware prompting (Stage 2). Continued advances may allow future models to disclose novel CBRN threats to naive experts (Stage 3) or unskilled users (Stage 4). While mirror life represents only one case study, monitoring new models' ability to reason about privately known threats may allow protective measures to be implemented before widespread disclosure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15182v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>q-bio.OT</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin M Esvelt</dc:creator>
    </item>
    <item>
      <title>A Peek Behind the Curtain: Using Step-Around Prompt Engineering to Identify Bias and Misinformation in GenAI Models</title>
      <link>https://arxiv.org/abs/2503.15205</link>
      <description>arXiv:2503.15205v1 Announce Type: new 
Abstract: This research examines the emerging technique of step-around prompt engineering in GenAI research, a method that deliberately bypasses AI safety measures to expose underlying biases and vulnerabilities in GenAI models. We discuss how Internet-sourced training data introduces unintended biases and misinformation into AI systems, which can be revealed through the careful application of step-around techniques.
  Drawing parallels with red teaming in cybersecurity, we argue that step-around prompting serves a vital role in identifying and addressing potential vulnerabilities while acknowledging its dual nature as both a research tool and a potential security threat. Our findings highlight three key implications: (1) the persistence of Internet-derived biases in AI training data despite content filtering, (2) the effectiveness of step-around techniques in exposing these biases when used responsibly, and (3) the need for robust safeguards against malicious applications of these methods.
  We conclude by proposing an ethical framework for using step-around prompting in AI research and development, emphasizing the importance of balancing system improvements with security considerations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15205v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Don Hickerson (British University Vietnam), Mike Perkins (British University Vietnam)</dc:creator>
    </item>
    <item>
      <title>Mapping AI Avant-Gardes in Time: Posthumanism, Transhumanism, Genhumanism</title>
      <link>https://arxiv.org/abs/2503.15364</link>
      <description>arXiv:2503.15364v1 Announce Type: new 
Abstract: Three directions for the AI avant-garde are sketched against the background of time. Posthumanism changes what we are, and belongs to the radical future. Transhumanism changes how we are, and corresponds with the radical past. Genhumanism changes who we are, and exists in the radical present. While developing the concepts, this essay intersects in two ways with theoretical debates about humanism in the face of technological advance. First, it describes how temporal divisions may cleanly differentiate post- and transhumanism. Second, the essay introduces generative humanism, which contributes to discussions about AI and society by delineating a novel humanistic response to contemporary technology. Finally, grounds are provided for a practical project, one where philosophers work with AI engineers in the area of genhumanism. Contemporary AI research into serendipity in recommendation engines provides natural support for the shared research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15364v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s44163-023-00080-6</arxiv:DOI>
      <dc:creator>James Brusseau</dc:creator>
    </item>
    <item>
      <title>Assessing AI vs Human-Authored Spear Phishing SMS Attacks: An Empirical Study</title>
      <link>https://arxiv.org/abs/2406.13049</link>
      <description>arXiv:2406.13049v2 Announce Type: replace 
Abstract: This paper explores the use of Large Language Models (LLMs) in spear phishing message generation and evaluates their performance compared to human-authored counterparts. Our pilot study examines the effectiveness of smishing (SMS phishing) messages created by GPT-4 and human authors, which have been personalized for willing targets. The targets assessed these messages in a modified ranked-order experiment using a novel methodology we call TRAPD (Threshold Ranking Approach for Personalized Deception). Experiments involved ranking each spear phishing message from most to least convincing, providing qualitative feedback, and guessing which messages were human- or AI-generated. Results show that LLM-generated messages are often perceived as more convincing than those authored by humans, particularly job-related messages. Targets also struggled to distinguish between human- and AI-generated messages. We analyze different criteria the targets used to assess the persuasiveness and source of messages. This study aims to highlight the urgent need for further research and improved countermeasures against personalized AI-enabled social engineering attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13049v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jerson Francia, Derek Hansen, Ben Schooley, Matthew Taylor, Shydra Murray, Greg Snow</dc:creator>
    </item>
    <item>
      <title>On the Need and Applicability of Causality for Fairness: A Unified Framework for AI Auditing and Legal Analysis</title>
      <link>https://arxiv.org/abs/2207.04053</link>
      <description>arXiv:2207.04053v4 Announce Type: replace-cross 
Abstract: As Artificial Intelligence (AI) increasingly influences decisions in critical societal sectors, understanding and establishing causality becomes essential for evaluating the fairness of automated systems. This article explores the significance of causal reasoning in addressing algorithmic discrimination, emphasizing both legal and societal perspectives. By reviewing landmark cases and regulatory frameworks, particularly within the European Union, we illustrate the challenges inherent in proving causal claims when confronted with opaque AI decision-making processes. The discussion outlines practical obstacles and methodological limitations in applying causal inference to real-world fairness scenarios, proposing actionable solutions to enhance transparency, accountability, and fairness in algorithm-driven decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.04053v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruta Binkyte, Ljupcho Grozdanovski, Sami Zhioua</dc:creator>
    </item>
    <item>
      <title>Towards the Pedagogical Steering of Large Language Models for Tutoring: A Case Study with Modeling Productive Failure</title>
      <link>https://arxiv.org/abs/2410.03781</link>
      <description>arXiv:2410.03781v2 Announce Type: replace-cross 
Abstract: One-to-one tutoring is one of the most efficient methods of teaching. With the growing popularity of Large Language Models (LLMs), there have been efforts to create LLM based conversational tutors which can expand the benefits of one to one tutoring to everyone. However, current LLMs are trained primarily to be helpful assistants and lack crucial pedagogical skills. For example, they often quickly reveal the solution to the student and fail to plan for a richer multi turn pedagogical interaction. To use LLMs in pedagogical settings, they need to be steered to use effective teaching strategies: a problem we introduce as Pedagogical Steering. We develop StratL, an algorithm to optimize LLM prompts and steer it to follow a predefined multi-turn tutoring plan represented as a transition graph. As a case study, we create a prototype tutor for high school math following Productive Failure (PF), an advanced and effective learning design. To validate our approach in a real-world setting, we run a field study with 17 high school students in Singapore and show that StratL succeeds in steering the LLM to follow the PF tutoring strategy. Finally, we highlight challenges in Pedagogical Steering of LLMs and offer opportunities for further improvements by publishing a dataset of PF problems and our code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03781v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Romain Puech, Jakub Macina, Julia Chatain, Mrinmaya Sachan, Manu Kapur</dc:creator>
    </item>
    <item>
      <title>A Guide to Misinformation Detection Data and Evaluation</title>
      <link>https://arxiv.org/abs/2411.05060</link>
      <description>arXiv:2411.05060v2 Announce Type: replace-cross 
Abstract: Misinformation is a complex societal issue, and mitigating solutions are difficult to create due to data deficiencies. To address this, we have curated the largest collection of (mis)information datasets in the literature, totaling 75. From these, we evaluated the quality of 36 datasets that consist of statements or claims, as well as the 9 datasets that consist of data in purely paragraph form. We assess these datasets to identify those with solid foundations for empirical work and those with flaws that could result in misleading and non-generalizable results, such as spurious correlations, or examples that are ambiguous or otherwise impossible to assess for veracity. We find the latter issue is particularly severe and affects most datasets in the literature. We further provide state-of-the-art baselines on all these datasets, but show that regardless of label quality, categorical labels may no longer give an accurate evaluation of detection model performance. Finally, we we propose and highlight Evaluation Quality Assessment (EQA) as a tool to guide the field toward systemic solutions rather than inadvertently propagating issues in evaluation. Overall, this guide aims to provide a roadmap for higher quality data and better grounded evaluations, ultimately improving research in misinformation detection. All datasets and other artifacts are available at misinfo-datasets.complexdatalab.com.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05060v2</guid>
      <category>cs.SI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Camille Thibault, Jacob-Junqi Tian, Gabrielle Peloquin-Skulski, Taylor Lynn Curtis, James Zhou, Florence Laflamme, Yuxiang Guan, Reihaneh Rabbany, Jean-Fran\c{c}ois Godbout, Kellin Pelrine</dc:creator>
    </item>
  </channel>
</rss>
