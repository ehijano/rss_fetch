<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Jun 2025 04:00:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Non-null Shrinkage Regression and Subset Selection via the Fractional Ridge Regression</title>
      <link>https://arxiv.org/abs/2505.23925</link>
      <description>arXiv:2505.23925v1 Announce Type: new 
Abstract: $\ell_p$-norm penalization, notably the Lasso, has become a standard technique, extending shrinkage regression to subset selection. Despite aiming for oracle properties and consistent estimation, existing Lasso-derived methods still rely on shrinkage toward a null model, necessitating careful tuning parameter selection and yielding monotone variable selection. This research introduces Fractional Ridge Regression, a novel generalization of the Lasso penalty that penalizes only a fraction of the coefficients. Critically, Fridge shrinks the model toward a non-null model of a prespecified target size, even under extreme regularization. By selectively penalizing coefficients associated with less important variables, Fridge aims to reduce bias, improve performance relative to the Lasso, and offer more intuitive model interpretation while retaining certain advantages of best subset selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23925v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sihyung Park (North Carolina State University), Leonard A. Stefanski (North Carolina State University)</dc:creator>
    </item>
    <item>
      <title>A2 Copula-Driven Spatial Bayesian Neural Network For Modeling Non-Gaussian Dependence: A Simulation Study</title>
      <link>https://arxiv.org/abs/2505.24006</link>
      <description>arXiv:2505.24006v1 Announce Type: new 
Abstract: In this paper, we introduce the A2 Copula Spatial Bayesian Neural Network (A2-SBNN), a predictive spatial model designed to map coordinates to continuous fields while capturing both typical spatial patterns and extreme dependencies. By embedding the dual-tail novel Archimedean copula viz. A2 directly into the network's weight initialization, A2-SBNN naturally models complex spatial relationships, including rare co-movements in the data. The model is trained through a calibration-driven process combining Wasserstein loss, moment matching, and correlation penalties to refine predictions and manage uncertainty. Simulation results show that A2-SBNN consistently delivers high accuracy across a wide range of dependency strengths, offering a new, effective solution for spatial data modeling beyond traditional Gaussian-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24006v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agnideep Aich, Sameera Hewage, Md Monzur Murshed, Ashit Baran Aich, Amanda Mayeaux, Asim K. Dey, Kumer P. Das, Bruce Wade</dc:creator>
    </item>
    <item>
      <title>Estimating dynamic transmission rates with a Black-Karasinski process in stochastic SIHR models using particle MCMC</title>
      <link>https://arxiv.org/abs/2505.24127</link>
      <description>arXiv:2505.24127v1 Announce Type: new 
Abstract: Compartmental models are effective in modeling the spread of infectious pathogens, but have remaining weaknesses in fitting to real datasets exhibiting stochastic effects. We propose a stochastic SIHR model with a dynamic transmission rate, where the rate is modeled by the Black-Karasinski (BK) process - a mean-reverting stochastic process with a stable equilibrium distribution, making it well-suited for modeling long-term epidemic dynamics. To generate sample paths of the BK process and estimate static parameters of the system, we employ particle Markov Chain Monte Carlo (pMCMC) methods due to their effectiveness in handling complex state-space models and jointly estimating parameters. We designed experiments on synthetic data to assess estimation accuracy and its impact on inferred transmission rates; all BK-process parameters were estimated accurately except the mean-reverting rate. We also assess the sensitivity of pMCMC to misspecification of the mean-reversion rate. Our results show that estimation accuracy remains stable across different mean-reversion rates, though smaller values increase error variance and complicate inference results. Finally, we apply our model to Arizona flu hospitalization data, finding that parameter estimates are consistent with published survey data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24127v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avery Drennan, Jeffrey Covington, Dan Han, Andrew Attilio, Jaechoul Lee, Richard Posner, Eck Doerry, Joseph Mihaljevic, Ye Chen</dc:creator>
    </item>
    <item>
      <title>Partially-shared Imaging Regression on Integrating Heterogeneous Brain-Cognition Associations across Alzheimer's Diagnoses</title>
      <link>https://arxiv.org/abs/2505.24259</link>
      <description>arXiv:2505.24259v1 Announce Type: new 
Abstract: This paper is motivated by the heterogeneous associations among demographic covariates, imaging data, and cognitive performances across different diagnostic groups within the Alzheimer's Disease Neuroimaging Initiative (ADNI) study. We propose a novel PArtially-shared Imaging Regression (PAIR) model with smooth spatial component integration to capture heterogeneous imaging coefficients across multiple data sources. The model assumes that each imaging coefficient can be represented as a weighted combination of a set of smooth spatial components. Additionally, we apply a Total Variation (TV) penalty on each component to capture complex spatial patterns and introduce a Selective Integration Penalty (SIP) to adaptively learn the degree of partial-sharing among imaging coefficients. Applied to ADNI data, PAIR significantly improves predictive performance and uncovers distinct heterogeneous relationships. After adjusting for demographic covariates, hippocampal imaging minimally contributes to cognitive scores in the cognitively normal (CN) group but substantially in the cognitively impaired (CI) group. Furthermore, the effects of demographic covariates on cognitive scores remain stable among CN participants yet change notably for CI participants after imaging adjustment, suggesting hippocampal structural modulation. Imaging coefficient analysis reveals weak hippocampal signals in CN subjects, whereas prominent positive signals in CA1, CA3, and presubiculum subfields characterize the CI group. These analyses facilitate further investigation into functional mechanisms underlying Alzheimer's disease (AD) progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24259v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Sui, Qi Xu, Ting Li, Yang Bai, Annie Qu</dc:creator>
    </item>
    <item>
      <title>Data Fusion for Partial Identification of Causal Effects</title>
      <link>https://arxiv.org/abs/2505.24296</link>
      <description>arXiv:2505.24296v1 Announce Type: new 
Abstract: Data fusion techniques integrate information from heterogeneous data sources to improve learning, generalization, and decision making across data sciences. In causal inference, these methods leverage rich observational data to improve causal effect estimation, while maintaining the trustworthiness of randomized controlled trials. Existing approaches often relax the strong no unobserved confounding assumption by instead assuming exchangeability of counterfactual outcomes across data sources. However, when both assumptions simultaneously fail - a common scenario in practice - current methods cannot identify or estimate causal effects. We address this limitation by proposing a novel partial identification framework that enables researchers to answer key questions such as: Is the causal effect positive or negative? and How severe must assumption violations be to overturn this conclusion? Our approach introduces interpretable sensitivity parameters that quantify assumption violations and derives corresponding causal effect bounds. We develop doubly robust estimators for these bounds and operationalize breakdown frontier analysis to understand how causal conclusions change as assumption violations increase. We apply our framework to the Project STAR study, which investigates the effect of classroom size on students' third-grade standardized test performance. Our analysis reveals that the Project STAR results are robust to simultaneous violations of key assumptions, both on average and across various subgroups of interest. This strengthens confidence in the study's conclusions despite potential unmeasured biases in the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24296v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Quinn Lanners, Cynthia Rudin, Alexander Volfovsky, Harsh Parikh</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Spatially-Temporally Misaligned Data Using Predictive Stacking</title>
      <link>https://arxiv.org/abs/2505.24397</link>
      <description>arXiv:2505.24397v1 Announce Type: new 
Abstract: Air pollution remains a major environmental risk factor that is often associated with adverse health outcomes. However, quantifying and evaluating its effects on human health is challenging due to the complex nature of exposure data. Recent technological advances have led to the collection of various indicators of air pollution at increasingly high spatial-temporal resolutions (e.g., daily averages of pollutant levels at spatial locations referenced by latitude-longitude). However, health outcomes are typically aggregated over several spatial-temporal coordinates (e.g., annual prevalence for a county) to comply with survey regulations. This article develops a Bayesian hierarchical model to analyze such spatially-temporally misaligned exposure and health outcome data. We introduce Bayesian predictive stacking, which optimally combines multiple predictive spatial-temporal models and avoids iterative estimation algorithms such as Markov chain Monte Carlo that struggle due to convergence issues inflicted by the presence of weakly identified parameters. We apply our proposed method to study the effects of ozone on asthma in the state of California.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24397v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumyakanti Pan, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Joint space-time modelling for upper daily maximum and minimum temperature record-breaking</title>
      <link>https://arxiv.org/abs/2505.24436</link>
      <description>arXiv:2505.24436v1 Announce Type: new 
Abstract: Record-breaking temperature events are now frequently in the news, proffered as evidence of climate change, and often bring significant economic and human impacts. Our previous work undertook the first substantial spatial modelling investigation of temperature record-breaking across years for any given day within the year, employing a dataset consisting of over sixty years of daily maximum temperatures across peninsular Spain. That dataset also supplies daily minimum temperatures (which, in fact, are now available through 2023). Here, the dataset is converted into a daily pair of binary events, indicators, for that day, of whether a yearly record was broken for the daily maximum temperature and/or for the daily minimum temperature. Joint modelling addresses several inference issues: (i) defining/modelling record-breaking with bivariate time series of yearly indicators, (ii) strength of relationship between record-breaking events, (iii) prediction of joint, conditional and marginal record-breaking, (iv) persistence in record-breaking across days, (v) spatial interpolation across peninsular Spain. We substantially expand our previous work to enable investigation of these issues. We observe strong correlation between both processes but a growing trend of climate change that is well differentiated between them both spatially and temporally as well as different strengths of persistence and spatial dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24436v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge Castillo-Mateo (University of Zaragoza), Zeus Gracia-Tabuenca (University of Zaragoza), Jes\'us As\'in (University of Zaragoza), Ana C. Cebri\'an (University of Zaragoza), Alan E. Gelfand (Duke University)</dc:creator>
    </item>
    <item>
      <title>Inhomogeneous mark correlation functions for general marked point processes</title>
      <link>https://arxiv.org/abs/2505.24501</link>
      <description>arXiv:2505.24501v1 Announce Type: new 
Abstract: Spatial phenomena in environmental and biological contexts often involve events that are unevenly distributed across space and carry attributes, whose associations/variations are space-dependent. In this paper, we introduce the class of inhomogeneous mark correlation functions, capturing mark associations/variations, while explicitly accounting for the spatial inhomogeneity of events. The proposed functions are designed to quantify how, on average, marks vary or associate with one another as a function of pairwise spatial distances. We develop nonparametric estimators and evaluate their performance through simulation studies covering a range of scenarios with mark association or variation, spanning from nonstationary point patterns without spatial interaction to those characterised by clustering tendencies. Our simulations reveal the shortcomings of traditional methods in the presence of spatial inhomogeneity, underscoring the necessity of our approach. Furthermore, the results show that our estimators accurately identify both the positivity/negativity and effective spatial range for detected mark associations/variations. The proposed inhomogeneous mark correlation functions are then applied to two distinct forest ecosystems: Longleaf pine trees in southern Georgia, USA, marked by their diameter at breast height, and Scots pine trees in Pfynwald, Switzerland, marked by their height. Our findings reveal that the inhomogeneous mark correlation functions provide deeper and more detailed insights into tree growth patterns compared to traditional methods</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24501v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mehdi Moradi, Matthias Eckardt</dc:creator>
    </item>
    <item>
      <title>Density Ratio Permutation Tests with connections to distributional shifts and conditional two-sample testing</title>
      <link>https://arxiv.org/abs/2505.24529</link>
      <description>arXiv:2505.24529v1 Announce Type: new 
Abstract: We introduce novel hypothesis tests to allow for statistical inference for density ratios. More precisely, we introduce the Density Ratio Permutation Test (DRPT) for testing $H_0: g \propto r f$ based on independent data drawn from distributions with densities $f$ and $g$, where the hypothesised density ratio $r$ is a fixed function. The proposed test employs an efficient Markov Chain Monte Carlo algorithm to draw permutations of the combined dataset according to a distribution determined by $r$, producing exchangeable versions of the whole sample and thereby establishing finite-sample validity. Regarding the test's behaviour under the alternative hypothesis, we begin by demonstrating that if the test statistic is chosen as an Integral Probability Metric (IPM), the DRPT is consistent under mild assumptions on the function class that defines the IPM. We then narrow our focus to the setting where the function class is a Reproducing Kernel Hilbert Space, and introduce a generalisation of the classical Maximum Mean Discrepancy (MMD), which we term Shifted-MMD. For continuous data, assuming that a normalised version of $g - rf$ lies in a Sobolev ball, we establish the minimax optimality of the DRPT based on the Shifted-MMD. We further extend our approach to scenarios with an unknown shift factor $r$, estimating it from part of the data using Density Ratio Estimation techniques, and derive Type-I error bounds based on estimation error. Additionally, we demonstrate how the DRPT can be adapted for conditional two-sample testing, establishing it as a versatile tool for assessing modelling assumptions on importance weights, covariate shifts and related scenarios, which frequently arise in contexts such as transfer learning and causal inference. Finally, we validate our theoretical findings through experiments on both simulated and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24529v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Bordino, Thomas B. Berrett</dc:creator>
    </item>
    <item>
      <title>Two-stage MCMC for Fast Bayesian Inference of Large Spatio-temporal Ordinal Data, with Application to US Drought</title>
      <link>https://arxiv.org/abs/2505.24594</link>
      <description>arXiv:2505.24594v1 Announce Type: new 
Abstract: High dimensional space-time data pose known computational challenges when fitting spatio-temporal models. Such data show dependence across several dimensions of space as well as in time, and can easily involve hundreds of thousands of observations. Many spatio-temporal models result in a dependence structure across all observations and can be fit only at a substantial computational cost, arising from dense matrix inversion, high dimensional parameter spaces, poor mixing in Markov Chain Monte Carlo, or the impossibility of utilizing parallel computing due to a lack of independence anywhere in the model fitting process. These computational challenges are exacerbated when the response variable is ordinal, and especially as the number of ordered categories grows. Some spatio-temporal models achieve computational feasibility for large datasets but only through overly restrictive model simplifications, which we seek to avoid here. In this paper we demonstrate a two-stage algorithm to fit a Bayesian spatio-temporal model to large datasets when the response variable is ordinal. The first stage models locations independently in space, capturing temporal dependence, and can be run in parallel. The second stage resamples from the first stage posterior distributions with an acceptance probability computed to impose spatial dependence from the full spatio-temporal model. The result is fast Bayesian inference which samples from the full spatio-temporal posterior and is computationally feasible even for large datasets. We quantify the substantial computational gains our approach achieves, and demonstrate the preservation of the posterior distribution as compared to the more costly single-stage model fit. We apply our approach to a large spatio-temporal drought dataset in the United States, a dataset too large for many existing spatio-temporal methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24594v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Staci Hepler, Rob Erhardt</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric clustering for spatio-temporal data, with an application to air pollution</title>
      <link>https://arxiv.org/abs/2505.24694</link>
      <description>arXiv:2505.24694v1 Announce Type: new 
Abstract: Air pollution is a major global health hazard, with fine particulate matter (PM10) linked to severe respiratory and cardiovascular diseases. Hence, analyzing and clustering spatio-temporal air quality data is crucial for understanding pollution dynamics and guiding policy interventions. This work provides a review of Bayesian nonparametric clustering methods, with a particular focus on their application to spatio-temporal data, which are ubiquitous in environmental sciences. We first introduce key modeling approaches for point-referenced spatio-temporal data, highlighting their flexibility in capturing complex spatial and temporal dependencies. We then review recent advancements in Bayesian clustering, focusing on spatial product partition models, which incorporate spatial structure into the clustering process. We illustrate the proposed methods on PM10 monitoring data from Northern Italy, demonstrating their ability to identify meaningful pollution patterns. This review highlights the potential of Bayesian nonparametric methods for environmental risk assessment and offers insights into future research directions in spatio-temporal clustering for public health and environmental science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24694v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luca Aiello, Raffaele Argiento, Sirio Legramanti, Lucia Paci</dc:creator>
    </item>
    <item>
      <title>A studentized permutation test for the treatment effect in individual participant data meta-analysis</title>
      <link>https://arxiv.org/abs/2505.24774</link>
      <description>arXiv:2505.24774v1 Announce Type: new 
Abstract: Meta-analysis is a well-established tool used to combine data from several independent studies, each of which usually compares the effect of an experimental treatment with a control group. While meta-analyses are often performed using aggregated study summaries, they may also be conducted using individual participant data (IPD). Classical meta-analysis models may be generalized to handle continuous IPD by formulating them within a linear mixed model framework. IPD meta-analyses are commonly based on a small number of studies. Technically, inference for the overall treatment effect can be performed using Student-t approximation. However, as some approaches may not adequately control the type I error, Satterthwaite's or Kenward-Roger's method have been suggested to set the degrees-of-freedom parameter. The latter also adjusts the standard error of the treatment effect estimator. Nevertheless, these methods may be conservative. Since permutation tests are known to control the type I error and offer robustness to violations of distributional assumptions, we propose a studentized permutation test for the treatment effect based on permutations of standardized residuals across studies in IPD meta-analysis. Also, we construct confidence intervals for the treatment effect based on this test. The first interval is derived from the percentiles of the permutation distribution. The second interval is obtained by searching values closest to the effect estimate that are just significantly different from the true effect. In a simulation study, we demonstrate satisfactory performance of the proposed methods, often producing shorter confidence intervals compared with competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24774v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phuc Thien Tran, Long-Hao Xu, Christian R\"over, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Paired comparison models with strength-dependent ties and order effects</title>
      <link>https://arxiv.org/abs/2505.24783</link>
      <description>arXiv:2505.24783v1 Announce Type: new 
Abstract: Paired comparison models, such as the Bradley-Terry (1952) model and its variants, are commonly used to measure competitor strength in games and sports. Extensions have been proposed to account for order effects (e.g., home-field advantage) as well as the possibility of a tie as a separate outcome, but such models are rarely adopted in practice due to poor fit with actual data. We propose a novel paired comparison model that accounts not only for ties and order effects, but recognizes two phenomena that are not addressed with commonly used models. First, the probability of a tie may be greater for stronger pairs of competitors. Second, order effects may be more pronounced for stronger competitors. This model is motivated in the context of tournament chess game outcomes. The models are demonstrated on the results of US Chess Open game outcomes from 2006 to 2019, large tournaments consisting of players of wide-ranging strengths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24783v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark E. Glickman</dc:creator>
    </item>
    <item>
      <title>Adaptive finite element type decomposition of Gaussian processes</title>
      <link>https://arxiv.org/abs/2505.24066</link>
      <description>arXiv:2505.24066v1 Announce Type: cross 
Abstract: In this paper, we investigate a class of approximate Gaussian processes (GP) obtained by taking a linear combination of compactly supported basis functions with the basis coefficients endowed with a dependent Gaussian prior distribution. This general class includes a popular approach that uses a finite element approximation of the stochastic partial differential equation (SPDE) associated with Mat\'ern GP. We explored another scalable alternative popularly used in the computer emulation literature where the basis coefficients at a lattice are drawn from a Gaussian process with an inverse-Gamma bandwidth. For both approaches, we study concentration rates of the posterior distribution. We demonstrated that the SPDE associated approach with a fixed smoothness parameter leads to a suboptimal rate despite how the number of basis functions and bandwidth are chosen when the underlying true function is sufficiently smooth. On the flip side, we showed that the later approach is rate-optimal adaptively over all smoothness levels of the underlying true function if an appropriate prior is placed on the number of basis functions. Efficient computational strategies are developed and numerics are provided to illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24066v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaehoan Kim, Anirban Bhattacharya, Debdeep Pati</dc:creator>
    </item>
    <item>
      <title>Multi-task Learning for Heterogeneous Data via Integrating Shared and Task-Specific Encodings</title>
      <link>https://arxiv.org/abs/2505.24281</link>
      <description>arXiv:2505.24281v1 Announce Type: cross 
Abstract: Multi-task learning (MTL) has become an essential machine learning tool for addressing multiple learning tasks simultaneously and has been effectively applied across fields such as healthcare, marketing, and biomedical research. However, to enable efficient information sharing across tasks, it is crucial to leverage both shared and heterogeneous information. Despite extensive research on MTL, various forms of heterogeneity, including distribution and posterior heterogeneity, present significant challenges. Existing methods often fail to address these forms of heterogeneity within a unified framework. In this paper, we propose a dual-encoder framework to construct a heterogeneous latent factor space for each task, incorporating a task-shared encoder to capture common information across tasks and a task-specific encoder to preserve unique task characteristics. Additionally, we explore the intrinsic similarity structure of the coefficients corresponding to learned latent factors, allowing for adaptive integration across tasks to manage posterior heterogeneity. We introduce a unified algorithm that alternately learns the task-specific and task-shared encoders and coefficients. In theory, we investigate the excess risk bound for the proposed MTL method using local Rademacher complexity and apply it to a new but related task. Through simulation studies, we demonstrate that the proposed method outperforms existing data integration methods across various settings. Furthermore, the proposed method achieves superior predictive performance for time to tumor doubling across five distinct cancer types in PDX data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24281v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Sui, Qi Xu, Yang Bai, Annie Qu</dc:creator>
    </item>
    <item>
      <title>A Time-Scaled ETAS Model for Earthquake Forecasting</title>
      <link>https://arxiv.org/abs/2505.24412</link>
      <description>arXiv:2505.24412v1 Announce Type: cross 
Abstract: The Himalayan region, particularly Nepal, is highly susceptible to frequent and severe seismic activity, underscoring the urgent need for robust earthquake forecasting models. This study introduces a suite of time-scaled Epidemic-Type Aftershock Sequence (ETAS) models tailored for earthquake forecasting in Nepal, leveraging seismic data from 2000 to 2020. By incorporating alternative time-scaling approaches - such as calibration, proportional hazards, log-linear, and power time scales - the models capture nuanced temporal patterns of aftershocks, improving event classification between background and triggered occurrences. We evaluate model performance under various assumptions of earthquake magnitude distributions (exponential, gamma, and radially symmetric) and employ optimization techniques including the Davidon-Fletcher-Powell algorithm and Iterative Stochastic De-clustering. The results reveal that time-scaling significantly enhances model interpretability and predictive accuracy, with the ISDM-based ETAS model achieving the best fit. This work not only deepens the statistical understanding of earthquake dynamics in Nepal but also lays a foundation for implementing more effective early warning systems in seismically active regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24412v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agniva Das, Muralidharan K</dc:creator>
    </item>
    <item>
      <title>Locally Differentially Private Two-Sample Testing</title>
      <link>https://arxiv.org/abs/2505.24811</link>
      <description>arXiv:2505.24811v1 Announce Type: cross 
Abstract: We consider the problem of two-sample testing under a local differential privacy constraint where a permutation procedure is used to calibrate the tests. We develop testing procedures which are optimal up to logarithmic factors, for general discrete distributions and continuous distributions subject to a smoothness constraint. Both non-interactive and interactive tests are considered, and we show allowing interactivity results in an improvement in the minimax separation rates. Our results show that permutation procedures remain feasible in practice under local privacy constraints, despite the inability to permute the non-private data directly and only the private views. Further, through a refined theoretical analysis of the permutation procedure, we are able to avoid an equal sample size assumption which has been made in the permutation testing literature regardless of the presence of the privacy constraint. Lastly, we conduct numerical experiments which demonstrate the performance of our proposed test and verify the theoretical findings, especially the improved performance enabled by allowing interactivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24811v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Kent, Thomas B. Berrett, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes Selection for Value Maximization</title>
      <link>https://arxiv.org/abs/2210.03905</link>
      <description>arXiv:2210.03905v3 Announce Type: replace 
Abstract: We study the problem of selecting the best $m$ units from a set of $n$ as $m / n \to \alpha \in (0, 1)$, where noisy, heteroskedastic measurements of the units' true values are available and the decision-maker wishes to maximize the aggregate true value of the units selected. Given a parametric prior distribution, the empirical Bayes decision rule incurs $O_p(n^{-1})$ regret relative to the Bayesian oracle that knows the true prior. More generally, if the error in the estimated prior is of order $O_p(r_n)$, regret is $O_p(r_n^2)$. In this sense \emph{selection} of the best units is fundamentally easier than \emph{estimation} of their values. We show this regret bound is sharp in the parametric case, by giving an example in which it is attained. Using priors calibrated from a dataset of over four thousand internet experiments, we confirm that empirical Bayes methods perform well in detecting the best treatments with only a modest number of experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.03905v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3711896.3736928</arxiv:DOI>
      <dc:creator>Dominic Coey, Kenneth Hung</dc:creator>
    </item>
    <item>
      <title>Propensity weighting plus adjustment in proportional hazards model is not doubly robust</title>
      <link>https://arxiv.org/abs/2310.16207</link>
      <description>arXiv:2310.16207v3 Announce Type: replace 
Abstract: Recently, it has become common for applied works to combine commonly used survival analysis modeling methods, such as the multivariable Cox model and propensity score weighting, with the intention of forming a doubly robust estimator of an exposure effect hazard ratio that is unbiased in large samples when either the Cox model or the propensity score model is correctly specified. This combination does not, in general, produce a doubly robust estimator, even after regression standardization, when there is truly a causal effect. We demonstrate via simulation this lack of double robustness for the semiparametric Cox model, the Weibull proportional hazards model, and a simple proportional hazards flexible parametric model, with both the latter models fit via maximum likelihood. We provide a novel proof that the combination of propensity score weighting and a proportional hazards survival model, fit either via full or partial likelihood, is consistent under the null of no causal effect of the exposure on the outcome under particular censoring mechanisms if either the propensity score or the outcome model is correctly specified and contains all confounders. Given our results suggesting that double robustness only exists under the null, we outline two simple alternative estimators that are doubly robust for the survival difference at a given time point (in the above sense), provided the censoring mechanism can be correctly modeled, and one doubly robust method of estimation for the full survival curve. We provide R code to use these estimators for estimation and inference in the supporting information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16207v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erin E Gabriel, Michael C Sachs, Ingeborg Waernbaum, Els Goetghebeur, Paul F Blanche, Stijn Vansteelandt, Arvid Sj\"olander, Thomas Scheike</dc:creator>
    </item>
    <item>
      <title>Perturbation-based Effect Measures for Compositional Data</title>
      <link>https://arxiv.org/abs/2311.18501</link>
      <description>arXiv:2311.18501v5 Announce Type: replace 
Abstract: Existing effect measures for compositional features are inadequate for many modern applications, for example, in microbiome research, since they display traits such as high-dimensionality and sparsity that can be poorly modelled with traditional parametric approaches. Further, assessing -- in an unbiased way -- how summary statistics of a composition (e.g., racial diversity) affect a response variable is not straightforward. We propose a framework based on hypothetical data perturbations which defines interpretable statistical functionals on the compositions themselves, which we call average perturbation effects. These effects naturally account for confounding that biases frequently used marginal dependence analyses. We show how average perturbation effects can be estimated efficiently by deriving a perturbation-dependent reparametrization and applying semiparametric estimation techniques. We analyze the proposed estimators empirically on simulated and semi-synthetic data and demonstrate advantages over existing techniques on data from New York schools and microbiome data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18501v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton Rask Lundborg, Niklas Pfister</dc:creator>
    </item>
    <item>
      <title>Least squares for cardinal paired comparisons data</title>
      <link>https://arxiv.org/abs/2401.07018</link>
      <description>arXiv:2401.07018v3 Announce Type: replace 
Abstract: Least square estimators for graphical models for cardinal paired comparison data with and without covariates are rigorously analyzed. Novel, graph--based, necessary and sufficient conditions that guarantee strong consistency, asymptotic normality and the exponential convergence of the estimated ranks are emphasized. A complete theory for models with covariates is laid out. In particular, conditions under which covariates can be safely omitted from the model are provided. The methodology is employed in the analysis of both finite and infinite sets of ranked items where the case of large sparse comparison graphs is addressed. The proposed methods are explored by simulation and applied to the ranking of teams in the National Basketball Association (NBA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07018v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rahul Singh, George Iliopoulos, Ori Davidov</dc:creator>
    </item>
    <item>
      <title>Fast and flexible inference for spatial extremes</title>
      <link>https://arxiv.org/abs/2407.13958</link>
      <description>arXiv:2407.13958v5 Announce Type: replace 
Abstract: Statistical modelling of spatial extreme events has gained increasing attention over the last few decades with max-stable processes, and more recently $r$-Pareto processes, becoming the reference tools for the statistical analysis of asymptotically dependent data. Although inference for r-Pareto processes is easier than for max-stable processes, there remain major hurdles for their application to high dimensional datasets within a reasonable timeframe. In addition, both approaches have almost exclusively focused on the Brown-Resnick model, for its Gaussian foundations, and for the continuity of its exponent measure. In this paper, we derive a class of models for which this continuity property holds and present the skewed Brown-Resnick model, an extension of the Brown-Resnick that allows for non-stationarity in the dependence structure, and the truncated extremal-t model, a refinement of the well-known extremal-$t$ model. We use an inference methodology based on the intensity function of the process which is derived from the exponent measure, and demonstrate the statistical and computational efficiency of this approach. Applications to two real-world problems illustrate valuable gains in modelling flexibility as well as appealing computational gains over reference methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13958v5</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peng Zhong, Scott A. Sisson, Boris Beranger</dc:creator>
    </item>
    <item>
      <title>A Dirichlet stochastic block model for composition-weighted networks</title>
      <link>https://arxiv.org/abs/2408.00651</link>
      <description>arXiv:2408.00651v2 Announce Type: replace 
Abstract: Network data are observed in various applications where the individual entities of the system interact with or are connected to each other, and often these interactions are defined by their associated strength or importance. Clustering is a common task in network analysis that involves finding groups of nodes displaying similarities in the way they interact with the rest of the network. However, most clustering methods use the strengths of connections between entities in their original form, ignoring the possible differences in the capacities of individual nodes to send or receive edges. This often leads to clustering solutions that are heavily influenced by the nodes' capacities. One way to overcome this is to analyse the strengths of connections in relative rather than absolute terms, expressing each edge weight as a proportion of the sending (or receiving) capacity of the respective node. This, however, induces additional modelling constraints that most existing clustering methods are not designed to handle. In this work we propose a stochastic block model for composition-weighted networks based on direct modelling of compositional weight vectors using a Dirichlet mixture, with the parameters determined by the cluster labels of the sender and the receiver nodes. Inference is implemented via an extension of the classification expectation-maximisation algorithm that uses a working independence assumption, expressing the complete data likelihood of each node of the network as a function of fixed cluster labels of the remaining nodes. A model selection criterion is derived to aid the choice of the number of clusters. The model is validated using simulation studies, and showcased on network data from the Erasmus exchange program and a bike sharing network for the city of London.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00651v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iuliia Promskaia, Adrian O'Hagan, Michael Fop</dc:creator>
    </item>
    <item>
      <title>Network Weighted Functional Regression: a method for modeling dependencies between functional data in a network</title>
      <link>https://arxiv.org/abs/2501.18221</link>
      <description>arXiv:2501.18221v3 Announce Type: replace 
Abstract: In this paper, we propose a Network-Weighted Functional Regression (NWFR) model, an extension of Spatially Weighted Functional Regression (SWFR) to functional data defined on network-structured settings. To asses predictive uncertainity, we develop a functional conformal prediction procedure that yields a distribution free prediction intervals with guaranteed coverage. Through extensive evaluation on both simulated and real-world datasets, we demonstrate that the explicit modeling of network structure yields substantive improvements in point-prediction accuracy and markedly enhances the validity and precision of the resulting prediction intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18221v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elvira Romano, Antonio Irpino, Claire Miller</dc:creator>
    </item>
    <item>
      <title>Tightening Causal Bounds via Covariate-Aware Optimal Transport</title>
      <link>https://arxiv.org/abs/2502.01164</link>
      <description>arXiv:2502.01164v2 Announce Type: replace 
Abstract: Causal estimands can vary significantly depending on the relationship between outcomes in treatment and control groups, potentially leading to wide partial identification (PI) intervals that impede decision making. Incorporating covariates can substantially tighten these bounds, but requires determining the range of PI over probability models consistent with the joint distributions of observed covariates and outcomes in treatment and control groups. This problem is known to be equivalent to a conditional optimal transport (COT) optimization task, which is more challenging than standard optimal transport (OT) due to the additional conditioning constraints. In this work, we study a tight relaxation of COT that effectively reduces it to standard OT, leveraging its well-established computational and theoretical foundations. Our relaxation incorporates covariate information and ensures narrower PI intervals for any value of the penalty parameter, while becoming asymptotically exact as a penalty increases to infinity. This approach preserves the benefits of covariate adjustment in PI and results in a data-driven estimator for the PI set that is easy to implement using existing OT packages. We analyze the convergence rate of our estimator and demonstrate the effectiveness of our approach through extensive simulations, highlighting its practical use and superior performance compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01164v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sirui Lin, Zijun Gao, Jose Blanchet, Peter Glynn</dc:creator>
    </item>
    <item>
      <title>Graphical Models and Efficient Inference Methods for Multivariate Phase Probability Distributions</title>
      <link>https://arxiv.org/abs/2504.00459</link>
      <description>arXiv:2504.00459v2 Announce Type: replace 
Abstract: Multivariate phase relationships are important to characterize and understand numerous physical, biological, and chemical systems, from electromagnetic waves to neural oscillations. These systems exhibit complex spatiotemporal dynamics and intricate interdependencies among their constituent elements. While classical models of multivariate phase relationships, such as the wave equation and Kuramoto model, give theoretical models to describe phenomena, the development of statistical tools for hypothesis testing and inference for multivariate phase relationships in complex systems remains limited. This paper introduces a novel probabilistic modeling framework to characterize multivariate phase relationships, with wave-like phenomena serving as a key example. This approach describes spatial patterns and interactions between oscillators through a pairwise exponential family distribution. Building upon the literature of graphical model inference, including methods like Ising models, graphical lasso, and interaction screening, this work bridges the gap between classical wave dynamics and modern statistical approaches. Efficient inference methods are introduced, leveraging the Chow-Liu algorithm for directed tree approximations and interaction screening for general graphical models. Simulated experiments demonstrate the utility of these methods for uncovering wave properties and sparse interaction structures, highlighting their applicability to diverse scientific domains. This framework establishes a new paradigm for statistical modeling of multivariate phase relationships, providing a powerful toolset for exploring the complexity of these systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00459v2</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew S. Perley, Todd P. Coleman</dc:creator>
    </item>
    <item>
      <title>Addressing Misspecification in Simulation-based Inference through Data-driven Calibration</title>
      <link>https://arxiv.org/abs/2405.08719</link>
      <description>arXiv:2405.08719v2 Announce Type: replace-cross 
Abstract: Driven by steady progress in deep generative modeling, simulation-based inference (SBI) has emerged as the workhorse for inferring the parameters of stochastic simulators. However, recent work has demonstrated that model misspecification can compromise the reliability of SBI, preventing its adoption in important applications where only misspecified simulators are available. This work introduces robust posterior estimation~(RoPE), a framework that overcomes model misspecification with a small real-world calibration set of ground-truth parameter measurements. We formalize the misspecification gap as the solution of an optimal transport~(OT) problem between learned representations of real-world and simulated observations, allowing RoPE to learn a model of the misspecification without placing additional assumptions on its nature. RoPE demonstrates how OT and a calibration set provide a controllable balance between calibrated uncertainty and informative inference, even under severely misspecified simulators. Results on four synthetic tasks and two real-world problems with ground-truth labels demonstrate that RoPE outperforms baselines and consistently returns informative and calibrated credible intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08719v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Antoine Wehenkel, Juan L. Gamella, Ozan Sener, Jens Behrmann, Guillermo Sapiro, J\"orn-Henrik Jacobsen, Marco Cuturi</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning for Causal Discovery without Acyclicity Constraints</title>
      <link>https://arxiv.org/abs/2408.13448</link>
      <description>arXiv:2408.13448v4 Announce Type: replace-cross 
Abstract: Recently, reinforcement learning (RL) has proved a promising alternative for conventional local heuristics in score-based approaches to learning directed acyclic causal graphs (DAGs) from observational data. However, the intricate acyclicity constraint still challenges the efficient exploration of the vast space of DAGs in existing methods. In this study, we introduce ALIAS (reinforced dAg Learning wIthout Acyclicity conStraints), a novel approach to causal discovery powered by the RL machinery. Our method features an efficient policy for generating DAGs in just a single step with an optimal quadratic complexity, fueled by a novel parametrization of DAGs that directly translates a continuous space to the space of all DAGs, bypassing the need for explicitly enforcing acyclicity constraints. This approach enables us to navigate the search space more effectively by utilizing policy gradient methods and established scoring functions. In addition, we provide compelling empirical evidence for the strong performance of ALIAS in comparison with state-of-the-arts in causal discovery over increasingly difficult experiment conditions on both synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13448v4</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Transactions on Machine Learning Research (2025)</arxiv:journal_reference>
      <dc:creator>Bao Duong, Hung Le, Biwei Huang, Thin Nguyen</dc:creator>
    </item>
    <item>
      <title>Estimating Time Delays between Signals under Mixed Noise Influence with Novel Cross- and Bispectral Methods</title>
      <link>https://arxiv.org/abs/2502.17474</link>
      <description>arXiv:2502.17474v2 Announce Type: replace-cross 
Abstract: A common problem to signal processing are biases introduced by correlated noise. When quantifying time delays between two signals, mixed noise introduces a bias towards zero delay in conventional delay estimates based on the cross- or bispectrum. Here we propose two novel time delay estimators that address these shortcomings: (1) A cross-spectrum based approach that relies on estimating the periodicity of the phase spectrum rather than its slope, and (2) a bispectrum based approach, bispectral antisymmetrization, which removes contributions from not just Gaussian but all independent sources. In a simulation study, we compare conventional and novel TDE approaches and resolve differences in performance with respect to noise Gaussianity and auto-correlation structure. As a proof-of concept, we also perform TDE analysis on a neural stimulation dataset (n=3). We find that antisymmetrization consistently outperforms conventional bispectral methods at low signal-to-noise ratios (SNR) and prevents spurious zero-delay estimates in all mixed-noise environments. Time delay estimation based on phase periodicity also improves signal sensitivity compared to conventional cross-spectral methods. These observations are stable with respect to the magnitude of the delay and the statistical properties of the noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17474v2</guid>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tin Jurhar, Franziska Pellegrini, Ana I. Nu\~nes del Toro, Tilman Stephani, Guido Nolte, Stefan Haufe</dc:creator>
    </item>
    <item>
      <title>The Relativity of Causal Knowledge</title>
      <link>https://arxiv.org/abs/2503.11718</link>
      <description>arXiv:2503.11718v2 Announce Type: replace-cross 
Abstract: Recent advances in artificial intelligence reveal the limits of purely predictive systems and call for a shift toward causal and collaborative reasoning. Drawing inspiration from the revolution of Grothendieck in mathematics, we introduce the relativity of causal knowledge, which posits structural causal models (SCMs) are inherently imperfect, subjective representations embedded within networks of relationships. By leveraging category theory, we arrange SCMs into a functor category and show that their observational and interventional probability measures naturally form convex structures. This result allows us to encode non-intervened SCMs with convex spaces of probability measures. Next, using sheaf theory, we construct the network sheaf and cosheaf of causal knowledge. These structures enable the transfer of causal knowledge across the network while incorporating interventional consistency and the perspective of the subjects, ultimately leading to the formal, mathematical definition of relative causal knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11718v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.CT</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele D'Acunto, Claudio Battiloro</dc:creator>
    </item>
    <item>
      <title>M-learner:A Flexible And Powerful Framework To Study Heterogeneous Treatment Effect In Mediation Model</title>
      <link>https://arxiv.org/abs/2505.17917</link>
      <description>arXiv:2505.17917v2 Announce Type: replace-cross 
Abstract: We propose a novel method, termed the M-learner, for estimating heterogeneous indirect and total treatment effects and identifying relevant subgroups within a mediation framework. The procedure comprises four key steps. First, we compute individual-level conditional average indirect/total treatment effect Second, we construct a distance matrix based on pairwise differences. Third, we apply tSNE to project this matrix into a low-dimensional Euclidean space, followed by K-means clustering to identify subgroup structures. Finally, we calibrate and refine the clusters using a threshold-based procedure to determine the optimal configuration. To the best of our knowledge, this is the first approach specifically designed to capture treatment effect heterogeneity in the presence of mediation. Experimental results validate the robustness and effectiveness of the proposed framework. Application to the real-world Jobs II dataset highlights the broad adaptability and potential applicability of our method.Code is available at https: //anonymous.4open.science/r/M-learner-C4BB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17917v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingyu Li, Qing Liu, Tony Jiang, Hong Amy Xia, Brian P. Hobbs, Peng Wei</dc:creator>
    </item>
  </channel>
</rss>
