<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 19 Jan 2026 16:02:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Rigidity theory in statistical inference</title>
      <link>https://arxiv.org/abs/2601.10864</link>
      <description>arXiv:2601.10864v1 Announce Type: new 
Abstract: In this expository article, we summarize what is known about maximum likelihood thresholds of Gaussian models, paying special attention to connections with rigidity theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10864v1</guid>
      <category>math.ST</category>
      <category>math.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Irving Bernstein</dc:creator>
    </item>
    <item>
      <title>Noise-resilient penalty operators based on statistical differentiation schemes</title>
      <link>https://arxiv.org/abs/2601.11033</link>
      <description>arXiv:2601.11033v1 Announce Type: new 
Abstract: Penalized smoothing is a standard tool in regression analysis. Classical approaches often rely on basis or kernel expansions, which constrain the estimator to a fixed span and impose smoothness assumptions that may be restrictive for discretely observed data. We introduce a class of penalized estimators that operate directly on the data grid, denoising sampled trajectories under minimal smoothness assumptions by penalizing local roughness through statistically calibrated difference operators. Some distributional and asymptotic properties of sample-based contrast statistics associated with the resulting linear smoothers are established under Hellinger differentiability of the model, without requiring Fr\'echet differentiability in function space. Simulation results confirm that the proposed estimators perform competitively across both smooth and locally irregular settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11033v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marc Vidal, Yves Rosseel</dc:creator>
    </item>
    <item>
      <title>Optimal e-values for testing the mean of a bounded random variable against a composite alternative</title>
      <link>https://arxiv.org/abs/2601.11347</link>
      <description>arXiv:2601.11347v1 Announce Type: new 
Abstract: We derive the unique e-values with optimal (relative) growth rate in the worst case for testing the mean of a bounded random variable, hereby contributing with the first application beyond the assumption of mutually absolutely continuous hypotheses of the (RE)GROW quality criteria for e-values originally proposed by Gr\"unwald et al. (2024). For both criteria, we characterise explicitly the alternatives for which it is most difficult to test against, which also admit a meaningful interpretation. We give two important examples of interest where REGROW provides a powerful quality criterion to choose optimal e-variables whereas GROW leads to trivial solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11347v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Arnold, Eugenio Clerico</dc:creator>
    </item>
    <item>
      <title>Stein's method for the matrix normal distribution</title>
      <link>https://arxiv.org/abs/2601.11422</link>
      <description>arXiv:2601.11422v1 Announce Type: new 
Abstract: This work presents the first systematic development of Stein's method for matrix distributions. We establish the basic essential ingredients of Stein's method for matrix normal approximation: we derive a generator-based Stein identity from a matrix Ornstein--Uhlenbeck diffusion with two-sided scales, provide an explicit semigroup representation for the solution of the Stein equation, and obtain regularity estimates for the solution. The new methodology is illustrated with three statistical applications, these being smooth Wasserstein distance bounds to quantify the matrix central limit theorem, a Wasserstein distance bound for the matrix normal approximation of the centered matrix $T$ distribution, and the derivation of Stein's method-of-moments estimators for scale parameters of the matrix normal distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11422v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert E. Gaunt, Fr\'ed\'eric Ouimet, Donald Richards</dc:creator>
    </item>
    <item>
      <title>Optimal transport based theory for latent structured models</title>
      <link>https://arxiv.org/abs/2601.11465</link>
      <description>arXiv:2601.11465v1 Announce Type: new 
Abstract: This article is an exposition on some recent theoretical advances in learning latent structured models, with a primary focus on the fundamental roles that optimal transport distances play in the statistical theory. We aim at what may be the most critical and novel ingredient in this theory: the motivation, formulation, derivation and ramification of inverse bounds, a rich collection of structural inequalities for latent structured models which connect the space of distributions of unobserved structures of interest to the space of distributions for observed data. This theory is illustrated on classical mixture models, as well as the more modern hierarchical models that have been developed in Bayesian statistics, machine learning and related fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11465v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>XuanLong Nguyen, Yun Wei</dc:creator>
    </item>
    <item>
      <title>Robust $M$-Estimation of Scatter Matrices via Precision Structure Shrinkage</title>
      <link>https://arxiv.org/abs/2601.11099</link>
      <description>arXiv:2601.11099v1 Announce Type: cross 
Abstract: Maronna's and Tyler's $M$-estimators are among the most widely used robust estimators for scatter matrices. However, when the dimension of observations is relatively high, their performance can substantially deteriorate in certain situations, particularly in the presence of clustered outliers. To address this issue, we propose an estimator that shrinks the estimated precision matrix toward the identity matrix. We derive a sufficient condition for its existence, discuss its statistical interpretation, and establish upper and lower bounds for its breakdown point. Numerical experiments confirm robustness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11099v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soma Nikai, Yuichi Goto, Koji Tsukuda</dc:creator>
    </item>
    <item>
      <title>When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models</title>
      <link>https://arxiv.org/abs/2601.11444</link>
      <description>arXiv:2601.11444v1 Announce Type: cross 
Abstract: Diffusion models now generate high-quality, diverse samples, with an increasing focus on more powerful models. Although ensembling is a well-known way to improve supervised models, its application to unconditional score-based diffusion models remains largely unexplored. In this work we investigate whether it provides tangible benefits for generative modelling. We find that while ensembling the scores generally improves the score-matching loss and model likelihood, it fails to consistently enhance perceptual quality metrics such as FID on image datasets. We confirm this observation across a breadth of aggregation rules using Deep Ensembles, Monte Carlo Dropout, on CIFAR-10 and FFHQ. We attempt to explain this discrepancy by investigating possible explanations, such as the link between score estimation and image quality. We also look into tabular data through random forests, and find that one aggregation strategy outperforms the others. Finally, we provide theoretical insights into the summing of score models, which shed light not only on ensembling but also on several model composition techniques (e.g. guidance).</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11444v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rapha\"el Razafindralambo, R\'emy Sun, Fr\'ed\'eric Precioso, Damien Garreau, Pierre-Alexandre Mattei</dc:creator>
    </item>
    <item>
      <title>Smooth SCAD: A Raised Cosine SCAD Type Thresholding Rule for Wavelet Denoising</title>
      <link>https://arxiv.org/abs/2601.11461</link>
      <description>arXiv:2601.11461v1 Announce Type: cross 
Abstract: We introduce a smooth variant of the SCAD thresholding rule for wavelet denoising by replacing its piecewise linear transition with a raised cosine. The resulting shrinkage function is odd, continuous on R, and continuously differentiable away from the main threshold, yet retains the hallmark SCAD properties of sparsity for small coefficients and near unbiasedness for large ones. This smoothness places the rule within the continuous thresholding class for which Stein's unbiased risk estimate is valid. As a result, unbiased risk computation, stable data-driven threshold selection, and the asymptotic theory of Kudryavtsev and Shestakov apply.
  A corresponding nonconvex prior is obtained whose posterior mode coincides with the estimator, yielding a transparent Bayesian interpretation. We give an explicit SURE risk expression, discuss the oracle scale of the optimal threshold, and describe both global and level-dependent adaptive versions. The smooth SCAD rule therefore offers a tractable refinement of SCAD, combining low bias, exact sparsity, and analytical convenience in a single wavelet shrinkage procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11461v1</guid>
      <category>stat.CO</category>
      <category>cs.CE</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radhika Kulkarni, Aluisio Pinheiro, Brani Vidakovic, Abdourrahmane M. Atto</dc:creator>
    </item>
    <item>
      <title>Extremal Quantiles under Two-Way Clustering</title>
      <link>https://arxiv.org/abs/2402.19268</link>
      <description>arXiv:2402.19268v4 Announce Type: replace 
Abstract: This paper studies extremal quantiles under two-way clustered dependence. We show that the limiting distribution of unconditional intermediate-order tail quantiles is Gaussian. This result is notable because two-way clustering typically leads to non-Gaussian limiting behavior. Remarkably, extremal quantiles remain asymptotically Gaussian even in degenerate cases. Building on this insight, we extend our analysis to extremal quantile regression at intermediate orders. Simulation results corroborate our theoretical findings. Finally, we provide an empirical application to growth-at-risk, showing that earlier empirical conclusions remain robust even after accounting for two-way clustered dependence in panel data and the focus on extreme quantiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.19268v4</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harold D. Chiang, Ryutah Kato, Yuya Sasaki</dc:creator>
    </item>
    <item>
      <title>Robust Tail Index Estimation under Random Censoring via Minimum Density Power Divergence</title>
      <link>https://arxiv.org/abs/2507.18737</link>
      <description>arXiv:2507.18737v5 Announce Type: replace 
Abstract: We propose a robust estimator for the tail index of Pareto-type distributions under random right-censoring, based on the minimum density power divergence (MDPD) framework. To our knowledge, this is the first application of the MDPD approach to extreme value models with random censoring, opening a new direction for robust inference in this setting. Under mild regularity conditions, the estimator is shown to be consistent and asymptotically normal. Its performance in finite samples is extensively evaluated through simulation studies, demonstrating superior robustness and efficiency compared to existing methods. Contamination is introduced only before censoring to provide a meaningful assessment of robustness, while contamination after censoring is shown to yield distorted or unrealistic results. The practical relevance of the approach is illustrated using a real dataset on insurance claims, which features light censoring and fully observable extremes, and a dataset on AIDS survival times, which, despite stronger censoring (p&lt;1/2), allows for illustrative comparisons and highlights practical limitations and challenges in more difficult scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18737v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nour Elhouda Guesmia, Abdelhakim Necir, Djamel Meraghni</dc:creator>
    </item>
    <item>
      <title>Pivotal inference for linear predictions in stationary processes</title>
      <link>https://arxiv.org/abs/2508.21025</link>
      <description>arXiv:2508.21025v2 Announce Type: replace 
Abstract: In this paper we develop pivotal inference for the final (FPE) and relative final prediction error (RFPE) of linear forecasts in stationary processes. Our approach is based on a self-normalizing technique and avoids the estimation of the asymptotic variances of the empirical autocovariances. We provide pivotal confidence intervals for the (R)FPE, develop estimates for the minimal order of a linear prediction that is required to obtain a prespecified forecasting accuracy and also propose (pivotal) statistical tests for the hypotheses that the (R)FPE exceeds a given threshold. Additionally, we provide pivotal uncertainty quantification for the commonly used coefficient of determination $R^2$ obtained from a linear prediction based on the past $p \geq 1$ observations and develop new (pivotal) inference tools for the partial autocorrelation, which do not require the assumption of an autoregressive process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21025v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Holger Dette, Sebastian K\"uhnert</dc:creator>
    </item>
    <item>
      <title>Low-order outcomes and clustered designs: combining design and analysis for causal inference under network interference</title>
      <link>https://arxiv.org/abs/2405.07979</link>
      <description>arXiv:2405.07979v4 Announce Type: replace-cross 
Abstract: Variance reduction for causal inference in the presence of network interference is often achieved through either outcome modeling, typically analyzed under unit-randomized Bernoulli designs, or clustered experimental designs, typically analyzed without strong parametric assumptions. In this work, we study the intersection of these two approaches and make the following threefold contributions. First, we present an estimator of the total treatment effect (or global average treatment effect) in low-order outcome models when the data are collected under general experimental designs, generalizing previous results for Bernoulli designs. We refer to this estimator as the pseudoinverse estimator and give bounds on its bias and variance in terms of properties of the experimental design. Second, we evaluate these bounds for the case of Bernoulli graph cluster randomized (GCR) designs. Its variance scales like the smaller of the variance obtained by the estimator derived under a low-order assumption, and the variance obtained from cluster randomization, showing that combining these variance reduction strategies is preferable to using either individually. When the order of the potential outcomes model is correctly specified, our estimator is always unbiased, and under a misspecified model, we upper bound the bias by the closeness of the ground truth model to a low-order model. Third, we give empirical evidence that our variance bounds can be used to select a good clustering that minimizes the worst-case variance under a cluster randomized design from a set of candidate clusterings. Across a range of graphs and clustering algorithms, our method consistently selects clusterings that perform well on a range of response models, suggesting the practical use of our bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07979v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Eichhorn, Samir Khan, Johan Ugander, Christina Lee Yu</dc:creator>
    </item>
    <item>
      <title>Skew-symmetric approximations of posterior distributions</title>
      <link>https://arxiv.org/abs/2409.14167</link>
      <description>arXiv:2409.14167v3 Announce Type: replace-cross 
Abstract: Popular deterministic approximations of posterior distributions from, e.g. the Laplace method, variational Bayes and expectation-propagation, generally rely on symmetric approximating families, often taken to be Gaussian. This choice facilitates optimization and inference, but typically affects the quality of the overall approximation. In fact, even in basic parametric models, the posterior distribution often displays asymmetries that yield bias and a reduced accuracy when considering symmetric approximations. Recent research has moved towards more flexible approximating families which incorporate skewness. However, current solutions are often model specific, lack a general supporting theory, increase the computational complexity of the optimization problem, and do not provide a broadly applicable solution to incorporate skewness in any symmetric approximation. This article addresses such a gap by introducing a general and provably optimal strategy to perturb any off-the-shelf symmetric approximation of a generic posterior distribution. This novel perturbation scheme is derived without additional optimization steps, and yields a similarly tractable approximation within the class of skew-symmetric densities that provably enhances the finite sample accuracy of the original symmetric counterpart. Furthermore, under suitable assumptions, it improves the convergence rate to the exact posterior by at least a $\sqrt{n}$ factor, in asymptotic regimes. These advancements are illustrated in numerical studies focusing on skewed perturbations of state-of-the-art Gaussian approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14167v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Pozza, Daniele Durante, Botond Szabo</dc:creator>
    </item>
    <item>
      <title>New matrix perturbation bounds with relative norm: Perturbation of eigenspaces</title>
      <link>https://arxiv.org/abs/2409.20207</link>
      <description>arXiv:2409.20207v5 Announce Type: replace-cross 
Abstract: Matrix perturbation bounds (such as Weyl and Davis-Kahan) are used abundantly in many areas of mathematics and data science. Many bounds (such as the above two) involve the spectral norm of the noise matrix and are sharp in worst case analysis.
  In order to refine these classical bounds, we introduce a new parameter, which we refer to as the relative norm. This parameter measures the strength of the action of the noise matrix on the relevant eigenvectors of the ground matrix. It has turned out that in a number of situations, we can use the relative norm as a replacement for the spectral norm. This has led to a number of notable improvements under certain sets of assumptions, which are frequently met in practice. For instance, our new results apply very well in the case when the noise matrix is random.
  For the purpose of our study, we introduce a new method of analysis, which combines the classical contour integral argument with new (combinatorial) ideas. This method is robust and of independent interest.
  In the current paper, we focus on the perturbation of eigenspaces (Davis-Kahan type results). Perturbation bounds for eigenspaces are essential in statistics and theoretical computer science, and thus deserve a special treatment. Furthermore, this will lay the ground for the more technical treatment of general matrix functionals, which appears in a future paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20207v5</guid>
      <category>math.SP</category>
      <category>math.CO</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Phuc Tran, Van Vu</dc:creator>
    </item>
    <item>
      <title>Recovering a (1+1)-dimensional wave equation from a single white noise boundary measurement</title>
      <link>https://arxiv.org/abs/2503.18515</link>
      <description>arXiv:2503.18515v3 Announce Type: replace-cross 
Abstract: We consider the following inverse problem: Suppose a $(1+1)$-dimensional wave equation on $\mathbb{R}_+$ with zero initial conditions is excited with a Neumann boundary data modelled as a white noise process. Given also the Dirichlet data at the same point, determine the unknown first order coefficient function of the system.
  We first establish that direct problem is well-posed. The inverse problem is then solved by showing that correlations of the boundary data determine the Neumann-to-Dirichlet operator in the sense of distributions, which is known to uniquely identify the coefficient. This approach has applications in acoustic measurements of internal cross-sections of fluid pipes such as pressurised water supply pipes and vocal tract shape determination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18515v3</guid>
      <category>math.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emilia L. K. Bl{\aa}sten, Tapio Helin, Antti Kujanp\"a\"a, Lauri Oksanen, Jesse Railo</dc:creator>
    </item>
    <item>
      <title>Manifolds with kinks and the asymptotic behavior of the graph Laplacian operator with Gaussian kernel</title>
      <link>https://arxiv.org/abs/2507.07751</link>
      <description>arXiv:2507.07751v4 Announce Type: replace-cross 
Abstract: We introduce manifolds with kinks, a class of manifolds with possibly singular boundary that notably contains manifolds with smooth boundary and corners. We derive the asymptotic behavior of the Graph Laplace operator with Gaussian kernel and its deterministic limit on these spaces as bandwidth goes to zero. We show that this asymptotic behavior is determined by the inward sector of the tangent space and, as special cases, we derive its behavior near interior and singular points. Lastly, we show the validity of our theoretical results using numerical simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07751v4</guid>
      <category>math.DG</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susovan Pal, David Tewodrose</dc:creator>
    </item>
    <item>
      <title>A nonparametric Bayesian analysis of independent and identically distributed observations of covariate-driven Poisson processes</title>
      <link>https://arxiv.org/abs/2509.02299</link>
      <description>arXiv:2509.02299v2 Announce Type: replace-cross 
Abstract: An important task in the statistical analysis of inhomogeneous point processes is to investigate the influence of a set of covariates on the point-generating mechanism. In this article, we consider the nonparametric Bayesian approach to this problem, assuming that $n$ independent and identically distributed realizations of the point pattern and the covariate random field are available. In many applications, different covariates are often vastly diverse in physical nature, resulting in anisotropic intensity functions whose variations along distinct directions occur at different smoothness levels. To model this scenario, we employ hierarchical prior distributions based on multi-bandwidth Gaussian processes. We prove that the resulting posterior distributions concentrate around the ground truth at optimal rate as $n\to\infty$, and achieve automatic adaptation to the anisotropic smoothness. Posterior inference is concretely implemented via a Metropolis-within-Gibbs Markov chain Monte Carlo algorithm that incorporates a dimension-robust sampling scheme to handle the functional component of the proposed nonparametric Bayesian model. Our theoretical results are supported by extensive numerical simulation studies. Further, we present an application to the analysis of a Canadian wildfire dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02299v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patric Dolmeta, Matteo Giordano</dc:creator>
    </item>
    <item>
      <title>Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression</title>
      <link>https://arxiv.org/abs/2510.15337</link>
      <description>arXiv:2510.15337v2 Announce Type: replace-cross 
Abstract: Transfer learning is a key component of modern machine learning, enhancing the performance of target tasks by leveraging diverse data sources. Simultaneously, overparameterized models such as the minimum-$\ell_2$-norm interpolator (MNI) in high-dimensional linear regression have garnered significant attention for their remarkable generalization capabilities, a property known as benign overfitting. Despite their individual importance, the intersection of transfer learning and MNI remains largely unexplored. Our research bridges this gap by proposing a novel two-step Transfer MNI approach and analyzing its trade-offs. We characterize its non-asymptotic excess risk and identify conditions under which it outperforms the target-only MNI. Our analysis reveals free-lunch covariate shift regimes, where leveraging heterogeneous data yields the benefit of knowledge transfer at limited cost. To operationalize our findings, we develop a data-driven procedure to detect informative sources and introduce an ensemble method incorporating multiple informative Transfer MNIs. Finite-sample experiments demonstrate the robustness of our methods to model and data heterogeneity, confirming their advantage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15337v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yeichan Kim, Ilmun Kim, Seyoung Park</dc:creator>
    </item>
    <item>
      <title>A phase transition in Erd\H{o}s-Barak random graphs</title>
      <link>https://arxiv.org/abs/2512.05756</link>
      <description>arXiv:2512.05756v2 Announce Type: replace-cross 
Abstract: We study monotone paths in Erd\H{o}s-R\'enyi random graphs on numbered vertices. Benjamini &amp; Tzalik established a phase transition at $p = \frac{\log n}{n}$ for this model. We refine the critical value to $p = \frac{\log n - \log \log n }{n}$ and identify the critical window of order $\Theta(1/n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05756v2</guid>
      <category>math.PR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gilles Blanchard, Nicolas Curien, Klara Krause, Alexander Reisach</dc:creator>
    </item>
  </channel>
</rss>
