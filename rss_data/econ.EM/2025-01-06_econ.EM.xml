<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 Jan 2025 05:05:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Instrumental Variables with Time-Varying Exposure: New Estimates of Revascularization Effects on Quality of Life</title>
      <link>https://arxiv.org/abs/2501.01623</link>
      <description>arXiv:2501.01623v1 Announce Type: new 
Abstract: The ISCHEMIA Trial randomly assigned patients with ischemic heart disease to an invasive treatment strategy centered on revascularization with a control group assigned non-invasive medical therapy. As is common in such ``strategy trials,'' many participants assigned to treatment remained untreated while many assigned to control crossed over into treatment. Intention-to-treat (ITT) analyses of strategy trials preserve randomization-based comparisons, but ITT effects are diluted by non-compliance. Conventional per-protocol analyses that condition on treatment received are likely biased by discarding random assignment. In trials where compliance choices are made shortly after assignment, instrumental variables (IV) methods solve both problems -- recovering an undiluted average causal effect of treatment for treated subjects who comply with trial protocol. In ISCHEMIA, however, some controls were revascularized as long as five years after random assignment. This paper extends the IV framework for strategy trials, allowing for such dynamic non-random compliance behavior. IV estimates of long-run revascularization effects on quality of life are markedly larger than previously reported ITT and per-protocol estimates. We also show how to estimate complier characteristics in a dynamic-treatment setting. These estimates reveal increasing selection bias in naive time-varying per-protocol estimates of revascularization effects. Compliers have baseline health similar to that of the study population, while control-group crossovers are far sicker.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01623v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua D. Angrist, Bruno Ferman, Carol Gao, Peter Hull, Otavio L. Tecchio, Robert W. Yeh</dc:creator>
    </item>
    <item>
      <title>Analyzing Country-Level Vaccination Rates and Determinants of Practical Capacity to Administer COVID-19 Vaccines</title>
      <link>https://arxiv.org/abs/2501.01447</link>
      <description>arXiv:2501.01447v1 Announce Type: cross 
Abstract: The COVID-19 vaccine development, manufacturing, transportation, and administration proved an extreme logistics operation of global magnitude. Global vaccination levels, however, remain a key concern in preventing the emergence of new strains and minimizing the impact of the pandemic's disruption of daily life. In this paper, country-level vaccination rates are analyzed through a queuing framework to extract service rates that represent the practical capacity of a country to administer vaccines. These rates are further characterized through regression and interpretable machine learning methods with country-level demographic, governmental, and socio-economic variates. Model results show that participation in multi-governmental collaborations such as COVAX may improve the ability to vaccinate. Similarly, improved transportation and accessibility variates such as roads per area for low-income countries and rail lines per area for high-income countries can improve rates. It was also found that for low-income countries specifically, improvements in basic and health infrastructure (as measured through spending on healthcare, number of doctors and hospital beds per 100k, population percent with access to electricity, life expectancy, and vehicles per 1000 people) resulted in higher vaccination rates. Of the high-income countries, those with larger 65-plus populations struggled to vaccinate at high rates, indicating potential accessibility issues for the elderly. This study finds that improving basic and health infrastructure, focusing on accessibility in the last mile, particularly for the elderly, and fostering global partnerships can improve logistical operations of such a scale. Such structural impediments and inequities in global health care must be addressed in preparation for future global public health crises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01447v1</guid>
      <category>econ.GN</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sharika J. Hegde, Max T. M. Ng, Marcos Rios, Hani S. Mahmassani, Ying Chen, Karen Smilowitz</dc:creator>
    </item>
    <item>
      <title>Quantifying A Firm's AI Engagement: Constructing Objective, Data-Driven, AI Stock Indices Using 10-K Filings</title>
      <link>https://arxiv.org/abs/2501.01763</link>
      <description>arXiv:2501.01763v1 Announce Type: cross 
Abstract: Following an analysis of existing AI-related exchange-traded funds (ETFs), we reveal the selection criteria for determining which stocks qualify as AI-related are often opaque and rely on vague phrases and subjective judgments. This paper proposes a new, objective, data-driven approach using natural language processing (NLP) techniques to classify AI stocks by analyzing annual 10-K filings from 3,395 NASDAQ-listed firms between 2011 and 2023. This analysis quantifies each company's engagement with AI through binary indicators and weighted AI scores based on the frequency and context of AI-related terms. Using these metrics, we construct four AI stock indices-the Equally Weighted AI Index (AII), the Size-Weighted AI Index (SAII), and two Time-Discounted AI Indices (TAII05 and TAII5X)-offering different perspectives on AI investment. We validate our methodology through an event study on the launch of OpenAI's ChatGPT, demonstrating that companies with higher AI engagement saw significantly greater positive abnormal returns, with analyses supporting the predictive power of our AI measures. Our indices perform on par with or surpass 14 existing AI-themed ETFs and the Nasdaq Composite Index in risk-return profiles, market responsiveness, and overall performance, achieving higher average daily returns and risk-adjusted metrics without increased volatility. These results suggest our NLP-based approach offers a reliable, market-responsive, and cost-effective alternative to existing AI-related ETF products. Our innovative methodology can also guide investors, asset managers, and policymakers in using corporate data to construct other thematic portfolios, contributing to a more transparent, data-driven, and competitive approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01763v1</guid>
      <category>q-fin.GN</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.techfore.2024.123965</arxiv:DOI>
      <arxiv:journal_reference>Technological Forecasting and Social Change, 212, 123965 (2025)</arxiv:journal_reference>
      <dc:creator>Lennart Ante, Aman Saggu</dc:creator>
    </item>
    <item>
      <title>Grid-level impacts of renewable energy on thermal generation: efficiency, emissions and flexibility</title>
      <link>https://arxiv.org/abs/2501.01954</link>
      <description>arXiv:2501.01954v1 Announce Type: cross 
Abstract: Wind and solar generation constitute an increasing share of electricity supply globally. We find that this leas to shifts in the operational dynamics of thermal power plants. Using fixed effects panel regression across seven major U.S. balancing authorities, we analyze the impact of renewable generation on coal, natural gas combined cycle plants, and natural gas combustion turbines. Wind generation consistently displaces thermal output, while effects from solar vary significantly by region, achieving substantial displacement in areas with high solar penetration such as the California Independent System Operator but limited impacts in coal reliant grids such as the Midcontinent Independent System Operator. Renewable energy sources effectively reduce carbon dioxide emissions in regions with flexible thermal plants, achieving displacement effectiveness as high as one hundred and two percent in the California Independent System Operator and the Electric Reliability Council of Texas. However, in coal heavy areas such as the Midcontinent Independent System Operator and the Pennsylvania New Jersey Maryland Interconnection, inefficiencies from ramping and cycling reduce carbon dioxide displacement to as low as seventeen percent and often lead to elevated nitrogen oxides and sulfur dioxide emissions. These findings underscore the critical role of grid design, fuel mix, and operational flexibility in shaping the emissions benefits of renewables. Targeted interventions, including retrofitting high emitting plants and deploying energy storage, are essential to maximize emissions reductions and support the decarbonization of electricity systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01954v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhruv Suri, Jacques de Chalendar, Ines Azevedo</dc:creator>
    </item>
    <item>
      <title>Marginal Effects for Probit and Tobit with Endogeneity</title>
      <link>https://arxiv.org/abs/2306.14862</link>
      <description>arXiv:2306.14862v4 Announce Type: replace 
Abstract: When evaluating partial effects, it is important to distinguish between structural endogeneity and measurement errors. In contrast to linear models, these two sources of endogeneity affect partial effects differently in nonlinear models. We study this issue focusing on the Instrumental Variable (IV) Probit and Tobit models. We show that even when a valid IV is available, failing to differentiate between the two types of endogeneity can lead to either under- or over-estimation of the partial effects. We develop simple estimators of the bounds on the partial effects and provide easy to implement confidence intervals that correctly account for both types of endogeneity. We illustrate the methods in a Monte Carlo simulation and an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14862v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kirill S. Evdokimov, Ilze Kalnina, Andrei Zeleneev</dc:creator>
    </item>
    <item>
      <title>Large Language Models: An Applied Econometric Framework</title>
      <link>https://arxiv.org/abs/2412.07031</link>
      <description>arXiv:2412.07031v2 Announce Type: replace 
Abstract: How can we use the novel capacities of large language models (LLMs) in empirical research? And how can we do so while accounting for their limitations, which are themselves only poorly understood? We develop an econometric framework to answer this question that distinguishes between two types of empirical tasks. Using LLMs for prediction problems (including hypothesis generation) is valid under one condition: no ``leakage'' between the LLM's training dataset and the researcher's sample. No leakage can be ensured by using open-source LLMs with documented training data and published weights. Using LLM outputs for estimation problems to automate the measurement of some economic concept (expressed either by some text or from human subjects) requires the researcher to collect at least some validation data: without such data, the errors of the LLM's automation cannot be assessed and accounted for. As long as these steps are taken, LLM outputs can be used in empirical research with the familiar econometric guarantees we desire. Using two illustrative applications to finance and political economy, we find that these requirements are stringent; when they are violated, the limitations of LLMs now result in unreliable empirical estimates. Our results suggest the excitement around the empirical uses of LLMs is warranted -- they allow researchers to effectively use even small amounts of language data for both prediction and estimation -- but only with these safeguards in place.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07031v2</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jens Ludwig, Sendhil Mullainathan, Ashesh Rambachan</dc:creator>
    </item>
    <item>
      <title>Sparsified Simultaneous Confidence Intervals for High-Dimensional Linear Models</title>
      <link>https://arxiv.org/abs/2307.07574</link>
      <description>arXiv:2307.07574v2 Announce Type: replace-cross 
Abstract: Statistical inference of the high-dimensional regression coefficients is challenging because the uncertainty introduced by the model selection procedure is hard to account for. A critical question remains unsettled; that is, is it possible and how to embed the inference of the model into the simultaneous inference of the coefficients? To this end, we propose a notion of simultaneous confidence intervals called the sparsified simultaneous confidence intervals. Our intervals are sparse in the sense that some of the intervals' upper and lower bounds are shrunken to zero (i.e., $[0,0]$), indicating the unimportance of the corresponding covariates. These covariates should be excluded from the final model. The rest of the intervals, either containing zero (e.g., $[-1,1]$ or $[0,1]$) or not containing zero (e.g., $[2,3]$), indicate the plausible and significant covariates, respectively. The proposed method can be coupled with various selection procedures, making it ideal for comparing their uncertainty. For the proposed method, we establish desirable asymptotic properties, develop intuitive graphical tools for visualization, and justify its superior performance through simulation and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07574v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s00184-024-00975-z</arxiv:DOI>
      <arxiv:journal_reference>Metrika, 2024</arxiv:journal_reference>
      <dc:creator>Xiaorui Zhu, Yichen Qin, Peng Wang</dc:creator>
    </item>
    <item>
      <title>Robust Market Interventions</title>
      <link>https://arxiv.org/abs/2411.03026</link>
      <description>arXiv:2411.03026v2 Announce Type: replace-cross 
Abstract: When can interventions in markets be designed to increase surplus robustly -- i.e., with high probability -- accounting for uncertainty due to imprecise information about economic primitives? In a setting with many strategic firms, each possessing some market power, we present conditions for such interventions to exist. The key condition, recoverable structure, requires large-scale complementarities among families of products. The analysis works by decomposing the incidence of interventions in terms of principal components of a Slutsky matrix. Under recoverable structure, a noisy signal of this matrix reveals enough about these principal components to design robust interventions. Our results demonstrate the usefulness of spectral methods for analyzing imperfectly observed strategic interactions with many agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03026v2</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Galeotti, Benjamin Golub, Sanjeev Goyal, Eduard Talam\`as, Omer Tamuz</dc:creator>
    </item>
  </channel>
</rss>
