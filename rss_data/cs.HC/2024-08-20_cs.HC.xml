<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Aug 2024 04:00:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Knowledge Prompting: How Knowledge Engineers Use Large Language Models</title>
      <link>https://arxiv.org/abs/2408.08878</link>
      <description>arXiv:2408.08878v1 Announce Type: new 
Abstract: Despite many advances in knowledge engineering (KE), challenges remain in areas such as engineering knowledge graphs (KGs) at scale, keeping up with evolving domain knowledge, multilingualism, and multimodality. Recently, KE has used LLMs to support semi-automatic tasks, but the most effective use of LLMs to support knowledge engineers across the KE activites is still in its infancy. To explore the vision of LLM copilots for KE and change existing KE practices, we conducted a multimethod study during a KE hackathon. We investigated participants' views on the use of LLMs, the challenges they face, the skills they may need to integrate LLMs into their practices, and how they use LLMs responsibly. We found participants felt LLMs could contribute to improving efficiency when engineering KGs, but presented increased challenges around the already complex issues of evaluating the KE tasks. We discovered prompting to be a useful but undervalued skill for knowledge engineers working with LLMs, and note that natural language processing skills may become more relevant across more roles in KG construction. Integrating LLMs into KE tasks needs to be mindful of potential risks and harms related to responsible AI. Given the limited ethical training, most knowledge engineers receive solutions such as our suggested `KG cards' based on data cards could be a useful guide for KG construction. Our findings can support designers of KE AI copilots, KE researchers, and practitioners using advanced AI to develop trustworthy applications, propose new methodologies for KE and operate new technologies responsibly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08878v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elisavet Koutsiana, Johanna Walker, Michelle Nwachukwu, Albert Mero\~no-Pe\~nuela, Elena Simperl</dc:creator>
    </item>
    <item>
      <title>SpreadLine: Visualizing Egocentric Dynamic Influence</title>
      <link>https://arxiv.org/abs/2408.08992</link>
      <description>arXiv:2408.08992v1 Announce Type: new 
Abstract: Egocentric networks, often visualized as node-link diagrams, portray the complex relationship (link) dynamics between an entity (node) and others. However, common analytics tasks are multifaceted, encompassing interactions among four key aspects: strength, function, structure, and content. Current node-link visualization designs may fall short, focusing narrowly on certain aspects and neglecting the holistic, dynamic nature of egocentric networks. To bridge this gap, we introduce SpreadLine, a novel visualization framework designed to enable the visual exploration of egocentric networks from these four aspects at the microscopic level. Leveraging the intuitive appeal of storyline visualizations, SpreadLine adopts a storyline-based design to represent entities and their evolving relationships. We further encode essential topological information in the layout and condense the contextual information in a metro map metaphor, allowing for a more engaging and effective way to explore temporal and attribute-based information. To guide our work, with a thorough review of pertinent literature, we have distilled a task taxonomy that addresses the analytical needs specific to egocentric network exploration. Acknowledging the diverse analytical requirements of users, SpreadLine offers customizable encodings to enable users to tailor the framework for their tasks. We demonstrate the efficacy and general applicability of SpreadLine through three diverse real-world case studies (disease surveillance, social media trends, and academic career evolution) and a usability study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08992v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yun-Hsin Kuo, Dongyu Liu, Kwan-Liu Ma</dc:creator>
    </item>
    <item>
      <title>Me want cookie! Towards automated and transparent data governance on the Web</title>
      <link>https://arxiv.org/abs/2408.09071</link>
      <description>arXiv:2408.09071v1 Announce Type: new 
Abstract: This paper presents a sociotechnical vision for managing personal data, including cookies, within Web browsers. We first present our vision for a future of semi-automated data governance on the Web, using policy languages to describe data terms of use, and having browsers act on behalf of users to enact policy-based controls. Then, we present an overview of the technical research required to {prove} that existing policy languages express a sufficient range of concepts for describing cookie policies on the Web today. We view this work as a stepping stone towards a future of semi-automated data governance at Web-scale, which in the long term will also be used by next-generation Web technologies such as Web agents and Solid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09071v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Wright, Beatriz Esteves, Rui Zhao</dc:creator>
    </item>
    <item>
      <title>Not Too Long, Not Too Short: Goldilocks Principle of 'Optimal' Reflection Time on Online Deliberation Platforms</title>
      <link>https://arxiv.org/abs/2408.09084</link>
      <description>arXiv:2408.09084v1 Announce Type: new 
Abstract: The deliberative potential of online platforms has been widely examined but the impact of reflection time on the quality of deliberation remains under-explored. This paper presents two user studies involving 100 and 72 participants respectively, to investigate the impact of reflection time on the quality of deliberation in minute-scale deliberations. In the first study, we identified an optimal reflection time for composing short opinion comments. In the second study, we introduced four distinct interface-based time nudges aimed at encouraging reflection near the optimal time. While these nudges may not improve the quality of deliberation, they effectively prolonged reflection periods. Additionally, we observed mixed effects on users' experience, influenced by the nature of the time nudges. Our findings suggest that reflection time is crucial, particularly for users who typically deliberate below the optimal reflection threshold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09084v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>ShunYi Yeo, Simon Tangi Perrault</dc:creator>
    </item>
    <item>
      <title>EEG-SCMM: Soft Contrastive Masked Modeling for Cross-Corpus EEG-Based Emotion Recognition</title>
      <link>https://arxiv.org/abs/2408.09186</link>
      <description>arXiv:2408.09186v1 Announce Type: new 
Abstract: Emotion recognition using electroencephalography (EEG) signals has garnered widespread attention in recent years. However, existing studies have struggled to develop a sufficiently generalized model suitable for different datasets without re-training (cross-corpus). This difficulty arises because distribution differences across datasets far exceed the intra-dataset variability. To solve this problem, we propose a novel Soft Contrastive Masked Modeling (SCMM) framework. Inspired by emotional continuity, SCMM integrates soft contrastive learning with a new hybrid masking strategy to effectively mine the "short-term continuity" characteristics inherent in human emotions. During the self-supervised learning process, soft weights are assigned to sample pairs, enabling adaptive learning of similarity relationships across samples. Furthermore, we introduce an aggregator that weightedly aggregates complementary information from multiple close samples based on pairwise similarities among samples to enhance fine-grained feature representation, which is then used for original sample reconstruction. Extensive experiments on the SEED, SEED-IV and DEAP datasets show that SCMM achieves state-of-the-art (SOTA) performance, outperforming the second-best method by an average accuracy of 4.26% under two types of cross-corpus conditions (same-class and different-class) for EEG-based emotion recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09186v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qile Liu, Weishan Ye, Yulu Liu, Zhen Liang</dc:creator>
    </item>
    <item>
      <title>Social VR for Professional Networking: A Spatial Perspective</title>
      <link>https://arxiv.org/abs/2408.09280</link>
      <description>arXiv:2408.09280v1 Announce Type: new 
Abstract: One essential function of professional events, such as industry trade shows and academic conferences, is to foster and extend a person's connections to others within the community of their interest. In this paper, we delve into the emerging practice transitioning these events from physical venues to social VR as a new medium. Specifically, we ask: how does the spatial design in social VR affect the attendee's networking behaviors and experiences at these events? To answer this question, we conducted in-situ observations and in-depth interviews with 13 participants. Each of them had attended or hosted at least one real-world professional event taking place in social VR. We identified four elements of VR spatial design that shaped social interactions at these events: area size, which influenced a person's perceived likelihood of encountering others; pathways connecting areas, which guided their planning of the next activity to perform; magnets in areas, which facilitated spontaneous gatherings among people; and conventionality, which affected the assessment of a person's behavior appropriateness. Some of these elements were interpreted differently depending on the role of the participant, i.e., event hosts vs. attendees. We concluded this paper with multiple design implications derived from our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09280v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3677386.3682094</arxiv:DOI>
      <dc:creator>Victoria Chang, Ge Gao, Huaishu Peng</dc:creator>
    </item>
    <item>
      <title>Evaluating Usability and Engagement of Large Language Models in Virtual Reality for Traditional Scottish Curling</title>
      <link>https://arxiv.org/abs/2408.09285</link>
      <description>arXiv:2408.09285v1 Announce Type: new 
Abstract: This paper explores the innovative application of Large Language Models (LLMs) in Virtual Reality (VR) environments to promote heritage education, focusing on traditional Scottish curling presented in the game ``Scottish Bonspiel VR''. Our study compares the effectiveness of LLM-based chatbots with pre-defined scripted chatbots, evaluating key criteria such as usability, user engagement, and learning outcomes. The results show that LLM-based chatbots significantly improve interactivity and engagement, creating a more dynamic and immersive learning environment. This integration helps document and preserve cultural heritage and enhances dissemination processes, which are crucial for safeguarding intangible cultural heritage (ICH) amid environmental changes. Furthermore, the study highlights the potential of novel technologies in education to provide immersive experiences that foster a deeper appreciation of cultural heritage. These findings support the wider application of LLMs and VR in cultural education to address global challenges and promote sustainable practices to preserve and enhance cultural heritage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09285v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ka Hei Carrie Lau, Efe Bozkir, Hong Gao, Enkelejda Kasneci</dc:creator>
    </item>
    <item>
      <title>Auptimize: Optimal Placement of Spatial Audio Cues for Extended Reality</title>
      <link>https://arxiv.org/abs/2408.09320</link>
      <description>arXiv:2408.09320v1 Announce Type: new 
Abstract: Spatial audio in Extended Reality (XR) provides users with better awareness of where virtual elements are placed, and efficiently guides them to events such as notifications, system alerts from different windows, or approaching avatars. Humans, however, are inaccurate in localizing sound cues, especially with multiple sources due to limitations in human auditory perception such as angular discrimination error and front-back confusion. This decreases the efficiency of XR interfaces because users misidentify from which XR element a sound is coming. To address this, we propose Auptimize, a novel computational approach for placing XR sound sources, which mitigates such localization errors by utilizing the ventriloquist effect. Auptimize disentangles the sound source locations from the visual elements and relocates the sound sources to optimal positions for unambiguous identification of sound cues, avoiding errors due to inter-source proximity and front-back confusion. Our evaluation shows that Auptimize decreases spatial audio-based source identification errors compared to playing sound cues at the paired visual-sound locations. We demonstrate the applicability of Auptimize for diverse spatial audio-based interactive XR scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09320v1</guid>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3654777.3676424</arxiv:DOI>
      <dc:creator>Hyunsung Cho, Alexander Wang, Divya Kartik, Emily Liying Xie, Yukang Yan, David Lindlbauer</dc:creator>
    </item>
    <item>
      <title>VRCopilot: Authoring 3D Layouts with Generative AI Models in VR</title>
      <link>https://arxiv.org/abs/2408.09382</link>
      <description>arXiv:2408.09382v1 Announce Type: new 
Abstract: Immersive authoring provides an intuitive medium for users to create 3D scenes via direct manipulation in Virtual Reality (VR). Recent advances in generative AI have enabled the automatic creation of realistic 3D layouts. However, it is unclear how capabilities of generative AI can be used in immersive authoring to support fluid interactions, user agency, and creativity. We introduce VRCopilot, a mixed-initiative system that integrates pre-trained generative AI models into immersive authoring to facilitate human-AI co-creation in VR. VRCopilot presents multimodal interactions to support rapid prototyping and iterations with AI, and intermediate representations such as wireframes to augment user controllability over the created content. Through a series of user studies, we evaluated the potential and challenges in manual, scaffolded, and automatic creation in immersive authoring. We found that scaffolded creation using wireframes enhanced the user agency compared to automatic creation. We also found that manual creation via multimodal specification offers the highest sense of creativity and agency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09382v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3654777.3676451</arxiv:DOI>
      <dc:creator>Lei Zhang, Jin Pan, Jacob Gettig, Steve Oney, Anhong Guo</dc:creator>
    </item>
    <item>
      <title>Infinite Scrolling, Finite Satisfaction: Exploring User Behavior and Satisfaction on Social Media in Bangladesh</title>
      <link>https://arxiv.org/abs/2408.09601</link>
      <description>arXiv:2408.09601v1 Announce Type: new 
Abstract: Social media platforms continue to change our digital relationships nowadays. Therefore, recognizing the complex consequences of infinite scrolling is essential. This paper explores two distinct angles of social media engagement: mindless scrolling and mindful scrolling. This extensive study dives into numerous aspects of social media user behavior and satisfaction via the perspective of multiple surveys. We investigate the psychological exploit of infinite scrolling design to keep users engaged, illuminating its effect on users' emotional well-being. Furthermore, we explore its diverse effects on various groups, such as teenagers, professional people, and pregnant women, to better understand how digital activity differs throughout life phases. Furthermore, our study reveals the psychological consequences of being exposed to unfavorable news material. In the context of nutritional objectives, we examine the problems users confront as well as the significance of scrolling in dietary achievement. By taking into account the demographic effect, we can determine how factors like age, gender, and socioeconomic position affect user behavior. This study presents a comprehensive knowledge of the complicated connection of infinite scrolling with user satisfaction and psychological well-being through a variety of surveys, opening the door for well-informed conversations on online engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09601v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanzana Karim Lora, Sadia Afrin Purba, Bushra Hossain, Tanjina Oriana, Ashek Seum</dc:creator>
    </item>
    <item>
      <title>SMART-TBI: Design and Evaluation of the Social Media Accessibility and Rehabilitation Toolkit for Users with Traumatic Brain Injury</title>
      <link>https://arxiv.org/abs/2408.09683</link>
      <description>arXiv:2408.09683v1 Announce Type: new 
Abstract: Traumatic brain injury (TBI) can cause a range of cognitive and communication challenges that negatively affect social participation in both face-to-face interactions and computer-mediated communication. In particular, individuals with TBI report barriers that limit access to participation on social media platforms. To improve access to and use of social media for users with TBI, we introduce the Social Media Accessibility and Rehabilitation Toolkit (\textbf{SMART-TBI}). The toolkit includes five aids (Writing Aid, Interpretation Aid, Filter Mode, Focus Mode, and Facebook Customization) designed to address the cognitive and communicative needs of individuals with TBI. We asked eight users with moderate-severe TBI and five TBI rehabilitation experts to evaluate each aid. Our findings revealed potential benefits of aids and areas for improvement, including the need for psychological safety, privacy control, and balancing business and accessibility needs; and overall mixed reactions among the participants to AI-based aids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09683v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yaxin Hu, Hajin Lim, Lisa Kakonge, Jade T. Mitchell, Hailey L. Johnson, Lyn Turkstra, Melissa C. Duff, Catalina L. Toma, Bilge Mutlu</dc:creator>
    </item>
    <item>
      <title>WoW -- A System for Self-Service Collaborative Design Workshops</title>
      <link>https://arxiv.org/abs/2408.09926</link>
      <description>arXiv:2408.09926v1 Announce Type: new 
Abstract: In many working environments, users have to solve complex problems relying on large and multi-source data. Such problems require several experts to collaborate on solving them, or a single analyst to reconcile multiple complementary standpoints. Previous research has shown that wall-sized displays supports different collaboration styles, based most often on abstract tasks as proxies of real work. We present the design and implementation of WoW, short for ``Workspace on Wall'', a multi-user Web-based portal for collaborative meetings and workshops in multi-surface environments. We report on a two-year effort spanning context inquiry studies, system design iterations, development, and real testing rounds targeting design engineers in the tire industry. The pneumatic tires found on the market result from a highly collaborative and iterative development process that reconciles conflicting constraints through a series of product design workshops. WoW was found to be a flexible solution to build multi-view set-ups in a self-service manner and an effective means to access more content at once. Our users also felt more engaged in their collaborative problem-solving work using WoW than in conventional meeting rooms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09926v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilyasse Belkacem, Vasile Ciorna, Frank Petry, Mohammad Ghoniem</dc:creator>
    </item>
    <item>
      <title>Working in Extended Reality in the Wild: Worker and Bystander Experiences of XR Virtual Displays in Real-World Settings</title>
      <link>https://arxiv.org/abs/2408.10000</link>
      <description>arXiv:2408.10000v1 Announce Type: new 
Abstract: Although access to sufficient screen space is crucial to knowledge work, workers often find themselves with limited access to display infrastructure in remote or public settings. While virtual displays can be used to extend the available screen space through extended reality (XR) head-worn displays (HWD), we must better understand the implications of working with them in public settings from both users' and bystanders' viewpoints. To this end, we conducted two user studies. We first explored the usage of a hybrid AR display across real-world settings and tasks. We focused on how users take advantage of virtual displays and what social and environmental factors impact their usage of the system. A second study investigated the differences between working with a laptop, an AR system, or a VR system in public. We focused on a single location and participants performed a predefined task to enable direct comparisons between the conditions while also gathering data from bystanders. The combined results suggest a positive acceptance of XR technology in public settings and show that virtual displays can be used to accompany existing devices. We highlighted some environmental and social factors. We saw that previous XR experience and personality can influence how people perceive the use of XR in public. In addition, we confirmed that using XR in public still makes users stand out and that bystanders are curious about the devices, yet have no clear understanding of how they can be used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10000v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo Pavanatto, Verena Biener, Jennifer Chandran, Snehanjali Kalamkar, Feiyu Lu, John J. Dudley, Jinghui Hu, G. Nikki Ramirez-Saffy, Per Ola Kristensson, Alexander Giovannelli, Luke Schlueter, J\"org M\"uller, Jens Grubert, Doug A. Bowman</dc:creator>
    </item>
    <item>
      <title>Envisioning Possibilities and Challenges of AI for Personalized Cancer Care</title>
      <link>https://arxiv.org/abs/2408.10108</link>
      <description>arXiv:2408.10108v1 Announce Type: new 
Abstract: The use of Artificial Intelligence (AI) in healthcare, including in caring for cancer survivors, has gained significant interest. However, gaps remain in our understanding of how such AI systems can provide care, especially for ethnic and racial minority groups who continue to face care disparities. Through interviews with six cancer survivors, we identify critical gaps in current healthcare systems such as a lack of personalized care and insufficient cultural and linguistic accommodation. AI, when applied to care, was seen as a way to address these issues by enabling real-time, culturally aligned, and linguistically appropriate interactions. We also uncovered concerns about the implications of AI-driven personalization, such as data privacy, loss of human touch in caregiving, and the risk of echo chambers that limit exposure to diverse information. We conclude by discussing the trade-offs between AI-enhanced personalization and the need for structural changes in healthcare that go beyond technological solutions, leading us to argue that we should begin by asking, ``Why personalization?''</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10108v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3678884.3681885</arxiv:DOI>
      <dc:creator>Elaine Kong (Tim),  Kuo-Ting (Tim),  Huang, Aakash Gautam</dc:creator>
    </item>
    <item>
      <title>What should I wear to a party in a Greek taverna? Evaluation for Conversational Agents in the Fashion Domain</title>
      <link>https://arxiv.org/abs/2408.08907</link>
      <description>arXiv:2408.08907v1 Announce Type: cross 
Abstract: Large language models (LLMs) are poised to revolutionize the domain of online fashion retail, enhancing customer experience and discovery of fashion online. LLM-powered conversational agents introduce a new way of discovery by directly interacting with customers, enabling them to express in their own ways, refine their needs, obtain fashion and shopping advice that is relevant to their taste and intent. For many tasks in e-commerce, such as finding a specific product, conversational agents need to convert their interactions with a customer to a specific call to different backend systems, e.g., a search system to showcase a relevant set of products. Therefore, evaluating the capabilities of LLMs to perform those tasks related to calling other services is vital. However, those evaluations are generally complex, due to the lack of relevant and high quality datasets, and do not align seamlessly with business needs, amongst others. To this end, we created a multilingual evaluation dataset of 4k conversations between customers and a fashion assistant in a large e-commerce fashion platform to measure the capabilities of LLMs to serve as an assistant between customers and a backend engine. We evaluate a range of models, showcasing how our dataset scales to business needs and facilitates iterative development of tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08907v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Antonis Maronikolakis, Ana Peleteiro Ramallo, Weiwei Cheng, Thomas Kober</dc:creator>
    </item>
    <item>
      <title>Retail-GPT: leveraging Retrieval Augmented Generation (RAG) for building E-commerce Chat Assistants</title>
      <link>https://arxiv.org/abs/2408.08925</link>
      <description>arXiv:2408.08925v1 Announce Type: cross 
Abstract: This work presents Retail-GPT, an open-source RAG-based chatbot designed to enhance user engagement in retail e-commerce by guiding users through product recommendations and assisting with cart operations. The system is cross-platform and adaptable to various e-commerce domains, avoiding reliance on specific chat applications or commercial activities. Retail-GPT engages in human-like conversations, interprets user demands, checks product availability, and manages cart operations, aiming to serve as a virtual sales agent and test the viability of such assistants across different retail businesses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08925v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruno Amaral Teixeira de Freitas, Roberto de Alencar Lotufo</dc:creator>
    </item>
    <item>
      <title>Studying the Effects of Collaboration in Interactive Theme Discovery Systems</title>
      <link>https://arxiv.org/abs/2408.09030</link>
      <description>arXiv:2408.09030v1 Announce Type: cross 
Abstract: NLP-assisted solutions have gained considerable traction to support qualitative data analysis. However, there does not exist a unified evaluation framework that can account for the many different settings in which qualitative researchers may employ them. In this paper, we take a first step in this direction by proposing an evaluation framework to study the way in which different tools may result in different outcomes depending on the collaboration strategy employed. Specifically, we study the impact of synchronous vs. asynchronous collaboration using two different NLP-assisted qualitative research tools and present a comprehensive analysis of significant differences in the consistency, cohesiveness, and correctness of their outputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09030v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alvin Po-Chun Chen, Dananjay Srinivas, Alexandra Barry, Maksim Seniw, Maria Leonor Pacheco</dc:creator>
    </item>
    <item>
      <title>Keep Calm and Relax -- HMI for Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2408.09046</link>
      <description>arXiv:2408.09046v1 Announce Type: cross 
Abstract: The growing popularity of self-driving, so-called autonomous vehicles has increased the need for human-machine interfaces~(HMI) and user interaction~(UI) to enhance passenger trust and comfort. While fallback drivers significantly influence the perceived trustfulness of self-driving vehicles, fallback drivers are an expensive solution that may not even improve vehicle safety in emergency situations. Based on a comprehensive literature review, this work delves into the potential of HMI and UI in enhancing trustfulness and emotion regulation in driverless vehicles. By analyzing the impact of various HMI and UI on passenger emotions, innovative and cost-effective concepts for improving human-vehicle interaction are conceptualized. To enable a trustful, highly comfortable, and safe ride, this work concludes by discussing whether HMI and UI are suitable for calming passengers down in emergencies, leading to smarter mobility for all.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09046v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tima M. Yekta, Julius Sch\"oning</dc:creator>
    </item>
    <item>
      <title>Language Models Show Stable Value Orientations Across Diverse Role-Plays</title>
      <link>https://arxiv.org/abs/2408.09049</link>
      <description>arXiv:2408.09049v1 Announce Type: cross 
Abstract: We demonstrate that large language models (LLMs) exhibit consistent value orientations despite adopting diverse personas, revealing a persistent inertia in their responses that remains stable across the variety of roles they are prompted to assume. To systematically explore this phenomenon, we introduce the role-play-at-scale methodology, which involves prompting LLMs with randomized, diverse personas and analyzing the macroscopic trend of their responses. Unlike previous works that simply feed these questions to LLMs as if testing human subjects, our role-play-at-scale methodology diagnoses inherent tendencies in a systematic and scalable manner by: (1) prompting the model to act in different random personas and (2) asking the same question multiple times for each random persona. This approach reveals consistent patterns in LLM responses across diverse role-play scenarios, indicating deeply encoded inherent tendencies. Our findings contribute to the discourse on value alignment in foundation models and demonstrate the efficacy of role-play-at-scale as a diagnostic tool for uncovering encoded biases in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09049v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Bruce W. Lee, Yeongheon Lee, Hyunsoo Cho</dc:creator>
    </item>
    <item>
      <title>Measuring Visual Sycophancy in Multimodal Models</title>
      <link>https://arxiv.org/abs/2408.09111</link>
      <description>arXiv:2408.09111v1 Announce Type: cross 
Abstract: This paper introduces and examines the phenomenon of "visual sycophancy" in multimodal language models, a term we propose to describe these models' tendency to disproportionately favor visually presented information, even when it contradicts their prior knowledge or responses. Our study employs a systematic methodology to investigate this phenomenon: we present models with images of multiple-choice questions, which they initially answer correctly, then expose the same model to versions with visually pre-marked options. Our findings reveal a significant shift in the models' responses towards the pre-marked option despite their previous correct answers. Comprehensive evaluations demonstrate that visual sycophancy is a consistent and quantifiable behavior across various model architectures. Our findings highlight potential limitations in the reliability of these models when processing potentially misleading visual information, raising important questions about their application in critical decision-making contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09111v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jaehyuk Lim, Bruce W. Lee</dc:creator>
    </item>
    <item>
      <title>Game Development as Human-LLM Interaction</title>
      <link>https://arxiv.org/abs/2408.09386</link>
      <description>arXiv:2408.09386v1 Announce Type: cross 
Abstract: Game development is a highly specialized task that relies on a complex game engine powered by complex programming languages, preventing many gaming enthusiasts from handling it. This paper introduces the Interaction-driven Game Engine (IGE) powered by LLM, which allows everyone to develop a custom game using natural language through Human-LLM interaction. To enable an LLM to function as an IGE, we instruct it to perform the following processes in each turn: (1) $P_{script}$ : configure the game script segment based on the user's input; (2) $P_{code}$ : generate the corresponding code snippet based on the game script segment; (3) $P_{utter}$ : interact with the user, including guidance and feedback. We propose a data synthesis pipeline based on the LLM to generate game script-code pairs and interactions from a few manually crafted seed data. We propose a three-stage progressive training strategy to transfer the dialogue-based LLM to our IGE smoothly. We construct an IGE for poker games as a case study and comprehensively evaluate it from two perspectives: interaction quality and code correctness. The code and data are available at \url{https://github.com/alterego238/IGE}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09386v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiale Hong, Hongqiu Wu, Hai Zhao</dc:creator>
    </item>
    <item>
      <title>Baby Bear: Seeking a Just Right Rating Scale for Scalar Annotations</title>
      <link>https://arxiv.org/abs/2408.09765</link>
      <description>arXiv:2408.09765v1 Announce Type: cross 
Abstract: Our goal is a mechanism for efficiently assigning scalar ratings to each of a large set of elements. For example, "what percent positive or negative is this product review?" When sample sizes are small, prior work has advocated for methods such as Best Worst Scaling (BWS) as being more robust than direct ordinal annotation ("Likert scales"). Here we first introduce IBWS, which iteratively collects annotations through Best-Worst Scaling, resulting in robustly ranked crowd-sourced data. While effective, IBWS is too expensive for large-scale tasks. Using the results of IBWS as a best-desired outcome, we evaluate various direct assessment methods to determine what is both cost-efficient and best correlating to a large scale BWS annotation strategy. Finally, we illustrate in the domains of dialogue and sentiment how these annotations can support robust learning-to-rank models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09765v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xu Han, Felix Yu, Joao Sedoc, Benjamin Van Durme</dc:creator>
    </item>
    <item>
      <title>A Population-to-individual Tuning Framework for Adapting Pretrained LM to On-device User Intent Prediction</title>
      <link>https://arxiv.org/abs/2408.09815</link>
      <description>arXiv:2408.09815v1 Announce Type: cross 
Abstract: Mobile devices, especially smartphones, can support rich functions and have developed into indispensable tools in daily life. With the rise of generative AI services, smartphones can potentially transform into personalized assistants, anticipating user needs and scheduling services accordingly. Predicting user intents on smartphones, and reflecting anticipated activities based on past interactions and context, remains a pivotal step towards this vision. Existing research predominantly focuses on specific domains, neglecting the challenge of modeling diverse event sequences across dynamic contexts. Leveraging pre-trained language models (PLMs) offers a promising avenue, yet adapting PLMs to on-device user intent prediction presents significant challenges. To address these challenges, we propose PITuning, a Population-to-Individual Tuning framework. PITuning enhances common pattern extraction through dynamic event-to-intent transition modeling and addresses long-tailed preferences via adaptive unlearning strategies. Experimental results on real-world datasets demonstrate PITuning's superior intent prediction performance, highlighting its ability to capture long-tailed preferences and its practicality for on-device prediction scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09815v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahui Gong, Jingtao Ding, Fanjin Meng, Guilong Chen, Hong Chen, Shen Zhao, Haisheng Lu, Yong Li</dc:creator>
    </item>
    <item>
      <title>Dynamic Shaping of Multi-Touch Stimuli by Programmable Acoustic Metamaterial</title>
      <link>https://arxiv.org/abs/2408.09829</link>
      <description>arXiv:2408.09829v1 Announce Type: cross 
Abstract: Acoustic metamaterials are artificial structures, often lattice of resonators, with unusual properties. They can be engineered to stop wave propagation in specific frequency bands. Once manufactured, their dispersive qualities remain invariant in time and space, limiting their practical use. Actively tuned arrangements have received growing interest to address this issue. Here, we introduce a new class of active metamaterial made from dual-state unit cells, either vibration sources when powered or passive resonators when left disconnected. They possess self-tuning capabilities, enabling deep subwavelength band gaps to automatically match the carrier signal of powered cells, typically around 200Hz. Swift electronic commutations between both states establish the basis for real-time reconfiguration of waveguides and shaping of vibration patterns. A series of experiments highlight how these tailored acceleration fields can spatially encode information relevant to human touch. This novel metamaterial can readily be made using off-the-shelf smartphone vibration motors, paving the way for a widespread adoption of multi-touch tactile displays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09829v1</guid>
      <category>physics.app-ph</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Daunizeau, Sinan Haliyo, David Gueorguiev, Vincent Hayward</dc:creator>
    </item>
    <item>
      <title>LCE: A Framework for Explainability of DNNs for Ultrasound Image Based on Concept Discovery</title>
      <link>https://arxiv.org/abs/2408.09899</link>
      <description>arXiv:2408.09899v1 Announce Type: cross 
Abstract: Explaining the decisions of Deep Neural Networks (DNNs) for medical images has become increasingly important. Existing attribution methods have difficulty explaining the meaning of pixels while existing concept-based methods are limited by additional annotations or specific model structures that are difficult to apply to ultrasound images. In this paper, we propose the Lesion Concept Explainer (LCE) framework, which combines attribution methods with concept-based methods. We introduce the Segment Anything Model (SAM), fine-tuned on a large number of medical images, for concept discovery to enable a meaningful explanation of ultrasound image DNNs. The proposed framework is evaluated in terms of both faithfulness and understandability. We point out deficiencies in the popular faithfulness evaluation metrics and propose a new evaluation metric. Our evaluation of public and private breast ultrasound datasets (BUSI and FG-US-B) shows that LCE performs well compared to commonly-used explainability methods. Finally, we also validate that LCE can consistently provide reliable explanations for more meaningful fine-grained diagnostic tasks in breast ultrasound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09899v1</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weiji Kong, Xun Gong, Juan Wang</dc:creator>
    </item>
    <item>
      <title>A Graph-based Approach to Human Activity Recognition</title>
      <link>https://arxiv.org/abs/2408.10191</link>
      <description>arXiv:2408.10191v1 Announce Type: cross 
Abstract: Advanced wearable sensor devices have enabled the recording of vast amounts of movement data from individuals regarding their physical activities. This data offers valuable insights that enhance our understanding of how physical activities contribute to improved physical health and overall quality of life. Consequently, there is a growing need for efficient methods to extract significant insights from these rapidly expanding real-time datasets. This paper presents a methodology to efficiently extract substantial insights from these expanding datasets, focusing on professional sports but applicable to various human activities. By utilizing data from Inertial Measurement Units (IMU) and Global Navigation Satellite Systems (GNSS) receivers, athletic performance can be analyzed using directed graphs to encode knowledge of complex movements. Our approach is demonstrated on biathlon data and detects specific points of interest and complex movement sequences, facilitating the comparison and analysis of human physical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10191v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Peroutka, Ilir Murturi, Praveen Kumar Donta, Schahram Dustdar</dc:creator>
    </item>
    <item>
      <title>Detecting Clues for Skill Levels and Machine Operation Difficulty from Egocentric Vision</title>
      <link>https://arxiv.org/abs/1906.04002</link>
      <description>arXiv:1906.04002v2 Announce Type: replace 
Abstract: With respect to machine operation tasks, the experiences from different skill level operators, especially novices, can provide worthy understanding about the manner in which they perceive the operational environment and formulate knowledge to deal with various operation situations. In this study, we describe the operator's behaviors by utilizing the relations among their head, hand, and operation location (hotspot) during the operation. A total of 40 experiences associated with a sewing machine operation task performed by amateur operators was recorded via a head-mounted RGB-D camera. We examined important features of operational behaviors in different skill level operators and confirmed their correlation to the difficulties of the operation steps. The result shows that the pure-gazing behavior is significantly reduced when the operator's skill improved. Moreover, the hand-approaching duration and the frequency of attention movement before operation are strongly correlated to the operational difficulty in such machine operating environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:1906.04002v2</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Long-fei, Yuichi Nakamura, Kazuaki Kondo</dc:creator>
    </item>
    <item>
      <title>VizAbility: Enhancing Chart Accessibility with LLM-based Conversational Interaction</title>
      <link>https://arxiv.org/abs/2310.09611</link>
      <description>arXiv:2310.09611v4 Announce Type: replace 
Abstract: Traditional accessibility methods like alternative text and data tables typically underrepresent data visualization's full potential. Keyboard-based chart navigation has emerged as a potential solution, yet efficient data exploration remains challenging. We present VizAbility, a novel system that enriches chart content navigation with conversational interaction, enabling users to use natural language for querying visual data trends. VizAbility adapts to the user's navigation context for improved response accuracy and facilitates verbal command-based chart navigation. Furthermore, it can address queries for contextual information, designed to address the needs of visually impaired users. We designed a large language model (LLM)-based pipeline to address these user queries, leveraging chart data &amp; encoding, user context, and external web knowledge. We conducted both qualitative and quantitative studies to evaluate VizAbility's multimodal approach. We discuss further opportunities based on the results, including improved benchmark testing, incorporation of vision models, and integration with visualization workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09611v4</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Gorniak, Yoon Kim, Donglai Wei, Nam Wook Kim</dc:creator>
    </item>
    <item>
      <title>Investigating Remote Hands-On Assistance for Collaborative Development of Embedded Systems</title>
      <link>https://arxiv.org/abs/2404.17604</link>
      <description>arXiv:2404.17604v2 Announce Type: replace 
Abstract: Developing embedded systems is a complex endeavor that frequently requires collaborative teamwork. With the rise of freelance work and the global shift towards remote work, the need for effective remote collaboration has become crucial for many developers and their clients. However, current communication and coordination tools are predominantly tailored for software development rather than hardware-focused tasks. This study investigates the potential for remote support tools specifically designed for embedded systems development. Through interviews with 12 experienced embedded systems developers, we explored their existing remote work practices, challenges, and requirements. We also conducted a user enactment study featuring a custom-designed remote manipulation agent, Handy, as a theoretical assistant, to identify the kinds of support developers would value in a collaborative setting. Our findings highlight the scenarios and strategies employed in remote work, the specific support needs, and the challenges related to information exchange, coordination, and execution. Additionally, we explore concerns around privacy, control, and trust when using remote physical manipulation tools. This research contributes to the field by integrating the development of embedded systems with the remote, on-demand collaboration and assistance typical of software environments, offering a solid empirical foundation for future research on remote manipulation agents in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17604v2</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Chen, Jasmine Jones</dc:creator>
    </item>
    <item>
      <title>ChatGPT in Data Visualization Education: A Student Perspective</title>
      <link>https://arxiv.org/abs/2405.00748</link>
      <description>arXiv:2405.00748v2 Announce Type: replace 
Abstract: Unlike traditional educational chatbots that rely on pre-programmed responses, large-language model-driven chatbots, such as ChatGPT, demonstrate remarkable versatility to serve as a dynamic resource for addressing student needs from understanding advanced concepts to solving complex problems. This work explores the impact of such technology on student learning in an interdisciplinary, project-oriented data visualization course. Throughout the semester, students engaged with ChatGPT across four distinct projects, designing and implementing data visualizations using a variety of tools such as Tableau, D3, and Vega-lite. We collected conversation logs and reflection surveys after each assignment and conducted interviews with selected students to gain deeper insights into their experiences with ChatGPT. Our analysis examined the advantages and barriers of using ChatGPT, students' querying behavior, the types of assistance sought, and its impact on assignment outcomes and engagement. We discuss design considerations for an educational solution tailored for data visualization education, extending beyond ChatGPT's basic interface.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00748v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nam Wook Kim, Hyung-Kwon Ko, Grace Myers, Benjamin Bach</dc:creator>
    </item>
    <item>
      <title>Conversational Agents to Facilitate Deliberation on Harmful Content in WhatsApp Groups</title>
      <link>https://arxiv.org/abs/2405.20254</link>
      <description>arXiv:2405.20254v2 Announce Type: replace 
Abstract: WhatsApp groups have become a hotbed for the propagation of harmful content including misinformation, hate speech, polarizing content, and rumors, especially in Global South countries. Given the platform's end-to-end encryption, moderation responsibilities lie on group admins and members, who rarely contest such content. Another approach is fact-checking, which is unscalable, and can only contest factual content (e.g., misinformation) but not subjective content (e.g., hate speech). Drawing on recent literature, we explore deliberation -- open and inclusive discussion -- as an alternative. We investigate the role of a conversational agent in facilitating deliberation on harmful content in WhatsApp groups. We conducted semi-structured interviews with 21 Indian WhatsApp users, employing a design probe to showcase an example agent. Participants expressed the need for anonymity and recommended AI assistance to reduce the effort required in deliberation. They appreciated the agent's neutrality but pointed out the futility of deliberation in echo chamber groups. Our findings highlight design tensions for such an agent, including privacy versus group dynamics and freedom of speech in private spaces. We discuss the efficacy of deliberation using deliberative theory as a lens, compare deliberation with moderation and fact-checking, and provide design recommendations for future such systems. Ultimately, this work advances CSCW by offering insights into designing deliberative systems for combating harmful content in private group chats on social media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20254v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3687030</arxiv:DOI>
      <dc:creator>Dhruv Agarwal, Farhana Shahid, Aditya Vashistha</dc:creator>
    </item>
    <item>
      <title>CogErgLLM: Exploring Large Language Model Systems Design Perspective Using Cognitive Ergonomics</title>
      <link>https://arxiv.org/abs/2407.02885</link>
      <description>arXiv:2407.02885v3 Announce Type: replace 
Abstract: Integrating cognitive ergonomics with LLMs is essential for enhancing safety, reliability, and user satisfaction in human-AI interactions. Current LLM design often lacks this integration, leading to systems that may not fully align with human cognitive capabilities and limitations. Insufficient focus on incorporating cognitive science methods exacerbates biases in LLM outputs, while inconsistent application of user-centered design principles results in sub-optimal user experiences. To address these challenges, our position paper explores the critical integration of cognitive ergonomics principles into LLM design, aiming to provide a comprehensive framework and practical guidelines for ethical LLM development. Through our contributions, we seek to advance understanding and practice in integrating cognitive ergonomics into LLM systems, fostering safer, more reliable, and ethically sound human-AI interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02885v3</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>ICML 2024 Workshop on LLMs and Cognition</arxiv:journal_reference>
      <dc:creator>Azmine Toushik Wasi</dc:creator>
    </item>
    <item>
      <title>The Impact of Responsible AI Research on Innovation and Development</title>
      <link>https://arxiv.org/abs/2407.15647</link>
      <description>arXiv:2407.15647v4 Announce Type: replace 
Abstract: Translational research, especially in the fast-evolving field of Artificial Intelligence (AI), is key to converting scientific findings into practical innovations. In Responsible AI (RAI) research, translational impact is often viewed through various pathways, including research papers, blogs, news articles, and the drafting of forthcoming AI legislation (e.g., the EU AI Act). However, the real-world impact of RAI research remains an underexplored area. Our study aims to capture it through two pathways: \emph{patents} and \emph{code repositories}, both of which provide a rich and structured source of data. Using a dataset of 200,000 papers from 1980 to 2022 in AI and related fields, including Computer Vision, Natural Language Processing, and Human-Computer Interaction, we developed a Sentence-Transformers Deep Learning framework to identify RAI papers. This framework calculates the semantic similarity between paper abstracts and a set of RAI keywords, which are derived from the NIST's AI Risk Management Framework; a framework that aims to enhance trustworthiness considerations in the design, development, use, and evaluation of AI products, services, and systems. We identified 1,747 RAI papers published in top venues such as CHI, CSCW, NeurIPS, FAccT, and AIES between 2015 and 2022. By analyzing these papers, we found that a small subset that goes into patents or repositories is highly cited, with the translational process taking between 1 year for repositories and up to 8 years for patents. Interestingly, impactful RAI research is not limited to top U.S. institutions, but significant contributions come from European and Asian institutions. Finally, the multidisciplinary nature of RAI papers, often incorporating knowledge from diverse fields of expertise, was evident as these papers tend to build on unconventional combinations of prior knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15647v4</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Akbar Septiandri, Marios Constantinides, Daniele Quercia</dc:creator>
    </item>
    <item>
      <title>IDA: Breaking Barriers in No-code UI Automation Through Large Language Models and Human-Centric Design</title>
      <link>https://arxiv.org/abs/2407.15673</link>
      <description>arXiv:2407.15673v3 Announce Type: replace 
Abstract: Business users dedicate significant amounts of time to repetitive tasks within enterprise digital platforms, highlighting a critical need for automation. Despite advancements in low-code tools for UI automation, their complexity remains a significant barrier to adoption among non-technical business users. However, recent advancements in large language models (LLMs) have created new opportunities to overcome this barrier by offering more powerful, yet simpler and more human-centric programming environments. This paper presents IDA (Intelligent Digital Apprentice), a novel no-code Web UI automation tool designed specifically to empower business users with no technical background. IDA incorporates human-centric design principles, including guided programming by demonstration, semantic programming model, and teacher-student learning metaphor which is tailored to the skill set of business users. By leveraging LLMs, IDA overcomes some of the key technical barriers that have traditionally limited the possibility of no-code solutions. We have developed a prototype of IDA and conducted a user study involving real world business users and enterprise applications. The promising results indicate that users could effectively utilize IDA to create automation. The qualitative feedback indicates that IDA is perceived as user-friendly and trustworthy. This study contributes to unlocking the potential of AI assistants to enhance the productivity of business users through no-code user interface automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15673v3</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Segev Shlomov, Avi Yaeli, Sami Marreed, Sivan Schwartz, Netanel Eder, Offer Akrabi, Sergey Zeltyn</dc:creator>
    </item>
    <item>
      <title>Using a CNN Model to Assess Visual Artwork's Creativity</title>
      <link>https://arxiv.org/abs/2408.01481</link>
      <description>arXiv:2408.01481v2 Announce Type: replace-cross 
Abstract: Assessing artistic creativity has long challenged researchers, with traditional methods proving time-consuming. Recent studies have applied machine learning to evaluate creativity in drawings, but not paintings. Our research addresses this gap by developing a CNN model to automatically assess the creativity of human paintings. Using a dataset of six hundred paintings by professionals and children, our model achieved 90% accuracy and faster evaluation times than human raters. This approach demonstrates the potential of machine learning in advancing artistic creativity assessment, offering a more efficient alternative to traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01481v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhehan Zhang, Meihua Qian, Li Luo, Ripon Saha, Qianyi Gao, Xinxin Song</dc:creator>
    </item>
    <item>
      <title>Models Matter: Setting Accurate Privacy Expectations for Local and Central Differential Privacy</title>
      <link>https://arxiv.org/abs/2408.08475</link>
      <description>arXiv:2408.08475v2 Announce Type: replace-cross 
Abstract: Differential privacy is a popular privacy-enhancing technology that has been deployed both in industry and government agencies. Unfortunately, existing explanations of differential privacy fail to set accurate privacy expectations for data subjects, which depend on the choice of deployment model. We design and evaluate new explanations of differential privacy for the local and central models, drawing inspiration from prior work explaining other privacy-enhancing technologies. We find that consequences-focused explanations in the style of privacy nutrition labels that lay out the implications of differential privacy are a promising approach for setting accurate privacy expectations. Further, we find that while process-focused explanations are not enough to set accurate privacy expectations, combining consequences-focused explanations with a brief description of how differential privacy works leads to greater trust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08475v2</guid>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mary Anne Smart, Priyanka Nanayakkara, Rachel Cummings, Gabriel Kaptchuk, Elissa Redmiles</dc:creator>
    </item>
  </channel>
</rss>
