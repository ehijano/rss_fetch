<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Mar 2025 09:23:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Failure of Optimal Design Theory? A Case Study in Toxicology Using Sequential Robust Optimal Design Framework</title>
      <link>https://arxiv.org/abs/2503.00002</link>
      <description>arXiv:2503.00002v1 Announce Type: new 
Abstract: This paper presents a quasi-sequential optimal design framework for toxicology experiments, specifically applied to sea urchin embryos. The authors propose a novel approach combining robust optimal design with adaptive, stage-based testing to improve efficiency in toxicological studies, particularly where traditional uniform designs fall short. The methodology uses statistical models to refine dose levels across experimental phases, aiming for increased precision while reducing costs and complexity. Key components include selecting an initial design, iterative dose optimization based on preliminary results, and assessing various model fits to ensure robust, data-driven adjustments. Through case studies, we demonstrate improved statistical efficiency and adaptability in toxicology, with potential applications in other experimental domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00002v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elvis Han Cui, Michael Collins, Jessica Munson, Weng Kee Wong</dc:creator>
    </item>
    <item>
      <title>A Framework to Analyze Multiscale Sampling MCMC Methods</title>
      <link>https://arxiv.org/abs/2503.00251</link>
      <description>arXiv:2503.00251v1 Announce Type: new 
Abstract: We consider the theoretical analysis of Multiscale Sampling Methods, which are a new class of gradient-free Markov chain Monte Carlo (MCMC) methods for high dimensional inverse differential equation problems. A detailed presentation of those methods is given, including a review of each MCMC technique that they employ. Then, we propose a two-part framework to study and compare those methods. The first part identifies the new corresponding state space for the chain of random fields, and the second assesses convergence conditions on the instrumental and target distributions. Three Multiscale Sampling Methods are then analyzed using this new framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00251v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lucas Seiffert, Felipe Pereira</dc:creator>
    </item>
    <item>
      <title>Heteroscedastic Growth Curve Modeling with Shape-Restricted Splines</title>
      <link>https://arxiv.org/abs/2503.00254</link>
      <description>arXiv:2503.00254v1 Announce Type: new 
Abstract: Growth curve analysis (GCA) has a wide range of applications in various fields where growth trajectories need to be modeled. Heteroscedasticity is often present in the error term, which can not be handled with sufficient flexibility by standard linear fixed or mixed-effects models. One situation that has been addressed is where the error variance is characterized by a linear predictor with certain covariates. A frequently encountered scenario in GCA, however, is one in which the variance is a smooth function of the mean with known shape restrictions. A naive application of standard linear mixed-effects models would underestimate the variance of the fixed effects estimators and, consequently, the uncertainty of the estimated growth curve. We propose to model the variance of the response variable as a shape-restricted (increasing/decreasing; convex/concave) function of the marginal or conditional mean using shape-restricted splines. A simple iteratively reweighted fitting algorithm that takes advantage of existing software for linear mixed-effects models is developed. For inference, a parametric bootstrap procedure is recommended. Our simulation study shows that the proposed method gives satisfactory inference with moderate sample sizes. The utility of the method is demonstrated using two real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00254v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jieying Jiao, Wenling Song, Yishu Xue, Jun Yan</dc:creator>
    </item>
    <item>
      <title>Error Bounds Revisited, and How to Use Bayesian Statistics While Remaining a Frequentist</title>
      <link>https://arxiv.org/abs/2503.00314</link>
      <description>arXiv:2503.00314v1 Announce Type: new 
Abstract: Signal processing makes extensive use of point estimators and accompanying error bounds. These work well up until the likelihood function has two or more high peaks. When it is important for an estimator to remain reliable, it becomes necessary to consider alternatives, such as set estimators. An obvious first choice might be confidence intervals or confidence regions, but there can be difficulties in computing and interpreting them (and sometimes they might still be blind to multiple peaks in the likelihood). Bayesians seize on this to argue for replacing confidence regions with credible regions. Yet Bayesian statistics require a prior, which is not always a natural part of the problem formulation. This paper demonstrates how a re-interpretation of the prior as a weighting function makes an otherwise Bayesian estimator meaningful in the frequentist context. The weighting function interpretation also serves as a reminder that an estimator should always be designed in the context of its intended application; unlike a prior which ostensibly depends on prior knowledge, a weighting function depends on the intended application. This paper uses the time-of-arrival (TOA) problem to illustrate all these points. It also derives a basic theory of region-based estimators distinct from confidence regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00314v1</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ning Xu, Christopher M. Foster, Jonathan H. Manton</dc:creator>
    </item>
    <item>
      <title>Learning Conditional Average Treatment Effects in Regression Discontinuity Designs using Bayesian Additive Regression Trees</title>
      <link>https://arxiv.org/abs/2503.00326</link>
      <description>arXiv:2503.00326v1 Announce Type: new 
Abstract: BART (Bayesian additive regression trees) has been established as a leading supervised learning method, particularly in the field of causal inference. This paper explores the use of BART models for learning conditional average treatment effects (CATE) from regression discontinuity designs, where treatment assignment is based on whether an observed covariate (called the running variable) exceeds a pre-specified threshold. A purpose-built version of BART that uses linear regression leaf models (of the running variable and treatment assignment dummy) is shown to out-perform off-the-shelf BART implementations as well as a local polynomial regression approach and a CART-based approach. The new method is evaluated in thorough simulation studies as well as an empirical application looking at the effect of academic probation on student performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00326v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rafael Alcantara, P. Richard Hahn, Carlos Carvalho, Hedibert Lopes</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization for Intrinsically Noisy Response Surfaces</title>
      <link>https://arxiv.org/abs/2503.00327</link>
      <description>arXiv:2503.00327v1 Announce Type: new 
Abstract: While many advanced statistical methods for the design of experiments exist, it is still typical for physical experiments to be performed adaptively based on human intuition. As a consequence, experimental resources are wasted on sub-optimal experimental designs. Conversely, in the simulation-based design community, Bayesian optimization (BO) is often used to adaptively and efficiently identify the global optimum of a response surface. However, adopting these methods directly for the optimization of physical experiments is problematic due to the existence of experimental noise and the typically more stringent constraints on the experimental budget. Consequently, many simplifying assumptions need to be made in the BO framework, and it is currently not fully understood how these assumptions influence the performance of the method and the optimality of the final design. In this paper, we present an experimental study to investigate the influence of the controllable (e.g., number of samples, acquisition function, and covariance function) and noise factors (e.g., problem dimensionality, experimental noise magnitude, and experimental noise form) on the efficiency of the BO framework. The findings in this study include, that the Mat\'{e}r covariance function shows superior performance over all test problems and that the available experimental budget is most consequential when selecting the other settings of the BO scheme. With this study, we enable designers to make more efficient use of their physical experiments and provide insight into the use of BO with intrinsically noisy training data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00327v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton van Beek</dc:creator>
    </item>
    <item>
      <title>Review on Determining the Number of Communities in Network Data</title>
      <link>https://arxiv.org/abs/2503.00352</link>
      <description>arXiv:2503.00352v1 Announce Type: new 
Abstract: This paper reviews statistical methods for hypothesis testing and clustering in network models. We analyze the method by Bickel et al. (2016) for deriving the asymptotic null distribution of the largest eigenvalue, noting its slow convergence and the need for bootstrap corrections. The SCORE method by Jin et al. (2015) and the NCV method by Chen et al. (2018) are evaluated for their efficacy in clustering within Degree-Corrected Block Models, with NCV facing challenges due to its time-intensive nature. We suggest exploring eigenvector entry distributions as a potential efficiency improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00352v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengyuan Du, Jason Cui</dc:creator>
    </item>
    <item>
      <title>Parametric MMD Estimation with Missing Values: Robustness to Missingness and Data Model Misspecification</title>
      <link>https://arxiv.org/abs/2503.00448</link>
      <description>arXiv:2503.00448v1 Announce Type: new 
Abstract: In the missing data literature, the Maximum Likelihood Estimator (MLE) is celebrated for its ignorability property under missing at random (MAR) data. However, its sensitivity to misspecification of the (complete) data model, even under MAR, remains a significant limitation. This issue is further exacerbated by the fact that the MAR assumption may not always be realistic, introducing an additional source of potential misspecification through the missingness mechanism. To address this, we propose a novel M-estimation procedure based on the Maximum Mean Discrepancy (MMD), which is provably robust to both model misspecification and deviations from the assumed missingness mechanism. Our approach offers strong theoretical guarantees and improved reliability in complex settings. We establish the consistency and asymptotic normality of the estimator under missingness completely at random (MCAR), provide an efficient stochastic gradient descent algorithm, and derive error bounds that explicitly separate the contributions of model misspecification and missingness bias. Furthermore, we analyze missing not at random (MNAR) scenarios where our estimator maintains controlled error, including a Huber setting where both the missingness mechanism and the data model are contaminated. Our contributions refine the understanding of the limitations of the MLE and provide a robust and principled alternative for handling missing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00448v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Badr-Eddine Ch\'erief-Abdellatif, Jeffrey N\"af</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Non-Synchronously Observed Diffusions</title>
      <link>https://arxiv.org/abs/2503.00465</link>
      <description>arXiv:2503.00465v1 Announce Type: new 
Abstract: We consider the problem of Bayesian inference for bi-variate data observed in time but with observation times which occur non-synchronously. In particular, this occurs in a wide variety of applications in finance, such as high-frequency trading or crude oil futures trading. We adopt a diffusion model for the data and formulate a Bayesian model with priors on unknown parameters along with a latent representation for the the so-called missing data. We then consider computational methodology to fit the model using Markov chain Monte Carlo (MCMC). We have to resort to time-discretization methods as the complete data likelihood is intractable and this can cause considerable issues for MCMC when the data are observed in low frequencies. In a high frequency observation frequencies we present a simple particle MCMC method based on an Euler--Maruyama time discretization, which can be enhanced using multilevel Monte Carlo (MLMC). In the low frequency observation regime we introduce a novel bridging representation of the posterior in continuous time to deal with the issues of MCMC in this case. This representation is discretized and fitted using MCMC and MLMC. We apply our methodology to real and simulated data to establish the efficacy of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00465v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajay Jasra, Kengo Kamatani, Amin Wu</dc:creator>
    </item>
    <item>
      <title>Nonparametric spectral density estimation from irregularly sampled data</title>
      <link>https://arxiv.org/abs/2503.00492</link>
      <description>arXiv:2503.00492v1 Announce Type: new 
Abstract: We introduce a nonparametric spectral density estimator for fully irregularly sampled points in one or more dimensions, constructed using a weighted nonuniform Fourier sum whose weights yield a high-accuracy quadrature rule for a user-specified window function. The resulting estimator significantly reduces the aliasing seen in periodogram approaches and least squares spectral analysis, sidesteps the dangers of ill-conditioning of the nonuniform Fourier inverse problem, and can be adapted to a wide variety of irregular sampling settings. After a discussion of methods for computing the necessary weights and a theoretical analysis of sources of bias, we close with demonstrations of the method's efficacy, including for processes that exhibit very slow spectral decay and for processes in multiple dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00492v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher J. Geoga, Paul G. Beckman</dc:creator>
    </item>
    <item>
      <title>Bias in Gini coefficient estimation for gamma mixture populations</title>
      <link>https://arxiv.org/abs/2503.00690</link>
      <description>arXiv:2503.00690v1 Announce Type: new 
Abstract: This paper examines the properties of the Gini coefficient estimator for gamma mixture populations and reveals the presence of bias. In contrast, we show that sampling from a gamma distribution yields an unbiased estimator, consistent with prior research (Baydil et al., 2025). We derive an explicit bias expression for the Gini coefficient in gamma mixture populations, which serves as the foundation for proposing a bias-corrected Gini estimator. We conduct a Monte Carlo simulation study to evaluate the behavior of the bias-corrected Gini estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00690v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>Quantile Residual Lifetime Regression for Multivariate Failure Time Data</title>
      <link>https://arxiv.org/abs/2503.00716</link>
      <description>arXiv:2503.00716v1 Announce Type: new 
Abstract: The quantile residual lifetime (QRL) regression is an attractive tool for assessing covariate effects on the distribution of residual life expectancy, which is often of interest in clinical studies. When the study subjects are exposed to multiple events of interest, the failure times observed for the same subject are potentially correlated. To address such correlation in assessing the covariate effects on QRL, we propose a marginal semiparametric QRL regression model for multivariate failure time data. Our new proposal facilitates estimation of the model parameters using unbiased estimating equations and results in estimators, which are shown to be consistent and asymptotically normal. To overcome additional challenges in inference, we provide three methods for variance estimation based on resampling techniques and a sandwich estimator, and further develop a Wald-type test statistic for inference. The simulation studies and a real data analysis offer evidence of the satisfactory performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00716v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tonghui Yu, Liming Xiang, Jong-Hyeon Jeong</dc:creator>
    </item>
    <item>
      <title>Time-Varying Causal Survival Learning</title>
      <link>https://arxiv.org/abs/2503.00730</link>
      <description>arXiv:2503.00730v1 Announce Type: new 
Abstract: This work bridges the gap between staggered adoption designs and survival analysis to estimate causal effects in settings with time-varying treatments, addressing a fundamental challenge in medical research exemplified by the Stanford Heart Transplant study. In medical interventions, particularly organ transplantation, the timing of treatment varies significantly across patients due to factors such as donor availability and patient readiness, introducing potential bias in treatment effect estimation if not properly accounted for. We identify conditions under which staggered adoption assumptions can justify the use of survival analysis techniques for causal inference with time-varying treatments. By establishing this connection, we enable the use of existing survival analysis methods while maintaining causal interpretability. Furthermore, we enhance estimation performance by incorporating double machine learning methods, improving efficiency when handling complex relationships between patient characteristics and survival outcomes. Through both simulation studies and application to heart transplant data, our approach demonstrates superior performance compared to traditional methods, reducing bias and offering theoretical guarantees for improved efficiency in survival analysis settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00730v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiang Meng, Iavor Bojinov</dc:creator>
    </item>
    <item>
      <title>Strategic decision points in experiments: A predictive Bayesian optional stopping method</title>
      <link>https://arxiv.org/abs/2503.00818</link>
      <description>arXiv:2503.00818v1 Announce Type: new 
Abstract: Sample size determination is crucial in experimental design, especially in traffic and transport research. Frequentist statistics require a fixed sample size determined by power analysis, which cannot be adjusted once the experiment starts. Bayesian sample size determination, with proper priors, offers an alternative. Bayesian optional stopping (BOS) allows experiments to stop when statistical targets are met. We introduce predictive Bayesian optional stopping (pBOS), combining BOS with Bayesian rehearsal simulations to predict future data and stop experiments if targets are unlikely to be met within resource constraints. We identified and corrected a bias in predictions using multiple linear regression. pBOS shows up to 118% better cost benefit than traditional BOS and is more efficient than frequentist methods. pBOS allows researchers to, under certain conditions, stop experiments when resources are insufficient or when enough data is collected, optimizing resource use and cost savings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00818v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaomi Yang, Carol Flannagan, Jonas B\"argman</dc:creator>
    </item>
    <item>
      <title>On the use of the principle of maximum entropy in bivariate splines least-squares approximation</title>
      <link>https://arxiv.org/abs/2503.00942</link>
      <description>arXiv:2503.00942v1 Announce Type: new 
Abstract: We consider fitting a bivariate spline regression model to data using a weighted least-squares cost function, with weights that sum to one to form a discrete probability distribution. By applying the principle of maximum entropy, the weight distribution is determined by maximizing the associated entropy function. This approach, previously applied successfully to polynomials and spline curves, enhances the robustness of the regression model by automatically detecting and down-weighting anomalous data during the fitting process. To demonstrate the effectiveness of the method, we present applications to two image processing problems and further illustrate its potential through two synthetic examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00942v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pierluigi Amodio, Luigi Brugnano, Felice Iavernaro</dc:creator>
    </item>
    <item>
      <title>Multivariable Behavioral Change Modeling of Epidemics in the Presence of Undetected Infections</title>
      <link>https://arxiv.org/abs/2503.00982</link>
      <description>arXiv:2503.00982v1 Announce Type: new 
Abstract: Epidemic models are invaluable tools to understand and implement strategies to control the spread of infectious diseases, as well as to inform public health policies and resource allocation. However, current modeling approaches have limitations that reduce their practical utility, such as the exclusion of human behavioral change in response to the epidemic or ignoring the presence of undetected infectious individuals in the population. These limitations became particularly evident during the COVID-19 pandemic, underscoring the need for more accurate and informative models. Motivated by these challenges, we develop a novel Bayesian epidemic modeling framework to better capture the complexities of disease spread by incorporating behavioral responses and undetected infections. In particular, our framework makes three contributions: 1) leveraging additional data on hospitalizations and deaths in modeling the disease dynamics, 2) accounting data uncertainty arising from the large presence of asymptomatic and undetected infections, and 3) allowing the population behavioral change to be dynamically influenced by multiple data sources (cases and deaths). We thoroughly investigate the properties of the proposed model via simulation, and illustrate its utility on COVID-19 data from Montreal and Miami.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00982v1</guid>
      <category>stat.ME</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caitlin Ward, Rob Deardon, Alexandra M. Schmidt</dc:creator>
    </item>
    <item>
      <title>Testing for Repeated Motifs and Hierarchical Structure in Stochastic Blockmodels</title>
      <link>https://arxiv.org/abs/2503.01024</link>
      <description>arXiv:2503.01024v1 Announce Type: new 
Abstract: The rise in complexity of network data in neuroscience, social networks, and protein-protein interaction networks has been accompanied by several efforts to model and understand these data at different scales. A key multiscale network modeling technique posits hierarchical structure in the network, and by treating networks as multiple levels of subdivisions with shared statistical properties we can efficiently discover smaller subgraph primitives with manageable complexity. One such example of hierarchical modeling is the Hierarchical Stochastic Block Model, which seeks to model complex networks as being composed of community structures repeated across the network. Incorporating repeated structure allows for parameter tying across communities in the SBM, reducing the model complexity compared to the traditional blockmodel. In this work, we formulate a framework for testing for repeated motif hierarchical structure in the stochastic blockmodel framework. We describe a model which naturally expresses networks as a hierarchy of sub-networks with a set of motifs repeating across it, and we demonstrate the practical utility of the test through theoretical analysis and extensive simulation and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01024v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Al-Fahad Al-Qadhi, Keith Levin, Vincent Lyzinski</dc:creator>
    </item>
    <item>
      <title>Powerful rank verification for multivariate Gaussian data with any covariance structure</title>
      <link>https://arxiv.org/abs/2503.01065</link>
      <description>arXiv:2503.01065v1 Announce Type: new 
Abstract: Upon observing $n$-dimensional multivariate Gaussian data, when can we infer that the largest $K$ observations came from the largest $K$ means? When $K=1$ and the covariance is isotropic, \cite{Gutmann} argue that this inference is justified when the two-sided difference-of-means test comparing the largest and second largest observation rejects. Leveraging tools from selective inference, we provide a generalization of their procedure that applies for both any $K$ and any covariance structure. We show that our procedure draws the desired inference whenever the two-sided difference-of-means test comparing the pair of observations inside and outside the top $K$ with the smallest standardized difference rejects, and sometimes even when this test fails to reject. Using this insight, we argue that our procedure renders existing simultaneous inference approaches inadmissible when $n &gt; 2$. When the observations are independent (with possibly unequal variances) or equicorrelated, our procedure corresponds exactly to running the two-sided difference-of-means test comparing the pair of observations inside and outside the top $K$ with the smallest standardized difference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01065v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anav Sood</dc:creator>
    </item>
    <item>
      <title>A Dynamic Factor Model for Multivariate Counting Process Data</title>
      <link>https://arxiv.org/abs/2503.01081</link>
      <description>arXiv:2503.01081v1 Announce Type: new 
Abstract: We propose a dynamic multiplicative factor model for process data, which arise from complex problem-solving items, an emerging testing mode in large-scale educational assessment. The proposed model can be viewed as an extension of the classical frailty models developed in survival analysis for multivariate recurrent event times, but with two important distinctions: (i) the factor (frailty) is of primary interest; (ii) covariates are internal and embedded in the factor. It allows us to explore low dimensional structure with meaningful interpretation. We show that the proposed model is identifiable and that the maximum likelihood estimators are consistent and asymptotically normal. Furthermore, to obtain a parsimonious model and to improve interpretation of parameters therein, variable selection and estimation for both fixed and random effects are developed through suitable penalisation. The computation is carried out by a stochastic EM combined with the Metropolis algorithm and the coordinate descent algorithm. Simulation studies demonstrate that the proposed approach provides an effective recovery of the true structure. The proposed method is applied to analysing the log-file of an item from the Programme for the International Assessment of Adult Competencies (PIAAC), where meaningful relationships are discovered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01081v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fangyi Chen, Hok Kan Ling, Zhiliang Ying</dc:creator>
    </item>
    <item>
      <title>Bayesian spatio-temporal modelling for infectious disease outbreak detection</title>
      <link>https://arxiv.org/abs/2503.01456</link>
      <description>arXiv:2503.01456v1 Announce Type: new 
Abstract: The Bayesian analysis of infectious disease surveillance data from multiple locations typically involves building and fitting a spatio-temporal model of how the disease spreads in the structured population. Here we present new generally applicable methodology to perform this task. We introduce a parsimonious representation of seasonality and a biologically informed specification of the outbreak component to avoid parameter identifiability issues. We develop a computationally efficient Bayesian inference methodology for the proposed models, including techniques to detect outbreaks by computing marginal posterior probabilities at each spatial location and time point. We show that it is possible to efficiently integrate out the discrete parameters associated with outbreak states, enabling the use of dynamic Hamiltonian Monte Carlo (HMC) as a complementary alternative to a hybrid Markov chain Monte Carlo (MCMC) algorithm. Furthermore, we introduce a robust Bayesian model comparison framework based on importance sampling to approximate model evidence in high-dimensional space. The performance of our methodology is validated through systematic simulation studies, where simulated outbreaks were successfully detected, and our model comparison strategy demonstrates strong reliability. We also apply our new methodology to monthly incidence data on invasive meningococcal disease from 28 European countries. The results highlight outbreaks across multiple countries and months, with model comparison analysis showing that the new specification outperforms previous approaches. The accompanying software is freely available as a R package at https://github.com/Matthewadeoye/DetectOutbreaks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01456v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Adeoye, Xavier Didelot, Simon EF Spencer</dc:creator>
    </item>
    <item>
      <title>Recommendations for visual predictive checks in Bayesian workflow</title>
      <link>https://arxiv.org/abs/2503.01509</link>
      <description>arXiv:2503.01509v1 Announce Type: new 
Abstract: A key step in the Bayesian workflow for model building is the graphical assessment of model predictions, whether these are drawn from the prior or posterior predictive distribution. The goal of these assessments is to identify whether the model is a reasonable (and ideally accurate) representation of the domain knowledge and/or observed data. There are many commonly used visual predictive checks which can be misleading if their implicit assumptions do not match the reality. Thus, there is a need for more guidance for selecting, interpreting, and diagnosing appropriate visualizations. As a visual predictive check itself can be viewed as a model fit to data, assessing when this model fails to represent the data is important for drawing well-informed conclusions.
  We present recommendations and diagnostic tools to mitigate ad-hoc decision-making in visual predictive checks. These contributions aim to improve the robustness and interpretability of Bayesian model criticism practices. We offer recommendations for appropriate visual predictive checks for observations that are: continuous, discrete, or a mixture of the two. We also discuss diagnostics to aid in the selection of visual methods. Specifically, in the detection of an incorrect assumption of continuously-distributed data: identifying when data is likely to be discrete or contain discrete components, detecting and estimating possible bounds in data, and a diagnostic of the goodness-of-fit to data for density plots made through kernel density estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01509v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teemu S\"ailynoja, Andrew R. Johnson, Osvaldo A. Martin, Aki Vehtari</dc:creator>
    </item>
    <item>
      <title>A Generalized Fr\'echet Test for Object Data with Unequal Repeated Measurements</title>
      <link>https://arxiv.org/abs/2503.01514</link>
      <description>arXiv:2503.01514v1 Announce Type: new 
Abstract: Advancements in data collection have led to increasingly common repeated observations with complex structures in biomedical studies. Treating these observations as random objects, rather than summarizing features as vectors, avoids feature extraction and better reflects the data's nature. Examples include repeatedly measured activity intensity distributions in physical activity analysis and brain networks in neuroimaging. Testing whether these repeated random objects differ across groups is fundamentally important; however, traditional statistical tests often face challenges due to the non-Euclidean nature of metric spaces, dependencies from repeated measurements, and the unequal number of repeated measures. By defining within-subject variability using pairwise distances between repeated measures and extending Fr\'echet analysis of variance, we develop a generalized Fr\'echet test for exchangeable repeated random objects, applicable to general metric space-valued data with unequal numbers of repeated measures. The proposed test can simultaneously detect differences in location, scale, and within-subject variability. We derive the asymptotic distribution of the test statistic, which follows a weighted chi-squared distribution. Simulations demonstrate that the proposed test performs well across different types of random objects. We illustrate its effectiveness through applications to physical activity data and resting-state functional magnetic resonance imaging data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01514v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingru Zhang, Shengjie Zhang, Christopher W Jones, Mathias Basner, Haochang Shou</dc:creator>
    </item>
    <item>
      <title>Subgroup learning in functional regression models under the RKHS framework</title>
      <link>https://arxiv.org/abs/2503.01515</link>
      <description>arXiv:2503.01515v1 Announce Type: new 
Abstract: Motivated by the inherent heterogeneity observed in many functional or imaging datasets, this paper focuses on subgroup learning in functional or image responses. While change-plane analysis has demonstrated empirical success in practice, the existing methodology is confined to scalar or longitudinal data. In this paper, we propose a novel framework for estimation, identifying, and testing the existence of subgroups in the functional or image response through the change-plane method. The asymptotic theories of the functional parameters are established based on the vector-valued Reproducing Kernel Hilbert Space (RKHS), and the asymptotic properties of the change-plane estimators are derived by a smoothing method since the objective function is nonconvex concerning the change-plane. A novel test statistic is proposed for testing the existence of subgroups, and its asymptotic properties are established under both the null hypothesis and local alternative hypotheses. Numerical studies have been conducted to elucidate the finite-sample performance of the proposed estimation and testing algorithms. Furthermore, an empirical application to the COVID-19 dataset is presented for comprehensive illustration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01515v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Guan, Yiyuan Li, Xu Liu, Jinhong You</dc:creator>
    </item>
    <item>
      <title>Nonparanormal Adjusted Marginal Inference</title>
      <link>https://arxiv.org/abs/2503.01657</link>
      <description>arXiv:2503.01657v1 Announce Type: new 
Abstract: Treatment effects for assessing the efficacy of a novel therapy are typically defined as measures comparing the marginal outcome distributions observed in two or more study arms. Although one can estimate such effects from the observed outcome distributions obtained from proper randomisation, covariate adjustment is recommended to increase precision in randomised clinical trials. For important treatment effects, such as odds or hazard ratios, conditioning on covariates in binary logistic or proportional hazards models changes the interpretation of the treatment effect under noncollapsibility and conditioning on different sets of covariates renders the resulting effect estimates incomparable.
  We propose a novel nonparanormal model formulation for adjusted marginal inference. This model for the joint distribution of outcome and covariates directly features a marginally defined treatment effect parameter, such as a marginal odds or hazard ratio. Marginal distributions are modelled by transformation models allowing broad applicability to diverse outcome types. Joint maximum likelihood estimation of all model parameters is performed. From the parameters not only the marginal treatment effect of interest can be identified but also an overall coefficient of determination and covariate-specific measures of prognostic strength can be derived. A reference implementation of this novel method is available in R add-on package tram.
  For the special case of Cohen's standardised mean difference d, we theoretically show that adjusting for an informative prognostic variable improves the precision of this marginal, noncollapsible effect. Empirical results confirm this not only for Cohen's d but also for log-odds ratios and log-hazard ratios in simulations and three applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01657v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susanne Dandl, Torsten Hothorn</dc:creator>
    </item>
    <item>
      <title>Diagnostic tools for exploring differences in distributional properties between two samples: nonparametric approach</title>
      <link>https://arxiv.org/abs/2503.01671</link>
      <description>arXiv:2503.01671v1 Announce Type: new 
Abstract: This paper reconsiders the problem of testing the equality of two unspecified continuous distributions. The framework, which we propose, allows for readable and insightful data visualisation and helps to understand and quantify how two groups of data differ. We consider a useful weighted rank empirical process on (0,1) and utilise a grid-based approach, based on diadic partitions of (0,1), to discretize the continuous process and construct local simultaneous acceptance regions. These regions help to identify statistically significant deviations from the null model. In addition, the form of the process and its dicretization lead to a highly interpretable visualisation of distributional differences. We also introduce a new two-sample test, explicitly related to the visualisation. Numerical studies show that the new test procedure performs very well. We illustrate the use and diagnostic capabilities of our approach by an application to a known set of neuroscience data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01671v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bogdan \'Cmiel, Teresa Ledwina</dc:creator>
    </item>
    <item>
      <title>Wild posteriors in the wild</title>
      <link>https://arxiv.org/abs/2503.00239</link>
      <description>arXiv:2503.00239v1 Announce Type: cross 
Abstract: Bayesian posterior approximation has become more accessible to practitioners than ever, thanks to modern black-box software. While these tools provide highly accurate approximations with minimal user effort, certain posterior geometries remain notoriously difficult for standard methods. As a result, research into alternative approximation techniques continues to flourish. In many papers, authors validate their new approaches by testing them on posterior shapes deemed challenging or "wild." However, these shapes are not always directly linked to real-world applications where they naturally occur. In this note, we present examples of practical applications that give rise to some commonly used benchmark posterior shapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00239v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunyi Shen, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Semi-Parametric Batched Global Multi-Armed Bandits with Covariates</title>
      <link>https://arxiv.org/abs/2503.00565</link>
      <description>arXiv:2503.00565v1 Announce Type: cross 
Abstract: The multi-armed bandits (MAB) framework is a widely used approach for sequential decision-making, where a decision-maker selects an arm in each round with the goal of maximizing long-term rewards. Moreover, in many practical applications, such as personalized medicine and recommendation systems, feedback is provided in batches, contextual information is available at the time of decision-making, and rewards from different arms are related rather than independent. We propose a novel semi-parametric framework for batched bandits with covariates and a shared parameter across arms, leveraging the single-index regression (SIR) model to capture relationships between arm rewards while balancing interpretability and flexibility. Our algorithm, Batched single-Index Dynamic binning and Successive arm elimination (BIDS), employs a batched successive arm elimination strategy with a dynamic binning mechanism guided by the single-index direction. We consider two settings: one where a pilot direction is available and another where the direction is estimated from data, deriving theoretical regret bounds for both cases. When a pilot direction is available with sufficient accuracy, our approach achieves minimax-optimal rates (with $d = 1$) for nonparametric batched bandits, circumventing the curse of dimensionality. Extensive experiments on simulated and real-world datasets demonstrate the effectiveness of our algorithm compared to the nonparametric batched bandit method introduced by \cite{jiang2024batched}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00565v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakshi Arya, Hyebin Song</dc:creator>
    </item>
    <item>
      <title>Causal Inference on Outcomes Learned from Text</title>
      <link>https://arxiv.org/abs/2503.00725</link>
      <description>arXiv:2503.00725v1 Announce Type: cross 
Abstract: We propose a machine-learning tool that yields causal inference on text in randomized trials. Based on a simple econometric framework in which text may capture outcomes of interest, our procedure addresses three questions: First, is the text affected by the treatment? Second, which outcomes is the effect on? And third, how complete is our description of causal effects? To answer all three questions, our approach uses large language models (LLMs) that suggest systematic differences across two groups of text documents and then provides valid inference based on costly validation. Specifically, we highlight the need for sample splitting to allow for statistical validation of LLM outputs, as well as the need for human labeling to validate substantive claims about how documents differ across groups. We illustrate the tool in a proof-of-concept application using abstracts of academic manuscripts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00725v1</guid>
      <category>econ.EM</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iman Modarressi, Jann Spiess, Amar Venugopal</dc:creator>
    </item>
    <item>
      <title>Cosmic Strings-induced CMB anisotropies in light of Weighted Morphology</title>
      <link>https://arxiv.org/abs/2503.00758</link>
      <description>arXiv:2503.00758v1 Announce Type: cross 
Abstract: Motivated by the morphological measures in assessing the geometrical and topological properties of a generic cosmological stochastic field, we propose an extension of the weighted morphological measures, specifically the $n$th conditional moments of derivative (cmd-$n$). This criterion assigns a distinct weight to each excursion set point based on the associated field. We apply the cmd-$n$ on the Cosmic Microwave Background (CMB) to identify the cosmic string networks (CSs) through their unique Gott-Kaiser-Stebbins effect on the temperature anisotropies. We also formulate the perturbative expansion of cmd-$n$ for the weak non-Gaussian regime up to $\mathcal{O}(\sigma_0^3)$. We propose a comprehensive pipeline designed for analyzing the morphological properties of string-induced CMB maps within the flat sky approximation. To evaluate the robustness of our proposed criteria, we employ string-induced high-resolution flat-sky CMB simulated patches of $7.2$ deg$^2$ size with a resolution of $0.42$ arc-minutes. Our results demonstrate that the minimum detectable value of cosmic string tension is $G\mu\gtrsim 1.9\times 10^{-7}$ when a noise-free map is analyzed with normalized cmd-$n$. Whereas for the ACT, CMB-S4, and Planck-like experiments at 95.45\% confidence level, the normalized cmd-$n$ can distinguish the CSs network for $G\mu\gtrsim2.9 \times 10^{-7}$, $G\mu\gtrsim 2.4\times 10^{-7}$ and $G\mu\gtrsim 5.8\times 10^{-7}$, respectively. The normalized cmd-$n$ exhibits a significantly enhanced capability in the detection of CSs relative to the Minkowski Functionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00758v1</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>physics.comp-ph</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adeela Afzal, M. Alakhras, M. H. Jalali Kanafi, S. M. S. Movahed</dc:creator>
    </item>
    <item>
      <title>Vector Copula Variational Inference and Dependent Block Posterior Approximations</title>
      <link>https://arxiv.org/abs/2503.01072</link>
      <description>arXiv:2503.01072v1 Announce Type: cross 
Abstract: Variational inference (VI) is a popular method to estimate statistical and econometric models. The key to VI is the selection of a tractable density to approximate the Bayesian posterior. For large and complex models a common choice is to assume independence between multivariate blocks in a partition of the parameter space. While this simplifies the problem it can reduce accuracy. This paper proposes using vector copulas to capture dependence between the blocks parsimoniously. Tailored multivariate marginals are constructed using learnable cyclically monotone transformations. We call the resulting joint distribution a ``dependent block posterior'' approximation. Vector copula models are suggested that make tractable and flexible variational approximations. They allow for differing marginals, numbers of blocks, block sizes and forms of between block dependence. They also allow for solution of the variational optimization using fast and efficient stochastic gradient methods. The efficacy and versatility of the approach is demonstrated using four different statistical models and 16 datasets which have posteriors that are challenging to approximate. In all cases, our method produces more accurate posterior approximations than benchmark VI methods that either assume block independence or factor-based dependence, at limited additional computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01072v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yu Fu, Michael Stanley Smith, Anastasios Panagiotelis</dc:creator>
    </item>
    <item>
      <title>Can Large Language Models Help Experimental Design for Causal Discovery?</title>
      <link>https://arxiv.org/abs/2503.01139</link>
      <description>arXiv:2503.01139v1 Announce Type: cross 
Abstract: Designing proper experiments and selecting optimal intervention targets is a longstanding problem in scientific or causal discovery. Identifying the underlying causal structure from observational data alone is inherently difficult.Obtaining interventional data, on the other hand, is crucial to causal discovery, yet it is usually expensive and time-consuming to gather sufficient interventional data to facilitate causal discovery.Previous approaches commonly utilize uncertainty or gradient signals to determine the intervention targets. However, numerical-based approaches may yield suboptimal results due to the inaccurate estimation of the guiding signals at the beginning when with limited interventional data. In this work, we investigate a different approach, whether we can leverage Large Language Models (LLMs) to assist with the intervention targeting in causal discovery by making use of the rich world knowledge about the experimental design in LLMs.Specifically, we present \oursfull (\ours) -- a robust framework that effectively incorporates LLMs to augment existing numerical approaches for the intervention targeting in causal discovery. Across $4$ realistic benchmark scales, \ours demonstrates significant improvements and robustness over existing methods and even surpasses humans, which demonstrates the usefulness of LLMs in assisting with experimental design for scientific discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01139v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyi Li, Yongqiang Chen, Chenxi Liu, Qianyi Cai, Tongliang Liu, Bo Han, Kun Zhang, Hui Xiong</dc:creator>
    </item>
    <item>
      <title>DeepSuM: Deep Sufficient Modality Learning Framework</title>
      <link>https://arxiv.org/abs/2503.01728</link>
      <description>arXiv:2503.01728v1 Announce Type: cross 
Abstract: Multimodal learning has become a pivotal approach in developing robust learning models with applications spanning multimedia, robotics, large language models, and healthcare. The efficiency of multimodal systems is a critical concern, given the varying costs and resource demands of different modalities. This underscores the necessity for effective modality selection to balance performance gains against resource expenditures. In this study, we propose a novel framework for modality selection that independently learns the representation of each modality. This approach allows for the assessment of each modality's significance within its unique representation space, enabling the development of tailored encoders and facilitating the joint analysis of modalities with distinct characteristics. Our framework aims to enhance the efficiency and effectiveness of multimodal learning by optimizing modality integration and selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01728v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Gao, Jian Huang, Ting Li, Xueqin Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian variance change point detection with credible sets</title>
      <link>https://arxiv.org/abs/2211.14097</link>
      <description>arXiv:2211.14097v3 Announce Type: replace 
Abstract: This paper introduces a novel Bayesian approach to detect changes in the variance of a Gaussian sequence model, focusing on quantifying the uncertainty in the change point locations and providing a scalable algorithm for inference. Such a measure of uncertainty is necessary when change point methods are deployed in sensitive applications, for example, when one is interested in determining whether an organ is viable for transplant. The key of our proposal is framing the problem as a product of multiple single changes in the scale parameter. We fit the model through an iterative procedure similar to what is done for additive models. The novelty is that each iteration returns a probability distribution on time instances, which captures the uncertainty in the change point location. Leveraging a recent result in the literature, we can show that our proposal is a variational approximation of the exact model posterior distribution. We study the algorithm's convergence and the change point localization rate. Extensive experiments in simulation studies illustrate the performance of our method and the possibility of generalizing it to more complex data-generating mechanisms. We apply the new model to an experiment involving a novel technique to assess the viability of a liver and oceanographic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.14097v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Cappello, Oscar Hernan Madrid Padilla</dc:creator>
    </item>
    <item>
      <title>A Differential Effect Approach to Partial Identification of Treatment Effects</title>
      <link>https://arxiv.org/abs/2303.06332</link>
      <description>arXiv:2303.06332v4 Announce Type: replace 
Abstract: We consider identification and inference for the average treatment effect and heterogeneous treatment effect conditional on observable covariates in the presence of unmeasured confounding. Since point identification of these treatment effects is not achievable without strong assumptions, we obtain bounds on these treatment effects by leveraging differential effects, a tool that allows for using a second treatment to learn the effect of the first treatment. The differential effect is the effect of using one treatment in lieu of the other. We provide conditions under which differential treatment effects can be used to point identify or partially identify treatment effects. Under these conditions, we develop a flexible and easy-to-implement semi-parametric framework to estimate bounds and leverage a two-stage approach to conduct statistical inference on effects of interest. The proposed method is examined through a simulation study and a case study that investigates the effect of smoking on the blood level of cadmium using the National Health and Nutrition Examination Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.06332v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kan Chen, Jeffrey Zhang, Bingkai Wang, Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>Change Point Detection on A Separable Model for Dynamic Networks</title>
      <link>https://arxiv.org/abs/2303.17642</link>
      <description>arXiv:2303.17642v4 Announce Type: replace 
Abstract: This paper studies the unsupervised change point detection problem in time series of networks using the Separable Temporal Exponential-family Random Graph Model (STERGM). Inherently, dynamic network patterns can be complex due to dyadic and temporal dependence, and change points detection can identify the discrepancies in the underlying data generating processes to facilitate downstream analysis. Moreover, the STERGM that utilizes network statistics to represent the structural patterns is a flexible and parsimonious model to fit dynamic networks. We propose a new estimator derived from the Alternating Direction Method of Multipliers (ADMM) procedure and Group Fused Lasso (GFL) regularization to simultaneously detect multiple time points, where the parameters of a time-heterogeneous STERGM have changed. We also provide a Bayesian information criterion for model selection and an R package CPDstergm to implement the proposed method. Experiments on simulated and real data show good performance of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.17642v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yik Lun Kei, Hangjian Li, Yanzhen Chen, Oscar Hernan Madrid Padilla</dc:creator>
    </item>
    <item>
      <title>High-dimensional iterative variable selection for accelerated failure time models</title>
      <link>https://arxiv.org/abs/2304.11902</link>
      <description>arXiv:2304.11902v2 Announce Type: replace 
Abstract: We propose an iterative variable selection method for the accelerated failure time model using high-dimensional survival data. Our method pioneers the use of the recently proposed structured screen-and-select framework for survival analysis. We use the marginal utility as the measure of association to inform the structured screening process. For the selection steps, we use Bayesian model selection based on non-local priors. We compare the proposed method with a few well-known methods. Assessment in terms of true positive rate and false discovery rate shows the usefulness of our method. We have implemented the method within the R package GWASinlps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.11902v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nilotpal Sanyal</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Spatial-temporal Non-Gaussian Data Using Predictive Stacking</title>
      <link>https://arxiv.org/abs/2406.04655</link>
      <description>arXiv:2406.04655v2 Announce Type: replace 
Abstract: Analysing non-Gaussian spatial-temporal data typically requires introducing spatial dependence in generalised linear models through the link function of an exponential family distribution. However, unlike in Gaussian likelihoods, inference is considerably encumbered by the inability to analytically integrate out the random effects and reduce the dimension of the parameter space. Iterative estimation algorithms struggle to converge due to the presence of weakly identified parameters. We devise an approach that obviates these issues by exploiting generalised conjugate multivariate distribution theory for exponential families, which enables exact sampling from analytically available posterior distributions conditional upon some fixed process parameters. More specifically, we expand upon the Diaconis-Ylvisaker family of conjugate priors to achieve analytically tractable posterior inference for spatially-temporally varying regression models conditional on some kernel parameters. Subsequently, we assimilate inference from these individual posterior distributions over a range of values of these parameters using Bayesian predictive stacking. We evaluate inferential performance on simulated data, compare with fully Bayesian inference using Markov chain Monte Carlo and apply our proposed method to analyse spatially-temporally referenced avian count data from the North American Breeding Bird Survey database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04655v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumyakanti Pan, Lu Zhang, Jonathan R. Bradley, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>An Empirical Bayes Jackknife Regression Framework for Covariance Matrix Estimation</title>
      <link>https://arxiv.org/abs/2406.13876</link>
      <description>arXiv:2406.13876v3 Announce Type: replace 
Abstract: Covariance matrix estimation, a classical statistical topic, poses significant challenges when the sample size is comparable to or smaller than the number of features. In this paper, we frame covariance matrix estimation as a compound decision problem and apply an optimal decision rule to estimate covariance parameters. To approximate this rule, we introduce an algorithm that integrates jackknife techniques with machine learning regression methods. This algorithm exhibits adaptability across diverse scenarios without relying on assumptions about data distribution. Simulation results and gene network inference from an RNA-seq experiment in mice demonstrate that our approach either matches or surpasses several state-of-the-art methods</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13876v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huqin Xin, Sihai Dave Zhao</dc:creator>
    </item>
    <item>
      <title>Monte Carlo Integration in Simple and Complex Simulation Designs</title>
      <link>https://arxiv.org/abs/2406.15285</link>
      <description>arXiv:2406.15285v3 Announce Type: replace 
Abstract: Simulation studies are used to evaluate and compare the properties of statistical methods in controlled experimental settings. In most cases, performing a simulation study requires knowledge of the true value of the parameter, or estimand, of interest. However, in many simulation designs, the true value of the estimand is difficult to compute analytically. Here, we illustrate the use of Monte Carlo integration to compute true estimand values in simple and complex simulation designs. We provide general pseudocode that can be replicated in any software program of choice to demonstrate key principles in using Monte Carlo integration in two scenarios: a simple three variable simulation where interest lies in the marginally adjusted odds ratio; and a more complex causal mediation analysis where interest lies in the controlled direct effect in the presence of mediator-outcome confounders affected by the exposure. We discuss general strategies that can be used to minimize Monte Carlo error, and to serve as checks on the simulation program to avoid coding errors. R programming code is provided illustrating the application of our pseudocode in these settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15285v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashley I. Naimi, David Benkeser, Jacqueline E. Rudolph</dc:creator>
    </item>
    <item>
      <title>Exploring the difficulty of estimating win probability: a simulation study</title>
      <link>https://arxiv.org/abs/2406.16171</link>
      <description>arXiv:2406.16171v4 Announce Type: replace 
Abstract: Estimating win probability is one of the classic modeling tasks of sports analytics. Many widely used win probability estimators use machine learning to fit the relationship between a binary win/loss outcome variable and certain game-state variables. To illustrate just how difficult it is to accurately fit such a model from noisy and highly correlated observational data, in this paper we conduct a simulation study. We create a simplified random walk version of football in which true win probability at each game-state is known, and we see how well a model recovers it. We find that the dependence structure of observational play-by-play data substantially inflates the bias and variance of estimators and lowers the effective sample size. Further, to achieve approximately valid marginal coverage, win probability confidence intervals need to be substantially wide. Concisely, these are high variance estimators subject to substantial uncertainty. Our findings are not unique to the particular application of estimating win probability; they are broadly applicable across sports analytics, as myriad other sports datasets are clustered into groups of observations that share the same outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16171v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Ronald Yurko, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Covariate-dependent hierarchical Dirichlet processes</title>
      <link>https://arxiv.org/abs/2407.02676</link>
      <description>arXiv:2407.02676v3 Announce Type: replace 
Abstract: Bayesian hierarchical modelling is a natural framework to effectively integrate data and borrow information across groups. In this paper, we delve into problems related to density estimation and identifying clusters across related groups, proposing a Bayesian approach that extends existing approaches in the presence of additional covariate information. To achieve flexibility, our approach is built on ideas from Bayesian nonparametrics, combining the hierarchical Dirichlet process and dependent Dirichlet process. The proposed model is general, accommodating multiple and mixed covariate types through appropriate kernel functions as well as different output types through suitable likelihoods. This extends our ability to discern the relationship between covariates and clusters, while effectively borrowing information across groups. By employing a data augmentation trick, we are able to tackle the intractable normalized weights and construct a Markov chain Monte Carlo (MCMC) algorithm for posterior inference. The utility of the method is illustrated on simulated data and two real data sets on single-cell RNA sequencing (scRNA-seq) and calcium imaging for studying neuronal activity. For scRNA-seq data, we show that the incorporation of cell dynamics facilitates the discovery of cell subgroups. On calcium imaging data, our method identifies meaningful clusters of time frames with similar neural activity, that aligns with the observed behaviour of the mouse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02676v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Huizi Zhang, Sara Wade, Natalia Bochkina</dc:creator>
    </item>
    <item>
      <title>Robust Score-Based Quickest Change Detection</title>
      <link>https://arxiv.org/abs/2407.11094</link>
      <description>arXiv:2407.11094v3 Announce Type: replace 
Abstract: Methods in the field of quickest change detection rapidly detect in real-time a change in the data-generating distribution of an online data stream. Existing methods have been able to detect this change point when the densities of the pre- and post-change distributions are known. Recent work has extended these results to the case where the pre- and post-change distributions are known only by their score functions. This work considers the case where the pre- and post-change score functions are known only to correspond to distributions in two disjoint sets. This work employs a pair of "least-favorable" distributions to robustify the existing score-based quickest change detection algorithm, the properties of which are studied. This paper calculates the least-favorable distributions for specific model classes and provides methods of estimating the least-favorable distributions for common constructions. Simulation results are provided demonstrating the performance of our robust change detection algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11094v3</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sean Moushegian, Suya Wu, Enmao Diao, Jie Ding, Taposh Banerjee, Vahid Tarokh</dc:creator>
    </item>
    <item>
      <title>Stratification in Randomised Clinical Trials and Analysis of Covariance: Some Simple Theory and Recommendations</title>
      <link>https://arxiv.org/abs/2408.06760</link>
      <description>arXiv:2408.06760v2 Announce Type: replace 
Abstract: A simple device for balancing for a continuous covariate in clinical trials is to stratify by whether the covariate is above or below some target value, typically the predicted median. This raises an issue as to which model should be used for modelling the effect of treatment on the outcome variable, $Y$. Should one fit, the stratum indicator, $S$, the continuous covariate, $X$, both or neither?
  This question has sometimes been investigated using simulations targeting the overall effect on inferences about treatment, in terms, for example, of power for a given alternative hypothesis. However, when a covariate is added to a linear model there are three consequences for inference: 1) the mean square error effect, 2) the variance inflation factor and 3) second order precision. We consider that it is valuable to consider these three factors separately, even if, ultimately, it is their joint effect that matters. We present some simple theory, concentrating in particular on the variance inflation factor, that may be used to guide trialists in their choice of model. We also consider the case where the precise form of the relationship between the outcome and the covariate is not known. We conclude by recommending that the continuous covariate should always be in the model but that, depending on circumstances, there may be some justification in fitting the stratum indicator also.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06760v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stephen Senn, Franz K\"onig, Martin Posch</dc:creator>
    </item>
    <item>
      <title>Air-HOLP: Adaptive Regularized Feature Screening for High Dimensional Correlated Data</title>
      <link>https://arxiv.org/abs/2408.13000</link>
      <description>arXiv:2408.13000v2 Announce Type: replace 
Abstract: Handling high-dimensional datasets presents substantial computational challenges, particularly when the number of features far exceeds the number of observations and when features are highly correlated. A modern approach to mitigate these issues is feature screening. In this work, the High-dimensional Ordinary Least-squares Projection (HOLP) feature screening method is advanced by employing adaptive ridge regularization. The impact of the ridge tuning parameter on the Ridge-HOLP method is examined and Adaptive iterative ridge-HOLP (Air-HOLP) is proposed, a data-adaptive advance to Ridge-HOLP where the ridge-regularization tuning parameter is selected iteratively and optimally for better feature screening performance. The proposed method addresses the challenges of tuning parameter selection in high dimensions by offering a computationally efficient and stable alternative to traditional methods like bootstrapping and cross-validation. Air-HOLP is evaluated using simulated data and a prostate cancer genetic dataset. The empirical results demonstrate that Air-HOLP has improved performance over a large range of simulation settings. We provide R codes implementing the Air-HOLP feature screening method and integrating it into existing feature screening methods that utilize the HOLP formula.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13000v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ibrahim Joudah, Samuel Muller, Houying Zhu</dc:creator>
    </item>
    <item>
      <title>Group Difference in Differences can Identify Effect Heterogeneity in Non-Canonical Settings</title>
      <link>https://arxiv.org/abs/2408.16039</link>
      <description>arXiv:2408.16039v3 Announce Type: replace 
Abstract: Consider a very general setting in which data on an outcome of interest is collected in two `groups' at two time periods, with certain group-periods deemed `treated' and others `untreated'. A special case is the canonical Difference-in-Differences (DiD) setting in which one group is treated only in the second period while the other is treated in neither period. Then it is well known that under a parallel trends assumption across the two groups the classic DiD formula (subtracting the average change in outcome across periods in the treated group by the average change in the outcome across periods in the untreated group) identifies the average treatment effect on the treated in the second period. But other relations between group, period, and treatment are possible. For example, the groups might be demographic (or other baseline covariate) categories with all units in both groups treated in the second period and none treated in the first, i.e. a pre-post design. Or one group might be treated in both periods while the other is treated in neither. Furthermore, other parallel trends assumptions under other treatment regimes are possible. For example, we could assume the two groups' potential outcomes would evolve in parallel under a regime of `untreated in the first period and treated in the second period'. In fact, there is a literal array of data structures and parallel trends assumptions. The difference between the changes in outcomes of the two groups, which we dub the `group DiD' (gDiD) formula, will identify different causal estimands depending on the data structure and parallel trends assumption adopted. In this paper, we determine under which combinations of data structure and parallel trends assumptions the gDiD formula identifies meaningful causal estimands. We also explore when non-canonical parallel trends assumptions are amenable to empirical checks or structural justification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16039v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zach Shahn, Laura Hatfield</dc:creator>
    </item>
    <item>
      <title>Just Ramp-up: Unleash the Potential of Regression-based Estimator for A/B Tests under Network Interference</title>
      <link>https://arxiv.org/abs/2410.12740</link>
      <description>arXiv:2410.12740v2 Announce Type: replace 
Abstract: Recent research in causal inference under network interference has explored various experimental designs and estimation techniques to address this issue. However, existing methods, which typically rely on single experiments, often reach a performance bottleneck and face limitations in handling diverse interference structures. In contrast, we propose leveraging multiple experiments to overcome these limitations. In industry, the use of sequential experiments, often known as the ramp-up process, where traffic to the treatment gradually increases, is common due to operational needs like risk management and cost control. Our approach shifts the focus from operational aspects to the statistical advantages of merging data from multiple experiments. By combining data from sequentially conducted experiments, we aim to estimate the global average treatment effect more effectively. In this paper, we begin by analyzing the bias and variance of the linear regression estimator for GATE under general linear network interference. We demonstrate that bias plays a dominant role in the bias-variance tradeoff and highlight the intrinsic bias reduction achieved by merging data from experiments with strictly different treatment proportions. Herein the improvement introduced by merging two steps of experimental data is essential. In addition, we show that merging more steps of experimental data is unnecessary under general linear interference, while it can become beneficial when nonlinear interference occurs. Furthermore, we look into a more advanced estimator based on graph neural networks. Through extensive simulation studies, we show that the regression-based estimator benefits remarkably from training on merged experiment data, achieving outstanding statistical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12740v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianyi Chen, Bo Li</dc:creator>
    </item>
    <item>
      <title>Asymptotic theory of the quadratic assignment procedure for dyadic data analysis</title>
      <link>https://arxiv.org/abs/2411.00947</link>
      <description>arXiv:2411.00947v2 Announce Type: replace 
Abstract: The quadratic assignment procedure (QAP) is a popular tool for analyzing dyadic data in medical and social sciences. To test the association between two dyadic measurements represented by two symmetric matrices, QAP calculates the p-value by permuting the units, or equivalently, by permuting the rows and columns of one matrix in the same way. Its extension to the regression setting, known as the multiple regression QAP, has also gained popularity, especially in psychometrics. However, the statistics theory for QAP has not been fully established in the literature. We fill the gap in this paper. We formulate the network models underlying various QAPs. We derive (a) the asymptotic sampling distributions of some canonical test statistics and (b) the corresponding asymptotic permutation distributions induced by QAP under strong and weak null hypotheses. Task (a) relies on applying the theory of U-statistics, and task (b) relies on applying the theory of double-indexed permutation statistics. The combination of tasks (a) and (b) provides a relatively complete picture of QAP. Overall, our asymptotic theory suggests that using properly studentized statistics in QAP is a robust choice in that it is finite-sample exact under the strong null hypothesis and preserves the asymptotic type one error rate under the weak null hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00947v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lei Shi, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Marginally interpretable spatial logistic regression with bridge processes</title>
      <link>https://arxiv.org/abs/2412.04744</link>
      <description>arXiv:2412.04744v2 Announce Type: replace 
Abstract: In including random effects to account for dependent observations, the odds ratio interpretation of logistic regression coefficients is changed from population-averaged to subject-specific. This is unappealing in many applications, motivating a rich literature on methods that maintain the marginal logistic regression structure without random effects, such as generalized estimating equations. However, for spatial data, random effect approaches are appealing in providing a full probabilistic characterization of the data that can be used for prediction. We propose a new class of spatial logistic regression models that maintain both population-averaged and subject-specific interpretations through a novel class of bridge processes for spatial random effects. These processes are shown to have appealing computational and theoretical properties, including a scale mixture of normal representation. The new methodology is illustrated with simulations and an analysis of childhood malaria prevalence data in the Gambia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04744v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changwoo J. Lee, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Bayesian penalized empirical likelihood and Markov Chain Monte Carlo sampling</title>
      <link>https://arxiv.org/abs/2412.17354</link>
      <description>arXiv:2412.17354v3 Announce Type: replace 
Abstract: In this study, we introduce a novel methodological framework called Bayesian Penalized Empirical Likelihood (BPEL), designed to address the computational challenges inherent in empirical likelihood (EL) approaches. Our approach has two primary objectives: (i) to enhance the inherent flexibility of EL in accommodating diverse model conditions, and (ii) to facilitate the use of well-established Markov Chain Monte Carlo (MCMC) sampling schemes as a convenient alternative to the complex optimization typically required for statistical inference using EL. To achieve the first objective, we propose a penalized approach that regularizes the Lagrange multipliers, significantly reducing the dimensionality of the problem while accommodating a comprehensive set of model conditions. For the second objective, our study designs and thoroughly investigates two popular sampling schemes within the BPEL context. We demonstrate that the BPEL framework is highly flexible and efficient, enhancing the adaptability and practicality of EL methods. Our study highlights the practical advantages of using sampling techniques over traditional optimization methods for EL problems, showing rapid convergence to the global optima of posterior distributions and ensuring the effective resolution of complex statistical inference challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17354v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Cheng Yong Tang, Yuanzheng Zhu</dc:creator>
    </item>
    <item>
      <title>A Bi-failure Mode Model for Competing Risk Modeling with HMC-Driven Bayesian Framework</title>
      <link>https://arxiv.org/abs/2502.11507</link>
      <description>arXiv:2502.11507v2 Announce Type: replace 
Abstract: Bathtub failure rate (BFR) and roller-coaster failure rate (RCFR - a sequence of BFR and inverted BFR (IBFR)) shapes are among the non-monotone failure rate function (FRF) behaviors often observed in complex or competing risks (CR) datasets. Recent studies have introduced varied bathtub failure rate models for reliability modeling of such datasets. However, limited attention is paid to the reliability study of CR datasets characterized by RCFR. Motivated by this drawback, this paper proposes the so-called Bi-Failure Modes (BFM) model for robust reliability analysis of CR data exhibiting BFR, RCFR, and several other FRF shapes. The mean residual life function (MRLF) and cause-specific failure probabilities are studied in detail. The fundamental reciprocal relationships between the MRLF and FRF are established. We propose the Hamiltonian Monte Carlo (HMC)-based Bayesian framework for estimating the BFM parameters and its reliability attributes to offer greater computational efficiency and faster inference. Two CR datasets from electrode voltage endurance life and electrical appliance tests, respectively, characterized by BFR and RCFR behaviors, are employed to demonstrate the BFM adequacy. The recently introduced Bridge Criterion (BC) metric and other metrics are used to evaluate the BFM modeling performance against five recent methodologies under the maximum likelihood technique. The BFM compatibility with the two datasets is also examined. The findings portrayed the BFM's advantage over other competing candidates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11507v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Badamasi Abba, Mustapha Muhammad, Muhammad Salihu Isa, Jinbiao Wu</dc:creator>
    </item>
    <item>
      <title>Principled priors for Bayesian inference of circular models</title>
      <link>https://arxiv.org/abs/2502.18223</link>
      <description>arXiv:2502.18223v2 Announce Type: replace 
Abstract: Advancements in computational power and methodologies have enabled research on massive datasets. However, tools for analyzing data with directional or periodic characteristics, such as wind directions and customers' arrival time in 24-hour clock, remain underdeveloped. While statisticians have proposed circular distributions for such analyses, significant challenges persist in constructing circular statistical models, particularly in the context of Bayesian methods. These challenges stem from limited theoretical development and a lack of historical studies on prior selection for circular distribution parameters.
  In this article, we propose a principled, practical and systematic framework for selecting priors that effectively prevents overfitting in circular scenarios, especially when there is insufficient information to guide prior selection. We introduce well-examined Penalized Complexity (PC) priors for the most widely used circular distributions. Comprehensive comparisons with existing priors in the literature are conducted through simulation studies and a practical case study. Finally, we discuss the contributions and implications of our work, providing a foundation for further advancements in constructing Bayesian circular statistical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18223v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiang Ye, Janet Van Niekerk, H\r{a}vard Rue</dc:creator>
    </item>
    <item>
      <title>Optimal Decision Rules Under Partial Identification</title>
      <link>https://arxiv.org/abs/2111.04926</link>
      <description>arXiv:2111.04926v4 Announce Type: replace-cross 
Abstract: I consider a class of statistical decision problems in which the policymaker must decide between two policies to maximize social welfare (e.g., the population mean of an outcome) based on a finite sample. The framework introduced in this paper allows for various types of restrictions on the structural parameter (e.g., the smoothness of a conditional mean potential outcome function) and accommodates settings with partial identification of social welfare. As the main theoretical result, I derive a finite-sample optimal decision rule under the minimax regret criterion. This rule has a simple form, yet achieves optimality among all decision rules; no ad hoc restrictions are imposed on the class of decision rules. I apply my results to the problem of whether to change an eligibility cutoff in a regression discontinuity setup, and illustrate them in an empirical application to a school construction program in Burkina Faso.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.04926v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kohei Yata</dc:creator>
    </item>
    <item>
      <title>A direct extension of Azadkia &amp; Chatterjee's rank correlation to multi-response vectors</title>
      <link>https://arxiv.org/abs/2212.01621</link>
      <description>arXiv:2212.01621v4 Announce Type: replace-cross 
Abstract: Recently, Chatterjee (2023) recognized the lack of a direct generalization of his rank correlation $\xi$ in Azadkia and Chatterjee (2021) to a multi-dimensional response vector. As a natural solution to this problem, we here propose an extension of $\xi$ that is applicable to a set of $q \geq 1$ response variables, where our approach builds upon converting the original vector-valued problem into a univariate problem and then applying the rank correlation $\xi$ to it. Our novel measure $T$ quantifies the scale-invariant extent of functional dependence of a response vector $\mathbf{Y} = (Y_1,\dots,Y_q)$ on predictor variables $\mathbf{X} = (X_1, \dots,X_p)$, characterizes independence of $\mathbf{X}$ and $\mathbf{Y}$ as well as perfect dependence of $\mathbf{Y}$ on $\mathbf{X}$ and hence fulfills all the characteristics of a measure of predictability. Aiming at maximum interpretability, we provide various invariance results for $T$ as well as a closed-form expression in multivariate normal models. Building upon the graph-based estimator for $\xi$ in Azadkia and Chatterjee (2021), we obtain a non-parametric, strongly consistent estimator for $T$ and show -- as a main contribution -- its asymptotic normality. Based on this estimator, we develop a model-free and rank-based feature ranking and forward feature selection for multiple-outcome data that works without any tuning parameters. Simulation results and real case studies illustrate $T$'s broad applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.01621v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Ansari, Sebastian Fuchs</dc:creator>
    </item>
    <item>
      <title>Semi-parametric inference based on adaptively collected data</title>
      <link>https://arxiv.org/abs/2303.02534</link>
      <description>arXiv:2303.02534v2 Announce Type: replace-cross 
Abstract: Many standard estimators, when applied to adaptively collected data, fail to be asymptotically normal, thereby complicating the construction of confidence intervals. We address this challenge in a semi-parametric context: estimating the parameter vector of a generalized linear regression model contaminated by a non-parametric nuisance component. We construct suitably weighted estimating equations that account for adaptivity in data collection, and provide conditions under which the associated estimates are asymptotically normal. Our results characterize the degree of "explorability" required for asymptotic normality to hold. For the simpler problem of estimating a linear functional, we provide similar guarantees under much weaker assumptions. We illustrate our general theory with concrete consequences for various problems, including standard linear bandits and sparse generalized bandits, and compare with other methods via simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.02534v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Licong Lin, Koulik Khamaru, Martin J. Wainwright</dc:creator>
    </item>
    <item>
      <title>Generalized Random Forests using Fixed-Point Trees</title>
      <link>https://arxiv.org/abs/2306.11908</link>
      <description>arXiv:2306.11908v2 Announce Type: replace-cross 
Abstract: We propose a computationally efficient alternative to generalized random forests arXiv:1610.01271 (GRFs) for estimating heterogeneous effects in large dimensions. While GRFs rely on a gradient-based splitting criterion, which is large dimensions is computationally expensive and unstable, our method introduces a fixed-point approximation that eliminates the need for Jacobian estimation. This gradient-free approach preserves GRFs theoretical guarantees of consistency and asymptotic normality while significantly improving computational efficiency. We demonstrate that our method achieves multiple times the speed over standard GRFs without compromising statistical accuracy. Experiments on both simulated and real-world data, validate our approach. Our findings suggest that the proposed method is a scalable alternative for localized effect estimation in machine learning and causal inference applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11908v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Fleischer, David A. Stephens, Archer Yang</dc:creator>
    </item>
    <item>
      <title>Asymptotic Behavior of Adversarial Training Estimator under $\ell_\infty$-Perturbation</title>
      <link>https://arxiv.org/abs/2401.15262</link>
      <description>arXiv:2401.15262v2 Announce Type: replace-cross 
Abstract: Adversarial training has been proposed to protect machine learning models against adversarial attacks. This paper focuses on adversarial training under $\ell_\infty$-perturbation, which has recently attracted much research attention. The asymptotic behavior of the adversarial training estimator is investigated in the generalized linear model. The results imply that the asymptotic distribution of the adversarial training estimator under $\ell_\infty$-perturbation could put a positive probability mass at $0$ when the true parameter is $0$, providing a theoretical guarantee of the associated sparsity-recovery ability. Alternatively, a two-step procedure is proposed -- adaptive adversarial training, which could further improve the performance of adversarial training under $\ell_\infty$-perturbation. Specifically, the proposed procedure could achieve asymptotic variable-selection consistency and unbiasedness. Numerical experiments are conducted to show the sparsity-recovery ability of adversarial training under $\ell_\infty$-perturbation and to compare the empirical performance between classic adversarial training and adaptive adversarial training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15262v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiling Xie, Xiaoming Huo</dc:creator>
    </item>
    <item>
      <title>DCILP: A Distributed Approach for Large-Scale Causal Structure Learning</title>
      <link>https://arxiv.org/abs/2406.10481</link>
      <description>arXiv:2406.10481v2 Announce Type: replace-cross 
Abstract: Causal learning tackles the computationally demanding task of estimating causal graphs. This paper introduces a new divide-and-conquer approach for causal graph learning, called DCILP. In the divide phase, the Markov blanket MB($X_i$) of each variable $X_i$ is identified, and causal learning subproblems associated with each MB($X_i$) are independently addressed in parallel. This approach benefits from a more favorable ratio between the number of data samples and the number of variables considered. In counterpart, it can be adversely affected by the presence of hidden confounders, as variables external to MB($X_i$) might influence those within it. The reconciliation of the local causal graphs generated during the divide phase is a challenging combinatorial optimization problem, especially in large-scale applications. The main novelty of DCILP is an original formulation of this reconciliation as an integer linear programming (ILP) problem, which can be delegated and efficiently handled by an ILP solver. Through experiments on medium to large scale graphs, and comparisons with state-of-the-art methods, DCILP demonstrates significant improvements in terms of computational complexity, while preserving the learning accuracy on real-world problem and suffering at most a slight loss of accuracy on synthetic problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10481v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyu Dong, Mich\`ele Sebag, Kento Uemura, Akito Fujii, Shuang Chang, Yusuke Koyanagi, Koji Maruhashi</dc:creator>
    </item>
    <item>
      <title>Testing Elliptical Models in High Dimensions</title>
      <link>https://arxiv.org/abs/2408.05514</link>
      <description>arXiv:2408.05514v2 Announce Type: replace-cross 
Abstract: Due to the broad applications of elliptical models, there is a long line of research on goodness-of-fit tests for empirically validating them. However, the existing literature on this topic is generally confined to low-dimensional settings, and to the best of our knowledge, there are no established goodness-of-fit tests for elliptical models that are supported by theoretical guarantees in high dimensions. In this paper, we propose a new goodness-of-fit test for this problem, and our main result shows that the test is asymptotically valid when the dimension and sample size diverge proportionally. Remarkably, it also turns out that the asymptotic validity of the test requires no assumptions on the population covariance matrix. With regard to numerical performance, we confirm that the empirical level of the test is close to the nominal level across a range of conditions, and that the test is able to reliably detect non-elliptical distributions. Moreover, when the proposed test is specialized to the problem of testing normality in high dimensions, we show that it compares favorably with a state-of-the-art method, and hence, this way of using the proposed test is of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05514v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyao Wang, Miles E. Lopes</dc:creator>
    </item>
    <item>
      <title>A Meta-Learning Approach to Bayesian Causal Discovery</title>
      <link>https://arxiv.org/abs/2412.16577</link>
      <description>arXiv:2412.16577v2 Announce Type: replace-cross 
Abstract: Discovering a unique causal structure is difficult due to both inherent identifiability issues, and the consequences of finite data. As such, uncertainty over causal structures, such as those obtained from a Bayesian posterior, are often necessary for downstream tasks. Finding an accurate approximation to this posterior is challenging, due to the large number of possible causal graphs, as well as the difficulty in the subproblem of finding posteriors over the functional relationships of the causal edges. Recent works have used meta-learning to view the problem of estimating the maximum a-posteriori causal graph as supervised learning. Yet, these methods are limited when estimating the full posterior as they fail to encode key properties of the posterior, such as correlation between edges and permutation equivariance with respect to nodes. Further, these methods also cannot reliably sample from the posterior over causal structures. To address these limitations, we propose a Bayesian meta learning model that allows for sampling causal structures from the posterior and encodes these key properties. We compare our meta-Bayesian causal discovery against existing Bayesian causal discovery methods, demonstrating the advantages of directly learning a posterior over causal structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16577v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anish Dhir, Matthew Ashman, James Requeima, Mark van der Wilk</dc:creator>
    </item>
    <item>
      <title>Bayesian calculus and predictive characterizations of extended feature allocation models</title>
      <link>https://arxiv.org/abs/2502.10257</link>
      <description>arXiv:2502.10257v2 Announce Type: replace-cross 
Abstract: We introduce and study a unified Bayesian framework for extended feature allocations which flexibly captures interactions -- such as repulsion or attraction -- among features and their associated weights. We provide a complete Bayesian analysis of the proposed model and specialize our general theory to noteworthy classes of priors. This includes a novel prior based on determinantal point processes, for which we show promising results in a spatial statistics application. Within the general class of extended feature allocations, we further characterize those priors that yield predictive probabilities of discovering new features depending either solely on the sample size or on both the sample size and the distinct number of observed features. These predictive characterizations, known as "sufficientness" postulates, have been extensively studied in the literature on species sampling models starting from the seminal contribution of the English philosopher W.E. Johnson for the Dirichlet distribution. Within the feature allocation setting, existing predictive characterizations are limited to very specific examples; in contrast, our results are general, providing practical guidance for prior selection. Additionally, our approach, based on Palm calculus, is analytical in nature and yields a novel characterization of the Poisson point process through its reduced Palm kernel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10257v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Federico Camerlenghi, Lorenzo Ghilotti</dc:creator>
    </item>
  </channel>
</rss>
