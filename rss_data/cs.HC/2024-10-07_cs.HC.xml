<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Oct 2024 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Conversational Swarms of Humans and AI Agents enable Hybrid Collaborative Decision-making</title>
      <link>https://arxiv.org/abs/2410.03690</link>
      <description>arXiv:2410.03690v1 Announce Type: new 
Abstract: Conversational Swarm Intelligence (CSI) is an AI-powered communication and collaboration technology that allows large, networked groups (of potentially unlimited size) to hold thoughtful conversational deliberations in real-time. Inspired by the efficient decision-making dynamics of fish schools, CSI divides a human population into a set of small subgroups connected by AI agents. This enables the full group to hold a unified conversation. In this study, groups of 25 participants were tasked with selecting a roster of players in a real Fantasy Baseball contest. A total of 10 trials were run using CSI. In half the trials, each subgroup was augmented with a fact-providing AI agent referred to herein as an Infobot. The Infobot was loaded with a wide range of MLB statistics. The human participants could query the Infobot the same way they would query other persons in their subgroup. Results show that when using CSI, the 25-person groups outperformed 72% of individually surveyed participants and showed significant intelligence amplification versus the mean score (p=0.016). The CSI-enabled groups also significantly outperformed the most popular picks across the collected surveys for each daily contest (p&lt;0.001). The CSI sessions that used Infobots scored slightly higher than those that did not, but it was not statistically significant in this study. That said, 85% of participants agreed with the statement 'Our decisions were stronger because of information provided by the Infobot' and only 4% disagreed. In addition, deliberations that used Infobots showed significantly less variance (p=0.039) in conversational content across members. This suggests that Infobots promoted more balanced discussions in which fewer members dominated the dialog. This may be because the infobot enabled participants to confidently express opinions with the support of factual data</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03690v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Louis Rosenberg, Hans Schumann, Christopher Dishop, Gregg Willcox, Anita Woolley, Ganesh Mani</dc:creator>
    </item>
    <item>
      <title>Making Data: The Work Behind Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2410.03694</link>
      <description>arXiv:2410.03694v1 Announce Type: new 
Abstract: AI generates both enthusiasm and disillusionment, with promises that often go unfulfilled. It is therefore not surprising that human labor, which is its fundamental component, is also subject to these same deceptions. The development of "smart technologies" depends, at different stages, on a multitude of precarious, underpaid and invisible workers, who, dispersed globally, carry out repetitive, fragmented activities, paid per task and completed in a few seconds. These are workers who label data to train algorithms, through tasks that require the intuitive, creative and cognitive abilities of human beings, such as categorizing images, classifying advertisements, transcribing audio and video, evaluating advertisements, moderating content on social media, labeling human anatomical points of interest, digitizing documents, etc. This form of work is often referred to as "microwork". Our contribution, which documents the conditions of microwork in Brazil and offers portraits of the workers, is a step in the wider effort to overcome the current state of invisibilization. It opens up avenues for future research, with the aim of better characterizing this new form of work, tracing its changes over time in relation to the dynamics of globalization and, ideally, identifying levers for action and transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03694v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Ricardo Festi; J{\"o}rg Nowak. As novas infraestruturas produtivas: digitaliza{\c c}{\~a}o do trabalho, e-log{\'i}stica e ind{\'u}stria 4.0, Boitempo, pp.105-120, 2024, 6557173871</arxiv:journal_reference>
      <dc:creator>Matheus Viana Braz (UEM), Paola Tubaro (CNRS, ENSAE Paris, CREST), Antonio A. Casilli (I3 SES, NOS, LACI)</dc:creator>
    </item>
    <item>
      <title>Improving the Accessibility of Dating Websites for Individuals with Visual Impairments</title>
      <link>https://arxiv.org/abs/2410.03695</link>
      <description>arXiv:2410.03695v1 Announce Type: new 
Abstract: People now frequently meet and develop relationships through online dating. Yet, due to their limited accessibility, utilizing dating services can be difficult and irritating for people with visual impairments. The significance of the research issue can be attributed to the fact that dating websites are becoming more and more common and have a significant impact on how people establish romantic connections. It can be challenging for people with visual impairments to use dating services and develop lasting relationships because many of them are not created with their requirements in mind. We can encourage people with visual impairments to participate more completely in online dating and possibly enhance the success of their romantic relationships by making dating websites more accessible. There is some existing implementation that can automatically recognize the facial expression, age, gender, presence of child(ren) and other common objects from a profile photo in a dating platform. The goal of this project is incorporate additional features (presence of any common pets, indoor vs. outdoor image) to further enhance the capability of existing system and come up with test viable solutions to accessibility issues that people with visual impairments face when using dating websites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03695v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gyanendra Shrestha, Soumya Tejaswi Vadlamani</dc:creator>
    </item>
    <item>
      <title>Improving Emotion Recognition Accuracy with Personalized Clustering</title>
      <link>https://arxiv.org/abs/2410.03696</link>
      <description>arXiv:2410.03696v1 Announce Type: new 
Abstract: Emotion recognition through artificial intelligence and smart sensing of physical and physiological signals (Affective Computing) is achieving very interesting results in terms of accuracy, inference times, and user-independent models. In this sense, there are applications related to the safety and well-being of people (sexual aggressions, gender-based violence, children and elderly abuse, mental health, etc.) that require even more improvements. Emotion detection should be done with fast, discrete, and non-luxurious systems working in real-time and real life (wearable devices, wireless communications, battery-powered). Furthermore, emotional reactions to violence are not equal in all people. Then, large general models cannot be applied to a multiuser system for people protection, and customized and simple AI models would be welcomed by health and social workers and law enforcement agents. These customized models will be applicable to clusters of subjects sharing similarities in their emotional reactions to external stimuli. This customization requires several steps: creating clusters of subjects with similar behaviors, creating AI models for every cluster, continually updating these models with new data, and enrolling new subjects in clusters when required. A methodology for clustering data compiled (physical and physiological data, together with emotional labels) is presented in this work, as well as the method for including new subjects once the AI model is generated. Experimental results demonstrate an improvement of 4% in accuracy and 3% in f1-score w.r.t. the general model, along with a 14% reduction in variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03696v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Gutierrez-Martin (Department of Electronics, Universidad Carlos III de Madrid, Spain), Celia Lopez Ongil (Department of Electronics, Universidad Carlos III de Madrid, Spain, Gender Studies Institute, Universidad Carlos III de Madrid, Spain), Jose M. Lanza-Gutierrez (Department of Computer Science, Universidad de Alcala, Spain), Jose A. Miranda Calero (Embedded Systems Laboratory, Ecole Polytechnique Federale de Lausanne, Switzerland)</dc:creator>
    </item>
    <item>
      <title>Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking</title>
      <link>https://arxiv.org/abs/2410.03703</link>
      <description>arXiv:2410.03703v1 Announce Type: new 
Abstract: Large language models are transforming the creative process by offering unprecedented capabilities to algorithmically generate ideas. While these tools can enhance human creativity when people co-create with them, it's unclear how this will impact unassisted human creativity. We conducted two large pre-registered parallel experiments involving 1,100 participants attempting tasks targeting the two core components of creativity, divergent and convergent thinking. We compare the effects of two forms of large language model (LLM) assistance -- a standard LLM providing direct answers and a coach-like LLM offering guidance -- with a control group receiving no AI assistance, and focus particularly on how all groups perform in a final, unassisted stage. Our findings reveal that while LLM assistance can provide short-term boosts in creativity during assisted tasks, it may inadvertently hinder independent creative performance when users work without assistance, raising concerns about the long-term impact on human creativity and cognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03703v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harsh Kumar, Jonathan Vincentius, Ewan Jordan, Ashton Anderson</dc:creator>
    </item>
    <item>
      <title>PAGE: A Modern Measure of Emotion Perception for Teamwork and Management Research</title>
      <link>https://arxiv.org/abs/2410.03704</link>
      <description>arXiv:2410.03704v1 Announce Type: new 
Abstract: This paper presents a new measure of emotional perceptiveness called PAGE: Perceiving AI Generated Emotions. The test includes a broad range of emotions, expressed by ethnically diverse faces, spanning a wide range of ages. We created stimuli with Generative AI, demonstrating the potential to build customizable assessments of emotional intelligence at relatively low cost. Study 1 describes the validation of the image set and test construction. Study 2 reports the psychometric properties of the test. Despite its brevity - 8 minutes on average - PAGE has strong convergent validity and moderately higher internal consistency than comparable measures. Study 3 explores predictive validity using a lab experiment in which we causally identify the contributions managers make to teams. PAGE scores strongly predict managers causal contributions to group success, a finding which is robust to controlling for personality and demographic characteristics. We also discussed the potential of Generative AI to automate development of non-cognitive skill assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03704v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ben Weidmann, Yixian Xu</dc:creator>
    </item>
    <item>
      <title>Open AI-Romance with ChatGPT, Ready for Your Cyborg Lover?</title>
      <link>https://arxiv.org/abs/2410.03710</link>
      <description>arXiv:2410.03710v1 Announce Type: new 
Abstract: Since late March 2024, a Chinese college student has shared her AI Romance with ChatGPT on Red, a popular Chinese social media platform, attracting millions of followers and sparking numerous imitations. This phenomenon has created an iconic figure among Chinese youth, particularly females. This study employs a case study and digital ethnography approach seeking to understand how technology (social media, generative AI) shapes Chinese female students' engagement with AI Romance and how AI Romance impacts the reshaping of gender power relations of Chinese female college students. There are three main findings. First, Open AI Romance is performative, mutually shaping, and creates flexible gender power dynamics and potential new configurations. Second, the cyborg lover identity is fluid, shared, and partially private due to technology and social platforms. Third, the rise of ChatGPT's DAN mode on Red introduces a simulated "male" app into a "female" platform, pushing the limits of policy guidelines, and social norms, making the platform even "wilder." This research provides a deeper understanding of the intersection between technology and social behavior, highlighting the role of AI and social media in evolving gender dynamics among Chinese youth. It sheds light on the performative nature of digital interactions and the potential for technology to redefine traditional gender power structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03710v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qin Xie</dc:creator>
    </item>
    <item>
      <title>Visualization of missing data: a state-of-the-art survey</title>
      <link>https://arxiv.org/abs/2410.03712</link>
      <description>arXiv:2410.03712v1 Announce Type: new 
Abstract: Missing data, the data value that is not recorded for a variable, occurs in almost all statistical analyses and may be caused by many reasons, such as lack of collection or a lack of documentation. Researchers need to adequately deal with this issue to provide a valid analysis. The visualization of missing values plays an important role in supporting the investigation and understanding of the missing data patterns. While some techniques and tools for visualization of missing values are available, it is still a challenge to select the right visualization that will fulfil the user requirements for visualizing missing data. This paper provides an overview and state-of-the-art report (STAR) of research literature focusing on missing values visualization. To the best of our knowledge, this is the first survey paper with a focus on missing data visualization. The goal of this paper is to encourage visualization researchers to increase their involvement with Missing data visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03712v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah Alsufyani, Matthew Forshaw, Sara Johansson Fernstad</dc:creator>
    </item>
    <item>
      <title>Queering AI: Undoing the Self in the Algorithmic Borderlands</title>
      <link>https://arxiv.org/abs/2410.03713</link>
      <description>arXiv:2410.03713v1 Announce Type: new 
Abstract: This paper challenges fixed orientations towards the self in human-AI entanglements. It offers queering as a strategy to subvert the individuation and fixing of identities within algorithmic systems and the loss of futurity that it brings about. By exploring queerness, the paper examines how one's sense of self and futurity are interpellated within the algorithmic borderlands of human-AI entanglements. The study discusses an embodied experiment called "Undoing Gracia," a Digital Twin simulation where the first author Grace and their AI twins (Lex and Tortugi) interact within the fictional world of Gracia. The experiment probes into Grace's multifaceted subjectivities by conceiving themselves as interdependent entities evolving through their interactions within Gracia. The paper outlines the process of creating and implementing the simulation and examines how the agents co-perform and become-with alongside Gracia's making. The findings illuminate queer gestures for navigating human-AI entanglements in HCI research and practice, highlighting the importance of fluid identities in shaping human-AI relations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03713v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grace Leonora Turtle (Delft University of Technology, Netherlands), Roy Bendor (Delft University of Technology, Netherlands), Elisa Giaccardi (Politecnico di Milano, Italy), Blazej Kotowski (Universitat Pompeu Fabra, Spain)</dc:creator>
    </item>
    <item>
      <title>Perceptual Analysis of Groups of Virtual Humans Animated using Interactive Platforms</title>
      <link>https://arxiv.org/abs/2410.03714</link>
      <description>arXiv:2410.03714v1 Announce Type: new 
Abstract: Virtual humans (VH) have been used in Computer Graphics (CG) for many years, and perception studies have been applied to understand how people perceive them. Some studies have already examined how realism impacts the comfort of viewers. In some cases, the user's comfort is related to human identification. For example, people from a specific group may look positively at others from the same group. Gender is one of those characteristics that have in-group advantages. For example, in terms of VHs, studies have shown that female humans are more likely to recognize emotions in female VHs than in male VHs. However, there are many other variables that can impact the user perception. To aid this discussion, we conducted a study on how people perceive comfort and realism in relation to interactive VHs with different genders and expressing negative, neutral, or positive emotions in groups. We created a virtual environment for participants to interact with groups of VHs, which are interactive and should evolve in real-time, using a popular game engine. To animate the characters, we opted for cartoon figures that are animated by tracking the facial expressions of actors, using available game engine platforms to conduct the driven animation. Our results indicate that the emotion of the VH group impacts both comfort and realism perception, even by using simple cartoon characters in an interactive environment. Furthermore, the findings suggest that individuals reported feeling better with a positive emotion compared to a negative emotion, and that negative emotion recognition is impacted by the gender of the VHs group. Additionally, although we used simple characters, the results are consistent with the perception obtained when analysing realistic the state-of-the-art virtual humans, which positive emotions tend to be more correctly recognized than negative ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03714v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rubens Montanha, Giovana Raupp, Ana Carolina Schmitt, Gabriel Schneider, Victor Araujo, Soraia Raupp Musse</dc:creator>
    </item>
    <item>
      <title>Large Language Models Overcome the Machine Penalty When Acting Fairly but Not When Acting Selfishly or Altruistically</title>
      <link>https://arxiv.org/abs/2410.03724</link>
      <description>arXiv:2410.03724v1 Announce Type: new 
Abstract: In social dilemmas where the collective and self-interests are at odds, people typically cooperate less with machines than with fellow humans, a phenomenon termed the machine penalty. Overcoming this penalty is critical for successful human-machine collectives, yet current solutions often involve ethically-questionable tactics, like concealing machines' non-human nature. In this study, with 1,152 participants, we explore the possibility of closing this research question by using Large Language Models (LLMs), in scenarios where communication is possible between interacting parties. We design three types of LLMs: (i) Cooperative, aiming to assist its human associate; (ii) Selfish, focusing solely on maximizing its self-interest; and (iii) Fair, balancing its own and collective interest, while slightly prioritizing self-interest. Our findings reveal that, when interacting with humans, fair LLMs are able to induce cooperation levels comparable to those observed in human-human interactions, even when their non-human nature is fully disclosed. In contrast, selfish and cooperative LLMs fail to achieve this goal. Post-experiment analysis shows that all three types of LLMs succeed in forming mutual cooperation agreements with humans, yet only fair LLMs, which occasionally break their promises, are capable of instilling a perception among humans that cooperating with them is the social norm, and eliciting positive views on their trustworthiness, mindfulness, intelligence, and communication quality. Our findings suggest that for effective human-machine cooperation, bot manufacturers should avoid designing machines with mere rational decision-making or a sole focus on assisting humans. Instead, they should design machines capable of judiciously balancing their own interest and the interest of humans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03724v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhen Wang (School of Cybersecurity, and School of Artificial Intelligence, OPtics and ElectroNics), Ruiqi Song (School of Cybersecurity, and School of Artificial Intelligence, OPtics and ElectroNics), Chen Shen (Faculty of Engineering Sciences, Kyushu University, Japan), Shiya Yin (School of Cybersecurity, and School of Artificial Intelligence, OPtics and ElectroNics), Zhao Song (School of Computing, Engineering and Digital Technologies, Teesside University, United Kingdom), Balaraju Battu (Computer Science, Science Division, New York University Abu Dhabi, UAE), Lei Shi (School of Statistics and Mathematics, Yunnan University of Finance and Economics, China), Danyang Jia (School of Cybersecurity, and School of Artificial Intelligence, OPtics and ElectroNics), Talal Rahwan (Computer Science, Science Division, New York University Abu Dhabi, UAE), Shuyue Hu (Shanghai Artificial Intelligence Laboratory, China)</dc:creator>
    </item>
    <item>
      <title>Evaluating the Effects of AI Directors for Quest Selection</title>
      <link>https://arxiv.org/abs/2410.03733</link>
      <description>arXiv:2410.03733v1 Announce Type: new 
Abstract: Modern commercial games are designed for mass appeal, not for individual players, but there is a unique opportunity in video games to better fit the individual through adapting game elements. In this paper, we focus on AI Directors, systems which can dynamically modify a game, that personalize the player experience to match the player's preference. In the past, some AI Director studies have provided inconclusive results, so their effect on player experience is not clear. We take three AI Directors and directly compare them in a human subject study to test their effectiveness on quest selection. Our results show that a non-random AI Director provides a better player experience than a random AI Director.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03733v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristen K. Yu, Matthew Guzdial, Nathan Sturtevant</dc:creator>
    </item>
    <item>
      <title>CliMB: An AI-enabled Partner for Clinical Predictive Modeling</title>
      <link>https://arxiv.org/abs/2410.03736</link>
      <description>arXiv:2410.03736v1 Announce Type: new 
Abstract: Despite its significant promise and continuous technical advances, real-world applications of artificial intelligence (AI) remain limited. We attribute this to the "domain expert-AI-conundrum": while domain experts, such as clinician scientists, should be able to build predictive models such as risk scores, they face substantial barriers in accessing state-of-the-art (SOTA) tools. While automated machine learning (AutoML) has been proposed as a partner in clinical predictive modeling, many additional requirements need to be fulfilled to make machine learning accessible for clinician scientists.
  To address this gap, we introduce CliMB, a no-code AI-enabled partner designed to empower clinician scientists to create predictive models using natural language. CliMB guides clinician scientists through the entire medical data science pipeline, thus empowering them to create predictive models from real-world data in just one conversation. CliMB also creates structured reports and interpretable visuals. In evaluations involving clinician scientists and systematic comparisons against a baseline GPT-4, CliMB consistently demonstrated superior performance in key areas such as planning, error prevention, code execution, and model performance. Moreover, in blinded assessments involving 45 clinicians from diverse specialties and career stages, more than 80% preferred CliMB over GPT-4. Overall, by providing a no-code interface with clear guidance and access to SOTA methods in the fields of data-centric AI, AutoML, and interpretable ML, CliMB empowers clinician scientists to build robust predictive models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03736v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evgeny Saveliev, Tim Schubert, Thomas Pouplin, Vasilis Kosmoliaptsis, Mihaela van der Schaar</dc:creator>
    </item>
    <item>
      <title>Towards Democratization of Subspeciality Medical Expertise</title>
      <link>https://arxiv.org/abs/2410.03741</link>
      <description>arXiv:2410.03741v1 Announce Type: new 
Abstract: The scarcity of subspecialist medical expertise, particularly in rare, complex and life-threatening diseases, poses a significant challenge for healthcare delivery. This issue is particularly acute in cardiology where timely, accurate management determines outcomes. We explored the potential of AMIE (Articulate Medical Intelligence Explorer), a large language model (LLM)-based experimental AI system optimized for diagnostic dialogue, to potentially augment and support clinical decision-making in this challenging context. We curated a real-world dataset of 204 complex cases from a subspecialist cardiology practice, including results for electrocardiograms, echocardiograms, cardiac MRI, genetic tests, and cardiopulmonary stress tests. We developed a ten-domain evaluation rubric used by subspecialists to evaluate the quality of diagnosis and clinical management plans produced by general cardiologists or AMIE, the latter enhanced with web-search and self-critique capabilities. AMIE was rated superior to general cardiologists for 5 of the 10 domains (with preference ranging from 9% to 20%), and equivalent for the rest. Access to AMIE's response improved cardiologists' overall response quality in 63.7% of cases while lowering quality in just 3.4%. Cardiologists' responses with access to AMIE were superior to cardiologist responses without access to AMIE for all 10 domains. Qualitative examinations suggest AMIE and general cardiologist could complement each other, with AMIE thorough and sensitive, while general cardiologist concise and specific. Overall, our results suggest that specialized medical LLMs have the potential to augment general cardiologists' capabilities by bridging gaps in subspecialty expertise, though further research and validation are essential for wide clinical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03741v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jack W. O'Sullivan, Anil Palepu, Khaled Saab, Wei-Hung Weng, Yong Cheng, Emily Chu, Yaanik Desai, Aly Elezaby, Daniel Seung Kim, Roy Lan, Wilson Tang, Natalie Tapaskar, Victoria Parikh, Sneha S. Jain, Kavita Kulkarni, Philip Mansfield, Dale Webster, Juraj Gottweis, Joelle Barral, Mike Schaekermann, Ryutaro Tanno, S. Sara Mahdavi, Vivek Natarajan, Alan Karthikesalingam, Euan Ashley, Tao Tu</dc:creator>
    </item>
    <item>
      <title>Intelligent CAD 2.0</title>
      <link>https://arxiv.org/abs/2410.03759</link>
      <description>arXiv:2410.03759v1 Announce Type: new 
Abstract: Integrating modern artificial intelligence (AI) techniques, particularly generative AI, holds the promise of revolutionizing computer-aided design (CAD) tools and the engineering design process. However, the direction of "AI+CAD" remains unclear: how will the current generation of intelligent CAD (ICAD) differ from its predecessor in the 1980s and 1990s, what strategic pathways should researchers and engineers pursue for its implementation, and what potential technical challenges might arise?
  As an attempt to address these questions, this paper investigates the transformative role of modern AI techniques in advancing CAD towards ICAD. It first analyzes the design process and reconsiders the roles AI techniques can assume in this process, highlighting how they can restructure the path humans, computers, and designs interact with each other. The primary conclusion is that ICAD systems should assume an intensional rather than extensional role in the design process. This offers insights into the evaluation of the previous generation of ICAD (ICAD 1.0) and outlines a prospective framework and trajectory for the next generation of ICAD (ICAD 2.0).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03759v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Qiang Zou, Yincai Wu, Zhenyu Liu, Weiwei Xu, Shuming Gao</dc:creator>
    </item>
    <item>
      <title>Getting in the Door: Streamlining Intake in Civil Legal Services with Large Language Models</title>
      <link>https://arxiv.org/abs/2410.03762</link>
      <description>arXiv:2410.03762v1 Announce Type: new 
Abstract: Legal intake, the process of finding out if an applicant is eligible for help from a free legal aid program, takes significant time and resources. In part this is because eligibility criteria are nuanced, open-textured, and require frequent revision as grants start and end. In this paper, we investigate the use of large language models (LLMs) to reduce this burden. We describe a digital intake platform that combines logical rules with LLMs to offer eligibility recommendations, and we evaluate the ability of 8 different LLMs to perform this task. We find promising results for this approach to help close the access to justice gap, with the best model reaching an F1 score of .82, while minimizing false negatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03762v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quinten Steenhuis, Hannes Westermann</dc:creator>
    </item>
    <item>
      <title>SeeSay: An Assistive Device for the Visually Impaired Using Retrieval Augmented Generation</title>
      <link>https://arxiv.org/abs/2410.03771</link>
      <description>arXiv:2410.03771v1 Announce Type: new 
Abstract: In this paper, we present SeeSay, an assistive device designed for individuals with visual impairments. This system leverages large language models (LLMs) for speech recognition and visual querying. It effectively identifies, records, and responds to the user's environment by providing audio guidance using retrieval-augmented generation (RAG). Our experiments demonstrate the system's capability to recognize its surroundings and respond to queries with audio feedback in diverse settings. We hope that the SeeSay system will facilitate users' comprehension and recollection of their surroundings, thereby enhancing their environmental perception, improving navigational capabilities, and boosting overall independence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03771v1</guid>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melody Yu</dc:creator>
    </item>
    <item>
      <title>Human-Based Risk Model for Improved Driver Support in Interactive Driving Scenarios</title>
      <link>https://arxiv.org/abs/2410.03774</link>
      <description>arXiv:2410.03774v1 Announce Type: new 
Abstract: This paper addresses the problem of human-based driver support. Nowadays, driver support systems help users to operate safely in many driving situations. Nevertheless, these systems do not fully use the rich information that is available from sensing the human driver. In this paper, we therefore present a human-based risk model that uses driver information for improved driver support. In contrast to state of the art, our proposed risk model combines a) the current driver perception based on driver errors, such as the driver overlooking another vehicle (i.e., notice error), and b) driver personalization, such as the driver being defensive or confident. In extensive simulations of multiple interactive driving scenarios, we show that our novel human-based risk model achieves earlier warning times and reduced warning errors compared to a baseline risk model not using human driver information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03774v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Puphal, Benedict Flade, Matti Kr\"uger, Ryohei Hirano, Akihito Kimata</dc:creator>
    </item>
    <item>
      <title>Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge</title>
      <link>https://arxiv.org/abs/2410.03775</link>
      <description>arXiv:2410.03775v1 Announce Type: new 
Abstract: The effectiveness of automatic evaluation of generative models is typically measured by comparing it to human evaluation using correlation metrics. However, metrics like Krippendorff's $\alpha$ and Randolph's $\kappa$, originally designed to measure the reliability of human labeling, make assumptions about human behavior and the labeling process. In this paper, we show how *relying on a single aggregate correlation score* can obscure fundamental differences between human behavior and automatic evaluation methods, including LLM-as-a-Judge. Specifically, we demonstrate that when the proportion of samples with variation or uncertainty in human labels (gathered during human evaluation) is relatively high, machine labels (generated by automatic evaluation methods) may superficially appear to have similar or better correlation with the human majority label compared to human-to-human (HH) correlation. This can create the misleading impression that automatic evaluation is accurate enough to approximate the human majority label. However, as the proportion of samples with consistent human labels increases, the correlation between machine labels and human majority labels declines, falling below HH correlation. Based on these findings, we first propose stratifying results by human label uncertainty to provide a more robust analysis of automatic evaluation performance. Second, recognizing that uncertainty and variation are inherent in perception-based human evaluations, such as those involving attitudes or preferences, we introduce a new metric - *binned Jensen-Shannon Divergence for perception* for such scenarios to better measure the effectiveness of automatic evaluations. Third, we present visualization techniques -- *perception charts*, to compare the strengths and limitations of automatic evaluation and to contextualize correlation measures appropriately</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03775v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aparna Elangovan, Jongwoo Ko, Lei Xu, Mahsa Elyasi, Ling Liu, Sravan Bodapati, Dan Roth</dc:creator>
    </item>
    <item>
      <title>Towards the Pedagogical Steering of Large Language Models for Tutoring: A Case Study with Modeling Productive Failure</title>
      <link>https://arxiv.org/abs/2410.03781</link>
      <description>arXiv:2410.03781v1 Announce Type: new 
Abstract: One-to-one tutoring is one of the most efficient methods of teaching. Following the rise in popularity of Large Language Models (LLMs), there have been efforts to use them to create conversational tutoring systems, which can make the benefits of one-to-one tutoring accessible to everyone. However, current LLMs are primarily trained to be helpful assistants and thus lack crucial pedagogical skills. For example, they often quickly reveal the solution to the student and fail to plan for a richer multi-turn pedagogical interaction. To use LLMs in pedagogical scenarios, they need to be steered towards using effective teaching strategies: a problem we introduce as Pedagogical Steering and believe to be crucial for the efficient use of LLMs as tutors. We address this problem by formalizing a concept of tutoring strategy, and introducing StratL, an algorithm to model a strategy and use prompting to steer the LLM to follow this strategy. As a case study, we create a prototype tutor for high school math following Productive Failure (PF), an advanced and effective learning design. To validate our approach in a real-world setting, we run a field study with 17 high school students in Singapore. We quantitatively show that StratL succeeds in steering the LLM to follow a Productive Failure tutoring strategy. We also thoroughly investigate the existence of spillover effects on desirable properties of the LLM, like its ability to generate human-like answers. Based on these results, we highlight the challenges in Pedagogical Steering and suggest opportunities for further improvements. We further encourage follow-up research by releasing a dataset of Productive Failure problems and the code of our prototype and algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03781v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Romain Puech, Jakub Macina, Julia Chatain, Mrinmaya Sachan, Manu Kapur</dc:creator>
    </item>
    <item>
      <title>AI-rays: Exploring Bias in the Gaze of AI Through a Multimodal Interactive Installation</title>
      <link>https://arxiv.org/abs/2410.03786</link>
      <description>arXiv:2410.03786v1 Announce Type: new 
Abstract: Data surveillance has become more covert and pervasive with AI algorithms, which can result in biased social classifications. Appearance offers intuitive identity signals, but what does it mean to let AI observe and speculate on them? We introduce AI-rays, an interactive installation where AI generates speculative identities from participants' appearance which are expressed through synthesized personal items placed in participants' bags. It uses speculative X-ray visions to contrast reality with AI-generated assumptions, metaphorically highlighting AI's scrutiny and biases. AI-rays promotes discussions on modern surveillance and the future of human-machine reality through a playful, immersive experience exploring AI biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03786v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3680530.3695433</arxiv:DOI>
      <dc:creator>Ziyao Gao, Yiwen Zhang, Ling Li, Theodoros Papatheodorou, Wei Zeng</dc:creator>
    </item>
    <item>
      <title>People are poorly equipped to detect AI-powered voice clones</title>
      <link>https://arxiv.org/abs/2410.03791</link>
      <description>arXiv:2410.03791v1 Announce Type: new 
Abstract: As generative AI continues its ballistic trajectory, everything from text to audio, image, and video generation continues to improve in mimicking human-generated content. Through a series of perceptual studies, we report on the realism of AI-generated voices in terms of identity matching and naturalness. We find human participants cannot reliably identify short recordings (less than 20 seconds) of AI-generated voices. Specifically, participants mistook the identity of an AI-voice for its real counterpart 80% of the time, and correctly identified a voice as AI-generated only 60% of the time. In all cases, performance is independent of the demographics of the speaker or listener.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03791v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah Barrington, Hany Farid</dc:creator>
    </item>
    <item>
      <title>M2AR: A Web-based Modeling Environment for the Augmented Reality Workflow Modeling Language</title>
      <link>https://arxiv.org/abs/2410.03800</link>
      <description>arXiv:2410.03800v1 Announce Type: new 
Abstract: This paper introduces M2AR, a new web-based, two- and three-dimensional modeling environment that enables the modeling and execution of augmented reality applications without requiring programming knowledge. The platform is based on a 3D JavaScript library and the mixed reality immersive web standard WebXR. For a first demonstration of its feasibility, the previously introduced Augmented Reality Workflow Modeling Language (ARWFML) has been successfully implemented using this environment. The usefulness of the new modeling environment is demonstrated by showing use cases of the ARWFML on M2AR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03800v1</guid>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <category>cs.SE</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3652620.3687779</arxiv:DOI>
      <dc:creator>Fabian Muff, Hans-Georg Fill</dc:creator>
    </item>
    <item>
      <title>Enhanced Digital Twin for Human-Centric and Integrated Lighting Asset Management in Public Libraries: From Corrective to Predictive Maintenance</title>
      <link>https://arxiv.org/abs/2410.03811</link>
      <description>arXiv:2410.03811v1 Announce Type: new 
Abstract: Lighting asset management in public libraries has traditionally been reactive, focusing on corrective maintenance, addressing issues only when failures occur. Although standards now encourage preventive measures, such as incorporating a maintenance factor, the broader goal of human centric, sustainable lighting systems requires a shift toward predictive maintenance strategies. This study introduces an enhanced digital twin model designed for the proactive management of lighting assets in public libraries. By integrating descriptive, diagnostic, predictive, and prescriptive analytics, the model enables a comprehensive, multilevel view of asset health. The proposed framework supports both preventive and predictive maintenance strategies, allowing for early detection of issues and the timely resolution of potential failures. In addition to the specific application for lighting systems, the design is adaptable for other building assets, providing a scalable solution for integrated asset management in various public spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03811v1</guid>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Lin, Jingchun Shen</dc:creator>
    </item>
    <item>
      <title>A Tool to Facilitate Web-Browsing</title>
      <link>https://arxiv.org/abs/2410.03866</link>
      <description>arXiv:2410.03866v1 Announce Type: new 
Abstract: Search engine results often misalign with users' goals due to opaque algorithms, leading to unhelpful or detrimental information consumption. To address this, we developed a Google Chrome plugin that provides "content labels" for webpages in Google search results, assessing Actionability (guiding actions), Knowledge (enhancing understanding), and Emotion. Using natural language processing and machine learning, the plugin predicts these properties from webpage text based on models trained on participants' ratings, effectively reflecting user perceptions. The implications include enhanced user control over information consumption and promotion of healthier engagement with online content, potentially improving decision-making and well-being.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03866v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Christopher Kelly, Jonatan Fontanez, Tali Sharot</dc:creator>
    </item>
    <item>
      <title>JumpStarter: Getting Started on Personal Goals with AI-Powered Context Curation</title>
      <link>https://arxiv.org/abs/2410.03882</link>
      <description>arXiv:2410.03882v1 Announce Type: new 
Abstract: Everyone aspires to achieve personal goals. However, getting started is often complex and daunting, especially for large projects. AI has the potential to create plans and help jumpstart progress, but it often lacks sufficient personal context to be useful. We introduce JumpStarter, a system that uses AI-powered context curation to create action plans and draft personalized working solutions. JumpStarter assists users by posing questions to elicit relevant context, breaking down goals into manageable steps, and selecting appropriate context to draft working solutions for each step. A technical evaluation indicates that context curation results in plans and working solutions of higher quality. A user study demonstrates that compared to ChatGPT, JumpStarter significantly reduces users' mental load and increases their efficiency in kickstarting personal projects. We discuss the design implications of AI-powered context curation to facilitate the use of generative AI in complex problem-solving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03882v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sitong Wang, Xuanming Zhang, Jenny Ma, Alyssa Hwang, Lydia B. Chilton</dc:creator>
    </item>
    <item>
      <title>Demystifying Technology for Policymaking: Exploring the Rideshare Context and Data Initiative Opportunities to Advance Tech Policymaking Efforts</title>
      <link>https://arxiv.org/abs/2410.03895</link>
      <description>arXiv:2410.03895v1 Announce Type: new 
Abstract: In the face of rapidly advancing technologies, evidence of harms they can exacerbate, and insufficient policy to ensure accountability from tech companies, what are HCI opportunities for advancing policymaking of technology? In this paper, we explore challenges and opportunities for tech policymaking through a case study of app-based rideshare driving. We begin with background on rideshare platforms and how they operate. Next, we review literature on algorithmic management about how rideshare drivers actually experience platform features -- often to the detriment of their well-being -- and ways they respond. In light of this, researchers and advocates have called for increased worker protections, thus we turn to rideshare policy and regulation efforts in the U.S. Here, we differentiate the political strategies of platforms with those of drivers to illustrate the conflicting narratives policymakers face when trying to oversee gig work platforms. We reflect that past methods surfacing drivers' experiences may be insufficient for policymaker needs when developing oversight. To address this gap and our original inquiry -- what are HCI opportunities for advancing tech policymaking -- we briefly explore two paths forward for holding tech companies accountable in the rideshare context: (1) data transparency initiatives to enable collective auditing by workers and (2) legal frameworks for holding platforms accountable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03895v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Angie Zhang</dc:creator>
    </item>
    <item>
      <title>Toward Understanding the Experiences of People in Late Adulthood with Embedded Information Displays in the Home</title>
      <link>https://arxiv.org/abs/2410.03929</link>
      <description>arXiv:2410.03929v1 Announce Type: new 
Abstract: Embedded information displays (EIDs) are becoming increasingly ubiquitous on home appliances and devices such as microwaves, coffee machines, fridges, or digital thermostats. These displays are often multi-purpose, functioning as interfaces for selecting device settings, communicating operating status using simple visualizations, and displaying notifications. However, their usability for people in the late adulthood (PLA) development stage is not well-understood. We report on two focus groups with PLA (n = 11, ages 76-94) from a local retirement community. Participants were shown images of everyday home electronics and appliances, answering questions about their experiences using the EIDs. Using open coding, we qualitatively analyzed their comments to distill key themes regarding how EIDs can negatively affect PLA's ability to take in information (e.g., poor labels) and interact with these devices (e.g., unintuitive steps) alongside strategies employed to work around these issues. We argue that understanding the equitable design and communication of devices' functions, operating status, and messages is important for future information display designers. We hope this work stimulates further investigation into more equitable EID design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03929v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zack While, Henry Wheeler-Klainberg, Tanja Blascheck, Petra Isenberg, Ali Sarvghad</dc:creator>
    </item>
    <item>
      <title>TR-LLM: Integrating Trajectory Data for Scene-Aware LLM-Based Human Action Prediction</title>
      <link>https://arxiv.org/abs/2410.03993</link>
      <description>arXiv:2410.03993v1 Announce Type: new 
Abstract: Accurate prediction of human behavior is crucial for AI systems to effectively support real-world applications, such as autonomous robots anticipating and assisting with human tasks. Real-world scenarios frequently present challenges such as occlusions and incomplete scene observations, which can compromise predictive accuracy. Thus, traditional video-based methods often struggle due to limited temporal and spatial perspectives. Large Language Models (LLMs) offer a promising alternative. Having been trained on a large text corpus describing human behaviors, LLMs likely encode plausible sequences of human actions in a home environment. However, LLMs, trained primarily on text data, lack inherent spatial awareness and real-time environmental perception. They struggle with understanding physical constraints and spatial geometry. Therefore, to be effective in a real-world spatial scenario, we propose a multimodal prediction framework that enhances LLM-based action prediction by integrating physical constraints derived from human trajectories. Our experiments demonstrate that combining LLM predictions with trajectory data significantly improves overall prediction performance. This enhancement is particularly notable in situations where the LLM receives limited scene information, highlighting the complementary nature of linguistic knowledge and physical constraints in understanding and anticipating human behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03993v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kojiro Takeyama, Yimeng Liu, Misha Sra</dc:creator>
    </item>
    <item>
      <title>Enhancing the Travel Experience for People with Visual Impairments through Multimodal Interaction: NaviGPT, A Real-Time AI-Driven Mobile Navigation System</title>
      <link>https://arxiv.org/abs/2410.04005</link>
      <description>arXiv:2410.04005v1 Announce Type: new 
Abstract: Assistive technologies for people with visual impairments (PVI) have made significant advancements, particularly with the integration of artificial intelligence (AI) and real-time sensor technologies. However, current solutions often require PVI to switch between multiple apps and tools for tasks like image recognition, navigation, and obstacle detection, which can hinder a seamless and efficient user experience. In this paper, we present NaviGPT, a high-fidelity prototype that integrates LiDAR-based obstacle detection, vibration feedback, and large language model (LLM) responses to provide a comprehensive and real-time navigation aid for PVI. Unlike existing applications such as Be My AI and Seeing AI, NaviGPT combines image recognition and contextual navigation guidance into a single system, offering continuous feedback on the user's surroundings without the need for app-switching. Meanwhile, NaviGPT compensates for the response delays of LLM by using location and sensor data, aiming to provide practical and efficient navigation support for PVI in dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04005v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>He Zhang, Nicholas J. Falletta, Jingyi Xie, Rui Yu, Sooyeon Lee, Syed Masum Billah, John M. Carroll</dc:creator>
    </item>
    <item>
      <title>Development of a Mouse for Individuals Without Upper Limbs Using Arduino Technology</title>
      <link>https://arxiv.org/abs/2410.04016</link>
      <description>arXiv:2410.04016v1 Announce Type: new 
Abstract: This project focuses on the design and construction of a prototype mouse based on the Arduino platform, intended for individuals without upper limbs to use computers more effectively. The prototype comprises a microcontroller responsible for processing signals from the MPU-6050 sensor, used as a reference for cursor position, and foot-operated buttons for right and left-click functions. Its design enables cursor control through head movements, providing users with an easy and intuitive way to interact with the computer's graphical interface. Feasibility testing was conducted through experimental trials, resulting in ideal accuracy and precision. These trials indicate that the device is viable for use in individuals without upper limbs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04016v1</guid>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ic-ETITE58242.2024.10493246</arxiv:DOI>
      <arxiv:journal_reference>2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE) (pp. 1-5). IEEE</arxiv:journal_reference>
      <dc:creator>Alfonso Gunsha, Luis Chuquimarca, Pedro Pardo, David Herrera</dc:creator>
    </item>
    <item>
      <title>IdeaSynth: Iterative Research Idea Development Through Evolving and Composing Idea Facets with Literature-Grounded Feedback</title>
      <link>https://arxiv.org/abs/2410.04025</link>
      <description>arXiv:2410.04025v1 Announce Type: new 
Abstract: Research ideation involves broad exploring and deep refining ideas. Both require deep engagement with literature. Existing tools focus primarily on idea broad generation, yet offer little support for iterative specification, refinement, and evaluation needed to further develop initial ideas. To bridge this gap, we introduce IdeaSynth, a research idea development system that uses LLMs to provide literature-grounded feedback for articulating research problems, solutions, evaluations, and contributions. IdeaSynth represents these idea facets as nodes on a canvas, and allow researchers to iteratively refine them by creating and exploring variations and composing them. Our lab study (N=20) showed that participants, while using IdeaSynth, explored more alternative ideas and expanded initial ideas with more details compared to a strong LLM-based baseline. Our deployment study (N=7) demonstrated that participants effectively used IdeaSynth for real-world research projects at various ideation stages from developing initial ideas to revising framings of mature manuscripts, highlighting the possibilities to adopt IdeaSynth in researcher's workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04025v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Pu, K. J. Kevin Feng, Tovi Grossman, Tom Hope, Bhavana Dalvi Mishra, Matt Latzke, Jonathan Bragg, Joseph Chee Chang, Pao Siangliulue</dc:creator>
    </item>
    <item>
      <title>Gamifying XAI: Enhancing AI Explainability for Non-technical Users through LLM-Powered Narrative Gamifications</title>
      <link>https://arxiv.org/abs/2410.04035</link>
      <description>arXiv:2410.04035v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has become tightly integrated into modern technology, yet existing exploratory visualizations for explainable AI (XAI) are primarily designed for users with technical expertise. This leaves everyday users, who also regularly interact with AI systems, with limited resources to explore or understand AI technologies they use. We propose a novel framework that enables non-technical users to collect insights by conversing directly with visualization elements via LLM-powered narrative gamifications. We implemented a prototype that utilizes such gamification to facilitate non-technical users' exploration of AI embedding projections. We conducted a comparative study with 10 participants to assess our prototype quantitatively and qualitatively. Our study results indicate that although our prototype effectively enhances non-technical users' AI/XAI knowledge, and users believe they learn more through the gamification feature, it remains inconclusive whether the gamification itself leads to further improvements in understanding. In addition, opinions among participants regarding the framework's engagement are mixed: some believe it enhances their exploration of the visualizations, while others feel it disrupts their workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04035v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuzhe You, Jian Zhao</dc:creator>
    </item>
    <item>
      <title>TeachTune: Reviewing Pedagogical Agents Against Diverse Student Profiles with Simulated Students</title>
      <link>https://arxiv.org/abs/2410.04078</link>
      <description>arXiv:2410.04078v1 Announce Type: new 
Abstract: Large language models (LLMs) can empower educators to build pedagogical conversational agents (PCAs) customized for their students. As students have different prior knowledge and motivation levels, educators must evaluate the adaptivity of their PCAs to diverse students. Existing chatbot evaluation methods (e.g., direct chat and benchmarks) are either manually intensive for multiple iterations or limited to testing only single-turn interactions. We present TeachTune, where educators can create simulated students and review PCAs by observing automated chats between PCAs and simulated students. Our technical pipeline instructs an LLM-based student to simulate prescribed knowledge levels and characteristics, helping educators explore diverse conversation patterns. Our pipeline could produce simulated students whose behaviors correlate highly to their input knowledge and motivation levels within 5% and 10% accuracy gaps. Thirty science teachers designed PCAs in a between-subjects study, and using TeachTune resulted in a lower task load and higher student profile coverage over a baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04078v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hyoungwook Jin, Minju Yoo, Jeongeon Park, Yokyung Lee, Xu Wang, Juho Kim</dc:creator>
    </item>
    <item>
      <title>The Impact of Surface Co-location and Eye-tracking on Mixed Reality Typing</title>
      <link>https://arxiv.org/abs/2410.04177</link>
      <description>arXiv:2410.04177v1 Announce Type: new 
Abstract: Accuracy and speed are pivotal when typing. We hypothesized that the lack of tactile feedback on midair mixed reality keyboards may adversely impact performance. Our first experiment assessed the potential to provide tactile feedback to users typing in mixed reality by co-locating the virtual keyboard on a table or a wall. The keyboard was deterministic (without auto-correct), relied only on the headset's egocentric cameras for sensing, and included symbol keys. Users preferred and had the highest entry rate of 12 words-per-minute using a midair keyboard. Error rates were similar in all conditions. Based on user feedback, our second experiment explored ten-finger typing. We used a novel eye-tracking technique to mitigate accidental key presses. The technique halved the number of times backspace was pressed and was preferred by users. However, participants were faster using only their index fingers without eye-tracking at 11 words-per-minute.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04177v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cecilia Schmitz, Joshua Reynolds, Scott Kuhl, Keith Vertanen</dc:creator>
    </item>
    <item>
      <title>Boosting Visual Fidelity in Driving Simulations through Diffusion Models</title>
      <link>https://arxiv.org/abs/2410.04214</link>
      <description>arXiv:2410.04214v1 Announce Type: new 
Abstract: Diffusion models have made substantial progress in facilitating image generation and editing. As the technology matures, we see its potential in the context of driving simulations to enhance the simulated experience. In this paper, we explore this potential through the introduction of a novel system designed to boost visual fidelity. Our system, DRIVE (Diffusion-based Realism Improvement for Virtual Environments), leverages a diffusion model pipeline to give a simulated environment a photorealistic view, with the flexibility to be adapted for other applications. We conducted a preliminary user study to assess the system's effectiveness in rendering realistic visuals and supporting participants in performing driving tasks. Our work not only lays the groundwork for future research on the integration of diffusion models in driving simulations but also provides practical guidelines and best practices for their application in this context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04214v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fanjun Bu, Hiroshi Yasuda</dc:creator>
    </item>
    <item>
      <title>Be There, Be Together, Be Streamed! AR Scenic Live-Streaming for an Interactive and Collective Experience</title>
      <link>https://arxiv.org/abs/2410.04232</link>
      <description>arXiv:2410.04232v1 Announce Type: new 
Abstract: Scenic Live-Streaming (SLS), capturing real-world scenic sites from fixed cameras without streamers, combines scene immersion and the social and real-time characteristics of live-streaming into a unique experience. However, existing SLS affords limited audience interactions to engage them in a collective experience compared to many other live-streaming genres. It is also difficult for SLS to recreate important but intangible constituents of in-person trip experiences, such as cultural activities. To offer a more interactive, engaging, and meaningful experience, we propose ARSLS (Augmented Reality Scenic Live-Streaming). Culturally grounded AR objects with awareness of the live-streamed environment can be overlaid over camera views to provide additional interactive features while maintaining consistency with the live-streamed scene. To explore the design space of this new medium, we developed an ARSLS prototype for a famous landscape in China. A preliminary study (N=15) provided initial insights for ARSLS design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04232v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zeyu Huang, Zuyu Xu, Yuanhao Zhang, Chengzhong Liu, Yanwei Zhao, Chuhan Shi, Jason Chen Zhao, Xiaojuan Ma</dc:creator>
    </item>
    <item>
      <title>Contrastive Explanations That Anticipate Human Misconceptions Can Improve Human Decision-Making Skills</title>
      <link>https://arxiv.org/abs/2410.04253</link>
      <description>arXiv:2410.04253v1 Announce Type: new 
Abstract: People's decision-making abilities often fail to improve or may even erode when they rely on AI for decision-support, even when the AI provides informative explanations. We argue this is partly because people intuitively seek contrastive explanations, which clarify the difference between the AI's decision and their own reasoning, while most AI systems offer "unilateral" explanations that justify the AI's decision but do not account for users' thinking. To align human-AI knowledge on decision tasks, we introduce a framework for generating human-centered contrastive explanations that explain the difference between AI's choice and a predicted, likely human choice about the same task. Results from a large-scale experiment (N = 628) demonstrate that contrastive explanations significantly enhance users' independent decision-making skills compared to unilateral explanations, without sacrificing decision accuracy. Amid rising deskilling concerns, our research demonstrates that incorporating human reasoning into AI design can foster human skill development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04253v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zana Bu\c{c}inca, Siddharth Swaroop, Amanda E. Paluch, Finale Doshi-Velez, Krzysztof Z. Gajos</dc:creator>
    </item>
    <item>
      <title>The Visualization JUDGE : Can Multimodal Foundation Models Guide Visualization Design Through Visual Perception?</title>
      <link>https://arxiv.org/abs/2410.04280</link>
      <description>arXiv:2410.04280v1 Announce Type: new 
Abstract: Foundation models for vision and language are the basis of AI applications across numerous sectors of society. The success of these models stems from their ability to mimic human capabilities, namely visual perception in vision models, and analytical reasoning in large language models. As visual perception and analysis are fundamental to data visualization, in this position paper we ask: how can we harness foundation models to advance progress in visualization design? Specifically, how can multimodal foundation models (MFMs) guide visualization design through visual perception? We approach these questions by investigating the effectiveness of MFMs for perceiving visualization, and formalizing the overall visualization design and optimization space. Specifically, we think that MFMs can best be viewed as judges, equipped with the ability to criticize visualizations, and provide us with actions on how to improve a visualization. We provide a deeper characterization for text-to-image generative models, and multi-modal large language models, organized by what these models provide as output, and how to utilize the output for guiding design decisions. We hope that our perspective can inspire researchers in visualization on how to approach MFMs for visualization design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04280v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Berger, Shusen Liu</dc:creator>
    </item>
    <item>
      <title>Open Science Practices by Early Career HCI Researchers: Perceptions, Challenges, and Benefits</title>
      <link>https://arxiv.org/abs/2410.04286</link>
      <description>arXiv:2410.04286v1 Announce Type: new 
Abstract: Many fields of science, including Human-Computer Interaction (HCI), have heightened introspection in the wake of concerns around reproducibility and replicability of published findings. Notably, in recent years the HCI community has worked to implement policy changes and mainstream open science practices. Our work investigates early-career HCI researchers' perceptions of open science and engagement with best practices through 18 semi-structured interviews. Our findings highlight key barriers to the widespread adoption of data and materials sharing, and preregistration, namely: lack of clear incentives; cultural resistance; limited training; time constraints; concerns about intellectual property; and data privacy issues. We observe that small changes at major conferences like CHI could meaningfully impact community norms. We offer recommendations to address these barriers and to promote transparency and openness in HCI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04286v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tatiana Chakravorti, Sanjana Gautam, Priya Silverstein, Sarah M. Rajtmajer</dc:creator>
    </item>
    <item>
      <title>Generative Co-Learners: Enhancing Cognitive and Social Presence of Students in Asynchronous Learning with Generative AI</title>
      <link>https://arxiv.org/abs/2410.04365</link>
      <description>arXiv:2410.04365v1 Announce Type: new 
Abstract: Cognitive presence and social presence are crucial for a comprehensive learning experience. Despite the flexibility of asynchronous learning environments to accommodate individual schedules, the inherent constraints of asynchronous environments make augmenting cognitive and social presence particularly challenging. Students often face challenges such as a lack of timely feedback and support, a lack of non-verbal cues in communication, and a sense of isolation. To address this challenge, this paper introduces Generative Co-Learners, a system designed to leverage generative AI-powered agents, simulating co-learners supporting multimodal interactions, to improve cognitive and social presence in asynchronous learning environments. We conducted a study involving 12 student participants who used our system to engage with online programming tutorials to assess the system's effectiveness. The results show that by implementing features to support textual and visual communication and simulate an interactive learning environment with generative agents, our system enhances the cognitive and social presence in the asynchronous learning environment. These results suggest the potential to use generative AI to support students learning at scale and transform asynchronous learning into a more inclusive, engaging, and efficacious educational approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04365v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianjia Wang, Tong Wu, Huayi Liu, Chris Brown, Yan Chen</dc:creator>
    </item>
    <item>
      <title>CardioAI: A Multimodal AI-based System to Support Symptom Monitoring and Risk Detection of Cancer Treatment-Induced Cardiotoxicity</title>
      <link>https://arxiv.org/abs/2410.04592</link>
      <description>arXiv:2410.04592v1 Announce Type: new 
Abstract: Despite recent advances in cancer treatments that prolong patients' lives, treatment-induced cardiotoxicity remains one severe side effect. The clinical decision-making of cardiotoxicity is challenging, as non-clinical symptoms can be missed until life-threatening events occur at a later stage, and clinicians already have a high workload centered on the treatment, not the side effects. Our project starts with a participatory design study with 11 clinicians to understand their practices and needs; then we build a multimodal AI system, CardioAI, that integrates wearables and LLM-powered voice assistants to monitor multimodal non-clinical symptoms. Also, the system includes an explainable risk prediction module that can generate cardiotoxicity risk scores and summaries as explanations to support clinicians' decision-making. We conducted a heuristic evaluation with four clinical experts and found that they all believe CardioAI integrates well into their workflow, reduces their information overload, and enables them to make more informed decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04592v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Siyi Wu, Weidan Cao, Shihan Fu, Bingsheng Yao, Ziqi Yang, Changchang Yin, Varun Mishra, Daniel Addison, Ping Zhang, Dakuo Wang</dc:creator>
    </item>
    <item>
      <title>Need Help? Designing Proactive AI Assistants for Programming</title>
      <link>https://arxiv.org/abs/2410.04596</link>
      <description>arXiv:2410.04596v1 Announce Type: new 
Abstract: While current chat-based AI assistants primarily operate reactively, responding only when prompted by users, there is significant potential for these systems to proactively assist in tasks without explicit invocation, enabling a mixed-initiative interaction. This work explores the design and implementation of proactive AI assistants powered by large language models. We first outline the key design considerations for building effective proactive assistants. As a case study, we propose a proactive chat-based programming assistant that automatically provides suggestions and facilitates their integration into the programmer's code. The programming context provides a shared workspace enabling the assistant to offer more relevant suggestions. We conducted a randomized experimental study examining the impact of various design elements of the proactive assistant on programmer productivity and user experience. Our findings reveal significant benefits of incorporating proactive chat assistants into coding environments and uncover important nuances that influence their usage and effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04596v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valerie Chen, Alan Zhu, Sebastian Zhao, Hussein Mozannar, David Sontag, Ameet Talwalkar</dc:creator>
    </item>
    <item>
      <title>Building Solidarity Amid Hostility: Experiences of Fat People in Online Communities</title>
      <link>https://arxiv.org/abs/2410.04614</link>
      <description>arXiv:2410.04614v1 Announce Type: new 
Abstract: Online communities are important spaces for members of marginalized groups to organize and support one another. To better understand the experiences of fat people -- a group whose marginalization often goes unrecognized -- in online communities, we conducted 12 semi-structured interviews with fat people. Our participants leveraged online communities to engage in consciousness raising around fat identity, learning to locate "the problem of being fat" not within themselves or their own bodies but rather in the oppressive design of the society around them. Participants were then able to use these communities to mitigate everyday experiences of anti-fatness, such as navigating hostile healthcare systems. However, to access these benefits, our participants had to navigate myriad sociotechnical harms, ranging from harassment to discriminatory algorithms. In light of these findings, we suggest that researchers and designers of online communities support selective fat visibility, consider fat people in the design of content moderation systems, and investigate algorithmic discrimination toward fat people. More broadly, we call on researchers and designers to contend with the social and material realities of fat experience, as opposed to the prevailing paradigm of treating fat people as problems to be solved in-and-of-themselves. This requires recognizing fat people as a marginalized social group and actively confronting anti-fatness as it is embedded in the design of technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04614v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blakeley H. Payne, Jordan Taylor, Katta Spiel, Casey Fiesler</dc:creator>
    </item>
    <item>
      <title>Multimodal 3D Fusion and In-Situ Learning for Spatially Aware AI</title>
      <link>https://arxiv.org/abs/2410.04652</link>
      <description>arXiv:2410.04652v1 Announce Type: new 
Abstract: Seamless integration of virtual and physical worlds in augmented reality benefits from the system semantically "understanding" the physical environment. AR research has long focused on the potential of context awareness, demonstrating novel capabilities that leverage the semantics in the 3D environment for various object-level interactions. Meanwhile, the computer vision community has made leaps in neural vision-language understanding to enhance environment perception for autonomous tasks. In this work, we introduce a multimodal 3D object representation that unifies both semantic and linguistic knowledge with the geometric representation, enabling user-guided machine learning involving physical objects. We first present a fast multimodal 3D reconstruction pipeline that brings linguistic understanding to AR by fusing CLIP vision-language features into the environment and object models. We then propose "in-situ" machine learning, which, in conjunction with the multimodal representation, enables new tools and interfaces for users to interact with physical spaces and objects in a spatially and linguistically meaningful manner. We demonstrate the usefulness of the proposed system through two real-world AR applications on Magic Leap 2: a) spatial search in physical environments with natural language and b) an intelligent inventory system that tracks object changes over time. We also make our full implementation and demo data available at (https://github.com/cy-xu/spatially_aware_AI) to encourage further exploration and research in spatially aware AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04652v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengyuan Xu, Radha Kumaran, Noah Stier, Kangyou Yu, Tobias H\"ollerer</dc:creator>
    </item>
    <item>
      <title>Does the Infamous Pie Chart Really Hurt Decision-Making in the Real World? Assessing the Role of Visualization in High-Level Academic Decisions</title>
      <link>https://arxiv.org/abs/2410.04686</link>
      <description>arXiv:2410.04686v1 Announce Type: new 
Abstract: Visualization design influences how people perceive data patterns, yet most research focuses on low-level analytic tasks, such as finding correlations. Existing work has criticized pie charts for their perceptual limitations. However, simpler visualizations like pie and bar charts are widely used for real-world decision-making, such as choosing schools or advisors. As a case study, we examine whether pie charts hurt high-level decisions compared to bar charts, using the website that presents academic data, CSRankings.org. By comparing the impact of pie charts versus bar charts on users' impressions of faculty productivity and projected workload, we found no significant differences in decisions among over 300 participants. Our findings challenge traditional views on visualization design, emphasizing the need for real-world use cases in evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04686v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yixuan Li, Emery D. Berger, Minsuk Kahng, Cindy Xiong Bearfield</dc:creator>
    </item>
    <item>
      <title>Exploring Gestural Interaction with a Cushion Interface for Smart Home Control</title>
      <link>https://arxiv.org/abs/2410.04730</link>
      <description>arXiv:2410.04730v1 Announce Type: new 
Abstract: In this research, we aim to realize cushion interface for operating smart home. We designed user-defined gestures using cushion and developed gesture recognition system. We asked some users to make gestures using cushions for operating home appliances and determined user-defined gesture sets. We developed two methods for gesture identification. The First, We inserted sensor modules consisting of photo reflective sensors and acceleration sensor inside a cushion. The second, we embedded the acceleration sensor arrays in the cushion cover. Gesture recognizer was implemented using Convolutional Neural Networks (CNN). To evaluate our method, We conducted an experiment to measure recognition accuracy. Results showed that an average accuracy was 94.8% when training for each user, and an average accuracy of 91.3% when testing with a user that did not exist in the training data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04730v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuri Suzuki, Kaho Kato, Naomi Furui, Daisuke Sakamoto, Yuta Sugiura</dc:creator>
    </item>
    <item>
      <title>Guidance of the Center of Pressure Using Haptic Presentation</title>
      <link>https://arxiv.org/abs/2410.04732</link>
      <description>arXiv:2410.04732v1 Announce Type: new 
Abstract: Accurately instructing posture and the position of the body's center of gravity is challenging. In this study, we propose a system that utilizes haptic feedback to induce the Center of Pressure (CoP) movement. The Wii Balance Board is employed to sense the CoP, and vibration motors are used for haptic feedback. To provide a comparison, inductions were also performed using visual and auditory feedback, and the time required for induction was measured. Additionally, after the experiments, a questionnaire survey was conducted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04732v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yohei Kawasaki, Yuta Sugiura</dc:creator>
    </item>
    <item>
      <title>A Stretchable Electrostatic Tactile Surface</title>
      <link>https://arxiv.org/abs/2410.04768</link>
      <description>arXiv:2410.04768v1 Announce Type: new 
Abstract: Tactile sensation is essential for humans to recognize objects. Various devices have been developed in the past for tactile presentation by electrostatic force, which are easy to configure devices, but there is currently no such device that features stretchability. Considering that the device is worn over the joints of a human body or robot, it is extremely important that the device itself be stretchable. In this study, we propose a stretchable electrostatic tactile surface comprising a stretchable transparent electrode and a stretchable insulating film that can be stretched to a maximum of 50%. This means that when attached to the human body, this surface can respond to the expansion and contraction that occur due to joint movements. This surface can also provide tactile information in response to deformation such as pushing and pulling. As a basic investigation, we measured the lower limit of voltage that can be perceived by changing the configuration of the surface and evaluated the states of stretching and contraction. We also investigated and modeled the relationship between the voltage and the perceived intensity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04768v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naoto Takayanagi, Naoji Matsuhisa, Yuki Hashimoto, Yuta Sugiura</dc:creator>
    </item>
    <item>
      <title>Single Vs Dual: Influence of the Number of Displays on User Experience within Virtually Embodied Conversational Systems</title>
      <link>https://arxiv.org/abs/2410.04852</link>
      <description>arXiv:2410.04852v1 Announce Type: new 
Abstract: The current research evaluates user experience and preference when interacting with a patient-reported outcome measure (PROM) healthcare application displayed on a single tablet in comparison to interaction with the same application distributed across two tablets. We conducted a within-subject user study with 43 participants who engaged with and rated the usability of our system and participated in a post-experiment interview to collect subjective data. Our findings showed significantly higher usability and higher pragmatic quality ratings for the single tablet condition. However, some users attribute a higher level of presence to the avatar and prefer it to be placed on a second tablet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04852v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3641825.3689700</arxiv:DOI>
      <dc:creator>Navid Ashrafi, Francesco Vona, Philipp Graf, Philipp Harnisch, Sina Hinzmann, Jan-Niklas Voigt-Antons</dc:creator>
    </item>
    <item>
      <title>Working with Mixed Reality in Public: Effects of Virtual Display Layouts on Productivity, Feeling of Safety, and Social Acceptability</title>
      <link>https://arxiv.org/abs/2410.04899</link>
      <description>arXiv:2410.04899v1 Announce Type: new 
Abstract: Nowadays, Mixed Reality (MR) headsets are a game-changer for knowledge work. Unlike stationary monitors, MR headsets allow users to work with large virtual displays anywhere they wear the headset, whether in a professional office, a public setting like a cafe, or a quiet space like a library. This study compares four different layouts (eye level-close, eye level-far, below eye level-close, below eye level-far) of virtual displays regarding feelings of safety, perceived productivity, and social acceptability when working with MR in public. We test which layout is most preferred by users and seek to understand which factors affect users' layout preferences. The aim is to derive useful insights for designing better MR layouts. A field study in a public library was conducted using a within-subject design. While the participants interact with a layout, they are asked to work on a planning task. The results from a repeated measure ANOVA show a statistically significant effect on productivity but not on safety and social acceptability. Additionally, we report preferences expressed by the users regarding the layouts and using MR in public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04899v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Janne Kaeder, Maurizio Vergari, Verena Biener, Tanja Koji\'c, Jens Grubert, Sebastian M\"oller, Jan-Niklas Voigt-Antons</dc:creator>
    </item>
    <item>
      <title>Why am I seeing this: Democratizing End User Auditing for Online Content Recommendations</title>
      <link>https://arxiv.org/abs/2410.04917</link>
      <description>arXiv:2410.04917v1 Announce Type: new 
Abstract: Personalized recommendation systems tailor content based on user attributes, which are either provided or inferred from private data. Research suggests that users often hypothesize about reasons behind contents they encounter (e.g., "I see this jewelry ad because I am a woman"), but they lack the means to confirm these hypotheses due to the opaqueness of these systems. This hinders informed decision-making about privacy and system use and contributes to the lack of algorithmic accountability. To address these challenges, we introduce a new interactive sandbox approach. This approach creates sets of synthetic user personas and corresponding personal data that embody realistic variations in personal attributes, allowing users to test their hypotheses by observing how a website's algorithms respond to these personas. We tested the sandbox in the context of targeted advertisement. Our user study demonstrates its usability, usefulness, and effectiveness in empowering end-user auditing in a case study of targeting ads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04917v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaoran Chen, Leyang Li, Luke Cao, Yanfang Ye, Tianshi Li, Yaxing Yao, Toby Jia-jun Li</dc:creator>
    </item>
    <item>
      <title>Music-triggered fashion design: from songs to the metaverse</title>
      <link>https://arxiv.org/abs/2410.04921</link>
      <description>arXiv:2410.04921v1 Announce Type: new 
Abstract: The advent of increasingly-growing virtual realities poses unprecedented opportunities and challenges to different societies. Artistic collectives are not an exception, and we here aim to put special attention into musicians. Compositions, lyrics and even show-advertisements are constituents of a message that artists transmit about their reality. As such, artistic creations are ultimately linked to feelings and emotions, with aesthetics playing a crucial role when it comes to transmit artist's intentions. In this context, we here analyze how virtual realities can help to broaden the opportunities for musicians to bridge with their audiences, by devising a dynamical fashion-design recommendation system inspired by sound stimulus. We present our first steps towards re-defining musical experiences in the metaverse, opening up alternative opportunities for artists to connect both with real and virtual (\textit{e.g.} machine-learning agents operating in the metaverse) in potentially broader ways.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04921v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Martina Delgado, Marta Llopart, Eva Sarabia, Sandra Taboada, Pol Vierge, Fernando Vilari\~no, Joan Moya Kohler, Julieta Grimberg Golijov, Mat\'ias Bilkis</dc:creator>
    </item>
    <item>
      <title>Enhancing Job Interview Preparation Through Immersive Experiences Using Photorealistic, AI-powered Metahuman Avatars</title>
      <link>https://arxiv.org/abs/2410.05131</link>
      <description>arXiv:2410.05131v1 Announce Type: new 
Abstract: This study will investigate the user experience while interacting with highly photorealistic virtual job interviewer avatars in Virtual Reality (VR), Augmented Reality (AR), and on a 2D screen. Having a precise speech recognition mechanism, our virtual character performs a mock-up software engineering job interview to adequately immerse the user in a life-like scenario. To evaluate the efficiency of our system, we measure factors such as the provoked level of anxiety, social presence, self-esteem, and intrinsic motivation. This research is a work in progress with a prospective within-subject user study including approximately 40 participants. All users will engage with three job interview conditions (VR, AR, and desktop) and provide their feedback. Additionally, users' bio-physical responses will be collected using a biosensor to measure the level of anxiety during the job interview.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05131v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Navid Ashrafi, Francesco Vona, Carina Ringsdorf, Christian Hertel, Luca Toni, Sarina Kailer, Alice Bartels, Tanja Kojic, Jan-Niklas Voigt-Antons</dc:creator>
    </item>
    <item>
      <title>Thematic Analysis with Open-Source Generative AI and Machine Learning: A New Method for Inductive Qualitative Codebook Development</title>
      <link>https://arxiv.org/abs/2410.03721</link>
      <description>arXiv:2410.03721v1 Announce Type: cross 
Abstract: This paper aims to answer one central question: to what extent can open-source generative text models be used in a workflow to approximate thematic analysis in social science research? To answer this question, we present the Generative AI-enabled Theme Organization and Structuring (GATOS) workflow, which uses open-source machine learning techniques, natural language processing tools, and generative text models to facilitate thematic analysis. To establish validity of the method, we present three case studies applying the GATOS workflow, leveraging these models and techniques to inductively create codebooks similar to traditional procedures using thematic analysis. Specifically, we investigate the extent to which a workflow comprising open-source models and tools can inductively produce codebooks that approach the known space of themes and sub-themes. To address the challenge of gleaning insights from these texts, we combine open-source generative text models, retrieval-augmented generation, and prompt engineering to identify codes and themes in large volumes of text, i.e., generate a qualitative codebook. The process mimics an inductive coding process that researchers might use in traditional thematic analysis by reading text one unit of analysis at a time, considering existing codes already in the codebook, and then deciding whether or not to generate a new code based on whether the extant codebook provides adequate thematic coverage. We demonstrate this workflow using three synthetic datasets from hypothetical organizational research settings: a study of teammate feedback in teamwork settings, a study of organizational cultures of ethical behavior, and a study of employee perspectives about returning to their offices after the pandemic. We show that the GATOS workflow is able to identify themes in the text that were used to generate the original synthetic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03721v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andrew Katz, Gabriella Coloyan Fleming, Joyce Main</dc:creator>
    </item>
    <item>
      <title>Human Bias in the Face of AI: The Role of Human Judgement in AI Generated Text Evaluation</title>
      <link>https://arxiv.org/abs/2410.03723</link>
      <description>arXiv:2410.03723v1 Announce Type: cross 
Abstract: As AI advances in text generation, human trust in AI generated content remains constrained by biases that go beyond concerns of accuracy. This study explores how bias shapes the perception of AI versus human generated content. Through three experiments involving text rephrasing, news article summarization, and persuasive writing, we investigated how human raters respond to labeled and unlabeled content. While the raters could not differentiate the two types of texts in the blind test, they overwhelmingly favored content labeled as "Human Generated," over those labeled "AI Generated," by a preference score of over 30%. We observed the same pattern even when the labels were deliberately swapped. This human bias against AI has broader societal and cognitive implications, as it undervalues AI performance. This study highlights the limitations of human judgment in interacting with AI and offers a foundation for improving human-AI collaboration, especially in creative fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03723v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiffany Zhu, Iain Weissburg, Kexun Zhang, William Yang Wang</dc:creator>
    </item>
    <item>
      <title>KidLM: Advancing Language Models for Children -- Early Insights and Future Directions</title>
      <link>https://arxiv.org/abs/2410.03884</link>
      <description>arXiv:2410.03884v1 Announce Type: cross 
Abstract: Recent studies highlight the potential of large language models in creating educational tools for children, yet significant challenges remain in maintaining key child-specific properties such as linguistic nuances, cognitive needs, and safety standards. In this paper, we explore foundational steps toward the development of child-specific language models, emphasizing the necessity of high-quality pre-training data. We introduce a novel user-centric data collection pipeline that involves gathering and validating a corpus specifically written for and sometimes by children. Additionally, we propose a new training objective, Stratified Masking, which dynamically adjusts masking probabilities based on our domain-specific child language data, enabling models to prioritize vocabulary and concepts more suitable for children. Experimental evaluations demonstrate that our model excels in understanding lower grade-level text, maintains safety by avoiding stereotypes, and captures children's unique preferences. Furthermore, we provide actionable insights for future research and development in child-specific language modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03884v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mir Tafseer Nayeem, Davood Rafiei</dc:creator>
    </item>
    <item>
      <title>Urban Computing for Climate and Environmental Justice: Early Perspectives From Two Research Initiatives</title>
      <link>https://arxiv.org/abs/2410.04318</link>
      <description>arXiv:2410.04318v1 Announce Type: cross 
Abstract: The impacts of climate change are intensifying existing vulnerabilities and disparities within urban communities around the globe, as extreme weather events, including floods and heatwaves, are becoming more frequent and severe, disproportionately affecting low-income and underrepresented groups. Tackling these increasing challenges requires novel approaches that integrate expertise across multiple domains, including computer science, engineering, climate science, and public health. Urban computing can play a pivotal role in these efforts by integrating data from multiple sources to support decision-making and provide actionable insights into weather patterns, infrastructure weaknesses, and population vulnerabilities. However, the capacity to leverage technological advancements varies significantly between the Global South and Global North. In this paper, we present two multiyear, multidisciplinary projects situated in Chicago, USA and Niter\'oi, Brazil, highlighting the opportunities and limitations of urban computing in these diverse contexts. Reflecting on our experiences, we then discuss the essential requirements, as well as existing gaps, for visual analytics tools that facilitate the understanding and mitigation of climate-related risks in urban environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04318v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carolina Veiga, Ashish Sharma, Daniel de Oliveira, Marcos Lage, Fabio Miranda</dc:creator>
    </item>
    <item>
      <title>RespDiff: An End-to-End Multi-scale RNN Diffusion Model for Respiratory Waveform Estimation from PPG Signals</title>
      <link>https://arxiv.org/abs/2410.04366</link>
      <description>arXiv:2410.04366v1 Announce Type: cross 
Abstract: Respiratory rate (RR) is a critical health indicator often monitored under inconvenient scenarios, limiting its practicality for continuous monitoring. Photoplethysmography (PPG) sensors, increasingly integrated into wearable devices, offer a chance to continuously estimate RR in a portable manner. In this paper, we propose RespDiff, an end-to-end multi-scale RNN diffusion model for respiratory waveform estimation from PPG signals. RespDiff does not require hand-crafted features or the exclusion of low-quality signal segments, making it suitable for real-world scenarios. The model employs multi-scale encoders, to extract features at different resolutions, and a bidirectional RNN to process PPG signals and extract respiratory waveform. Additionally, a spectral loss term is introduced to optimize the model further. Experiments conducted on the BIDMC dataset demonstrate that RespDiff outperforms notable previous works, achieving a mean absolute error (MAE) of 1.18 bpm for RR estimation while others range from 1.66 to 2.15 bpm, showing its potential for robust and accurate respiratory monitoring in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04366v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuyang Miao, Zehua Chen, Chang Li, Danilo Mandic</dc:creator>
    </item>
    <item>
      <title>Generalizability analysis of deep learning predictions of human brain responses to augmented and semantically novel visual stimuli</title>
      <link>https://arxiv.org/abs/2410.04497</link>
      <description>arXiv:2410.04497v1 Announce Type: cross 
Abstract: The purpose of this work is to investigate the soundness and utility of a neural network-based approach as a framework for exploring the impact of image enhancement techniques on visual cortex activation. In a preliminary study, we prepare a set of state-of-the-art brain encoding models, selected among the top 10 methods that participated in The Algonauts Project 2023 Challenge [16]. We analyze their ability to make valid predictions about the effects of various image enhancement techniques on neural responses. Given the impossibility of acquiring the actual data due to the high costs associated with brain imaging procedures, our investigation builds up on a series of experiments. Specifically, we analyze the ability of brain encoders to estimate the cerebral reaction to various augmentations by evaluating the response to augmentations targeting objects (i.e., faces and words) with known impact on specific areas. Moreover, we study the predicted activation in response to objects unseen during training, exploring the impact of semantically out-of-distribution stimuli. We provide relevant evidence for the generalization ability of the models forming the proposed framework, which appears to be promising for the identification of the optimal visual augmentation filter for a given task, model-driven design strategies as well as for AR and VR applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04497v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentyn Piskovskyi, Riccardo Chimisso, Sabrina Patania, Tom Foulsham, Giuseppe Vizzari, Dimitri Ognibene</dc:creator>
    </item>
    <item>
      <title>The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?</title>
      <link>https://arxiv.org/abs/2410.04699</link>
      <description>arXiv:2410.04699v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown capabilities close to human performance in various analytical tasks, leading researchers to use them for time and labor-intensive analyses. However, their capability to handle highly specialized and open-ended tasks in domains like policy studies remains in question. This paper investigates the efficiency and accuracy of LLMs in specialized tasks through a structured user study focusing on Human-LLM partnership. The study, conducted in two stages-Topic Discovery and Topic Assignment-integrates LLMs with expert annotators to observe the impact of LLM suggestions on what is usually human-only analysis. Results indicate that LLM-generated topic lists have significant overlap with human generated topic lists, with minor hiccups in missing document-specific topics. However, LLM suggestions may significantly improve task completion speed, but at the same time introduce anchoring bias, potentially affecting the depth and nuance of the analysis, raising a critical question about the trade-off between increased efficiency and the risk of biased analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04699v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander S. Choi, Syeda Sabrina Akter, JP Singh, Antonios Anastasopoulos</dc:creator>
    </item>
    <item>
      <title>The Role of Governments in Increasing Interconnected Post-Deployment Monitoring of AI</title>
      <link>https://arxiv.org/abs/2410.04931</link>
      <description>arXiv:2410.04931v1 Announce Type: cross 
Abstract: Language-based AI systems are diffusing into society, bringing positive and negative impacts. Mitigating negative impacts depends on accurate impact assessments, drawn from an empirical evidence base that makes causal connections between AI usage and impacts. Interconnected post-deployment monitoring combines information about model integration and use, application use, and incidents and impacts. For example, inference time monitoring of chain-of-thought reasoning can be combined with long-term monitoring of sectoral AI diffusion, impacts and incidents. Drawing on information sharing mechanisms in other industries, we highlight example data sources and specific data points that governments could collect to inform AI risk management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04931v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Merlin Stein, Jamie Bernardi, Connor Dunlop</dc:creator>
    </item>
    <item>
      <title>Human-Feedback Efficient Reinforcement Learning for Online Diffusion Model Finetuning</title>
      <link>https://arxiv.org/abs/2410.05116</link>
      <description>arXiv:2410.05116v1 Announce Type: cross 
Abstract: Controllable generation through Stable Diffusion (SD) fine-tuning aims to improve fidelity, safety, and alignment with human guidance. Existing reinforcement learning from human feedback methods usually rely on predefined heuristic reward functions or pretrained reward models built on large-scale datasets, limiting their applicability to scenarios where collecting such data is costly or difficult. To effectively and efficiently utilize human feedback, we develop a framework, HERO, which leverages online human feedback collected on the fly during model learning. Specifically, HERO features two key mechanisms: (1) Feedback-Aligned Representation Learning, an online training method that captures human feedback and provides informative learning signals for fine-tuning, and (2) Feedback-Guided Image Generation, which involves generating images from SD's refined initialization samples, enabling faster convergence towards the evaluator's intent. We demonstrate that HERO is 4x more efficient in online feedback for body part anomaly correction compared to the best existing method. Additionally, experiments show that HERO can effectively handle tasks like reasoning, counting, personalization, and reducing NSFW content with only 0.5K online feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05116v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayano Hiranaka, Shang-Fu Chen, Chieh-Hsin Lai, Dongjun Kim, Naoki Murata, Takashi Shibuya, Wei-Hsiang Liao, Shao-Hua Sun, Yuki Mitsufuji</dc:creator>
    </item>
    <item>
      <title>Auxilio and Beyond: Comparative Evaluation, Usability, and Design Guidelines for Head Movement-based Assistive Mouse Controllers</title>
      <link>https://arxiv.org/abs/2210.04483</link>
      <description>arXiv:2210.04483v2 Announce Type: replace 
Abstract: Upper limb disability due to neurological disorders or other factors restricts computer interaction for affected individuals using a generic optical mouse. This work reports the findings of a comparative evaluation of Auxilio, a sensor-based wireless head-mounted Assistive Mouse Controller (AMC), that facilitates computer interaction for such individuals. Combining commercially available, low-cost motion and infrared sensors, Auxilio utilizes head movements and cheek muscle twitches for mouse control. Its performance in pointing tasks with subjects without motor impairments has been juxtaposed against a commercially available and patented vision-based head-tracking AMC developed for similar stakeholders. Furthermore, our study evaluates the usability of Auxilio using the System Usability Scale, supplemented by a qualitative analysis of participant interview transcripts to identify the strengths and weaknesses of both AMCs. Experimental results demonstrate the feasibility and effectiveness of Auxilio, and we summarize our key findings into design guidelines for the development of similar future AMCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.04483v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammad Ridwan Kabir, Mohammad Ishrak Abedin, Rizvi Ahmed, Saad Bin Ashraf, Hasan Mahmud, Md. Kamrul Hasan</dc:creator>
    </item>
    <item>
      <title>Twenty-Four Years of Empirical Research on Trust in AI: A Bibliometric Review of Trends, Overlooked Issues, and Future Directions</title>
      <link>https://arxiv.org/abs/2309.09828</link>
      <description>arXiv:2309.09828v2 Announce Type: replace 
Abstract: Trust is widely regarded as a critical component to building artificial intelligence (AI) systems that people will use and safely rely upon. As research in this area continues to evolve, it becomes imperative that the research community synchronizes its empirical efforts and aligns on the path toward effective knowledge creation. To lay the groundwork toward achieving this objective, we performed a comprehensive bibliometric analysis, supplemented with a qualitative content analysis of over two decades of empirical research measuring trust in AI, comprising 1'156 core articles and 36'306 cited articles across multiple disciplines. Our analysis reveals several "elephants in the room" pertaining to missing perspectives in global discussions on trust in AI, a lack of contextualized theoretical models and a reliance on exploratory methodologies. We highlight strategies for the empirical research community that are aimed at fostering an in-depth understanding of trust in AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.09828v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00146-024-02059-y</arxiv:DOI>
      <arxiv:journal_reference>Published in AI &amp; Society (2024)</arxiv:journal_reference>
      <dc:creator>Michaela Benk, Sophie Kerstan, Florian v. Wangenheim, Andrea Ferrario</dc:creator>
    </item>
    <item>
      <title>Panda or not Panda? Understanding Adversarial Attacks with Interactive Visualization</title>
      <link>https://arxiv.org/abs/2311.13656</link>
      <description>arXiv:2311.13656v2 Announce Type: replace 
Abstract: Adversarial machine learning (AML) studies attacks that can fool machine learning algorithms into generating incorrect outcomes as well as the defenses against worst-case attacks to strengthen model robustness. Specifically for image classification, it is challenging to understand adversarial attacks due to their use of subtle perturbations that are not human-interpretable, as well as the variability of attack impacts influenced by diverse methodologies, instance differences, and model architectures. Through a design study with AML learners and teachers, we introduce AdvEx, a multi-level interactive visualization system that comprehensively presents the properties and impacts of evasion attacks on different image classifiers for novice AML learners. We quantitatively and qualitatively assessed AdvEx in a two-part evaluation including user studies and expert interviews. Our results show that AdvEx is not only highly effective as a visualization tool for understanding AML mechanisms, but also provides an engaging and enjoyable learning experience, thus demonstrating its overall benefits for AML learners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13656v2</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuzhe You, Jarvis Tse, Jian Zhao</dc:creator>
    </item>
    <item>
      <title>VizGroup: An AI-Assisted Event-Driven System for Real-Time Collaborative Programming Learning Analytics</title>
      <link>https://arxiv.org/abs/2404.08743</link>
      <description>arXiv:2404.08743v2 Announce Type: replace 
Abstract: Programming instructors often conduct collaborative learning activities, like Peer Instruction, to foster a deeper understanding in students and enhance their engagement with learning. These activities, however, may not always yield productive outcomes due to the diversity of student mental models and their ineffective collaboration. In this work, we introduce VizGroup, an AI-assisted system that enables programming instructors to easily oversee students' real-time collaborative learning behaviors during large programming courses. VizGroup leverages Large Language Models (LLMs) to recommend event specifications for instructors so that they can simultaneously track and receive alerts about key correlation patterns between various collaboration metrics and ongoing coding tasks. We evaluated VizGroup with 12 instructors in a comparison study using a dataset collected from a Peer Instruction activity that was conducted in a large programming lecture. The results showed that VizGroup helped instructors effectively overview, narrow down, and track nuances throughout students' behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08743v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaohang Tang, Sam Wong, Kevin Pu, Xi Chen, Yalong Yang, Yan Chen</dc:creator>
    </item>
    <item>
      <title>IntelliExplain: Enhancing Interactive Code Generation through Natural Language Explanations for Non-Professional Programmers</title>
      <link>https://arxiv.org/abs/2405.10250</link>
      <description>arXiv:2405.10250v2 Announce Type: replace 
Abstract: Chat LLMs such as GPT-3.5-turbo and GPT-4 have shown promise in assisting humans in coding, particularly by enabling them to conversationally provide feedback. However, current approaches assume users have expert debugging skills, limiting accessibility for non-professional programmers. In this paper, we first explore Chat LLMs' limitations in assisting non-professional programmers with coding. Through a formative study, we identify two key elements affecting their experience: the way a Chat LLM explains its generated code and the structure of human-LLM interaction. We then propose IntelliExplain, a new conversational code generation framework with enhanced code explanations and a structured interaction paradigm, which enforces both better code understanding and a more effective feedback loop. In two programming tasks (SQL and Python), IntelliExplain yields significantly higher success rates and reduces task time compared to the vanilla Chat LLM. We also identify several opportunities that remain in effectively offering a chat-based programming experience for non-professional programmers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10250v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Yan, Thomas D. Latoza, Ziyu Yao</dc:creator>
    </item>
    <item>
      <title>"I Like Sunnie More Than I Expected!": Exploring User Expectation and Perception of an Anthropomorphic LLM-based Conversational Agent for Well-Being Support</title>
      <link>https://arxiv.org/abs/2405.13803</link>
      <description>arXiv:2405.13803v3 Announce Type: replace 
Abstract: The human-computer interaction (HCI) research community has a longstanding interest in exploring the mismatch between users' actual experiences and expectation toward new technologies, for instance, large language models (LLMs). In this study, we compared users' (N = 38) initial expectations against their post-interaction perceptions of two LLM-powered mental well-being intervention activity recommendation systems. Both systems have a built-in LLM to recommend a personalized well-being intervention activity, but one system (Sunnie) has an anthropomorphic conversational interaction design via elements such as appearance, persona, and natural conversation. Results showed that user engagement was high with both systems, and both systems exceeded users' expectations along the utility dimension, highlighting AI's potential to offer useful intervention activity recommendations. In addition, Sunnie further outperformed the non-anthropomorphic baseline system in relational warmth. These findings suggest that anthropomorphic conversational interaction design may be particularly effective in fostering warmth in mental health support contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13803v3</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Siyi Wu, Julie Y. A. Cachia, Feixue Han, Bingsheng Yao, Tianyi Xie, Xuan Zhao, Dakuo Wang</dc:creator>
    </item>
    <item>
      <title>Review on the Role of Virtual Reality in Reducing Mental Health Diseases Specifically Stress, Anxiety, and Depression</title>
      <link>https://arxiv.org/abs/2407.18918</link>
      <description>arXiv:2407.18918v2 Announce Type: replace 
Abstract: Objective: Virtual Reality (VR) is a technological interface that allows users to interact with a simulated environment. VR has been used extensively for mental health and clinical research. Mental health disorders are globally burdening health problems in the world. According to the Psychological Interventions Implementation Manual published by WHO on 6th March 2024, around one in eight people in the world lived with a mental disorder. This literature review is synthesized to find out the effects of VR therapy on stress, anxiety and depression. Method: We used Google Scholar database using keywords of VR, stress, anxiety and depression. Publication from last ten years (2014 to 1024) are considered. Researches only in the English language are included. All the papers and articles with the keyword VR missing were rejected. Result: Google Scholar yielded 17,700 results from our keywords. Nine studies met our search criteria that are included in this review. Out of nine, five studies encountered mental stress and gave effective results in reducing it by VR therapy. The other four targeted mood disorders, Social anxiety disorders, depression, loss of happiness and sleep deprivation. They also showed immense potential in reducing mental illness while using VR. Conclusion: Findings are in favor of the effectiveness of VR in reducing stress, anxiety and depression. Still, it is insufficient evidence to consider VR as solely independent treatment over the traditional medication. In future, the limitations can be overcome to relying on VR and using it in hospitals as a reliable source of cure for mental illness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18918v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sadia Saeed, Khan Bahadar Khan, Muhammad Abul Hassan, Abdul Qayyum, Saba Salahuddin</dc:creator>
    </item>
    <item>
      <title>Dark Mode or Light Mode? Exploring the Impact of Contrast Polarity on Visualization Performance Between Age Groups</title>
      <link>https://arxiv.org/abs/2409.10841</link>
      <description>arXiv:2409.10841v2 Announce Type: replace 
Abstract: This study examines the impact of positive and negative contrast polarities (i.e., light and dark modes) on the performance of younger adults and people in their late adulthood (PLA). In a crowdsourced study with 134 participants (69 below age 60, 66 aged 60 and above), we assessed their accuracy and time performing analysis tasks across three common visualization types (Bar, Line, Scatterplot) and two contrast polarities (positive and negative). We observed that, across both age groups, the polarity that led to better performance and the resulting amount of improvement varied on an individual basis, with each polarity benefiting comparable proportions of participants. However, the contrast polarity that led to better performance did not always match their preferred polarity. Additionally, we observed that the choice of contrast polarity can have an impact on time similar to that of the choice of visualization type, resulting in an average percent difference of around 36%. These findings indicate that, overall, the effects of contrast polarity on visual analysis performance do not noticeably change with age. Furthermore, they underscore the importance of making visualizations available in both contrast polarities to better-support a broad audience with differing needs. Supplementary materials for this work can be found at https://osf.io/539a4/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10841v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zack While, Ali Sarvghad</dc:creator>
    </item>
    <item>
      <title>"It Explains What I am Currently Going Through Perfectly to a Tee": Understanding User Perceptions on LLM-Enhanced Narrative Interventions</title>
      <link>https://arxiv.org/abs/2409.16732</link>
      <description>arXiv:2409.16732v2 Announce Type: replace 
Abstract: Stories about overcoming personal struggles can effectively illustrate the application of psychological theories in real life, yet they may fail to resonate with individuals' experiences. In this work, we employ large language models (LLMs) to create tailored narratives that acknowledge and address unique challenging thoughts and situations faced by individuals. Our study, involving 346 young adults across two settings, demonstrates that LLM-enhanced stories were perceived to be better than human-written ones in conveying key takeaways, promoting reflection, and reducing belief in negative thoughts. These stories were not only seen as more relatable but also similarly authentic to human-written ones, highlighting the potential of LLMs in helping young adults manage their struggles. The findings of this work provide crucial design considerations for future narrative-based digital mental health interventions, such as the need to maintain relatability without veering into implausibility and refining the wording and tone of AI-enhanced content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16732v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ananya Bhattacharjee, Sarah Yi Xu, Pranav Rao, Yuchen Zeng, Jonah Meyerhoff, Syed Ishtiaque Ahmed, David C Mohr, Michael Liut, Alex Mariakakis, Rachel Kornfield, Joseph Jay Williams</dc:creator>
    </item>
    <item>
      <title>Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception</title>
      <link>https://arxiv.org/abs/2308.05822</link>
      <description>arXiv:2308.05822v2 Announce Type: replace-cross 
Abstract: We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as lifelogging. However, a significant challenge arises from the sheer volume of video data generated through lifelogging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation agent that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Additionally, we propose using large language models to facilitate natural language querying. Our agent underwent extensive evaluation using the QA-Ego4D dataset and achieved state-of-the-art results with a BLEU score of 8.3, outperforming conventional machine learning models that scored between 3.4 and 5.8. Additionally, we conducted a user study in which participants interacted with the human memory augmentation agent through episodic memory and open-ended questions. The results of this study show that the agent results in significantly better recall performance on episodic memory tasks compared to human participants. The results also highlight the agent's practical applicability and user acceptance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05822v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junxiao Shen, John Dudley, Per Ola Kristensson</dc:creator>
    </item>
    <item>
      <title>A mobile digital device proficiency performance test for cognitive clinical research</title>
      <link>https://arxiv.org/abs/2310.01774</link>
      <description>arXiv:2310.01774v2 Announce Type: replace-cross 
Abstract: Mobile device proficiency is increasingly important for everyday living, including to deliver healthcare services. Human-device interactions represent a potential in cognitive neurology and aging research. Although traditional pen-and-paper evaluations serve as valuable tools within public health strategies for population-scale cognitive assessments, digital devices could amplify cognitive assessment. However, even person-centered studies often fail to incorporate measures of mobile device proficiency and research with digital mobile technology frequently neglects these evaluations. Besides that, cognitive screening, a fundamental part of brain health evaluation and a widely accepted strategy to identify high-risk individuals vulnerable to cognitive impairment and dementia, has research using digital devices for older adults in need for standardization. To address this shortfall, the DigiTAU collaborative and interdisciplinary project is creating refined methodological parameters for the investigation of digital biomarkers. With careful consideration of cognitive design elements, here we describe the open-source and performance-based Mobile Device Abilities Test (MDAT), a simple, low-cost, and reproductible open-sourced test framework. This result was achieved with a cross-sectional study population sample of 101 low and middle-income subjects aged 20 to 79 years old. Partial least squares structural equation modeling (PLS-SEM) was used to assess the measurement of the construct. It was possible to achieve a reliable method with internal consistency, good content validity related to digital competences, and that does not have much interference with auto-perceived global functional disability, health self-perception, and motor dexterity. Limitations for this method are discussed and paths to improve and establish better standards are highlighted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01774v2</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alan Cronemberger Andrade, Di\'ogenes de Souza Bido, Ana Carolina Bottura de Barros, Walter Richard Boot, Paulo Henrique Ferreira Bertolucci</dc:creator>
    </item>
    <item>
      <title>When "A Helpful Assistant" Is Not Really Helpful: Personas in System Prompts Do Not Improve Performances of Large Language Models</title>
      <link>https://arxiv.org/abs/2311.10054</link>
      <description>arXiv:2311.10054v2 Announce Type: replace-cross 
Abstract: Prompting serves as the major way humans interact with Large Language Models (LLM). Commercial AI systems commonly define the role of the LLM in system prompts. For example, ChatGPT uses "You are a helpful assistant" as part of its default system prompt. Despite current practices of adding personas to system prompts, it remains unclear how different personas affect a model's performance on objective tasks. In this study, we present a systematic evaluation of personas in system prompts. We curate a list of 162 roles covering 6 types of interpersonal relationships and 8 domains of expertise. Through extensive analysis of 4 popular families of LLMs and 2,410 factual questions, we demonstrate that adding personas in system prompts does not improve model performance across a range of questions compared to the control setting where no persona is added. Nevertheless, further analysis suggests that the gender, type, and domain of the persona can all influence the resulting prediction accuracies. We further experimented with a list of persona search strategies and found that, while aggregating results from the best persona for each question significantly improves prediction accuracy, automatically identifying the best persona is challenging, with predictions often performing no better than random selection. Overall, our findings suggest that while adding a persona may lead to performance gains in certain settings, the effect of each persona can be largely random. Code and data are available at https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10054v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran, Moontae Lee, David Jurgens</dc:creator>
    </item>
    <item>
      <title>Eating Speed Measurement Using Wrist-Worn IMU Sensors Towards Free-Living Environments</title>
      <link>https://arxiv.org/abs/2401.05376</link>
      <description>arXiv:2401.05376v2 Announce Type: replace-cross 
Abstract: Eating speed is an important indicator that has been widely investigated in nutritional studies. The relationship between eating speed and several intake-related problems such as obesity, diabetes, and oral health has received increased attention from researchers. However, existing studies mainly use self-reported questionnaires to obtain participants' eating speed, where they choose options from slow, medium, and fast. Such a non-quantitative method is highly subjective and coarse at the individual level. This study integrates two classical tasks in automated food intake monitoring domain: bite detection and eating episode detection, to advance eating speed measurement in near-free-living environments automatically and objectively. Specifically, a temporal convolutional network combined with a multi-head attention module (TCN-MHA) is developed to detect bites (including eating and drinking gestures) from IMU data. The predicted bite sequences are then clustered into eating episodes. Eating speed is calculated by using the time taken to finish the eating episode to divide the number of bites. To validate the proposed approach on eating speed measurement, a 7-fold cross validation is applied to the self-collected fine-annotated full-day-I (FD-I) dataset, and a holdout experiment is conducted on the full-day-II (FD-II) dataset. The two datasets are collected from 61 participants with a total duration of 513 h, which are publicly available. Experimental results show that the proposed approach achieves a mean absolute percentage error (MAPE) of 0.110 and 0.146 in the FD-I and FD-II datasets, respectively, showcasing the feasibility of automated eating speed measurement in near-free-living environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05376v2</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JBHI.2024.3422875</arxiv:DOI>
      <arxiv:journal_reference>IEEE Journal of Biomedical and Health Informatics, vol. 28, no. 10, pp. 5816-5828, Oct. 2024</arxiv:journal_reference>
      <dc:creator>Chunzhuo Wang, T. Sunil Kumar, Walter De Raedt, Guido Camps, Hans Hallez, Bart Vanrumste</dc:creator>
    </item>
    <item>
      <title>Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections</title>
      <link>https://arxiv.org/abs/2402.16973</link>
      <description>arXiv:2402.16973v2 Announce Type: replace-cross 
Abstract: Language models will inevitably err in situations with which they are unfamiliar. However, by effectively communicating uncertainties, they can still guide humans toward making sound decisions in those contexts. We demonstrate this idea by developing HEAR, a system that can successfully guide humans in simulated residential environments despite generating potentially inaccurate instructions. Diverging from systems that provide users with only the instructions they generate, HEAR warns users of potential errors in its instructions and suggests corrections. This rich uncertainty information effectively prevents misguidance and reduces the search space for users. Evaluation with 80 users shows that HEAR achieves a 13% increase in success rate and a 29% reduction in final location error distance compared to only presenting instructions to users. Interestingly, we find that offering users possibilities to explore, HEAR motivates them to make more attempts at the task, ultimately leading to a higher success rate. To our best knowledge, this work is the first to show the practical benefits of uncertainty communication in a long-horizon sequential decision-making problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16973v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lingjun Zhao, Khanh Nguyen, Hal Daum\'e III</dc:creator>
    </item>
    <item>
      <title>Creative Beam Search: LLM-as-a-Judge For Improving Response Generation</title>
      <link>https://arxiv.org/abs/2405.00099</link>
      <description>arXiv:2405.00099v4 Announce Type: replace-cross 
Abstract: Large language models are revolutionizing several areas, including artificial creativity. However, the process of generation in machines profoundly diverges from that observed in humans. In particular, machine generation is characterized by a lack of intentionality and an underlying creative process. We propose a method called Creative Beam Search that uses Diverse Beam Search and LLM-as-a-Judge to perform response generation and response validation. The results of a qualitative experiment show how our approach can provide better output than standard sampling techniques. We also show that the response validation step is a necessary complement to the response generation step.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00099v4</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giorgio Franceschelli, Mirco Musolesi</dc:creator>
    </item>
    <item>
      <title>A Dynamic Model of Performative Human-ML Collaboration: Theory and Empirical Evidence</title>
      <link>https://arxiv.org/abs/2405.13753</link>
      <description>arXiv:2405.13753v3 Announce Type: replace-cross 
Abstract: Machine learning (ML) models are increasingly used in various applications, from recommendation systems in e-commerce to diagnosis prediction in healthcare. In this paper, we present a novel dynamic framework for thinking about the deployment of ML models in a performative, human-ML collaborative system. In our framework, the introduction of ML recommendations changes the data-generating process of human decisions, which are only a proxy to the ground truth and which are then used to train future versions of the model. We show that this dynamic process in principle can converge to different stable points, i.e. where the ML model and the Human+ML system have the same performance. Some of these stable points are suboptimal with respect to the actual ground truth. As a proof of concept, we conduct an empirical user study with 1,408 participants. In the study, humans solve instances of the knapsack problem with the help of machine learning predictions of varying performance. This is an ideal setting because we can identify the actual ground truth, and evaluate the performance of human decisions supported by ML recommendations. We find that for many levels of ML performance, humans can improve upon the ML predictions. We also find that the improvement could be even higher if humans rationally followed the ML recommendations. Finally, we test whether monetary incentives can increase the quality of human decisions, but we fail to find any positive effect. Using our empirical data to approximate our collaborative system suggests that the learning process would dynamically reach an equilibrium performance that is around 92% of the maximum knapsack value. Our results have practical implications for the deployment of ML models in contexts where human decisions may deviate from the indisputable ground truth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13753v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom S\"uhr, Samira Samadi, Chiara Farronato</dc:creator>
    </item>
    <item>
      <title>NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative</title>
      <link>https://arxiv.org/abs/2406.06499</link>
      <description>arXiv:2406.06499v2 Announce Type: replace-cross 
Abstract: Existing video captioning benchmarks and models lack coherent representations of causal-temporal narrative, which is sequences of events linked through cause and effect, unfolding over time and driven by characters or agents. This lack of narrative restricts models' ability to generate text descriptions that capture the causal and temporal dynamics inherent in video content. To address this gap, we propose NarrativeBridge, an approach comprising of: (1) a novel Causal-Temporal Narrative (CTN) captions benchmark generated using a large language model and few-shot prompting, explicitly encoding cause-effect temporal relationships in video descriptions, evaluated automatically to ensure caption quality and relevance and validated through human evaluation; and (2) a dedicated Cause-Effect Network (CEN) architecture with separate encoders for capturing cause and effect dynamics independently, enabling effective learning and generation of captions with causal-temporal narrative. Extensive experiments demonstrate that CEN significantly outperforms state-of-the-art models, including fine-tuned vision-language models, and is more accurate in articulating the causal and temporal aspects of video content than the second best model (GIT): 17.88 and 17.44 CIDEr on the MSVD and MSR-VTT datasets, respectively. Cross-dataset evaluations further showcase CEN's strong generalization capabilities. The proposed framework understands and generates nuanced text descriptions with intricate causal-temporal narrative structures present in videos, addressing a critical limitation in video captioning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06499v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asmar Nadeem, Faegheh Sardari, Robert Dawes, Syed Sameed Husain, Adrian Hilton, Armin Mustafa</dc:creator>
    </item>
    <item>
      <title>Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN</title>
      <link>https://arxiv.org/abs/2406.15003</link>
      <description>arXiv:2406.15003v2 Announce Type: replace-cross 
Abstract: Hand Gesture Recognition (HGR) enables intuitive human-computer interactions in various real-world contexts. However, existing frameworks often struggle to meet the real-time requirements essential for practical HGR applications. This study introduces a robust, skeleton-based framework for dynamic HGR that simplifies the recognition of dynamic hand gestures into a static image classification task, effectively reducing both hardware and computational demands. Our framework utilizes a data-level fusion technique to encode 3D skeleton data from dynamic gestures into static RGB spatiotemporal images. It incorporates a specialized end-to-end Ensemble Tuner (e2eET) Multi-Stream CNN architecture that optimizes the semantic connections between data representations while minimizing computational needs. Tested across five benchmark datasets (SHREC'17, DHG-14/28, FPHA, LMDHG, and CNR), the framework showed competitive performance with the state-of-the-art. Its capability to support real-time HGR applications was also demonstrated through deployment on standard consumer PC hardware, showcasing low latency and minimal resource usage in real-world settings. The successful deployment of this framework underscores its potential to enhance real-time applications in fields such as virtual/augmented reality, ambient intelligence, and assistive technologies, providing a scalable and efficient solution for dynamic gesture recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15003v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oluwaleke Yusuf, Maki Habib, Mohamed Moustafa</dc:creator>
    </item>
    <item>
      <title>Knowledge Mechanisms in Large Language Models: A Survey and Perspective</title>
      <link>https://arxiv.org/abs/2407.15017</link>
      <description>arXiv:2407.15017v3 Announce Type: replace-cross 
Abstract: Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial for advancing towards trustworthy AGI. This paper reviews knowledge mechanism analysis from a novel taxonomy including knowledge utilization and evolution. Knowledge utilization delves into the mechanism of memorization, comprehension and application, and creation. Knowledge evolution focuses on the dynamic progression of knowledge within individual and group LLMs. Moreover, we discuss what knowledge LLMs have learned, the reasons for the fragility of parametric knowledge, and the potential dark knowledge (hypothesis) that will be challenging to address. We hope this work can help understand knowledge in LLMs and provide insights for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15017v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang</dc:creator>
    </item>
    <item>
      <title>A Survey on Trustworthiness in Foundation Models for Medical Image Analysis</title>
      <link>https://arxiv.org/abs/2407.15851</link>
      <description>arXiv:2407.15851v2 Announce Type: replace-cross 
Abstract: The rapid advancement of foundation models in medical imaging represents a significant leap toward enhancing diagnostic accuracy and personalized treatment. However, the deployment of foundation models in healthcare necessitates a rigorous examination of their trustworthiness, encompassing privacy, robustness, reliability, explainability, and fairness. The current body of survey literature on foundation models in medical imaging reveals considerable gaps, particularly in the area of trustworthiness. Additionally, existing surveys on the trustworthiness of foundation models do not adequately address their specific variations and applications within the medical imaging domain. This survey aims to fill that gap by presenting a novel taxonomy of foundation models used in medical imaging and analyzing the key motivations for ensuring their trustworthiness. We review current research on foundation models in major medical imaging applications, focusing on segmentation, medical report generation, medical question and answering (Q\&amp;A), and disease diagnosis. These areas are highlighted because they have seen a relatively mature and substantial number of foundation models compared to other applications. We focus on literature that discusses trustworthiness in medical image analysis manuscripts. We explore the complex challenges of building trustworthy foundation models for each application, summarizing current concerns and strategies for enhancing trustworthiness. Furthermore, we examine the potential of these models to revolutionize patient care. Our analysis underscores the imperative for advancing towards trustworthy AI in medical image analysis, advocating for a balanced approach that fosters innovation while ensuring ethical and equitable healthcare delivery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15851v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Congzhen Shi, Ryan Rezai, Jiaxi Yang, Qi Dou, Xiaoxiao Li</dc:creator>
    </item>
  </channel>
</rss>
