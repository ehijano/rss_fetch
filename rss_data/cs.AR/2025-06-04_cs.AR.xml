<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Jun 2025 01:38:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>HEC: Equivalence Verification Checking for Code Transformation via Equality Saturation</title>
      <link>https://arxiv.org/abs/2506.02290</link>
      <description>arXiv:2506.02290v1 Announce Type: new 
Abstract: In modern computing systems, compilation employs numerous optimization techniques to enhance code performance. Source-to-source code transformations, which include control flow and datapath transformations, have been widely used in High-Level Synthesis (HLS) and compiler optimization.
  While researchers actively investigate methods to improve performance with source-to-source code transformations, they often overlook the significance of verifying their correctness. Current tools cannot provide a holistic verification of these transformations. This paper introduces HEC, a framework for equivalence checking that leverages the e-graph data structure to comprehensively verify functional equivalence between programs. HEC utilizes the MLIR as its frontend and integrates MLIR into the e-graph framework. Through the combination of dynamic and static e-graph rewriting, HEC facilitates the validation of comprehensive code transformations.
  We demonstrate effectiveness of HEC on PolyBenchC benchmarks, successfully verifying loop unrolling, tiling, and fusion transformations. HEC processes over 100,000 lines of MLIR code in 40 minutes with predictable runtime scaling. Importantly, HEC identified two critical compilation errors in mlir-opt: loop boundary check errors causing unintended executions during unrolling, and memory read-after-write violations in loop fusion that alter program semantics. These findings demonstrate HEC practical value in detecting real-world compiler bugs and highlight the importance of formal verification in optimization pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02290v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaqi Yin, Zhan Song, Nicolas Bohm Agostini, Antonino Tumeo, Cunxi Yu</dc:creator>
    </item>
    <item>
      <title>Unicorn-CIM: Unconvering the Vulnerability and Improving the Resilience of High-Precision Compute-in-Memory</title>
      <link>https://arxiv.org/abs/2506.02311</link>
      <description>arXiv:2506.02311v1 Announce Type: new 
Abstract: Compute-in-memory (CIM) architecture has been widely
  explored to address the von Neumann bottleneck in accelerating deep
  neural networks (DNNs). However, its reliability remains largely understudied, particularly in the emerging domain of floating-point (FP)
  CIM, which is crucial for speeding up high-precision inference and on device training. This paper introduces Unicorn-CIM, a framework to
  uncover the vulnerability and improve the resilience of high-precision
  CIM, built on static random-access memory (SRAM)-based FP CIM
  architecture. Through the development of fault injection and extensive
  characterizations across multiple DNNs, Unicorn-CIM reveals how soft
  errors manifest in FP operations and impact overall model performance.
  Specifically, we find that high-precision DNNs are extremely sensitive
  to errors in the exponent part of FP numbers. Building on this insight,
  Unicorn-CIM develops an efficient algorithm-hardware co-design method
  that optimizes model exponent distribution through fine-tuning and
  incorporates a lightweight Error Correcting Code (ECC) scheme to
  safeguard high-precision DNNs on FP CIM. Comprehensive experiments
  show that our approach introduces just an 8.98% minimal logic overhead
  on the exponent processing path while providing robust error protection
  and maintaining model accuracy. This work paves the way for developing
  more reliable and efficient CIM hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02311v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiufeng Li, Yiwen Liang, Weidong Cao</dc:creator>
    </item>
    <item>
      <title>Memory Access Vectors: Improving Sampling Fidelity for CPU Performance Simulations</title>
      <link>https://arxiv.org/abs/2506.02344</link>
      <description>arXiv:2506.02344v1 Announce Type: new 
Abstract: Accurate performance projection of large-scale benchmarks is essential for CPU architects to evaluate and optimize future processor designs. SimPoint sampling, which uses Basic Block Vectors (BBVs), is a widely adopted technique to reduce simulation time by selecting representative program phases. However, BBVs often fail to capture the behavior of applications with extensive array-indirect memory accesses, leading to inaccurate projections. In particular, the 523.xalancbmk_r benchmark exhibits complex data movement patterns that challenge traditional SimPoint methods. To address this, we propose enhancing SimPoint's BBV methodology by incorporating Memory Access Vectors (MAV), a microarchitecture independent technique that tracks functional memory access patterns. This combined approach significantly improves the projection accuracy of 523.xalancbmk_r on a 192-core system-on-chip, increasing it from 80% to 98%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02344v1</guid>
      <category>cs.AR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sriyash Caculo, Mahesh Madhav, Jeff Baxter</dc:creator>
    </item>
    <item>
      <title>Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention</title>
      <link>https://arxiv.org/abs/2506.02523</link>
      <description>arXiv:2506.02523v1 Announce Type: new 
Abstract: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, improves the efficiency of large language models by projecting query, key, and value tensors into a compact latent space. This architectural change reduces the KV-cache size and significantly lowers memory bandwidth demands, particularly in the autoregressive decode phase. This letter presents the first hardware-centric analysis of MLA, comparing it to conventional Multi-Head Attention (MHA) and evaluating its implications for accelerator performance. We identify two alternative execution schemes of MLA--reusing, resp. recomputing latent projection matrices--which offer distinct trade-offs between compute and memory access. Using the Stream design space exploration framework, we model their throughput and energy cost across a range of hardware platforms and find that MLA can shift attention workloads toward the compute-bound regime.
  Our results show that MLA not only reduces bandwidth usage but also enables adaptable execution strategies aligned with hardware constraints. Compared to MHA, it provides more stable and efficient performance, particularly on bandwidth-limited hardware platforms. These findings emphasize MLA's relevance as a co-design opportunity for future AI accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02523v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robin Geens, Marian Verhelst</dc:creator>
    </item>
    <item>
      <title>CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the Edge</title>
      <link>https://arxiv.org/abs/2506.02847</link>
      <description>arXiv:2506.02847v1 Announce Type: new 
Abstract: Deploying large language models (LLMs) on edge devices is crucial for delivering fast responses and ensuring data privacy. However, the limited storage, weight, and power of edge devices make it difficult to deploy LLM-powered applications. These devices must balance latency requirements with energy consumption and model accuracy. In this paper, we first quantify the challenges of deploying LLMs on off-the-shelf edge devices and then we present CLONE, an in-depth algorithm-hardware co-design at both the model- and system-level that intelligently integrates real-time, energy optimization while maintaining robust generality. In order to maximize the synergistic benefits of these algorithms in always-on and intermediate edge computing settings, we specialize in a 28nm scalable hardware accelerator system. We implement and extensively evaluate CLONE on two off-the-shelf edge platforms. Experiments show that CLONE effectively accelerates the inference process up to 11.92x, and saves energy up to 7.36x, while maintaining high-generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02847v1</guid>
      <category>cs.AR</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunlin Tian, Xinpeng Qin, Kahou Tam, Li Li, Zijian Wang, Yuanzhe Zhao, Minglei Zhang, Chengzhong Xu</dc:creator>
    </item>
    <item>
      <title>Large Processor Chip Model</title>
      <link>https://arxiv.org/abs/2506.02929</link>
      <description>arXiv:2506.02929v1 Announce Type: new 
Abstract: Computer System Architecture serves as a crucial bridge between software applications and the underlying hardware, encompassing components like compilers, CPUs, coprocessors, and RTL designs. Its development, from early mainframes to modern domain-specific architectures, has been driven by rising computational demands and advancements in semiconductor technology. However, traditional paradigms in computer system architecture design are confronting significant challenges, including a reliance on manual expertise, fragmented optimization across software and hardware layers, and high costs associated with exploring expansive design spaces. While automated methods leveraging optimization algorithms and machine learning have improved efficiency, they remain constrained by a single-stage focus, limited data availability, and a lack of comprehensive human domain knowledge. The emergence of large language models offers transformative opportunities for the design of computer system architecture. By leveraging the capabilities of LLMs in areas such as code generation, data analysis, and performance modeling, the traditional manual design process can be transitioned to a machine-based automated design approach. To harness this potential, we present the Large Processor Chip Model (LPCM), an LLM-driven framework aimed at achieving end-to-end automated computer architecture design. The LPCM is structured into three levels: Human-Centric; Agent-Orchestrated; and Model-Governed. This paper utilizes 3D Gaussian Splatting as a representative workload and employs the concept of software-hardware collaborative design to examine the implementation of the LPCM at Level 1, demonstrating the effectiveness of the proposed approach. Furthermore, this paper provides an in-depth discussion on the pathway to implementing Level 2 and Level 3 of the LPCM, along with an analysis of the existing challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02929v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaiyan Chang, Mingzhi Chen, Yunji Chen, Zhirong Chen, Dongrui Fan, Junfeng Gong, Nan Guo, Yinhe Han, Qinfen Hao, Shuo Hou, Xuan Huang, Pengwei Jin, Changxin Ke, Cangyuan Li, Guangli Li, Huawei Li, Kuan Li, Naipeng Li, Shengwen Liang, Cheng Liu, Hongwei Liu, Jiahua Liu, Junliang Lv, Jianan Mu, Jin Qin, Bin Sun, Chenxi Wang, Duo Wang, Mingjun Wang, Ying Wang, Chenggang Wu, Peiyang Wu, Teng Wu, Xiao Xiao, Mengyao Xie, Chenwei Xiong, Ruiyuan Xu, Mingyu Yan, Xiaochun Ye, Kuai Yu, Rui Zhang, Shuoming Zhang, Jiacheng Zhao</dc:creator>
    </item>
    <item>
      <title>Minimal Neuron Circuits -- Part I: Resonators</title>
      <link>https://arxiv.org/abs/2506.02341</link>
      <description>arXiv:2506.02341v1 Announce Type: cross 
Abstract: Spiking Neural Networks have earned increased recognition in recent years owing to their biological plausibility and event-driven computation. Spiking neurons are the fundamental building components of Spiking Neural Networks. Those neurons act as computational units that determine the decision to fire an action potential. This work presents a methodology to implement biologically plausible yet scalable spiking neurons in hardware. We show that it is more efficient to design neurons that mimic the $I_{Na,p}+I_{K}$ model rather than the more complicated Hodgkin-Huxley model. We demonstrate our methodology by presenting eleven novel minimal spiking neuron circuits in Parts I and II of the paper. We categorize the neuron circuits presented into two types: Resonators and Integrators. We discuss the methodology employed in designing neurons of the resonator type in Part I, while we discuss neurons of the integrator type in Part II. In part I, we postulate that Sodium channels exhibit type-N negative differential resistance. Consequently, we present three novel minimal neuron circuits that use type-N negative differential resistance circuits or devices as the Sodium channel. Nevertheless, the aim of the paper is not to present a set of minimal neuron circuits but rather the methodology utilized to construct those circuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02341v1</guid>
      <category>cs.NE</category>
      <category>cs.AR</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amr Nabil, T. Nandha Kumar, Haider Abbas F. Almurib</dc:creator>
    </item>
    <item>
      <title>Learning Cache Coherence Traffic for NoC Routing Design</title>
      <link>https://arxiv.org/abs/2504.04005</link>
      <description>arXiv:2504.04005v2 Announce Type: replace 
Abstract: The rapid growth of multi-core systems highlights the need for efficient Network-on-Chip (NoC) design to ensure seamless communication. Cache coherence, essential for data consistency, substantially reduces task computation time by enabling data sharing among caches. As a result, routing serves two roles: facilitating data sharing (influenced by topology) and managing NoC-level communication. However, cache coherence is often overlooked in routing, causing mismatches between design expectations and evaluation outcomes. Two main challenges are the lack of specialized tools to assess cache coherence's impact and the neglect of topology selection in routing. In this work, we propose a cache coherence-aware routing approach with integrated topology selection, guided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up to 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total energy savings, underscoring the critical role of cache coherence in NoC design and enabling effective co-design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04005v2</guid>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3716368.3735166</arxiv:DOI>
      <arxiv:journal_reference>Great Lakes Symposium on VLSI 2025 (GLSVLSI '25)</arxiv:journal_reference>
      <dc:creator>Guochu Xiong, Xiangzhong Luo, Weichen Liu</dc:creator>
    </item>
    <item>
      <title>Scalable Connectivity for Ising Machines: Dense to Sparse</title>
      <link>https://arxiv.org/abs/2503.01177</link>
      <description>arXiv:2503.01177v3 Announce Type: replace-cross 
Abstract: In recent years, hardware implementations of Ising machines have emerged as a viable alternative to quantum computing for solving hard optimization problems among other applications. Unlike quantum hardware, dense connectivity can be achieved in classical systems. However, we show that dense connectivity leads to severe frequency slowdowns and interconnect congestion scaling unfavorably with system sizes. As a scalable solution, we propose a systematic sparsification method for dense graphs by introducing copy nodes to limit the number of neighbors per graph node. In addition to solving interconnect congestion, this approach enables constant frequency scaling where all spins in a network can be updated in constant time. On the other hand, sparsification introduces new difficulties, such as constraint-breaking between copied spins and increased convergence times to solve optimization problems, especially if exact ground states are sought. Relaxing the exact solution requirements, we find that the overheads in convergence times are milder. We demonstrate these ideas by designing probabilistic bit Ising machines using ASAP7 (a predictive 7nm FinFET technology model) process design kits as well as Field Programmable Gate Array (FPGA)-based implementations. Finally, we show how formulating problems in naturally sparse networks (e.g., by invertible logic) sidesteps challenges introduced by sparsification methods. Our results are applicable to a broad family of Ising machines using different hardware implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01177v3</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1103/kx8m-5h3h</arxiv:DOI>
      <arxiv:journal_reference>Physical Review Applied (2025)</arxiv:journal_reference>
      <dc:creator>M Mahmudul Hasan Sajeeb, Navid Anjum Aadit, Shuvro Chowdhury, Tong Wu, Cesely Smith, Dhruv Chinmay, Atharva Raut, Kerem Y. Camsari, Corentin Delacour, Tathagata Srimani</dc:creator>
    </item>
  </channel>
</rss>
