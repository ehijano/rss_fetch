<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Dec 2024 03:50:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Dual approach to proving electoral fraud via statistics and forensics (Dvojnoe dokazatel'stvo falsifikazij na vyborah statistikoj i kriminalistikoj)</title>
      <link>https://arxiv.org/abs/2412.04535</link>
      <description>arXiv:2412.04535v1 Announce Type: new 
Abstract: Electoral fraud often manifests itself as statistical anomalies in election results, yet its extent can rarely be reliably confirmed by other evidence. Here we report complete results of municipal elections in Vlasikha town near Moscow, where we observe both statistical irregularities in the vote-counting transcripts and forensic evidence of tampering with ballots during their overnight storage. We evaluate two types of statistical signatures in the vote sequence that can prove batches of fraudulent ballots have been injected. We find that pairs of factory-made security bags with identical serial numbers are used in this fraud scheme. At 8 out of our 9 polling stations, the statistical and forensic evidence agrees (identifying 7 as fraudulent and 1 as honest), while at the remaining station the statistical evidence detects the fraud while the forensic one is insufficient. We also illustrate that the use of tamper-indicating seals at elections is inherently unreliable. -- --
  Tezis po-russki est' v russkoj versii stat'i (normal'noj kirillicej, ne translitom)</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04535v1</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrey Podlazov, Vadim Makarov</dc:creator>
    </item>
    <item>
      <title>Low-Rank Expectile Representations of a Data Matrix, with Application to Diurnal Heart Rates</title>
      <link>https://arxiv.org/abs/2412.04765</link>
      <description>arXiv:2412.04765v1 Announce Type: new 
Abstract: Low-rank matrix factorization is a powerful tool for understanding the structure of 2-way data, and is usually accomplished by minimizing a sum of squares criterion. Expectile analysis generalizes squared-error loss by introducing asymmetry, allowing tail behavior to be elicited. Here we present a framework for low-rank expectile analysis of a data matrix that incorporates both additive and multiplicative effects, utilizing expectile loss, and accommodating arbitrary patterns of missing data. The representation can be fit with gradient-descent. Simulation studies demonstrate the accuracy of the structure recovery. Using diurnal heart rate data indexed by person-days versus minutes within a day, we find divergent behavior for lower versus upper expectiles, with the lower expectiles being much more stable within subjects across days, while the upper expectiles are much more variable, even within subjects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04765v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuge Ouyang, Yunxuan Tang, Benjamin Osafo Agyare</dc:creator>
    </item>
    <item>
      <title>Estimating causal effects of customer satisfaction on downstream metrics in a multi-queue contact center</title>
      <link>https://arxiv.org/abs/2412.04860</link>
      <description>arXiv:2412.04860v1 Announce Type: new 
Abstract: Contact centers are crucial in shaping customer experience, especially in industries like airlines where they significantly influence brand perception and satisfaction. Despite their importance, the effect of contact center improvements on business metrics remains uncertain, complicating investment decisions and often leading to insufficient resource allocation. This paper employs an instrumental-variable approach to estimate the causal effect of customer service interactions at the contact center of LATAM airlines on downstream metrics. Leveraging observational data and the examiner design, we identify causal effects through the quasi-random assignment of agents to calls, accounting for the multi-queue structure and agent certification heterogeneity. Our empirical results highlight the necessity of an instrumental variable approach to accurately estimate causal effects in contact centers, revealing substantial biases from spurious correlations. This methodology provides managers with tools to estimate the impact of call satisfaction on key business metrics, offering valuable insights to solve operational trade-offs of call centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04860v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sebasti\'an Orellana, Leandro Magga, Paolo Gorgi, Hyeokmoon Kweon, Felipe Bahamonde</dc:creator>
    </item>
    <item>
      <title>Fairness-aware Principal Component Analysis for Mortality Forecasting and Annuity Pricing</title>
      <link>https://arxiv.org/abs/2412.04663</link>
      <description>arXiv:2412.04663v1 Announce Type: cross 
Abstract: Fairness-aware statistical learning is critical for data-driven decision-making to mitigate discrimination against protected attributes, such as gender, race, and ethnicity. This is especially important for high-stake decision-making, such as insurance underwriting and annuity pricing. This paper proposes a new fairness-regularized principal component analysis - Fair PCA, in the context of high-dimensional factor models. An efficient gradient descent algorithm is constructed with adaptive selection criteria for hyperparameter tuning. The Fair PCA is applied to mortality modelling to mitigate gender discrimination in annuity pricing. The model performance has been validated through both simulation studies and empirical data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04663v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fei Huang, Junhao Shen, Yanrong Yang, Ran Zhao</dc:creator>
    </item>
    <item>
      <title>A dynamical measure of algorithmically infused visibility</title>
      <link>https://arxiv.org/abs/2412.04735</link>
      <description>arXiv:2412.04735v1 Announce Type: cross 
Abstract: This work focuses on the nature of visibility in societies where the behaviours of humans and algorithms influence each other - termed algorithmically infused societies. We propose a quantitative measure of visibility, with implications and applications to an array of disciplines including communication studies, political science, marketing, technology design, and social media analytics. The measure captures the basic characteristics of the visibility of a given topic, in algorithm/AI-mediated communication/social media settings. Topics, when trending, are ranked against each other, and the proposed measure combines the following two attributes of a topic: (i) the amount of time a topic spends at different ranks, and (ii) the different ranks the topic attains. The proposed measure incorporates a tunable parameter, termed the discrimination level, whose value determines the relative weights of the two attributes that contribute to visibility. Analysis of a large-scale, real-time dataset of trending topics, from one of the largest social media platforms, demonstrates that the proposed measure can explain a large share of the variability of the accumulated views of a topic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04735v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaojing Sun, Zhiyuan Liu, David Waxman</dc:creator>
    </item>
    <item>
      <title>Marginally interpretable spatial logistic regression with bridge processes</title>
      <link>https://arxiv.org/abs/2412.04744</link>
      <description>arXiv:2412.04744v1 Announce Type: cross 
Abstract: In including random effects to account for dependent observations, the odds ratio interpretation of logistic regression coefficients is changed from population-averaged to subject-specific. This is unappealing in many applications, motivating a rich literature on methods that maintain the marginal logistic regression structure without random effects, such as generalized estimating equations. However, for spatial data, random effect approaches are appealing in providing a full probabilistic characterization of the data that can be used for prediction. We propose a new class of spatial logistic regression models that maintain both population-averaged and subject-specific interpretations through a novel class of bridge processes for spatial random effects. These processes are shown to have appealing computational and theoretical properties, including a scale mixture of normal representation. The new methodology is illustrated with simulations and an analysis of childhood malaria prevalence data in the Gambia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04744v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changwoo J. Lee, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>The expected value of sample information calculations for external validation of risk prediction models</title>
      <link>https://arxiv.org/abs/2401.01849</link>
      <description>arXiv:2401.01849v3 Announce Type: replace 
Abstract: In designing external validation studies of clinical prediction models, contemporary sample size calculation methods are based on the frequentist inferential paradigm. One of the widely reported metrics of model performance is net benefit (NB), and the relevance of conventional inference around NB as a measure of clinical utility is doubtful. Value of Information methodology quantifies the consequences of uncertainty in terms of its impact on clinical utility of decisions. We introduce the expected value of sample information (EVSI) for validation as the expected gain in NB from conducting an external validation study of a given size. We propose algorithms for EVSI computation, and in a case study demonstrate how EVSI changes as a function of the amount of current information and future study's sample size. Value of Information methodology provides a decision-theoretic lens to the process of planning a validation study of a risk prediction model and can complement conventional methods when designing such studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01849v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Sadatsafavi, Andrew J Vickers, Tae Yoon Lee, Paul Gustafson, Laure Wynants</dc:creator>
    </item>
    <item>
      <title>Flexible Covariate Adjustments in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2107.07942</link>
      <description>arXiv:2107.07942v4 Announce Type: replace-cross 
Abstract: Empirical regression discontinuity (RD) studies often use covariates to increase the precision of their estimates. In this paper, we propose a novel class of estimators that use such covariate information more efficiently than existing methods and can accommodate many covariates. It involves running a standard RD analysis in which a function of the covariates has been subtracted from the original outcome variable. We characterize the function that leads to the estimator with the smallest asymptotic variance, and consider feasible versions of such estimators in which this function is estimated, for example, through modern machine learning techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.07942v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudia Noack, Tomasz Olma, Christoph Rothe</dc:creator>
    </item>
    <item>
      <title>Estimation of Over-parameterized Models from an Auto-Modeling Perspective</title>
      <link>https://arxiv.org/abs/2206.01824</link>
      <description>arXiv:2206.01824v5 Announce Type: replace-cross 
Abstract: From a model-building perspective, we propose a paradigm shift for fitting over-parameterized models. Philosophically, the mindset is to fit models to future observations rather than to the observed sample. Technically, given an imputation method to generate future observations, we fit over-parameterized models to these future observations by optimizing an approximation of the desired expected loss function based on its sample counterpart and an adaptive $\textit{duality function}$. The required imputation method is also developed using the same estimation technique with an adaptive $m$-out-of-$n$ bootstrap approach. We illustrate its applications with the many-normal-means problem, $n &lt; p$ linear regression, and neural network-based image classification of MNIST digits. The numerical results demonstrate its superior performance across these diverse applications. While primarily expository, the paper conducts an in-depth investigation into the theoretical aspects of the topic. It concludes with remarks on some open problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01824v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Jiang, Chuanhai Liu</dc:creator>
    </item>
    <item>
      <title>Visual Error Patterns in Multi-Modal AI: A Statistical Approach</title>
      <link>https://arxiv.org/abs/2412.00083</link>
      <description>arXiv:2412.00083v3 Announce Type: replace-cross 
Abstract: Multi-modal large language models (MLLMs), such as GPT-4o, excel at integrating text and visual data but face systematic challenges when interpreting ambiguous or incomplete visual stimuli. This study leverages statistical modeling to analyze the factors driving these errors, using a dataset of geometric stimuli characterized by features like 3D, rotation, and missing face/side. We applied parametric methods, non-parametric methods, and ensemble techniques to predict classification errors, with the non-linear gradient boosting model achieving the highest performance (AUC=0.85) during cross-validation. Feature importance analysis highlighted difficulties in depth perception and reconstructing incomplete structures as key contributors to misclassification. These findings demonstrate the effectiveness of statistical approaches for uncovering limitations in MLLMs and offer actionable insights for enhancing model architectures by integrating contextual reasoning mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00083v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ching-Yi Wang</dc:creator>
    </item>
  </channel>
</rss>
