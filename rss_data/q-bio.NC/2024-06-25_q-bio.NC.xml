<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Jun 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Evaluating the Influence of Temporal Context on Automatic Mouse Sleep Staging through the Application of Human Models</title>
      <link>https://arxiv.org/abs/2406.16911</link>
      <description>arXiv:2406.16911v1 Announce Type: new 
Abstract: In human sleep staging models, augmenting the temporal context of the input to the range of tens of minutes has recently demonstrated performance improvement. In contrast, the temporal context of mouse sleep staging models is typically in the order of tens of seconds. While long-term time patterns are less clear in mouse sleep, increasing the temporal context further than that of the current mouse sleep staging models might still result in a performance increase, given that the current methods only model very short term patterns. In this study, we examine the influence of increasing the temporal context in mouse sleep staging up to 15 minutes in three mouse cohorts using two recent and high-performing human sleep staging models that account for long-term dependencies. These are compared to two prominent mouse sleep staging models that use a local context of 12 s and 20 s, respectively. An increase in context up to 28 s is observed to have a positive impact on sleep stage classification performance, especially in REM sleep. However, the impact is limited for longer context windows. One of the human sleep scoring models, L-SeqSleepNet, outperforms both mouse models in all cohorts. This suggests that mouse sleep staging can benefit from more temporal context than currently used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16911v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javier Garc\'ia Ciudad, Morten M{\o}rup, Birgitte Rahbek Kornum, Alexander Neergaard Zahid</dc:creator>
    </item>
    <item>
      <title>Why Quantum-like Models of Cognition Work</title>
      <link>https://arxiv.org/abs/2406.16991</link>
      <description>arXiv:2406.16991v1 Announce Type: new 
Abstract: It is shown that Brownian motions executed by state points of neural membranes generate a Schr\"{o}dinger-like equation with $\hbar/m$ replaced by the coefficient of diffusion $\sigma$ of the substrates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16991v1</guid>
      <category>q-bio.NC</category>
      <category>quant-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Partha Ghose</dc:creator>
    </item>
    <item>
      <title>Comparing fingers and gestures for bci control using an optimized classical machine learning decoder</title>
      <link>https://arxiv.org/abs/2406.17391</link>
      <description>arXiv:2406.17391v1 Announce Type: new 
Abstract: Severe impairment of the central motor network can result in loss of motor function, clinically recognized as Locked-in Syndrome. Advances in Brain-Computer Interfaces offer a promising avenue for partially restoring compromised communicative abilities by decoding different types of hand movements from the sensorimotor cortex. In this study, we collected ECoG recordings from 8 epilepsy patients and compared the decodability of individual finger flexion and hand gestures with the resting state, as a proxy for a one-dimensional brain-click. The results show that all individual finger flexion and hand gestures are equally decodable across multiple models and subjects (&gt;98.0\%). In particular, hand movements, involving index finger flexion, emerged as promising candidates for brain-clicks. When decoding among multiple hand movements, finger flexion appears to outperform hand gestures (96.2\% and 92.5\% respectively) and exhibit greater robustness against misclassification errors when all hand movements are included. These findings highlight that optimized classical machine learning models with feature engineering are viable decoder designs for communication-assistive systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17391v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D. Keller, M. J. Vansteensel, S. Mehrkanoon, M. P. Branco</dc:creator>
    </item>
    <item>
      <title>Mind's Eye: Image Recognition by EEG via Multimodal Similarity-Keeping Contrastive Learning</title>
      <link>https://arxiv.org/abs/2406.16910</link>
      <description>arXiv:2406.16910v1 Announce Type: cross 
Abstract: Decoding images from non-invasive electroencephalographic (EEG) signals has been a grand challenge in understanding how the human brain process visual information in real-world scenarios. To cope with the issues of signal-to-noise ratio and nonstationarity, this paper introduces a MUltimodal Similarity-keeping contrastivE learning (MUSE) framework for zero-shot EEG-based image classification. We develop a series of multivariate time-series encoders tailored for EEG signals and assess the efficacy of regularized contrastive EEG-Image pretraining using an extensive visual EEG dataset. Our method achieves state-of-the-art performance, with a top-1 accuracy of 19.3% and a top-5 accuracy of 48.8% in 200-way zero-shot image classification. Furthermore, we visualize neural patterns via model interpretation, shedding light on the visual processing dynamics in the human brain. The code repository for this work is available at: https://github.com/ChiShengChen/MUSE_EEG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16910v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi-Sheng Chen, Chun-Shu Wei</dc:creator>
    </item>
    <item>
      <title>BrainMAE: A Region-aware Self-supervised Learning Framework for Brain Signals</title>
      <link>https://arxiv.org/abs/2406.17086</link>
      <description>arXiv:2406.17086v1 Announce Type: cross 
Abstract: The human brain is a complex, dynamic network, which is commonly studied using functional magnetic resonance imaging (fMRI) and modeled as network of Regions of interest (ROIs) for understanding various brain functions. Recent studies utilize deep learning approaches to learn the brain network representation based on functional connectivity (FC) profile, broadly falling into two main categories. The Fixed-FC approaches, utilizing the FC profile which represents the linear temporal relation within the brain network, are limited by failing to capture informative brain temporal dynamics. On the other hand, the Dynamic-FC approaches, modeling the evolving FC profile over time, often exhibit less satisfactory performance due to challenges in handling the inherent noisy nature of fMRI data.
  To address these challenges, we propose Brain Masked Auto-Encoder (BrainMAE) for learning representations directly from fMRI time-series data. Our approach incorporates two essential components: a region-aware graph attention mechanism designed to capture the relationships between different brain ROIs, and a novel self-supervised masked autoencoding framework for effective model pre-training. These components enable the model to capture rich temporal dynamics of brain activity while maintaining resilience to inherent noise in fMRI data. Our experiments demonstrate that BrainMAE consistently outperforms established baseline methods by significant margins in four distinct downstream tasks. Finally, leveraging the model's inherent interpretability, our analysis of model-generated representations reveals findings that resonate with ongoing research in the field of neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17086v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Yang, Yutong Mao, Xufu Liu, Xiao Liu</dc:creator>
    </item>
    <item>
      <title>TVB C++: A Fast and Flexible Back-End for The Virtual Brain</title>
      <link>https://arxiv.org/abs/2405.18788</link>
      <description>arXiv:2405.18788v2 Announce Type: replace 
Abstract: This paper introduces TVB C++, a streamlined and fast C++ Back-End for The Virtual Brain (TVB), a renowned platform and a benchmark tool for full-brain simulation. TVB C++ is engineered with speed as a primary focus while retaining the flexibility and ease of use characteristic of the original TVB platform. Positioned as a complementary tool, TVB serves as a prototyping platform, whereas TVB C++ becomes indispensable when performance is paramount, particularly for large-scale simulations and leveraging advanced computation facilities like supercomputers. Developed as a TVB-compatible Back-End, TVB C++ seamlessly integrates with the original TVB implementation, facilitating effortless usage. Users can easily configure TVB C++ to execute the same code as in TVB but with enhanced performance and parallelism capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18788v2</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ignacio Mart\'in, Gorka Zamora, Jan Fousek, Michael Schirner, Petra Ritter, Viktor Jirsa, Gustavo Deco, Gustavo Patow</dc:creator>
    </item>
  </channel>
</rss>
