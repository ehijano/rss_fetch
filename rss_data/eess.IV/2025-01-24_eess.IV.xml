<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jan 2025 05:00:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Linea alba 3D morphometric variability by CT scan exploration</title>
      <link>https://arxiv.org/abs/2501.13116</link>
      <description>arXiv:2501.13116v1 Announce Type: new 
Abstract: Purpose: The width of the Linea alba, which is often gauged by inter-rectus distance, is a key risk factor for incisional hernia and recurrence. Previous studies provided limited descriptions with no consideration for width, location variability, or curvature. We aimed to offer a comprehensive 3D anatomical analysis of the Linea alba, emphasizing its variations across diverse demographics. Methods: Using open source software, 2D sagittal plane and 3D reconstructions were performed on 117 patients' CT scans. Linea alba length, curvature assessed by the sagitta (the longest perpendicular segment between xipho-pubic line and the Linea alba), and continuous width along the height were measured. Results: The Linea alba had a rhombus shape, with a maximum width at the umbilicus of 4.4$\pm$1.9 cm and a larger width above the umbilicus than below. Its length was 37.5$\pm$3.6 cm, which increased with body mass index (BMI) (p$\leq$0.001), and was shorter in women (p$\leq$0.001). The sagitta was 2.6$\pm$2.2 cm, three times higher in the obese group (p$\leq$0.001), majorated with age (p=0.009), but was independent of gender (p=0.212). Linea alba width increased with both age and BMI (p$\leq$0.001, p=0.002), being notably wider in women halfway between the umbilicus and pubis (p=0.007). Conclusion: This study provides an exhaustive 3D description of Linea alba's anatomical variability, presenting new considerations for curvature. This method provides a patient-specific anatomy description of the Linea alba. Further studies are needed to determine whether 3D reconstruction correlates with pathologies, such as hernias and diastasis recti.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13116v1</guid>
      <category>eess.IV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10029-023-02939-0</arxiv:DOI>
      <arxiv:journal_reference>Hernia, 2024, 28 (2), pp.485-494</arxiv:journal_reference>
      <dc:creator>P. Gueroult (LBA), V. Joppin (LBA), K. Chaumoitre (ADES), M. Di Bisceglie (LBA), C. Masson (LBA), T. Bege (LBA)</dc:creator>
    </item>
    <item>
      <title>A Learnt Half-Quadratic Splitting-Based Algorithm for Fast and High-Quality Industrial Cone-beam CT Reconstruction</title>
      <link>https://arxiv.org/abs/2501.13128</link>
      <description>arXiv:2501.13128v1 Announce Type: new 
Abstract: Industrial X-ray cone-beam CT (XCT) scanners are widely used for scientific imaging and non-destructive characterization. Industrial CBCT scanners use large detectors containing millions of pixels and the subsequent 3D reconstructions can be of the order of billions of voxels. In order to obtain high-quality reconstruction when using typical analytic algorithms, the scan involves collecting a large number of projections/views which results in large measurement times - limiting the utility of the technique. Model-based iterative reconstruction (MBIR) algorithms can produce high-quality reconstructions from fast sparse-view CT scans, but are computationally expensive and hence are avoided in practice. Single-step deep-learning (DL) based methods have demonstrated that it is possible to obtain fast and high-quality reconstructions from sparse-view data but they do not generalize well to out-of-distribution scenarios. In this work, we propose a half-quadratic splitting-based algorithm that uses convolutional neural networks (CNN) in order to obtain high-quality reconstructions from large sparse-view cone-beam CT (CBCT) measurements while overcoming the challenges with typical approaches. The algorithm alternates between the application of a CNN and a conjugate gradient (CG) step enforcing data-consistency (DC). The proposed method outperforms other methods on the publicly available Walnuts data-set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13128v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aniket Pramanik, Singanallur V. Venkatakrishnan, Obaidullah Rahman, Amirkoushyar Ziabari</dc:creator>
    </item>
    <item>
      <title>Hybridization of Attention UNet with Repeated Atrous Spatial Pyramid Pooling for Improved Brain Tumour Segmentation</title>
      <link>https://arxiv.org/abs/2501.13129</link>
      <description>arXiv:2501.13129v1 Announce Type: new 
Abstract: Brain tumors are highly heterogeneous in terms of their spatial and scaling characteristics, making tumor segmentation in medical images a difficult task that might result in wrong diagnosis and therapy. Automation of a task like tumor segmentation is expected to enhance objectivity, repeatability and at the same time reducing turn around time. Conventional convolutional neural networks (CNNs) exhibit sub-par performance as a result of their inability to accurately represent the range of tumor sizes and forms. Developing on that, UNets have been a commonly used solution for semantic segmentation, and it uses a downsampling-upsampling approach to segment tumors. This paper proposes a novel architecture that integrates Attention-UNet with repeated Atrous Spatial Pyramid Pooling (ASPP). ASPP effectively captures multi-scale contextual information through parallel atrous convolutions with varying dilation rates. This allows for efficient expansion of the receptive field while maintaining fine details. The attention provides the necessary context by incorporating local characteristics with their corresponding global dependencies. This integration significantly enhances semantic segmentation performance. Our approach demonstrates significant improvements over UNet, Attention UNet and Attention UNet with Spatial Pyramid Pooling allowing to set a new benchmark for tumor segmentation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13129v1</guid>
      <category>eess.IV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satyaki Roy Chowdhury, Golrokh Mirzaei</dc:creator>
    </item>
    <item>
      <title>A Novel Scene Coupling Semantic Mask Network for Remote Sensing Image Segmentation</title>
      <link>https://arxiv.org/abs/2501.13130</link>
      <description>arXiv:2501.13130v1 Announce Type: new 
Abstract: As a common method in the field of computer vision, spatial attention mechanism has been widely used in semantic segmentation of remote sensing images due to its outstanding long-range dependency modeling capability. However, remote sensing images are usually characterized by complex backgrounds and large intra-class variance that would degrade their analysis performance. While vanilla spatial attention mechanisms are based on dense affine operations, they tend to introduce a large amount of background contextual information and lack of consideration for intrinsic spatial correlation. To deal with such limitations, this paper proposes a novel scene-Coupling semantic mask network, which reconstructs the vanilla attention with scene coupling and local global semantic masks strategies. Specifically, scene coupling module decomposes scene information into global representations and object distributions, which are then embedded in the attention affinity processes. This Strategy effectively utilizes the intrinsic spatial correlation between features so that improve the process of attention modeling. Meanwhile, local global semantic masks module indirectly correlate pixels with the global semantic masks by using the local semantic mask as an intermediate sensory element, which reduces the background contextual interference and mitigates the effect of intra-class variance. By combining the above two strategies, we propose the model SCSM, which not only can efficiently segment various geospatial objects in complex scenarios, but also possesses inter-clean and elegant mathematical representations. Experimental results on four benchmark datasets demonstrate the the effectiveness of the above two strategies for improving the attention modeling of remote sensing images. The dataset and code are available at https://github.com/xwmaxwma/rssegmentation</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13130v1</guid>
      <category>eess.IV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaowen Ma, Rongrong Lian, Zhenkai Wu, Renxiang Guan, Tingfeng Hong, Mengjiao Zhao, Mengting Ma, Jiangtao Nie, Zhenhong Du, Siyang Song, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Need for Speed: A Comprehensive Benchmark of JPEG Decoders in Python</title>
      <link>https://arxiv.org/abs/2501.13131</link>
      <description>arXiv:2501.13131v1 Announce Type: new 
Abstract: Image loading represents a critical bottleneck in modern machine learning pipelines, particularly in computer vision tasks where JPEG remains the dominant format. This study presents a systematic performance analysis of nine popular Python JPEG decoding libraries on different computing architectures. We benchmark traditional image processing libraries (Pillow, OpenCV), machine learning frameworks (TensorFlow, PyTorch), and specialized decoders (jpeg4py, kornia-rs) on both ARM64 (Apple M4 Max) and x86\_64 (AMD Threadripper) platforms. Our findings reveal that modern implementations using libjpeg-turbo achieve up to 1.5x faster decoding speeds compared to traditional approaches. We provide evidence-based recommendations for choosing optimal JPEG decoders across different scenarios, from high-throughput training pipelines to real-time applications. This comprehensive analysis helps practitioners make informed decisions about image loading infrastructure, potentially reducing training times and improving system efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13131v1</guid>
      <category>eess.IV</category>
      <category>cs.PF</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladimir Iglovikov</dc:creator>
    </item>
    <item>
      <title>UniRestore: Unified Perceptual and Task-Oriented Image Restoration Model Using Diffusion Prior</title>
      <link>https://arxiv.org/abs/2501.13134</link>
      <description>arXiv:2501.13134v1 Announce Type: new 
Abstract: Image restoration aims to recover content from inputs degraded by various factors, such as adverse weather, blur, and noise. Perceptual Image Restoration (PIR) methods improve visual quality but often do not support downstream tasks effectively. On the other hand, Task-oriented Image Restoration (TIR) methods focus on enhancing image utility for high-level vision tasks, sometimes compromising visual quality. This paper introduces UniRestore, a unified image restoration model that bridges the gap between PIR and TIR by using a diffusion prior. The diffusion prior is designed to generate images that align with human visual quality preferences, but these images are often unsuitable for TIR scenarios. To solve this limitation, UniRestore utilizes encoder features from an autoencoder to adapt the diffusion prior to specific tasks. We propose a Complementary Feature Restoration Module (CFRM) to reconstruct degraded encoder features and a Task Feature Adapter (TFA) module to facilitate adaptive feature fusion in the decoder. This design allows UniRestore to optimize images for both human perception and downstream task requirements, addressing discrepancies between visual quality and functional needs. Integrating these modules also enhances UniRestore's adapability and efficiency across diverse tasks. Extensive expertments demonstrate the superior performance of UniRestore in both PIR and TIR scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13134v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>I-Hsiang Chen, Wei-Ting Chen, Yu-Wei Liu, Yuan-Chun Chiang, Sy-Yen Kuo, Ming-Hsuan Yang</dc:creator>
    </item>
    <item>
      <title>Revisiting Data Augmentation for Ultrasound Images</title>
      <link>https://arxiv.org/abs/2501.13193</link>
      <description>arXiv:2501.13193v1 Announce Type: new 
Abstract: Data augmentation is a widely used and effective technique to improve the generalization performance of deep neural networks. Yet, despite often facing limited data availability when working with medical images, it is frequently underutilized. This appears to come from a gap in our collective understanding of the efficacy of different augmentation techniques across different tasks and modalities. One modality where this is especially true is ultrasound imaging. This work addresses this gap by analyzing the effectiveness of different augmentation techniques at improving model performance across a wide range of ultrasound image analysis tasks. To achieve this, we introduce a new standardized benchmark of 14 ultrasound image classification and semantic segmentation tasks from 10 different sources and covering 11 body regions. Our results demonstrate that many of the augmentations commonly used for tasks on natural images are also effective on ultrasound images, even more so than augmentations developed specifically for ultrasound images in some cases. We also show that diverse augmentation using TrivialAugment, which is widely used for natural images, is also effective for ultrasound images. Moreover, our proposed methodology represents a structured approach for assessing various data augmentations that can be applied to other contexts and modalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13193v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Tupper, Christian Gagn\'e</dc:creator>
    </item>
    <item>
      <title>Scalable dataset acquisition for data-driven lensless imaging</title>
      <link>https://arxiv.org/abs/2501.13334</link>
      <description>arXiv:2501.13334v1 Announce Type: new 
Abstract: Data-driven developments in lensless imaging, such as machine learning-based reconstruction algorithms, require large datasets. In this work, we introduce a data acquisition pipeline that can capture from multiple lensless imaging systems in parallel, under the same imaging conditions, and paired with computational ground truth registration. We provide an open-access 25,000 image dataset with two lensless imagers, a reproducible hardware setup, and open-source camera synchronization code. Experimental datasets from our system can enable data-driven developments in lensless imaging, such as machine learning-based reconstruction algorithms and end-to-end system design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13334v1</guid>
      <category>eess.IV</category>
      <category>physics.optics</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clara S. Hung, Leyla A. Kabuli, Vasilisa Ponomarenko, Laura Waller</dc:creator>
    </item>
    <item>
      <title>Polyhedra Encoding Transformers: Enhancing Diffusion MRI Analysis Beyond Voxel and Volumetric Embedding</title>
      <link>https://arxiv.org/abs/2501.13352</link>
      <description>arXiv:2501.13352v1 Announce Type: new 
Abstract: Diffusion-weighted Magnetic Resonance Imaging (dMRI) is an essential tool in neuroimaging. It is arguably the sole noninvasive technique for examining the microstructural properties and structural connectivity of the brain. Recent years have seen the emergence of machine learning and data-driven approaches that enhance the speed, accuracy, and consistency of dMRI data analysis. However, traditional deep learning models often fell short, as they typically utilize pixel-level or volumetric patch-level embeddings similar to those used in structural MRI, and do not account for the unique distribution of various gradient encodings. In this paper, we propose a novel method called Polyhedra Encoding Transformer (PE-Transformer) for dMRI, designed specifically to handle spherical signals. Our approach involves projecting an icosahedral polygon onto a unit sphere to resample signals from predetermined directions. These resampled signals are then transformed into embeddings, which are processed by a transformer encoder that incorporates orientational information reflective of the icosahedral structure. Through experimental validation with various gradient encoding protocols, our method demonstrates superior accuracy in estimating multi-compartment models and Fiber Orientation Distributions (FOD), outperforming both conventional CNN architectures and standard transformers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13352v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyuan Yao, Zhiyuan Li, Praitayini Kanakaraj, Derek B. Archer, Kurt Schilling, Lori Beason-Held, Susan Resnick, Bennett A. Landman, Yuankai Huo</dc:creator>
    </item>
    <item>
      <title>Unraveling Normal Anatomy via Fluid-Driven Anomaly Randomization</title>
      <link>https://arxiv.org/abs/2501.13370</link>
      <description>arXiv:2501.13370v1 Announce Type: new 
Abstract: Data-driven machine learning has made significant strides in medical image analysis. However, most existing methods are tailored to specific modalities and assume a particular resolution (often isotropic). This limits their generalizability in clinical settings, where variations in scan appearance arise from differences in sequence parameters, resolution, and orientation. Furthermore, most general-purpose models are designed for healthy subjects and suffer from performance degradation when pathology is present. We introduce UNA (Unraveling Normal Anatomy), the first modality-agnostic learning approach for normal brain anatomy reconstruction that can handle both healthy scans and cases with pathology. We propose a fluid-driven anomaly randomization method that generates an unlimited number of realistic pathology profiles on-the-fly. UNA is trained on a combination of synthetic and real data, and can be applied directly to real images with potential pathology without the need for fine-tuning. We demonstrate UNA's effectiveness in reconstructing healthy brain anatomy and showcase its direct application to anomaly detection, using both simulated and real images from 3D healthy and stroke datasets, including CT and MRI scans. By bridging the gap between healthy and diseased images, UNA enables the use of general-purpose models on diseased images, opening up new opportunities for large-scale analysis of uncurated clinical images in the presence of pathology. Code is available at https://github.com/peirong26/UNA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13370v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peirong Liu, Ana Lawry Aguila, Juan E. Iglesias</dc:creator>
    </item>
    <item>
      <title>Scalable Evaluation Framework for Foundation Models in Musculoskeletal MRI Bridging Computational Innovation with Clinical Utility</title>
      <link>https://arxiv.org/abs/2501.13376</link>
      <description>arXiv:2501.13376v1 Announce Type: new 
Abstract: Foundation models hold transformative potential for medical imaging, but their clinical utility requires rigorous evaluation to address their strengths and limitations. This study introduces an evaluation framework for assessing the clinical impact and translatability of SAM, MedSAM, and SAM2, using musculoskeletal MRI as a case study. We tested these models across zero-shot and finetuned paradigms to assess their ability to process diverse anatomical structures and effectuate clinically reliable biomarkers, including cartilage thickness, muscle volume, and disc height. We engineered a modular pipeline emphasizing scalability, clinical relevance, and workflow integration, reducing manual effort and aligning validation with end-user expectations. Hierarchical modeling revealed how dataset mixing, anatomical complexity, and MRI acquisition parameters influence performance, providing insights into the role of imaging refinements in improving segmentation accuracy. This work demonstrates how clinically focused evaluations can connect computational advancements with tangible applications, creating a pathway for foundation models to address medical challenges. By emphasizing interdisciplinary collaboration and aligning technical innovation with clinical priorities, our framework provides a roadmap for advancing machine learning technologies into scalable and impactful biomedical solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13376v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabrielle Hoyer, Michelle W Tong, Rupsa Bhattacharjee, Valentina Pedoia, Sharmila Majumdar</dc:creator>
    </item>
    <item>
      <title>Self-Supervised Diffusion MRI Denoising via Iterative and Stable Refinement</title>
      <link>https://arxiv.org/abs/2501.13514</link>
      <description>arXiv:2501.13514v1 Announce Type: new 
Abstract: Magnetic Resonance Imaging (MRI), including diffusion MRI (dMRI), serves as a ``microscope'' for anatomical structures and routinely mitigates the influence of low signal-to-noise ratio scans by compromising temporal or spatial resolution. However, these compromises fail to meet clinical demands for both efficiency and precision. Consequently, denoising is a vital preprocessing step, particularly for dMRI, where clean data is unavailable. In this paper, we introduce Di-Fusion, a fully self-supervised denoising method that leverages the latter diffusion steps and an adaptive sampling process. Unlike previous approaches, our single-stage framework achieves efficient and stable training without extra noise model training and offers adaptive and controllable results in the sampling process. Our thorough experiments on real and simulated data demonstrate that Di-Fusion achieves state-of-the-art performance in microstructure modeling, tractography tracking, and other downstream tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13514v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>ICLR 2025</arxiv:journal_reference>
      <dc:creator>Chenxu Wu, Qingpeng Kong, Zihang Jiang, S. Kevin Zhou</dc:creator>
    </item>
    <item>
      <title>Survey of image processing settings used for mammography systems in the United Kingdom: how variable is it?</title>
      <link>https://arxiv.org/abs/2501.13595</link>
      <description>arXiv:2501.13595v1 Announce Type: new 
Abstract: The aim was to undertake a national survey of the setup of mammography imaging systems in the UK, we were particularly interested in image processing and software version. We created a program that can extract selected tags from the DICOM header. 28 medical physics departments used the program on processed images of the TORMAM phantom acquired since 2023 and this produced data for 497 systems. We received data for 7 different models of mammography systems. We found that currently in use each model had between 2 and 7 different versions of software for the acquisition workstation. Each of the systems had multiple versions of image processing settings, a preliminary investigation with TORMAM demonstrated large differences in the appearance of the image for the same X-ray model. The Fujifilm, GE and Siemens systems showed differences in the setup of the dose levels. In addition to these settings there were differences in the paddles used and grid type. Our snapshot of system set up showed that there is a potential for the images to appear differently according to the settings seen in the headers. These differences may affect the outcomes of AI and also human readers. Thus the introduction of AI must take these differences into consideration and the inevitably changes of settings in the future. There are responsibilities on AI suppliers, physics, mammographic equipment manufacturers, and breast-screening units to manage the use of AI and ensure the outcomes of breast screening are not adversely affected by the set-up of equipment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13595v1</guid>
      <category>eess.IV</category>
      <category>cs.CY</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1117/12.3026876</arxiv:DOI>
      <arxiv:journal_reference>SPIE IWBI 2024</arxiv:journal_reference>
      <dc:creator>Alistair Mackenzie, John Loveland, Ruben van Engen</dc:creator>
    </item>
    <item>
      <title>Enhancing Medical Image Analysis through Geometric and Photometric transformations</title>
      <link>https://arxiv.org/abs/2501.13643</link>
      <description>arXiv:2501.13643v1 Announce Type: new 
Abstract: Medical image analysis suffers from a lack of labeled data due to several challenges including patient privacy and lack of experts. Although some AI models only perform well with large amounts of data, we will move to data augmentation where there is a solution to improve the performance of our models and increase the dataset size through traditional or advanced techniques. In this paper, we evaluate the effectiveness of data augmentation techniques on two different medical image datasets. In the first step, we applied some transformation techniques to the skin cancer dataset containing benign and malignant classes. Then, we trained the convolutional neural network (CNN) on the dataset before and after augmentation, which significantly improved test accuracy from 90.74% to 96.88% and decreased test loss from 0.7921 to 0.1468 after augmentation. In the second step, we used the Mixup technique by mixing two random images and their corresponding masks using the retina and blood vessels dataset, then we trained the U-net model and obtained the Dice coefficient which increased from 0 before augmentation to 0.4163 after augmentation. The result shows the effect of using data augmentation to increase the dataset size on the classification and segmentation performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13643v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khadija Rais, Mohamed Amroune, Mohamed Yassine Haouam</dc:creator>
    </item>
    <item>
      <title>Variational U-Net with Local Alignment for Joint Tumor Extraction and Registration (VALOR-Net) of Breast MRI Data Acquired at Two Different Field Strengths</title>
      <link>https://arxiv.org/abs/2501.13690</link>
      <description>arXiv:2501.13690v1 Announce Type: new 
Abstract: Background: Multiparametric breast MRI data might improve tumor diagnostics, characterization, and treatment planning. Accurate alignment and delineation of images acquired at different field strengths such as 3T and 7T, remain challenging research tasks. Purpose: To address alignment challenges and enable consistent tumor segmentation across different MRI field strengths. Study type: Retrospective. Subjects: Nine female subjects with breast tumors were involved: six histologically proven invasive ductal carcinomas (IDC) and three fibroadenomas. Field strength/sequence: Imaging was performed at 3T and 7T scanners using post-contrast T1-weighted three-dimensional time-resolved angiography with stochastic trajectories (TWIST) sequence. Assessments: The method's performance for joint image registration and tumor segmentation was evaluated using several quantitative metrics, including signal-to-noise ratio (PSNR), structural similarity index (SSIM), normalized cross-correlation (NCC), Dice coefficient, F1 score, and relative sum of squared differences (rel SSD). Statistical tests: The Pearson correlation coefficient was used to test the relationship between the registration and segmentation metrics. Results: When calculated for each subject individually, the PSNR was in a range from 27.5 to 34.5 dB, and the SSIM was from 82.6 to 92.8%. The model achieved an NCC from 96.4 to 99.3% and a Dice coefficient of 62.9 to 95.3%. The F1 score was between 55.4 and 93.2% and the rel SSD was in the range of 2.0 and 7.5%. The segmentation metrics Dice and F1 Score are highly correlated (0.995), while a moderate correlation between NCC and SSIM (0.681) was found for registration. Data conclusion: Initial results demonstrate that the proposed method may be feasible in providing joint tumor segmentation and registration of MRI data acquired at different field strengths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13690v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Shahkar Khan, Haider Ali, Laura Villazan Garcia, Noor Badshah, Siegfried Trattnig, Florian Schwarzhans, Ramona Woitek, Olgica Zaric</dc:creator>
    </item>
    <item>
      <title>On Disentangled Training for Nonlinear Transform in Learned Image Compression</title>
      <link>https://arxiv.org/abs/2501.13751</link>
      <description>arXiv:2501.13751v1 Announce Type: new 
Abstract: Learned image compression (LIC) has demonstrated superior rate-distortion (R-D) performance compared to traditional codecs, but is challenged by training inefficiency that could incur more than two weeks to train a state-of-the-art model from scratch. Existing LIC methods overlook the slow convergence caused by compacting energy in learning nonlinear transforms. In this paper, we first reveal that such energy compaction consists of two components, i.e., feature decorrelation and uneven energy modulation. On such basis, we propose a linear auxiliary transform (AuxT) to disentangle energy compaction in training nonlinear transforms. The proposed AuxT obtains coarse approximation to achieve efficient energy compaction such that distribution fitting with the nonlinear transforms can be simplified to fine details. We then develop wavelet-based linear shortcuts (WLSs) for AuxT that leverages wavelet-based downsampling and orthogonal linear projection for feature decorrelation and subband-aware scaling for uneven energy modulation. AuxT is lightweight and plug-and-play to be integrated into diverse LIC models to address the slow convergence issue. Experimental results demonstrate that the proposed approach can accelerate training of LIC models by 2 times and simultaneously achieves an average 1\% BD-rate reduction. To our best knowledge, this is one of the first successful attempt that can significantly improve the convergence of LIC with comparable or superior rate-distortion performance. Code will be released at \url{https://github.com/qingshi9974/AuxT}</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13751v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Han Li, Shaohui Li, Wenrui Dai, Maida Cao, Nuowen Kan, Chenglin Li, Junni Zou, Hongkai Xiong</dc:creator>
    </item>
    <item>
      <title>Multimodal AI on Wound Images and Clinical Notes for Home Patient Referral</title>
      <link>https://arxiv.org/abs/2501.13247</link>
      <description>arXiv:2501.13247v1 Announce Type: cross 
Abstract: Chronic wounds affect 8.5 million Americans, particularly the elderly and patients with diabetes. These wounds can take up to nine months to heal, making regular care essential to ensure healing and prevent severe outcomes like limb amputations. Many patients receive care at home from visiting nurses with varying levels of wound expertise, leading to inconsistent care. Problematic, non-healing wounds should be referred to wound specialists, but referral decisions in non-clinical settings are often erroneous, delayed, or unnecessary.
  This paper introduces the Deep Multimodal Wound Assessment Tool (DM-WAT), a machine learning framework designed to assist visiting nurses in deciding whether to refer chronic wound patients. DM-WAT analyzes smartphone-captured wound images and clinical notes from Electronic Health Records (EHRs). It uses DeiT-Base-Distilled, a Vision Transformer (ViT), to extract visual features from images and DeBERTa-base to extract text features from clinical notes. DM-WAT combines visual and text features using an intermediate fusion approach. To address challenges posed by a small and imbalanced dataset, it integrates image and text augmentation with transfer learning to achieve high performance. In evaluations, DM-WAT achieved 77% with std 3% accuracy and a 70% with std 2% F1 score, outperforming prior approaches. Score-CAM and Captum interpretation algorithms provide insights into specific parts of image and text inputs that influence recommendations, enhancing interpretability and trust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13247v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reza Saadati Fard, Emmanuel Agu, Palawat Busaranuvong, Deepak Kumar, Shefalika Gautam, Bengisu Tulu, Diane Strong</dc:creator>
    </item>
    <item>
      <title>Gradient-Free Adversarial Purification with Diffusion Models</title>
      <link>https://arxiv.org/abs/2501.13336</link>
      <description>arXiv:2501.13336v1 Announce Type: cross 
Abstract: Adversarial training and adversarial purification are two effective and practical defense methods to enhance a model's robustness against adversarial attacks. However, adversarial training necessitates additional training, while adversarial purification suffers from low time efficiency. More critically, current defenses are designed under the perturbation-based adversarial threat model, which is ineffective against the recently proposed unrestricted adversarial attacks. In this paper, we propose an effective and efficient adversarial defense method that counters both perturbation-based and unrestricted adversarial attacks. Our defense is inspired by the observation that adversarial attacks are typically located near the decision boundary and are sensitive to pixel changes. To address this, we introduce adversarial anti-aliasing to mitigate adversarial modifications. Additionally, we propose adversarial super-resolution, which leverages prior knowledge from clean datasets to benignly recover images. These approaches do not require additional training and are computationally efficient without calculating gradients. Extensive experiments against both perturbation-based and unrestricted adversarial attacks demonstrate that our defense method outperforms state-of-the-art adversarial purification methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13336v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuelong Dai, Dong Wang, Duan Mingxing, Bin Xiao</dc:creator>
    </item>
    <item>
      <title>A light-weight model to generate NDWI from Sentinel-1</title>
      <link>https://arxiv.org/abs/2501.13357</link>
      <description>arXiv:2501.13357v1 Announce Type: cross 
Abstract: The use of Sentinel-2 images to compute Normalized Difference Water Index (NDWI) has many applications, including water body area detection. However, cloud cover poses significant challenges in this regard, which hampers the effectiveness of Sentinel-2 images in this context. In this paper, we present a deep learning model that can generate NDWI given Sentinel-1 images, thereby overcoming this cloud barrier. We show the effectiveness of our model, where it demonstrates a high accuracy of 0.9134 and an AUC of 0.8656 to predict the NDWI. Additionally, we observe promising results with an R2 score of 0.4984 (for regressing the NDWI values) and a Mean IoU of 0.4139 (for the underlying segmentation task). In conclusion, our model offers a first and robust solution for generating NDWI images directly from Sentinel-1 images and subsequent use for various applications even under challenging conditions such as cloud cover and nighttime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13357v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saleh Sakib Ahmed, Saifur Rahman Jony, Md. Toufikuzzaman, Saifullah Sayed, Rashed Uz Zzaman, Sara Nowreen, M. Sohel Rahman</dc:creator>
    </item>
    <item>
      <title>From Images to Point Clouds: An Efficient Solution for Cross-media Blind Quality Assessment without Annotated Training</title>
      <link>https://arxiv.org/abs/2501.13387</link>
      <description>arXiv:2501.13387v1 Announce Type: cross 
Abstract: We present a novel quality assessment method which can predict the perceptual quality of point clouds from new scenes without available annotations by leveraging the rich prior knowledge in images, called the Distribution-Weighted Image-Transferred Point Cloud Quality Assessment (DWIT-PCQA). Recognizing the human visual system (HVS) as the decision-maker in quality assessment regardless of media types, we can emulate the evaluation criteria for human perception via neural networks and further transfer the capability of quality prediction from images to point clouds by leveraging the prior knowledge in the images. Specifically, domain adaptation (DA) can be leveraged to bridge the images and point clouds by aligning feature distributions of the two media in the same feature space. However, the different manifestations of distortions in images and point clouds make feature alignment a difficult task. To reduce the alignment difficulty and consider the different distortion distribution during alignment, we have derived formulas to decompose the optimization objective of the conventional DA into two suboptimization functions with distortion as a transition. Specifically, through network implementation, we propose the distortion-guided biased feature alignment which integrates existing/estimated distortion distribution into the adversarial DA framework, emphasizing common distortion patterns during feature alignment. Besides, we propose the quality-aware feature disentanglement to mitigate the destruction of the mapping from features to quality during alignment with biased distortions. Experimental results demonstrate that our proposed method exhibits reliable performance compared to general blind PCQA methods without needing point cloud annotations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13387v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yipeng Liu, Qi Yang, Yujie Zhang, Yiling Xu, Le Yang, Zhu Li</dc:creator>
    </item>
    <item>
      <title>Diffusion-based Perceptual Neural Video Compression with Temporal Diffusion Information Reuse</title>
      <link>https://arxiv.org/abs/2501.13528</link>
      <description>arXiv:2501.13528v1 Announce Type: cross 
Abstract: Recently, foundational diffusion models have attracted considerable attention in image compression tasks, whereas their application to video compression remains largely unexplored. In this article, we introduce DiffVC, a diffusion-based perceptual neural video compression framework that effectively integrates foundational diffusion model with the video conditional coding paradigm. This framework uses temporal context from previously decoded frame and the reconstructed latent representation of the current frame to guide the diffusion model in generating high-quality results. To accelerate the iterative inference process of diffusion model, we propose the Temporal Diffusion Information Reuse (TDIR) strategy, which significantly enhances inference efficiency with minimal performance loss by reusing the diffusion information from previous frames. Additionally, to address the challenges posed by distortion differences across various bitrates, we propose the Quantization Parameter-based Prompting (QPP) mechanism, which utilizes quantization parameters as prompts fed into the foundational diffusion model to explicitly modulate intermediate features, thereby enabling a robust variable bitrate diffusion-based neural compression framework. Experimental results demonstrate that our proposed solution delivers excellent performance in both perception metrics and visual quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13528v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenzhuo Ma, Zhenzhong Chen</dc:creator>
    </item>
    <item>
      <title>Autonomous Polycrystalline Material Decomposition for Hyperspectral Neutron Tomography</title>
      <link>https://arxiv.org/abs/2302.13921</link>
      <description>arXiv:2302.13921v3 Announce Type: replace 
Abstract: Hyperspectral neutron tomography is an effective method for analyzing crystalline material samples with complex compositions in a non-destructive manner. Since the counts in the hyperspectral neutron radiographs directly depend on the neutron cross-sections, materials may exhibit contrasting neutron responses across wavelengths. Therefore, it is possible to extract the unique signatures associated with each material and use them to separate the crystalline phases simultaneously.
  We introduce an autonomous material decomposition (AMD) algorithm to automatically characterize and localize polycrystalline structures using Bragg edges with contrasting neutron responses from hyperspectral data. The algorithm estimates the linear attenuation coefficient spectra from the measured radiographs and then uses these spectra to perform polycrystalline material decomposition and reconstructs 3D material volumes to localize materials in the spatial domain. Our results demonstrate that the method can accurately estimate both the linear attenuation coefficient spectra and associated reconstructions on both simulated and experimental neutron data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13921v3</guid>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Samin Nur Chowdhury, Diyu Yang, Shimin Tang, Singanallur V. Venkatakrishnan, Hassina Z. Bilheux, Gregery T. Buzzard, Charles A. Bouman</dc:creator>
    </item>
    <item>
      <title>Guided Reconstruction with Conditioned Diffusion Models for Unsupervised Anomaly Detection in Brain MRIs</title>
      <link>https://arxiv.org/abs/2312.04215</link>
      <description>arXiv:2312.04215v2 Announce Type: replace 
Abstract: The application of supervised models to clinical screening tasks is challenging due to the need for annotated data for each considered pathology. Unsupervised Anomaly Detection (UAD) is an alternative approach that aims to identify any anomaly as an outlier from a healthy training distribution. A prevalent strategy for UAD in brain MRI involves using generative models to learn the reconstruction of healthy brain anatomy for a given input image. As these models should fail to reconstruct unhealthy structures, the reconstruction errors indicate anomalies. However, a significant challenge is to balance the accurate reconstruction of healthy anatomy and the undesired replication of abnormal structures. While diffusion models have shown promising results with detailed and accurate reconstructions, they face challenges in preserving intensity characteristics, resulting in false positives. We propose conditioning the denoising process of diffusion models with additional information derived from a latent representation of the input image. We demonstrate that this conditioning allows for accurate and local adaptation to the general input intensity distribution while avoiding the replication of unhealthy structures. We compare the novel approach to different state-of-the-art methods and for different data sets. Our results show substantial improvements in the segmentation performance, with the Dice score improved by 11.9%, 20.0%, and 44.6%, for the BraTS, ATLAS and MSLUB data sets, respectively, while maintaining competitive performance on the WMH data set. Furthermore, our results indicate effective domain adaptation across different MRI acquisitions and simulated contrasts, an important attribute for general anomaly detection methods. The code for our work is available at https://github.com/FinnBehrendt/Conditioned-Diffusion-Models-UAD</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04215v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.compbiomed.2025.109660</arxiv:DOI>
      <arxiv:journal_reference>Computers in Biology and Medicine Volume 186, March 2025, 109660</arxiv:journal_reference>
      <dc:creator>Finn Behrendt, Debayan Bhattacharya, Robin Mieling, Lennart Maack, Julia Kr\"uger, Roland Opfer, Alexander Schlaefer</dc:creator>
    </item>
    <item>
      <title>Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative Priors</title>
      <link>https://arxiv.org/abs/2407.21600</link>
      <description>arXiv:2407.21600v2 Announce Type: replace 
Abstract: Simultaneous multislice (SMS) imaging is a powerful technique for accelerating magnetic resonance imaging (MRI) acquisitions. However, SMS reconstruction remains challenging due to complex signal interactions between and within the excited slices. In this study, we introduce ROGER, a robust SMS MRI reconstruction method based on deep generative priors. Utilizing denoising diffusion probabilistic models (DDPM), ROGER begins with Gaussian noise and gradually recovers individual slices through reverse diffusion iterations while enforcing data consistency from measured k-space data within the readout concatenation framework. The posterior sampling procedure is designed such that the DDPM training can be performed on single-slice images without requiring modifications for SMS tasks. Additionally, our method incorporates a low-frequency enhancement (LFE) module to address the practical issue that SMS-accelerated fast spin echo (FSE) and echo planar imaging (EPI) sequences cannot easily embed fully-sampled autocalibration signals. Extensive experiments on both retrospectively and prospectively accelerated datasets demonstrate that ROGER consistently outperforms existing methods, enhancing both anatomical and functional imaging with strong out-of-distribution generalization. The source code and sample data for ROGER are available at https://github.com/Solor-pikachu/ROGER.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21600v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <category>physics.med-ph</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shoujin Huang, Guanxiong Luo, Yunlin Zhao, Yilong Liu, Yuwan Wang, Kexin Yang, Jingzhe Liu, Hua Guo, Min Wang, Lingyan Zhang, Mengye Lyu</dc:creator>
    </item>
    <item>
      <title>IPMN Risk Assessment under Federated Learning Paradigm</title>
      <link>https://arxiv.org/abs/2411.05697</link>
      <description>arXiv:2411.05697v2 Announce Type: replace 
Abstract: Accurate classification of Intraductal Papillary Mucinous Neoplasms (IPMN) is essential for identifying high-risk cases that require timely intervention. In this study, we develop a federated learning framework for multi-center IPMN classification utilizing a comprehensive pancreas MRI dataset. This dataset includes 652 T1-weighted and 655 T2-weighted MRI images, accompanied by corresponding IPMN risk scores from 7 leading medical institutions, making it the largest and most diverse dataset for IPMN classification to date. We assess the performance of DenseNet-121 in both centralized and federated settings for training on distributed data. Our results demonstrate that the federated learning approach achieves high classification accuracy comparable to centralized learning while ensuring data privacy across institutions. This work marks a significant advancement in collaborative IPMN classification, facilitating secure and high-accuracy model training across multiple centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05697v2</guid>
      <category>eess.IV</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongyi Pan, Ziliang Hong, Gorkem Durak, Elif Keles, Halil Ertugrul Aktas, Yavuz Taktak, Alpay Medetalibeyoglu, Zheyuan Zhang, Yury Velichko, Concetto Spampinato, Ivo Schoots, Marco J. Bruno, Pallavi Tiwari, Candice Bolan, Tamas Gonda, Frank Miller, Rajesh N. Keswani, Michael B. Wallace, Ziyue Xu, Ulas Bagci</dc:creator>
    </item>
    <item>
      <title>Fast Hyperspectral Reconstruction for Neutron Computed Tomography Using Subspace Extraction</title>
      <link>https://arxiv.org/abs/2411.13557</link>
      <description>arXiv:2411.13557v2 Announce Type: replace 
Abstract: Hyperspectral neutron computed tomography enables 3D non-destructive imaging of the spectral characteristics of materials. In traditional hyperspectral reconstruction, the data for each neutron wavelength bin is reconstructed separately. This per-bin reconstruction is extremely time-consuming due to the typically large number of wavelength bins. Furthermore, these reconstructions may suffer from severe artifacts due to the low signal-to-noise ratio in each wavelength bin.
  We present a novel fast hyperspectral reconstruction algorithm for computationally efficient and accurate reconstruction of hyperspectral neutron data. Our algorithm uses a subspace extraction procedure that transforms hyperspectral data into low-dimensional data within an intermediate subspace. This step effectively reduces data dimensionality and spectral noise. High-quality reconstructions are then performed within this low-dimensional subspace. Finally, the algorithm expands the subspace reconstructions into hyperspectral reconstructions. We apply our algorithm to measured neutron data and demonstrate that it reduces computation and improves reconstruction quality compared to the conventional approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13557v2</guid>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Samin Nur Chowdhury, Diyu Yang, Shimin Tang, Singanallur V. Venkatakrishnan, Andrew W. Needham, Hassina Z. Bilheux, Gregery T. Buzzard, Charles A. Bouman</dc:creator>
    </item>
    <item>
      <title>Boosting Diffusion Guidance via Learning Degradation-Aware Models for Blind Super Resolution</title>
      <link>https://arxiv.org/abs/2501.08819</link>
      <description>arXiv:2501.08819v3 Announce Type: replace 
Abstract: Recently, diffusion-based blind super-resolution (SR) methods have shown great ability to generate high-resolution images with abundant high-frequency detail, but the detail is often achieved at the expense of fidelity. Meanwhile, another line of research focusing on rectifying the reverse process of diffusion models (i.e., diffusion guidance), has demonstrated the power to generate high-fidelity results for non-blind SR. However, these methods rely on known degradation kernels, making them difficult to apply to blind SR. To address these issues, we present DADiff in this paper. DADiff incorporates degradation-aware models into the diffusion guidance framework, eliminating the need to know degradation kernels. Additionally, we propose two novel techniques: input perturbation and guidance scalar, to further improve our performance. Extensive experimental results show that our proposed method has superior performance over state-of-the-art methods on blind SR benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08819v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shao-Hao Lu, Ren Wang, Ching-Chun Huang, Wei-Chen Chiu</dc:creator>
    </item>
    <item>
      <title>Learning Hemodynamic Scalar Fields on Coronary Artery Meshes: A Benchmark of Geometric Deep Learning Models</title>
      <link>https://arxiv.org/abs/2501.09046</link>
      <description>arXiv:2501.09046v2 Announce Type: replace 
Abstract: Coronary artery disease, caused by the narrowing of coronary vessels due to atherosclerosis, is the leading cause of death worldwide. The diagnostic gold standard, fractional flow reserve (FFR), measures the trans-stenotic pressure ratio during maximal vasodilation but is invasive and costly. This has driven the development of virtual FFR (vFFR) using computational fluid dynamics (CFD) to simulate coronary flow. Geometric deep learning algorithms have shown promise for learning features on meshes, including cardiovascular research applications. This study empirically analyzes various backends for predicting vFFR fields in coronary arteries as CFD surrogates, comparing six backends for learning hemodynamics on meshes using CFD solutions as ground truth.
  The study has two parts: i) Using 1,500 synthetic left coronary artery bifurcations, models were trained to predict pressure-related fields for vFFR reconstruction, comparing different learning variables. ii) Using 427 patient-specific CFD simulations, experiments were repeated focusing on the best-performing learning variable from the synthetic dataset.
  Most backends performed well on the synthetic dataset, especially when predicting pressure drop over the manifold. Transformer-based backends outperformed others when predicting pressure and vFFR fields and were the only models achieving strong performance on patient-specific data, excelling in both average per-point error and vFFR accuracy in stenotic lesions.
  These results suggest geometric deep learning backends can effectively replace CFD for simple geometries, while transformer-based networks are superior for complex, heterogeneous datasets. Pressure drop was identified as the optimal network output for learning pressure-related fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09046v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Guido Nannini, Julian Suk, Patryk Rygiel, Simone Saitta, Luca Mariani, Riccardo Maragna, Andrea Baggiano, Gianluca Pontone, Jelmer M. Wolterink, Alberto Redaelli</dc:creator>
    </item>
    <item>
      <title>Enhanced Encoder-Decoder Architecture for Accurate Monocular Depth Estimation</title>
      <link>https://arxiv.org/abs/2410.11610</link>
      <description>arXiv:2410.11610v4 Announce Type: replace-cross 
Abstract: Estimating depth from a single 2D image is a challenging task due to the lack of stereo or multi-view data, which are typically required for depth perception. In state-of-the-art architectures, the main challenge is to efficiently capture complex objects and fine-grained details, which are often difficult to predict. This paper introduces a novel deep learning-based approach using an enhanced encoder-decoder architecture, where the Inception-ResNet-v2 model serves as the encoder. This is the first instance of utilizing Inception-ResNet-v2 as an encoder for monocular depth estimation, demonstrating improved performance over previous models. It incorporates multi-scale feature extraction to enhance depth prediction accuracy across various object sizes and distances. We propose a composite loss function comprising depth loss, gradient edge loss, and Structural Similarity Index Measure (SSIM) loss, with fine-tuned weights to optimize the weighted sum, ensuring a balance across different aspects of depth estimation. Experimental results on the KITTI dataset show that our model achieves a significantly faster inference time of 0.019 seconds, outperforming vision transformers in efficiency while maintaining good accuracy. On the NYU Depth V2 dataset, the model establishes state-of-the-art performance, with an Absolute Relative Error (ARE) of 0.064, a Root Mean Square Error (RMSE) of 0.228, and an accuracy of 89.3% for $\delta$ &lt; 1.25. These metrics demonstrate that our model can accurately and efficiently predict depth even in challenging scenarios, providing a practical solution for real-time applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11610v4</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dabbrata Das, Argho Deb Das, Farhan Sadaf</dc:creator>
    </item>
  </channel>
</rss>
