<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Dec 2024 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>XSema: A Novel Framework for Semantic Extraction of Cross-chain Transactions</title>
      <link>https://arxiv.org/abs/2412.18129</link>
      <description>arXiv:2412.18129v1 Announce Type: new 
Abstract: As the number of blockchain platforms continues to grow, the independence of these networks poses challenges for transferring assets and information across chains. Cross-chain bridge technology has emerged to address this issue, establishing communication protocols to facilitate cross-chain interaction of assets and information, thereby enhancing user experience. However, the complexity of cross-chain transactions increases the difficulty of security regulation, rendering traditional single-chain detection methods inadequate for cross-chain scenarios. Therefore, understanding cross-chain transaction semantics is crucial, as it forms the foundation for cross-chain security detection tasks. Although there are existing methods for extracting transaction semantics specifically for single chains, these approaches often overlook the unique characteristics of cross-chain scenarios, limiting their applicability. This paper introduces XSema, a novel cross-chain semantic extraction framework grounded in asset transfer and message-passing, designed specifically for cross-chain contexts. Experimental results demonstrate that XSema effectively distinguishes between cross-chain and non-cross-chain transactions, surpassing existing methods by over 9% for the generality metric and over 10% for the generalization metric. Furthermore, we analyze the underlying asset transfer patterns and message-passing event logs associated with cross-chain transactions. We offer new insights into the coexistence of multiple blockchains and the cross-chain ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18129v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziye Zheng, Jiajing Wu, Dan Lin, Quanzhong Li, Na Ruan</dc:creator>
    </item>
    <item>
      <title>KunServe: Elastic and Efficient Large Language Model Serving with Parameter-centric Memory Management</title>
      <link>https://arxiv.org/abs/2412.18169</link>
      <description>arXiv:2412.18169v1 Announce Type: new 
Abstract: The stateful nature of large language model (LLM) servingcan easily throttle precious GPU memory under load burstor long-generation requests like chain-of-thought reasoning,causing latency spikes due to queuing incoming requests. However, state-of-the-art KVCache centric approaches handleload spikes by dropping, migrating, or swapping KVCache,which faces an essential tradeoff between the performance ofongoing vs. incoming requests and thus still severely violatesSLO.This paper makes a key observation such that model param-eters are independent of the requests and are replicated acrossGPUs, and thus proposes a parameter-centric approach byselectively dropping replicated parameters to leave preciousmemory for requests. However, LLM requires KVCache tobe saved in bound with model parameters and thus droppingparameters can cause either huge computation waste or longnetwork delay, affecting all ongoing requests. Based on the ob-servation that attention operators can be decoupled from otheroperators, this paper further proposes a novel remote attentionmechanism through pipeline parallelism so as to serve up-coming requests with the additional memory borrowed fromparameters on remote GPUs. This paper further addresses sev-eral other challenges including lively exchanging KVCachewith incomplete parameters, generating an appropriate planthat balances memory requirements with cooperative exe-cution overhead, and seamlessly restoring parameters whenthe throttling has gone. Evaluations show thatKUNSERVEreduces the tail TTFT of requests under throttling by up to 27.3x compared to the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18169v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rongxin Cheng, Yifan Peng, Yuxin Lai, Xingda Wei, Rong Chen, Haibo Chen</dc:creator>
    </item>
    <item>
      <title>Hardware-aware Circuit Cutting and Distributed Qubit Mapping for Connected Quantum Systems</title>
      <link>https://arxiv.org/abs/2412.18458</link>
      <description>arXiv:2412.18458v1 Announce Type: new 
Abstract: Quantum computing offers unparalleled computational capabilities but faces significant challenges, including limited qubit counts, diverse hardware topologies, and dynamic noise/error rates, which hinder scalability and reliability. Distributed quantum computing, particularly chip-to-chip connections, has emerged as a solution by interconnecting multiple processors to collaboratively execute large circuits. While hardware advancements, such as IBM's Quantum Flamingo, focus on improving inter-chip fidelity, limited research addresses efficient circuit cutting and qubit mapping in distributed systems. This project introduces DisMap, a self-adaptive, hardware-aware framework for chip-to-chip distributed quantum systems. DisMap analyzes qubit noise and error rates to construct a virtual system topology, guiding circuit partitioning, and distributed qubit mapping to minimize SWAP overhead and enhance fidelity. Implemented with IBM Qiskit and compared with the state-of-the-art, DisMap achieves up to a 20.8\% improvement in fidelity and reduces SWAP overhead by as much as 80.2\%, demonstrating scalability and effectiveness in extensive evaluations on real quantum hardware topologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18458v1</guid>
      <category>cs.DC</category>
      <category>quant-ph</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zefan Du, Yanni Li, Zijian Mo, Wenqi Wei, Juntao Chen, Rajkumar Buyya, Ying Mao</dc:creator>
    </item>
    <item>
      <title>Parallel Contraction Hierarchies Can Be Efficient and Scalable</title>
      <link>https://arxiv.org/abs/2412.18008</link>
      <description>arXiv:2412.18008v1 Announce Type: cross 
Abstract: Contraction Hierarchies (CH) (Geisberger et al., 2008) is one of the most widely used algorithms for shortest-path queries on road networks. Compared to Dijkstra's algorithm, CH enables orders of magnitude faster query performance through a preprocessing phase, which iteratively categorizes vertices into hierarchies and adds shortcuts. However, constructing a CH is an expensive task. Existing solutions, including parallel ones, may suffer from long construction time. Especially in our experiments, we observe that existing parallel solutions demonstrate unsatisfactory scalability and have close performance to sequential algorithms.
  In this paper, we present \textsf{SPoCH} (\textbf{S}calable \textbf{P}arallelization \textbf{o}f \textbf{C}ontraction \textbf{H}ierarchies), an efficient and scalable CH construction algorithm in parallel. To address the challenges in previous work, our improvements focus on both redesigning the algorithm and leveraging parallel data structures. %to maintain the original and shortcut edges dynamically. We implement our algorithm and compare it with the state-of-the-art sequential and parallel implementations on 13 graphs, including road networks, synthetic graphs, and $k$-NN graphs. Our experiments show that \textsf{SPoCH} achieves $17$--$131\times$ speedups in CH construction over the best baseline, while maintaining competitive query performance and CH graph size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18008v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zijin Wan, Xiaojun Dong, Letong Wang, Enzuo Zhu, Yan Gu, Yihan Sun</dc:creator>
    </item>
    <item>
      <title>Tackling the Dynamicity in a Production LLM Serving System with SOTA Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient Meta-kernels</title>
      <link>https://arxiv.org/abs/2412.18106</link>
      <description>arXiv:2412.18106v1 Announce Type: cross 
Abstract: Meeting growing demands for low latency and cost efficiency in production-grade large language model (LLM) serving systems requires integrating advanced optimization techniques. However, dynamic and unpredictable input-output lengths of LLM, compounded by these optimizations, exacerbate the issues of workload variability, making it difficult to maintain high efficiency on AI accelerators, especially DSAs with tile-based programming models. To address this challenge, we introduce XY-Serve, a versatile, Ascend native, end-to-end production LLM-serving system. The core idea is an abstraction mechanism that smooths out the workload variability by decomposing computations into unified, hardware-friendly, fine-grained meta primitives. For attention, we propose a meta-kernel that computes the basic pattern of matmul-softmax-matmul with architectural-aware tile sizes. For GEMM, we introduce a virtual padding scheme that adapts to dynamic shape changes while using highly efficient GEMM primitives with assorted fixed tile sizes. XY-Serve sits harmoniously with vLLM. Experimental results show up to 89% end-to-end throughput improvement compared with current publicly available baselines on Ascend NPUs. Additionally, our approach outperforms existing GEMM (average 14.6% faster) and attention (average 21.5% faster) kernels relative to existing libraries. While the work is Ascend native, we believe the approach can be readily applicable to SIMT architectures as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18106v1</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingcong Song, Xinru Tang, Fengfan Hou, Jing Li, Wei Wei, Yipeng Ma, Runqiu Xiao, Hongjie Si, Dingcheng Jiang, Shouyi Yin, Yang Hu, Guoping Long</dc:creator>
    </item>
    <item>
      <title>Accelerating AIGC Services with Latent Action Diffusion Scheduling in Edge Networks</title>
      <link>https://arxiv.org/abs/2412.18212</link>
      <description>arXiv:2412.18212v1 Announce Type: cross 
Abstract: Artificial Intelligence Generated Content (AIGC) has gained significant popularity for creating diverse content. Current AIGC models primarily focus on content quality within a centralized framework, resulting in a high service delay and negative user experiences. However, not only does the workload of an AIGC task depend on the AIGC model's complexity rather than the amount of data, but the large model and its multi-layer encoder structure also result in a huge demand for computational and memory resources. These unique characteristics pose new challenges in its modeling, deployment, and scheduling at edge networks. Thus, we model an offloading problem among edges for providing real AIGC services and propose LAD-TS, a novel Latent Action Diffusion-based Task Scheduling method that orchestrates multiple edge servers for expedited AIGC services. The LAD-TS generates a near-optimal offloading decision by leveraging the diffusion model's conditional generation capability and the reinforcement learning's environment interaction ability, thereby minimizing the service delays under multiple resource constraints. Meanwhile, a latent action diffusion strategy is designed to guide decision generation by utilizing historical action probability, enabling rapid achievement of near-optimal decisions. Furthermore, we develop DEdgeAI, a prototype edge system with a refined AIGC model deployment to implement and evaluate our LAD-TS method. DEdgeAI provides a real AIGC service for users, demonstrating up to 29.18% shorter service delays than the current five representative AIGC platforms. We release our open-source code at https://github.com/ChangfuXu/DEdgeAI/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18212v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changfu Xu, Jianxiong Guo, Wanyu Lin, Haodong Zou, Wentao Fan, Tian Wang, Xiaowen Chu, Jiannong Cao</dc:creator>
    </item>
    <item>
      <title>Pilot-Quantum: A Quantum-HPC Middleware for Resource, Workload and Task Management</title>
      <link>https://arxiv.org/abs/2412.18519</link>
      <description>arXiv:2412.18519v1 Announce Type: cross 
Abstract: As quantum hardware continues to scale, managing the heterogeneity of resources and applications -- spanning diverse quantum and classical hardware and software frameworks -- becomes increasingly critical. \textit{Pilot-Quantum} addresses these challenges as a middleware designed to provide unified application-level management of resources and workloads across hybrid quantum-classical environments. It is built on a rigorous analysis of existing quantum middleware systems and application execution patterns. It implements the Pilot Abstraction conceptual model, originally developed for HPC, to manage resources, workloads, and tasks. It is designed for quantum applications that rely on task parallelism, including: (i) \textit{Hybrid algorithms}, such as variational approaches, and (ii) \textit{Circuit cutting systems}, used to partition and execute large quantum circuits. \textit{Pilot-Quantum} facilitates seamless integration of quantum processing units (QPUs), classical CPUs, and GPUs, while supporting high-level programming frameworks like Qiskit and Pennylane. This enables users to design and execute hybrid workflows across diverse computing resources efficiently. The capabilities of \textit{Pilot-Quantum} are demonstrated through mini-applications -- simplified yet representative kernels focusing on critical performance bottlenecks. We present several mini-apps, including circuit execution across hardware and simulator platforms (e.\,g., IBM's Eagle QPU), distributed state vector simulation, circuit cutting, and quantum machine learning workflows, demonstrating significant scale (e.\,g., a 41-qubit simulation on 256 GPUs) and speedups (e.\,g., 15x for QML, 3.5x for circuit cutting).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18519v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pradeep Mantha, Florian J. Kiwit, Nishant Saurabh, Shantenu Jha, Andre Luckow</dc:creator>
    </item>
    <item>
      <title>Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining Rates with Ruin Theory</title>
      <link>https://arxiv.org/abs/2412.18599</link>
      <description>arXiv:2412.18599v1 Announce Type: cross 
Abstract: Theoretical guarantees for double spending probabilities for the Nakamoto consensus under the $k$-deep confirmation rule have been extensively studied for zero/bounded network delays and fixed mining rates. In this paper, we introduce a ruin-theoretical model of double spending for Nakamoto consensus under the $k$-deep confirmation rule when the honest mining rate is allowed to be an arbitrary function of time including the block delivery periods, i.e., time periods during which mined blocks are being delivered to all other participants of the network. Time-varying mining rates are considered to capture the intrinsic characteristics of the peer to peer network delays as well as dynamic participation of miners such as the gap game and switching between different cryptocurrencies. Ruin theory is leveraged to obtain the double spend probabilities and numerical examples are presented to validate the effectiveness of the proposed analytical method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18599v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mustafa Doger, Sennur Ulukus, Nail Akar</dc:creator>
    </item>
    <item>
      <title>LoHan: Low-Cost High-Performance Framework to Fine-Tune 100B Model on a Consumer GPU</title>
      <link>https://arxiv.org/abs/2403.06504</link>
      <description>arXiv:2403.06504v2 Announce Type: replace 
Abstract: Nowadays, AI researchers become more and more interested in fine-tuning a pre-trained LLM, whose size has grown to up to over 100B parameters, for their downstream tasks. One approach to fine-tune such huge models is to aggregate device memory from many GPUs. However, this approach introduces prohibitive costs for most data scientists with a limited budget for high-end GPU servers. In this paper, we focus on LLM fine-tuning on a single consumer-grade GPU in a commodity server with limited main memory capacity, which is accessible to most AI researchers. In such a scenario, existing offloading-based methods fail to fine-tune an LLM efficiently due to a lack of holistic intra-server tensor movement management. To this end, we present LoHan, a low-cost, high-performance deep learning training framework that enables efficient 100B-scale model fine-tuning on a commodity server with a consumer-grade GPU and limited main memory capacity. The key idea is to add holistic offloading traffic as an optimization dimension for 1)active gradient offloading, and 2)holistic traffic-aware activation swapping mechanism. The experimental results show that 1)LoHan is the first to fine-tune a 175B model on an RTX 4090 and 256 GB main memory, 2)LoHan achieves 2.32x throughput than the state-of-the-art baselines when fine-tuning a small 13B model, and 3)LoHan enables a cheap low-end consumer GPU to have higher cost-effectiveness than a DGX-A100 cluster when fine-tuning a 175B model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06504v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Changyue Liao, Mo Sun, Zihan Yang, Jun Xie, Kaiqi Chen, Binhang Yuan, Fei Wu, Zeke Wang</dc:creator>
    </item>
    <item>
      <title>Causal Graph Dynamics and Kan Extensions</title>
      <link>https://arxiv.org/abs/2403.13393</link>
      <description>arXiv:2403.13393v2 Announce Type: replace 
Abstract: On the one side, the formalism of Global Transformations comes with the claim of capturing any transformation of space that is local, synchronous and deterministic.The claim has been proven for different classes of models such as mesh refinements from computer graphics, Lindenmayer systems from morphogenesis modeling and cellular automata from biological, physical and parallel computation modeling.The Global Transformation formalism achieves this by using category theory for its genericity, and more precisely the notion of Kan extension to determine the global behaviors based on the local ones.On the other side, Causal Graph Dynamics describe the transformation of port graphs in a synchronous and deterministic way and has not yet being tackled.In this paper, we show the precise sense in which the claim of Global Transformations holds for them as well.This is done by showing different ways in which they can be expressed as Kan extensions, each of them highlighting different features of Causal Graph Dynamics.Along the way, this work uncovers the interesting class of Monotonic Causal Graph Dynamics and their universality among General Causal Graph Dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13393v2</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.MA</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luidnel Maignan (LACL), Antoine Spicher (LACL)</dc:creator>
    </item>
    <item>
      <title>Validated Strong Consensus Protocol for Asynchronous Vote-based Blockchains</title>
      <link>https://arxiv.org/abs/2409.08161</link>
      <description>arXiv:2409.08161v2 Announce Type: replace 
Abstract: Vote-based blockchains construct a state machine replication (SMR) system among participating nodes, using Byzantine Fault Tolerance (BFT) consensus protocols to transition from one state to another. Currently, they rely on either synchronous or partially synchronous networks with leader-based coordination or costly Asynchronous Common Subset (ACS) protocols in asynchronous settings, making them impractical for large-scale asynchronous applications.
  To make Asynchronous SMR scalable, this paper proposes a \emph{validated strong} BFT consensus model that allows leader-based coordination in asynchronous settings. Our BFT consensus model offers the same level of tolerance as binary byzantine agreement but does not demand consistency among honest nodes before they vote. An SMR using our model allows nodes to operate in different, tentative, but mutually exclusive states until they eventually converge on the same state. We propose an asynchronous BFT protocol for vote-based blockchains employing our consensus model to address several critical challenges: how to ensure that nodes eventually converge on the same state across voting rounds, how to assure that a blockchain will steadily progress through epochs while reaching consensus for previous epochs, and how to maintain robust byzantine fault tolerance.
  Our protocol greatly reduces message complexity and is the first one to achieve linear view changes without relying on threshold signatures. We prove that an asynchronous blockchain built on our protocol can operate with the \emph{same} simplicity and efficiency as partially synchronous blockchains built on, e.g. HotStuff-2. This facilitates deploying asynchronous blockchains across large-scale networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08161v2</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yibin Xu, Jianhua Shao, Tijs Slaats, Boris D\"udder, Yongluan Zhou</dc:creator>
    </item>
    <item>
      <title>Tackling Intertwined Data and Device Heterogeneities in Federated Learning with Unlimited Staleness</title>
      <link>https://arxiv.org/abs/2309.13536</link>
      <description>arXiv:2309.13536v4 Announce Type: replace-cross 
Abstract: Federated Learning (FL) can be affected by data and device heterogeneities, caused by clients' different local data distributions and latencies in uploading model updates (i.e., staleness). Traditional schemes consider these heterogeneities as two separate and independent aspects, but this assumption is unrealistic in practical FL scenarios where these heterogeneities are intertwined. In these cases, traditional FL schemes are ineffective, and a better approach is to convert a stale model update into a unstale one. In this paper, we present a new FL framework that ensures the accuracy and computational efficiency of this conversion, hence effectively tackling the intertwined heterogeneities that may cause unlimited staleness in model updates. Our basic idea is to estimate the distributions of clients' local training data from their uploaded stale model updates, and use these estimations to compute unstale client model updates. In this way, our approach does not require any auxiliary dataset nor the clients' local models to be fully trained, and does not incur any additional computation or communication overhead at client devices. We compared our approach with the existing FL strategies on mainstream datasets and models, and showed that our approach can improve the trained model accuracy by up to 25% and reduce the number of required training epochs by up to 35%. Source codes can be found at: https://github.com/pittisl/FL-with-intertwined-heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13536v4</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Haoming Wang, Wei Gao</dc:creator>
    </item>
    <item>
      <title>BlockEmulator: An Emulator Enabling to Test Blockchain Sharding Protocols</title>
      <link>https://arxiv.org/abs/2311.03612</link>
      <description>arXiv:2311.03612v4 Announce Type: replace-cross 
Abstract: Numerous blockchain simulators have been proposed to allow researchers to simulate mainstream blockchains. However, we have not yet found a testbed that enables researchers to develop and evaluate their new consensus algorithms or new protocols for blockchain sharding systems. To fill this gap, we developed BlockEmulator, which is designed as an experimental platform, particularly for emulating blockchain sharding mechanisms. BlockEmulator adopts a lightweight blockchain architecture so developers can only focus on implementing their new protocols or mechanisms. Using layered modules and useful programming interfaces offered by BlockEmulator, researchers can implement a new protocol with minimum effort. Through experiments, we test various functionalities of BlockEmulator in two steps. Firstly, we prove the correctness of the emulation results yielded by BlockEmulator by comparing the theoretical analysis with the observed experiment results. Secondly, other experimental results demonstrate that BlockEmulator can facilitate measuring a series of metrics, including throughput, transaction confirmation latency, cross-shard transaction ratio, the queuing status of transaction pools, workload distribution across blockchain shards, etc. We have made BlockEmulator open-source in Github.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03612v4</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huawei Huang, Guang Ye, Qinglin Yang, Qinde Chen, Zhaokang Yin, Xiaofei Luo, Jianru Lin, Taotao Li, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>Distance-Restricted Explanations: Theoretical Underpinnings &amp; Efficient Implementation</title>
      <link>https://arxiv.org/abs/2405.08297</link>
      <description>arXiv:2405.08297v2 Announce Type: replace-cross 
Abstract: The uses of machine learning (ML) have snowballed in recent years. In many cases, ML models are highly complex, and their operation is beyond the understanding of human decision-makers. Nevertheless, some uses of ML models involve high-stakes and safety-critical applications. Explainable artificial intelligence (XAI) aims to help human decision-makers in understanding the operation of such complex ML models, thus eliciting trust in their operation. Unfortunately, the majority of past XAI work is based on informal approaches, that offer no guarantees of rigor. Unsurprisingly, there exists comprehensive experimental and theoretical evidence confirming that informal methods of XAI can provide human-decision makers with erroneous information. Logic-based XAI represents a rigorous approach to explainability; it is model-based and offers the strongest guarantees of rigor of computed explanations. However, a well-known drawback of logic-based XAI is the complexity of logic reasoning, especially for highly complex ML models. Recent work proposed distance-restricted explanations, i.e. explanations that are rigorous provided the distance to a given input is small enough. Distance-restricted explainability is tightly related with adversarial robustness, and it has been shown to scale for moderately complex ML models, but the number of inputs still represents a key limiting factor. This paper investigates novel algorithms for scaling up the performance of logic-based explainers when computing and enumerating ML model explanations with a large number of inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08297v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yacine Izza, Xuanxiang Huang, Antonio Morgado, Jordi Planes, Alexey Ignatiev, Joao Marques-Silva</dc:creator>
    </item>
  </channel>
</rss>
