<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Jan 2026 02:36:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Optimising Cylindrical Algebraic Coverings for use in SMT by Solving a Set Covering Problem with Reasons</title>
      <link>https://arxiv.org/abs/2601.14424</link>
      <description>arXiv:2601.14424v1 Announce Type: new 
Abstract: The Conflict-Driven Cylindrical Algebraic Covering algorithm has proven well suited for performing theory validation checks in the satisfiability modulo theories paradigm for non-linear real arithmetic. CDCAC repurposes the theory underpinning classical cylindrical algebraic decomposition for SMT solving and is implemented in the SMT solvers cvc5 and SMT-RAT, as well as the computer algebra system Maple. It was previously observed that when using cylindrical algebraic decomposition for an SMT theory call, the output can be optimised by solving a single set covering problem instance that minimises the conflict clause. In this paper we consider the corresponding optimisation for CDCAC and observe that CDCAC naturally gives rise to multiple such optimisations within a single call. Each time a covering is generalised in one dimension, the resulting cell in the next dimension is labelled with theory constraints that cannot be satisfied together. We seek the smallest subset of constraints whose union covers all labels from the cells in the current covering. We call this optimisation problem a set covering problem with reasons. To simplify this problem, we introduce a data reduction step that generalises Beasley reduction for the classical set covering problem and show that this step alone solves many of the instances arising from SMT-LIB benchmarks. We then propose an exact solver based on linear programming to efficiently solve the remaining cases. Integrating these techniques into CDCAC has the potential to significantly improve SMT solver performance for non-linear real arithmetic problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14424v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abiola Babatunde, Matthew England, AmirHosein Sadeghimanesh</dc:creator>
    </item>
    <item>
      <title>Economic Warehouse Lot Scheduling: Approximation Schemes via Efficiently-Representable DP-Encoded Policies</title>
      <link>https://arxiv.org/abs/2601.14993</link>
      <description>arXiv:2601.14993v1 Announce Type: new 
Abstract: In this focused technical paper, we present long-awaited algorithmic advances toward the efficient construction of near-optimal replenishment policies for a true inventory management classic, the economic warehouse lot scheduling problem. While this paradigm has accumulated a massive body of surrounding literature since its inception in the late '50s, we are still very much in the dark as far as basic computational questions are concerned, perhaps due to the intrinsic complexity of dynamic policies in this context. The latter feature forced earlier attempts to either study highly-structured classes of policies or to forgo provably-good performance guarantees altogether; to this day, rigorously analyzable results have been few and far between.
  The current paper develops novel analytical foundations for directly competing against dynamic policies. Combined with further algorithmic progress and newly-gained insights, these ideas culminate in a polynomial-time approximation scheme for constantly-many commodities. In this regard, the efficient design of $\epsilon$-optimal dynamic policies appeared to have been out of reach, since beyond their inherent algorithmic challenges, even the polynomial-space representation of such policies has been a fundamental open question.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14993v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Danny Segev</dc:creator>
    </item>
    <item>
      <title>Economic Warehouse Lot Scheduling: Breaking the 2-Approximation Barrier</title>
      <link>https://arxiv.org/abs/2601.15068</link>
      <description>arXiv:2601.15068v1 Announce Type: new 
Abstract: The economic warehouse lot scheduling problem is a foundational inventory-theory model, capturing computational challenges in dynamically coordinating replenishment decisions for multiple commodities subject to a shared capacity constraint. Even though this model has generated a vast body of literature over the last six decades, our algorithmic understanding has remained surprisingly limited. Indeed, for general problem instances, the best-known approximation guarantees have remained at a factor of $2$ since the mid-1990s. These guarantees were attained by the now-classic work of Anily [Operations Research, 1991] and Gallego, Queyranne, and Simchi-Levi [Operations Research, 1996] via the highly-structured class of "stationary order sizes and stationary intervals" (SOSI) policies, thereby avoiding direct competition against fully dynamic policies.
  The main contribution of this paper resides in developing new analytical foundations and algorithmic techniques that enable such direct comparisons, leading to the first provable improvement over the $2$-approximation barrier. Leveraging these ideas, we design a constructive approach that allows us to balance cost and capacity at a finer granularity than previously possible via SOSI-based methods. Consequently, given any economic warehouse lot scheduling instance, we present a polynomial-time construction of a random capacity-feasible dynamic policy whose expected long-run average cost is within factor $2-\frac{17}{5000} + \epsilon$ of optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15068v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Danny Segev</dc:creator>
    </item>
    <item>
      <title>Product-State Approximation Algorithms for the Transverse Field Ising Model</title>
      <link>https://arxiv.org/abs/2601.13106</link>
      <description>arXiv:2601.13106v1 Announce Type: cross 
Abstract: We study classical polynomial-time approximation algorithms for the transverse-field Ising model (TFIM) Hamiltonian, allowing a mixture of ferromagnetic and anti-ferromagnetic interactions between pairs of qbits, alongside transverse field terms with arbitrary non-negative weights.
  Our main results are a series of approximation algorithms (all approximation ratios with respect to the true quantum optimum): (i) a simple maximum of two product state rounding algorithm achieving an approximation ratio $\gamma\approx 0.71$ , (ii) a strengthened rounding, inspired by the anticommutation property of the two $X_i, Z_iZ_j$ observables achieving ratio $\gamma\approx 0.7860$, and (iii) a further improvement by interpolation achieving ratio $\gamma \approx 0.8156$. We also give an explicit (purely ferromagnetic) TFIM instance on three qbits for which every product state achieves at most $169/180\approx 0.9389$ of the true optimum, yielding an upper bound for all algorithms producing product state approximations, even in the purely ferromagnetic case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13106v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincenzo Lipardi, David Mestel, Georgios Stamoulis</dc:creator>
    </item>
    <item>
      <title>Online Linear Programming with Replenishment</title>
      <link>https://arxiv.org/abs/2601.14629</link>
      <description>arXiv:2601.14629v1 Announce Type: cross 
Abstract: We study an online linear programming (OLP) model in which inventory is not provided upfront but instead arrives gradually through an exogenous stochastic replenishment process. This replenishment-based formulation captures operational settings, such as e-commerce fulfillment, perishable supply chains, and renewable-powered systems, where resources are accumulated gradually and initial inventories are small or zero. The introduction of dispersed, uncertain replenishment fundamentally alters the structure of classical OLPs, creating persistent stockout risk and eliminating advance knowledge of the total budget.
  We develop new algorithms and regret analyses for three major distributional regimes studied in the OLP literature: bounded distributions, finite-support distributions, and continuous-support distributions with a non-degeneracy condition. For bounded distributions, we design an algorithm that achieves $\widetilde{\mathcal{O}}(\sqrt{T})$ regret. For finite-support distributions with a non-degenerate induced LP, we obtain $\mathcal{O}(\log T)$ regret, and we establish an $\Omega(\sqrt{T})$ lower bound for degenerate instances, demonstrating a sharp separation from the classical setting where $\mathcal{O}(1)$ regret is achievable. For continuous-support, non-degenerate distributions, we develop a two-stage accumulate-then-convert algorithm that achieves $\mathcal{O}(\log^2 T)$ regret, comparable to the $\mathcal{O}(\log T)$ regret in classical OLPs. Together, these results provide a near-complete characterization of the optimal regret achievable in OLP with replenishment. Finally, we empirically evaluate our algorithms and demonstrate their advantages over natural adaptations of classical OLP methods in the replenishment setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14629v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuze Chen, Yuan Zhou, Baichuan Mo, Jie Ying, Yufei Ruan, Zhou Ye</dc:creator>
    </item>
    <item>
      <title>LZBE: an LZ-style compressor supporting $O(\log n)$-time random access</title>
      <link>https://arxiv.org/abs/2506.20107</link>
      <description>arXiv:2506.20107v3 Announce Type: replace 
Abstract: An LZ-like factorization of a string divides it into factors, each being either a single character or a copy of a preceding substring. While grammar-based compression schemes support efficient random access with space linear in the compressed size, no comparable guarantees are known for general LZ-like factorizations. This limitation motivated restricted variants such as LZ-End [Kreft and Navarro, 2013] and height-bounded LZ (LZHB) [Bannai et al., 2024], which trade off some compression efficiency for faster access. In this paper, we introduce LZ-Begin-End (LZBE), a new LZ-like variant in which every copy factor must refer to a contiguous sequence of preceding factors. This structural restriction ensures that any context-free grammar can be transformed into an LZBE factorization of the same size. We further study the greedy LZBE factorization, which selects each copy factor to be as long as possible while processing the input from left to right, and show that it can be computed in linear time. Moreover, we exhibit a family of strings for which the greedy LZBE factorization is asymptotically smaller than the smallest grammar. These results demonstrate that the LZBE scheme is strictly more expressive than grammar-based compression in the worst case. To support fast queries, we propose a data structure for LZBE-compressed strings that permits O(log n)-time random access within space linear in the compressed size, where n is the length of the input string.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20107v3</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroki Shibata, Yuto Nakashima, Yutaro Yamaguchi, Shunsuke Inenaga</dc:creator>
    </item>
    <item>
      <title>SAQ: Pushing the Limits of Vector Quantization through Code Adjustment and Dimension Segmentation</title>
      <link>https://arxiv.org/abs/2509.12086</link>
      <description>arXiv:2509.12086v2 Announce Type: replace-cross 
Abstract: Approximate Nearest Neighbor Search (ANNS) plays a critical role in applications such as search engines, recommender systems, and RAG for LLMs. Vector quantization (VQ), a crucial technique for ANNS, is commonly used to reduce space overhead and accelerate distance computations. However, despite significant research advances, state-of-the-art VQ methods still face challenges in balancing encoding efficiency and quantization accuracy. To address these limitations, we propose a novel VQ method called SAQ. To improve accuracy, SAQ employs a new dimension segmentation technique to strategically partition PCA-projected vectors into segments along their dimensions. By prioritizing leading dimension segments with larger magnitudes, SAQ allocates more bits to high-impact segments, optimizing the use of the available space quota. An efficient dynamic programming algorithm is developed to optimize dimension segmentation and bit allocation, ensuring minimal quantization error. To speed up vector encoding, SAQ devises a code adjustment technique to first quantize each dimension independently and then progressively refine quantized vectors using a coordinate-descent-like approach to avoid exhaustive enumeration. Extensive experiments demonstrate SAQ's superiority over classical methods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ, Extended RabitQ). SAQ achieves up to 80% reduction in quantization error and accelerates encoding speed by over 80x compared to Extended RabitQ.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12086v2</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3769824</arxiv:DOI>
      <dc:creator>Hui Li, Shiyuan Deng, Xiao Yan, Xiangyu Zhi, James Cheng</dc:creator>
    </item>
  </channel>
</rss>
