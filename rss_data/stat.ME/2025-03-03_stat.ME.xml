<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Mar 2025 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Modeling times to multiple events under informative censoring with C-vine copula</title>
      <link>https://arxiv.org/abs/2502.20608</link>
      <description>arXiv:2502.20608v1 Announce Type: new 
Abstract: The study of times to nonterminal events of different types and their interrelation is a compelling area of interest. The primary challenge in analyzing such multivariate event times is the presence of informative censoring by the terminal event. While numerous statistical methods have been proposed for a single nonterminal event, i.e., semi-competing risks data, there remains a dearth of tools for analyzing times to multiple nonterminal events. These events involve more complex dependence structures between nonterminal and terminal events and between nonterminal events themselves. This paper introduces a novel modeling framework leveraging the vine copula to directly estimate the joint distribution of the multivariate times to nonterminal and terminal events. Unlike the few existing methods based on multivariate or nested copulas, our model excels in capturing the heterogeneous dependence between each pair of event times in terms of strength and structure. Furthermore, our model allows regression modeling for all the marginal distributions of times to nonterminal and terminal events, a feature lacking in existing methods. We propose a likelihood-based estimation and inference procedure, which can be implemented efficiently in sequential stages. Through simulation studies, we demonstrate the satisfactory finite-sample performance of our proposed stage-wise estimators and analytical variance estimators, as well as their superiority over existing methods. We apply our approach to data from a crowdfunding platform to investigate the relationship between creator-backer interactions of various types and a creator's lifetime on the platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20608v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Yiwei Li, Qian M. Zhou</dc:creator>
    </item>
    <item>
      <title>Nonparanormal Modeling Framework for Prognostic Biomarker Assessment with Application to Amyotrophic Lateral Sclerosis</title>
      <link>https://arxiv.org/abs/2502.20892</link>
      <description>arXiv:2502.20892v1 Announce Type: new 
Abstract: Identifying reliable biomarkers for predicting clinical events in longitudinal studies is important for accurate disease prognosis and the development of new treatments. However, prognostic studies are often not randomized, making it difficult to account for patient heterogeneity. In amyotrophic lateral sclerosis (ALS), factors such as age, site of disease onset and genetics impact both survival duration and biomarker levels, yet their impact on the prognostic accuracy of biomarkers over different time horizons remains unclear. While existing methods for time-dependent receiver operating characteristic (ROC) analysis have been adapted for censored time-to-event outcomes, most do not adjust for patient covariates. To address this, we propose the nonparanormal prognostic biomarker (NPB) framework, which models the joint dependence between biomarker and event time distributions while accounting for covariates. This provides covariate-specific ROC curves which assess a potential biomarker's accuracy for a given time horizon. We apply this framework to evaluate serum neurofilament light (NfL) as a biomarker in ALS and demonstrate that its prognostic accuracy varies over time and across patient subgroups. The NPB framework is broadly applicable to other conditions and has the potential to improve clinical trial efficiency by refining patient stratification and reducing sample size requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20892v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ainesh Sewak, Vanda Inacio, Joanne Wuu, Michael Benatar, Torsten Hothorn</dc:creator>
    </item>
    <item>
      <title>The two filter formula reconsidered: Smoothing in partially observed Gauss--Markov models without information parametrization</title>
      <link>https://arxiv.org/abs/2502.21116</link>
      <description>arXiv:2502.21116v1 Announce Type: new 
Abstract: In this article, the two filter formula is re-examined in the setting of partially observed Gauss--Markov models. It is traditionally formulated as a filter running backward in time, where the Gaussian density is parametrized in ``information form''. However, the quantity in the backward recursion is strictly speaking not a distribution, but a likelihood. Taking this observation seriously, a recursion over log-quadratic likelihoods is formulated instead, which obviates the need for ``information'' parametrization. In particular, it greatly simplifies the square-root formulation of the algorithm. Furthermore, formulae are given for producing the forward Markov representation of the a posteriori distribution over paths from the proposed likelihood representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21116v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filip Tronarp</dc:creator>
    </item>
    <item>
      <title>A new block covariance regression model and inferential framework for massively large neuroimaging data</title>
      <link>https://arxiv.org/abs/2502.21235</link>
      <description>arXiv:2502.21235v1 Announce Type: new 
Abstract: Some evidence suggests that people with autism spectrum disorder exhibit patterns of brain functional dysconnectivity relative to their typically developing peers, but specific findings have yet to be replicated. To facilitate this replication goal with data from the Autism Brain Imaging Data Exchange (ABIDE), we propose a flexible and interpretable model for participant-specific voxel-level brain functional connectivity. Our approach efficiently handles massive participant-specific whole brain voxel-level connectivity data that exceed one trillion data points. The key component of the model is to leverage the block structure induced by defined regions of interest to introduce parsimony in the high-dimensional connectivity matrix through a block covariance structure. Associations between brain functional connectivity and participant characteristics -- including eye status during the resting scan, sex, age, and their interactions -- are estimated within a Bayesian framework. A spike-and-slab prior facilitates hypothesis testing to identify voxels associated with autism diagnosis. Simulation studies are conducted to evaluate the empirical performance of the proposed model and estimation framework. In ABIDE, the method replicates key findings from the literature and suggests new associations for investigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21235v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyoshin Kim, Sujit K. Ghosh, Emily C. Hector</dc:creator>
    </item>
    <item>
      <title>Boosting Prediction with Data Missing Not at Random</title>
      <link>https://arxiv.org/abs/2502.21276</link>
      <description>arXiv:2502.21276v1 Announce Type: new 
Abstract: Boosting has emerged as a useful machine learning technique over the past three decades, attracting increased attention. Most advancements in this area, however, have primarily focused on numerical implementation procedures, often lacking rigorous theoretical justifications. Moreover, these approaches are generally designed for datasets with fully observed data, and their validity can be compromised by the presence of missing observations. In this paper, we employ semiparametric estimation approaches to develop boosting prediction methods for data with missing responses. We explore two strategies for adjusting the loss functions to account for missingness effects. The proposed methods are implemented using a functional gradient descent algorithm, and their theoretical properties, including algorithm convergence and estimator consistency, are rigorously established. Numerical studies demonstrate that the proposed methods perform well in finite sample settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21276v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Bian, Grace Y. Yi, Wenqing He</dc:creator>
    </item>
    <item>
      <title>Large Sample Inference with Dynamic Information Borrowing</title>
      <link>https://arxiv.org/abs/2502.21282</link>
      <description>arXiv:2502.21282v1 Announce Type: new 
Abstract: Large sample behavior of dynamic information borrowing (DIB) estimators is investigated. Asymptotic properties of several DIB approaches (adaptive risk minimization, adaptive LASSO, Bayesian procedures with empirical power prior, fully Bayesian procedures, and a Bayes-frequentist compromise) are explored against shrinking to zero alternatives. As shown theoretically and with simulations, local asymptotic distributions of DIB estimators are often non-normal. A simple Gaussian setting with external information borrowing illustrates that none of the considered DIB methods outperforms others in terms of mean squared error (MSE): at different conflict values, the MSEs of DIBs are changing between the MSEs of the maximum likelihood estimators based on the current and pooled data. To uniquely determine an optimality criterion for DIB, a prior distribution on the conflict needs be either implicitly or explicitly determined using data independent considerations. Data independent assumptions on the conflict are also needed for DIB-based hypothesis testing. New families of DIB estimators parameterized by a sensitivity-to-conflict parameter S are suggested and their use is illustrated in an infant mortality example. The choice of S is determined in a data-independent manner by a cost-benefit compromise associated with the use of external data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21282v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergey Tarima, Silvia Calderazzo, Mary Homan</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Knockoffs Inference for Time Series Data</title>
      <link>https://arxiv.org/abs/2112.09851</link>
      <description>arXiv:2112.09851v3 Announce Type: replace 
Abstract: We make some initial attempt to establish the theoretical and methodological foundation for the model-X knockoffs inference for time series data. We suggest the method of time series knockoffs inference (TSKI) by exploiting the ideas of subsampling and e-values to address the difficulty caused by the serial dependence. We also generalize the robust knockoffs inference in Barber, Cand\`es, and Samworth to the time series setting to relax the assumption of known covariate distribution required by model-X knockoffs, since such an assumption is overly stringent for time series data. We establish sufficient conditions under which TSKI achieves the asymptotic false discovery rate (FDR) control. Our technical analysis reveals the effects of serial dependence and unknown covariate distribution on the FDR control. We conduct a power analysis of TSKI using the Lasso coefficient difference knockoff statistic under the generalized linear time series models. The finite-sample performance of TSKI is illustrated with several simulation examples and an economic inflation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.09851v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of the American Statistical Association 2025</arxiv:journal_reference>
      <dc:creator>Chien-Ming Chi, Yingying Fan, Ching-Kang Ing, Jinchi Lv</dc:creator>
    </item>
    <item>
      <title>Statistical inference for large-dimensional tensor factor model by iterative projections</title>
      <link>https://arxiv.org/abs/2206.09800</link>
      <description>arXiv:2206.09800v3 Announce Type: replace 
Abstract: Tensor Factor Models (TFM) are appealing dimension reduction tools for high-order large-dimensional tensor time series, and have wide applications in economics, finance and medical imaging. In this paper, we propose a projection estimator for the Tucker-decomposition based TFM, and provide its least-square interpretation which parallels to the least-square interpretation of the Principal Component Analysis (PCA) for the vector factor model. The projection technique simultaneously reduces the dimensionality of the signal component and the magnitudes of the idiosyncratic component tensor, thus leading to an increase of the signal-to-noise ratio. We derive a convergence rate of the projection estimator of the loadings and the common factor tensor which are faster than that of the naive PCA-based estimator. Our results are obtained under mild conditions which allow the idiosyncratic components to be weakly cross- and auto- correlated. We also provide a novel iterative procedure based on the eigenvalue-ratio principle to determine the factor numbers. Extensive numerical studies are conducted to investigate the empirical performance of the proposed projection estimators relative to the state-of-the-art ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.09800v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Yong He, Lingxiao Li, Lorenzo Trapani</dc:creator>
    </item>
    <item>
      <title>When Respondents Don't Care Anymore: Identifying the Onset of Careless Responding</title>
      <link>https://arxiv.org/abs/2303.07167</link>
      <description>arXiv:2303.07167v3 Announce Type: replace 
Abstract: Questionnaires in the behavioral and organizational sciences tend to be lengthy. However, literature suggests that survey length is a contributing factor to careless responding, with longer questionnaires yielding higher probability that participants start responding carelessly. Consequently, in long surveys a large number of participants may engage in careless responding, posing a major threat to internal validity. We propose a novel method for identifying the onset of careless responding (or an absence thereof) that searches for a changepoint in combined measurements of multiple dimensions in which carelessness may manifest, such as inconsistency and invariability. It is highly flexible, based on machine learning, and provides statistical guarantees for controlling the false positive rate. In simulation experiments, the proposed method achieves high accuracy in identifying carelessness onset and discriminates well between attentive and various types of careless responding, even when a large number of careless respondents are present. An empirical application highlights how identifying partial carelessness uncovers novel insights on careless responding behavior. Furthermore, we provide the freely available open source software package "carelessonset" to facilitate adoption by empirical researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07167v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Welz, Andreas Alfons</dc:creator>
    </item>
    <item>
      <title>Generalized Decomposition Priors on R2</title>
      <link>https://arxiv.org/abs/2401.10180</link>
      <description>arXiv:2401.10180v2 Announce Type: replace 
Abstract: The adoption of continuous shrinkage priors in high-dimensional linear models has gained widespread attention due to their practical and theoretical advantages. Among them, the R2D2 prior has gained popularity for its intuitive specification of the proportion of explained variance (R2) and its theoretically grounded properties. The R2D2 prior allocates variance among regression terms through a Dirichlet decomposition. However, this approach inherently limits the dependency structure among variance components to the negative dependence modeled by the Dirichlet distribution, which is fully determined by the mean. This limitation hinders the prior's ability to capture more nuanced or positive dependency patterns that may arise in real-world data.
  To address this, we propose the Generalized Decomposition R2 (GDR2) prior, which replaces the Dirichlet decomposition with the more flexible Logistic-Normal distribution and its variants. By allowing richer dependency structures, the GDR2 prior accommodates more realistic and adaptable competition among variance components, enhancing the expressiveness and applicability of R2-based priors in practice. Through simulations and real-world benchmarks, we demonstrate that the GDR2 prior improves out-of-sample predictive performance and parameter recovery compared to the R2D2 prior. Our framework bridges the gap between flexibility in variance decomposition and practical implementation, advancing the utility of shrinkage priors in complex regression settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10180v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javier Enrique Aguilar, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>An introduction to statistical models used to characterize species-habitat associations with animal movement data</title>
      <link>https://arxiv.org/abs/2401.17389</link>
      <description>arXiv:2401.17389v3 Announce Type: replace 
Abstract: Understanding species-habitat associations is fundamental to ecological sciences and for species conservation. Consequently, various statistical approaches have been designed to infer species-habitat associations. Due to their conceptual and mathematical differences, these methods can yield contrasting results. We describe and compare commonly used statistical models that relate animal movement data to environmental data, including resource selection functions (RSF), step-selection functions (SSF), and hidden Markov models (HMMs). We demonstrate differences in assumptions and highlighting advantages and limitations of each method. Additionally, we provide guidance on selecting the most appropriate statistical method based on the scale of the data and intended inference. To illustrate the varying ecological insights derived from each model, we apply them to the movement track of a single ringed seal in a case study. We demonstrate that each model yields varying ecological insights. For example, while the selection coefficient values from RSFs appear to show a stronger positive relationship with prey diversity than those of the SSFs, when we accounted for the autocorrelation in the data none of these relationships with prey diversity were statistically significant. The HMM reveals variable associations with prey diversity across different behaviors. Notably, the three models identified different important areas. This case study highlights the critical significance of selecting the appropriate model as an essential step in the process of identifying species-habitat relationships and specific areas of importance. Our review provides the foundational information required for making informed decisions when choosing the most suitable statistical methods to address specific questions, such as identifying protected zones, understanding movement patterns, or studying behaviours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17389v3</guid>
      <category>stat.ME</category>
      <category>q-bio.PE</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katie R. N. Florko, Ron R. Togunov, Rowenna Gryba, Evan Sidrow, Steven H. Ferguson, David J. Yurkowski, Marie Auger-M\'eth\'e</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis for Attributable Effects in Case$^2$ Studies</title>
      <link>https://arxiv.org/abs/2405.16046</link>
      <description>arXiv:2405.16046v2 Announce Type: replace 
Abstract: The case$^2$ study, also referred to as the case-case study design, is a valuable approach for conducting inference for treatment effects. Unlike traditional case-control studies, the case$^2$ design compares treatment in two types of cases with the same disease. A key quantity of interest is the attributable effect, which is the number of cases of disease among treated units which are caused by the treatment. Two key assumptions that are usually made for making inferences about the attributable effect in case$^2$ studies are 1.) treatment does not cause the second type of case, and 2.) the treatment does not alter an individual's case type. However, these assumptions are not realistic in many real-data applications. In this article, we present a sensitivity analysis framework to scrutinize the impact of deviations from these assumptions on obtained results. We also include sensitivity analyses related to the assumption of unmeasured confounding, recognizing the potential bias introduced by unobserved covariates. The proposed methodology is exemplified through an investigation into whether having violent behavior in the last year of life increases suicide risk via 1993 National Mortality Followback Survey dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16046v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kan Chen, Ting Ye, Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>Bayesian Stability Selection and Inference on Selection Probabilities</title>
      <link>https://arxiv.org/abs/2410.21914</link>
      <description>arXiv:2410.21914v2 Announce Type: replace 
Abstract: Stability selection is a versatile framework for structure estimation and variable selection in high-dimensional setting, primarily grounded in frequentist principles. In this paper, we propose an enhanced methodology that integrates Bayesian analysis to refine the inference of selection probabilities within the stability selection framework. Traditional approaches rely on selection frequencies for decision-making, often disregarding domain-specific knowledge. Our methodology uses prior information to derive posterior distributions of selection probabilities, thereby improving both inference and decision-making. We present a two-step process for engaging with domain experts, enabling statisticians to construct prior distributions informed by expert knowledge while allowing experts to control the weight of their input on the final results. Using posterior distributions, we offer Bayesian credible intervals to quantify uncertainty in the variable selection process. Furthermore, we demonstrate how the integration of prior knowledge reduces the variance of selection probabilities, thereby improving the stability of decision-making. Our approach preserves the versatility of stability selection and is suitable for a broad range of structure estimation challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21914v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Nouraie, Connor Smith, Samuel Muller</dc:creator>
    </item>
    <item>
      <title>Temporal Wasserstein Imputation: Versatile Missing Data Imputation for Time Series</title>
      <link>https://arxiv.org/abs/2411.02811</link>
      <description>arXiv:2411.02811v2 Announce Type: replace 
Abstract: Missing data can significantly hamper standard time series analysis, yet in practice they are frequently encountered. In this paper, we introduce temporal Wasserstein imputation, a novel method for imputing missing data in time series. Unlike existing techniques, our approach is fully nonparametric, circumventing the need for model specification prior to imputation, making it suitable for potential nonlinear dynamics. Its principled algorithmic implementation can seamlessly handle univariate or multivariate time series with any non-systematic missing pattern. In addition, the plausible range and side information of the missing entries (such as box constraints) can easily be incorporated. As a key advantage, our method mitigates the distributional bias typical of many existing approaches, ensuring more reliable downstream statistical analysis using the imputed series. Leveraging the benign landscape of the optimization formulation, we establish the convergence of an alternating minimization algorithm to critical points. We also provide conditions under which the marginal distributions of the underlying time series can be identified. Numerical experiments, including extensive simulations covering linear and nonlinear time series models and a real-world groundwater dataset laden with missing values, corroborate the practical usefulness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02811v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuo-Chieh Huang, Tengyuan Liang, Ruey S. Tsay</dc:creator>
    </item>
    <item>
      <title>Multi-Quantile Estimators for the parameters of Generalized Extreme Value distribution</title>
      <link>https://arxiv.org/abs/2412.04640</link>
      <description>arXiv:2412.04640v2 Announce Type: replace 
Abstract: We introduce and study Multi-Quantile estimators for the parameters $( \xi, \sigma, \mu)$ of Generalized Extreme Value (GEV) distributions to provide a robust approach to extreme value modeling. Unlike classical estimators, such as the Maximum Likelihood Estimation (MLE) estimator and the Probability Weighted Moments (PWM) estimator, which impose strict constraints on the shape parameter $\xi$, our estimators are always asymptotically normal and consistent across all values of the GEV parameters. The asymptotic variances of our estimators decrease with the number of quantiles increasing and can approach the Cram\'er-Rao lower bound very closely whenever it exists. Our Multi-Quantile Estimators thus offer a more flexible and efficient alternative for practical applications. We also discuss how they can be implemented in the context of Block Maxima method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04640v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sen Lin, Ao Kong, Robert Azencott</dc:creator>
    </item>
    <item>
      <title>Revisiting Optimal Proportions for Binary Responses: Insights from Incorporating the Absent Perspective of Type-I Error Rate Control</title>
      <link>https://arxiv.org/abs/2502.06381</link>
      <description>arXiv:2502.06381v2 Announce Type: replace 
Abstract: This work revisits optimal response-adaptive designs from a type-I error rate perspective, highlighting when and how much these allocations exacerbate type-I error rate inflation - an issue previously undocumented. We explore a range of approaches from the literature that can be applied to reduce type-I error rate inflation. However, we found that all of these approaches fail to give a robust solution to the problem. To address this, we derive two optimal proportions, incorporating the more robust score test (instead of the Wald test) with finite sample estimators (instead of the unknown true values) in the formulation of the optimization problem. One proportion optimizes statistical power and the other minimizes the total number failures in a trial while maintaining a predefined power level. Through simulations based on an early-phase and a confirmatory trial we provide crucial practical insight into how these new optimal proportion designs can offer substantial patient outcomes advantages while controlling type-I error rate. While we focused on binary outcomes, the framework offers valuable insights that naturally extend to other outcome types, multi-armed trials and alternative measures of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06381v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pin, Sof\'ia S. Villar, William F. Rosenberger</dc:creator>
    </item>
    <item>
      <title>Stochastic Block Covariance Matrix Estimation</title>
      <link>https://arxiv.org/abs/2502.11332</link>
      <description>arXiv:2502.11332v2 Announce Type: replace 
Abstract: Motivated by a neuroscience application we study the problem of statistical estimation of a high-dimensional covariance matrix with a block structure. The block model embeds a structural assumption: the population of items (neurons) can be divided into latent sub-populations with shared associative covariation within blocks and shared associative or dis-associative covariation across blocks. Unlike the block diagonal assumption, our block structure incorporates positive or negative pairwise correlation between blocks. In addition to offering reasonable modeling choices in neuroscience and economics, the block covariance matrix assumption is interesting purely from the perspective of statistical estimation theory: (a) it offers in-built dimension reduction and (b) it resembles a regularized factor model without the need of choosing the number of factors. We discuss a hierarchical Bayesian estimation method to simultaneously recover the latent blocks and estimate the overall covariance matrix. We show with numerical experiments that a hierarchical structure and a shrinkage prior are essential to accurate recovery when several blocks are present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11332v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunran Chen, Surya T Tokdar, Jennifer M Groh</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Random Unknowns via Modifications of Extended Likelihood</title>
      <link>https://arxiv.org/abs/2310.09955</link>
      <description>arXiv:2310.09955v3 Announce Type: replace-cross 
Abstract: Fisher's likelihood is widely used for statistical inference for fixed unknowns. This paper aims to extend two important likelihood-based methods, namely the maximum likelihood procedure for point estimation and the confidence procedure for interval estimation, to embrace a broader class of statistical models with additional random unknowns. We propose the new h-likelihood and the h-confidence by modifying extended likelihoods. Maximization of the h-likelihood yields both maximum likelihood estimators of fixed unknowns and asymptotically optimal predictors for random unknowns, achieving the generalized Cram\'er-Rao lower bound. The h-likelihood further offers advantages in scalability for large datasets and complex models. The h-confidence could yield a valid interval estimation and prediction by maintaining the coverage probability for both fixed and random unknowns in small samples. We study approximate methods for the h-likelihood and h-confidence, which can be applied to a general class of models with additional random unknowns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09955v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hangbin Lee, Youngjo Lee</dc:creator>
    </item>
    <item>
      <title>Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models</title>
      <link>https://arxiv.org/abs/2404.19707</link>
      <description>arXiv:2404.19707v3 Announce Type: replace-cross 
Abstract: Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian. We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes. We also discuss the problem of labelling the shocks, estimation of the parameters, and stationarity the model. The introduced methods are implemented to the accompanying R package sstvars. Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy. In a structural logistic smooth transition vector autoregressive model consisting of two regimes, we find that a positive climate policy uncertainty shock decreases production and increases inflation in times of both low and high economic policy uncertainty, but its inflationary effects are stronger in the periods of high economic policy uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19707v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Savi Virolainen</dc:creator>
    </item>
  </channel>
</rss>
