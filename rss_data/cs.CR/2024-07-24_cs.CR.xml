<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CR</link>
    <description>cs.CR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Jul 2024 01:37:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Data Poisoning Attacks in Intelligent Transportation Systems: A Survey</title>
      <link>https://arxiv.org/abs/2407.15855</link>
      <description>arXiv:2407.15855v1 Announce Type: new 
Abstract: Emerging technologies drive the ongoing transformation of Intelligent Transportation Systems (ITS). This transformation has given rise to cybersecurity concerns, among which data poisoning attack emerges as a new threat as ITS increasingly relies on data. In data poisoning attacks, attackers inject malicious perturbations into datasets, potentially leading to inaccurate results in offline learning and real-time decision-making processes. This paper concentrates on data poisoning attack models against ITS. We identify the main ITS data sources vulnerable to poisoning attacks and application scenarios that enable staging such attacks. A general framework is developed following rigorous study process from cybersecurity but also considering specific ITS application needs. Data poisoning attacks against ITS are reviewed and categorized following the framework. We then discuss the current limitations of these attack models and the future research directions. Our work can serve as a guideline to better understand the threat of data poisoning attacks against ITS applications, while also giving a perspective on the future development of trustworthy ITS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15855v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Feilong Wang, Xin Wang, Xuegang Ban</dc:creator>
    </item>
    <item>
      <title>Adversarial Attacks and Defenses on Text-to-Image Diffusion Models: A Survey</title>
      <link>https://arxiv.org/abs/2407.15861</link>
      <description>arXiv:2407.15861v1 Announce Type: new 
Abstract: Recently, the text-to-image diffusion model has gained considerable attention from the community due to its exceptional image generation capability. A representative model, Stable Diffusion, amassed more than 10 million users within just two months of its release. This surge in popularity has facilitated studies on the robustness and safety of the model, leading to the proposal of various adversarial attack methods. Simultaneously, there has been a marked increase in research focused on defense methods to improve the robustness and safety of these models. In this survey, we provide a comprehensive review of the literature on adversarial attacks and defenses targeting text-to-image diffusion models. We begin with an overview of text-to-image diffusion models, followed by an introduction to a taxonomy of adversarial attacks and an in-depth review of existing attack methods. We then present a detailed analysis of current defense methods that improve model robustness and safety. Finally, we discuss ongoing challenges and explore promising future research directions. For a complete list of the adversarial attack and defense methods covered in this survey, please refer to our curated repository at https://github.com/datar001/Awesome-AD-on-T2IDM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15861v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyu Zhang, Mingwang Hu, Wenhui Li, Lanjun Wang</dc:creator>
    </item>
    <item>
      <title>A Survey on Differential Privacy for SpatioTemporal Data in Transportation Research</title>
      <link>https://arxiv.org/abs/2407.15868</link>
      <description>arXiv:2407.15868v1 Announce Type: new 
Abstract: With low-cost computing devices, improved sensor technology, and the proliferation of data-driven algorithms, we have more data than we know what to do with. In transportation, we are seeing a surge in spatiotemporal data collection. At the same time, concerns over user privacy have led to research on differential privacy in applied settings. In this paper, we look at some recent developments in differential privacy in the context of spatiotemporal data. Spatiotemporal data contain not only features about users but also the geographical locations of their frequent visits. Hence, the public release of such data carries extreme risks. To address the need for such data in research and inference without exposing private information, significant work has been proposed. This survey paper aims to summarize these efforts and provide a review of differential privacy mechanisms and related software. We also discuss related work in transportation where such mechanisms have been applied. Furthermore, we address the challenges in the deployment and mass adoption of differential privacy in transportation spatiotemporal data for downstream analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15868v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Bhadani</dc:creator>
    </item>
    <item>
      <title>Blockchain in Healthcare: Implementing Hyperledger Fabric for Electronic Health Records at Frere Provincial Hospital</title>
      <link>https://arxiv.org/abs/2407.15876</link>
      <description>arXiv:2407.15876v1 Announce Type: new 
Abstract: As healthcare systems worldwide continue to grapple with the challenges of interoperability, data security, and accessibility, integrating emerging technologies becomes imperative. This paper investigates the implementation of blockchain technology, specifically Hyperledger Fabric, for Electronic Health Records (EHR) management at Frere Hospital in the Eastern Cape province of South Africa. The paper examines the benefits and challenges of integrating blockchain into healthcare information systems. Hyperledger Fabric's modular architecture is harnessed to create a secure, transparent, and decentralized platform for storing, managing, and sharing EHRs among stakeholders. The study used a mixed-methods approach, integrating case studies and data collection methods through observation and informal questions, with the specific goal of understanding current record management methods and challenges. This method offers practical insights and validates the approach. The result demonstrates the role of blockchain in transforming healthcare, framed within a rigorous exploration and analysis. The findings of this study have broader implications for healthcare institutions seeking advanced solutions to address the persistent challenges in electronic health record management. Ultimately, the research underscores the transformative potential of blockchain technology in healthcare settings, fostering trust, security, and efficiency in the management of sensitive patient data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15876v1</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abayomi Agbeyangi, Olukayode Oki, Aphelele Mgidi</dc:creator>
    </item>
    <item>
      <title>Decentralized Federated Anomaly Detection in Smart Grids: A P2P Gossip Approach</title>
      <link>https://arxiv.org/abs/2407.15879</link>
      <description>arXiv:2407.15879v1 Announce Type: new 
Abstract: The increasing security and privacy concerns in the Smart Grid sector have led to a significant demand for robust intrusion detection systems within critical smart grid infrastructure. To address the challenges posed by privacy preservation and decentralized power system zones with distinct data ownership, Federated Learning (FL) has emerged as a promising privacy-preserving solution which facilitates collaborative training of attack detection models without necessitating the sharing of raw data. However, FL presents several implementation limitations in the power system domain due to its heavy reliance on a centralized aggregator and the risks of privacy leakage during model update transmission. To overcome these technical bottlenecks, this paper introduces a novel decentralized federated anomaly detection scheme based on two main gossip protocols namely Random Walk and Epidemic. Our findings indicate that the Random Walk protocol exhibits superior performance compared to the Epidemic protocol, highlighting its efficacy in decentralized federated learning environments. Experimental validation of the proposed framework utilizing publicly available industrial control systems datasets demonstrates superior attack detection accuracy while safeguarding data confidentiality and mitigating the impact of communication latency and stragglers. Furthermore, our approach yields a notable 35% improvement in training time compared to conventional FL, underscoring the efficacy and robustness of our decentralized learning method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15879v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Akbar Husnoo, Adnan Anwar, Md Enamul Haque, A. N. Mahmood</dc:creator>
    </item>
    <item>
      <title>Revisiting the Robust Alignment of Circuit Breakers</title>
      <link>https://arxiv.org/abs/2407.15902</link>
      <description>arXiv:2407.15902v1 Announce Type: new 
Abstract: Over the past decade, adversarial training has emerged as one of the few reliable methods for enhancing model robustness against adversarial attacks [Szegedy et al., 2014, Madry et al., 2018, Xhonneux et al., 2024], while many alternative approaches have failed to withstand rigorous subsequent evaluations. Recently, an alternative defense mechanism, namely "circuit breakers" [Zou et al., 2024], has shown promising results for aligning LLMs. In this report, we show that the robustness claims of "Improving Alignment and Robustness with Circuit Breakers" against unconstraint continuous attacks in the embedding space of the input tokens may be overestimated [Zou et al., 2024]. Specifically, we demonstrate that by implementing a few simple changes to embedding space attacks [Schwinn et al., 2024a,b], we achieve 100% attack success rate (ASR) against circuit breaker models. Without conducting any further hyperparameter tuning, these adjustments increase the ASR by more than 80% compared to the original evaluation. Code is accessible at: https://github.com/SchwinnL/circuit-breakers-eval</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15902v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leo Schwinn, Simon Geisler</dc:creator>
    </item>
    <item>
      <title>The Shadow of Fraud: The Emerging Danger of AI-powered Social Engineering and its Possible Cure</title>
      <link>https://arxiv.org/abs/2407.15912</link>
      <description>arXiv:2407.15912v1 Announce Type: new 
Abstract: Social engineering (SE) attacks remain a significant threat to both individuals and organizations. The advancement of Artificial Intelligence (AI), including diffusion models and large language models (LLMs), has potentially intensified these threats by enabling more personalized and convincing attacks. This survey paper categorizes SE attack mechanisms, analyzes their evolution, and explores methods for measuring these threats. It highlights the challenges in raising awareness about the risks of AI-enhanced SE attacks and offers insights into developing proactive and adaptable defense strategies. Additionally, we introduce a categorization of the evolving nature of AI-powered social engineering attacks into "3E phases": Enlarging, wherein the magnitude of attacks expands through the leverage of digital media; Enriching, introducing novel attack vectors and techniques; and Emerging, signifying the advent of novel threats and methods. Moreover, we emphasize the necessity for a robust framework to assess the risk of AI-powered SE attacks. By identifying and addressing gaps in existing research, we aim to guide future studies and encourage the development of more effective defenses against the growing threat of AI-powered social engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15912v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingru Yu, Yi Yu, Xuhong Wang, Yilun Lin, Manzhi Yang, Yu Qiao, Fei-Yue Wang</dc:creator>
    </item>
    <item>
      <title>Virtual Reality and Augmented Reality Security: A Reconnaissance and Vulnerability Assessment Approach</title>
      <link>https://arxiv.org/abs/2407.15984</link>
      <description>arXiv:2407.15984v1 Announce Type: new 
Abstract: Various industries have widely adopted Virtual Reality (VR) and Augmented Reality (AR) technologies to enhance productivity and user experiences. However, their integration introduces significant security challenges. This systematic literature review focuses on identifying devices used in AR and VR technologies and specifies the associated vulnerabilities, particularly during the reconnaissance phase and vulnerability assessment, which are critical steps in penetration testing. Following Kitchenham and Charters' guidelines, we systematically selected and analyzed primary studies. The reconnaissance phase involves gathering detailed information about AR and VR systems to identify potential attack vectors. In the vulnerability assessment phase, these vectors are analyzed to pinpoint weaknesses that malicious actors could exploit. Our findings reveal that AR and VR devices, such as headsets (e.g., HTC Vive, Oculus Quest), development platforms (e.g., Unity Framework, Google Cardboard SDK), and applications (e.g., Bigscreen VR, VRChat), are susceptible to various attacks, including remote code execution, cross-site scripting (XSS), eavesdropping, and man-in-the-room attacks. Specifically, the Bigscreen VR application exhibited severe vulnerabilities like remote code execution (RCE) via the 'Application.OpenURL' API, XSS in user inputs, and botnet propagation. Similarly, the Oculus Quest demonstrated susceptibility to side-channel attacks and ransomware. This paper provides a detailed overview of specific device vulnerabilities and emphasizes the importance of the initial steps in penetration testing to identify security weaknesses in AR and VR systems. By highlighting these vulnerabilities, we aim to assist researchers in exploring and mitigating these security challenges, ensuring the safe deployment and use of AR and VR technologies across various sectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15984v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarina Dastgerdy</dc:creator>
    </item>
    <item>
      <title>ImPress: Securing DRAM Against Data-Disturbance Errors via Implicit Row-Press Mitigation</title>
      <link>https://arxiv.org/abs/2407.16006</link>
      <description>arXiv:2407.16006v1 Announce Type: new 
Abstract: DRAM cells are susceptible to Data-Disturbance Errors (DDE), which can be exploited by an attacker to compromise system security. Rowhammer is a well-known DDE vulnerability that occurs when a row is repeatedly activated. Rowhammer can be mitigated by tracking aggressor rows inside DRAM (in-DRAM) or at the Memory Controller (MC). Row-Press (RP) is a new DDE vulnerability that occurs when a row is kept open for a long time. RP significantly reduces the number of activations required to induce an error, thus breaking existing RH solutions. Prior work on Explicit Row-Press mitigation, ExPress, requires the memory controller to limit the maximum row-open-time, and redesign existing Rowhammer solutions with reduced Rowhammer threshold. Unfortunately, ExPress incurs significant performance and storage overheads, and being a memory controller-based solution, it is incompatible with in-DRAM trackers. In this paper, we propose Implicit Row-Press mitigation (ImPress), which does not restrict row-open-time, is compatible with memory controller-based and in-DRAM solutions and does not reduce the tolerated Rowhammer threshold. ImPress treats a row open for a specified time as equivalent to an activation. We design ImPress by developing a Unified Charge-Loss Model, which combines the net effect of both Rowhammer and Row-Press for arbitrary patterns. We analyze both controller-based (Graphene and PARA) and in-DRAM trackers (Mithril and MINT). We show that ImPress makes Rowhammer solutions resilient to Row-Press transparently, without affecting the Rowhammer threshold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16006v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moinuddin Qureshi, Anish Saxena, Aamer Jaleel</dc:creator>
    </item>
    <item>
      <title>Wallcamera: Reinventing the Wheel?</title>
      <link>https://arxiv.org/abs/2407.16015</link>
      <description>arXiv:2407.16015v1 Announce Type: new 
Abstract: Developed at MIT CSAIL, the Wallcamera has captivated the public's imagination. Here, we show that the key insight underlying the Wallcamera is the same one that underpins the concept and the prototype of differential imaging forensics (DIF), both of which were validated and reported several years prior to the Wallcamera's debut. Rather than being the first to extract and amplify invisible signals -- aka latent evidence in the forensics context -- from wall reflections in a video, or the first to propose activity recognition following that approach, the Wallcamera's actual innovation is achieving activity recognition at a finer granularity than DIF demonstrated. In addition to activity recognition, DIF as conceived has a number of other applications in forensics, including 1) the recovery of a photographer's personal identifiable information such as body width, height, and even the color of their clothing, from a single photo, and 2) the detection of image tampering and deepfake videos.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16015v1</guid>
      <category>cs.CR</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aur\'elien Bourquard, Jeff Yan</dc:creator>
    </item>
    <item>
      <title>MINT: Securely Mitigating Rowhammer with a Minimalist In-DRAM Tracker</title>
      <link>https://arxiv.org/abs/2407.16038</link>
      <description>arXiv:2407.16038v1 Announce Type: new 
Abstract: This paper investigates secure low-cost in-DRAM trackers for mitigating Rowhammer (RH). In-DRAM solutions have the advantage that they can solve the RH problem within the DRAM chip, without relying on other parts of the system. However, in-DRAM mitigation suffers from two key challenges: First, the mitigations are synchronized with refresh, which means we cannot mitigate at arbitrary times. Second, the SRAM area available for aggressor tracking is severely limited, to only a few bytes. Existing low-cost in-DRAM trackers (such as TRR) have been broken by well-crafted access patterns, whereas prior counter-based schemes require impractical overheads of hundreds or thousands of entries per bank. The goal of our paper is to develop an ultra low-cost secure in-DRAM tracker.
  Our solution is based on a simple observation: if only one row can be mitigated at refresh, then we should ideally need to track only one row. We propose a Minimalist In-DRAM Tracker (MINT), which provides secure mitigation with just a single entry. At each refresh, MINT probabilistically decides which activation in the upcoming interval will be selected for mitigation at the next refresh. MINT provides guaranteed protection against classic single and double-sided attacks. We also derive the minimum RH threshold (MinTRH) tolerated by MINT across all patterns. MINT has a MinTRH of 1482 which can be lowered to 356 with RFM. The MinTRH of MINT is lower than a prior counter-based design with 677 entries per bank, and is within 2x of the MinTRH of an idealized design that stores one-counter-per-row. We also analyze the impact of refresh postponement on the MinTRH of low-cost in-DRAM trackers, and propose an efficient solution to make such trackers compatible with refresh postponement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16038v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moinuddin Qureshi, Salman Qazi, Aamer Jaleel</dc:creator>
    </item>
    <item>
      <title>An updated review on cross-correlation of m-sequences</title>
      <link>https://arxiv.org/abs/2407.16072</link>
      <description>arXiv:2407.16072v1 Announce Type: new 
Abstract: Maximum-length sequences (m-sequences for short) over finite fields are generated by linear feedback shift registers with primitive characteristic polynomials. These sequences have nice mathematical structures and good randomness properties that are favorable in practical applications. During the past five decades, the crosscorrelation between m-sequences of the same period has been intensively studied, and a particular research focus has been on investigating the cross-correlation spectra with few possibles values. In this chapter we summarize all known results on this topic in the literature and promote several open problems for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16072v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tor Helleseth, Chunlei Li</dc:creator>
    </item>
    <item>
      <title>Users Feel Guilty: Measurement of Illegal Software Installation Guide Videos on YouTube for Malware Distribution</title>
      <link>https://arxiv.org/abs/2407.16132</link>
      <description>arXiv:2407.16132v1 Announce Type: new 
Abstract: This study introduces and examines a sophisticated malware distribution technique that exploits popular video sharing platforms. In this attack, threat actors distribute malware through deceptive content that promises free versions of premium software and game cheats. Throughout this paper, we call this attack MalTube. MalTube is particularly insidious because it exploits the guilt feelings of users for engaging in potentially illegal activity, making them less likely to report the infection or ask for a help. To investigate this emerging threat, we developed video platform exploitation reconnaissance VIPER, a novel monitoring system designed to detect, monitor, and analyze MalTube activity at scale. Over a four-month data collection period, VIPER processed and analyzed 14,363 videos, 8,671 associated channels, and 1,269 unique fully qualified domain names associated with malware downloads. Our findings reveal that MalTube attackers primarily target young gamers, using the lure of free software and game cheats as infection vectors. The attackers employ various sophisticated social engineering techniques to maximize user engagement and ensure successful malware propagation. These techniques include the strategic use of platform-specific features such as trending keywords, emoticons, and eye-catching thumbnails. These tactics closely mimic legitimate content creation strategies while providing detailed instructions for malware infection. Based on our in-depth analysis, we propose a set of robust detection and mitigation strategies that exploit the invariant characteristics of MalTube videos, offering the potential for automated threat detection and prevention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16132v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rei Yamagishi, Shota Fujii, Tatsuya Mori</dc:creator>
    </item>
    <item>
      <title>Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models</title>
      <link>https://arxiv.org/abs/2407.16205</link>
      <description>arXiv:2407.16205v1 Announce Type: new 
Abstract: The rapid development of Large Language Models (LLMs) has brought remarkable generative capabilities across diverse tasks. However, despite the impressive achievements, these models still have numerous security vulnerabilities, particularly when faced with jailbreak attacks. Therefore, by investigating jailbreak attacks, we can uncover hidden weaknesses in LLMs and guide us in developing more robust defense mechanisms to fortify their security. In this paper, we further explore the boundary of jailbreak attacks on LLMs and propose Analyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes advantage of LLMs' growing analyzing and reasoning capability and reveals their underlying vulnerabilities when facing analysis-based tasks. We conduct a detailed evaluation of ABJ across various open-source and closed-source LLMs, which achieves 94.8% Attack Success Rate (ASR) and 1.06 Attack Efficiency (AE) on GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and efficiency. Our research highlights the importance of prioritizing and enhancing the safety of LLMs to mitigate the risks of misuse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16205v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shi Lin, Rongchang Li, Xun Wang, Changting Lin, Wenpeng Xing, Meng Han</dc:creator>
    </item>
    <item>
      <title>VidyaRANG: Conversational Learning Based Platform powered by Large Language Model</title>
      <link>https://arxiv.org/abs/2407.16209</link>
      <description>arXiv:2407.16209v1 Announce Type: new 
Abstract: Providing authoritative information tailored to a student's specific doubt is a hurdle in this era where search engines return an overwhelming number of article links. Large Language Models such as GPTs fail to provide answers to questions that were derived from sensitive confidential information. This information which is specific to some organisations is not available to LLMs due to privacy constraints. This is where knowledge-augmented retrieval techniques become particularly useful. The proposed platform is designed to cater to the needs of learners from divergent fields. Today, the most common format of learning is video and books, which our proposed platform allows learners to interact and ask questions. This increases learners' focus time exponentially by restricting access to pertinent content and, at the same time allowing personalized access and freedom to gain in-depth knowledge. Instructor's roles and responsibilities are significantly simplified allowing them to train a larger audience. To preserve privacy, instructors can grant course access to specific individuals, enabling personalized conversation on the provided content. This work includes an extensive spectrum of software development and product management skills, which also circumscribe knowledge of cloud computing for running Large Language Models and maintaining the application. For Frontend development, which is responsible for user interaction and user experience, Streamlit and React framework have been utilized. To improve security and privacy, the server is routed to a domain with an SSL certificate, and all the API key/s are stored securely on an AWS EC2 instance, to enhance user experience, web connectivity to an Android Studio-based mobile app has been established, and in-process to publish the app on play store, thus addressing all major software engineering disciplines</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16209v1</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chitranshu Harbola, Anupam Purwar</dc:creator>
    </item>
    <item>
      <title>Hooked: A Real-World Study on QR Code Phishing</title>
      <link>https://arxiv.org/abs/2407.16230</link>
      <description>arXiv:2407.16230v1 Announce Type: new 
Abstract: The usage of quick response (QR) codes was limited in the pre-era of the COVID-19 pandemic. Due to the widespread and frequent application since then, this opened up an attractive phishing opportunity for malicious actors. They trick users into scanning the codes and redirecting them to malicious websites. In order to explore whether phishing with QR codes is another successful attack vector, we conducted a real-world phishing campaign with two different QR code variants at a research campus. The first version was rather plain, whereas the second version was more professionally designed and included the possibility to win a voucher. After the study was completed, a qualitative survey on phishing and QR codes was conducted to verify the results of the phishing campaign. Both, the phishing campaign and the survey, show that a professional design receives more attention. They also illustrate that QR codes are used more frequently by curious users because of their easy functionality. Although the results confirm that technical-savvy users are more aware of the risks, they also underpin the malicious potential for non-technical-savvy users and suggest further work regarding countermeasures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16230v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>31. DFN-Konferenz Sicherheit in vernetzten Systemen 2024</arxiv:journal_reference>
      <dc:creator>Marvin Geisler, Daniela P\"ohn</dc:creator>
    </item>
    <item>
      <title>How to Design a Blue Team Scenario for Beginners on the Example of Brute-Force Attacks on Authentications</title>
      <link>https://arxiv.org/abs/2407.16238</link>
      <description>arXiv:2407.16238v1 Announce Type: new 
Abstract: Cyber attacks are ubiquitous and a constantly growing threat in the age of digitization. In order to protect important data, developers and system administrators must be trained and made aware of possible threats. Practical training can be used for students alike to introduce them to the topic. A constant threat to websites that require user authentication is so-called brute-force attacks, which attempt to crack a password by systematically trying every possible combination. As this is a typical threat, but comparably easy to detect, it is ideal for beginners. Therefore, three open-source blue team scenarios are designed and systematically described. They are contiguous to maximize the learning effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16238v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5220/0011667300003405</arxiv:DOI>
      <dc:creator>Andreas Eipper, Daniela P\"ohn</dc:creator>
    </item>
    <item>
      <title>Evaluation Scheme to Analyze Keystroke Dynamics Methods</title>
      <link>https://arxiv.org/abs/2407.16247</link>
      <description>arXiv:2407.16247v1 Announce Type: new 
Abstract: Password authentication is a weak point for security as passwords are easily stolen and a user may ignore the security by using a simple password. Therefore, services increasingly demand a second factor. While this may enhance security, it comes with a lower level of usability and another factor to be forgotten. A smartphone is an important device in daily life. With the growing number of sensors and features in a smartphone, keystroke dynamics may provide an easy-to-use method. In this paper, we introduce requirements for biometric authentication and keystroke dynamics. This results in an evaluation scheme, which is applied to three selected approaches. Based on the comparison, keystroke dynamics and the evaluation scheme are discussed. The obtained results indicate that keystroke dynamics can be used as another authentication method but can be bypassed by stronger adversaries. For further research, a common data set would improve the comparability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16247v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5220/0011626100003405</arxiv:DOI>
      <dc:creator>Anastasia Dimaratos, Daniela P\"ohn</dc:creator>
    </item>
    <item>
      <title>Systematically Searching for Identity-Related Information in the Internet with OSINT Tools</title>
      <link>https://arxiv.org/abs/2407.16251</link>
      <description>arXiv:2407.16251v1 Announce Type: new 
Abstract: The increase of Internet services has not only created several digital identities but also more information available about the persons behind them. The data can be collected and used for attacks on digital identities as well as on identity management systems, which manage digital identities. In order to identify possible attack vectors and take countermeasures at an early stage, it is important for individuals and organizations to systematically search for and analyze the data. This paper proposes a classification of data and open-source intelligence (OSINT) tools related to identities. This classification helps to systematically search for data. In the next step, the data can be analyzed and countermeasures can be taken. Last but not least, an OSINT framework approach applying this classification for searching and analyzing data is presented and discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16251v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5220/0011644200003405</arxiv:DOI>
      <dc:creator>Marcus Walkow, Daniela P\"ohn</dc:creator>
    </item>
    <item>
      <title>Backdoor Attacks against Hybrid Classical-Quantum Neural Networks</title>
      <link>https://arxiv.org/abs/2407.16273</link>
      <description>arXiv:2407.16273v1 Announce Type: new 
Abstract: Hybrid Quantum Neural Networks (HQNNs) represent a promising advancement in Quantum Machine Learning (QML), yet their security has been rarely explored. In this paper, we present the first systematic study of backdoor attacks on HQNNs. We begin by proposing an attack framework and providing a theoretical analysis of the generalization bounds and minimum perturbation requirements for backdoor attacks on HQNNs. Next, we employ two classic backdoor attack methods on HQNNs and Convolutional Neural Networks (CNNs) to further investigate the robustness of HQNNs. Our experimental results demonstrate that HQNNs are more robust than CNNs, requiring more significant image modifications for successful attacks. Additionally, we introduce the Qcolor backdoor, which utilizes color shifts as triggers and employs the Non-dominated Sorting Genetic Algorithm II (NSGA-II) to optimize hyperparameters. Through extensive experiments, we demonstrate the effectiveness, stealthiness, and robustness of the Qcolor backdoor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16273v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ji Guo, Wenbo Jiang, Rui Zhang, Wenshu Fan, Jiachen Li, Guoming Lu</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of AES, Blowfish, Twofish, Salsa20, and ChaCha20 for Image Encryption</title>
      <link>https://arxiv.org/abs/2407.16274</link>
      <description>arXiv:2407.16274v1 Announce Type: new 
Abstract: Nowadays, cybersecurity has grown into a more significant and difficult scientific issue. The recog-nition of threats and attacks meant for knowledge and safety on the internet is growing harder to detect. Since cybersecurity guarantees the privacy and security of data sent via the Internet, it is essential, while also providing protection against malicious attacks. Encrypt has grown into an an-swer that has become an essential element of information security systems. To ensure the security of shared data, including text, images, or videos, it is essential to employ various methods and strategies. This study delves into the prevalent cryptographic methods and algorithms utilized for prevention and stream encryption, examining their encoding techniques such as advanced encryp-tion standard (AES), Blowfish, Twofish, Salsa20, and ChaCha20. The primary objective of this re-search is to identify the optimal times and throughputs (speeds) for data encryption and decryption processes. The methodology of this study involved selecting five distinct types of images to com-pare the outcomes of the techniques evaluated in this research. The assessment focused on pro-cessing time and speed parameters, examining visual encoding and decoding using Java as the pri-mary platform. A comparative analysis of several symmetric key ciphers was performed, focusing on handling large datasets. Despite this limitation, comparing different images helped evaluate the techniques' novelty. The results showed that ChaCha20 had the best average time for both encryp-tion and decryption, being over 50% faster than some other algorithms. However, the Twofish algo-rithm had lower throughput during testing. The paper concludes with findings and suggestions for future improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16274v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.24017/science.2024.1.5</arxiv:DOI>
      <dc:creator>Rebwar Khalid Muhammed, Ribwar Rashid Aziz, Alla Ahmad Hassan, Aso Mohammed Aladdin, Shaida Jumaah Saydah, Tarik Ahmed. Rashid, Bryar Ahmad Hassan</dc:creator>
    </item>
    <item>
      <title>Manifoldchain: Maximizing Blockchain Throughput via Bandwidth-Clustered Sharding</title>
      <link>https://arxiv.org/abs/2407.16295</link>
      <description>arXiv:2407.16295v1 Announce Type: new 
Abstract: Bandwidth limitation is the major bottleneck that hinders scaling throughput of proof-of-work blockchains. To guarantee security, the mining rate of the blockchain is determined by the miners with the lowest bandwidth, resulting in an inefficient bandwidth utilization among fast miners. We propose Manifoldchain, an innovative blockchain sharding protocol that alleviates the impact of slow miners to maximize blockchain throughput. Manifoldchain utilizes a bandwidth-clustered shard formation mechanism that groups miners with similar bandwidths into the same shard. Consequently, this approach enables us to set an optimal mining rate for each shard based on its bandwidth, effectively reducing the waiting time caused by slow miners. Nevertheless, the adversary could corrupt miners with similar bandwidths, thereby concentrating hashing power and potentially creating an adversarial majority within a single shard. To counter this adversarial strategy, we introduce sharing mining, allowing the honest mining power of the entire network to participate in the secure ledger formation of each shard, thereby achieving the same level of security as an unsharded blockchain. Additionally, we introduce an asynchronous atomic commitment mechanism to ensure transaction atomicity across shards with various mining rates. Our theoretical analysis demonstrates that Manifoldchain scales linearly in throughput with the increase in shard numbers and inversely with network delay in each shard. We implement a full system prototype of Manifoldchain, comprehensively evaluated on both simulated and real-world testbeds. These experiments validate its vertical scalability with network bandwidth and horizontal scalability with network size, achieving a substantial improvement of 186% in throughput over baseline sharding protocols, for scenarios where bandwidths of miners range from 5Mbps to 60Mbps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16295v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunjiang Che, Songze Li, Xuechao Wang</dc:creator>
    </item>
    <item>
      <title>Hidden Web Caches Discovery</title>
      <link>https://arxiv.org/abs/2407.16303</link>
      <description>arXiv:2407.16303v1 Announce Type: new 
Abstract: Web caches play a crucial role in web performance and scalability. However, detecting cached responses is challenging when web servers do not reliably communicate the cache status through standardized headers. This paper presents a novel methodology for cache detection using timing analysis. Our approach eliminates the dependency on cache status headers, making it applicable to any web server. The methodology relies on sending paired requests using HTTP multiplexing functionality and makes heavy use of cache-busting to control the origin of the responses. By measuring the time it takes to receive responses from paired requests, we can determine if a response is cached or not. In each pair, one request is cache-busted to force retrieval from the origin server, while the other request is not and might be served from the cache, if present. A faster response time for the non-cache-busted request compared to the cache-busted one suggests the first one is coming from the cache. We implemented this approach in a tool and achieved an estimated accuracy of 89.6% compared to state-of-the-art methods based on cache status headers. Leveraging our cache detection approach, we conducted a large-scale experiment on the Tranco Top 50k websites. We identified a significant presence of hidden caches (5.8%) that do not advertise themselves through headers. Additionally, we employed our methodology to detect Web Cache Deception (WCD) vulnerabilities in these hidden caches. We discovered that 1.020 of them are susceptible to WCD vulnerabilities, potentially leaking sensitive data. Our findings demonstrate the effectiveness of our timing analysis methodology for cache discovery and highlight the importance of a tool that does not rely on cache-communicated cache status headers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16303v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3678890.3678931</arxiv:DOI>
      <dc:creator>Matteo Golinelli, Bruno Crispo</dc:creator>
    </item>
    <item>
      <title>Understanding Impacts of Electromagnetic Signal Injection Attacks on Object Detection</title>
      <link>https://arxiv.org/abs/2407.16327</link>
      <description>arXiv:2407.16327v1 Announce Type: new 
Abstract: Object detection can localize and identify objects in images, and it is extensively employed in critical multimedia applications such as security surveillance and autonomous driving. Despite the success of existing object detection models, they are often evaluated in ideal scenarios where captured images guarantee the accurate and complete representation of the detecting scenes. However, images captured by image sensors may be affected by different factors in real applications, including cyber-physical attacks. In particular, attackers can exploit hardware properties within the systems to inject electromagnetic interference so as to manipulate the images. Such attacks can cause noisy or incomplete information about the captured scene, leading to incorrect detection results, potentially granting attackers malicious control over critical functions of the systems. This paper presents a research work that comprehensively quantifies and analyzes the impacts of such attacks on state-of-the-art object detection models in practice. It also sheds light on the underlying reasons for the incorrect detection outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16327v1</guid>
      <category>cs.CR</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youqian Zhang, Chunxi Yang, Eugene Y. Fu, Qinhong Jiang, Chen Yan, Sze-Yiu Chau, Grace Ngai, Hong-Va Leong, Xiapu Luo, Wenyuan Xu</dc:creator>
    </item>
    <item>
      <title>Prisec II -- A Comprehensive Model for IoT Security: Cryptographic Algorithms and Cloud Integration</title>
      <link>https://arxiv.org/abs/2407.16395</link>
      <description>arXiv:2407.16395v1 Announce Type: new 
Abstract: This study addresses the critical issue of ensuring data security and efficiency in interconnected devices, especially in IoT environments. The objective is to design and implement a model using cryptographic algorithms to enhance data security in 5G networks. Challenges arise from the limited computational capabilities of IoT devices, which require the analysis and selection of cryptographic algorithms to achieve efficient data transmission. This study proposes a model that includes four levels of security, each employing different levels of encryption to provide better data security. Finally, cloud computing optimizes processing efficiency and resource utilization to improve data transmission.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16395v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IEEE Latam Transactions 2024</arxiv:journal_reference>
      <dc:creator>Pedro Costa, Valderi Leithardt</dc:creator>
    </item>
    <item>
      <title>Securing Tomorrow's Smart Cities: Investigating Software Security in Internet of Vehicles and Deep Learning Technologies</title>
      <link>https://arxiv.org/abs/2407.16410</link>
      <description>arXiv:2407.16410v1 Announce Type: new 
Abstract: Integrating Deep Learning (DL) techniques in the Internet of Vehicles (IoV) introduces many security challenges and issues that require thorough examination. This literature review delves into the inherent vulnerabilities and risks associated with DL in IoV systems, shedding light on the multifaceted nature of security threats. Through an extensive analysis of existing research, we explore potential threats posed by DL algorithms, including adversarial attacks, data privacy breaches, and model poisoning. Additionally, we investigate the impact of DL on critical aspects of IoV security, such as intrusion detection, anomaly detection, and secure communication protocols. Our review emphasizes the complexities of ensuring the robustness, reliability, and trustworthiness of DL-based IoV systems, given the dynamic and interconnected nature of vehicular networks. Furthermore, we discuss the need for novel security solutions tailored to address these challenges effectively and enhance the security posture of DL-enabled IoV environments. By offering insights into these critical issues, this chapter aims to stimulate further research, innovation, and collaboration in securing DL techniques within the context of the IoV, thereby fostering a safer and more resilient future for vehicular communication and connectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16410v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ridhi Jain, Norbert Tihanyi, Mohamed Amine Ferrag</dc:creator>
    </item>
    <item>
      <title>Side-Channel Analysis of OpenVINO-based Neural Network Models</title>
      <link>https://arxiv.org/abs/2407.16467</link>
      <description>arXiv:2407.16467v1 Announce Type: new 
Abstract: Embedded devices with neural network accelerators offer great versatility for their users, reducing the need to use cloud-based services. At the same time, they introduce new security challenges in the area of hardware attacks, the most prominent being side-channel analysis (SCA). It was shown that SCA can recover model parameters with a high accuracy, posing a threat to entities that wish to keep their models confidential. In this paper, we explore the susceptibility of quantized models implemented in OpenVINO, an embedded framework for deploying neural networks on embedded and Edge devices. We show that it is possible to recover model parameters with high precision, allowing the recovered model to perform very close to the original one. Our experiments on GoogleNet v1 show only a 1% difference in the Top 1 and a 0.64% difference in the Top 5 accuracies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16467v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dirmanto Jap, Jakub Breier, Zdenko Lehock\'y, Shivam Bhasin, Xiaolu Hou</dc:creator>
    </item>
    <item>
      <title>CrudiTEE: A Stick-and-Carrot Approach to Building Trustworthy Cryptocurrency Wallets with TEEs</title>
      <link>https://arxiv.org/abs/2407.16473</link>
      <description>arXiv:2407.16473v1 Announce Type: new 
Abstract: Cryptocurrency introduces usability challenges by requiring users to manage signing keys. Popular signing key management services (e.g., custodial wallets), however, either introduce a trusted party or burden users with managing signing key shares, posing the same usability challenges. TEEs (Trusted Execution Environments) are a promising technology to avoid both, but practical implementations of TEEs suffer from various side-channel attacks that have proven hard to eliminate.
  This paper explores a new approach to side-channel mitigation through economic incentives for TEE-based cryptocurrency wallet solutions. By taking the cost and profit of side-channel attacks into consideration, we designed a Stick-and-Carrot-based cryptocurrency wallet, CrudiTEE, that leverages penalties (the stick) and rewards (the carrot) to disincentivize attackers from exfiltrating signing keys in the first place. We model the attacker's behavior using a Markov Decision Process (MDP) to evaluate the effectiveness of the bounty and enable the service provider to adjust the parameters of the bounty's reward function accordingly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16473v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lulu Zhou, Zeyu Liu, Fan Zhang, Michael K. Reiter</dc:creator>
    </item>
    <item>
      <title>Language-Based Security for Low-Level MPC</title>
      <link>https://arxiv.org/abs/2407.16504</link>
      <description>arXiv:2407.16504v1 Announce Type: new 
Abstract: Secure Multi-Party Computation (MPC) is an important enabling technology for data privacy in modern distributed applications. Currently, proof methods for low-level MPC protocols are primarily manual and thus tedious and error-prone, and are also non-standardized and unfamiliar to most PL theorists. As a step towards better language support and language-based enforcement, we develop a new staged PL for defining a variety of low-level probabilistic MPC protocols. We also formulate a collection of confidentiality and integrity hyperproperties for our language model that are familiar from information flow, including conditional noninterference, gradual release, and robust declassification. We demonstrate their relation to standard MPC threat models of passive and malicious security, and how they can be leveraged in security verification of protocols. To prove these properties we develop automated tactics in $\mathbb{F}_2$ that can be integrated with separation logic-style reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16504v1</guid>
      <category>cs.CR</category>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Skalka, Joseph P. Near</dc:creator>
    </item>
    <item>
      <title>Exploring Automatic Cryptographic API Misuse Detection in the Era of LLMs</title>
      <link>https://arxiv.org/abs/2407.16576</link>
      <description>arXiv:2407.16576v1 Announce Type: new 
Abstract: While the automated detection of cryptographic API misuses has progressed significantly, its precision diminishes for intricate targets due to the reliance on manually defined patterns. Large Language Models (LLMs), renowned for their contextual understanding, offer a promising avenue to address existing shortcomings. However, applying LLMs in this security-critical domain presents challenges, particularly due to the unreliability stemming from LLMs' stochastic nature and the well-known issue of hallucination. To explore the prevalence of LLMs' unreliable analysis and potential solutions, this paper introduces a systematic evaluation framework to assess LLMs in detecting cryptographic misuses, utilizing a comprehensive dataset encompassing both manually-crafted samples and real-world projects. Our in-depth analysis of 11,940 LLM-generated reports highlights that the inherent instabilities in LLMs can lead to over half of the reports being false positives. Nevertheless, we demonstrate how a constrained problem scope, coupled with LLMs' self-correction capability, significantly enhances the reliability of the detection. The optimized approach achieves a remarkable detection rate of nearly 90%, surpassing traditional methods and uncovering previously unknown misuses in established benchmarks. Moreover, we identify the failure patterns that persistently hinder LLMs' reliability, including both cryptographic knowledge deficiency and code semantics misinterpretation. Guided by these insights, we develop an LLM-based workflow to examine open-source repositories, leading to the discovery of 63 real-world cryptographic misuses. Of these, 46 have been acknowledged by the development community, with 23 currently being addressed and 6 resolved. Reflecting on developers' feedback, we offer recommendations for future research and the development of LLM-based security tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16576v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yifan Xia, Zichen Xie, Peiyu Liu, Kangjie Lu, Yan Liu, Wenhai Wang, Shouling Ji</dc:creator>
    </item>
    <item>
      <title>RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent</title>
      <link>https://arxiv.org/abs/2407.16667</link>
      <description>arXiv:2407.16667v1 Announce Type: new 
Abstract: Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot. These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of threats. Among them, jailbreak attacks that induce toxic responses through jailbreak prompts have raised critical safety concerns. To identify these threats, a growing number of red teaming approaches simulate potential adversarial scenarios by crafting jailbreak prompts to test the target LLM. However, existing red teaming methods do not consider the unique vulnerabilities of LLM in different scenarios, making it difficult to adjust the jailbreak prompts to find context-specific vulnerabilities. Meanwhile, these methods are limited to refining jailbreak templates using a few mutation operations, lacking the automation and scalability to adapt to different scenarios. To enable context-aware and efficient red teaming, we abstract and model existing attacks into a coherent concept called "jailbreak strategy" and propose a multi-agent LLM system named RedAgent that leverages these strategies to generate context-aware jailbreak prompts. By self-reflecting on contextual feedback in an additional memory buffer, RedAgent continuously learns how to leverage these strategies to achieve effective jailbreaks in specific contexts. Extensive experiments demonstrate that our system can jailbreak most black-box LLMs in just five queries, improving the efficiency of existing red teaming methods by two times. Additionally, RedAgent can jailbreak customized LLM applications more efficiently. By generating context-aware jailbreak prompts towards applications on GPTs, we discover 60 severe vulnerabilities of these real-world applications with only two queries per vulnerability. We have reported all found issues and communicated with OpenAI and Meta for bug fixes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16667v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huiyu Xu, Wenhui Zhang, Zhibo Wang, Feng Xiao, Rui Zheng, Yunhe Feng, Zhongjie Ba, Kui Ren</dc:creator>
    </item>
    <item>
      <title>Aster: Fixing the Android TEE Ecosystem with Arm CCA</title>
      <link>https://arxiv.org/abs/2407.16694</link>
      <description>arXiv:2407.16694v1 Announce Type: new 
Abstract: The Android ecosystem relies on either TrustZone (e.g., OP-TEE, QTEE, Trusty) or trusted hypervisors (pKVM, Gunyah) to isolate security-sensitive services from malicious apps and Android bugs. TrustZone allows any secure world code to access the normal world that runs Android. Similarly, a trusted hypervisor has full access to Android running in one VM and security services in other VMs. In this paper, we motivate the need for mutual isolation, wherein Android, hypervisors, and the secure world are isolated from each other. Then, we propose a sandboxed service abstraction, such that a sandboxed execution cannot access any other sandbox, Android, hypervisor, or secure world memory. We present Aster which achieves these goals while ensuring that sandboxed execution can still communicate with Android to get inputs and provide outputs securely. Our main insight is to leverage the hardware isolation offered by Arm Confidential Computing Architecture (CCA). However, since CCA does not satisfy our sandboxing and mutual isolation requirements, Aster repurposes its hardware enforcement to meet its goals while addressing challenges such as secure interfaces, virtio, and protection against interrupts. We implement Aster to demonstrate its feasibility and assess its compatibility. We take three case studies, including one currently deployed on Android phones and insufficiently secured using a trusted hypervisor, to demonstrate that they can be protected by Aster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16694v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Kuhne, Supraja Sridhara, Andrin Bertschi, Nicolas Dutly, Srdjan Capkun, Shweta Shinde</dc:creator>
    </item>
    <item>
      <title>Development of Multistage Machine Learning Classifier using Decision Trees and Boosting Algorithms over Darknet Network Traffic</title>
      <link>https://arxiv.org/abs/2407.15910</link>
      <description>arXiv:2407.15910v1 Announce Type: cross 
Abstract: In recent years, the clandestine nature of darknet activities has presented an escalating challenge to cybersecurity efforts, necessitating sophisticated methods for the detection and classification of network traffic associated with these covert operations. The system addresses the significant challenge of class imbalance within Darknet traffic datasets, where malicious traffic constitutes a minority, hindering effective discrimination between normal and malicious behavior. By leveraging boosting algorithms like AdaBoost and Gradient Boosting coupled with decision trees, this study proposes a robust solution for network traffic classification. Boosting algorithms ensemble learning corrects errors iteratively and assigns higher weights to minority class instances, complemented by the hierarchical structure of decision trees. The additional Feature Selection which is a preprocessing method by utilizing Information Gain metrics, Fisher's Score, and Chi-Square test selection for features is employed. Rigorous experimentation with diverse Darknet traffic datasets validates the efficacy of the proposed multistage classifier, evaluated through various performance metrics such as accuracy, precision, recall, and F1-score, offering a comprehensive solution for accurate detection and classification of Darknet activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15910v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjali Sureshkumar Nair, Dr. Prashant Nitnaware</dc:creator>
    </item>
    <item>
      <title>Versioned Analysis of Software Quality Indicators and Self-admitted Technical Debt in Ethereum Smart Contracts with Ethstractor</title>
      <link>https://arxiv.org/abs/2407.15967</link>
      <description>arXiv:2407.15967v1 Announce Type: cross 
Abstract: The rise of decentralized applications (dApps) has made smart contracts imperative components of blockchain technology. As many smart contracts process financial transactions, their security is paramount. Moreover, the immutability of blockchains makes vulnerabilities in smart contracts particularly challenging because it requires deploying a new version of the contract at a different address, incurring substantial fees paid in Ether. This paper proposes Ethstractor, the first smart contract collection tool for gathering a dataset of versioned smart contracts. The collected dataset is then used to evaluate the reliability of code metrics as indicators of vulnerabilities in smart contracts. Our findings indicate that code metrics are ineffective in signalling the presence of vulnerabilities. Furthermore, we investigate whether vulnerabilities in newer versions of smart contracts are mitigated and identify that the number of vulnerabilities remains consistent over time. Finally, we examine the removal of self-admitted technical debt in contracts and uncover that most of the introduced debt has never been subsequently removed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15967v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khalid Hassan, Saeed Moradi, Shaiful Chowdhury, Sara Rouhani</dc:creator>
    </item>
    <item>
      <title>Representation Magnitude has a Liability to Privacy Vulnerability</title>
      <link>https://arxiv.org/abs/2407.16164</link>
      <description>arXiv:2407.16164v1 Announce Type: cross 
Abstract: The privacy-preserving approaches to machine learning (ML) models have made substantial progress in recent years. However, it is still opaque in which circumstances and conditions the model becomes privacy-vulnerable, leading to a challenge for ML models to maintain both performance and privacy. In this paper, we first explore the disparity between member and non-member data in the representation of models under common training frameworks. We identify how the representation magnitude disparity correlates with privacy vulnerability and address how this correlation impacts privacy vulnerability. Based on the observations, we propose Saturn Ring Classifier Module (SRCM), a plug-in model-level solution to mitigate membership privacy leakage. Through a confined yet effective representation space, our approach ameliorates models' privacy vulnerability while maintaining generalizability. The code of this work can be found here: \url{https://github.com/JEKimLab/AIES2024_SRCM}</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16164v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xingli Fang, Jung-Eun Kim</dc:creator>
    </item>
    <item>
      <title>Securing The Future Of Healthcare: Building A Resilient Defense System For Patient Data Protection</title>
      <link>https://arxiv.org/abs/2407.16170</link>
      <description>arXiv:2407.16170v1 Announce Type: cross 
Abstract: The increasing importance of data in the healthcare sector has led to a rise in cybercrime targeting patient information. Data breaches pose significant financial and reputational risks to many healthcare organizations including clinics and hospitals. This study aims to propose the ideal approach to developing a defense system that ensures that patient data is protected from the insidious acts of healthcare data threat actors. Using a gradientboosting classifier machine learning model, the study predicts the severity of healthcare data breaches. Secondary data was collected from the U.S. Department of Health and Human Services Portal with key indicators. Also, the study gathers key cyber-security data from Kaggle, which was utilized for the study. The findings revealed that hacking and IT incidents are the most common type of breaches in the healthcare industry, with network servers being targeted in most cases. The model evaluation showed that the gradient boosting algorithm performs well. Therefore, the study recommends that organizations implement comprehensive security protocols, particularly focusing on robust network security to protect servers</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16170v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5121/csit.2024.141303</arxiv:DOI>
      <dc:creator>Oluomachi Ejiofor, Ahmed Akinsola</dc:creator>
    </item>
    <item>
      <title>Multimodal Unlearnable Examples: Protecting Data against Multimodal Contrastive Learning</title>
      <link>https://arxiv.org/abs/2407.16307</link>
      <description>arXiv:2407.16307v1 Announce Type: cross 
Abstract: Multimodal contrastive learning (MCL) has shown remarkable advances in zero-shot classification by learning from millions of image-caption pairs crawled from the Internet. However, this reliance poses privacy risks, as hackers may unauthorizedly exploit image-text data for model training, potentially including personal and privacy-sensitive information. Recent works propose generating unlearnable examples by adding imperceptible perturbations to training images to build shortcuts for protection. However, they are designed for unimodal classification, which remains largely unexplored in MCL. We first explore this context by evaluating the performance of existing methods on image-caption pairs, and they do not generalize effectively to multimodal data and exhibit limited impact to build shortcuts due to the lack of labels and the dispersion of pairs in MCL. In this paper, we propose Multi-step Error Minimization (MEM), a novel optimization process for generating multimodal unlearnable examples. It extends the Error-Minimization (EM) framework to optimize both image noise and an additional text trigger, thereby enlarging the optimized space and effectively misleading the model to learn the shortcut between the noise features and the text trigger. Specifically, we adopt projected gradient descent to solve the noise minimization problem and use HotFlip to approximate the gradient and replace words to find the optimal text trigger. Extensive experiments demonstrate the effectiveness of MEM, with post-protection retrieval results nearly half of random guessing, and its high transferability across different models. Our code is available on the https://github.com/thinwayliu/Multimodal-Unlearnable-Examples</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16307v1</guid>
      <category>cs.MM</category>
      <category>cs.CR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinwei Liu, Xiaojun Jia, Yuan Xun, Siyuan Liang, Xiaochun Cao</dc:creator>
    </item>
    <item>
      <title>PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing</title>
      <link>https://arxiv.org/abs/2407.16318</link>
      <description>arXiv:2407.16318v1 Announce Type: cross 
Abstract: Deploying language models (LMs) necessitates outputs to be both high-quality and compliant with safety guidelines. Although Inference-Time Guardrails (ITG) offer solutions that shift model output distributions towards compliance, we find that current methods struggle in balancing safety with helpfulness. ITG Methods that safely address non-compliant queries exhibit lower helpfulness while those that prioritize helpfulness compromise on safety. We refer to this trade-off as the guardrail tax, analogous to the alignment tax. To address this, we propose PrimeGuard, a novel ITG method that utilizes structured control flow.
  PrimeGuard routes requests to different self-instantiations of the LM with varying instructions, leveraging its inherent instruction-following capabilities and in-context learning. Our tuning-free approach dynamically compiles system-designer guidelines for each query. We construct and release safe-eval, a diverse red-team safety benchmark. Extensive evaluations demonstrate that PrimeGuard, without fine-tuning, overcomes the guardrail tax by (1) significantly increasing resistance to iterative jailbreak attacks and (2) achieving state-of-the-art results in safety guardrailing while (3) matching helpfulness scores of alignment-tuned models. Extensive evaluations demonstrate that PrimeGuard, without fine-tuning, outperforms all competing baselines and overcomes the guardrail tax by improving the fraction of safe responses from 61% to 97% and increasing average helpfulness scores from 4.17 to 4.29 on the largest models, while reducing attack success rate from 100% to 8%.
  PrimeGuard implementation is available at https://github.com/dynamofl/PrimeGuard and safe-eval dataset is available at https://huggingface.co/datasets/dynamoai/safe_eval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16318v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blazej Manczak, Eliott Zemour, Eric Lin, Vaikkunth Mugunthan</dc:creator>
    </item>
    <item>
      <title>Improving the Computational Efficiency of Adaptive Audits of IRV Elections</title>
      <link>https://arxiv.org/abs/2407.16465</link>
      <description>arXiv:2407.16465v1 Announce Type: cross 
Abstract: AWAIRE is one of two extant methods for conducting risk-limiting audits of instant-runoff voting (IRV) elections. In principle AWAIRE can audit IRV contests with any number of candidates, but the original implementation incurred memory and computation costs that grew superexponentially with the number of candidates. This paper improves the algorithmic implementation of AWAIRE in three ways that make it practical to audit IRV contests with 55 candidates, compared to the previous 6 candidates. First, rather than trying from the start to rule out all candidate elimination orders that produce a different winner, the algorithm starts by considering only the final round, testing statistically whether each candidate could have won that round. For those candidates who cannot be ruled out at that stage, it expands to consider earlier and earlier rounds until either it provides strong evidence that the reported winner really won or a full hand count is conducted, revealing who really won. Second, it tests a richer collection of conditions, some of which can rule out many elimination orders at once. Third, it exploits relationships among those conditions, allowing it to abandon testing those that are unlikely to help. We provide real-world examples with up to 36 candidates and synthetic examples with up to 55 candidates, showing how audit sample size depends on the margins and on the tuning parameters. An open-source Python implementation is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16465v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Ek, Michelle Blom, Philip B. Stark, Peter J. Stuckey, Damjan Vukcevic</dc:creator>
    </item>
    <item>
      <title>Protecting Quantum Procrastinators with Signature Lifting: A Case Study in Cryptocurrencies</title>
      <link>https://arxiv.org/abs/2303.06754</link>
      <description>arXiv:2303.06754v2 Announce Type: replace 
Abstract: Current solutions to quantum vulnerabilities of widely used cryptographic schemes involve migrating users to post-quantum schemes before quantum attacks become feasible. This work deals with protecting quantum procrastinators: users that failed to migrate to post-quantum cryptography in time.
  To address this problem in the context of digital signatures, we introduce a technique called signature lifting, that allows us to lift a deployed pre-quantum signature scheme satisfying a certain property to a post-quantum signature scheme that uses the same keys. Informally, the said property is that a post-quantum one-way function is used "somewhere along the way" to derive the public-key from the secret-key. Our constructions of signature lifting relies heavily on the post-quantum digital signature scheme Picnic (Chase et al., CCS'17).
  Our main case-study is cryptocurrencies, where this property holds in two scenarios: when the public-key is generated via a key-derivation function or when the public-key hash is posted instead of the public-key itself. We propose a modification, based on signature lifting, that can be applied in many cryptocurrencies for securely spending pre-quantum coins in presence of quantum adversaries. Our construction improves upon existing constructions in two major ways: it is not limited to pre-quantum coins whose ECDSA public-key has been kept secret (and in particular, it handles all coins that are stored in addresses generated by HD wallets), and it does not require access to post-quantum coins or using side payments to pay for posting the transaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.06754v2</guid>
      <category>cs.CR</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Or Sattath, Shai Wyborski</dc:creator>
    </item>
    <item>
      <title>Unsupervised Log Anomaly Detection with Few Unique Tokens</title>
      <link>https://arxiv.org/abs/2310.08951</link>
      <description>arXiv:2310.08951v2 Announce Type: replace 
Abstract: This article introduces a method to detect anomalies in the log data generated by control system nodes at the European XFEL accelerator. The primary aim of this proposed method is to provide operators a comprehensive understanding of the availability, status, and problems specific to each node. This information is vital for ensuring the smooth operation. The sequential nature of logs and the absence of a rich text corpus that is specific to our nodes poses significant limitations for traditional and learning-based approaches for anomaly detection. To overcome this limitation, we propose a method that uses word embedding and models individual nodes as a sequence of these vectors that commonly co-occur, using a Hidden Markov Model (HMM). We score individual log entries by computing a probability ratio between the probability of the full log sequence including the new entry and the probability of just the previous log entries, without the new entry. This ratio indicates how probable the sequence becomes when the new entry is added. The proposed approach can detect anomalies by scoring and ranking log entries from European XFEL nodes where entries that receive high scores are potential anomalies that do not fit the routine of the node. This method provides a warning system to alert operators about these irregular log events that may indicate issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08951v2</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonin Sulc, Annika Eichler, Tim Wilksen</dc:creator>
    </item>
    <item>
      <title>Formal Security Analysis of the AMD SEV-SNP Software Interface</title>
      <link>https://arxiv.org/abs/2403.10296</link>
      <description>arXiv:2403.10296v3 Announce Type: replace 
Abstract: AMD Secure Encrypted Virtualization technologies enable confidential computing by protecting virtual machines from highly privileged software such as hypervisors. In this work, we develop the first, comprehensive symbolic model of the software interface of the latest SEV iteration called SEV Secure Nested Paging (SEV-SNP). Our model covers remote attestation, key derivation, page swap and live migration. We analyze the security of the software interface of SEV-SNP by verifying critical secrecy, authentication, attestation and freshness properties, and find that the platform-agnostic nature of messages exchanged between SNP guests and the AMD Secure Processor firmware presents a weakness of the design. We show multiple ways of exploiting this weakness, including the compromise of attestation report integrity, and suggest slight modifications to the design which let third parties detect guest migrations to vulnerable platforms</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10296v3</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Petar Parad\v{z}ik, Ante Derek, Marko Horvat</dc:creator>
    </item>
    <item>
      <title>The syzygy distinguisher</title>
      <link>https://arxiv.org/abs/2407.15740</link>
      <description>arXiv:2407.15740v3 Announce Type: replace 
Abstract: We present a new distinguisher for alternant and Goppa codes, whose complexity is subexponential in the error-correcting capability, hence better than that of generic decoding algorithms. Moreover it does not suffer from the strong regime limitations of the previous distinguishers or structure recovery algorithms: in particular, it applies to the codes used in the Classic McEliece candidate for postquantum cryptography standardization. The invariants that allow us to distinguish are graded Betti numbers of the homogeneous coordinate ring of a shortening of the dual code.
  Since its introduction in 1978, this is the first time an analysis of the McEliece cryptosystem breaks the exponential barrier.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15740v3</guid>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>math.AG</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugues Randriambololona</dc:creator>
    </item>
    <item>
      <title>Defending Our Privacy With Backdoors</title>
      <link>https://arxiv.org/abs/2310.08320</link>
      <description>arXiv:2310.08320v4 Announce Type: replace-cross 
Abstract: The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information, such as names and faces of individuals, from vision-language models by fine-tuning them for only a few minutes instead of re-training them from scratch. Specifically, by strategically inserting backdoors into text encoders, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's actual name. For image encoders, we map individuals' embeddings to be removed from the model to a universal, anonymous embedding. The results of our extensive experimental evaluation demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides a new "dual-use" perspective on backdoor attacks and presents a promising avenue to enhance the privacy of individuals within models trained on uncurated web-scraped data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08320v4</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting</dc:creator>
    </item>
    <item>
      <title>Differentially private projection-depth-based medians</title>
      <link>https://arxiv.org/abs/2312.07792</link>
      <description>arXiv:2312.07792v3 Announce Type: replace-cross 
Abstract: We develop $(\epsilon,\delta)$-differentially private projection-depth-based medians using the propose-test-release (PTR) and exponential mechanisms. Under general conditions on the input parameters and the population measure, (e.g. we do not assume any moment bounds), we quantify the probability the test in PTR fails, as well as the cost of privacy via finite sample deviation bounds. Next, we show that when some observations are contaminated, the private projection-depth-based median does not break down, provided its input location and scale estimators do not break down. We demonstrate our main results on the canonical projection-depth-based median, as well as on projection-depth-based medians derived from trimmed estimators. In the Gaussian setting, we show that the resulting deviation bound matches the known lower bound for private Gaussian mean estimation. In the Cauchy setting, we show that the ``outlier error amplification'' effect resulting from the heavy tails outweighs the cost of privacy. This result is then verified via numerical simulations. Additionally, we present results on general PTR mechanisms and a uniform concentration result on the projected spacings of order statistics, which may be of general interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07792v3</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelly Ramsay, Dylan Spicker</dc:creator>
    </item>
    <item>
      <title>Automated Security Response through Online Learning with Adaptive Conjectures</title>
      <link>https://arxiv.org/abs/2402.12499</link>
      <description>arXiv:2402.12499v2 Announce Type: replace-cross 
Abstract: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We present our method through an advanced persistent threat use case. Testbed evaluations show that our method produces effective security strategies that adapt to a changing environment. We also find that our method enables faster convergence than current reinforcement learning techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12499v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kim Hammar, Tao Li, Rolf Stadler, Quanyan Zhu</dc:creator>
    </item>
    <item>
      <title>Honeybee: Decentralized Peer Sampling with Verifiable Random Walks for Blockchain Data Sharding</title>
      <link>https://arxiv.org/abs/2402.16201</link>
      <description>arXiv:2402.16201v2 Announce Type: replace-cross 
Abstract: Data sharding$\unicode{x2013}$in which block data is sharded without sharding compute$\unicode{x2013}$is at the present the favored approach for scaling Ethereum and other popular blockchains. A key challenge toward implementing data sharding is verifying whether the entirety of a block's data is available in the network (across its shards). A central technique proposed to conduct this verification uses erasure-coded blocks and is called data availability sampling (DAS). While the high-level protocol details of DAS have been well discussed in the community, discussions around how such a protocol will be implemented at the peer-to-peer layer are lacking. We identify random sampling of nodes as a fundamental primitive necessary to carry out DAS and present Honeybee, a decentralized algorithm for sampling nodes that uses verifiable random walks. Honeybee is secure against attacks even in the presence of a large number of Byzantine nodes (e.g., 50% of the network). We evaluate Honeybee through experiments and show that the quality of sampling achieved by Honeybee is significantly better compared to the state-of-the-art. Our proposed algorithm has implications for DAS functions in both full nodes and light nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16201v2</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunqi Zhang, Shaileshh Bojja Venkatakrishnan</dc:creator>
    </item>
  </channel>
</rss>
