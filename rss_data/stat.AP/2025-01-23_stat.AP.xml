<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jan 2025 02:39:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The BAD Paradox: A Critical Assessment of the Belin/Ambr\'osio Deviation Model</title>
      <link>https://arxiv.org/abs/2501.12531</link>
      <description>arXiv:2501.12531v1 Announce Type: new 
Abstract: The Belin/Ambr\'osio Deviation (BAD) model is a widely used diagnostic tool for detecting keratoconus and corneal ectasia. The input to the model is a set of z-score normalized $D$ indices that represent physical characteristics of the cornea. Paradoxically, the output of the model, Total Deviation Value ($D_{\text{final}}$), is reported in standard deviations from the mean, but $D_{\text{final}}$ does not behave like a z-score normalized value. Although thresholds like $D_{\text{final}} \ge 1.6$ for "suspicious" and $D_{\text{final}} \ge 3.0$ for "abnormal" are commonly cited, there is little explanation on how to interpret values outside of those thresholds or to understand how they relate to physical characteristics of the cornea. This study explores the reasons for $D_{\text{final}}$'s apparent inconsistency through a meta-analysis of published data and a more detailed statistical analysis of over 1,600 Pentacam exams. The results reveal that systematic bias in the BAD regression model, multicollinearity among predictors, and inconsistencies in normative datasets contribute to the non-zero mean of $D_{\text{final}}$, complicating its clinical interpretation. These findings highlight critical limitations in the model's design and underscore the need for recalibration to enhance its transparency and diagnostic reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12531v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Sielinski</dc:creator>
    </item>
    <item>
      <title>Interpolation pour l'augmentation de donnees : Application \`a la gestion des adventices de la canne a sucre a la Reunion</title>
      <link>https://arxiv.org/abs/2501.12400</link>
      <description>arXiv:2501.12400v1 Announce Type: cross 
Abstract: Data augmentation is a crucial step in the development of robust supervised learning models, especially when dealing with limited datasets. This study explores interpolation techniques for the augmentation of geo-referenced data, with the aim of predicting the presence of Commelina benghalensis L. in sugarcane plots in La R\'eunion. Given the spatial nature of the data and the high cost of data collection, we evaluated two interpolation approaches: Gaussian processes (GPs) with different kernels and kriging with various variograms. The objectives of this work are threefold: (i) to identify which interpolation methods offer the best predictive performance for various regression algorithms, (ii) to analyze the evolution of performance as a function of the number of observations added, and (iii) to assess the spatial consistency of augmented datasets. The results show that GP-based methods, in particular with combined kernels (GP-COMB), significantly improve the performance of regression algorithms while requiring less additional data. Although kriging shows slightly lower performance, it is distinguished by a more homogeneous spatial coverage, a potential advantage in certain contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12400v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederick Fabre Ferber, Dominique Gay, Jean-Christophe Soulie, Jean Diatta, Odalric-Ambrym Maillard</dc:creator>
    </item>
    <item>
      <title>On the two-step hybrid design for augmenting randomized trials using real-world data</title>
      <link>https://arxiv.org/abs/2501.12453</link>
      <description>arXiv:2501.12453v1 Announce Type: cross 
Abstract: Hybrid clinical trials, that borrow real-world data (RWD), are gaining interest, especially for rare diseases. They assume RWD and randomized control arm be exchangeable, but violations can bias results, inflate type I error, or reduce power. A two-step hybrid design first tests exchangeability, reducing inappropriate borrowing but potentially inflating type I error (Yuan et al., 2019). We propose four methods to better control type I error. Approach 1 estimates the variance of test statistics, rejecting the null hypothesis based on large sample normal approximation. Approach 2 uses a numerical approach for exact critical value determination. Approach 3 splits type I error rates by equivalence test outcome. Approach 4 adjusts the critical value only when equivalence is established. Simulation studies using a hypothetical ALS scenario, evaluate type I error and power under various conditions, compared to the Bayesian power prior approach (Ibrahim et al., 2015). Our methods and the Bayesian power prior control type I error, whereas Yuan et al. (2019) increases it under exchangeability. If exchangeability doesn't hold, all methods fail to control type I error. Our methods show type I error inflation of 6%-8%, compared to 10% for Yuan et al. (2019) and 16% for the Bayesian power prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12453v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiapeng Xu, Ruben P. A. van Eijk, Alicia Ellis, Tianyu Pan, Lorene M. Nelson, Kit C. B. Roes, Marc van Dijk, Maria Sarno, Leonard H. van den Berg, Lu Tian, Ying Lu</dc:creator>
    </item>
    <item>
      <title>Outcome-Assisted Multiple Imputation of Missing Treatments</title>
      <link>https://arxiv.org/abs/2501.12471</link>
      <description>arXiv:2501.12471v1 Announce Type: cross 
Abstract: We provide guidance on multiple imputation of missing at random treatments in observational studies. Specifically, analysts should account for both covariates and outcomes, i.e., not just use propensity scores, when imputing the missing treatments. To do so, we develop outcome-assisted multiple imputation of missing treatments: the analyst fits a regression for the outcome on the treatment indicator and covariates, which is used to sharpen the predictive probabilities for missing treatments under an estimated propensity score model. We derive an expression for the bias of the inverse probability weighted estimator for the average treatment effect under multiple imputation of missing treatments, and we show theoretically that this bias can be made small by using outcome-assisted multiple imputation. Simulations demonstrate empirically that outcome-assisted multiple imputation can offer better inferential properties than using the treatment assignment model alone. We illustrate the procedure in an analysis of data from the National Longitudinal Survey of Youth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12471v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Feldman, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Adapting OpenAI's CLIP Model for Few-Shot Image Inspection in Manufacturing Quality Control: An Expository Case Study with Multiple Application Examples</title>
      <link>https://arxiv.org/abs/2501.12596</link>
      <description>arXiv:2501.12596v1 Announce Type: cross 
Abstract: This expository paper introduces a simplified approach to image-based quality inspection in manufacturing using OpenAI's CLIP (Contrastive Language-Image Pretraining) model adapted for few-shot learning. While CLIP has demonstrated impressive capabilities in general computer vision tasks, its direct application to manufacturing inspection presents challenges due to the domain gap between its training data and industrial applications. We evaluate CLIP's effectiveness through five case studies: metallic pan surface inspection, 3D printing extrusion profile analysis, stochastic textured surface evaluation, automotive assembly inspection, and microstructure image classification. Our results show that CLIP can achieve high classification accuracy with relatively small learning sets (50-100 examples per class) for single-component and texture-based applications. However, the performance degrades with complex multi-component scenes. We provide a practical implementation framework that enables quality engineers to quickly assess CLIP's suitability for their specific applications before pursuing more complex solutions. This work establishes CLIP-based few-shot learning as an effective baseline approach that balances implementation simplicity with robust performance, demonstrated in several manufacturing quality control applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12596v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fadel M. Megahed, Ying-Ju Chen, Bianca Maria Colosimo, Marco Luigi Giuseppe Grasso, L. Allison Jones-Farmer, Sven Knoth, Hongyue Sun, Inez Zwetsloot</dc:creator>
    </item>
    <item>
      <title>Maximizing Predictive Performance for Small Subgroups: Functionally Adaptive Interaction Regularization (FAIR)</title>
      <link>https://arxiv.org/abs/2412.20190</link>
      <description>arXiv:2412.20190v2 Announce Type: replace 
Abstract: In many healthcare settings, it is both critical to consider fairness when building analytical applications but also uniquely unacceptable to lower model performance for one group to match that of another (e.g. fairness cannot be achieved by lowering the diagnostic ability of a model for one group to match that of another and lose overall diagnostic power). Therefore a modeler needs to maximize model performance across groups as much as possible, often while maintaining a model's interpretability, which is a challenge for a number of reasons. In this paper we therefore suggest a new modeling framework, FAIR, to maximize performance across imbalanced groups, based on existing linear regression approaches already commonly used in healthcare settings. We propose a full linear interaction model between groups and all other covariates, paired with a weighting of samples by group size and independent regularization penalties for each group. This efficient approach overcomes many of the limitations in current approaches and manages to balance learning from other groups with tailoring prediction to the small focal group(s). FAIR has an added advantage in that it still allows for model interpretability in research and clinical settings. We demonstrate its usefulness with numerical and health data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20190v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Smolyak, Courtney Paulson, Margr\'et V. Bjarnad\'ottir</dc:creator>
    </item>
    <item>
      <title>Slamming the sham: A Bayesian model for adaptive adjustment with noisy control data</title>
      <link>https://arxiv.org/abs/1905.09693</link>
      <description>arXiv:1905.09693v3 Announce Type: replace-cross 
Abstract: It is not always clear how to adjust for control data in causal inference, balancing the goals of reducing bias and variance. We show how, in a setting with repeated experiments, Bayesian hierarchical modeling yields an adaptive procedure that uses the data to determine how much adjustment to perform. The result is a novel analysis with increased statistical efficiency compared to the default analysis based on difference estimates. We demonstrate this procedure on two real examples, as well as on a series of simulated datasets. We show that the increased efficiency can have real-world consequences in terms of the conclusions that can be drawn from the experiments. We also discuss the relevance of this work to causal inference and statistical design and analysis more generally.</description>
      <guid isPermaLink="false">oai:arXiv.org:1905.09693v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Gelman, Matthijs V\'ak\'ar</dc:creator>
    </item>
    <item>
      <title>Credit scoring using neural networks and SURE posterior probability calibration</title>
      <link>https://arxiv.org/abs/2107.07206</link>
      <description>arXiv:2107.07206v2 Announce Type: replace-cross 
Abstract: In this article we compare the performances of a logistic regression and a feed forward neural network for credit scoring purposes. Our results show that the logistic regression gives quite good results on the dataset and the neural network can improve a little the performance. We also consider different sets of features in order to assess their importance in terms of prediction accuracy. We found that temporal features (i.e. repeated measures over time) can be an important source of information resulting in an increase in the overall model accuracy. Finally, we introduce a new technique for the calibration of predicted probabilities based on Stein's unbiased risk estimate (SURE). This calibration technique can be applied to very general calibration functions. In particular, we detail this method for the sigmoid function as well as for the Kumaraswamy function, which includes the identity as a particular case. We show that stacking the SURE calibration technique with the classical Platt method can improve the calibration of predicted probabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.07206v2</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthieu Garcin, Samuel Stephan</dc:creator>
    </item>
    <item>
      <title>The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective</title>
      <link>https://arxiv.org/abs/2312.15524</link>
      <description>arXiv:2312.15524v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown impressive potential to simulate human behavior. We identify a fundamental challenge in using them to simulate experiments: when LLM-simulated subjects are blind to the experimental design (as is standard practice with human subjects), variations in treatment systematically affect unspecified variables that should remain constant, violating the unconfoundedness assumption. Using demand estimation as a context and an actual experiment as a benchmark, we show this can lead to implausible results. While confounding may in principle be addressed by controlling for covariates, this can compromise ecological validity in the context of LLM simulations: controlled covariates become artificially salient in the simulated decision process, which introduces focalism. This trade-off between unconfoundedness and ecological validity is usually absent in traditional experimental design and represents a unique challenge in LLM simulations. We formalize this challenge theoretically, showing it stems from ambiguous prompting strategies, and hence cannot be fully addressed by improving training data or by fine-tuning. Alternative approaches that unblind the experimental design to the LLM show promise. Our findings suggest that effectively leveraging LLMs for experimental simulations requires fundamentally rethinking established experimental design practices rather than simply adapting protocols developed for human subjects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15524v2</guid>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2139/ssrn.4650172</arxiv:DOI>
      <dc:creator>George Gui, Olivier Toubia</dc:creator>
    </item>
    <item>
      <title>On the Role of Surrogates in Conformal Inference of Individual Causal Effects</title>
      <link>https://arxiv.org/abs/2412.12365</link>
      <description>arXiv:2412.12365v2 Announce Type: replace-cross 
Abstract: Learning the Individual Treatment Effect (ITE) is essential for personalized decision-making, yet causal inference has traditionally focused on aggregated treatment effects. While integrating conformal prediction with causal inference can provide valid uncertainty quantification for ITEs, the resulting prediction intervals are often excessively wide, limiting their practical utility. To address this limitation, we introduce \underline{S}urrogate-assisted \underline{C}onformal \underline{I}nference for \underline{E}fficient I\underline{N}dividual \underline{C}ausal \underline{E}ffects (SCIENCE), a framework designed to construct more efficient prediction intervals for ITEs. SCIENCE accommodates the covariate shifts between source data and target data and applies to various data configurations, including semi-supervised and surrogate-assisted semi-supervised learning. Leveraging semi-parametric efficiency theory, SCIENCE produces rate double-robust prediction intervals under mild rate convergence conditions, permitting the use of flexible non-parametric models to estimate nuisance functions. We quantify efficiency gains by comparing semi-parametric efficiency bounds with and without the surrogates. Simulation studies demonstrate that our surrogate-assisted intervals offer substantial efficiency improvements over existing methods while maintaining valid group-conditional coverage. Applied to the phase 3 Moderna COVE COVID-19 vaccine trial, SCIENCE illustrates how multiple surrogate markers can be leveraged to generate more efficient prediction intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12365v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chenyin Gao, Peter B. Gilbert, Larry Han</dc:creator>
    </item>
    <item>
      <title>Minimax Optimal Simple Regret in Two-Armed Best-Arm Identification</title>
      <link>https://arxiv.org/abs/2412.17753</link>
      <description>arXiv:2412.17753v2 Announce Type: replace-cross 
Abstract: This study investigates an asymptotically minimax optimal algorithm in the two-armed fixed-budget best-arm identification (BAI) problem. Given two treatment arms, the objective is to identify the arm with the highest expected outcome through an adaptive experiment. We focus on the Neyman allocation, where treatment arms are allocated following the ratio of their outcome standard deviations. Our primary contribution is to prove the minimax optimality of the Neyman allocation for the simple regret, defined as the difference between the expected outcomes of the true best arm and the estimated best arm. Specifically, we first derive a minimax lower bound for the expected simple regret, which characterizes the worst-case performance achievable under the location-shift distributions, including Gaussian distributions. We then show that the simple regret of the Neyman allocation asymptotically matches this lower bound, including the constant term, not just the rate in terms of the sample size, under the worst-case distribution. Notably, our optimality result holds without imposing locality restrictions on the distribution, such as the local asymptotic normality. Furthermore, we demonstrate that the Neyman allocation reduces to the uniform allocation, i.e., the standard randomized controlled trial, under Bernoulli distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17753v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Cheap Subsampling bootstrap confidence intervals for fast and robust inference</title>
      <link>https://arxiv.org/abs/2501.10289</link>
      <description>arXiv:2501.10289v2 Announce Type: replace-cross 
Abstract: Bootstrapping is often applied to get confidence limits for semiparametric inference of a target parameter in the presence of nuisance parameters. Bootstrapping with replacement can be computationally expensive and problematic when cross-validation is used in the estimation algorithm due to duplicate observations in the bootstrap samples. We provide a valid, fast, easy-to-implement subsampling bootstrap method for constructing confidence intervals for asymptotically linear estimators and discuss its application to semiparametric causal inference. Our method, inspired by the Cheap Bootstrap (Lam, 2022), leverages the quantiles of a t-distribution and has the desired coverage with few bootstrap replications. We show that the method is asymptotically valid if the subsample size is chosen appropriately as a function of the sample size. We illustrate our method with data from the LEADER trial (Marso et al., 2016), obtaining confidence intervals for a longitudinal targeted minimum loss-based estimator (van der Laan and Gruber, 2012). Through a series of empirical experiments, we also explore the impact of subsample size, sample size, and the number of bootstrap repetitions on the performance of the confidence interval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10289v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johan Sebastian Ohlendorff, Anders Munch, Kathrine Kold S{\o}rensen, Thomas Alexander Gerds</dc:creator>
    </item>
  </channel>
</rss>
