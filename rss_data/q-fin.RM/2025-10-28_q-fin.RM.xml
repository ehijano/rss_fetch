<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.RM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.RM</link>
    <description>q-fin.RM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.RM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Oct 2025 01:58:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Right Place, Right Time: Market Simulation-based RL for Execution Optimisation</title>
      <link>https://arxiv.org/abs/2510.22206</link>
      <description>arXiv:2510.22206v1 Announce Type: cross 
Abstract: Execution algorithms are vital to modern trading, they enable market participants to execute large orders while minimising market impact and transaction costs. As these algorithms grow more sophisticated, optimising them becomes increasingly challenging. In this work, we present a reinforcement learning (RL) framework for discovering optimal execution strategies, evaluated within a reactive agent-based market simulator. This simulator creates reactive order flow and allows us to decompose slippage into its constituent components: market impact and execution risk. We assess the RL agent's performance using the efficient frontier based on work by Almgren and Chriss, measuring its ability to balance risk and cost. Results show that the RL-derived strategies consistently outperform baselines and operate near the efficient frontier, demonstrating a strong ability to optimise for risk and impact. These findings highlight the potential of reinforcement learning as a powerful tool in the trader's toolkit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22206v1</guid>
      <category>q-fin.CP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-fin.RM</category>
      <category>q-fin.TR</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ollie Olby, Andreea Bacalum, Rory Baggott, Namid Stillman</dc:creator>
    </item>
    <item>
      <title>Revisiting the Structure of Trend Premia: When Diversification Hides Redundancy</title>
      <link>https://arxiv.org/abs/2510.23150</link>
      <description>arXiv:2510.23150v2 Announce Type: cross 
Abstract: Recent work has emphasized the diversification benefits of combining trend signals across multiple horizons, with the medium-term window-typically six months to one year-long viewed as the "sweet spot" of trend-following. This paper revisits this conventional view by reallocating exposure dynamically across horizons using a Bayesian optimization framework designed to learn the optimal weights assigned to each trend horizon at the asset level. The common practice of equal weighting implicitly assumes that all assets benefit equally from all horizons; we show that this assumption is both theoretically and empirically suboptimal. We first optimize the horizon-level weights at the asset level to maximize the informativeness of trend signals before applying Bayesian graphical models-with sparsity and turnover control-to allocate dynamically across assets. The key finding is that the medium-term band contributes little incremental performance or diversification once short- and long-term components are included. Removing the 125-day layer improves Sharpe ratios and drawdown efficiency while maintaining benchmark correlation. We then rationalize this outcome through a minimum-variance formulation, showing that the medium-term horizon largely overlaps with its neighboring horizons. The resulting "barbell" structure-combining short- and long-term trends-captures most of the performance while reducing model complexity. This result challenges the common belief that more horizons always improve diversification and suggests that some forms of time-scale diversification may conceal unnecessary redundancy in trend premia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23150v2</guid>
      <category>q-fin.PR</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>q-fin.TR</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alban Etienne, Jean-Jacques Ohana, Eric Benhamou, B\'eatrice Guez, Ethan Setrouk, Thomas Jacquot</dc:creator>
    </item>
    <item>
      <title>Temporal Relational Reasoning of Large Language Models for Detecting Stock Portfolio Crashes</title>
      <link>https://arxiv.org/abs/2410.17266</link>
      <description>arXiv:2410.17266v2 Announce Type: replace 
Abstract: Stock portfolios are often exposed to rare consequential events (e.g., 2007 global financial crisis, 2020 COVID-19 stock market crash), as they do not have enough historical information to learn from. Large Language Models (LLMs) now present a possible tool to tackle this problem, as they can generalize across their large corpus of training data and perform zero-shot reasoning on new events, allowing them to detect possible portfolio crash events without requiring specific training data. However, detecting portfolio crashes is a complex problem that requires more than reasoning abilities. Investors need to dynamically process the impact of each new piece of information found in news articles, analyze the relational network of impacts across different events and portfolio stocks, as well as understand the temporal context between impacts across time-steps, in order to obtain the aggregated impact on the target portfolio. In this work, we propose an algorithmic framework named Temporal Relational Reasoning (TRR). It seeks to emulate the spectrum of human cognitive capabilities used for complex problem-solving, which include brainstorming, memory, attention and reasoning. Through extensive experiments, we show that TRR is able to outperform state-of-the-art techniques on detecting stock portfolio crashes, and demonstrate how each of the proposed components help to contribute to its performance through an ablation study. Additionally, we further explore the possible applications of TRR by extending it to other related complex problems, such as the detection of possible global crisis events in Macroeconomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17266v2</guid>
      <category>q-fin.RM</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>q-fin.CP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelvin J. L. Koa, Yunshan Ma, Yi Xu, Ritchie Ng, Huanhuan Zheng, Tat-Seng Chua</dc:creator>
    </item>
    <item>
      <title>Approaches for modelling the term-structure of default risk under IFRS 9: A tutorial using discrete-time survival analysis</title>
      <link>https://arxiv.org/abs/2507.15441</link>
      <description>arXiv:2507.15441v2 Announce Type: replace 
Abstract: Under the International Financial Reporting Standards (IFRS) 9, credit losses ought to be recognised timeously and accurately. This requirement belies a certain degree of dynamicity when estimating the constituent parts of a credit loss event, most notably the probability of default (PD). It is notoriously difficult to produce such PD-estimates at every point of loan life that are adequately dynamic and accurate, especially when considering the ever-changing macroeconomic background. In rendering these lifetime PD-estimates, the choice of modelling technique plays an important role, which is why we first review a few classes of techniques, including the merits and limitations of each. Our main contribution however is the development of an in-depth and data-driven tutorial using a particular class of techniques called discrete-time survival analysis. This tutorial is accompanied by a diverse set of reusable diagnostic measures for evaluating various aspects of a survival model and the underlying data. A comprehensive R-based codebase is further contributed. We believe that our work can help cultivate common modelling practices under IFRS 9, and should be valuable to practitioners, model validators, and regulators alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15441v2</guid>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Botha, Tanja Verster</dc:creator>
    </item>
    <item>
      <title>Compensation-based risk-sharing</title>
      <link>https://arxiv.org/abs/2510.19511</link>
      <description>arXiv:2510.19511v2 Announce Type: replace 
Abstract: This paper studies the mathematical problem of allocating payouts (compensations) in an endowment contingency fund using a risk-sharing rule that satisfies full allocation. Besides the participants, an administrator manages the fund by collecting ex-ante contributions to establish the fund and distributing ex-post payouts to members. Two types of administrators are considered. An 'active' administrator both invests in the fund and receives the payout of the fund when no participant receives a payout. A 'passive' administrator performs only administrative tasks and neither invests in nor receives a payout from the fund. We analyze the actuarial fairness of both compensation-based risk-sharing schemes and provide general conditions under which fairness is achieved. The results extend earlier work by Denuit and Robert (2025) and Dhaene and Milevsky (2024), who focused on payouts based on Bernoulli distributions, by allowing for general non-negative loss distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19511v2</guid>
      <category>q-fin.RM</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Dhaene, Atibhav Chaudhry, Ka Chun Cheung, Austin Riis-Due</dc:creator>
    </item>
    <item>
      <title>The Boosted Difference of Convex Functions Algorithm for Value-at-Risk Constrained Portfolio Optimization</title>
      <link>https://arxiv.org/abs/2402.09194</link>
      <description>arXiv:2402.09194v2 Announce Type: replace-cross 
Abstract: A highly relevant problem of modern finance is the design of Value-at-Risk (VaR) optimal portfolios. Due to contemporary financial regulations, banks and other financial institutions are tied to use the risk measure to control their credit, market, and operational risks. Despite its practical relevance, the non-convexity induced by VaR constraints in portfolio optimization problems remains a major challenge. To address this complexity more effectively, this paper proposes the use of the Boosted Difference-of-Convex Functions Algorithm (BDCA) to approximately solve a Markowitz-style portfolio selection problem with a VaR constraint. As one of the key contributions, we derive a novel line search framework that allows the application of the algorithm to Difference-of-Convex functions (DC) programs where both components are non-smooth. Moreover, we prove that the BDCA linearly converges to a Karush-Kuhn-Tucker point for the problem at hand using the Kurdyka-Lojasiewicz property. We also outline that this result can be generalized to a broader class of piecewise-linear DC programs with linear equality and inequality constraints. In the practical part, extensive numerical experiments under consideration of best practices then demonstrate the robustness of the BDCA under challenging constraint settings and adverse initialization. In particular, the algorithm consistently identifies the highest number of feasible solutions even under the most challenging conditions, while other approaches from chance-constrained programming lead to a complete failure in these settings. Due to the open availability of all data sets and code, this paper further provides a practical guide for transparent and easily reproducible comparisons of VaR-constrained portfolio selection problems in Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09194v2</guid>
      <category>math.OC</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marah-Lisanne Thormann, Phan Tu Vuong, Alain B. Zemkoho</dc:creator>
    </item>
  </channel>
</rss>
