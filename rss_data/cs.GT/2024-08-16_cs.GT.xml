<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Aug 2024 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 16 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Decentralized Fair Division</title>
      <link>https://arxiv.org/abs/2408.07821</link>
      <description>arXiv:2408.07821v1 Announce Type: new 
Abstract: Fair division is typically framed from a centralized perspective. We study a decentralized variant of fair division inspired by the dynamics observed in community-based targeting, mutual aid networks, and community resource management paradigms. We develop an approach for decentralized fair division and compare it with a centralized approach with respect to fairness and social welfare guarantees. In the context of the existing literature, our decentralized model can be viewed as a relaxation of previous models of sequential exchange in light of impossibility results concerning the inability of those models to achieve desirable outcomes. We find that in settings representative of many real world situations, the two models of resource allocation offer contrasting fairness and social welfare guarantees. In particular, we show that under appropriate conditions, our model of decentralized allocation can ensure high-quality allocative decisions in an efficient fashion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07821v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joel Miller, Rishi Advani, Ian Kash, Chris Kanich, Lenore Zuck</dc:creator>
    </item>
    <item>
      <title>Is Knowledge Power? On the (Im)possibility of Learning from Strategic Interaction</title>
      <link>https://arxiv.org/abs/2408.08272</link>
      <description>arXiv:2408.08272v1 Announce Type: new 
Abstract: When learning in strategic environments, a key question is whether agents can overcome uncertainty about their preferences to achieve outcomes they could have achieved absent any uncertainty. Can they do this solely through interactions with each other? We focus this question on the ability of agents to attain the value of their Stackelberg optimal strategy and study the impact of information asymmetry. We study repeated interactions in fully strategic environments where players' actions are decided based on learning algorithms that take into account their observed histories and knowledge of the game. We study the pure Nash equilibria (PNE) of a meta-game where players choose these algorithms as their actions. We demonstrate that if one player has perfect knowledge about the game, then any initial informational gap persists. That is, while there is always a PNE in which the informed agent achieves her Stackelberg value, there is a game where no PNE of the meta-game allows the partially informed player to achieve her Stackelberg value. On the other hand, if both players start with some uncertainty about the game, the quality of information alone does not determine which agent can achieve her Stackelberg value. In this case, the concept of information asymmetry becomes nuanced and depends on the game's structure. Overall, our findings suggest that repeated strategic interactions alone cannot facilitate learning effectively enough to earn an uninformed player her Stackelberg value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08272v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nivasini Ananthakrishnan, Nika Haghtalab, Chara Podimata, Kunhe Yang</dc:creator>
    </item>
    <item>
      <title>Capturing the Complexity of Human Strategic Decision-Making with Machine Learning</title>
      <link>https://arxiv.org/abs/2408.07865</link>
      <description>arXiv:2408.07865v1 Announce Type: cross 
Abstract: Understanding how people behave in strategic settings--where they make decisions based on their expectations about the behavior of others--is a long-standing problem in the behavioral sciences. We conduct the largest study to date of strategic decision-making in the context of initial play in two-player matrix games, analyzing over 90,000 human decisions across more than 2,400 procedurally generated games that span a much wider space than previous datasets. We show that a deep neural network trained on these data predicts people's choices better than leading theories of strategic behavior, indicating that there is systematic variation that is not explained by those theories. We then modify the network to produce a new, interpretable behavioral model, revealing what the original network learned about people: their ability to optimally respond and their capacity to reason about others are dependent on the complexity of individual games. This context-dependence is critical in explaining deviations from the rational Nash equilibrium, response times, and uncertainty in strategic decisions. More broadly, our results demonstrate how machine learning can be applied beyond prediction to further help generate novel explanations of complex human behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07865v1</guid>
      <category>econ.GN</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jian-Qiao Zhu, Joshua C. Peterson, Benjamin Enke, Thomas L. Griffiths</dc:creator>
    </item>
    <item>
      <title>Independent Policy Mirror Descent for Markov Potential Games: Scaling to Large Number of Players</title>
      <link>https://arxiv.org/abs/2408.08075</link>
      <description>arXiv:2408.08075v1 Announce Type: cross 
Abstract: Markov Potential Games (MPGs) form an important sub-class of Markov games, which are a common framework to model multi-agent reinforcement learning problems. In particular, MPGs include as a special case the identical-interest setting where all the agents share the same reward function. Scaling the performance of Nash equilibrium learning algorithms to a large number of agents is crucial for multi-agent systems. To address this important challenge, we focus on the independent learning setting where agents can only have access to their local information to update their own policy. In prior work on MPGs, the iteration complexity for obtaining $\epsilon$-Nash regret scales linearly with the number of agents $N$. In this work, we investigate the iteration complexity of an independent policy mirror descent (PMD) algorithm for MPGs. We show that PMD with KL regularization, also known as natural policy gradient, enjoys a better $\sqrt{N}$ dependence on the number of agents, improving over PMD with Euclidean regularization and prior work. Furthermore, the iteration complexity is also independent of the sizes of the agents' action spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08075v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>CDC 2024 - Proceedings of the 63rd IEEE Conference on Decision and Control</arxiv:journal_reference>
      <dc:creator>Pragnya Alatur, Anas Barakat, Niao He</dc:creator>
    </item>
    <item>
      <title>Stochastic Semi-Gradient Descent for Learning Mean Field Games with Population-Aware Function Approximation</title>
      <link>https://arxiv.org/abs/2408.08192</link>
      <description>arXiv:2408.08192v1 Announce Type: cross 
Abstract: Mean field games (MFGs) model the interactions within a large-population multi-agent system using the population distribution. Traditional learning methods for MFGs are based on fixed-point iteration (FPI), which calculates best responses and induced population distribution separately and sequentially. However, FPI-type methods suffer from inefficiency and instability, due to oscillations caused by the forward-backward procedure. This paper considers an online learning method for MFGs, where an agent updates its policy and population estimates simultaneously and fully asynchronously, resulting in a simple stochastic gradient descent (SGD) type method called SemiSGD. Not only does SemiSGD exhibit numerical stability and efficiency, but it also provides a novel perspective by treating the value function and population distribution as a unified parameter. We theoretically show that SemiSGD directs this unified parameter along a descent direction to the mean field equilibrium. Motivated by this perspective, we develop a linear function approximation (LFA) for both the value function and the population distribution, resulting in the first population-aware LFA for MFGs on continuous state-action space. Finite-time convergence and approximation error analysis are provided for SemiSGD equipped with population-aware LFA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08192v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyu Zhang, Xu Chen, Xuan Di</dc:creator>
    </item>
    <item>
      <title>Federated Fairness Analytics: Quantifying Fairness in Federated Learning</title>
      <link>https://arxiv.org/abs/2408.08214</link>
      <description>arXiv:2408.08214v1 Announce Type: cross 
Abstract: Federated Learning (FL) is a privacy-enhancing technology for distributed ML. By training models locally and aggregating updates - a federation learns together, while bypassing centralised data collection. FL is increasingly popular in healthcare, finance and personal computing. However, it inherits fairness challenges from classical ML and introduces new ones, resulting from differences in data quality, client participation, communication constraints, aggregation methods and underlying hardware. Fairness remains an unresolved issue in FL and the community has identified an absence of succinct definitions and metrics to quantify fairness; to address this, we propose Federated Fairness Analytics - a methodology for measuring fairness. Our definition of fairness comprises four notions with novel, corresponding metrics. They are symptomatically defined and leverage techniques originating from XAI, cooperative game-theory and networking engineering. We tested a range of experimental settings, varying the FL approach, ML task and data settings. The results show that statistical heterogeneity and client participation affect fairness and fairness conscious approaches such as Ditto and q-FedAvg marginally improve fairness-performance trade-offs. Using our techniques, FL practitioners can uncover previously unobtainable insights into their system's fairness, at differing levels of granularity in order to address fairness challenges in FL. We have open-sourced our work at: https://github.com/oscardilley/federated-fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08214v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oscar Dilley, Juan Marcelo Parra-Ullauri, Rasheed Hussain, Dimitra Simeonidou</dc:creator>
    </item>
    <item>
      <title>Robust Online Selection with Uncertain Offer Acceptance</title>
      <link>https://arxiv.org/abs/2112.00842</link>
      <description>arXiv:2112.00842v2 Announce Type: replace 
Abstract: Online advertising has motivated interest in online selection problems. Displaying ads to the right users benefits both the platform (e.g., via pay-per-click) and the advertisers (by increasing their reach). In practice, not all users click on displayed ads, while the platform's algorithm may miss the users most disposed to do so. This mismatch decreases the platform's revenue and the advertiser's chances to reach the right customers. With this motivation, we propose a secretary problem where a candidate may or may not accept an offer according to a known probability $p$. Because we do not know the top candidate willing to accept an offer, the goal is to maximize a robust objective defined as the minimum over integers $k$ of the probability of choosing one of the top $k$ candidates, given that one of these candidates will accept an offer. Using Markov decision process theory, we derive a linear program for this max-min objective whose solution encodes an optimal policy. The derivation may be of independent interest, as it is generalizable and can be used to obtain linear programs for many online selection models. We further relax this linear program into an infinite counterpart, which we use to provide bounds for the objective and closed-form policies. For $p \geq p^* \approx 0.6$, an optimal policy is a simple threshold rule that observes the first $p^{1/(1-p)}$ fraction of candidates and subsequently makes offers to the best candidate observed so far.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.00842v2</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Perez-Salazar, Mohit Singh, Alejandro Toriello</dc:creator>
    </item>
    <item>
      <title>When Simple is Near Optimal in Security Games</title>
      <link>https://arxiv.org/abs/2402.11209</link>
      <description>arXiv:2402.11209v3 Announce Type: replace 
Abstract: Fraud is ubiquitous across applications and involve users bypassing the rule of law, often with the strategic aim of obtaining some benefit that would otherwise be unattainable within the bounds of lawful conduct. However, user fraud can be detrimental.
  To mitigate the harms of user fraud, we study the problem of policing fraud as a security game between an administrator and users. In this game, an administrator deploys $R$ security resources (e.g., police officers) across $L$ locations and levies fines against users engaging in fraud at those locations. For this security game, we study both payoff and revenue maximization administrator objectives. In both settings, we show that computing the optimal administrator strategy is NP-hard and develop natural greedy algorithm variants for the respective settings that achieve at least half the payoff or revenue as the payoff-maximizing or revenue-maximizing solutions, respectively. We also establish a resource augmentation guarantee that our proposed greedy algorithms with one extra resource, i.e., $R+1$ resources, achieve at least the same payoff (revenue) as the payoff-maximizing (revenue-maximizing) outcome with $R$ resources. Moreover, in the setting when user types are homogeneous, we develop a near-linear time algorithm for the revenue maximization problem and a polynomial time approximation scheme for the payoff maximization problem.
  Next, we present numerical experiments based on a case study of parking enforcement at Stanford University's campus, highlighting the efficacy of our algorithms in increasing parking permit earnings at the university by over \$300,000 annually. Finally, we study several model extensions, including incorporating contracts to bridge the gap between the payoff and revenue-maximizing outcomes and generalizing our model to incorporate additional constraints beyond a resource budget constraint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11209v3</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <category>econ.TH</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Devansh Jalota, Michael Ostrovsky, Marco Pavone</dc:creator>
    </item>
    <item>
      <title>Strategic Network Creation for Enabling Greedy Routing</title>
      <link>https://arxiv.org/abs/2403.15307</link>
      <description>arXiv:2403.15307v2 Announce Type: replace 
Abstract: Today we rely on networks that are created and maintained by smart devices. For such networks, there is no governing central authority but instead the network structure is shaped by the decisions of selfish intelligent agents. A key property of such communication networks is that they should be easy to navigate for routing data. For this, a common approach is greedy routing, where every device simply routes data to a neighbor that is closer to the respective destination.
  Networks of intelligent agents can be analyzed via a game-theoretic approach and in the last decades many variants of network creation games have been proposed and analyzed. In this paper we present the first game-theoretic network creation model that incorporates greedy routing, i.e., the strategic agents in our model are embedded in some metric space and strive for creating a network among themselves where all-pairs greedy routing is enabled. Besides this, the agents optimize their connection quality within the created network by aiming for greedy routing paths with low stretch.
  For our model, we analyze the existence of (approximate)-equilibria and the computational hardness in different underlying metric spaces. E.g., we characterize the set of equilibria in 1-2-metrics and tree metrics and show that Nash equilibria always exist. For Euclidean space, the setting which is most relevant in practice, we prove that equilibria are not guaranteed to exist but that the well-known $\Theta$-graph construction yields networks having a low stretch that are game-theoretically almost stable. For general metric spaces, we show that approximate equilibria exist where the approximation factor depends on the cost of maintaining any link.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15307v2</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julian Berger, Tobias Friedrich, Pascal Lenzner, Paraskevi Machaira, Janosch Ruff</dc:creator>
    </item>
    <item>
      <title>Decentralized and Uncoordinated Learning of Stable Matchings: A Game-Theoretic Approach</title>
      <link>https://arxiv.org/abs/2407.21294</link>
      <description>arXiv:2407.21294v2 Announce Type: replace 
Abstract: We consider the problem of learning stable matchings with unknown preferences in a decentralized and uncoordinated manner, where "decentralized" means that players make decisions individually without the influence of a central platform, and "uncoordinated" means that players do not need to synchronize their decisions using pre-specified rules. First, we provide a game formulation for this problem with known preferences, where the set of pure Nash equilibria (NE) coincides with the set of stable matchings, and mixed NE can be rounded to a stable matching. Then, we show that for hierarchical markets, applying the exponential weight (EXP) learning algorithm to the stable matching game achieves logarithmic regret in a fully decentralized and uncoordinated fashion. Moreover, we show that EXP converges locally and exponentially fast to a stable matching in general markets. We also introduce another decentralized and uncoordinated learning algorithm that globally converges to a stable matching with arbitrarily high probability. Finally, we provide stronger feedback conditions under which it is possible to drive the market faster toward an approximate stable matching. Our proposed game-theoretic framework bridges the discrete problem of learning stable matchings with the problem of learning NE in continuous-action games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21294v2</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Rasoul Etesami, R. Srikant</dc:creator>
    </item>
    <item>
      <title>Can LLMs Replace Economic Choice Prediction Labs? The Case of Language-based Persuasion Games</title>
      <link>https://arxiv.org/abs/2401.17435</link>
      <description>arXiv:2401.17435v4 Announce Type: replace-cross 
Abstract: Human choice prediction in economic contexts is crucial for applications in marketing, finance, public policy, and more. This task, however, is often constrained by the difficulties in acquiring human choice data. With most experimental economics studies focusing on simple choice settings, the AI community has explored whether LLMs can substitute for humans in these predictions and examined more complex experimental economics settings. However, a key question remains: can LLMs generate training data for human choice prediction? We explore this in language-based persuasion games, a complex economic setting involving natural language in strategic interactions. Our experiments show that models trained on LLM-generated data can effectively predict human behavior in these games and even outperform models trained on actual human data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17435v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eilam Shapira, Omer Madmon, Roi Reichart, Moshe Tennenholtz</dc:creator>
    </item>
  </channel>
</rss>
