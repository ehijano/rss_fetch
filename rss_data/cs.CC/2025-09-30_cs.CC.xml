<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Sep 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Parameterized Hardness of Zonotope Containment and Neural Network Verification</title>
      <link>https://arxiv.org/abs/2509.22849</link>
      <description>arXiv:2509.22849v1 Announce Type: new 
Abstract: Neural networks with ReLU activations are a widely used model in machine learning. It is thus important to have a profound understanding of the properties of the functions computed by such networks. Recently, there has been increasing interest in the (parameterized) computational complexity of determining these properties. In this work, we close several gaps and resolve an open problem posted by Froese et al. [COLT '25] regarding the parameterized complexity of various problems related to network verification. In particular, we prove that deciding positivity (and thus surjectivity) of a function $f\colon\mathbb{R}^d\to\mathbb{R}$ computed by a 2-layer ReLU network is W[1]-hard when parameterized by $d$. This result also implies that zonotope (non-)containment is W[1]-hard with respect to $d$, a problem that is of independent interest in computational geometry, control theory, and robotics. Moreover, we show that approximating the maximum within any multiplicative factor in 2-layer ReLU networks, computing the $L_p$-Lipschitz constant for $p\in(0,\infty]$ in 2-layer networks, and approximating the $L_p$-Lipschitz constant in 3-layer networks are NP-hard and W[1]-hard with respect to $d$. Notably, our hardness results are the strongest known so far and imply that the naive enumeration-based methods for solving these fundamental problems are all essentially optimal under the Exponential Time Hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22849v1</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Froese, Moritz Grillo, Christoph Hertrich, Moritz Stargalla</dc:creator>
    </item>
    <item>
      <title>Hardness and Algorithmic Results for Roman \{3\}-Domination</title>
      <link>https://arxiv.org/abs/2509.23615</link>
      <description>arXiv:2509.23615v1 Announce Type: new 
Abstract: A Roman $\{3\}$-dominating function on a graph $G = (V, E)$ is a function $f: V \rightarrow \{0, 1, 2, 3\}$ such that for each vertex $u \in V$, if $f(u) = 0$ then $\sum_{v \in N(u)} f(v) \geq 3$ and if $f(u) = 1$ then $\sum_{v \in N(u)} f(v) \geq 2$. The weight of a Roman $\{3\}$-dominating function $f$ is $\sum_{u \in V} f(u)$. The objective of \rtd{} is to compute a Roman $\{3\}$-dominating function of minimum weight. The problem is known to be NP-complete on chordal graphs, star-convex bipartite graphs and comb-convex bipartite graphs. In this paper, we study the complexity of \rtd{} and show that the problem is NP-complete on split graphs. In addition, we prove that the problem is W[2]-hard parameterized by weight. On the positive front, we present a polynomial-time algorithm for block graphs, thereby resolving an open question posed by Chaudhary and Pradhan [Discrete Applied Mathematics, 2024].</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23615v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sangam Balchandar Reddy</dc:creator>
    </item>
    <item>
      <title>An SoS Entropy Dichotomy via Windowed Hypercontractivity</title>
      <link>https://arxiv.org/abs/2509.24280</link>
      <description>arXiv:2509.24280v1 Announce Type: new 
Abstract: We prove an entropy versus degree dichotomy for low-degree tests and the Sum-of-Squares (SoS) hierarchy on a calibrated window after a gadget layer. For a target distribution \(\mu\) and a product-like proxy \(u\), we study the low-degree discrepancy \(\Delta_k(\mu,u)\), defined as the optimal distinguishing advantage of degree \(\le k\) polynomial tests. Using a bias-orthonormal Walsh basis and a test-moment equivalence on the window, we relate \(\Delta_k\) (up to constants) to the squared \(\ell_2\) mass of signed low-degree moments. Calibrated pseudoexpectations match \(u\) on all moments of degree \(\le k\), hence test discrepancy equals SoS pseudoexpectation deviation. Under bias, product, and width assumptions along a switching path, a windowed Bonami--Beckner inequality yields hypercontractive tail bounds. Combining these with moment matching, we obtain a discrepancy-to-degree theorem: if \(\Delta_k(\mu,u) \ge n^{-\beta}\), then any polynomial-calculus or SoS refutation separating \(\mu\) from \(u\) requires degree \(\Omega(k)\). Instantiating \(k = c \log n\) gives an explicit \(\Omega(\log n)\) SoS degree lower bound whenever \(\Delta_k \ge n^{-\eta}\). All constants are explicit and depend only on calibrated window parameters. This work provides the SoS/low-degree core and complements a prior calibration blueprint; a companion paper lifts the windowed statements to full distribution families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24280v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marko Lela</dc:creator>
    </item>
    <item>
      <title>Structural Separation and Semantic Incompatibility in the P vs. NP Problem: Computational Complexity Analysis with Construction Defining Functionality</title>
      <link>https://arxiv.org/abs/2509.22995</link>
      <description>arXiv:2509.22995v1 Announce Type: cross 
Abstract: The Boolean satisfiability problem (SAT) holds a central place in computational complexity theory as the first shown NP-complete problem. Due to this role, SAT is often used as the benchmark for polynomial-time reductions: if a problem can be reduced to SAT, it is at least as hard as SAT, and hence considered NP-complete. However, the CDF framework offers a structural inversion of this traditional view. Rather than treating SAT as merely a representative of NP-completeness, we investigate whether the syntactic structure of SAT itself -- especially in its 3SAT form -- is the source of semantic explosion and computational intractability observed in NP problems. In other words, SAT is not just the yardstick of NP-completeness, but may be the structural archetype that induces NP-type complexity. This reframing suggests that the P vs NP question is deeply rooted not only in computational resource limits, but in the generative principles of problem syntax, with 3SAT capturing the recursive and non-local constructions that define the boundary between tractable and intractable problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22995v1</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yumiko Nishiyama</dc:creator>
    </item>
    <item>
      <title>Two bases suffice for QMA1-completeness</title>
      <link>https://arxiv.org/abs/2509.24390</link>
      <description>arXiv:2509.24390v1 Announce Type: cross 
Abstract: We introduce a basis-restricted variant of the Quantum-k-SAT problem, in which each term in the input Hamiltonian is required to be diagonal in either the standard or Hadamard basis. Our main result is that the Quantum-6-SAT problem with this basis restriction is already QMA1-complete, defined with respect to a natural gateset. Our construction is based on the Feynman-Kitaev circuit-to-Hamiltonian construction, with a modified clock encoding that interleaves two clocks in the standard and Hadamard bases. In light of the central role played by CSS codes and the uncertainty principle in the proof of the NLTS theorem of Anshu, Breuckmann, and Nirkhe (STOC '23), we hope that the CSS-like structure of our Hamiltonians will make them useful for progress towards a quantum PCP theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24390v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Henry Ma, Anand Natarajan</dc:creator>
    </item>
    <item>
      <title>Structure of sparse Boolean functions over Abelian groups, and its application to testing</title>
      <link>https://arxiv.org/abs/2406.18700</link>
      <description>arXiv:2406.18700v3 Announce Type: replace 
Abstract: We study Fourier-sparse Boolean functions over general finite Abelian groups. A Boolean function $f : G \to \{-1,+1\}$ is $s$-sparse if it has at most $s$ non-zero Fourier coefficients. We introduce a general notion of granularity of Fourier coefficients and prove that every non-zero coefficient of an $s$-sparse Boolean function has magnitude at least \begin{equation*} \frac{1}{2^{\varphi(\Delta)/2} \, s^{\varphi(\Delta)/2}}, \end{equation*} where $\Delta$ denotes the exponent of the group $G$ (that is, the maximum order of an element in $G$) and $\varphi$ is the Euler's totient function. This generalizes the celebrated result of Gopalan et al. (SICOMP 2011) for $\mathbb{Z}_2^n$, extending it to all finite Abelian groups via new techniques from group theory and algebraic number theory.
  Using our new structural results on the Fourier coefficients of sparse functions, we design an efficient sparsity testing algorithm for Boolean functions. The tester distinguishes whether a given function is $s$-sparse or $\epsilon$-far from every $s$-sparse Boolean function, with query complexity $poly\left((2s)^{\varphi(\Delta)},1/\epsilon \right)$. In addition, we generalize the classical notion of Boolean degree to arbitrary Abelian groups and establish an $\Omega(\sqrt{s})$ lower bound for adaptive sparsity testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18700v3</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourav Chakraborty, Swarnalipa Datta, Pranjal Dutta, Arijit Ghosh, Swagato Sanyal</dc:creator>
    </item>
    <item>
      <title>Matrix Multiplication in the MPC Model</title>
      <link>https://arxiv.org/abs/2505.19137</link>
      <description>arXiv:2505.19137v2 Announce Type: replace 
Abstract: In this paper, we present algorithms to solve matrix multiplication problems in the MPC model. In particular, we consider the problem under various processor/memory constraints in the MPC model and prove the following results.
  1. Multiplication of two rectangular matrices of size $d \times n$ and $n \times d$ ( where $d \leq n$) respectively can be done in,
  i) $O(\sqrt{d} + \log_d n)$ rounds with $n$ processors and $\Theta(d)$ memory per processor
  ii) $O(\frac{d}{\sqrt{n}})$ rounds with $d$ processors and $\Theta(n)$ memory per processor.
  2. Multiplication of two rectangular matrices of size $n \times d$ and $d \times n$ (where $d \leq n$) respectively, with $n$ processors of $\Theta(n)$ memory per processor, can be done in $O(\frac{d}{\sqrt{n}})$ rounds.
  3.The multiplication of two $d$-sparse matrices (matrices that contain at most $d$-nonzero elements in each row and in each column) with $n$ processors and $\Theta(d)$ memory per processor can be done in $O(d^{0.9})$ rounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19137v2</guid>
      <category>cs.CC</category>
      <category>cs.DC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lakshya Joshi, Arya Deshmukh, Atharv Chhabra, Chetan Gupta</dc:creator>
    </item>
    <item>
      <title>Constant Bit-size Transformers Are Turing Complete</title>
      <link>https://arxiv.org/abs/2506.12027</link>
      <description>arXiv:2506.12027v2 Announce Type: replace 
Abstract: We prove that any Turing machine running on inputs of arbitrary length can be simulated by a constant bit-size transformer, as long as the context window is sufficiently long. This improves previous works, which require scaling up either the model's precision or the number of parameters on longer inputs. Furthermore, we prove that the complexity class SPACE$[s(n)]$ exactly characterizes the expressive power of a constant bit-size transformer with a context window of length $s(n)$. Our approach relies on simulating Post machines, a Turing-complete computational model. Post machines can be modeled as automata equipped with a queue, exhibiting computational behaviors naturally aligned with those of transformers. The behavioral similarity between transformers and Post machines may offer new insights into the mechanisms underlying the reasoning abilities of transformers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12027v2</guid>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qian Li, Yuyi Wang</dc:creator>
    </item>
    <item>
      <title>Explicit Second-Order Min-Max Optimization: Practical Algorithms and Complexity Analysis</title>
      <link>https://arxiv.org/abs/2210.12860</link>
      <description>arXiv:2210.12860v5 Announce Type: replace-cross 
Abstract: We propose and analyze several inexact regularized Newton-type methods for finding a global saddle point of \emph{convex-concave} unconstrained min-max optimization problems. Compared to first-order methods, our understanding of second-order methods for min-max optimization is relatively limited, as obtaining global rates of convergence with second-order information can be much more involved. In this paper, we examine how second-order information is used to speed up extra-gradient methods, even under inexactness. In particular, we show that the proposed methods generate iterates that remain within a bounded set and that the averaged iterates converge to an $\epsilon$-saddle point within $O(\epsilon^{-2/3})$ iterations in terms of a restricted gap function. We also provide a simple routine for solving the subproblem at each iteration, requiring a single Schur decomposition and $O(\log\log(1/\epsilon))$ calls to a linear system solver in a quasi-upper-triangular system. Thus, our method improves the existing line-search-based second-order min-max optimization methods by shaving off an $O(\log\log(1/\epsilon))$ factor in the required number of Schur decompositions. Finally, we conduct experiments on synthetic and real data to demonstrate the efficiency of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.12860v5</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyi Lin, Panayotis Mertikopoulos, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Gromov's Approximating Tree and the All-Pairs Bottleneck Paths Problem</title>
      <link>https://arxiv.org/abs/2408.05338</link>
      <description>arXiv:2408.05338v2 Announce Type: replace-cross 
Abstract: Given a pointed metric space $(X,\mathsf{dist}, w)$ on $n$ points, its Gromov's approximating tree is a 0-hyperbolic pseudo-metric space $(X,\mathsf{dist}_T)$ such that $\mathsf{dist}(x,w)=\mathsf{dist}_T(x,w)$ and $\mathsf{dist}(x, y)-2 \delta \log_2n \leq \mathsf{dist}_T (x, y) \leq \mathsf{dist}(x, y)$ for all $x, y \in X$ where $\delta$ is the Gromov hyperbolicity of $X$. On the other hand, the all pairs bottleneck paths (APBP) problem asks, given an undirected graph with some capacities on its edges, to find the maximal path capacity between each pair of vertices. In this note, we prove:
  $\bullet$ Computing Gromov's approximating tree for a metric space with $n+1$ points from its matrix of distances reduces to solving the APBP problem on an connected graph with $n$ vertices.
  $\bullet$ There is an explicit algorithm that computes Gromov's approximating tree for a graph from its adjacency matrix in quadratic time.
  $\bullet$ Solving the APBP problem on a weighted graph with $n$ vertices reduces to finding Gromov's approximating tree for a metric space with $n+1$ points from its distance matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05338v2</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anders Cornect, Eduardo Mart\'inez-Pedroza</dc:creator>
    </item>
    <item>
      <title>Learning Randomized Reductions</title>
      <link>https://arxiv.org/abs/2412.18134</link>
      <description>arXiv:2412.18134v2 Announce Type: replace-cross 
Abstract: A self-corrector for a function $f$ takes a black-box oracle computing $f$ that is correct on most inputs and turns it into one that is correct on every input with high probability. Self-correctors exist for any function that is randomly self-reducible (RSR), where the value $f$ at a given point $x$ can be recovered by computing $f$ on random correlated points. While RSRs enable powerful self-correction capabilities and have applications in complexity theory and cryptography, their discovery has traditionally required manual derivation by experts. We present Bitween, a method and tool for automated learning of randomized self-reductions for mathematical functions. We make two key contributions: First, we demonstrate that our learning framework based on linear regression outperforms sophisticated methods including genetic algorithms, symbolic regression, and mixed-integer linear programming for discovering RSRs from correlated samples. Second, we introduce Agentic Bitween, a neuro-symbolic approach where large language models dynamically discover novel query functions for RSR property discovery, leveraging vanilla Bitween as a tool for inference and verification, moving beyond the fixed query functions ($x+r$, $x-r$, $x \cdot r$, $x$, $r$) previously used in the literature. On RSR-Bench, our benchmark suite of 80 scientific and machine learning functions, vanilla Bitween surpasses existing symbolic methods, while Agentic Bitween discovers new RSR properties using frontier models to uncover query functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18134v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ferhat Erata, Orr Paradise, Thanos Typaldos, Timos Antonopoulos, ThanhVu Nguyen, Shafi Goldwasser, Ruzica Piskac</dc:creator>
    </item>
    <item>
      <title>Noise Sensitivity and Learning Lower Bounds for Hierarchical Functions</title>
      <link>https://arxiv.org/abs/2502.05073</link>
      <description>arXiv:2502.05073v3 Announce Type: replace-cross 
Abstract: Recent works explore deep learning's success by examining functions or data with hierarchical structure. To study the learning complexity of functions with hierarchical structure, we study the noise stability of functions with tree hierarchical structure on independent inputs. We show that if each function in the hierarchy is $\varepsilon$-far from linear, the noise stability is exponentially small in the depth of the hierarchy.
  Our results have immediate applications for agnostic learning. In the Boolean setting using the results of Dachman-Soled, Feldman, Tan, Wan and Wimmer (2014), our results provide Statistical Query super-polynomial lower bounds for agnostically learning classes that are based on hierarchical functions.
  We also derive similar SQ lower bounds based on the indicators of crossing events in critical site percolation. These crossing events are not formally hierarchical as we define but still have some hierarchical features as studied in mathematical physics.
  Using the results of Abbe, Bengio, Cornacchiam, Kleinberg, Lotfi, Raghu and Zhang (2022), our results imply sample complexity lower bounds for learning hierarchical functions with gradient descent on fully connected neural networks.
  Finally in the Gaussian setting, using the results of Diakonikolas, Kane, Pittas and Zarifis (2021), our results provide super-polynomial lower bounds for agnostic SQ learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05073v3</guid>
      <category>math.PR</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rupert Li, Elchanan Mossel</dc:creator>
    </item>
    <item>
      <title>Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory</title>
      <link>https://arxiv.org/abs/2509.18057</link>
      <description>arXiv:2509.18057v3 Announce Type: replace-cross 
Abstract: We explore whether techniques from AI can help discover new combinatorial structures that improve on known limits on efficient algorithms. Specifically, we use AlphaEvolve (an LLM coding agent) to study two settings:
  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. Our improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using AlphaEvolve. Additionally, via analytical arguments we strengthen the upper bounds to settle the computational hardness of these questions up to an error in the third decimal place.
  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new inapproximability results, proving that it is NP-hard to approximate MAX-4-CUT and MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of improving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget reduction from "standard" H{\aa}stad-style PCPs.
  A key technical challenge we faced: verifying a candidate construction produced by AlphaEvolve is costly (often requiring exponential time). In both settings above, our results were enabled by using AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by $10,000\times$). We conclude with a discussion of norms by which to assess the assistance from AI in developing proofs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18057v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ansh Nagda, Prabhakar Raghavan, Abhradeep Thakurta</dc:creator>
    </item>
  </channel>
</rss>
