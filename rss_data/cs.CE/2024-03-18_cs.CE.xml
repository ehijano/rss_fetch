<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Mar 2024 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>FloodGenome: Interpretable Machine Learning for Decoding Features Shaping Property Flood Risk Predisposition in Cities</title>
      <link>https://arxiv.org/abs/2403.10625</link>
      <description>arXiv:2403.10625v1 Announce Type: new 
Abstract: Understanding the fundamental characteristics that shape the inherent flood risk disposition of urban areas is critical for integrated urban design strategies for flood risk reduction. Flood risk disposition specifies an inherent and event-independent magnitude of property flood risk and measures the extent to which urban areas are susceptible to property damage if exposed to a weather hazard. This study presents FloodGenome as an interpretable machine learning model for evaluation of the extent to which various hydrological, topographic, and built-environment features and their interactions shape flood risk disposition in urban areas. Using flood damage claims data from the U.S. National Flood Insurance Program covering the period 2003 through 2023 across four metropolitan statistical areas (MSAs), the analysis computes building damage ratios and flood claim counts by employing k-means clustering for classifying census block groups (CBGs) into distinct property flood risk disposition levels. Then a random forest model is created to specify property flood risk levels of CBGs based on various intertwined hydrological, topographic, and built-environment features. The model transferability analysis results show consistent performance across MSAs, revealing the universality of underlying features that shape city property flood risks. The FloodGenome model is then used to:(1) evaluate the extent to which future urban development would exacerbate flood risk disposition of urban areas; and (2) specify property flood risk levels at finer spatial resolution providing critical insights for flood risk management processes. The FloodGenome model and the findings provide novel tools and insights for improving the characterization and understanding of intertwined features that shape flood risk profiles of cities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10625v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenyue Liu, Ali Mostafavi</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Review of Latent Space Dynamics Identification Algorithms for Intrusive and Non-Intrusive Reduced-Order-Modeling</title>
      <link>https://arxiv.org/abs/2403.10748</link>
      <description>arXiv:2403.10748v1 Announce Type: new 
Abstract: Numerical solvers of partial differential equations (PDEs) have been widely employed for simulating physical systems. However, the computational cost remains a major bottleneck in various scientific and engineering applications, which has motivated the development of reduced-order models (ROMs). Recently, machine-learning-based ROMs have gained significant popularity and are promising for addressing some limitations of traditional ROM methods, especially for advection dominated systems. In this chapter, we focus on a particular framework known as Latent Space Dynamics Identification (LaSDI), which transforms the high-fidelity data, governed by a PDE, to simpler and low-dimensional latent-space data, governed by ordinary differential equations (ODEs). These ODEs can be learned and subsequently interpolated to make ROM predictions. Each building block of LaSDI can be easily modulated depending on the application, which makes the LaSDI framework highly flexible. In particular, we present strategies to enforce the laws of thermodynamics into LaSDI models (tLaSDI), enhance robustness in the presence of noise through the weak form (WLaSDI), select high-fidelity training data efficiently through active learning (gLaSDI, GPLaSDI), and quantify the ROM prediction uncertainty through Gaussian processes (GPLaSDI). We demonstrate the performance of different LaSDI approaches on Burgers equation, a non-linear heat conduction problem, and a plasma physics problem, showing that LaSDI algorithms can achieve relative errors of less than a few percent and up to thousands of times speed-ups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10748v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Bonneville, Xiaolong He, April Tran, Jun Sur Park, William Fries, Daniel A. Messenger, Siu Wun Cheung, Yeonjong Shin, David M. Bortz, Debojyoti Ghosh, Jiun-Shyan Chen, Jonathan Belof, Youngsoo Choi</dc:creator>
    </item>
    <item>
      <title>Vehicle single track modeling using physics guided neural differential equations</title>
      <link>https://arxiv.org/abs/2403.11648</link>
      <description>arXiv:2403.11648v1 Announce Type: new 
Abstract: In this paper, we follow the physics guided modeling approach and integrate a neural differential equation network into the physical structure of a vehicle single track model. By relying on the kinematic relations of the single track ordinary differential equations (ODE), a small neural network and few training samples are sufficient to substantially improve the model accuracy compared with a pure physics based vehicle single track model. To be more precise, the sum of squared error is reduced by 68% in the considered scenario. In addition, it is demonstrated that the prediction capabilities of the physics guided neural ODE model are superior compared with a pure black box neural differential equation approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11648v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stephan Rhode, Fabian Jarmolowitz, Felix Berkel</dc:creator>
    </item>
    <item>
      <title>Default Resilience and Worst-Case Effects in Financial Networks</title>
      <link>https://arxiv.org/abs/2403.10631</link>
      <description>arXiv:2403.10631v1 Announce Type: cross 
Abstract: In this paper we analyze the resilience of a network of banks to joint price fluctuations of the external assets in which they have shared exposures, and evaluate the worst-case effects of the possible default contagion. Indeed, when the prices of certain external assets either decrease or increase, all banks exposed to them experience varying degrees of simultaneous shocks to their balance sheets. These coordinated and structured shocks have the potential to exacerbate the likelihood of defaults. In this context, we introduce first a concept of {default resilience margin}, $\epsilon^*$, i.e., the maximum amplitude of asset prices fluctuations that the network can tolerate without generating defaults. Such threshold value is computed by considering two different measures of price fluctuations, one based on the maximum individual variation of each asset, and the other based on the sum of all the asset's absolute variations. For any price perturbation having amplitude no larger than $\epsilon^*$, the network absorbs the shocks remaining default free. When the perturbation amplitude goes beyond $\epsilon^*$, however, defaults may occur. In this case we find the worst-case systemic loss, that is, the total unpaid debt under the most severe price variation of given magnitude. Computation of both the threshold level $\epsilon^*$ and of the worst-case loss and of a corresponding worst-case asset price scenario, amounts to solving suitable linear programming problems.}</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10631v1</guid>
      <category>q-fin.RM</category>
      <category>cs.CE</category>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giuseppe Calafiore, Giulia Fracastoro, Anton Proskurnikov</dc:creator>
    </item>
    <item>
      <title>Circle Packing Problem Using Nature-Inspired Optimization Techniques</title>
      <link>https://arxiv.org/abs/2403.10965</link>
      <description>arXiv:2403.10965v1 Announce Type: cross 
Abstract: This paper deals with the problem of circle packing, in which the largest radii circle is to be fit in a confined space filled with arbitrary circles of different radii and centers. A circle packing problem is one of a variety of cutting and packing problems. We suggest four different nature-inspired Meta-heuristic algorithms to solve this problem. Algorithms are based on the social behavior of other biology species such as birds, wolves, fireflies, and bats. Moreover, recent advancements in these algorithms are also considered for problem-solving. The circle packing problem is one of the NP-hard problems. It is challenging to solve NP-hard problems exactly, so the proposed algorithms provide an approximate solution within the allotted time. Standard statistical parameters are used for comparison, and simulation and results indicate that the problem is highly non-linear and sensitive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10965v1</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pulkit Mundra, Veni Goyal, Kusum Deep</dc:creator>
    </item>
    <item>
      <title>From Pixels to Predictions: Spectrogram and Vision Transformer for Better Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2403.11047</link>
      <description>arXiv:2403.11047v1 Announce Type: cross 
Abstract: Time series forecasting plays a crucial role in decision-making across various domains, but it presents significant challenges. Recent studies have explored image-driven approaches using computer vision models to address these challenges, often employing lineplots as the visual representation of time series data. In this paper, we propose a novel approach that uses time-frequency spectrograms as the visual representation of time series data. We introduce the use of a vision transformer for multimodal learning, showcasing the advantages of our approach across diverse datasets from different domains. To evaluate its effectiveness, we compare our method against statistical baselines (EMA and ARIMA), a state-of-the-art deep learning-based approach (DeepAR), other visual representations of time series data (lineplot images), and an ablation study on using only the time series as input. Our experiments demonstrate the benefits of utilizing spectrograms as a visual representation for time series data, along with the advantages of employing a vision transformer for simultaneous learning in both the time and frequency domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11047v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhen Zeng, Rachneet Kaur, Suchetha Siddagangappa, Tucker Balch, Manuela Veloso</dc:creator>
    </item>
    <item>
      <title>Stochastic approach for elliptic problems in perforated domains</title>
      <link>https://arxiv.org/abs/2403.11385</link>
      <description>arXiv:2403.11385v1 Announce Type: cross 
Abstract: A wide range of applications in science and engineering involve a PDE model in a domain with perforations, such as perforated metals or air filters. Solving such perforated domain problems suffers from computational challenges related to resolving the scale imposed by the geometries of perforations. We propose a neural network-based mesh-free approach for perforated domain problems. The method is robust and efficient in capturing various configuration scales, including the averaged macroscopic behavior of the solution that involves a multiscale nature induced by small perforations. The new approach incorporates the derivative-free loss method that uses a stochastic representation or the Feynman-Kac formulation. In particular, we implement the Neumann boundary condition for the derivative-free loss method to handle the interface between the domain and perforations. A suite of stringent numerical tests is provided to support the proposed method's efficacy in handling various perforation scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11385v1</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jihun Han, Yoonsang Lee</dc:creator>
    </item>
    <item>
      <title>HDLdebugger: Streamlining HDL debugging with Large Language Models</title>
      <link>https://arxiv.org/abs/2403.11671</link>
      <description>arXiv:2403.11671v1 Announce Type: cross 
Abstract: In the domain of chip design, Hardware Description Languages (HDLs) play a pivotal role. However, due to the complex syntax of HDLs and the limited availability of online resources, debugging HDL codes remains a difficult and time-intensive task, even for seasoned engineers. Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers. Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results. In this paper, we propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for retrieval-augmented generation, and a retrieval-augmented LLM fine-tuning approach. Through the integration of these components, HDLdebugger can automate and streamline HDL debugging for chip design. Our comprehensive experiments, conducted on an HDL code dataset sourced from Huawei, reveal that HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional effectiveness in HDL code debugging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11671v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xufeng Yao, Haoyang Li, Tsz Ho Chan, Wenyi Xiao, Mingxuan Yuan, Yu Huang, Lei Chen, Bei Yu</dc:creator>
    </item>
    <item>
      <title>Efficient Training of Learning-Based Thermal Power Flow for 4th Generation District Heating Grids</title>
      <link>https://arxiv.org/abs/2403.11877</link>
      <description>arXiv:2403.11877v1 Announce Type: cross 
Abstract: Thermal power flow (TPF) is an important task for various control purposes in 4 Th generation district heating grids with multiple decentral heat sources and meshed grid structures. Computing the TPF, i.e., determining the grid state consisting of temperatures, pressures, and mass flows for given supply and demand values, is classically done by solving the nonlinear heat grid equations, but can be sped up by orders of magnitude using learned models such as neural networks. We propose a novel, efficient scheme to generate a sufficiently large training data set covering relevant supply and demand values. Instead of sampling supply and demand values, our approach generates training examples from a proxy distribution over generator and consumer mass flows, omitting the iterations needed for solving the heat grid equations. The exact, but slightly different, training examples can be weighted to represent the original training distribution. We show with simulations for typical grid structures that the new approach can reduce training set generation times by two orders of magnitude compared to sampling supply and demand values directly, without loss of relevance for the training samples. Moreover, learning TPF with a training data set is shown to outperform sample-free, physics-aware training approaches significantly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11877v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andreas Bott, Mario Beykirch, Florian Steinke</dc:creator>
    </item>
    <item>
      <title>Exploring Estonia's Open Government Data Development as a Journey towards Excellence: Unveiling the Progress of Local Governments in Open Data Provision</title>
      <link>https://arxiv.org/abs/2403.11952</link>
      <description>arXiv:2403.11952v1 Announce Type: cross 
Abstract: Estonia has a global reputation of a digital state or e-country. However, despite the success in digital governance, the country has faced challenges in the realm of Open Government Data (OGD) area, with significant advancements in its OGD ecosystem, as reflected in various open data rankings from 2020 and onwards, in the recent years being recognized among trend-setters. This paper aims to explore the evolution and positioning of Estonia's OGD development, encompassing national and local levels, through an integrated analysis of various indices, primary data from the Estonian OGD portal, and a thorough literature review. The research shows that Estonia has made progress in the national level open data ecosystem, primarily due to improvements in the OGD portal usability and legislation amendments. However, the local level is not as developed, with local governments lagging behind in OGD provision. The literature review highlights the lack of previous research focusing on Estonian and European local open data, emphasizing the need for future studies to explore the barriers and enablers of municipal OGD. This study contributes to a nuanced understanding of Estonia's dynamic journey in the OGD landscape, shedding light on both achievements and areas warranting further attention for establishing a sustainable open data ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11952v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.DB</category>
      <category>cs.SE</category>
      <category>cs.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katrin Rajam\"ae-Soosaar, Anastasija Nikiforova</dc:creator>
    </item>
    <item>
      <title>A force-based higher-order shear beam element model with rational shear stress distribution for accurate analyses of FG sandwich beams</title>
      <link>https://arxiv.org/abs/2312.14367</link>
      <description>arXiv:2312.14367v3 Announce Type: replace 
Abstract: This study introduces a force-based higher-order shear deformable beam finite element model that incorporates a rational shear stress distribution, designed for the precise analysis of functionally graded sandwich beams. Unlike conventional higher-order shear beam finite elements that regard generalized displacements as unknown fields, this model considers the distributions of stress resultants along the beam axis as the unknown fields. The specific forms of these stress resultants and the generalized displacements are analytically determined, based on the differential equilibrium equations of the higher-order shear beam. This approach effectively circumvents numerical errors that can arise from finite element discretization. Furthermore, the model introduces a stress equilibrium equation to accurately depict the distribution of transverse shear stress across the beam thickness. A corrected shear stiffness, which takes into account rational shear stress, is derived and incorporated into the proposed beam element. Numerical examples underscore the accuracy and efficacy of the proposed higher-order beam element model in the static analysis of functionally graded sandwich beams, particularly in terms of true transverse shear stress distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14367v3</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxiong Li, Huiyi Chen, Suiyin Chen</dc:creator>
    </item>
    <item>
      <title>Sparse Index Tracking: Simultaneous Asset Selection and Capital Allocation via $\ell_0$-Constrained Portfolio</title>
      <link>https://arxiv.org/abs/2309.10152</link>
      <description>arXiv:2309.10152v3 Announce Type: replace-cross 
Abstract: Sparse index tracking is a prominent passive portfolio management strategy that constructs a sparse portfolio to track a financial index. A sparse portfolio is preferable to a full portfolio in terms of reducing transaction costs and avoiding illiquid assets. To achieve portfolio sparsity, conventional studies have utilized $\ell_p$-norm regularizations as a continuous surrogate of the $\ell_0$-norm regularization. Although these formulations can construct sparse portfolios, their practical application is challenging due to the intricate and time-consuming process of tuning parameters to define the precise upper limit of assets in the portfolio. In this paper, we propose a new problem formulation of sparse index tracking using an $\ell_0$-norm constraint that enables easy control of the upper bound on the number of assets in the portfolio. Moreover, our approach offers a choice between constraints on portfolio and turnover sparsity, further reducing transaction costs by limiting asset updates at each rebalancing interval. Furthermore, we develop an efficient algorithm for solving this problem based on a primal-dual splitting method. Finally, we illustrate the effectiveness of the proposed method through experiments on the S&amp;P500 and Russell3000 index datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10152v3</guid>
      <category>q-fin.PM</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eisuke Yamagata, Shunsuke Ono</dc:creator>
    </item>
    <item>
      <title>Limit Order Book Dynamics and Order Size Modelling Using Compound Hawkes Process</title>
      <link>https://arxiv.org/abs/2312.08927</link>
      <description>arXiv:2312.08927v3 Announce Type: replace-cross 
Abstract: Hawkes Process has been used to model Limit Order Book (LOB) dynamics in several ways in the literature however the focus has been limited to capturing the inter-event times while the order size is usually assumed to be constant. We propose a novel methodology of using Compound Hawkes Process for the LOB where each event has an order size sampled from a calibrated distribution. The process is formulated in a novel way such that the spread of the process always remains positive. Further, we condition the model parameters on time of day to support empirical observations. We make use of an enhanced non-parametric method to calibrate the Hawkes kernels and allow for inhibitory cross-excitation kernels. We showcase the results and quality of fits for an equity stock's LOB in the NASDAQ exchange and compare them against several baselines. Finally, we conduct a market impact study of the simulator and show the empirical observation of a concave market impact function is indeed replicated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08927v3</guid>
      <category>q-fin.TR</category>
      <category>cs.CE</category>
      <category>q-fin.CP</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konark Jain, Nick Firoozye, Jonathan Kochems, Philip Treleaven</dc:creator>
    </item>
    <item>
      <title>Multi-Objective Reinforcement Learning-based Approach for Pressurized Water Reactor Optimization</title>
      <link>https://arxiv.org/abs/2312.10194</link>
      <description>arXiv:2312.10194v3 Announce Type: replace-cross 
Abstract: A novel method, the Pareto Envelope Augmented with Reinforcement Learning (PEARL), has been developed to address the challenges posed by multi-objective problems, particularly in the field of engineering where the evaluation of candidate solutions can be time-consuming. PEARL distinguishes itself from traditional policy-based multi-objective Reinforcement Learning methods by learning a single policy, eliminating the need for multiple neural networks to independently solve simpler sub-problems. Several versions inspired from deep learning and evolutionary techniques have been crafted, catering to both unconstrained and constrained problem domains. Curriculum Learning is harnessed to effectively manage constraints in these versions. PEARL's performance is first evaluated on classical multi-objective benchmarks. Additionally, it is tested on two practical PWR core Loading Pattern optimization problems to showcase its real-world applicability. The first problem involves optimizing the Cycle length and the rod-integrated peaking factor as the primary objectives, while the second problem incorporates the mean average enrichment as an additional objective. Furthermore, PEARL addresses three types of constraints related to boron concentration, peak pin burnup, and peak pin power. The results are systematically compared against conventional approaches. Notably, PEARL, specifically the PEARL-NdS variant, efficiently uncovers a Pareto front without necessitating additional efforts from the algorithm designer, as opposed to a single optimization with scaled objectives. It also outperforms the classical approach across multiple performance metrics, including the Hyper-volume.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10194v3</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Seurin, Koroush Shirvan</dc:creator>
    </item>
  </channel>
</rss>
