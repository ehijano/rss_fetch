<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Oct 2024 01:56:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Assessing the Performance of Human-Capable LLMs -- Are LLMs Coming for Your Job?</title>
      <link>https://arxiv.org/abs/2410.16285</link>
      <description>arXiv:2410.16285v1 Announce Type: new 
Abstract: The current paper presents the development and validation of SelfScore, a novel benchmark designed to assess the performance of automated Large Language Model (LLM) agents on help desk and professional consultation tasks. Given the increasing integration of AI in industries, particularly within customer service, SelfScore fills a crucial gap by enabling the comparison of automated agents and human workers. The benchmark evaluates agents on problem complexity and response helpfulness, ensuring transparency and simplicity in its scoring system. The study also develops automated LLM agents to assess SelfScore and explores the benefits of Retrieval-Augmented Generation (RAG) for domain-specific tasks, demonstrating that automated LLM agents incorporating RAG outperform those without. All automated LLM agents were observed to perform better than the human control group. Given these results, the study raises concerns about the potential displacement of human workers, especially in areas where AI technologies excel. Ultimately, SelfScore provides a foundational tool for understanding the impact of AI in help desk environments while advocating for ethical considerations in the ongoing transition towards automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16285v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>John Mavi, Nathan Summers, Sergio Coronado</dc:creator>
    </item>
    <item>
      <title>Challenges and Opportunities: Implementing Diversity and Inclusion in Software Engineering University Level Education in Finland</title>
      <link>https://arxiv.org/abs/2410.16288</link>
      <description>arXiv:2410.16288v1 Announce Type: new 
Abstract: Considerable efforts have been made at the high school level to encourage girls to pursue software engineering careers and raise awareness about diversity within the field. Similarly, software companies have become more active in diversity and inclusion (D&amp;I) topics, aiming to create more inclusive work environments. However, the way diversity and inclusion are approached inside software engineering university education remains less clear. This study investigates the current state of D&amp;I in software engineering education and faculties in Finland. An online survey (N=30) was conducted among Finnish software engineering university teachers to investigate which approaches and case examples of D&amp;I are most commonly used by software engineering teachers in Finland. In addition, it was researched how software engineering teachers perceive the importance of D&amp;I in their courses. As a result of the quantitative and thematic analysis, a framework to identify attitudes, approaches, challenges and pedagogical strategies when implementing D&amp;I themes in software engineering education is presented. This framework also offers a process for integrating D&amp;I themes for the curriculum or at the faculty level. The findings of this study emphasize that there is a continuing need for diverse-aware education and training. The results underline the responsibility of universities to ensure that future professionals are equipped with the necessary skills and knowledge to promote D&amp;I in the field of software engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16288v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sonja M. Hyrynsalmi</dc:creator>
    </item>
    <item>
      <title>Em\'ilias Podcast -- Mulheres na Computa\c{c}\~ao: Ampliando Horizontes e Inspirando Carreiras em STEM</title>
      <link>https://arxiv.org/abs/2410.16294</link>
      <description>arXiv:2410.16294v1 Announce Type: new 
Abstract: On October 3, 2024, the "Em\'ilias Podcast -- Women in Computing" celebrates its 5th anniversary, standing out as a platform that promotes the participation of women in STEM (an acronym for "science, technology, engineering, and mathematics"). The podcast aims to provide a space for women in computing and related fields to share their experiences and highlight the various opportunities in Information and Communication Technology (ICT). The methodology included a feedback survey with interviewees, conducted via Google Forms, to assess their experience and determine whether they would recommend the podcast. In addition, we analyzed audience data, which showed consistent growth over the five years. The results revealed that 100% of the interviewees would recommend "Em\'ilias Podcast," reflecting a high level of satisfaction with the project. The average participation experience rating was 4.7 on a scale of 1 to 5, highlighting positive aspects such as the quality of the script, the interview conduction, and the networking opportunities. The audience data also underscore the podcast's impact: with over 10,000 accumulated downloads and plays, it is primarily listened to by people aged 23 to 44, with 50.9% of the audience being female, demonstrating its relevance and reach. In conclusion, the feedback from interviewees and the audience data reinforce the podcast's positive impact and its crucial role in the inclusion of women in technology. The results highlight the importance of promoting the field and its opportunities, contributing to a more inclusive and inspiring future. The data analysis demonstrates the podcast's effectiveness in engaging and expanding its audience, establishing it as a significant example of social impact in ICT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16294v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nath\'alya Chaves Dos Santos, Adolfo Gustavo Serra Seca Neto</dc:creator>
    </item>
    <item>
      <title>Intelligent Computing Social Modeling and Methodological Innovations in Political Science in the Era of Large Language Models</title>
      <link>https://arxiv.org/abs/2410.16301</link>
      <description>arXiv:2410.16301v1 Announce Type: new 
Abstract: The recent wave of artificial intelligence, epitomized by large language models (LLMs), has presented opportunities and challenges for methodological innovation in political science, sparking discussions on a potential paradigm shift in the social sciences. However, how can we understand the impact of LLMs on knowledge production and paradigm transformation in the social sciences from a comprehensive perspective that integrates technology and methodology? What are LLMs' specific applications and representative innovative methods in political science research? These questions, particularly from a practical methodological standpoint, remain underexplored. This paper proposes the "Intelligent Computing Social Modeling" (ICSM) method to address these issues by clarifying the critical mechanisms of LLMs. ICSM leverages the strengths of LLMs in idea synthesis and action simulation, advancing intellectual exploration in political science through "simulated social construction" and "simulation validation." By simulating the U.S. presidential election, this study empirically demonstrates the operational pathways and methodological advantages of ICSM. By integrating traditional social science paradigms, ICSM not only enhances the quantitative paradigm's capability to apply big data to assess the impact of factors but also provides qualitative paradigms with evidence for social mechanism discovery at the individual level, offering a powerful tool that balances interpretability and predictability in social science research. The findings suggest that LLMs will drive methodological innovation in political science through integration and improvement rather than direct substitution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16301v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhenyu Wang, Yi Xu, Dequan Wang, Lingfeng Zhou, Yiqi Zhou</dc:creator>
    </item>
    <item>
      <title>FlashHack: Reflections on the Usage of a Micro Hackathon as an Assessment Tool in a Machine Learning Course</title>
      <link>https://arxiv.org/abs/2410.16305</link>
      <description>arXiv:2410.16305v1 Announce Type: new 
Abstract: Machine learning (ML) course for undergraduates face challenges in assessing student learning and providing practical exposure. Group project-based learning, an increasingly popular form of experiential learning in CS education, encounters certain limitation in participation and non-participation from a few students. Studies also suggest that students find longer programming assignments and project-based assessments distracting and struggle to maintain focus when they coincide with other courses. To tackle these issues, we introduced FlashHack: a monitored, incremental, in-classroom micro Hackathon that combines project-based learning with Hackathon elements. Engaging 229 third year CS undergraduate students in teams of four, FlashHack prompted them to tackle predefined challenges using machine learning techniques within a set timeframe. Assessment criteria emphasized machine learning application, problem-solving, collaboration, and creativity. Our results indicate high student engagement and satisfaction, alongside simplified assessment processes for instructors. This experience report outlines the Hackathon design and implementation, highlights successes and areas for improvement making it feasible for replication by interested computing educators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16305v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3649165.3690128</arxiv:DOI>
      <dc:creator>R Indra, PD Parthasarathy, Jatin Ambasana, Spruha Satavlekar</dc:creator>
    </item>
    <item>
      <title>Why AI Is WEIRD and Should Not Be This Way: Towards AI For Everyone, With Everyone, By Everyone</title>
      <link>https://arxiv.org/abs/2410.16315</link>
      <description>arXiv:2410.16315v1 Announce Type: new 
Abstract: This paper presents a vision for creating AI systems that are inclusive at every stage of development, from data collection to model design and evaluation. We address key limitations in the current AI pipeline and its WEIRD representation, such as lack of data diversity, biases in model performance, and narrow evaluation metrics. We also focus on the need for diverse representation among the developers of these systems, as well as incentives that are not skewed toward certain groups. We highlight opportunities to develop AI systems that are for everyone (with diverse stakeholders in mind), with everyone (inclusive of diverse data and annotators), and by everyone (designed and developed by a globally diverse workforce).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16315v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rada Mihalcea, Oana Ignat, Longju Bai, Angana Borah, Luis Chiruzzo, Zhijing Jin, Claude Kwizera, Joan Nwatu, Soujanya Poria, Thamar Solorio</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Human Mobility Patterns: Utilizing Taxi and Mobile (SafeGraph) Data to Investigate Neighborhood-Scale Mobility in New York City</title>
      <link>https://arxiv.org/abs/2410.16462</link>
      <description>arXiv:2410.16462v1 Announce Type: new 
Abstract: Numerous researchers have utilized GPS-enabled vehicle data and SafeGraph mobility data to analyze human movements. However, the comparison of their ability to capture human mobility remains unexplored. This study investigates differences in human mobility using taxi trip records and the SafeGraph dataset in New York City neighborhoods. The analysis includes neighborhood clustering to identify population characteristics and a comparative analysis of mobility patterns. Our findings show that taxi data tends to capture human mobility to and from locations such as Lower Manhattan, where taxi demand is consistently high, while often underestimating the volume of trips originating from areas with lower taxi demand, particularly in the suburbs of NYC. In contrast, SafeGraph data excels in capturing trips to and from areas where commuting by driving one's own car is common, but underestimates trips in pedestrian-heavy areas. The comparative analysis also sheds new light on transportation mode choices for trips across various neighborhoods. The results of this study underscore the importance of understanding the representativeness of human mobility big data and highlight the necessity for careful consideration when selecting the most suitable dataset for human mobility research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16462v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuqin Jiang, Zhenlong Li, Joon-Seok Kim, Huan Ning, Su Yeon Han</dc:creator>
    </item>
    <item>
      <title>Vernacularizing Taxonomies of Harm is Essential for Operationalizing Holistic AI Safety</title>
      <link>https://arxiv.org/abs/2410.16562</link>
      <description>arXiv:2410.16562v1 Announce Type: new 
Abstract: Operationalizing AI ethics and safety principles and frameworks is essential to realizing the potential benefits and mitigating potential harms caused by AI systems. To that end, actors across industry, academia, and regulatory bodies have created formal taxonomies of harm to support operationalization efforts. These include novel holistic methods that go beyond exclusive reliance on technical benchmarking. However, our paper argues that such taxonomies must also be transferred into local categories to be readily implemented in sector-specific AI safety operationalization efforts, and especially in underresourced or high-risk sectors. This is because many sectors are constituted by discourses, norms, and values that "refract" or even directly conflict with those operating in society more broadly. Drawing from emerging anthropological theories of human rights, we propose that the process of "vernacularization"--a participatory, decolonial practice distinct from doctrinary "translation" (the dominant mode of AI safety operationalization)--can help bridge this gap. To demonstrate this point, we consider the education sector, and identify precisely how vernacularizing a leading holistic taxonomy of harm leads to a clearer view of how harms AI systems may cause are substantially intensified when deployed in educational spaces. We conclude by discussing the generalizability of vernacularization as a useful AI safety methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16562v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Wm. Matthew Kennedy, Daniel Vargas Campos</dc:creator>
    </item>
    <item>
      <title>Cutting Through the Confusion and Hype: Understanding the True Potential of Generative AI</title>
      <link>https://arxiv.org/abs/2410.16629</link>
      <description>arXiv:2410.16629v1 Announce Type: new 
Abstract: This paper explores the nuanced landscape of generative AI (genAI), particularly focusing on neural network-based models like Large Language Models (LLMs). While genAI garners both optimistic enthusiasm and sceptical criticism, this work seeks to provide a balanced examination of its capabilities, limitations, and the profound impact it may have on societal functions and personal interactions. The first section demystifies language-based genAI through detailed discussions on how LLMs learn, their computational needs, distinguishing features from supporting technologies, and the inherent limitations in their accuracy and reliability. Real-world examples illustrate the practical applications and implications of these technologies. The latter part of the paper adopts a systems perspective, evaluating how the integration of LLMs with existing technologies can enhance productivity and address emerging concerns. It highlights the need for significant investment to understand the implications of recent advancements, advocating for a well-informed dialogue to ethically and responsibly integrate genAI into diverse sectors. The paper concludes with prospective developments and recommendations, emphasizing a forward-looking approach to harnessing genAI`s potential while mitigating its risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16629v1</guid>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <category>cs.MA</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ante Prodan, Jo-An Occhipinti, Rehez Ahlip, Goran Ujdur, Harris A. Eyre, Kyle Goosen, Luke Penza, Mark Heffernan</dc:creator>
    </item>
    <item>
      <title>Contrasting Attitudes Towards Current and Future AI Applications for Computerised Interpretation of ECG: A Clinical Stakeholder Interview Study</title>
      <link>https://arxiv.org/abs/2410.16879</link>
      <description>arXiv:2410.16879v1 Announce Type: new 
Abstract: Objectives: To investigate clinicians' attitudes towards current automated interpretation of ECG and novel AI technologies and their perception of computer-assisted interpretation. Materials and Methods: We conducted a series of interviews with clinicians in the UK. Our study: (i) explores the potential for AI, specifically future 'human-like' computing approaches, to facilitate ECG interpretation and support clinical decision making, and (ii) elicits their opinions about the importance of explainability and trustworthiness of AI algorithms. Results: We performed inductive thematic analysis on interview transcriptions from 23 clinicians and identified the following themes: (i) a lack of trust in current systems, (ii) positive attitudes towards future AI applications and requirements for these, (iii) the relationship between the accuracy and explainability of algorithms, and (iv) opinions on education, possible deskilling, and the impact of AI on clinical competencies. Discussion: Clinicians do not trust current computerised methods, but welcome future 'AI' technologies. Where clinicians trust future AI interpretation to be accurate, they are less concerned that it is explainable. They also preferred ECG interpretation that demonstrated the results of the algorithm visually. Whilst clinicians do not fear job losses, they are concerned about deskilling and the need to educate the workforce to use AI responsibly. Conclusion: Clinicians are positive about the future application of AI in clinical decision-making. Accuracy is a key factor of uptake and visualisations are preferred over current computerised methods. This is viewed as a potential means of training and upskilling, in contrast to the deskilling that automation might be perceived to bring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16879v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Hughes-Noehrer, Leda Channer, Gabriel Strain, Gregory Yates, Richard Body, Caroline Jay</dc:creator>
    </item>
    <item>
      <title>AI Future Envisioning with PLACARD</title>
      <link>https://arxiv.org/abs/2410.17155</link>
      <description>arXiv:2410.17155v1 Announce Type: new 
Abstract: At EuroPLoP 2024 Mary Tedeschi led the "AI Future Envisioning with PLACARD" focus group in Germany. Three conference attendees joined in the room while Sridevi, Paola, and Charles co-facilitated remotely via a web conference. The participants were introduced to a Futures Studies technique with the goal of capturing envisionments of Artificial Intelligence (AI) going forward. To set an atmosphere a technology focused card game was used to make the session more interactive. To close everyone co-created a Project Action Review to recap of the event to capture learnings that has been summarized in this paper. The Focus Group was structured based on lessons learned over six earlier iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17155v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3698322.3698444</arxiv:DOI>
      <dc:creator>Mary C. Tedeschi, Paola Ricaurte, Sridevi Ayloo, Joseph Corneli, Charles Jeffrey Danoff, Sergio Belich</dc:creator>
    </item>
    <item>
      <title>How the Internet Facilitates Adverse Childhood Experiences for Youth Who Self-Identify as in Need of Services</title>
      <link>https://arxiv.org/abs/2410.16507</link>
      <description>arXiv:2410.16507v1 Announce Type: cross 
Abstract: Youth implicated in the child welfare and juvenile justice systems, as well as those with an incarcerated parent, are considered the most vulnerable Children in Need of Services (CHINS). We identified 1,160 of these at-risk youth (ages 13-17) who sought support via an online peer support platform to understand their adverse childhood experiences and explore how the internet played a role in providing an outlet for support, as well as potentially facilitating risks. We first analyzed posts from 1,160 youth who self-identified as CHINS while sharing about their adverse experiences. Then, we retrieved all 239,929 posts by these users to identify salient topics within their support-seeking posts: 1) Urges to self-harm due to social drama, 2) desire for social connection, 3) struggles with family, and 4) substance use and sexual risks. We found that the internet often helped facilitate these problems; for example, the desperation for social connection often led to meeting unsafe people online, causing additional trauma. Family members and other unsafe people used the internet to perpetrate cyberabuse, while CHINS themselves leveraged online channels to engage in illegal and risky behavior. Our study calls for tailored support systems that address the unique needs of CHINS to promote safe online spaces and foster resilience to break the cycle of adversity. Empowering CHINS requires amplifying their voices and acknowledging the challenges they face as a result of their adverse childhood experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16507v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ozioma C. Oguine, Jinkyung Katie Park, Mamtaj Akter, Johanna Olesk, Abdulmalik Alluhidan, Pamela Wisniewski, Karla Badillo-Urquiola</dc:creator>
    </item>
    <item>
      <title>The Social Cost of Growth: Evaluating GMV-Centric and Welfare-Centric Strategies in Online Food Delivery Platforms</title>
      <link>https://arxiv.org/abs/2410.16566</link>
      <description>arXiv:2410.16566v1 Announce Type: cross 
Abstract: This paper develops a comprehensive theoretical framework to analyze the trade-offs between Gross Merchandise Volume (GMV) maximization and social welfare optimization in online food delivery platforms. Using a multi-agent simulation and a dual-model approach based on two-sided market theory and welfare economics, we evaluate the impact of GMV-centric and welfare-centric strategies on platform dynamics, including pricing mechanisms, stakeholder welfare, and market efficiency. Our results show that GMV maximization strategies drive rapid short-term transaction growth but lead to uneven welfare distribution, particularly disadvantaging delivery workers. In contrast, welfare-centric strategies promote a more balanced and equitable distribution of benefits among consumers, restaurants, and delivery workers, enhancing platform sustainability in the long run. These findings provide actionable insights for platform operators and policymakers to design strategies that balance growth with social welfare, ensuring both economic efficiency and fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16566v1</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yukun Zhang</dc:creator>
    </item>
    <item>
      <title>ViMGuard: A Novel Multi-Modal System for Video Misinformation Guarding</title>
      <link>https://arxiv.org/abs/2410.16592</link>
      <description>arXiv:2410.16592v1 Announce Type: cross 
Abstract: The rise of social media and short-form video (SFV) has facilitated a breeding ground for misinformation. With the emergence of large language models, significant research has gone into curbing this misinformation problem with automatic false claim detection for text. Unfortunately, the automatic detection of misinformation in SFV is a more complex problem that remains largely unstudied. While text samples are monomodal (only containing words), SFVs comprise three different modalities: words, visuals, and non-linguistic audio. In this work, we introduce Video Masked Autoencoders for Misinformation Guarding (ViMGuard), the first deep-learning architecture capable of fact-checking an SFV through analysis of all three of its constituent modalities. ViMGuard leverages a dual-component system. First, Video and Audio Masked Autoencoders analyze the visual and non-linguistic audio elements of a video to discern its intention; specifically whether it intends to make an informative claim. If it is deemed that the SFV has informative intent, it is passed through our second component: a Retrieval Augmented Generation system that validates the factual accuracy of spoken words. In evaluation, ViMGuard outperformed three cutting-edge fact-checkers, thus setting a new standard for SFV fact-checking and marking a significant stride toward trustworthy news on social platforms. To promote further testing and iteration, VimGuard was deployed into a Chrome extension and all code was open-sourced on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16592v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Andrew Kan, Christopher Kan, Zaid Nabulsi</dc:creator>
    </item>
    <item>
      <title>SafetyAnalyst: Interpretable, transparent, and steerable LLM safety moderation</title>
      <link>https://arxiv.org/abs/2410.16665</link>
      <description>arXiv:2410.16665v1 Announce Type: cross 
Abstract: The ideal LLM content moderation system would be both structurally interpretable (so its decisions can be explained to users) and steerable (to reflect a community's values or align to safety standards). However, current systems fall short on both of these dimensions. To address this gap, we present SafetyAnalyst, a novel LLM safety moderation framework. Given a prompt, SafetyAnalyst creates a structured "harm-benefit tree," which identifies 1) the actions that could be taken if a compliant response were provided, 2) the harmful and beneficial effects of those actions (along with their likelihood, severity, and immediacy), and 3) the stakeholders that would be impacted by those effects. It then aggregates this structured representation into a harmfulness score based on a parameterized set of safety preferences, which can be transparently aligned to particular values. Using extensive harm-benefit features generated by SOTA LLMs on 19k prompts, we fine-tuned an open-weight LM to specialize in generating harm-benefit trees through symbolic knowledge distillation. On a comprehensive set of prompt safety benchmarks, we show that our system (average F1=0.75) outperforms existing LLM safety moderation systems (average F1$&lt;$0.72) on prompt harmfulness classification, while offering the additional advantages of interpretability and steerability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16665v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing-Jing Li, Valentina Pyatkin, Max Kleiman-Weiner, Liwei Jiang, Nouha Dziri, Anne G. E. Collins, Jana Schaich Borg, Maarten Sap, Yejin Choi, Sydney Levine</dc:creator>
    </item>
    <item>
      <title>AskBeacon -- Performing genomic data exchange and analytics with natural language</title>
      <link>https://arxiv.org/abs/2410.16700</link>
      <description>arXiv:2410.16700v2 Announce Type: cross 
Abstract: Enabling clinicians and researchers to directly interact with global genomic data resources by removing technological barriers is vital for medical genomics. AskBeacon enables Large Language Models to be applied to securely shared cohorts via the GA4GH Beacon protocol. By simply "asking" Beacon, actionable insights can be gained, analyzed and made publication-ready.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16700v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>q-bio.GN</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anuradha Wickramarachchi, Shakila Tonni, Sonali Majumdar, Sarvnaz Karimi, Sulev K\~oks, Brendan Hosking, Jordi Rambla, Natalie A. Twine, Yatish Jain, Denis C. Bauer</dc:creator>
    </item>
    <item>
      <title>Privacy-hardened and hallucination-resistant synthetic data generation with logic-solvers</title>
      <link>https://arxiv.org/abs/2410.16705</link>
      <description>arXiv:2410.16705v1 Announce Type: cross 
Abstract: Machine-generated data is a valuable resource for training Artificial Intelligence algorithms, evaluating rare workflows, and sharing data under stricter data legislations. The challenge is to generate data that is accurate and private. Current statistical and deep learning methods struggle with large data volumes, are prone to hallucinating scenarios incompatible with reality, and seldom quantify privacy meaningfully. Here we introduce Genomator, a logic solving approach (SAT solving), which efficiently produces private and realistic representations of the original data. We demonstrate the method on genomic data, which arguably is the most complex and private information. Synthetic genomes hold great potential for balancing underrepresented populations in medical research and advancing global data exchange. We benchmark Genomator against state-of-the-art methodologies (Markov generation, Restricted Boltzmann Machine, Generative Adversarial Network and Conditional Restricted Boltzmann Machines), demonstrating an 84-93% accuracy improvement and 95-98% higher privacy. Genomator is also 1000-1600 times more efficient, making it the only tested method that scales to whole genomes. We show the universal trade-off between privacy and accuracy, and use Genomator's tuning capability to cater to all applications along the spectrum, from provable private representations of sensitive cohorts, to datasets with indistinguishable pharmacogenomic profiles. Demonstrating the production-scale generation of tuneable synthetic data can increase trust and pave the way into the clinic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16705v1</guid>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mark A. Burgess, Brendan Hosking, Roc Reguant, Anubhav Kaphle, Mitchell J. O'Brien, Letitia M. F. Sng, Yatish Jain, Denis C. Bauer</dc:creator>
    </item>
    <item>
      <title>Revealing Hidden Bias in AI: Lessons from Large Language Models</title>
      <link>https://arxiv.org/abs/2410.16927</link>
      <description>arXiv:2410.16927v1 Announce Type: cross 
Abstract: As large language models (LLMs) become integral to recruitment processes, concerns about AI-induced bias have intensified. This study examines biases in candidate interview reports generated by Claude 3.5 Sonnet, GPT-4o, Gemini 1.5, and Llama 3.1 405B, focusing on characteristics such as gender, race, and age. We evaluate the effectiveness of LLM-based anonymization in reducing these biases. Findings indicate that while anonymization reduces certain biases, particularly gender bias, the degree of effectiveness varies across models and bias types. Notably, Llama 3.1 405B exhibited the lowest overall bias. Moreover, our methodology of comparing anonymized and non-anonymized data reveals a novel approach to assessing inherent biases in LLMs beyond recruitment applications. This study underscores the importance of careful LLM selection and suggests best practices for minimizing bias in AI applications, promoting fairness and inclusivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16927v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Django Beatty, Kritsada Masanthia, Teepakorn Kaphol, Niphan Sethi</dc:creator>
    </item>
    <item>
      <title>Science Out of Its Ivory Tower: Improving Accessibility with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2410.17088</link>
      <description>arXiv:2410.17088v1 Announce Type: cross 
Abstract: A vast amount of scholarly work is published daily, yet much of it remains inaccessible to the general public due to dense jargon and complex language. To address this challenge in science communication, we introduce a reinforcement learning framework that fine-tunes a language model to rewrite scholarly abstracts into more comprehensible versions. Guided by a carefully balanced combination of word- and sentence-level accessibility rewards, our language model effectively substitutes technical terms with more accessible alternatives, a task which models supervised fine-tuned or guided by conventional readability measures struggle to accomplish. Our best model adjusts the readability level of scholarly abstracts by approximately six U.S. grade levels -- in other words, from a postgraduate to a high school level. This translates to roughly a 90% relative boost over the supervised fine-tuning baseline, all while maintaining factual accuracy and high-quality language. An in-depth analysis of our approach shows that balanced rewards lead to systematic modifications in the base model, likely contributing to smoother optimization and superior performance. We envision this work as a step toward bridging the gap between scholarly research and the general public, particularly younger readers and those without a college degree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17088v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Haining Wang, Jason Clark, Hannah McKelvey, Leila Sterman, Zheng Gao, Zuoyu Tian, Sandra K\"ubler, Xiaozhong Liu</dc:creator>
    </item>
    <item>
      <title>Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh through Large Language Modeling</title>
      <link>https://arxiv.org/abs/2410.17210</link>
      <description>arXiv:2410.17210v1 Announce Type: cross 
Abstract: Purpose: Bangladesh's legal system struggles with major challenges like delays, complexity, high costs, and millions of unresolved cases, which deter many from pursuing legal action due to lack of knowledge or financial constraints. This research seeks to develop a specialized Large Language Model (LLM) to assist in the Bangladeshi legal system. Methods: We created UKIL-DB-EN, an English corpus of Bangladeshi legal documents, by collecting and scraping data on various legal acts. We fine-tuned the GPT-2 model on this dataset to develop GPT2-UKIL-EN, an LLM focused on providing legal assistance in English. Results: The model was rigorously evaluated using semantic assessments, including case studies supported by expert opinions. The evaluation provided promising results, demonstrating the potential for the model to assist in legal matters within Bangladesh. Conclusion: Our work represents the first structured effort toward building an AI-based legal assistant for Bangladesh. While the results are encouraging, further refinements are necessary to improve the model's accuracy, credibility, and safety. This is a significant step toward creating a legal AI capable of serving the needs of a population of 180 million.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17210v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Azmine Toushik Wasi, Wahid Faisal, Mst Rafia Islam, Mahathir Mohammad Bappy</dc:creator>
    </item>
    <item>
      <title>Dhoroni: Exploring Bengali Climate Change and Environmental Views with a Multi-Perspective News Dataset and Natural Language Processing</title>
      <link>https://arxiv.org/abs/2410.17225</link>
      <description>arXiv:2410.17225v1 Announce Type: cross 
Abstract: Climate change poses critical challenges globally, disproportionately affecting low-income countries that often lack resources and linguistic representation on the international stage. Despite Bangladesh's status as one of the most vulnerable nations to climate impacts, research gaps persist in Bengali-language studies related to climate change and NLP. To address this disparity, we introduce Dhoroni, a novel Bengali (Bangla) climate change and environmental news dataset, comprising a 2300 annotated Bangla news articles, offering multiple perspectives such as political influence, scientific/statistical data, authenticity, stance detection, and stakeholder involvement. Furthermore, we present an in-depth exploratory analysis of Dhoroni and introduce BanglaBERT-Dhoroni family, a novel baseline model family for climate and environmental opinion detection in Bangla, fine-tuned on our dataset. This research contributes significantly to enhancing accessibility and analysis of climate discourse in Bengali (Bangla), addressing crucial communication and research gaps in climate-impacted regions like Bangladesh with 180 million people.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17225v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Azmine Toushik Wasi, Wahid Faisal, Taj Ahmad, Abdur Rahman, Mst Rafia Islam</dc:creator>
    </item>
    <item>
      <title>From Text to Multimodality: Exploring the Evolution and Impact of Large Language Models in Medical Practice</title>
      <link>https://arxiv.org/abs/2410.01812</link>
      <description>arXiv:2410.01812v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have rapidly evolved from text-based systems to multimodal platforms, significantly impacting various sectors including healthcare. This comprehensive review explores the progression of LLMs to Multimodal Large Language Models (MLLMs) and their growing influence in medical practice. We examine the current landscape of MLLMs in healthcare, analyzing their applications across clinical decision support, medical imaging, patient engagement, and research. The review highlights the unique capabilities of MLLMs in integrating diverse data types, such as text, images, and audio, to provide more comprehensive insights into patient health. We also address the challenges facing MLLM implementation, including data limitations, technical hurdles, and ethical considerations. By identifying key research gaps, this paper aims to guide future investigations in areas such as dataset development, modality alignment methods, and the establishment of ethical guidelines. As MLLMs continue to shape the future of healthcare, understanding their potential and limitations is crucial for their responsible and effective integration into medical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01812v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Qian Niu, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Lawrence KQ Yan, Yichao Zhang, Caitlyn Heqi Yin, Cheng Fei, Junyu Liu, Benji Peng</dc:creator>
    </item>
    <item>
      <title>Evaluation of Study Plans using Partial Orders</title>
      <link>https://arxiv.org/abs/2410.03314</link>
      <description>arXiv:2410.03314v2 Announce Type: replace 
Abstract: In higher education, data is collected that indicate the term(s) that a course is taken and when it is passed. Often, study plans propose a suggested course order to students. Study planners can adjust these based on detected deviations between the proposed and actual order of the courses being taken. In this work, we detect deviations by combining (1) the deviation between the proposed and actual course order with (2) the temporal difference between the expected and actual course-taking term(s). Partially ordered alignments identify the deviations between the proposed and actual order. We compute a partial order alignment by modeling a study plan as a process model and a student's course-taking behavior as a partial order. Using partial orders in such use cases allows one to relax the constraints of strictly ordered traces. This makes our approach less prone to the order in which courses are offered. Further, when modeling course-taking behavior as partial orders, we propose distinguishing intended course-taking behavior from actual course-passing behavior of students by including either all terms in which a course is attempted or only the term that a course is passed, respectively. This provides more perspectives when comparing the proposed and actual course-taking behavior. The proposed deviation measuring approach is evaluated on real-life data from RWTH Aachen University.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03314v2</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Rennert, Mahsa Pourbafrani, Wil van der Aalst</dc:creator>
    </item>
    <item>
      <title>System 2 thinking in OpenAI's o1-preview model: Near-perfect performance on a mathematics exam</title>
      <link>https://arxiv.org/abs/2410.07114</link>
      <description>arXiv:2410.07114v3 Announce Type: replace 
Abstract: The processes underlying human cognition are often divided into System 1, which involves fast, intuitive thinking, and System 2, which involves slow, deliberate reasoning. Previously, large language models were criticized for lacking the deeper, more analytical capabilities of System 2. In September 2024, OpenAI introduced the o1 model series, designed to handle System 2-like reasoning. While OpenAI's benchmarks are promising, independent validation is still needed. In this study, we tested the o1-preview model twice on the Dutch 'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76 points. For context, only 24 out of 16,414 students in the Netherlands achieved a perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76, well above the Dutch average of 40.63 points. Neither model had access to the exam figures. Since there was a risk of model contamination (i.e., the knowledge cutoff of o1-preview and GPT-4o was after the exam was published online), we repeated the procedure with a new Mathematics B exam that was published after the cutoff date. The results again indicated that o1-preview performed strongly (97.8th percentile), which suggests that contamination was not a factor. We also show that there is some variability in the output of o1-preview, which means that sometimes there is 'luck' (the answer is correct) or 'bad luck' (the output has diverged into something that is incorrect). We demonstrate that a self-consistency approach, where repeated prompts are given and the most common answer is selected, is a useful strategy for identifying the correct answer. It is concluded that while OpenAI's new model series holds great potential, certain risks must be considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07114v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joost de Winter, Dimitra Dodou, Yke Bauke Eisma</dc:creator>
    </item>
    <item>
      <title>Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework</title>
      <link>https://arxiv.org/abs/2311.18460</link>
      <description>arXiv:2311.18460v3 Announce Type: replace-cross 
Abstract: Fairness for machine learning predictions is widely required in practice for legal, ethical, and societal reasons. Existing work typically focuses on settings without unobserved confounding, even though unobserved confounding can lead to severe violations of causal fairness and, thus, unfair predictions. In this work, we analyze the sensitivity of causal fairness to unobserved confounding. Our contributions are three-fold. First, we derive bounds for causal fairness metrics under different sources of unobserved confounding. This enables practitioners to examine the sensitivity of their machine learning models to unobserved confounding in fairness-critical applications. Second, we propose a novel neural framework for learning fair predictions, which allows us to offer worst-case guarantees of the extent to which causal fairness can be violated due to unobserved confounding. Third, we demonstrate the effectiveness of our framework in a series of experiments, including a real-world case study about predicting prison sentences. To the best of our knowledge, ours is the first work to study causal fairness under unobserved confounding. To this end, our work is of direct practical value as a refutation strategy to ensure the fairness of predictions in high-stakes applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18460v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Published as a conference paper at ICLR 2024</arxiv:journal_reference>
      <dc:creator>Maresa Schr\"oder, Dennis Frauen, Stefan Feuerriegel</dc:creator>
    </item>
    <item>
      <title>Knowledge Distillation-Based Model Extraction Attack using GAN-based Private Counterfactual Explanations</title>
      <link>https://arxiv.org/abs/2404.03348</link>
      <description>arXiv:2404.03348v2 Announce Type: replace-cross 
Abstract: In recent years, there has been a notable increase in the deployment of machine learning (ML) models as services (MLaaS) across diverse production software applications. In parallel, explainable AI (XAI) continues to evolve, addressing the necessity for transparency and trustworthiness in ML models. XAI techniques aim to enhance the transparency of ML models by providing insights, in terms of model's explanations, into their decision-making process. Simultaneously, some MLaaS platforms now offer explanations alongside the ML prediction outputs. This setup has elevated concerns regarding vulnerabilities in MLaaS, particularly in relation to privacy leakage attacks such as model extraction attacks (MEA). This is due to the fact that explanations can unveil insights about the inner workings of the model which could be exploited by malicious users. In this work, we focus on investigating how model explanations, particularly counterfactual explanations (CFs), can be exploited for performing MEA within the MLaaS platform. We also delve into assessing the effectiveness of incorporating differential privacy (DP) as a mitigation strategy. To this end, we first propose a novel approach for MEA based on Knowledge Distillation (KD) to enhance the efficiency of extracting a substitute model of a target model exploiting CFs, without any knowledge about the training data distribution by the attacker. Then, we advise an approach for training CF generators incorporating DP to generate private CFs. We conduct thorough experimental evaluations on real-world datasets and demonstrate that our proposed KD-based MEA can yield a high-fidelity substitute model with a reduced number of queries with respect to baseline approaches. Furthermore, our findings reveal that including a privacy layer can allow mitigating the MEA. However, on the account of the quality of CFs, impacts the performance of the explanations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03348v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fatima Ezzeddine, Omran Ayoub, Silvia Giordano</dc:creator>
    </item>
    <item>
      <title>Slicing Through Bias: Explaining Performance Gaps in Medical Image Analysis using Slice Discovery Methods</title>
      <link>https://arxiv.org/abs/2406.12142</link>
      <description>arXiv:2406.12142v2 Announce Type: replace-cross 
Abstract: Machine learning models have achieved high overall accuracy in medical image analysis. However, performance disparities on specific patient groups pose challenges to their clinical utility, safety, and fairness. This can affect known patient groups - such as those based on sex, age, or disease subtype - as well as previously unknown and unlabeled groups. Furthermore, the root cause of such observed performance disparities is often challenging to uncover, hindering mitigation efforts. In this paper, to address these issues, we leverage Slice Discovery Methods (SDMs) to identify interpretable underperforming subsets of data and formulate hypotheses regarding the cause of observed performance disparities. We introduce a novel SDM and apply it in a case study on the classification of pneumothorax and atelectasis from chest x-rays. Our study demonstrates the effectiveness of SDMs in hypothesis formulation and yields an explanation of previously observed but unexplained performance disparities between male and female patients in widely used chest X-ray datasets and models. Our findings indicate shortcut learning in both classification tasks, through the presence of chest drains and ECG wires, respectively. Sex-based differences in the prevalence of these shortcut features appear to cause the observed classification performance gap, representing a previously underappreciated interaction between shortcut learning and model fairness analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12142v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-72787-0_1</arxiv:DOI>
      <dc:creator>Vincent Olesen, Nina Weng, Aasa Feragen, Eike Petersen</dc:creator>
    </item>
    <item>
      <title>LLMs left, right, and center: Assessing GPT's capabilities to label political bias from web domains</title>
      <link>https://arxiv.org/abs/2407.14344</link>
      <description>arXiv:2407.14344v2 Announce Type: replace-cross 
Abstract: This research investigates whether OpenAI's GPT-4, a state-of-the-art large language model, can accurately classify the political bias of news sources based solely on their URLs. Given the subjective nature of political labels, third-party bias ratings like those from Ad Fontes Media, AllSides, and Media Bias/Fact Check (MBFC) are often used in research to analyze news source diversity. This study aims to determine if GPT-4 can replicate these human ratings on a seven-degree scale ("far-left" to "far-right"). The analysis compares GPT-4's classifications against MBFC's, and controls for website popularity using Open PageRank scores. Findings reveal a high correlation ($\text{Spearman's } \rho = .89$, $n = 5,877$, $p &lt; 0.001$) between GPT-4's and MBFC's ratings, indicating the model's potential reliability. However, GPT-4 abstained from classifying approximately $\frac{2}{3}$ of the dataset. It is more likely to abstain from rating unpopular websites, which also suffer from less accurate assessments. The LLM tends to avoid classifying sources that MBFC considers to be centrist, resulting in more polarized outputs. Finally, this analysis shows a slight leftward skew in GPT's classifications compared to MBFC's. Therefore, while this paper suggests that while GPT-4 can be a scalable, cost-effective tool for political bias classification of news websites, its use should be as a complement to human judgment to mitigate biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14344v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Hernandes, Giulio Corsi</dc:creator>
    </item>
    <item>
      <title>The Impact of Large Language Models in Academia: from Writing to Speaking</title>
      <link>https://arxiv.org/abs/2409.13686</link>
      <description>arXiv:2409.13686v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly impacting human society, particularly in textual information. Based on more than 30,000 papers and 1,000 presentations from machine learning conferences, we examined and compared the words used in writing and speaking, representing the first large-scale study of how LLMs influence the two main modes of verbal communication and expression within the same group of people. Our empirical results show that LLM-style words such as "significant" have been used more frequently in abstracts and oral presentations. The impact on speaking is beginning to emerge and is likely to grow in the future, calling attention to the implicit influence and ripple effect of LLMs on human society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13686v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingmeng Geng, Caixi Chen, Yanru Wu, Dongping Chen, Yao Wan, Pan Zhou</dc:creator>
    </item>
    <item>
      <title>Metamorphic Debugging for Accountable Software</title>
      <link>https://arxiv.org/abs/2409.16140</link>
      <description>arXiv:2409.16140v2 Announce Type: replace-cross 
Abstract: As the laws have become more complicated and enormous, the role of software systems in navigating and understanding these intricacies has become more critical. Given their socio-economic and legally critical implications, ensuring software accountability -- encompassing qualities such as legal compliance, explainability, perceptions of procedural justice, fairness of outcomes, and confidentiality/privacy -- is of paramount social importance. Moreover, software that accurately interprets its requirements, complies with legal standards and upholds social fairness can serve as a surrogate for legal and social norms, enabling policymakers to inquire about the law as seamlessly as a software engineer conducts a test. However, ensuring software accountability faces three key challenges: i) Translating legalese into formal specifications, ii) Lack of a definitive 'truth' for queries (the oracle problem), and iii) Scarcity of trustworthy datasets due to privacy and legal concerns.
  Drawing from the experiences in debugging U.S. tax preparation software, we propose that these challenges can be tackled by focusing on relational specifications. While the exact output for a given input may be unknown, the relationship between the outputs of two related inputs may be easier to express. This observation resembles i) the legal doctrine of precedent, meaning that similar cases must yield similar rulings; and ii) metamorphic relation (MR) in software engineering that requires a specific relation between software inputs and outputs. We propose metamorphic debugging as the foundation for detecting, explaining, and repairing socio-legal software for these relations. We showcase recent results that leverage metamorphic debugging to detect and explain accountability bugs in tax prep and poverty management software systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16140v2</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.PL</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saeid Tizpaz-Niari, Shiva Darian, Ashutosh Trivedi</dc:creator>
    </item>
    <item>
      <title>ABCFair: an Adaptable Benchmark approach for Comparing Fairness Methods</title>
      <link>https://arxiv.org/abs/2409.16965</link>
      <description>arXiv:2409.16965v2 Announce Type: replace-cross 
Abstract: Numerous methods have been implemented that pursue fairness with respect to sensitive features by mitigating biases in machine learning. Yet, the problem settings that each method tackles vary significantly, including the stage of intervention, the composition of sensitive features, the fairness notion, and the distribution of the output. Even in binary classification, these subtle differences make it highly complicated to benchmark fairness methods, as their performance can strongly depend on exactly how the bias mitigation problem was originally framed.
  Hence, we introduce ABCFair, a benchmark approach which allows adapting to the desiderata of the real-world problem setting, enabling proper comparability between methods for any use case. We apply ABCFair to a range of pre-, in-, and postprocessing methods on both large-scale, traditional datasets and on a dual label (biased and unbiased) dataset to sidestep the fairness-accuracy trade-off.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16965v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>MaryBeth Defrance, Maarten Buyl, Tijl De Bie</dc:creator>
    </item>
    <item>
      <title>Generative Image Steganography Based on Point Cloud</title>
      <link>https://arxiv.org/abs/2410.11673</link>
      <description>arXiv:2410.11673v2 Announce Type: replace-cross 
Abstract: In deep steganography, the model size is usually related to the underlying mesh resolution, and a separate neural network needs to be trained as a message extractor. In this paper, we propose a generative image steganography based on point cloud representation, which represents image data as a point cloud, learns the distribution of the point cloud data, and represents it in the form of a continuous function. This method breaks through the limitation of the image resolution, and can generate images with arbitrary resolution according to the actual need, and omits the need for explicit data for image steganography. At the same time, using a fixed point cloud extractor transfers the training of the network to the point cloud data, which saves the training time and avoids the risk of exposing the steganography behavior caused by the transmission of the message extractor. Experiments prove that the steganographic images generated by the scheme have very high image quality and the accuracy of message extraction reaches more than 99%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11673v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhong Yangjie, Liu Jia, Liu Meiqi, Ke Yan, Zhang Minqing</dc:creator>
    </item>
    <item>
      <title>Boardwalk Empire: How Generative AI is Revolutionizing Economic Paradigms</title>
      <link>https://arxiv.org/abs/2410.15212</link>
      <description>arXiv:2410.15212v2 Announce Type: replace-cross 
Abstract: The relentless pursuit of technological advancements has ushered in a new era where artificial intelligence (AI) is not only a powerful tool but also a critical economic driver. At the forefront of this transformation is Generative AI, which is catalyzing a paradigm shift across industries. Deep generative models, an integration of generative and deep learning techniques, excel in creating new data beyond analyzing existing ones, revolutionizing sectors from production and manufacturing to finance. By automating design, optimization, and innovation cycles, Generative AI is reshaping core industrial processes. In the financial sector, it is transforming risk assessment, trading strategies, and forecasting, demonstrating its profound impact. This paper explores the sweeping changes driven by deep learning models like Large Language Models (LLMs), highlighting their potential to foster innovative business models, disruptive technologies, and novel economic landscapes. As we stand at the threshold of an AI-driven economic era, Generative AI is emerging as a pivotal force, driving innovation, disruption, and economic evolution on a global scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15212v2</guid>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subramanyam Sahoo, Kamlesh Dutta</dc:creator>
    </item>
  </channel>
</rss>
