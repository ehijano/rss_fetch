<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Aug 2025 01:38:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A General Approach to Visualizing Uncertainty in Statistical Graphics</title>
      <link>https://arxiv.org/abs/2508.00937</link>
      <description>arXiv:2508.00937v1 Announce Type: new 
Abstract: Visualizing uncertainty is integral to data analysis, yet its application is often hindered by the need for specialized methods for quantifying and representing uncertainty for different types of graphics. We introduce a general approach that simplifies this process. The core idea is to treat the statistical graphic as a function of the underlying distribution. Instead of first calculating uncertainty metrics and then plotting them, the method propagates uncertainty through to the visualization. By repeatedly sampling from the data distribution and generating a complete statistical graphic for each sample, a distribution over graphics is produced. These graphics are aggregated pixel-by-pixel to create a single, static image. This approach is versatile, requires no specific knowledge from the user beyond how to create the basic statistical graphic, and comes with theoretical coverage guarantees for standard cases such as confidence intervals and bands. We provide a reference implementation as a Python library to demonstrate the method's utility. Our approach not only reproduces conventional uncertainty visualizations for point estimates and regression lines but also seamlessly extends to non-standard cases, including pie charts, stacked bar charts, and tables. This approach makes uncertainty visualization more accessible to practitioners and can be a valuable tool for teaching uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00937v1</guid>
      <category>stat.ME</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernarda Petek, David Nabergoj, Erik \v{S}trumbelj</dc:creator>
    </item>
    <item>
      <title>Frugal, Flexible, Faithful: Causal Data Simulation via Frengression</title>
      <link>https://arxiv.org/abs/2508.01018</link>
      <description>arXiv:2508.01018v1 Announce Type: new 
Abstract: Machine learning has revitalized causal inference by combining flexible models and principled estimators, yet robust benchmarking and evaluation remain challenging with real-world data. In this work, we introduce frengression, a deep generative realization of the frugal parameterization that models the joint distribution of covariates, treatments and outcomes around the causal margin of interest. Frengression provides accurate estimation and flexible, faithful simulation of multivariate, time-varying data; it also enables direct sampling from user-specified interventional distributions. Model consistency and extrapolation guarantees are established, with validation on real-world clinical trial data demonstrating frengression's practical utility. We envision this framework sparking new research into generative approaches for causal margin modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01018v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linying Yang, Robin J. Evans, Xinwei Shen</dc:creator>
    </item>
    <item>
      <title>A Comparative Evaluation of Statistical Methods in Hybrid Controlled Trials</title>
      <link>https://arxiv.org/abs/2508.01052</link>
      <description>arXiv:2508.01052v1 Announce Type: new 
Abstract: Randomized clinical trials (RCTs) are widely considered the gold standard for evaluating the effectiveness of new treatments or interventions in drug development. Still, they may not be feasible in certain cases, such as with rare diseases where randomization to a control group is ethically challenging. In such scenarios, external data can complement either a single-arm trial or a hybrid-controlled trial. The hybrid-control design involves enrolling fewer concurrent control patients and then supplementing the control arm using external or historical data. Various statistical approaches, including frequentist methods (e.g., propensity score methods), Bayesian borrowing approaches (e.g., meta-analytic-predictive prior), and their integration, have been utilized to incorporate external information in hybrid-controlled trials. We evaluate several accessible methods for their robustness to between-study heterogeneity and unmeasured confounding utilizing a case study based on data from the DAPA-HF trial, along with comprehensive simulation studies. Our findings indicate that the optimal methods must take into account the heterogeneities from both measured and unmeasured confounding. Since no single method consistently outperforms all others, researchers should explore multiple methods through extensive simulations to evaluate their effectiveness across various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01052v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Di Ran, Fanni Zhang, Sima Shahsavari, Kristine Broglio, Alasdair Henderson, Binbing Yu</dc:creator>
    </item>
    <item>
      <title>Constructing new probability distributions on the unit interval</title>
      <link>https://arxiv.org/abs/2508.01154</link>
      <description>arXiv:2508.01154v1 Announce Type: new 
Abstract: In this paper, we introduce a novel method for constructing probability distributions on the unit interval by exploiting the non-injective transformation defined by the ratio of two positive random variables, $X$ and $Y$. For simplicity and tractability, we focus on independent random variables $X$ and $Y$ following gamma distributions. We derive some results including laws, density functions, quantile function and closed-form expressions for the moments, and discuss maximum likelihood estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01154v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo, Poliana Matos, Subhankar Dutta</dc:creator>
    </item>
    <item>
      <title>To Vary or Not To Vary: A Flexible Empirical Bayes Factor for Testing Variance Components</title>
      <link>https://arxiv.org/abs/2508.01403</link>
      <description>arXiv:2508.01403v1 Announce Type: new 
Abstract: Random effects are the gold standard for capturing structural heterogeneity in data, such as spatial dependencies, individual differences, or temporal dependencies. However, testing for their presence is challenging, as it involves a variance component constrained to be non-negative -- a boundary problem. This paper proposes a flexible empirical Bayes factor (EBF) for testing random effects. Rather than testing whether a variance component is zero, the EBF tests the equivalent hypothesis that all random effects are zero. Crucially, it avoids manual prior specification based on external knowledge, as the distribution of random effects is part of the model's lower level and estimated from the data -- yielding an "empirical" Bayes factor. The EBF uses a Savage-Dickey density ratio, allowing all random effects to be tested using only the full model fit. This eliminates the need to fit multiple models with different combinations of random effects. Simulations on synthetic data evaluate the criterion's general behavior. To demonstrate its flexibility, the EBF is applied to generalized linear crossed mixed models, spatial random effects models, dynamic structural equation models, random intercept cross-lagged panel models, and nonlinear mixed effects models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01403v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabio Vieira, Hongwei Zhao, Joris Mulder</dc:creator>
    </item>
    <item>
      <title>Bayesian Conformal Prediction via the Bayesian Bootstrap</title>
      <link>https://arxiv.org/abs/2508.01418</link>
      <description>arXiv:2508.01418v1 Announce Type: new 
Abstract: Reliable uncertainty quantification remains a central challenge in predictive modeling. While Bayesian methods are theoretically appealing, their predictive intervals can exhibit poor frequentist calibration, particularly with small sample sizes or model misspecification. We introduce a practical and broadly applicable Bayesian conformal approach based on the influence-function Bayesian bootstrap (BB) with data-driven tuning of the Dirichlet concentration parameter, {\alpha}. By efficiently approximating the Bayesian bootstrap predictive distribution via influence functions and calibrating {\alpha} to optimize empirical coverage or average log-probability, our method constructs prediction intervals and distributions that are both well-calibrated and sharp. Across a range of regression models and data settings, this Bayesian conformal framework consistently yields improved empirical coverage and log-score compared to standard Bayesian posteriors. Our procedure is fast, easy to implement, and offers a flexible approach for distributional calibration in predictive modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01418v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Graham Gibson</dc:creator>
    </item>
    <item>
      <title>Modeling high and low extremes with a novel dynamic spatio-temporal model</title>
      <link>https://arxiv.org/abs/2508.01481</link>
      <description>arXiv:2508.01481v1 Announce Type: new 
Abstract: Extreme environmental events such as severe storms, drought, heat waves, flash floods, and abrupt species collapse have become more prevalent in the earth-atmosphere dynamic system in recent years. In order to fully understand the underlying mechanisms and enhance informed decision-making, a flexible model capable of accommodating extremes is necessary. Existing dynamic spatio-temporal statistical models exhibit limitations in capturing extremes when assuming Gaussian error distributions, whereas the current models for spatial extremes mostly assume temporal independence and are focused on joint upper tails at two or more locations. Here, we introduce a new class of dynamic spatio-temporal models that capture both high and low extremes using a mixture of heavy- and light-tailed distributions with varying tail indices. Our framework flexibly identifies extremal dependence and independence in both space and time with uncertainty quantification and supports missing data prediction, as in other dynamic spatio-temporal models. We demonstrate its effectiveness using a large reanalysis dataset of hourly particulate matter in the Central United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01481v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Myungsoo Yoo, Likun Zhang, Christopher K. Wikle, Thomas Opitz</dc:creator>
    </item>
    <item>
      <title>A strategy to avoid particle depletion in recursive Bayesian inference</title>
      <link>https://arxiv.org/abs/2508.01572</link>
      <description>arXiv:2508.01572v1 Announce Type: new 
Abstract: Recursive Bayesian inference, in which posterior beliefs are updated in light of accumulating data, is a tool for implementing Bayesian models in applications with streaming and/or very large data sets. As the posterior of one iteration becomes the prior for the next, beliefs are updated sequentially instead of all-at-once. Thus, recursive inference is relevant for both streaming data and settings where data too numerous to be analyzed together can be partitioned into manageable pieces. In practice, posteriors are characterized by samples obtained using, e.g., acceptance/rejection sampling in which draws from the posterior of one iteration are used as proposals for the next. While simple to implement, such filtering approaches suffer from particle depletion, degrading each sample's ability to represent its target posterior. As a remedy, we investigate generating proposals from a smoothed version of the preceding sample's empirical distribution. The method retains computationally valuable properties of similar methods, but without particle depletion, and we demonstrate its accuracy in simulation. We apply the method to data simulated from both a simple, logistic regression model as well as a hierarchical model originally developed for classifying forest vegetation in New Mexico using satellite imagery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01572v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Henry R. Scharf</dc:creator>
    </item>
    <item>
      <title>Sample size calculations for multilevel factorial longitudinal cluster randomised trials</title>
      <link>https://arxiv.org/abs/2508.01610</link>
      <description>arXiv:2508.01610v1 Announce Type: new 
Abstract: Typically, trials investigate the impact of either an individual-level intervention on participant outcomes, or the impact of a cluster-level intervention on participant outcomes. Factorial designs consider two (or more) treatments for each of two (or more) different factors. In factorial trial designs, trial units (individuals or clusters) are each randomised to a level of each of the treatments; these designs allow assessment of the interactions between different interventions. Recently, there has been growing interest in the design of trials that jointly assess the impact of individual- and cluster-level interventions (i.e. multi-level interventions); requiring the development of methodology that accommodates randomisation at multiple levels. While recent work has developed sample size methodology for variants combining standard cluster randomisation and individual randomisation, that work does not apply to longitudinal cluster randomised trial designs such as the stepped wedge design or cluster randomised crossover design. Here we present dedicated sample size methodology for "split-plot factorial longitudinal cluster randomised trials" with continuous outcomes: allowing for joint assessment of individual-level and cluster-level interventions that allows for the impact of the cluster-level intervention to be assessed using any longitudinal cluster randomised trial design. We show how the power to detect given effects of the individual-level intervention, the cluster-level intervention, and the interaction between the two depends on standard results for individually-randomised trials and longitudinal cluster randomised trials. We apply these results to the SharES trial, which considered the effects of a patient- and clinician-level interventions for patients with breast cancer on patient knowledge about the risks and benefits of treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01610v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rhys Bowden, Jessica Kasza, Andrew Forbes</dc:creator>
    </item>
    <item>
      <title>Density estimation with atoms, and functional estimation for mixed discrete-continuous data</title>
      <link>https://arxiv.org/abs/2508.01706</link>
      <description>arXiv:2508.01706v1 Announce Type: new 
Abstract: In classical density (or density-functional) estimation, it is standard to assume that the underlying distribution has a density with respect to the Lebesgue measure. However, when the data distribution is a mixture of continuous and discrete components, the resulting methods are inconsistent in theory and perform poorly in practice. In this paper, we point out that a minor modification of existing methods for nonparametric density (functional) estimation can allow us to fully remove this assumption while retaining nearly identical theoretical guarantees and improved empirical performance. Our approach is very simple: data points that appear exactly once are likely to originate from the continuous component, whereas repeated observations are indicative of the discrete part. Leveraging this observation, we modify existing estimators for a broad class of functionals of the continuous component of the mixture; this modification is a "wrapper" in the sense that the user can use any underlying method of their choice for continuous density functional estimation. Our modifications deliver consistency without requiring knowledge of the discrete support, the mixing proportion, and without imposing additional assumptions beyond those needed in the absence of the discrete part. Thus, various theorems and existing software packages can be made automatically more robust, with absolutely no additional price when the data is not truly mixed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01706v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aytijhya Saha, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Bayesian Smoothed Quantile Regression</title>
      <link>https://arxiv.org/abs/2508.01738</link>
      <description>arXiv:2508.01738v1 Announce Type: new 
Abstract: Bayesian quantile regression based on the asymmetric Laplace distribution (ALD) likelihood suffers from two fundamental limitations: the non-differentiability of the check loss precludes gradient-based Markov chain Monte Carlo (MCMC) methods, and the posterior mean provides biased quantile estimates. We propose Bayesian smoothed quantile regression (BSQR), which replaces the check loss with a kernel-smoothed version, creating a continuously differentiable likelihood. This smoothing has two crucial consequences: it enables efficient Hamiltonian Monte Carlo sampling, and it yields a consistent posterior distribution, thereby resolving the inferential bias of the standard approach. We further establish conditions for posterior propriety under various priors (including improper and hierarchical) and characterize how kernel choice affects posterior concentration and computational efficiency. Extensive simulations validate our theoretical findings, demonstrating that BSQR achieves up to a 50% reduction in predictive check loss at extreme quantiles compared to ALD-based methods, while improving MCMC efficiency by 20-40% in effective sample size. An empirical application to financial risk measurement during the COVID-19 era illustrates BSQR's practical advantages in capturing dynamic systemic risk. The BSQR framework provides a theoretically-grounded and computationally-efficient solution to longstanding challenges in Bayesian quantile regression, with compact-support kernels like the uniform and triangular emerging as particularly effective choices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01738v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingqi Liu, Kangqiang Li, Tianxiao Pang</dc:creator>
    </item>
    <item>
      <title>Decision Theory For Large Scale Outlier Detection Using Aleatoric Uncertainty: With a Note on Bayesian FDR</title>
      <link>https://arxiv.org/abs/2508.01988</link>
      <description>arXiv:2508.01988v1 Announce Type: new 
Abstract: Aleatoric and Epistemic uncertainty have achieved recent attention in the literature as different sources from which uncertainty can emerge in stochastic modeling. Epistemic being intrinsic or model based notions of uncertainty, and aleatoric being the uncertainty inherent in the data. We propose a novel decision theoretic framework for outlier detection in the context of aleatoric uncertainty; in the context of Bayesian modeling. The model incorporates bayesian false discovery rate control for multiplicty adjustment, and a new generalization of Bayesian FDR is introduced. The model is applied to simulations based on temporally fluctuating outlier detection where fixing thresholds often results in poor performance due to nonstationarity, and a case study is outlined on on a novel cybersecurity detection. Cyberthreat signals are highly nonstationary; giving a credible stress test of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01988v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ryan Warnick</dc:creator>
    </item>
    <item>
      <title>Estimation of Bivariate Normal Distributions from Marginal Summaries in Clinical Trials</title>
      <link>https://arxiv.org/abs/2508.02057</link>
      <description>arXiv:2508.02057v1 Announce Type: new 
Abstract: In certain privacy-sensitive scenarios within fields such as clinical trial simulations, federated learning, and distributed learning, researchers often face the challenge of estimating correlations between variables without access to individual-level data. To address this issue, we propose a novel method to estimate the correlation of bivariate normal variables using marginal information from multiple datasets. The method, based on maximum likelihood estimation (MLE), accommodates datasets with varying sample sizes and avoids reliance on sensitive information such as sample covariances, making it particularly suitable for privacy-restricted settings. Extensive simulation studies demonstrate the proposed method's effectiveness in accurately estimating correlations and its robustness across diverse data configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02057v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Longwen Shang, Min Tsao, Xuekui Zhang</dc:creator>
    </item>
    <item>
      <title>Factor-Driven Network Informed Restricted Vector Autoregression</title>
      <link>https://arxiv.org/abs/2508.02198</link>
      <description>arXiv:2508.02198v1 Announce Type: new 
Abstract: High-dimensional financial time series often exhibit complex dependence relations driven by both common market structures and latent connections among assets. To capture these characteristics, this paper proposes Factor-Driven Network Informed Restricted Vector Autoregression (FNIRVAR), a model for the common and idiosyncratic components of high-dimensional time series with an underlying unobserved network structure. The common component is modelled by a static factor model, which allows for strong cross-sectional dependence, whilst a network vector autoregressive process captures the residual co-movements due to the idiosyncratic component. An assortative stochastic block model underlies the network VAR, leading to groups of highly co-moving variables in the idiosyncratic component. For estimation, a two-step procedure is proposed, whereby the static factors are estimated via principal component analysis, followed by estimation of the network VAR parameters. The method is demonstrated in financial applications to daily returns, intraday returns, and FRED-MD macroeconomic variables. In all cases, the proposed method outperforms a static factor model, as well as a static factor plus LASSO-estimated sparse VAR model, in terms of forecasting and financial performance metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02198v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brendan Martin, Mihai Cucuringu, Alessandra Luati, Francesco Sanna Passino</dc:creator>
    </item>
    <item>
      <title>Robust Simulation Based Inference</title>
      <link>https://arxiv.org/abs/2508.02404</link>
      <description>arXiv:2508.02404v1 Announce Type: new 
Abstract: Simulation-Based Inference (SBI) is an approach to statistical inference where simulations from an assumed model are used to construct estimators and confidence sets. SBI is often used when the likelihood is intractable and to construct confidence sets that do not rely on asymptotic methods or regularity conditions. Traditional SBI methods assume that the model is correct, but, as always, this can lead to invalid inference when the model is misspecified. This paper introduces robust methods that allow for valid frequentist inference in the presence of model misspecification. We propose a framework where the target of inference is a projection parameter that minimizes a discrepancy between the true distribution and the assumed model. The method guarantees valid inference, even when the model is incorrectly specified and even if the standard regularity conditions fail. Alternatively, we introduce model expansion through exponential tilting as another way to account for model misspecification. We also develop an SBI based goodness-of-fit test to detect model misspecification. Finally, we propose two ideas that are useful in the SBI framework beyond robust inference: an SBI based method to obtain closed form approximations of intractable models and an active learning approach to more efficiently sample the parameter space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02404v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Tomaselli, Val\'erie Ventura, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Modelling Stochastic Inflow Patterns to a Reservoir with a Hidden Phase-Type Markov Model</title>
      <link>https://arxiv.org/abs/2508.02522</link>
      <description>arXiv:2508.02522v1 Announce Type: new 
Abstract: This paper presents a novel methodology for modelling precipitation patterns in a specific geographical region using Hidden Markov Models (HMMs). Departing from conventional HMMs, where the hidden state process is assumed to be Markovian, we introduce non-Markovian behaviour by incorporating phase-type distributions to model state durations. The primary objective is to capture the alternating sequences of dry and wet periods that characterize the local climate, providing deeper insight into its temporal structure. Building on this foundation, we extend the model to represent reservoir inflow patterns, which are then used to explain the observed water storage levels via a Moran model. The dataset includes historical rainfall and inflow records, where the latter is influenced by latent conditions governed by the hidden states. Direct modelling based solely on observed rainfall is insufficient due to the complexity of the system, hence the use of HMMs to infer these unobserved dynamics. This approach facilitates more accurate characterization of the underlying climatic processes and enables forecasting of future inflows based on historical data, supporting improved water resource management in the region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02522v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. L. Gamiz, D. Montoro, M. C Segovia-Garcia</dc:creator>
    </item>
    <item>
      <title>Optimal Adjustment and Combination of Independent Discrete $p$-Values</title>
      <link>https://arxiv.org/abs/2508.02647</link>
      <description>arXiv:2508.02647v1 Announce Type: new 
Abstract: Combining p-values from multiple independent tests is a fundamental task in statistical inference, but presents unique challenges when the p-values are discrete. We extend a recent optimal transport-based framework for combining discrete p-values, which constructs a continuous surrogate distribution by minimizing the Wasserstein distance between the transformed discrete null and its continuous analogue. We provide a unified approach for several classical combination methods, including Fisher's, Pearson's, George's, Stouffer's, and Edgington's statistics. Our theoretical analysis and extensive simulations show that accurate Type I error control is achieved when the variance of the adjusted discrete statistic closely matches that of the continuous case. We further demonstrate that, when the likelihood ratio test is a monotonic function of a combination statistic, the proposed approximation achieves power comparable to the uniformly most powerful (UMP) test. The methodology is illustrated with a genetic association study of rare variants using case-control data, and is implemented in the R package DPComb.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02647v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gonzalo Contador, Zheyang Wu</dc:creator>
    </item>
    <item>
      <title>PCS Workflow for Veridical Data Science in the Age of AI</title>
      <link>https://arxiv.org/abs/2508.00835</link>
      <description>arXiv:2508.00835v1 Announce Type: cross 
Abstract: Data science is a pillar of artificial intelligence (AI), which is transforming nearly every domain of human activity, from the social and physical sciences to engineering and medicine. While data-driven findings in AI offer unprecedented power to extract insights and guide decision-making, many are difficult or impossible to replicate. A key reason for this challenge is the uncertainty introduced by the many choices made throughout the data science life cycle (DSLC). Traditional statistical frameworks often fail to account for this uncertainty. The Predictability-Computability-Stability (PCS) framework for veridical (truthful) data science offers a principled approach to addressing this challenge throughout the DSLC. This paper presents an updated and streamlined PCS workflow, tailored for practitioners and enhanced with guided use of generative AI. We include a running example to display the PCS framework in action, and conduct a related case study which showcases the uncertainty in downstream predictions caused by judgment calls in the data cleaning stage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00835v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachary T. Rewolinski, Bin Yu</dc:creator>
    </item>
    <item>
      <title>A Dynamic, Context-Aware Framework for Risky Driving Prediction Using Naturalistic Data</title>
      <link>https://arxiv.org/abs/2508.00888</link>
      <description>arXiv:2508.00888v1 Announce Type: cross 
Abstract: Naturalistic driving studies offer a powerful means for observing and quantifying real-world driving behaviour. One of their prominent applications in traffic safety is the continuous monitoring and classification of risky driving behaviour. However, many existing frameworks rely on fixed time windows and static thresholds for distinguishing between safe and risky behaviour - limiting their ability to respond to the stochastic nature of real-world driving. This study proposes a dynamic and individualised framework for identifying risky driving behaviour using Belgian naturalistic driving data. The approach leverages a rolling time window and bi-level optimisation to dynamically calibrate both risk thresholds and model hyperparameters, capturing subtle behavioural shifts. Two safety indicators, speed-weighted headway and harsh driving events, were evaluated using three data-driven models: Random Forest, XGBoost, and Deep Neural Network (DNN). The DNN demonstrated strong capability in capturing subtle changes in driving behaviour, particularly excelling in high-recall tasks, making it promising for early-stage risk detection. XGBoost provided the most balanced and stable performance across different thresholds and evaluation metrics. While random forest showed more variability, it responded sensitively to dynamic threshold adjustments, which may be advantageous during model adaptation or tuning. Speed-weighted headway emerged as a more stable and context-sensitive risk indicator than harsh driving events, likely due to its robustness to label sparsity and contextual variation. Overall, the findings support the value of adaptive, personalised risk detection approaches for enhancing real-time safety feedback and tailoring driver support in intelligent transport systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00888v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Hossein Kalantari, Eleonora Papadimitriou, Amir Pooyan Afghari</dc:creator>
    </item>
    <item>
      <title>Consistent DAG selection for Bayesian causal discovery under general error distributions</title>
      <link>https://arxiv.org/abs/2508.00993</link>
      <description>arXiv:2508.00993v1 Announce Type: cross 
Abstract: We consider the problem of learning the underlying causal structure among a set of variables, which are assumed to follow a Bayesian network or, more specifically, a linear recursive structural equation model (SEM) with the associated errors being independent and allowed to be non-Gaussian. A Bayesian hierarchical model is proposed to identify the true data-generating directed acyclic graph (DAG) structure where the nodes and edges represent the variables and the direct causal effects, respectively. Moreover, incorporating the information of non-Gaussian errors, we characterize the distribution equivalence class of the true DAG, which specifies the best possible extent to which the DAG can be identified based on purely observational data. Furthermore, under the consideration that the errors are distributed as some scale mixture of Gaussian, where the mixing distribution is unspecified, and mild distributional assumptions, we establish that by employing a non-standard DAG prior, the posterior probability of the distribution equivalence class of the true DAG converges to unity as the sample size grows. This shows that the proposed method achieves the posterior DAG selection consistency, which is further illustrated with examples and simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00993v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anamitra Chaudhuri, Anirban Bhattacharya, Yang Ni</dc:creator>
    </item>
    <item>
      <title>Asymptotic guarantees for Bayesian phylogenetic tree reconstruction</title>
      <link>https://arxiv.org/abs/2508.00995</link>
      <description>arXiv:2508.00995v1 Announce Type: cross 
Abstract: We derive tractable criteria for the consistency of Bayesian tree reconstruction procedures, which constitute a central class of algorithms for inferring common ancestry among DNA sequence samples in phylogenetics. Our results encompass several Bayesian algorithms in widespread use, such as BEAST, MrBayes, and RevBayes. Unlike essentially all existing asymptotic guarantees for tree reconstruction, we require no discretization or boundedness assumptions on branch lengths. Our results are also very flexible, and easy to adapt to variations of the underlying inference problem. We demonstrate the practicality of our criteria on two examples: a Kingman coalescent prior on rooted, ultrametric trees, and an independence prior on unconstrained binary trees, though we emphasize that our result also applies to non-binary tree models. In both cases, the convergence rate we obtain matches known, frequentist results obtained using stronger boundedness assumptions, up to logarithmic factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00995v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2025.2485359</arxiv:DOI>
      <dc:creator>Alisa Kirichenko, Luke J. Kelly, Jere Koskela</dc:creator>
    </item>
    <item>
      <title>Anchoring-Based Causal Design (ABCD): Estimating the Effects of Beliefs</title>
      <link>https://arxiv.org/abs/2508.01677</link>
      <description>arXiv:2508.01677v1 Announce Type: cross 
Abstract: A central challenge in any study of the effects of beliefs on outcomes, such as decisions and behavior, is the risk of omitted variables bias. Omitted variables, frequently unmeasured or even unknown, can induce correlations between beliefs and decisions that are not genuinely causal, in which case the omitted variables are referred to as confounders. To address the challenge of causal inference, researchers frequently rely on information provision experiments to randomly manipulate beliefs. The information supplied in these experiments can serve as an instrumental variable (IV), enabling causal inference, so long as it influences decisions exclusively through its impact on beliefs. However, providing varying information to participants to shape their beliefs can raise both methodological and ethical concerns. Methodological concerns arise from potential violations of the exclusion restriction assumption. Such violations may stem from information source effects, when attitudes toward the source affect the outcome decision directly, thereby introducing a confounder. An ethical concern arises from manipulating the provided information, as it may involve deceiving participants. This paper proposes and empirically demonstrates a new method for treating beliefs and estimating their effects, the Anchoring-Based Causal Design (ABCD), which avoids deception and source influences. ABCD combines the cognitive mechanism known as anchoring with instrumental variable (IV) estimation. Instead of providing substantive information, the method employs a deliberately non-informative procedure in which participants compare their self-assessment of a concept to a randomly assigned anchor value. We present the method and the results of eight experiments demonstrating its application, strengths, and limitations. We conclude by discussing the potential of this design for advancing experimental social science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01677v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raanan Sulitzeanu-Kenan, Micha Mandel, Yosef Rinott</dc:creator>
    </item>
    <item>
      <title>Efficient optimization of expensive black-box simulators via marginal means, with application to neutrino detector design</title>
      <link>https://arxiv.org/abs/2508.01834</link>
      <description>arXiv:2508.01834v1 Announce Type: cross 
Abstract: With advances in scientific computing, computer experiments are increasingly used for optimizing complex systems. However, for modern applications, e.g., the optimization of nuclear physics detectors, each experiment run can require hundreds of CPU hours, making the optimization of its black-box simulator over a high-dimensional space a challenging task. Given limited runs at inputs $\mathbf{x}_1, \cdots, \mathbf{x}_n$, the best solution from these evaluated inputs can be far from optimal, particularly as dimensionality increases. Existing black-box methods, however, largely employ this ''pick-the-winner'' (PW) solution, which leads to mediocre optimization performance. To address this, we propose a new Black-box Optimization via Marginal Means (BOMM) approach. The key idea is a new estimator of a global optimizer $\mathbf{x}^*$ that leverages the so-called marginal mean functions, which can be efficiently inferred with limited runs in high dimensions. Unlike PW, this estimator can select solutions beyond evaluated inputs for improved optimization performance. Assuming the objective function follows a generalized additive model with unknown link function and under mild conditions, we prove that the BOMM estimator not only is consistent for optimization, but also has an optimization rate that tempers the ''curse-of-dimensionality'' faced by existing methods, thus enabling better performance as dimensionality increases. We present a practical framework for implementing BOMM using the transformed additive Gaussian process surrogate model. Finally, we demonstrate the effectiveness of BOMM in numerical experiments and an application on neutrino detector optimization in nuclear physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01834v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hwanwoo Kim, Simon Mak, Ann-Kathrin Schuetz, Alan Poon</dc:creator>
    </item>
    <item>
      <title>Causal Discovery in Multivariate Time Series through Mutual Information Featurization</title>
      <link>https://arxiv.org/abs/2508.01848</link>
      <description>arXiv:2508.01848v1 Announce Type: cross 
Abstract: Discovering causal relationships in complex multivariate time series is a fundamental scientific challenge. Traditional methods often falter, either by relying on restrictive linear assumptions or on conditional independence tests that become uninformative in the presence of intricate, non-linear dynamics. This paper proposes a new paradigm, shifting from statistical testing to pattern recognition. We hypothesize that a causal link creates a persistent and learnable asymmetry in the flow of information through a system's temporal graph, even when clear conditional independencies are obscured. We introduce Temporal Dependency to Causality (TD2C), a supervised learning framework that operationalizes this hypothesis. TD2C learns to recognize these complex causal signatures from a rich set of information-theoretic and statistical descriptors. Trained exclusively on a diverse collection of synthetic time series, TD2C demonstrates remarkable zero-shot generalization to unseen dynamics and established, realistic benchmarks. Our results show that TD2C achieves state-of-the-art performance, consistently outperforming established methods, particularly in high-dimensional and non-linear settings. By reframing the discovery problem, our work provides a robust and scalable new tool for uncovering causal structures in complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01848v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gian Marco Paldino, Gianluca Bontempi</dc:creator>
    </item>
    <item>
      <title>Structure Maintained Representation Learning Neural Network for Causal Inference</title>
      <link>https://arxiv.org/abs/2508.01865</link>
      <description>arXiv:2508.01865v1 Announce Type: cross 
Abstract: Recent developments in causal inference have greatly shifted the interest from estimating the average treatment effect to the individual treatment effect. In this article, we improve the predictive accuracy of representation learning and adversarial networks in estimating individual treatment effects by introducing a structure keeper which maintains the correlation between the baseline covariates and their corresponding representations in the high dimensional space. We train a discriminator at the end of representation layers to trade off representation balance and information loss. We show that the proposed discriminator minimizes an upper bound of the treatment estimation error. We can address the tradeoff between distribution balance and information loss by considering the correlations between the learned representation space and the original covariate feature space. We conduct extensive experiments with simulated and real-world observational data to show that our proposed Structure Maintained Representation Learning (SMRL) algorithm outperforms state-of-the-art methods. We also demonstrate the algorithms on real electronic health record data from the MIMIC-III database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01865v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yang Sun, Wenbin Lu, Yi-Hui Zhou</dc:creator>
    </item>
    <item>
      <title>Distribution-free data-driven smooth tests without $\chi^2$</title>
      <link>https://arxiv.org/abs/2508.01973</link>
      <description>arXiv:2508.01973v1 Announce Type: cross 
Abstract: This article demonstrates how recent developments in the theory of empirical processes allow us to construct a new family of asymptotically distribution-free smooth test statistics. Their distribution-free property is preserved even when the parameters are estimated, model selection is performed, and the sample size is only moderately large. A computationally efficient alternative to the classical parametric bootstrap is also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01973v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiangyu Zhang, Sara Algeri</dc:creator>
    </item>
    <item>
      <title>Estimating Causal Effects with Observational Data: Guidelines for Agricultural and Applied Economists</title>
      <link>https://arxiv.org/abs/2508.02310</link>
      <description>arXiv:2508.02310v1 Announce Type: cross 
Abstract: Most research questions in agricultural and applied economics are of a causal nature, i.e., how one or more variables (e.g., policies, prices, the weather) affect one or more other variables (e.g., income, crop yields, pollution). Only some of these research questions can be studied experimentally. Most empirical studies in agricultural and applied economics thus rely on observational data. However, estimating causal effects with observational data requires appropriate research designs and a transparent discussion of all identifying assumptions, together with empirical evidence to assess the probability that they hold. This paper provides an overview of various approaches that are frequently used in agricultural and applied economics to estimate causal effects with observational data. It then provides advice and guidelines for agricultural and applied economists who are intending to estimate causal effects with observational data, e.g., how to assess and discuss the chosen identification strategies in their publications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02310v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arne Henningsen (Department of Food and Resource Economics, University of Copenhagen, Denmark), Guy Low (Business Economics Group, Wageningen University &amp; Research, The Netherlands), David Wuepper (Institute for Food and Resource Economics, University of Bonn, Germany), Tobias Dalhaus (Business Economics Group, Wageningen University &amp; Research, The Netherlands), Hugo Storm (Institute for Food and Resource Economics, University of Bonn, Germany), Dagim Belay (Department of Food and Resource Economics, University of Copenhagen, Denmark), Stefan Hirsch (Department of Management in Agribusiness, University of Hohenheim, Germany)</dc:creator>
    </item>
    <item>
      <title>Unsupervised linear discrimination using skewness</title>
      <link>https://arxiv.org/abs/2508.02412</link>
      <description>arXiv:2508.02412v1 Announce Type: cross 
Abstract: It is well-known that, in Gaussian two-group separation, the optimally discriminating projection direction can be estimated without any knowledge on the group labels. In this work, we \revision{gather} several such unsupervised estimators based on skewness and derive their limiting distributions. As one of our main results, we show that all affine equivariant estimators of the optimal direction have proportional asymptotic covariance matrices, making their comparison straightforward. Two of our four estimators are novel and two have been proposed already earlier. We use simulations to verify our results and to inspect the finite-sample behaviors of the estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02412v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Una Radojicic, Klaus Nordhausen, Joni Virta</dc:creator>
    </item>
    <item>
      <title>Trustworthy scientific inference for inverse problems with generative models</title>
      <link>https://arxiv.org/abs/2508.02602</link>
      <description>arXiv:2508.02602v1 Announce Type: cross 
Abstract: Generative artificial intelligence (AI) excels at producing complex data structures (text, images, videos) by learning patterns from training examples. Across scientific disciplines, researchers are now applying generative models to ``inverse problems'' to infer hidden parameters from observed data. While these methods can handle intractable models and large-scale studies, they can also produce biased or overconfident conclusions. We present a solution with Frequentist-Bayes (FreB), a mathematically rigorous protocol that reshapes AI-generated probability distributions into confidence regions that consistently include true parameters with the expected probability, while achieving minimum size when training and target data align. We demonstrate FreB's effectiveness by tackling diverse case studies in the physical sciences: identifying unknown sources under dataset shift, reconciling competing theoretical models, and mitigating selection bias and systematics in observational studies. By providing validity guarantees with interpretable diagnostics, FreB enables trustworthy scientific inference across fields where direct likelihood evaluation remains impossible or prohibitively expensive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02602v1</guid>
      <category>stat.ML</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Carzon, Luca Masserano, Joshua D. Ingram, Alex Shen, Antonio Carlos Herling Ribeiro Junior, Tommaso Dorigo, Michele Doro, Joshua S. Speagle, Rafael Izbicki, Ann B. Lee</dc:creator>
    </item>
    <item>
      <title>The curse of isotropy: from principal components to principal subspaces</title>
      <link>https://arxiv.org/abs/2307.15348</link>
      <description>arXiv:2307.15348v4 Announce Type: replace 
Abstract: Principal component analysis is a ubiquitous tool in exploratory data analysis. It is widely used by applied scientists for visualization and interpretability purposes. We raise an important issue (the curse of isotropy) about the interpretation of principal components with close eigenvalues. They may indeed suffer from an important rotational variability, which is a pitfall for interpretation. Through the lens of a probabilistic covariance model parameterized with flags of subspaces, we show that the curse of isotropy cannot be overlooked in practice. In this context, we propose to transition from ill-defined principal components to more-interpretable principal subspaces. The final methodology (principal subspace analysis) is extremely simple and shows promising results on a variety of datasets from different fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15348v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Szwagier, Xavier Pennec</dc:creator>
    </item>
    <item>
      <title>Testing High-dimensional Nonstationary Time Series</title>
      <link>https://arxiv.org/abs/2308.06126</link>
      <description>arXiv:2308.06126v2 Announce Type: replace 
Abstract: In this article, we first establish the joint central limit theorem (CLT) for the extreme eigenvalues of the sample correlation matrix of high-dimensional random walks with cross-sectional dependence. We further investigate the asymptotic spectral properties of the sample correlation matrix of high-dimensional autoregressive processes. To apply our theoretical results, we propose a novel high-dimensional unit root test and develop a forward sequential test to determine the number of unit roots in high-dimensional time series data. Finally, we conduct an empirical study of the purchasing power parity (PPP) hypothesis in high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.06126v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruihan Liu, Chen Wang</dc:creator>
    </item>
    <item>
      <title>Causal Estimation and Inference in Nonlinear Mendelian Randomization Studies</title>
      <link>https://arxiv.org/abs/2402.01121</link>
      <description>arXiv:2402.01121v2 Announce Type: replace 
Abstract: Mendelian randomization (MR) is widely used to uncover causal relationships in the presence of unmeasured confounders. However, most existing MR methods presuppose linear causality, risking bias when the true relationships are nonlinear, which is a common empirical scenario. In this paper, we compared two prevalent instrumental variable techniques (the two-stage prediction method and the control function method) under both linear and nonlinear settings, and addressed key issues such as horizontal pleiotropy and violations of classical assumptions in control function method. Most notably, we proposed a flexible semiparametric approach that estimates the causal function without a priori specification, reducing the risk of model misspecification, and extended our methods to binary outcomes, broadening its applicability. For all approaches, we provided estimators, standard errors, and test statistics, to facilitate robust causal inference. Extensive numerical simulations demonstrated that our proposed methods exhibited both accuracy and robustness across diverse scenarios. Applying our methods to UK Biobank data uncovered significant nonlinear causal effects missed by linear MR approaches. We offer an R package implementation for broader and more convenient use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01121v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinpei Wang, Tao Huang, Jinzhu Jia</dc:creator>
    </item>
    <item>
      <title>A Latent Variable Approach to Learning High-dimensional Multivariate longitudinal Data</title>
      <link>https://arxiv.org/abs/2405.15053</link>
      <description>arXiv:2405.15053v2 Announce Type: replace 
Abstract: High-dimensional multivariate longitudinal data, which arise when many outcome variables are measured repeatedly over time, are becoming increasingly common in social, behavioral and health sciences. We propose a latent variable model for drawing statistical inferences on covariate effects and predicting future outcomes based on high-dimensional multivariate longitudinal data. This model introduces unobserved factors to account for the between-variable and across-time dependence and assist the prediction. Statistical inference and prediction tools are developed under a general setting that allows outcome variables to be of mixed types and possibly unobserved for certain time points, for example, due to right censoring. A central limit theorem is established for drawing statistical inferences on regression coefficients. Additionally, an information criterion is introduced to choose the number of factors. The proposed model is applied to customer grocery shopping records to predict and understand shopping behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15053v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sze Ming Lee, Yunxiao Chen, Tony Sit</dc:creator>
    </item>
    <item>
      <title>Replicable Bandits for Digital Health Interventions</title>
      <link>https://arxiv.org/abs/2407.15377</link>
      <description>arXiv:2407.15377v3 Announce Type: replace 
Abstract: Adaptive treatment assignment algorithms, such as bandit algorithms, are increasingly used in digital health intervention clinical trials. Frequently the data collected from these trials is used to conduct causal inference and related data analyses to decide how to refine the intervention, and whether to roll-out the intervention more broadly. This work studies inference for estimands that depend on the adaptive algorithm itself; a simple example is the mean reward under the adaptive algorithm. Specifically, we investigate the replicability of statistical analyses concerning such estimands when using data from trials deploying adaptive treatment assignment algorithms. We demonstrate that many standard statistical estimators can be inconsistent and fail to be replicable across repetitions of the clinical trial, even as the sample size grows large. We show that this non-replicability is intimately related to properties of the adaptive algorithm itself. We introduce a formal definition of a "replicable bandit algorithm" and prove that under such algorithms, a wide variety of common statistical estimators are guaranteed to be consistent and asymptotically normal. We present both theoretical results and simulation studies based on a mobile health oral health self-care intervention. Our findings underscore the importance of designing adaptive algorithms with replicability in mind, especially for settings like digital health where deployment decisions rely heavily on replicated evidence. We conclude by discussing open questions on the connections between algorithm design, statistical inference, and experimental replicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15377v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kelly W. Zhang, Nowell Closser, Anna L. Trella, Susan A. Murphy</dc:creator>
    </item>
    <item>
      <title>Huber-robust likelihood ratio tests for composite nulls and alternatives</title>
      <link>https://arxiv.org/abs/2408.14015</link>
      <description>arXiv:2408.14015v5 Announce Type: replace 
Abstract: We propose an e-value based framework for testing arbitrary composite nulls against composite alternatives, when an $\epsilon$ fraction of the data can be arbitrarily corrupted. Our tests are inherently sequential, being valid at arbitrary data-dependent stopping times, but they are new even for fixed sample sizes, giving type-I error control without any regularity conditions. We achieve this extending a classical robust test by Huber (1965) in the simple null versus simple alternative case, and a modern non-robust test by Larsson et al. (2024) for general composite nulls. Our test statistic is a nonnegative supermartingale under the (robust) null, even under a sequentially adaptive (non-i.i.d.) contamination model where the conditional distribution of each observation given the past data lies within an $\epsilon$ total variation (TV) ball of some distribution in the original composite null. The supermartingale grows to infinity exponentially fast under any distribution in the ($\epsilon$ TV-corruption of the) alternative; as $\epsilon\to 0$, the exponent approaches a certain Kullback-Leibler divergence between the null and alternative, which is the optimal non-robust growth rate. A key step is the derivation of a robust Reverse Information Projection (RIPr) and a certain dual object, the robust numeraire e-value. Simulations validate the theory and demonstrate reasonable practical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14015v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aytijhya Saha, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>A class of modular and flexible covariate-based covariance functions for nonstationary spatial modeling</title>
      <link>https://arxiv.org/abs/2410.16716</link>
      <description>arXiv:2410.16716v2 Announce Type: replace 
Abstract: Paradoxically, while the assumptions of second-order stationarity and isotropy appear outdated in light of modern spatial data, they remain remarkably robust in practice, as nonstationary methods often provide marginal improvements in predictive performance. This limitation reflects a fundamental trade-off: nonparametric approaches, while offering extreme flexibility, require substantial tuning to avoid overfitting and numerical challenges in practice, while parametric approaches are more robust against overfitting but are constrained in flexibility, often facing considerable numerical challenges as flexibility increases. In this article we introduce a parametric class of covariance functions that extends the use of parametric nonstationary spatial models, aiming to compete with the flexibility and local adaptability of nonparametric approaches. The covariance function is modular in the sense that allows for separate parametric structures for different sources of nonstationarity, such as marginal standard deviation, geometric anisotropy, and smoothness. The proposed covariance function retains the practical identifiability and computational stability of parametric forms while closing the performance gap with fully nonparametric methods. A Mat\'ern stationary isotropic model is nested within the complex model and can be adapted such that it is computationally feasible for handling thousands of observations. A two-stage approach can be employed for model selection. We explore the statistical properties of the presented approach, demonstrate its compatibility with the frequentist paradigm, and highlight the interpretability of its parameters. We illustrate its prediction capabilities as well as interpretability through an analysis of Swiss monthly precipitation data, showing that Gaussian process models with the presented covariance function, while remaining robust against overfitting, provide quantitative and qualitative improvements over existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16716v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Blasi, Reinhard Furrer</dc:creator>
    </item>
    <item>
      <title>Asymmetric Errors</title>
      <link>https://arxiv.org/abs/2411.15499</link>
      <description>arXiv:2411.15499v3 Announce Type: replace 
Abstract: We present a procedure for handling asymmetric errors. Many results in particle physics are presented as values with different positive and negative errors, and there is no consistent procedure for handling them. We consider the difference between errors quoted using pdfs and using likelihoods, and the difference between the rms spread of a measurement and the 68\% central confidence region. We provide a comprehensive analysis of the possibilities, and software tools to enable their use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15499v3</guid>
      <category>stat.ME</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roger Barlow, Alessandra Brazzale, Igor Volobouev</dc:creator>
    </item>
    <item>
      <title>Randomized interventional effects in semicompeting risks</title>
      <link>https://arxiv.org/abs/2412.06114</link>
      <description>arXiv:2412.06114v3 Announce Type: replace 
Abstract: In clinical studies, the risk of the primary (terminal) event may be modified by intermediate events, resulting in semicompeting risks. To study the treatment effect on the terminal event mediated by the intermediate event, researchers wish to decompose the total effect into direct and indirect effects. In this article, we extend the randomized interventional approach to time-to-event data, where both intermediate and terminal events are subject to right censoring. We envision a random draw for the intermediate event process according to some reference distribution, either marginally over time-varying confounders or conditionally given observed history. We present the identification formula for interventional effects and discuss some variants of the identification assumptions. We estimate the treatment effects using nonparametric maximum likelihood estimation and propose a sensitivity analysis. We study the effect of matched unrelated donor versus haploidentical donor on death mediated by relapse in a hematopoietic cell transplantation study with graft-versus-host disease as the time-varying confounder. We find that matched unrelated donor transplantation is preferable under the use of post-transplantation PTCy GVHD prophylaxis for lymphoma patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06114v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Deng, Rui Wang, Xiang Zhan</dc:creator>
    </item>
    <item>
      <title>On the use of the principle of maximum entropy to improve the robustness of bivariate spline least-squares approximation</title>
      <link>https://arxiv.org/abs/2503.00942</link>
      <description>arXiv:2503.00942v2 Announce Type: replace 
Abstract: We consider fitting a bivariate spline regression model to data using a weighted least-squares cost function, with weights that sum to one to form a discrete probability distribution. By applying the principle of maximum entropy, the weight distribution is determined by maximizing the associated entropy function. This approach, previously applied successfully to polynomials and spline curves, enhances the robustness of the regression model by automatically detecting and down-weighting anomalous data during the fitting process. To demonstrate the effectiveness of the method, we present applications to two image processing problems and further illustrate its potential through two synthetic examples.
  Unlike the standard ordinary least-squares method, the maximum entropy formulation leads to a nonlinear algebraic system whose solvability requires careful theoretical analysis. We provide preliminary results in this direction and discuss the computational implications of solving the associated constrained optimization problem, which calls for dedicated iterative algorithms. These aspects suggest natural directions for further research on both the theoretical and algorithmic fronts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00942v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.matcom.2025.07.053</arxiv:DOI>
      <arxiv:journal_reference>Mathematics and Computers in Simulation, available online 31 July 2025</arxiv:journal_reference>
      <dc:creator>Pierluigi Amodio, Luigi Brugnano, Felice Iavernaro</dc:creator>
    </item>
    <item>
      <title>e-GAI: e-value-based Generalized $\alpha$-Investing for Online False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2506.01452</link>
      <description>arXiv:2506.01452v2 Announce Type: replace 
Abstract: Online multiple hypothesis testing has attracted a lot of attention in many applications, e.g., anomaly status detection and stock market price monitoring. The state-of-the-art generalized $\alpha$-investing (GAI) algorithms can control online false discovery rate (FDR) on p-values only under specific dependence structures, a situation that rarely occurs in practice. The e-LOND algorithm (Xu &amp; Ramdas, 2024) utilizes e-values to achieve online FDR control under arbitrary dependence but suffers from a significant loss in power as testing levels are derived from pre-specified descent sequences. To address these limitations, we propose a novel framework on valid e-values named e-GAI. The proposed e-GAI can ensure provable online FDR control under more general dependency conditions while improving the power by dynamically allocating the testing levels. These testing levels are updated not only by relying on both the number of previous rejections and the prior costs, but also, differing from the GAI framework, by assigning less $\alpha$-wealth for each rejection from a risk aversion perspective. Within the e-GAI framework, we introduce two new online FDR procedures, e-LORD and e-SAFFRON, and provide strategies for the long-term performance to address the issue of $\alpha$-death, a common phenomenon within the GAI framework. Furthermore, we demonstrate that e-GAI can be generalized to conditionally super-uniform p-values. Both simulated and real data experiments demonstrate the advantages of both e-LORD and e-SAFFRON in FDR control and power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01452v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Zhang, Zijian Wei, Haojie Ren, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Local empirical Bayes correction for Bayesian modeling</title>
      <link>https://arxiv.org/abs/2506.11424</link>
      <description>arXiv:2506.11424v4 Announce Type: replace 
Abstract: The James-Stein estimator has attracted much interest as a shrinkage estimator that yields better estimates than the maximum likelihood estimator. The James-Stein estimator is also very useful as an argument in favor of empirical Bayesian methods. However, for problems involving large-scale data, such as differential gene expression data, the distribution is considered a mixture distribution with different means that cannot be considered sufficiently close. Therefore, it is not appropriate to apply the James-Stein estimator. Efron (2011) proposed a local empirical Bayes correction that attempted to correct a selection bias for large-scale data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11424v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.24644/keidaironshu.68.4_161</arxiv:DOI>
      <arxiv:journal_reference>Osaka Keidai Ronshu, vol.68, no.4, pp.161-172, 2017</arxiv:journal_reference>
      <dc:creator>Yoshiko Hayashi</dc:creator>
    </item>
    <item>
      <title>Network Cross-Validation for Nested Models by Edge-Sampling</title>
      <link>https://arxiv.org/abs/2506.14244</link>
      <description>arXiv:2506.14244v2 Announce Type: replace 
Abstract: In the network literature, a wide range of statistical models has been proposed to exploit structural patterns in the data. Therefore, model selection between different models is a fundamental problem. However, there remains a lack of systematic theoretical understanding for this problem when comparing across different model classes. In this paper, to address this challenging problem, we propose a penalized edge-sampling cross-validation framework for nested network model selection. By incorporating a model complexity penalty into the evaluation process, our method effectively mitigates the overfitting tendency of cross-validation and adapts to varying model structures. This framework supports comparisons among widely used models, including stochastic block models (SBMs), degree-corrected SBMs (DCBMs), and graphon models, providing the first consistency guarantees for model selection across these settings to our knowledge. Empirical evaluations, including both simulated data and the ``Political Books'' network, demonstrate that our method yields stable and accurate performance across various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14244v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bokai Yang, Yuanxing Chen, Yuhong Yang</dc:creator>
    </item>
    <item>
      <title>Weighted Parameter Estimators of the Generalized Extreme Value Distribution in the Presence of Missing Observations</title>
      <link>https://arxiv.org/abs/2506.15964</link>
      <description>arXiv:2506.15964v2 Announce Type: replace 
Abstract: Missing data occur in a variety of applications of extreme value analysis. In the block maxima approach to an extreme value analysis, missingness is often handled by either ignoring missing observations or dropping a block of observations from the analysis. However, in some cases, missingness may occur due to equipment failure during an extreme event, which can lead to bias in estimation. In this work, we propose weighted maximum likelihood and weighted moment-based estimators for the generalized extreme value distribution parameters to account for the presence of missing observations. We validate the procedures through an extensive simulation study and apply the estimation methods to data from multiple tidal gauges on the Eastern coast of Canada.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15964v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>James H. McVittie, Orla A. Murphy</dc:creator>
    </item>
    <item>
      <title>Constructing prediction intervals for the age distribution of deaths</title>
      <link>https://arxiv.org/abs/2506.17953</link>
      <description>arXiv:2506.17953v2 Announce Type: replace 
Abstract: We introduce a model-agnostic procedure to construct prediction intervals for the age distribution of deaths. The age distribution of deaths is an example of constrained data, which are nonnegative and have a constrained integral. A centered log-ratio transformation and a cumulative distribution function transformation are used to remove the two constraints, where the latter transformation can also handle the presence of zero counts. Our general procedure divides data samples into training, validation, and testing sets. Within the validation set, we can select an optimal tuning parameter by calibrating the empirical coverage probabilities to be close to their nominal ones. With the selected optimal tuning parameter, we then construct the pointwise prediction intervals using the same models for the holdout data in the testing set. Using Japanese age- and sex-specific life-table death counts, we assess and evaluate the interval forecast accuracy with a suite of functional time-series models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17953v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang, Steven Haberman</dc:creator>
    </item>
    <item>
      <title>The hierarchical barycenter: conditional probability simulation with structured and unobserved covariates</title>
      <link>https://arxiv.org/abs/2508.00206</link>
      <description>arXiv:2508.00206v2 Announce Type: replace 
Abstract: This paper presents a new method for conditional probability density simulation. The method is design to work with unstructured data set when data are not characterized by the same covariates yet share common information. Specific examples considered in the text are relative to two main classes: homogeneous data characterized by samples with missing value for the covariates and data set divided in two or more groups characterized by covariates that are only partially overlapping. The methodology is based on the mathematical theory of optimal transport extending the barycenter problem to the newly defined hierarchical barycenter problem. A newly, data driven, numerical procedure for the solution of the hierarchical barycenter problem is proposed and its advantages, over the use of classical barycenter, are illustrated on synthetic and real world data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00206v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Esteban G. Tabak, Giulio Trigila, Wenjun Zhao</dc:creator>
    </item>
    <item>
      <title>Yurinskii's Coupling for Martingales</title>
      <link>https://arxiv.org/abs/2210.00362</link>
      <description>arXiv:2210.00362v4 Announce Type: replace-cross 
Abstract: Yurinskii's coupling is a popular theoretical tool for non-asymptotic distributional analysis in mathematical statistics and applied probability, offering a Gaussian strong approximation with an explicit error bound under easily verifiable conditions. Originally stated in $\ell_2$-norm for sums of independent random vectors, it has recently been extended both to the $\ell_p$-norm, for $1 \leq p \leq \infty$, and to vector-valued martingales in $\ell_2$-norm, under some strong conditions. We present as our main result a Yurinskii coupling for approximate martingales in $\ell_p$-norm, under substantially weaker conditions than those previously imposed. Our formulation further allows for the coupling variable to follow a more general Gaussian mixture distribution, and we provide a novel third-order coupling method which gives tighter approximations in certain settings. We specialize our main result to mixingales, martingales, and independent data, and derive uniform Gaussian mixture strong approximations for martingale empirical processes. Applications to nonparametric partitioning-based and local polynomial regression procedures are provided, alongside central limit theorems for high-dimensional martingale vectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.00362v4</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Ricardo P. Masini, William G. Underwood</dc:creator>
    </item>
    <item>
      <title>Optimal Discriminant Analysis in High-Dimensional Latent Factor Models</title>
      <link>https://arxiv.org/abs/2210.12862</link>
      <description>arXiv:2210.12862v2 Announce Type: replace-cross 
Abstract: In high-dimensional classification problems, a commonly used approach is to first project the high-dimensional features into a lower dimensional space, and base the classification on the resulting lower dimensional projections. In this paper, we formulate a latent-variable model with a hidden low-dimensional structure to justify this two-step procedure and to guide which projection to choose. We propose a computationally efficient classifier that takes certain principal components (PCs) of the observed features as projections, with the number of retained PCs selected in a data-driven way. A general theory is established for analyzing such two-step classifiers based on any projections. We derive explicit rates of convergence of the excess risk of the proposed PC-based classifier. The obtained rates are further shown to be optimal up to logarithmic factors in the minimax sense. Our theory allows the lower-dimension to grow with the sample size and is also valid even when the feature dimension (greatly) exceeds the sample size. Extensive simulations corroborate our theoretical findings. The proposed method also performs favorably relative to other existing discriminant methods on three real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.12862v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Bing, Marten Wegkamp</dc:creator>
    </item>
    <item>
      <title>Distribution-free inference with hierarchical data</title>
      <link>https://arxiv.org/abs/2306.06342</link>
      <description>arXiv:2306.06342v4 Announce Type: replace-cross 
Abstract: This paper studies distribution-free inference in settings where the data set has a hierarchical structure -- for example, groups of observations, or repeated measurements. In such settings, standard notions of exchangeability may not hold. To address this challenge, a hierarchical form of exchangeability is derived, facilitating extensions of distribution-free methods, including conformal prediction and jackknife+. While the standard theoretical guarantee obtained by the conformal prediction framework is a marginal predictive coverage guarantee, in the special case of independent repeated measurements, it is possible to achieve a stronger form of coverage -- the "second-moment coverage" property -- to provide better control of conditional miscoverage rates, and distribution-free prediction sets that achieve this property are constructed. Simulations illustrate that this guarantee indeed leads to uniformly small conditional miscoverage rates. Empirically, this stronger guarantee comes at the cost of a larger width of the prediction set in scenarios where the fitted model is poorly calibrated, but this cost is very mild in cases where the fitted model is accurate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06342v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Rina Foygel Barber, Rebecca Willett</dc:creator>
    </item>
    <item>
      <title>Ensemble learning for uncertainty estimation with application to the correction of satellite precipitation products</title>
      <link>https://arxiv.org/abs/2403.10567</link>
      <description>arXiv:2403.10567v3 Announce Type: replace-cross 
Abstract: Predictions in the form of probability distributions are crucial for effective decision-making. Quantile regression enables such predictions within spatial prediction settings that aim to create improved precipitation datasets by merging remote sensing and gauge data. However, ensemble learning of quantile regression algorithms remains unexplored in this context and, at the same time, it has not been substantially developed so far in the broader machine learning research landscape. Here, we introduce nine quantile-based ensemble learners and address the aforementioned gap in precipitation dataset creation by presenting the first application of these learners to large precipitation datasets. We employed a novel feature engineering strategy, which reduces the number of predictors by using distance-weighted satellite precipitation at relevant locations, combined with location elevation. Our ensemble learners include six that are based on stacking ideas and three simple methods (mean, median, best combiner). Each of them combines the following six individual algorithms: quantile regression (QR), quantile regression forests (QRF), generalized random forests (GRF), gradient boosting machines (GBM), light gradient boosting machines (LightGBM), and quantile regression neural networks (QRNN). These algorithms serve as both base learners and combiners within different ensemble learning methods. We evaluated performance against a reference method (i.e., QR) using quantile scoring functions and a large dataset. The latter comprises 15 years of monthly gauge-measured and satellite precipitation in the contiguous United States (CONUS). Ensemble learning with QR and QRNN yielded the best results across the various investigated quantile levels, which range from 0.025 to 0.975, outperforming the reference method by 3.91% to 8.95%...</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10567v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/3049-4753/add93b</arxiv:DOI>
      <arxiv:journal_reference>Machine Learning: Earth 1 (2025) 015004</arxiv:journal_reference>
      <dc:creator>Georgia Papacharalampous, Hristos Tyralis, Nikolaos Doulamis, Anastasios Doulamis</dc:creator>
    </item>
    <item>
      <title>Functional limit theorems and parameter inference for multiscale stochastic models of enzyme kinetics</title>
      <link>https://arxiv.org/abs/2409.06565</link>
      <description>arXiv:2409.06565v2 Announce Type: replace-cross 
Abstract: We study a class of Stochastic Differential Equations (SDEs) with jumps modeling multistage Michaelis--Menten enzyme kinetics, in which a substrate is sequentially transformed into a product via a cascade of intermediate complexes. These networks are typically high dimensional and exhibit multiscale behavior with strong coupling between different components, posing substantial analytical and computational challenges. In particular, the problem of statistical inference of reaction rates is significantly difficult, and becomes even more intricate when direct observations of system states are unavailable and only a random sample of product formation times is observed. We address this in two stages. First, in a suitable scaling regime consistent with the Quasi-Steady State Approximation (QSSA), we rigorously establish two asymptotic results: (i) a stochastic averaging principle yielding a reduced model for the product--substrate dynamics; and (ii) a Functional Central Limit Theorem (FCLT) characterizing the associated fluctuations. Guided by the reduced-order dynamics, we next construct a novel Interacting Particle System (IPS) that approximates the product-substrate process at the particle level. This IPS plays a pivotal role in the inference methodology; in particular, we establish a propagation of chaos result that mathematically justifies an approximate product-form likelihood based solely on a random sample of product formation times, without requiring access to the system states. Numerical examples are presented to demonstrate the accuracy and applicability of the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06565v2</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnab Ganguly, Wasiur R. KhudaBukhsh</dc:creator>
    </item>
    <item>
      <title>Learning large softmax mixtures with warm start EM</title>
      <link>https://arxiv.org/abs/2409.09903</link>
      <description>arXiv:2409.09903v2 Announce Type: replace-cross 
Abstract: Softmax mixture models (SMMs) are discrete $K$-mixtures introduced to model the probability of choosing an attribute $x_j \in \RR^L$ from $p$ candidates, in heterogeneous populations. They have been known as mixed multinomial logits in the econometrics literature, and are gaining traction in the LLM literature, where single softmax models are routinely used in the final layer of a neural network. This paper provides a comprehensive analysis of the EM algorithm for SMMs in high dimensions. Its population-level theoretical analysis forms the basis for proving (i) local identifiability, in SSMs with generic features and, further, via a stochastic argument, (ii) full identifiability in SSMs with random features, when $p$ is large enough. These are the first results in this direction for SSMs with $L &gt; 1$. The population-level EM analysis characterizes the initialization radius for algorithmic convergence. This also guides the construction of warm starts of the sample level EM. Under suitable initialization, the EM algorithm is shown to recover the mixture atoms of the SSM at near-parametric rate. We provide two main directions for warm start construction, both based on a new method for estimating the moments of the mixing measure underlying an SSM with random design. First, we construct a method of moments (MoM) estimator of the mixture parameters, and provide its first theoretical analysis. While MoM can enjoy parametric rates of convergence, and thus can serve as a warm-start, the estimator's quality degrades exponentially in $K$. Our recommendation, when $K$ is not small, is to run the EM algorithm several times with random initializations. We again make use of the novel latent moments estimation method to estimate the $K$-dimensional subspace of the mixture atoms. Sampling from this subspace reduces substantially the number of required draws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09903v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Bing, Florentina Bunea, Jonathan Niles-Weed, Marten Wegkamp</dc:creator>
    </item>
    <item>
      <title>On the optimality of coin-betting for mean estimation</title>
      <link>https://arxiv.org/abs/2412.02640</link>
      <description>arXiv:2412.02640v4 Announce Type: replace-cross 
Abstract: We consider the problem of testing the mean of a bounded real random variable. We introduce a notion of optimal classes for e-variables and e-processes, and establish the optimality of the coin-betting formulation among e-variable-based algorithmic frameworks for testing and estimating the (conditional) mean. As a consequence, we provide a direct and explicit characterisation of all valid e-variables and e-processes for this testing problem. In the language of classical statistical decision theory, we fully describe the set of all admissible e-variables and e-processes, and identify the corresponding minimal complete class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02640v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eugenio Clerico</dc:creator>
    </item>
    <item>
      <title>Post-detection inference for sequential changepoint localization</title>
      <link>https://arxiv.org/abs/2502.06096</link>
      <description>arXiv:2502.06096v3 Announce Type: replace-cross 
Abstract: This paper addresses a fundamental but largely unexplored challenge in sequential changepoint analysis: conducting inference following a detected change. We develop a very general framework to construct confidence sets for the unknown changepoint using only the data observed up to a data-dependent stopping time at which an arbitrary sequential detection algorithm declares a change. Our framework is nonparametric, making no assumption on the composite post-change class, the observation space, or the sequential detection procedure used, and is nonasymptotically valid. We also extend it to handle composite pre-change classes under a suitable assumption, and also derive confidence sets for the change magnitude in parametric settings. Extensive simulations demonstrate that the produced sets have reasonable size, and slightly conservative coverage. In summary, we present the first general method for sequential changepoint localization, which is theoretically sound and broadly applicable in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06096v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aytijhya Saha, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Semi-Parametric Batched Global Multi-Armed Bandits with Covariates</title>
      <link>https://arxiv.org/abs/2503.00565</link>
      <description>arXiv:2503.00565v2 Announce Type: replace-cross 
Abstract: The multi-armed bandits (MAB) framework is a widely used approach for sequential decision-making, where a decision-maker selects an arm in each round with the goal of maximizing long-term rewards. Moreover, in many practical applications, such as personalized medicine and recommendation systems, feedback is provided in batches, contextual information is available at the time of decision-making, and rewards from different arms are related rather than independent. We propose a novel semi-parametric framework for batched bandits with covariates and a shared parameter across arms, leveraging the single-index regression (SIR) model to capture relationships between arm rewards while balancing interpretability and flexibility. Our algorithm, Batched single-Index Dynamic binning and Successive arm elimination (BIDS), employs a batched successive arm elimination strategy with a dynamic binning mechanism guided by the single-index direction. We consider two settings: one where a pilot direction is available and another where the direction is estimated from data, deriving theoretical regret bounds for both cases. When a pilot direction is available with sufficient accuracy, our approach achieves minimax-optimal rates (with $d = 1$) for nonparametric batched bandits, circumventing the curse of dimensionality. Extensive experiments on simulated and real-world datasets demonstrate the effectiveness of our algorithm compared to the nonparametric batched bandit method introduced by \cite{jiang2024batched}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00565v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakshi Arya, Hyebin Song</dc:creator>
    </item>
    <item>
      <title>How Can I Publish My LLM Benchmark Without Giving the True Answers Away?</title>
      <link>https://arxiv.org/abs/2505.18102</link>
      <description>arXiv:2505.18102v3 Announce Type: replace-cross 
Abstract: Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18102v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takashi Ishida, Thanawat Lodkaew, Ikko Yamane</dc:creator>
    </item>
    <item>
      <title>Identifying Robust Mediators of Health Disparities: A Review and Simulation Studies With Directed Acyclic Graphs</title>
      <link>https://arxiv.org/abs/2506.19047</link>
      <description>arXiv:2506.19047v2 Announce Type: replace-cross 
Abstract: Background. A central objective among health researchers across disciplines is to identify modifiable factors that can reduce health disparities. Three common methods--difference-in-coefficients (DIC), Kitagawa-Oaxaca-Blinder (KOB), and causal decomposition analysis (CDA)--share the same goal to identify such contributors but can produce divergent results depending on confounding and model assumptions. Despite these challenges, applied researchers lack clear guidance on selecting appropriate methods for different scenarios. Methods. We start with a brief review of each method, assuming no unmeasured confounders. We then move to two more realistic scenarios: 1) unmeasured confounders affect the relationship between intermediate confounders and the mediator, and 2) unmeasured confounders affect the relationship between the mediator and the outcome. For each scenario, we generate simulated data, apply all three methods, compare their estimates, and interpret the results using Directed Acyclic Graphs. Results. Under the assumption of no unmeasured confounders, the DIC approach is a simplistic method suitable only when no intermediate confounders are present. The KOB decomposition is appropriate unless adjustment for baseline covariates is necessary. When unmeasured confounding exists, the DIC method yields biased estimates in both scenarios, and all three methods produce biased results in the second scenario. However, CDA, when paired with sensitivity analysis can help assess the robustness of its estimates. Conclusions. We advise against using the DIC method, particularly, in observational studies, as the assumptions of no intermediate confounders is often unrealistic. When unmeasured confounders are anticipated, CDA combined with sensitivity analysis offers a more robust approach for identifying mediators over other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19047v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Soojin Park, Su Yeon Kim, Chioun Lee</dc:creator>
    </item>
    <item>
      <title>Scalable Subset Selection in Linear Mixed Models</title>
      <link>https://arxiv.org/abs/2506.20425</link>
      <description>arXiv:2506.20425v2 Announce Type: replace-cross 
Abstract: Linear mixed models (LMMs), which incorporate fixed and random effects, are key tools for analyzing heterogeneous data, such as in personalized medicine. Nowadays, this type of data is increasingly wide, sometimes containing thousands of candidate predictors, necessitating sparsity for prediction and interpretation. However, existing sparse learning methods for LMMs do not scale well beyond tens or hundreds of predictors, leaving a large gap compared with sparse methods for linear models, which ignore random effects. This paper closes the gap with a new $\ell_0$ regularized method for LMM subset selection that can run on datasets containing thousands of predictors in seconds to minutes. On the computational front, we develop a coordinate descent algorithm as our main workhorse and provide a guarantee of its convergence. We also develop a local search algorithm to help traverse the nonconvex optimization surface. Both algorithms readily extend to subset selection in generalized LMMs via a penalized quasi-likelihood approximation. On the statistical front, we provide a finite-sample bound on the Kullback-Leibler divergence of the new method. We then demonstrate its excellent performance in experiments involving synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20425v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Thompson, Matt P. Wand, Joanna J. J. Wang</dc:creator>
    </item>
    <item>
      <title>Probabilistic Modeling of Antibody Kinetics Post Infection and Vaccination: A Markov Chain Approach</title>
      <link>https://arxiv.org/abs/2507.10793</link>
      <description>arXiv:2507.10793v2 Announce Type: replace-cross 
Abstract: Understanding the dynamics of antibody levels is crucial for characterizing the time-dependent response to immune events: either infections or vaccinations. The sequence and timing of these events significantly influence antibody level changes. Despite extensive interest in the topic in the recent years and many experimental studies, the effect of immune event sequences on antibody levels is not well understood. Moreover, disease or vaccination prevalence in the population are time-dependent. This, alongside the complexities of personal antibody kinetics, makes it difficult to analyze a sample immune measurement from a population. As a solution, we design a rigorous mathematical characterization in terms of a time-inhomogeneous Markov chain model for event-to-event transitions coupled with a probabilistic framework for the post-event antibody kinetics of multiple immune events. We demonstrate that this is an ideal model for immune event sequences, referred to as personal trajectories. This novel modeling framework surpasses the susceptible-infected-recovered (SIR) characterizations by rigorously tracking the probability distribution of population antibody response across time. To illustrate our ideas, we apply our mathematical framework to longitudinal severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) data from individuals with multiple documented infection and vaccination events. Our work is an important step towards a comprehensive understanding of antibody kinetics that could lead to an effective way to analyze the protective power of natural immunity or vaccination, predict missed immune events at an individual level, and inform booster timing recommendations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10793v2</guid>
      <category>q-bio.PE</category>
      <category>math.PR</category>
      <category>physics.bio-ph</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rayanne A. Luke, Prajakta Bedekar, Lyndsey M. Muehling, Glenda Canderan, Yesun Lee, Wesley A. Cheng, Judith A. Woodfolk, Jeffrey M. Wilson, Pia S. Pannaraj, Anthony J. Kearsley</dc:creator>
    </item>
  </channel>
</rss>
