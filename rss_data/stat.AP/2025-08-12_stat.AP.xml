<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Aug 2025 01:23:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Actuarial Analysis of an Infectious Disease Insurance based on an SEIARD Epidemiological Model</title>
      <link>https://arxiv.org/abs/2508.06580</link>
      <description>arXiv:2508.06580v1 Announce Type: new 
Abstract: The growing number of infectious disease outbreaks, like the one caused by the SARS-CoV-2 virus, underscores the necessity of actuarial models that can adapt to epidemic-driven risks. Traditional life insurance frameworks often rely on static mortality assumptions that fail to capture the temporal and behavioral complexity of disease transmission. In this paper, we propose an integrated actuarial framework based on the SEIARD epidemiological model. This framework enables the explicit modeling of incubation periods and disease-induced mortality. We derive key actuarial quantities, including the present value of annuity benefits, payment streams, and net premiums, based on SEIARD dynamics. We formulate a prospective reserve function and analyze its evolution throughout the course of an epidemic. Additionally, we examine the forces of infection, mortality, and removal to assess their impact on epidemic-adjusted survival probabilities. Numerical simulations implemented via a nonstandard finite difference (NSFD) scheme illustrate the model's applicability under various parameter settings and insurance policy assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06580v1</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Achraf Zinihi, Matthias Ehrhardt, Moulay Rchid Sidi Ammi</dc:creator>
    </item>
    <item>
      <title>Forecasting solar power output in Ibadan: A machine learning approach leveraging weather data and system specifications</title>
      <link>https://arxiv.org/abs/2508.07462</link>
      <description>arXiv:2508.07462v1 Announce Type: new 
Abstract: This study predicts hourly solar irradiance components, Global Horizontal Irradiance (GHI), Direct Normal Irradiance (DNI), and Diffuse Horizontal Irradiance (DHI) using meteorological data to forecast solar energy output in Ibadan, Nigeria. The forecasting process follows a two-stage approach: first, clear-sky irradiance values are predicted using weather variables only (e.g., temperature, humidity, wind speed); second, actual (cloudy-sky) irradiance values are forecasted by integrating the predicted clear-sky irradiance with weather variables and cloud type. Historical meteorological data were preprocessed and used to train Random Forest, Convolutional Neural Network (CNN), and Long Short-Term Memory (LSTM) models, with Random Forest demonstrating the best performance. Models were developed for annual and seasonal forecasting, capturing variations between the wet and dry seasons. The annual Random Forest model's normalised Root Mean Square Error (nRMSE) values were 0.22 for DHI, 0.33 for DNI, and 0.19 for GHI. For seasonal forecasts, wet season nRMSE values were 0.27 for DHI, 0.50 for DNI, and 0.27 for GHI, while dry season nRMSE values were 0.15 for DHI, 0.22 for DNI, and 0.12 for GHI. The predicted actual irradiance values were combined with solar system specifications (e.g., maximum power (Pmax), open-circuit voltage (Voc), short-circuit current (Isc), and AC power (Pac)) using PVLib Python to estimate the final energy output. This methodology provides a cost-effective alternative to pyranometer-based measurements, enhances grid stability for solar energy integration, and supports efficient planning for off-grid and grid-connected photovoltaic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07462v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Obarotu Peter Urhuerhi, Christopher Udomboso, Caston Sigauke</dc:creator>
    </item>
    <item>
      <title>Decomposing Global AUC into Cluster-Level Contributions for Localized Model Diagnostics</title>
      <link>https://arxiv.org/abs/2508.07495</link>
      <description>arXiv:2508.07495v1 Announce Type: new 
Abstract: The Area Under the ROC Curve (AUC) is a widely used performance metric for binary classifiers. However, as a global ranking statistic, the AUC aggregates model behavior over the entire dataset, masking localized weaknesses in specific subpopulations. In high-stakes applications such as credit approval and fraud detection, these weaknesses can lead to financial risk or operational failures. In this paper, we introduce a formal decomposition of global AUC into intra- and inter-cluster components. This allows practitioners to evaluate classifier performance within and across clusters of data, enabling granular diagnostics and subgroup analysis. We also compare the AUC with additive performance metrics such as the Brier score and log loss, which support decomposability and direct attribution. Our framework enhances model development and validation practice by providing additional insights to detect model weakness for model risk management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07495v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agus Sudjianto, Alice J. Liu</dc:creator>
    </item>
    <item>
      <title>A new approach to probabilistic population forecasting with an application to Estonia</title>
      <link>https://arxiv.org/abs/2508.07580</link>
      <description>arXiv:2508.07580v1 Announce Type: new 
Abstract: This paper shows how measures of uncertainty can be applied to existing population forecasts using Estonia as a case study. The measures of forecast uncertainty are relatively easy to calculate and meet several important criteria used by demographers who routinely generate population forecasts. This paper applies the uncertainty measures to a population forecast based on the Cohort-Component Method, which links the probabilistic world forecast uncertainty to demographic theory, an important consideration in developing accurate forecasts. We applied this approach to world population projections and compared the results to the Bayesian-based probabilistic world forecast produced by the United Nations, which we found to be similar but with more uncertainty than found in the latter. We did a similar comparison in regard to sub-national probabilistic forecasts and found our results to be similar with Bayesian-based uncertainty measures. These results suggest that the probability forecasts produced using our approach for Estonia are consistent with knowledge about forecast uncertainty. We conclude that this new method appears to be well-suited for developing probabilistic world, national, and sub-national population forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07580v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David A. Swanson, Jeff Tayman</dc:creator>
    </item>
    <item>
      <title>Derivation of Dietary Patterns dependent on Diabetes status using ordinal Supervised Robust Profile Clustering: Results from Hispanic Community Health Study/Study of Latinos</title>
      <link>https://arxiv.org/abs/2508.08083</link>
      <description>arXiv:2508.08083v1 Announce Type: new 
Abstract: The burden of diabetes has disproportionately impacted Hispanic/Latino residents in the United States, with diet recognized as a major modifiable risk factor. Outcome-dependent dietary patterns provide insight into what foods may be associated with the increased severity and progression of diabetes. However, the ethnic and geographical heterogeneity of US Hispanic/Latino adults makes it difficult to identify and distinguish differences within their diet as risk increases. Supervised robust profile clustering (sRPC) is a flexible joint model that can identify dietary patterns associated with diabetes, while partitioning out those defined by their ethnicity and geography. However, sRPC has only been applied to binary outcomes. We extend the existing model to develop the ordinal sRPC. Using baseline dietary data (2008-2011) from the Hispanic Community Health Study/Study of Latinos, we illustrate the utility of our model to identify dietary patterns associated with the three-levels of diabetes status (i.e. normal, pre-diabetes, diabetes). Simulation studies confirmed that ordinal sRPC improved identification and characterization of these patterns compared to a standard supervised latent class model. Results indicated that participants who had greater consumption of fruits, snack foods, and refined grain breads may be more likely to be associated with an increasing severity of diabetes status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08083v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Briana J. K. Stephenson, Daniela Sotres-Alvarez, Martha Daviglus, Ramon A. Durazo-Arvizu, Yasmin Mossavar-Rahmani, Jianwen Cai</dc:creator>
    </item>
    <item>
      <title>Tidal Triggering of Magnitude 7+ Earthquakes by Jupiter</title>
      <link>https://arxiv.org/abs/2508.07064</link>
      <description>arXiv:2508.07064v1 Announce Type: cross 
Abstract: This work uses a chi-squared test of independence to determine if days that include earthquakes greater than or equal to magnitude 7 (M7+) from 1960 to 2024 are truly independent of the position of Earth in its orbit around the sun. To this end, this study breaks up Earth's orbit into days offset on either side of two reference Earth-Sun-Planet orientations, or zero-points: opposition and inferior conjunction. A computer program is used to sample U.S.G.S. earthquake and N.A.S.A. Horizons ephemeris data for the last 64 years with the purpose of conducting 28,782 chi-squared tests-of-independence for all intervals (5 to 45 days) spanning the entirety of Earth's synodic period relative to these zero points for Jupiter, Venus, Saturn, and Mars. For Jupiter, 1,071 statistically significant intervals of M7+ activity are associated with two particular points: the inferior conjunction and what is here termed the "preceding neap" position. At both of these points, M7+ activity first increases (&gt;125% average) and then sharply decreases in a pulse-like fashion, with those lulls in M7+ activity (&lt;75% of average) lasting about a month. Both of these pulses of M7+ activity begin at Sun-Observer-Target (SOT) Angles near 45 degrees and 135 degrees, and this also observed for Venus and Saturn; Mars synodic period prevented any comparison of chi-squared intervals to SOT Angle. Although this study did not observe any obvious correlation of M7+ activity with the lunar cycle, the medians, means, and modes of the significant intervals returned by the chi-squared analysis for Jupiter range from 27 to 34 days, suggesting that intervals are more likely to be found significant by the chi-squared analysis if they average out the lunar cycle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07064v1</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>E. W. Holt, Eric Newman</dc:creator>
    </item>
    <item>
      <title>The Interaction Between Domestic Monetary Policy and Macroprudential Policy in Israel</title>
      <link>https://arxiv.org/abs/2508.07082</link>
      <description>arXiv:2508.07082v1 Announce Type: cross 
Abstract: The global financial crisis (GFC) triggered the use of macroprudential policies imposed on the banking sector. Using bank-level panel data for Israel for the period 2004-2019, we find that domestic macroprudential measures changed the composition of bank credit growth but did not affect the total credit growth rate. Specifically, we show that macroprudential measures targeted at the housing sector moderated housing credit growth but tended to increase business credit growth. We also find that accommodative monetary policy surprises tended to increase bank credit growth before the GFC. We show that accommodative monetary policy surprises increased consumer credit when interacting with macroprudential policies targeting the housing market. Accommodative monetary policy interacted with nonhousing macroprudential measures to increase total credit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07082v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.GN</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.econmod.2022.105872</arxiv:DOI>
      <arxiv:journal_reference>Economic Modelling, 112, 2022, 105872</arxiv:journal_reference>
      <dc:creator>Jonathan Benchimol, Inon Gamrasni, Michael Kahn, Sigal Ribon, Yossi Saadon, Noam Ben-Ze'ev, Asaf Segal, Yitzchak Shizgal</dc:creator>
    </item>
    <item>
      <title>LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference</title>
      <link>https://arxiv.org/abs/2508.07221</link>
      <description>arXiv:2508.07221v1 Announce Type: cross 
Abstract: Estimating individualized treatment effects from observational data presents a persistent challenge due to unmeasured confounding and structural bias. Causal Machine Learning (causal ML) methods, such as causal trees and doubly robust estimators, provide tools for estimating conditional average treatment effects. These methods have limited effectiveness in complex real-world environments due to the presence of latent confounders or those described in unstructured formats. Moreover, reliance on domain experts for confounder identification and rule interpretation introduces high annotation cost and scalability concerns. In this work, we proposed Large Language Model-based agents for automated confounder discovery and subgroup analysis that integrate agents into the causal ML pipeline to simulate domain expertise. Our framework systematically performs subgroup identification and confounding structure discovery by leveraging the reasoning capabilities of LLM-based agents, which reduces human dependency while preserving interpretability. Experiments on real-world medical datasets show that our proposed approach enhances treatment effect estimation robustness by narrowing confidence intervals and uncovering unrecognized confounding biases. Our findings suggest that LLM-based agents offer a promising path toward scalable, trustworthy, and semantically aware causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07221v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Po-Han Lee, Yu-Cheng Lin, Chan-Tung Ku, Chan Hsu, Pei-Cing Huang, Ping-Hsun Wu, Yihuang Kang</dc:creator>
    </item>
    <item>
      <title>Is Repeated Bayesian Interim Analysis Consequence-Free?</title>
      <link>https://arxiv.org/abs/2508.07403</link>
      <description>arXiv:2508.07403v1 Announce Type: cross 
Abstract: Interim analyses are commonly used in clinical trials to enable early stopping for efficacy, futility, or safety. While their impact on frequentist operating characteristics is well studied and broadly understood, the effect of repeated Bayesian interim analyses - when conducted without appropriate multiplicity adjustment - remains an area of active debate. In this article, we provide both theoretical justification and numerical evidence illustrating how such analyses affect key inferential properties, including bias, mean squared error, the coverage probability of posterior credible intervals, false discovery rate, familywise error rate, and power. Our findings demonstrate that Bayesian interim analyses can significantly alter a trial's operating characteristics, even when the prior used for Bayesian inference is correctly specified and aligned with the data-generating process. Extensive simulation studies, covering a variety of endpoints, trial designs (single-arm and two-arm randomized controlled trials), and scenarios with both correctly specified and misspecified priors, support theoretical insights. Collectively, these results underscore the necessity of appropriate adjustment, thoughtful prior specification, and comprehensive evaluation to ensure valid and reliable inference in Bayesian adaptive trial designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07403v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Suyu Liu, Beibei Guo, Laura Thompson, Lei Nie, Ying Yuan</dc:creator>
    </item>
    <item>
      <title>An Approximate Maximum Likelihood Estimator for Discretely Observed Linear Birth-and-Death Processes</title>
      <link>https://arxiv.org/abs/2508.07527</link>
      <description>arXiv:2508.07527v1 Announce Type: cross 
Abstract: Linear birth-and-death processes (LBDPs) are foundational stochastic models in population dynamics, evolutionary biology, and hematopoiesis. Estimating parameters from discretely observed data is computationally demanding due to irregular sampling, noise, and missing values. We propose a novel approximate maximum likelihood estimator (MLE) for LBDPs based on a Gaussian approximation to transition probabilities. The approach transforms estimation into a univariate optimization problem, achieving substantial computational gains without sacrificing accuracy.
  Through simulations, we show that the approximate MLE outperforms Gaussian and saddlepoint-based estimators in speed and precision under realistic noise and sparsity. Applied to longitudinal clonal hematopoiesis data, the method produces biologically meaningful growth estimates even with noisy, compositional input. Unlike Gaussian and saddlepoint approximations, our estimator is invariant to data scaling, making it ideal for real-world applications such as variant allele frequency analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07527v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaochen Long, Marek Kimmel</dc:creator>
    </item>
    <item>
      <title>Human Extinction A Demographic Perspective</title>
      <link>https://arxiv.org/abs/2508.07568</link>
      <description>arXiv:2508.07568v1 Announce Type: cross 
Abstract: Studies that predict species extinction have focused on a range of flora and fauna but in regard to Homo sapiens there are, with one notable exception, no predictive studies, only considerations of possible ways this may occur. The exception believes extinction of Homo sapiens will happen in 10,000 years. We agree that extinction will happen, but we disagree on the timing: The work we present here suggests that if the current decline in birth rates continues, humans could be extinct by 2394. If we consider the absence of working-age people and the accompanying collapse of services, the survivorship rates would most likely be lower. Given this, it is plausible that extinction could occur around 2359. We also examined a scenario in which births ended in 2024, which revealed that Homo sapiens would become extinct in 2134. Given societal collapse, extinction under the zero births scenario could occur around 2089.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07568v1</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David A. Swanson, Jeff Tayman</dc:creator>
    </item>
    <item>
      <title>EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration</title>
      <link>https://arxiv.org/abs/2508.07671</link>
      <description>arXiv:2508.07671v1 Announce Type: cross 
Abstract: Current AI approaches to refugee integration optimize narrow objectives such as employment and fail to capture the cultural, emotional, and ethical dimensions critical for long-term success. We introduce EMPATHIA (Enriched Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance), a multi-agent framework addressing the central Creative AI question: how do we preserve human dignity when machines participate in life-altering decisions? Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes integration into three modules: SEED (Socio-cultural Entry and Embedding Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency Engine) for early independence, and THRIVE (Transcultural Harmony and Resilience through Integrated Values and Engagement) for sustained outcomes. SEED employs a selector-validator architecture with three specialized agents - emotional, cultural, and ethical - that deliberate transparently to produce interpretable recommendations. Experiments on the UN Kakuma dataset (15,026 individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic variables achieved 87.4% validation convergence and explainable assessments across five host countries. EMPATHIA's weighted integration of cultural, emotional, and ethical factors balances competing value systems while supporting practitioner-AI collaboration. By augmenting rather than replacing human expertise, EMPATHIA provides a generalizable framework for AI-driven allocation tasks where multiple values must be reconciled.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07671v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Rayan Barhdadi, Mehmet Tuncel, Erchin Serpedin, Hasan Kurban</dc:creator>
    </item>
    <item>
      <title>Soil Texture Prediction with Bayesian Generalized Additive Models for Spatial Compositional Data</title>
      <link>https://arxiv.org/abs/2508.07708</link>
      <description>arXiv:2508.07708v1 Announce Type: cross 
Abstract: Compositional data (CoDa) plays an important role in many fields such as ecology, geology, or biology. The most widely used modeling approaches are based on the Dirichlet and the logistic-normal formulation under Aitchison geometry. Recent developments in the mathematical field on the simplex geometry allow to express the regression model in terms of coordinates and estimate its coefficients. Once the model is projected in the real space, we can employ a multivariate Gaussian regression to deal with it. However, most existing methods focus on linear models, and there is a lack of flexible alternatives such as additive or spatial models, especially within a Bayesian framework and with practical implementation details.
  In this work, we present a geoadditive regression model for CoDa from a Bayesian perspective using the brms package in R. The model applies the isometric log-ratio (ilr) transformation and penalized splines to incorporate nonlinear effects. We also propose two new Bayesian goodness-of-fit measures for CoDa regression: BR-CoDa-$R^2$ and BM-CoDa-$R^2$, extending the Bayesian $R^2$ to the compositional setting. These measures, alongside WAIC, support model selection and evaluation. The methodology is validated through simulation studies and applied to predict soil texture composition in the Basque Country. Results demonstrate good performance, interpretable spatial patterns, and reliable quantification of explained variability in compositional outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07708v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joaqu\'in Mart\'inez-Minaya, Lore Zumeta-Olaskoaga, Dae-Jin Lee</dc:creator>
    </item>
    <item>
      <title>Symbolic Quantile Regression for the Interpretable Prediction of Conditional Quantiles</title>
      <link>https://arxiv.org/abs/2508.08080</link>
      <description>arXiv:2508.08080v1 Announce Type: cross 
Abstract: Symbolic Regression (SR) is a well-established framework for generating interpretable or white-box predictive models. Although SR has been successfully applied to create interpretable estimates of the average of the outcome, it is currently not well understood how it can be used to estimate the relationship between variables at other points in the distribution of the target variable. Such estimates of e.g. the median or an extreme value provide a fuller picture of how predictive variables affect the outcome and are necessary in high-stakes, safety-critical application domains. This study introduces Symbolic Quantile Regression (SQR), an approach to predict conditional quantiles with SR. In an extensive evaluation, we find that SQR outperforms transparent models and performs comparably to a strong black-box baseline without compromising transparency. We also show how SQR can be used to explain differences in the target distribution by comparing models that predict extreme and central outcomes in an airline fuel usage case study. We conclude that SQR is suitable for predicting conditional quantiles and understanding interesting feature influences at varying quantiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08080v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Cas Oude Hoekstra, Floris den Hengst</dc:creator>
    </item>
    <item>
      <title>Ease and Equity of Point of Interest Accessibility via Public Transit in the U.S</title>
      <link>https://arxiv.org/abs/2212.06954</link>
      <description>arXiv:2212.06954v2 Announce Type: replace 
Abstract: Equitable access to essential Points of Interest (POIs) such as healthcare facilities and grocery stores via public transit is a critical urban planning challenge. This paper introduces an interactive tool that analyzes the ease and equity of such access in major U.S. cities. Using a 2-step floating catchment area (2SFCA) approach, we calculate granular accessibility indices and present them on an interactive, web-based platform. A key contribution of this work is a scenario analysis feature that allows users to simulate the impact of adding or removing POIs and visualize the resulting changes in accessibility equity. Our analysis reveals significant disparities across demographic groups, and we further utilize machine learning models to demonstrate a strong, quantifiable link between accessibility scores and neighborhood racial composition, highlighting systemic inequities. The tool provides a practical framework for policymakers to make more informed and equitable infrastructure decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.06954v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Li, Mengyang Liu, Aurimas Racas, Tejas Santanam, Junaid Syed, Przemyslaw Zientala</dc:creator>
    </item>
    <item>
      <title>Redistricting Reforms Reduce Gerrymandering by Constraining Partisan Actors</title>
      <link>https://arxiv.org/abs/2407.11336</link>
      <description>arXiv:2407.11336v3 Announce Type: replace 
Abstract: Political actors often manipulate redistricting plans to gain electoral advantages, a process known as gerrymandering. Several states have implemented institutional reforms to address this problem, such as establishing map-drawing commissions. Estimating the impact of such reforms is challenging because each state structures its processes and rules differently. We model redistricting as a sequential game whose equilibrium solution summarizes multi-step institutional interactions as a univariate score. We argue this score measures the leeway political actors have over the partisan lean of the final plan. Using a differences-in-differences design, we demonstrate that reforms reduce partisan bias and increase competitiveness when they constrain partisan actors. We perform a counterfactual policy analysis to estimate the effects of enacting recent reforms nationwide. Though commissions generally reduce bias, reforms that restrict partisan actors in multiple ways, like removing veto points (Michigan), are more effective than commissions where parties retain some control (Ohio).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11336v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cory McCartan, Christopher T. Kenny, Tyler Simko, Emma Ebowe, Michael Y. Zhao, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>Collective Reasoning Among LLMs: A Framework for Answer Validation Without Ground Truth</title>
      <link>https://arxiv.org/abs/2502.20758</link>
      <description>arXiv:2502.20758v3 Announce Type: replace 
Abstract: We introduce a new approach in which several advanced large language models-specifically GPT-4-0125-preview, Meta-LLAMA-3-70B-Instruct, Claude-3-Opus, and Gemini-1.5-Flash-collaborate to both produce and answer intricate, doctoral-level probability problems without relying on any single "correct" reference. Rather than depending on an established ground truth, our investigation focuses on how agreement among diverse models can signal the reliability of their outputs and, by extension, reflect the overall quality of the generated questions. To measure this inter-model alignment, we apply a suite of statistical evaluations, including chi-square tests, Fleiss' Kappa coefficients, and confidence interval calculations, thereby capturing both precision in answers and clarity in question phrasing. Our analysis reveals that Claude and Gemini tend to frame questions more coherently and unambiguously, which is evidenced by their tighter confidence intervals and greater concordance with responding agents. In contrast, LLAMA exhibits wider confidence bands and a lower level of agreement, indicating more variability and reduced consistency in its question formulations. These observations support the notion that a multi-model collaborative strategy not only improves answer dependability but also offers an effective, data-driven mechanism for evaluating and refining question quality when no definitive solution exists. Ultimately, this work delivers actionable insights into enhancing AI-guided reasoning processes through coordinated interactions among heterogeneous language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20758v3</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Seyed Pouyan Mousavi Davoudi, Amin Gholami Davodi, Alireza Amiri-Margavi, Alireza Shafiee Fard, Mahdi Jafari</dc:creator>
    </item>
    <item>
      <title>Investigating Experiential Effects in Online Chess using a Hierarchical Bayesian Analysis</title>
      <link>https://arxiv.org/abs/2503.21713</link>
      <description>arXiv:2503.21713v2 Announce Type: replace 
Abstract: The presence or absence of winner-loser effects is a widely discussed phenomenon across both sports and psychology research. Investigation of such effects is often hampered by the limited availability of data. Online chess has exploded in popularity in recent years and provides vast amounts of data which can be used to explore this question. With a hierarchical Bayesian regression model, we carefully investigate the presence of such experiential effects in online chess. Using a large quantity of online chess data, we see little evidence for experiential effects that are consistent across all players, with some individual players showing some evidence for such effects. Given the challenging temporal nature of this data, we discuss several methods for assessing the suitability of our model and carefully check its validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21713v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Gee, Sydney O. Seese, James P. Curley, Owen G. Ward</dc:creator>
    </item>
    <item>
      <title>Forecasting age distribution of deaths across countries: Life expectancy and annuity valuation</title>
      <link>https://arxiv.org/abs/2507.04303</link>
      <description>arXiv:2507.04303v2 Announce Type: replace 
Abstract: We investigate two transformations within the framework of compositional data analysis for forecasting the age distribution of death counts. Drawing on age-specific period life-table death counts from 24 countries in the Human Mortality Database, we assess and compare the point and interval forecast accuracy of the two transformations. Enhancing the forecast accuracy of period life-table death counts holds significant value for demographers, who rely on such forecasts to estimate survival probabilities and life expectancy, and for actuaries, who use them to price temporary annuities across various entry ages and maturities. While our primary focus is on temporary annuities, we also consider long-term contracts that, particularly at higher entry ages, approximate lifetime annuities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04303v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang, Steven Haberman</dc:creator>
    </item>
    <item>
      <title>Randomization Inference for Before-and-After Studies with Multiple Units: An Application to a Criminal Procedure Reform in Uruguay</title>
      <link>https://arxiv.org/abs/2410.15477</link>
      <description>arXiv:2410.15477v2 Announce Type: replace-cross 
Abstract: Learning about the immediate causal effects of large-scale policy interventions poses a significant challenge for quasi-experimental methods that rely on long-term trends or parametric modeling assumptions. As an alternative, we develop a randomization inference framework for before-and-after studies with multiple units, designed specifically for short-term causal inference and allowing for general assignment mechanisms. The method provides finite-sample-valid statistical inferences without relying on parametric time series models or extrapolation. We demonstrate its utility by analyzing a major criminal justice reform in Uruguay that switched from an inquisitorial to an adversarial system in November 2017. Our method relies on the key assumption of no local time trends near the policy adoption time, which is supported by several falsification tests in our empirical study. We find a statistically significant short-term causal effect: an increase of approximately 25 daily police reports (an 8% rise) in the first week of the new justice system. Our randomization inference framework provides a robust and flexible methodology for evaluating policy adoptions in before-and-after studies with multiple units.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15477v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Carlos Diaz, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>Rigorous analytic solution to the gravitational-wave overlapping event rates</title>
      <link>https://arxiv.org/abs/2501.05218</link>
      <description>arXiv:2501.05218v2 Announce Type: replace-cross 
Abstract: In the era of the next-generation gravitational-wave detectors, signal overlaps will become prevalent due to high detection rate and long signal duration, posing significant challenges to data analysis. While effective algorithms are being developed, there still lacks an integrated understanding on the statistical properties for the population of overlapping compact-binary-coalescence signals. For the first time, in order to aid rapid and robust estimation, we rigorously derive and establish analytical expressions for the expectation and variance for the number of overlapping events. This framework is highly extensible, allowing analytical calculation for more complicated scenarios, such as multi-signal overlaps, overlaps between different types of sources, and source-dependent thresholds. We also mathematically prove that the time difference between events in a single observation run is described by the beta distribution, offering an analytical prior reference for Bayesian analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05218v2</guid>
      <category>gr-qc</category>
      <category>astro-ph.HE</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3847/1538-4357/adfa1d</arxiv:DOI>
      <dc:creator>Ziming Wang, Zexin Hu, Lijing Shao</dc:creator>
    </item>
    <item>
      <title>Adaptive sequential Monte Carlo for structured cross validation in Bayesian hierarchical models</title>
      <link>https://arxiv.org/abs/2501.07685</link>
      <description>arXiv:2501.07685v2 Announce Type: replace-cross 
Abstract: Importance sampling (IS) is commonly used for cross validation (CV) in Bayesian models, because it only involves reweighting existing posterior draws without needing to re-estimate the model by re-running Markov chain Monte Carlo (MCMC). For hierarchical models, standard IS can be unreliable; the out-of-sample generalization hypothesis may involve structured case-deletion schemes which significantly alter the posterior geometry. This can force costly MCMC re-runs and make CV impractical. As a principled alternative, we tailor adaptive sequential Monte Carlo to sample along a path of posteriors that leads to the case-deleted posterior. The sampler is designed to support various hypotheses by accommodating diverse CV designs, and to streamline the workflow by automating path construction and systematically minimizing MCMC intervention. We demonstrate its utility with three types of predictive model assessment: longitudinal leave-group-out CV, group $K$-fold CV, and sequential one-step-ahead validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07685v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geonhee Han, Andrew Gelman</dc:creator>
    </item>
    <item>
      <title>Likelihood-based Modeling of Covariate-Specific Time-Dependent ROC Curves</title>
      <link>https://arxiv.org/abs/2502.20892</link>
      <description>arXiv:2502.20892v2 Announce Type: replace-cross 
Abstract: Identifying reliable biomarkers for predicting clinical events in longitudinal studies is important for accurate disease prognosis and for guiding development of new treatments. However, prognostic studies are often observational, making it difficult to account for patient heterogeneity. In amyotrophic lateral sclerosis (ALS), factors such as age, site of onset and genetic status influence both survival and biomarker levels, yet their impact on the prognostic accuracy of biomarkers over time remains unclear. While time-dependent receiver operating characteristic methods have been developed to handle censored time-to-event outcomes, most do not adjust for covariates. To address this, we propose the nonparanormal prognostic biomarker framework, which models the joint distribution of the biomarker and event time while accounting for covariates. This allows estimation of covariate-specific time-dependent ROC curves and related summary measures. We apply the NPB framework to evaluate serum neurofilament light as a prognostic biomarker in ALS, showing that its accuracy varies over time and with patient characteristics. By capturing these covariate-specific effects, the NPB framework supports more targeted risk stratification and can potentially improve the design of clinical trials for new ALS treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20892v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ainesh Sewak, Vanda Inacio, Joanne Wuu, Michael Benatar, Torsten Hothorn</dc:creator>
    </item>
    <item>
      <title>Mechanistic inference of stochastic gene expression from structured single-cell data</title>
      <link>https://arxiv.org/abs/2505.11460</link>
      <description>arXiv:2505.11460v2 Announce Type: replace-cross 
Abstract: Single-cell gene expression measurements encode variability spanning molecular noise, cell-to-cell heterogeneity, and technical artifacts. Mechanistic stochastic models provide powerful approaches to disentangle these sources, yet inferring underlying dynamics from standard snapshot sequencing data faces fundamental identifiability limitations. This review focuses on how structured datasets with temporal, spatial, or multimodal features offer constraints to resolve these ambiguities, but demand more sophisticated models and inference strategies, including machine-learning techniques with inherent tradeoffs. We highlight recent progress in the judicious integration of structured single-cell data, stochastic model development, and innovative inference strategies to extract predictive, gene-level insights. These advances lay the foundation for scaling mechanistic inference upward to regulatory networks and multicellular tissues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11460v2</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher E. Miles</dc:creator>
    </item>
  </channel>
</rss>
