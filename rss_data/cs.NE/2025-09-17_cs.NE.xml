<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Sep 2025 04:00:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Snail Homing and Mating Search Algorithm for Weight Optimization of Stepped-Transmission Shaft</title>
      <link>https://arxiv.org/abs/2509.13721</link>
      <description>arXiv:2509.13721v1 Announce Type: new 
Abstract: In this paper, the steeped-transmission shaft design problem is proposed for weight optimization. The bio-inspired search-based Snail Homing and Mating Search (SHMS) algorithm is utilized to solve the problem. It is inspired by the social behaviour of snails and their inherent nature of finding better homes, and mate. The proposed steeped-transmission shaft design problem is modelled considering the fatigue loading, combined bending, torsion loads, and the principle of Modified Goodman criteria. The forces diagram and the bending moment diagrams are obtained using the MDSOLIDS software. The forces and bending moment are then used to mathematical model the objective function and constraints. The SHMS algorithm has yielded the desired solution with reasonable computational cost. The constraints are handled using a static penalty function approach. The statistical results obtained using SHMS algorithm are further used for generating CAD model. The analysis is carried out in ANSYS Workbench. Further, the deflection obtained from SHMS algorithm and ANSYS Workbench are compared and results are discussed in details.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13721v1</guid>
      <category>cs.NE</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaustav Saha, Ishaan R Kale, Vivek Patel, Anand J Kulkarni, Puskaraj D Sonawwanay</dc:creator>
    </item>
    <item>
      <title>A neuromorphic continuous soil monitoring system for precision irrigation</title>
      <link>https://arxiv.org/abs/2509.14066</link>
      <description>arXiv:2509.14066v1 Announce Type: new 
Abstract: Sensory processing at the edge requires ultra-low power stand-alone computing technologies. This is particularly true for modern agriculture and precision irrigation systems which aim to optimize water usage by monitoring key environmental observables continuously using distributed efficient embedded processing elements. Neuromorphic processing systems are emerging as a promising technology for extreme edge-computing applications that need to run on resource-constrained hardware. As such, they are a very good candidate for implementing efficient water management systems based on data measured from soil and plants, across large fields. In this work, we present a fully energy-efficient neuromorphic irrigation control system that operates autonomously without any need for data transmission or remote processing. Leveraging the properties of a biologically realistic spiking neural network, our system performs computation, and decision-making locally. We validate this approach using real-world soil moisture data from apple and kiwi orchards applied to a mixed-signal neuromorphic processor, and show that the generated irrigation commands closely match those derived from conventional methods across different soil depths. Our results show that local neuromorphic inference can maintain decision accuracy, paving the way for autonomous, sustainable irrigation solutions at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14066v1</guid>
      <category>cs.NE</category>
      <category>cs.ET</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mirco Tincani, Khaled Kerouch, Umberto Garlando, Mattia Barezzi, Alessandro Sanginario, Giacomo Indiveri, Chiara De Luca</dc:creator>
    </item>
    <item>
      <title>Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks</title>
      <link>https://arxiv.org/abs/2509.13338</link>
      <description>arXiv:2509.13338v1 Announce Type: cross 
Abstract: This work proposes an evidence-retrieval mechanism for uncertainty-aware decision-making that replaces a single global cutoff with an evidence-conditioned, instance-adaptive criterion. For each test instance, proximal exemplars are retrieved in an embedding space; their predictive distributions are fused via Dempster-Shafer theory. The resulting fused belief acts as a per-instance thresholding mechanism. Because the supporting evidences are explicit, decisions are transparent and auditable. Experiments on CIFAR-10/100 with BiT and ViT backbones show higher or comparable uncertainty-aware performance with materially fewer confidently incorrect outcomes and a sustainable review load compared with applying threshold on prediction entropy. Notably, only a few evidences are sufficient to realize these gains; increasing the evidence set yields only modest changes. These results indicate that evidence-conditioned tagging provides a more reliable and interpretable alternative to fixed prediction entropy thresholds for operational uncertainty-aware decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13338v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hassan Gharoun, Mohammad Sadegh Khorshidi, Kasra Ranjbarigderi, Fang Chen, Amir H. Gandomi</dc:creator>
    </item>
    <item>
      <title>Why all roads don't lead to Rome: Representation geometry varies across the human visual cortical hierarchy</title>
      <link>https://arxiv.org/abs/2509.13459</link>
      <description>arXiv:2509.13459v1 Announce Type: cross 
Abstract: Biological and artificial intelligence systems navigate the fundamental efficiency-robustness tradeoff for optimal encoding, i.e., they must efficiently encode numerous attributes of the input space while also being robust to noise. This challenge is particularly evident in hierarchical processing systems like the human brain. With a view towards understanding how systems navigate the efficiency-robustness tradeoff, we turned to a population geometry framework for analyzing representations in the human visual cortex alongside artificial neural networks (ANNs). In the ventral visual stream, we found general-purpose, scale-free representations characterized by a power law-decaying eigenspectrum in most areas. However, in certain higher-order visual areas did not have scale-free representations, indicating that scale-free geometry is not a universal property of the brain. In parallel, ANNs trained with a self-supervised learning objective also exhibited free-free geometry, but not after fine-tune on a specific task. Based on these empirical results and our analytical insights, we posit that a system's representation geometry is not a universal property and instead depends upon the computational objective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13459v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arna Ghosh, Zahraa Chorghay, Shahab Bakhtiari, Blake A. Richards</dc:creator>
    </item>
    <item>
      <title>Circuit realization and hardware linearization of monotone operator equilibrium networks</title>
      <link>https://arxiv.org/abs/2509.13793</link>
      <description>arXiv:2509.13793v1 Announce Type: cross 
Abstract: It is shown that the port behavior of a resistor-diode network corresponds to the solution of a ReLU monotone operator equilibrium network (a neural network in the limit of infinite depth), giving a parsimonious construction of a neural network in analog hardware. We furthermore show that the gradient of such a circuit can be computed directly in hardware, using a procedure we call hardware linearization. This allows the network to be trained in hardware, which we demonstrate with a device-level circuit simulation. We extend the results to cascades of resistor-diode networks, which can be used to implement feedforward and other asymmetric networks. We finally show that different nonlinear elements give rise to different activation functions, and introduce the novel diode ReLU which is induced by a non-ideal diode model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13793v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Thomas Chaffey</dc:creator>
    </item>
    <item>
      <title>How Fly Neural Perception Mechanisms Enhance Visuomotor Control of Micro Robots</title>
      <link>https://arxiv.org/abs/2509.13827</link>
      <description>arXiv:2509.13827v1 Announce Type: cross 
Abstract: Anyone who has tried to swat a fly has likely been frustrated by its remarkable agility.This ability stems from its visual neural perception system, particularly the collision-selective neurons within its small brain.For autonomous robots operating in complex and unfamiliar environments, achieving similar agility is highly desirable but often constrained by the trade-off between computational cost and performance.In this context, insect-inspired intelligence offers a parsimonious route to low-power, computationally efficient frameworks.In this paper, we propose an attention-driven visuomotor control strategy inspired by a specific class of fly visual projection neurons-the lobula plate/lobula column type-2 (LPLC2)-and their associated escape behaviors.To our knowledge, this represents the first embodiment of an LPLC2 neural model in the embedded vision of a physical mobile robot, enabling collision perception and reactive evasion.The model was simplified and optimized at 70KB in memory to suit the computational constraints of a vision-based micro robot, the Colias, while preserving key neural perception mechanisms.We further incorporated multi-attention mechanisms to emulate the distributed nature of LPLC2 responses, allowing the robot to detect and react to approaching targets both rapidly and selectively.We systematically evaluated the proposed method against a state-of-the-art locust-inspired collision detection model.Results showed that the fly-inspired visuomotor model achieved comparable robustness, at success rate of 96.1% in collision detection while producing more adaptive and elegant evasive maneuvers.Beyond demonstrating an effective collision-avoidance strategy, this work highlights the potential of fly-inspired neural models for advancing research into collective behaviors in insect intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13827v1</guid>
      <category>cs.RO</category>
      <category>cs.NE</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Renyuan Liu, Haoting Zhou, Chuankai Fang, Qinbing Fu</dc:creator>
    </item>
    <item>
      <title>Evolution Meets Diffusion: Efficient Neural Architecture Generation</title>
      <link>https://arxiv.org/abs/2504.17827</link>
      <description>arXiv:2504.17827v4 Announce Type: replace 
Abstract: Neural Architecture Search (NAS) has gained widespread attention for its transformative potential in deep learning model design. However, the vast and complex search space of NAS leads to significant computational and time costs. Neural Architecture Generation (NAG) addresses this by reframing NAS as a generation problem, enabling the precise generation of optimal architectures for specific tasks. Despite its promise, mainstream methods like diffusion models face limitations in global search capabilities and are still hindered by high computational and time demands. To overcome these challenges, we propose Evolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel approach that achieves efficient and training-free architecture generation. EDNAG leverages evolutionary algorithms to simulate the denoising process in diffusion models, using fitness to guide the transition from random Gaussian distributions to optimal architecture distributions. This approach combines the strengths of evolutionary strategies and diffusion models, enabling rapid and effective architecture generation. Extensive experiments demonstrate that EDNAG achieves state-of-the-art (SOTA) performance in architecture optimization, with an improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need for time-consuming training and boosts inference speed by an average of 50 times, showcasing its exceptional efficiency and effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17827v4</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bingye Zhou, Caiyang Yu</dc:creator>
    </item>
    <item>
      <title>Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization</title>
      <link>https://arxiv.org/abs/2507.20923</link>
      <description>arXiv:2507.20923v2 Announce Type: replace 
Abstract: Multi-objective combinatorial optimization problems (MOCOP) frequently arise in practical applications that require the simultaneous optimization of conflicting objectives. Although traditional evolutionary algorithms can be effective, they typically depend on domain knowledge and repeated parameter tuning, limiting flexibility when applied to unseen MOCOP instances. Recently, integration of Large Language Models (LLMs) into evolutionary computation has opened new avenues for automatic heuristic generation, using their advanced language understanding and code synthesis capabilities. Nevertheless, most existing approaches predominantly focus on single-objective tasks, often neglecting key considerations such as runtime efficiency and heuristic diversity in multi-objective settings. To bridge this gap, we introduce Multi-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a novel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO) framework that leverages LLMs and Pareto Front Grid (PFG) technique. By partitioning the objective space into grids and retaining top-performing candidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize heuristics with semantically distinct logical structures during variation, thus promoting diversity and mitigating redundancy within the population. Through extensive evaluations, MPaGE demonstrates superior performance over existing LLM-based frameworks, and achieves competitive results to traditional Multi-objective evolutionary algorithms (MOEAs), with significantly faster runtime. Our code is available at: https://github.com/langkhachhoha/MPaGE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20923v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minh Hieu Ha, Hung Phan, Tung Duy Doan, Tung Dao, Dao Tran, Huynh Thi Thanh Binh</dc:creator>
    </item>
    <item>
      <title>Utilizing Novelty-based Evolution Strategies to Train Transformers in Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2502.06301</link>
      <description>arXiv:2502.06301v2 Announce Type: replace-cross 
Abstract: In this paper, we experiment with novelty-based variants of OpenAI-ES, the NS-ES and NSR-ES algorithms, and evaluate their effectiveness in training complex, transformer-based architectures designed for the problem of reinforcement learning, such as Decision Transformers. We also test if we can accelerate the novelty-based training of these larger models by seeding the training with a pretrained models. The experimental results were mixed. NS-ES showed progress, but it would clearly need many more iterations for it to yield interesting agents. NSR-ES, on the other hand, proved quite capable of being straightforwardly used on larger models, since its performance appears as similar between the feed-forward model and Decision Transformer, as it was for the OpenAI-ES in our previous work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06301v2</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maty\'a\v{s} Lorenc, Roman Neruda</dc:creator>
    </item>
    <item>
      <title>Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling</title>
      <link>https://arxiv.org/abs/2505.21717</link>
      <description>arXiv:2505.21717v4 Announce Type: replace-cross 
Abstract: We present LrcSSM, a $\textit{non-linear}$ recurrent model that processes long sequences as fast as today's linear state-space layers. By forcing the Jacobian matrix to be diagonal, the full sequence can be solved in parallel, giving $\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential depth, for input-sequence length $T$ and a state dimension $D$. Moreover, LrcSSM offers a formal gradient-stability guarantee that other input-varying systems such as Liquid-S4 and Mamba do not provide. Importantly, the diagonal Jacobian structure of our model results in no performance loss compared to the original model with dense Jacobian, and the approach can be generalized to other non-linear recurrent models, demonstrating broader applicability. On a suite of long-range forecasting tasks, we demonstrate that LrcSSM outperforms Transformers, LRU, S5, and Mamba.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21717v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>M\'onika Farsang, Ramin Hasani, Daniela Rus, Radu Grosu</dc:creator>
    </item>
  </channel>
</rss>
