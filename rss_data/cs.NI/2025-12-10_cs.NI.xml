<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Dec 2025 05:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Collaborative Intelligence for UAV-Satellite Network Slicing: Towards a Joint QoS-Energy-Fairness MADRL Optimization</title>
      <link>https://arxiv.org/abs/2512.08322</link>
      <description>arXiv:2512.08322v1 Announce Type: new 
Abstract: Non terrestrial networks are critical for achieving global 6G coverage, yet efficient resource management in aerial and space environments remains challenging due to limited onboard power and dynamic operational conditions. Network slicing offers a promising solution for spectrum optimization in UAV based systems serving heterogeneous service demands. For that, this paper proposes a hierarchical network slicing framework for UAV satellite integrated networks supporting eMBB, URLLC, and mMTC services. Specifically, we formulate a joint optimization of UAV trajectory, transmission power, and spectrum allocation as a decentralized partially observable Markov decision process that ensures quality of service while minimizing energy consumption and maximizing resource fairness. To address the computational intractability and partial observability, we develop a multi agent deep reinforcement learning solution under the centralized training and decentralized execution paradigm. In the proposed system, UAV agents act as distributed actors coordinated by a shared critic operating with multi head attention mechanism at a low Earth orbit satellite. Experimental results then demonstrate that our approach outperforms existing methods by up to 33% in cumulative reward while achieving superior energy efficiency and fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08322v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thanh-Dao Nguyen, Ngoc-Tan Nguyen, Thai-Duong Nguyen, Nguyen Van Huynh, Dinh-Hieu Tran, Symeon Chatzinotas</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks</title>
      <link>https://arxiv.org/abs/2512.08341</link>
      <description>arXiv:2512.08341v1 Announce Type: new 
Abstract: The deployment of Unmanned Aerial Vehicle (UAV) swarms as dynamic communication relays is critical for next-generation tactical networks. However, operating in contested environments requires solving a complex trade-off, including maximizing system throughput while ensuring collision avoidance and resilience against adversarial jamming. Existing heuristic-based approaches often struggle to find effective solutions due to the dynamic and multi-objective nature of this problem. This paper formulates this challenge as a cooperative Multi-Agent Reinforcement Learning (MARL) problem, solved using the Centralized Training with Decentralized Execution (CTDE) framework. Our approach employs a centralized critic that uses global state information to guide decentralized actors which operate using only local observations. Simulation results show that our proposed framework significantly outperforms heuristic baselines, increasing the total system throughput by approximately 50% while simultaneously achieving a near-zero collision rate. A key finding is that the agents develop an emergent anti-jamming strategy without explicit programming. They learn to intelligently position themselves to balance the trade-off between mitigating interference from jammers and maintaining effective communication links with ground users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08341v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thai Duong Nguyen, Ngoc-Tan Nguyen, Thanh-Dao Nguyen, Nguyen Van Huynh, Dinh-Hieu Tran, Symeon Chatzinotas</dc:creator>
    </item>
    <item>
      <title>Turning Threat into Opportunity: DRL-Powered Anti-Jamming via Energy Harvesting in UAV-Disrupted Channels</title>
      <link>https://arxiv.org/abs/2512.08351</link>
      <description>arXiv:2512.08351v1 Announce Type: new 
Abstract: The open and broadcast nature of wireless communication systems, while enabling ubiquitous connectivity, also exposes them to jamming attacks that may critically compromise network performance or disrupt service availability. The proliferation of Unmanned Aerial Vehicles (UAVs) introduces a new dimension to this threat, as UAVs can act as mobile, intelligent jammers capable of launching sophisticated attacks by leveraging Line-of-Sight (LoS) channels and adaptive strategies. This paper addresses a critical challenge of countering intelligent UAV jamming in the context of energy-constrained ambient backscatter communication systems. Traditional anti-jamming techniques often fall short against such dynamic threats or are unsuitable for low-power backscatter devices. Hence, we propose a novel anti-jamming framework based on Deep Reinforcement Learning (DRL) that empowers the transmitter to not only defend against but also strategically exploit the UAV's jamming signals. In particular, our approach allows the transmitter to learn an optimal policy for switching between active transmission, energy harvesting from the jamming signal, and backscattering information using the jammer's own emissions. We then formulate the problem as a Markov Decision Process (MDP) and employ a Deep Q-Network (DQN) to derive the optimal operational strategy. Simulation results demonstrate that our DQN-based method significantly outperforms conventional Q-learning in convergence speed and surpasses a greedy anti-jamming strategy in terms of average throughput, packet loss rate, and packet delivery ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08351v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ngoc-Tan Nguyen, Thi-Thu Hoang, Trung-Dung Hoang, Thai-Duong Nguyen</dc:creator>
    </item>
    <item>
      <title>Improvement and Stabilization of Output Voltages in a Vertical Tidal Turbine Using Intelligent Control Strategies</title>
      <link>https://arxiv.org/abs/2512.08416</link>
      <description>arXiv:2512.08416v1 Announce Type: new 
Abstract: This article investigates on the improvement and stabilization of alternating current (AC) and direct current (DC) output voltages in a Permanent Magnet Synchronous Generator (PMSG) driven by a vertical-axis tidal turbine using advanced control strategies. The research integrates artificial intelligence (AI)-based techniques to enhance voltage stability and efficiency. Initially, the Maximum Power Point Tracking (MPPT) approach based on Tip Speed Ratio (TSR) and Artificial Neural Network (ANN) Fuzzy logic controllers is explored. To further optimize the performance, Particle Swarm Optimization (PSO) and a hybrid ANN-PSO methodology are implemented. These strategies aim to refine the reference rotational speed of the turbine while minimizing deviations from optimal power extraction conditions. The simulation results of a tidal turbine operating at a water flow velocity of 1.5 m/s demonstrate that the PSO-based control approach significantly enhances the voltage stability compared to conventional MPPT-TSR and ANN-Fuzzy controllers. The hybrid ANN-PSO technique improves the voltage regulation by dynamically adapting to system variations and providing real-time reference speed adjustments. This research highlights the AI-based hybrid optimization benefit to stabilize the output voltage of tidal energy systems, thereby increasing reliability and efficiency in renewable energy applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08416v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>International Conference on Electrical and Computer Engineering Researches (ICECER 2025), Dec 2025, Antananarivo, Madagascar</arxiv:journal_reference>
      <dc:creator>Fanambinantsoa Philibert Andriniriniaimalaza (PIMENT), Nour Murad (PIMENT), Randriamaitso Telesphore (SPE), Bilal Habachi (SPE), Randriatefison Nirilalaina (NUIST), Manasina Ruffin (NUIST), Andrianirina Charles Bernard (NUIST), Ravelo Blaise (NUIST)</dc:creator>
    </item>
    <item>
      <title>Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR</title>
      <link>https://arxiv.org/abs/2512.08626</link>
      <description>arXiv:2512.08626v1 Announce Type: new 
Abstract: Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.
  In this paper, we introduce the \textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.
  We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08626v1</guid>
      <category>cs.NI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agrim Bari, Gustavo de Veciana, Yuqi Zhou</dc:creator>
    </item>
    <item>
      <title>ITU-T Y.2325: NGN Evolution Towards Future</title>
      <link>https://arxiv.org/abs/2512.08695</link>
      <description>arXiv:2512.08695v1 Announce Type: new 
Abstract: International Telecommunications Union (ITU) defined Next Generation Network (NGN) underlies most wireline and wireless packet-based telecommunications networks. A key design principle of NGN is decoupling of service-related functions from the underlying transport stratum, making user services independent of transport technologies. Interestingly, the NGN architecture, as defined in ITU standards, did not follow this design principle for internal network services, e.g., mobility, or authentication though adhering for external user services like IPTV or Multimedia services. These internal services are handled by the NGN transport control plane, making them an intrinsic part of the transport stratum, resulting in a tightly coupled service and transport functionality as opposed to the proclaimed design goal. This design choice may force each transport technology to support internal services individually, e.g., separate authentication service for each transport, leading to duplication. Since the NGN architecture is the base underlying architecture for most packet-based telecommunications network including advanced cellular networks like 4th/5th Generation cellular networks, the limitation persists in these cellular networks as well. To remedy the situation, the decoupling of service and transport can be generalized to include internal services like mobility and authentication also. In this context, the recently published ITU Y.2325 recommendation, defines an evolved NGN architecture, wherein all services, including internal network services, are decoupled from the transport stratum. The proposal results in a more scalable and modular evolved NGN architecture that can be used as a template for all future telecom networks including IMT-2030 (6th generation mobile networks). In this article, we review the evolved NGN architecture, as proposed in ITU-T Y.2325.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08695v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rashmi Kamran, Shwetha Kiran, Pranav Jha, Rashmi Yadav, Abhay Karandikar, Prasanna Chaporkar</dc:creator>
    </item>
    <item>
      <title>WikIPedia: Unearthing a 20-Year History of IPv6 Client Addressing</title>
      <link>https://arxiv.org/abs/2512.08808</link>
      <description>arXiv:2512.08808v1 Announce Type: new 
Abstract: Due to their article editing policies, Wikimedia sites like Wikipedia have become inadvertent time capsules for IPv6 addresses. When Wikimedia users make edits without signing into an account, their IP addresses are used in lieu of a username. Wikimedia site dumps therefore provide researchers with over two decades worth of timestamped client IPv6 addresses to understand address assignments and how they have changed over time and space.
  In this work, we extract 19M unique IPv6 addresses from Wikimedia sites like Wikipedia that were used by editors from 2003 to 2024. We use these addresses to understand the prevalence of IPv6 in countries corresponding to Wikimedia site languages, how IPv6 adoption has grown over time, and the prevalence of EUI-64 addressing on client devices like desktops, laptops, and mobile phones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08808v1</guid>
      <category>cs.NI</category>
      <category>cs.SI</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erik Rye, Dave Levin</dc:creator>
    </item>
    <item>
      <title>Evaluating Vulnerabilities of Connected Vehicles Under Cyber Attacks by Attack-Defense Tree</title>
      <link>https://arxiv.org/abs/2512.08204</link>
      <description>arXiv:2512.08204v1 Announce Type: cross 
Abstract: Connected vehicles represent a key enabler of intelligent transportation systems, where vehicles are equipped with advanced communication, sensing, and computing technologies to interact not only with one another but also with surrounding infrastructures and the environment. Through continuous data exchange, such vehicles are capable of enhancing road safety, improving traffic efficiency, and ensuring more reliable mobility services. Further, when these capabilities are integrated with advanced automation technologies, the concept essentially evolves into connected and autonomous vehicles (CAVs). While connected vehicles primarily focus on seamless information sharing, autonomous vehicles are mainly dependent on advanced perception, decision-making, and control mechanisms to operate with minimal or without human intervention. However, as a result of connectivity, an adversary with malicious intentions might be able to compromise successfully by breaching the system components of CAVs. In this paper, we present an attack-tree based methodology for evaluating cyber security vulnerabilities in CAVs. In particular, we utilize the attack-defense tree formulation to systematically assess attack-leaf vulnerabilities, and before analyzing the vulnerability indices, we also define a measure of vulnerabilities, which is based on existing cyber security threats and corresponding defensive countermeasures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08204v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Baqer Mollah, Honggang Wang, Hua Fang</dc:creator>
    </item>
    <item>
      <title>Delay-Oriented Distributed Scheduling with TransGNN</title>
      <link>https://arxiv.org/abs/2512.08799</link>
      <description>arXiv:2512.08799v1 Announce Type: cross 
Abstract: Minimizing transmission delay in wireless multi-hop networks is a fundamental yet challenging task due to the complex coupling among interference, queue dynamics, and distributed control. Traditional scheduling algorithms, such as max-weight or queue-length-based policies, primarily aim to optimize throughput but often suffer from high latency, especially in heterogeneous or dynamically changing topologies. Recent learning-based approaches, particularly those employing Graph Neural Networks (GNNs), have shown promise in capturing spatial interference structures. However, conventional Graph Convolutional Networks (GCNs) remain limited by their local aggregation mechanism and their inability to model long-range dependencies within the conflict graph. To address these challenges, this paper proposes a delay-oriented distributed scheduling framework based on Transformer GNN. The proposed model employs an attention-based graph encoder to generate adaptive per-link utility scores that reflect both queue backlog and interference intensity. A Local Greedy Solver (LGS) then utilizes these utilities to construct a feasible independent set of links for transmission, ensuring distributed and conflict-free scheduling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08799v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boxuan Wen, Junyu Luo</dc:creator>
    </item>
    <item>
      <title>Architecture Proposal for 6G Systems Integrating Sensing and Communication</title>
      <link>https://arxiv.org/abs/2411.10138</link>
      <description>arXiv:2411.10138v2 Announce Type: replace 
Abstract: Integrating sensing functionality into 6G communication networks requires some changes to existing components as well as new entities processing the radar sensing signals received by the communication antennas. This whitepaper provides a comprehensive overview of the 6G design proposal for ISaC (Integrated Sensing and Communication). The whitepaper has been created by the architecture group of the KOMSENS-6G project with the intend to serve as a basis for further discussions and alignment across innovative 6G projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10138v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Gersing, Mark Doll, Joerg Huschke, Jan Plachy, Stefan K\"opsell</dc:creator>
    </item>
    <item>
      <title>Cross-Problem Solving for Network Optimization: Is Problem-Aware Learning the Key?</title>
      <link>https://arxiv.org/abs/2505.05067</link>
      <description>arXiv:2505.05067v2 Announce Type: replace 
Abstract: As intelligent network services continue to diversify, ensuring efficient and adaptive resource allocation in edge networks has become increasingly critical. Yet the wide functional variations across services often give rise to new and unforeseen optimization problems, rendering traditional manual modeling and solver design both time-consuming and inflexible. This limitation reveals a key gap between current methods and human solving - the inability to recognize and understand problem characteristics. It raises the question of whether problem-aware learning can bridge this gap and support effective cross-problem generalization. To answer this question, we propose a problem-aware diffusion (PAD) model, which leverages a problem-aware learning framework to enable cross-problem generalization. By explicitly encoding the mathematical formulations of optimization problems into token-level embeddings, PAD empowers the model to understand and adapt to problem structures. Extensive experiments across ten representative network optimization problems show that PAD generalizes well to unseen problems while avoiding the inefficiency of building new solvers from scratch, yet still delivering competitive solution quality. Meanwhile, an auxiliary constraint-aware module is designed to enforce solution validity further. The experiments indicate that problem-aware learning opens a promising direction toward general-purpose solvers for intelligent network operation and resource management. Our code is open source at https://github.com/qiyu3816/PAD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05067v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruihuai Liang, Bo Yang, Pengyu Chen, Xuelin Cao, Zhiwen Yu, H. Vincent Poor, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>FedJam: Multimodal Federated Learning Framework for Jamming Detection</title>
      <link>https://arxiv.org/abs/2508.09369</link>
      <description>arXiv:2508.09369v2 Announce Type: replace 
Abstract: Jamming attacks pose a critical threat to wireless networks, yet existing detection methods remain largely unimodal, centralized and resource-intensive, limiting their performance, scalability, and deployment feasibility, respectively. To address these limitations, we present FedJam, a multimodal Federated Learning (FL) framework for on-device jamming detection and classification. FedJam locally fuses spectrograms and cross-layer network Key Performance Indicators (KPIs) using a lightweight dual-encoder architecture with an integrated fusion module and multimodal projection head, that enables privacy-preserving training and inference without transmitting raw data. We prototype and deploy FedJam on a wireless experimental testbed and evaluate it using the first, over-the-air multimodal dataset comprising synchronized samples across benign and three distinct jamming attack types. FedJam outperforms state-of-the-art unimodal baselines by up to 15% in accuracy, while requiring 60% fewer communication rounds to converge, and maintains low resource utilization. Its advantage is especially pronounced in realistic scenarios, where it remains extremely robust under heterogeneous data distributions across devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09369v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Panitsas, Iason Ofeidis, Leandros Tassiulas</dc:creator>
    </item>
    <item>
      <title>AoI-based Scheduling of Correlated Sources for Timely Inference</title>
      <link>https://arxiv.org/abs/2509.01926</link>
      <description>arXiv:2509.01926v2 Announce Type: replace 
Abstract: We investigate a real-time remote inference system where multiple correlated sources transmit observations over a communication channel to a receiver. The receiver utilizes these observations to infer multiple time-varying targets. Due to limited communication resources, the delivered observations may not be fresh. To quantify data freshness, we employ the Age of Information (AoI) metric. To minimize the inference error, we aim to design a signal-agnostic scheduling policy that leverages AoI without requiring knowledge of the actual target values or the source observations. This scheduling problem is a restless multi-armed bandit (RMAB) problem with a non-separable penalty function. Unlike traditional RMABs, the correlation among sources introduces a unique challenge: the penalty function of each source depends on the AoI of other correlated sources, preventing the problem from decomposing into multiple independent Markov Decision Processes (MDPs), a key step in applying traditional RMAB solutions. To address this, we propose a novel approach that approximates the penalty function for each source and establishes an analytical bound on the approximation error. We then develop scheduling policies for two scenarios: (i) full knowledge of the penalty functions and (ii) no knowledge of the penalty functions. For the case of known penalty functions, we present an upper bound on the optimality gap that highlights the impact of the correlation parameter and the system size. For the case of unknown penalty functions and signal distributions, we develop an online learning approach that utilizes bandit feedback to learn an online Maximum Gain First policy. Simulation results demonstrate the effectiveness of our proposed policies in minimizing inference error and achieving scalability in the number of sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01926v2</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Kamran Chowdhury Shisher, Vishrant Tripathi, Mung Chiang, Christopher G. Brinton</dc:creator>
    </item>
    <item>
      <title>SlicePilot: Demystifying Network Slice Placement in Heterogeneous Cloud Infrastructures</title>
      <link>https://arxiv.org/abs/2509.18545</link>
      <description>arXiv:2509.18545v2 Announce Type: replace 
Abstract: Cellular networks are comprised of software-based entities, with main functions encapsulated as Virtual Network Functions (VNFs) deployed on Commercial-off-the-Shelf (COTS) hardware. As a key enabler of 5G, network slicing offers logically isolated Quality of Service (QoS) for diverse use cases. With the transition to cloud-native infrastructures, optimizing network slice placement across multi-cloud environments remains challenging due to heterogeneous resource capabilities and varying slice-specific demands. This paper presents SlicePilot, a modular framework that enables autonomous and near-optimal VNF placement using a disaggregated Multi-Agent Reinforcement Learning (MARL) approach. SlicePilot collects real-world traffic profiles to estimate resource needs for each slice type. These estimates guide a MARL-based scheduler that minimizes deployment costs while satisfying QoS constraints. We evaluate SlicePilot on a multi-cloud testbed and demonstrate a 19x speed-up over combinatorial optimization methods, while keeping deployment costs within 7.8% of the optimal. Although SlicePilot results in 2.42x more QoS violations under high-load conditions, this trade-off is offset by faster decision-making and reduced computational overhead. Overall, SlicePilot delivers a scalable, cost-efficient solution for network slice placement, making it suitable for real-time deployments where responsiveness and efficiency are critical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18545v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Panitsas, Tolga O. Atalay, Dragoslav Stojadinovic, Angelos Stavrou, Leandros Tassiulas</dc:creator>
    </item>
    <item>
      <title>HERMES: Heterogeneous Application-Enabled Routing Middleware for Edge-IoT Systems</title>
      <link>https://arxiv.org/abs/2512.01824</link>
      <description>arXiv:2512.01824v2 Announce Type: replace 
Abstract: The growth of the Internet of Things has enabled a new generation of applications, pushing computation and intelligence toward the network edge. This trend, however, exposes challenges, as the heterogeneity of devices and the complex requirements of applications are often misaligned with the assumptions of traditional routing protocols, which lack the flexibility to accommodate application-layer metrics and policies. This work addresses this gap by proposing a software framework that enhances routing flexibility by dynamically incorporating application-aware decisions. The core of the work establishes a multi-hop Wi-Fi network of heterogeneous devices, specifically ESP8266, ESP32, and Raspberry Pi 3B. The routing layer follows a proactive approach, while the network is fault-tolerant, maintaining operation despite both node loss and message loss. On top of this, a middleware layer introduces three strategies for influencing routing behavior: two adapt the path a message traverses until arriving at the destination, while the third allows applications to shape the network topology. This layer offers a flexible interface for diverse applications. The framework was validated on a physical testbed through edge intelligence use cases, including distributing neural network inference computations across multiple devices and offloading the entire workload to the most capable node. Distributed inference is useful in scenarios requiring low latency, energy efficiency, privacy, and autonomy. Experimental results indicated that device heterogeneity significantly impacts network performance. Throughput and inference duration analysis showed the influence of the strategies on application behaviour, revealed that topology critically affects decentralized performance, and demonstrated the suitability of the framework for complex tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01824v2</guid>
      <category>cs.NI</category>
      <category>cs.NE</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\'essica Consci\^encia, Ant\'onio Grilo</dc:creator>
    </item>
  </channel>
</rss>
