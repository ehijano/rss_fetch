<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Aug 2025 01:30:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Polynomial complexity sampling from multimodal distributions using Sequential Monte Carlo</title>
      <link>https://arxiv.org/abs/2508.02763</link>
      <description>arXiv:2508.02763v1 Announce Type: new 
Abstract: We study a sequential Monte Carlo algorithm to sample from the Gibbs measure with a non-convex energy function at a low temperature. We use the practical and popular geometric annealing schedule, and use a Langevin diffusion at each temperature level. The Langevin diffusion only needs to run for a time that is long enough to ensure local mixing within energy valleys, which is much shorter than the time required for global mixing. Our main result shows convergence of Monte Carlo estimators with time complexity that, approximately, scales like the forth power of the inverse temperature, and the square of the inverse allowed error. We also study this algorithm in an illustrative model scenario where more explicit estimates can be given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02763v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiyu Han, Gautam Iyer, Dejan Slep\v{c}ev</dc:creator>
    </item>
    <item>
      <title>Expanding the Standard Diffusion Process to Specified Non-Gaussian Marginal Distributions</title>
      <link>https://arxiv.org/abs/2508.03617</link>
      <description>arXiv:2508.03617v1 Announce Type: new 
Abstract: We develop a class of non-Gaussian translation processes that extend classical stochastic differential equations (SDEs) by prescribing arbitrary absolutely continuous marginal distributions. Our approach uses a copula-based transformation to flexibly model skewness, heavy tails, and other non-Gaussian features often observed in real data. We rigorously define the process, establish key probabilistic properties, and construct a corresponding diffusion model via stochastic calculus, including proofs of existence and uniqueness. A simplified approximation is introduced and analyzed, with error bounds derived from asymptotic expansions. Simulations demonstrate that both the full and simplified models recover target marginals with high accuracy. Examples using the Student's t, asymmetric Laplace, and Exponentialized Generalized Beta of the Second Kind (EGB2) distributions illustrate the flexibility and tractability of the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03617v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Robert Richardson, H. Dennis Tolley, Kenneth Kuttler</dc:creator>
    </item>
    <item>
      <title>Advancing Computational Tools for Analyzing Commutative Hypercomplex Algebras</title>
      <link>https://arxiv.org/abs/2508.02709</link>
      <description>arXiv:2508.02709v1 Announce Type: cross 
Abstract: Commutative hypercomplex algebras offer significant advantages over traditional quaternions due to their compatibility with linear algebra techniques and efficient computational implementation, which is crucial for broad applicability. This paper explores a novel family of commutative hypercomplex algebras, referred to as (alpha,beta)-tessarines, which extend the system of generalized Segre's quaternions and, consequently, elliptic quaternions. The main contribution of this work is the development of theoretical and computational tools for matrices within this algebraic system, including inversion, square root computation, LU factorization with partial pivoting, and determinant calculation. Additionally, a spectral theory for (alpha,beta)-tessarines is established, covering eigenvalue and eigenvector analysis, the power method, singular value decomposition, rank-k approximation, and the pseudoinverse. Solutions to the classical least squares problem are also presented. These results not only enhance the fundamental understanding of hypercomplex algebras but also provide researchers with novel matrix operations that have not been extensively explored in previous studies. The theoretical findings are supported by real-world examples, including image reconstruction and color face recognition, which demonstrate the potential of the proposed techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02709v1</guid>
      <category>math.RA</category>
      <category>math.AC</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jos\'e Domingo Jim\'enez-L\'opez, Jes\'us Navarro-Moreno, Rosa Mar\'ia Fern\'andez-Alcal\'a, Juan Carlos Ruiz Molina</dc:creator>
    </item>
    <item>
      <title>Cholesky decomposition for symmetric matrices, Riemannian geometry, and random matrices</title>
      <link>https://arxiv.org/abs/2508.02715</link>
      <description>arXiv:2508.02715v1 Announce Type: cross 
Abstract: For each $n \geq 1$ and sign pattern $\epsilon \in \{ \pm 1 \}^n$, we introduce a cone of real symmetric matrices $LPM_n(\epsilon)$: those with leading principal $k \times k$ minors of signs $\epsilon_k$. These cones are pairwise disjoint and their union $LPM_n$ is a dense cone in all symmetric matrices; they subsume positive and negative definite matrices, and symmetric (P-,) N-, PN-, almost P-, and almost N- matrices. We show that each $LPM_n$ matrix $A$ admits an uncountable family of Cholesky-type factorizations - yielding a unique lower triangular matrix $L$ with positive diagonals - with additional attractive properties: (i) each such factorization is algorithmic; and (ii) each such Cholesky map $A \mapsto L$ is a smooth diffeomorphism from $LPM_n(\epsilon)$ onto an open Euclidean ball.
  We then show that (iii) the (diffeomorphic) balls $LPM_n(\epsilon)$ are isometric Riemannian manifolds as well as isomorphic abelian Lie groups, each equipped with a translation-invariant Riemannian metric (and hence Riemannian means/barycentres). Moreover, (iv) this abelian metric group structure on each $LPM_n(\epsilon)$ - and hence the log-Cholesky metric on Cholesky space - yields an isometric isomorphism onto a finite-dimensional Euclidean space. The complex version of this also holds.
  In the latter part, we show that the abelian group $PD_n$ of positive definite matrices, with its bi-invariant log-Cholesky metric, is precisely the identity-component of a larger group with an alternate metric: the dense cone $LPM_n$. This also holds for Hermitian matrices over several subfields $\mathbb{F} \subseteq \mathbb{C}$. As a result, (v) the groups $LPM_n^{\mathbb{F}}$ and $LPM_\infty^{\mathbb{F}}$ admit a rich probability theory, and the cones $LPM_n(\epsilon), TPM_n(\epsilon)$ admit Wishart densities with signed Bartlett decompositions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02715v1</guid>
      <category>math.RA</category>
      <category>math.DG</category>
      <category>math.PR</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Apoorva Khare, Prateek Kumar Vishwakarma</dc:creator>
    </item>
    <item>
      <title>Reconstructing the Probability Measure of a Curie-Weiss Model Observing the Realisations of a Subset of Spins</title>
      <link>https://arxiv.org/abs/2508.03452</link>
      <description>arXiv:2508.03452v1 Announce Type: cross 
Abstract: We study the problem of reconstructing the probability measure of the Curie-Weiss model from a sample of the voting behaviour of a subset of the population. While originally used to study phase transitions in statistical mechanics, the Curie-Weiss or mean-field model has been applied to study phenomena, where many agents interact with each other. It is useful to measure the degree of social cohesion in social groups, which manifests in the way the members of the group influence each others' decisions. In practice, statisticians often only have access to survey data from a representative subset of a population. As such, it is useful to provide methods to estimate social cohesion from such data. The estimators we study have some positive properties, such as consistency, asymptotic normality, and large deviation principles. The main advantages are that they require only a sample of votes belonging to a (possibly very small) subset of the population and have a low computational cost. Due to the wide application of models such as Curie-Weiss, these estimators are potentially useful in disciplines such as political science, sociology, automated voting, and preference aggregation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03452v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miguel Ballesteros, Ivan Naumkin, Gabor Toth</dc:creator>
    </item>
    <item>
      <title>Likelihood Matching for Diffusion Models</title>
      <link>https://arxiv.org/abs/2508.03636</link>
      <description>arXiv:2508.03636v1 Announce Type: cross 
Abstract: We propose a Likelihood Matching approach for training diffusion models by first establishing an equivalence between the likelihood of the target data distribution and a likelihood along the sample path of the reverse diffusion. To efficiently compute the reverse sample likelihood, a quasi-likelihood is considered to approximate each reverse transition density by a Gaussian distribution with matched conditional mean and covariance, respectively. The score and Hessian functions for the diffusion generation are estimated by maximizing the quasi-likelihood, ensuring a consistent matching of both the first two transitional moments between every two time points. A stochastic sampler is introduced to facilitate computation that leverages on both the estimated score and Hessian information. We establish consistency of the quasi-maximum likelihood estimation, and provide non-asymptotic convergence guarantees for the proposed sampler, quantifying the rates of the approximation errors due to the score and Hessian estimation, dimensionality, and the number of diffusion steps. Empirical and simulation evaluations demonstrate the effectiveness of the proposed Likelihood Matching and validate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03636v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lei Qian, Wu Su, Yanqi Huang, Song Xi Chen</dc:creator>
    </item>
    <item>
      <title>Estimation and variable selection in high dimension in a causal joint model of survival times and longitudinal outcomes with random effects</title>
      <link>https://arxiv.org/abs/2306.16765</link>
      <description>arXiv:2306.16765v3 Announce Type: replace 
Abstract: We consider a joint survival and mixed-effects model to explain the survival time from longitudinal data and high-dimensional covariates in a population. The longitudinal data is modeled using a non linear mixed-effects model to account for the inter-individual variability in the population. The corresponding regression function serves as a link function incorporated into the survival model. In that way, the longitudinal data is related to the survival time. We consider a Cox model that takes into account both high-dimensional covariates and the link function. There are two main objectives: first, identify the relevant covariates that contribute to explaining survival time, and second, estimate all unknown parameters of the joint model. For the first objective, we consider the estimate defined by maximizing the marginal log-likelihood regularized with a l1-penalty term. To tackle the optimization problem, we implement an adaptive stochastic gradient to handle the latent variables of the non linear mixed-effects model associated with a proximal operator to manage the non-differentiability of the penalty. We rely on an eBIC model choice criterion to select an optimal value for the regularization parameter. Once the relevant covariates are selected, we re-estimate the parameters in the reduced model by maximizing the likelihood using an adaptive stochastic gradient descent. We provide relevant simulations that showcase the performance of the proposed variable selection and parameter estimation method in the joint model. We investigate the effect of censoring and of the presence of correlation between the individual parameters in the mixed model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16765v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Caillebotte (MaIAGE), Estelle Kuhn (MaIAGE), Sarah Lemler (MICS)</dc:creator>
    </item>
    <item>
      <title>Estimation and variable selection in high dimension in nonlinear mixed-effects models</title>
      <link>https://arxiv.org/abs/2503.20401</link>
      <description>arXiv:2503.20401v2 Announce Type: replace 
Abstract: We consider nonlinear mixed effects models including high-dimensional covariates to model individual parameters variability. The objective is to identify relevant covariates among a large set under sparsity assumption and to estimate model parameters. To face the high dimensional setting we consider a regularized estimator namely the maximum likelihood estimator penalized with the l1-penalty. We rely on the use of the eBIC model choice criteria to select an optimal reduced model. Then we estimate the parameters by maximizing the likelihood of the reduced model. We calculate in practice the maximum likelihood estimator penalized with the l1-penalty though a weighted proximal stochastic gradient descent algorithm with an adaptive learning rate. This choice allows us to consider very general models, in particular models that do not belong to the curved exponential family. We demonstrate first in a simple linear toy model through a simulation study the good convergence properties of this optimization algorithm. We compare then the performance of the proposed methodology with those of the \glmmLasso procedure in a linear mixed effects model in a simulation study. We illustrate also its performance in a nonlinear mixed-effects logistic growth model through simulation. We finally highlight the beneficit of the proposed procedure relying on an integrated single step approach regarding two others two steps approaches for variable selection objective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20401v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Caillebotte (MaIAGE, GQE-Le Moulon), Estelle Kuhn (MaIAGE), Sarah Lemler (MICS)</dc:creator>
    </item>
    <item>
      <title>Identifiability and Estimation in High-Dimensional Nonparametric Latent Structure Models</title>
      <link>https://arxiv.org/abs/2506.09165</link>
      <description>arXiv:2506.09165v2 Announce Type: replace 
Abstract: This paper studies the problems of identifiability and estimation in high-dimensional nonparametric latent structure models. We introduce an identifiability theorem that generalizes existing conditions, establishing a unified framework applicable to diverse statistical settings. Our results rigorously demonstrate how increased dimensionality, coupled with diversity in variables, inherently facilitates identifiability. For the estimation problem, we establish near-optimal minimax rate bounds for the high-dimensional nonparametric density estimation under latent structures with smooth marginals. Contrary to the conventional curse of dimensionality, our sample complexity scales only polynomially with the dimension. Additionally, we develop a perturbation theory for component recovery and propose a recovery procedure based on simultaneous diagonalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09165v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yichen Lyu, Pengkun Yang</dc:creator>
    </item>
    <item>
      <title>Predictive information criterion for jump diffusion processes</title>
      <link>https://arxiv.org/abs/2508.00411</link>
      <description>arXiv:2508.00411v2 Announce Type: replace 
Abstract: In this paper, we address a model selection problem for ergodic jump diffusion processes based on high-frequency samples. We evaluate the expected genuine log-likelihood function and derive an Akaike-type information criterion based on the threshold-based quasi-likelihood function. In the derivation process, we also give new estimates of the transition density of jump diffusion processes. We also provide the relative selection probability of the proposed information criterion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00411v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuma Uehara</dc:creator>
    </item>
    <item>
      <title>Anytime-valid FDR control with the stopped e-BH procedure</title>
      <link>https://arxiv.org/abs/2502.08539</link>
      <description>arXiv:2502.08539v3 Announce Type: replace-cross 
Abstract: The recent e-Benjamini-Hochberg (e-BH) procedure for multiple hypothesis testing is known to control the false discovery rate (FDR) under arbitrary dependence between the input e-values. This paper points out an important subtlety when applying the e-BH procedure with e-processes, which are sequential generalizations of e-values (where the data are observed sequentially). Since adaptively stopped e-processes are e-values, the e-BH procedure can be repeatedly applied at every time step, and one can continuously monitor the e-processes and the rejection sets obtained. One would hope that the "stopped e-BH procedure" (se-BH) has an FDR guarantee for the rejection set obtained at any stopping time. However, while this is true if the data in different streams are independent, it is not true in full generality, because each stopped e-process is an e-value only for stopping times in its own local filtration, but the se-BH procedure employs a stopping time with respect to a global filtration. This can cause information to leak across time, allowing one stream to know its future by knowing past data of another stream. This paper formulates a simple causal condition under which local e-processes are also global e-processes and thus the se-BH procedure does indeed control the FDR. The condition excludes unobserved confounding from the past and is met under most reasonable scenarios including genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08539v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
  </channel>
</rss>
