<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2024 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Combining Incomplete Observational and Randomized Data for Heterogeneous Treatment Effects</title>
      <link>https://arxiv.org/abs/2410.21343</link>
      <description>arXiv:2410.21343v1 Announce Type: new 
Abstract: Data from observational studies (OSs) is widely available and readily obtainable yet frequently contains confounding biases. On the other hand, data derived from randomized controlled trials (RCTs) helps to reduce these biases; however, it is expensive to gather, resulting in a tiny size of randomized data. For this reason, effectively fusing observational data and randomized data to better estimate heterogeneous treatment effects (HTEs) has gained increasing attention. However, existing methods for integrating observational data with randomized data must require \textit{complete} observational data, meaning that both treated subjects and untreated subjects must be included in OSs. This prerequisite confines the applicability of such methods to very specific situations, given that including all subjects, whether treated or untreated, in observational studies is not consistently achievable. In our paper, we propose a resilient approach to \textbf{C}ombine \textbf{I}ncomplete \textbf{O}bservational data and randomized data for HTE estimation, which we abbreviate as \textbf{CIO}. The CIO is capable of estimating HTEs efficiently regardless of the completeness of the observational data, be it full or partial. Concretely, a confounding bias function is first derived using the pseudo-experimental group from OSs, in conjunction with the pseudo-control group from RCTs, via an effect estimation procedure. This function is subsequently utilized as a corrective residual to rectify the observed outcomes of observational data during the HTE estimation by combining the available observational data and the all randomized data. To validate our approach, we have conducted experiments on a synthetic dataset and two semi-synthetic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21343v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3627673.3679593</arxiv:DOI>
      <dc:creator>Dong Yao, Caizhi Tang, Qing Cui, Longfei Li</dc:creator>
    </item>
    <item>
      <title>Enhanced sequential directional importance sampling for structural reliability analysis</title>
      <link>https://arxiv.org/abs/2410.21350</link>
      <description>arXiv:2410.21350v1 Announce Type: new 
Abstract: Sequential directional importance sampling (SDIS) is an efficient adaptive simulation method for estimating failure probabilities. It expresses the failure probability as the product of a group of integrals that are easy to estimate, wherein the first one is estimated with Monte Carlo simulation (MCS), and all the subsequent ones are estimated with directional importance sampling. In this work, we propose an enhanced SDIS method for structural reliability analysis. We discuss the efficiency of MCS for estimat?ing the first integral in standard SDIS and propose using Subset Simulation as an alternative method. Additionally, we propose a Kriging-based active learning algorithm tailored to identify multiple roots in certain important di?rections within a specificed search interval. The performance of the enhanced SDIS is demonstrated through various complex benchmark problems. The results show that the enhanced SDIS is a versatile reliability analysis method that can efficiently and robustly solve challenging reliability problems</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21350v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kai Chenga, Iason Papaioannou, Daniel Straub</dc:creator>
    </item>
    <item>
      <title>Generalized Method of Moments and Percentile Method: Estimating parameters of the Novel Median Based Unit Weibull Distribution</title>
      <link>https://arxiv.org/abs/2410.21355</link>
      <description>arXiv:2410.21355v1 Announce Type: new 
Abstract: The Median Based Unit Weibull is a new 2 parameter unit Weibull distribution defined on the unit interval (0,1). Estimation of the parameters using MLE encountered some problems like large variance. Using generalized method of moments (GMMs) and percentile method may ameliorate this condition. This paper introduces GMMs and the percentile methods for estimating the parameters of the new distribution with illustrative real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21355v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohamed Attia</dc:creator>
    </item>
    <item>
      <title>Causal Bootstrap for General Randomized Designs</title>
      <link>https://arxiv.org/abs/2410.21464</link>
      <description>arXiv:2410.21464v1 Announce Type: new 
Abstract: We distinguish between two sources of uncertainty in experimental causal inference: design uncertainty, due to the treatment assignment mechanism, and sampling uncertainty, when the sample is drawn from a super-population. This distinction matters in settings with small fixed samples and heterogeneous treatment effects, as in geographical experiments. Most bootstrap procedures used by practitioners primarily estimate sampling uncertainty. Other methods for quantifying design uncertainty also fall short, because they are restricted to common designs and estimators, whereas non-standard designs and estimators are often used in these low-power regimes. We address this gap by proposing an integer programming approach, which allows us to estimate design uncertainty for any known and probabilistic assignment mechanisms, and linear-in-treatment and quadratic-in-treatment estimators. We include asymptotic validity results and demonstrate the refined confidence intervals achieved by accurately accounting for non-standard design uncertainty through simulations of geographical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21464v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jennifer Brennan, S\'ebastien Lahaie, Adel Javanmard, Nick Doudchenko, Jean Pouget-Abadie</dc:creator>
    </item>
    <item>
      <title>Hybrid Bayesian Smoothing on Surfaces</title>
      <link>https://arxiv.org/abs/2410.21469</link>
      <description>arXiv:2410.21469v1 Announce Type: new 
Abstract: Modeling spatial processes that exhibit both smooth and rough features poses a significant challenge. This is especially true in fields where complex physical variables are observed across spatial domains. Traditional spatial techniques, such as Gaussian processes (GPs), are ill-suited to capture sharp transitions and discontinuities in spatial fields. In this paper, we propose a new approach incorporating non-Gaussian processes (NGPs) into a hybrid model which identifies both smooth and rough components. Specifically, we model the rough process using scaled mixtures of Gaussian distributions in a Bayesian hierarchical model (BHM).
  Our motivation comes from the Community Earth System Model Large Ensemble (CESM-LE), where we seek to emulate climate sensitivity fields that exhibit complex spatial patterns, including abrupt transitions at ocean-land boundaries. We demonstrate that traditional GP models fail to capture such abrupt changes and that our proposed hybrid model, implemented through a full Gibbs sampler. This significantly improves model interpretability and accurate recovery of process parameters.
  Through a multi-factor simulation study, we evaluate the performance of several scaled mixtures designed to model the rough process. The results highlight the advantages of using these heavier tailed priors as a replacement to the Bayesian fused LASSO. One prior in particular, the normal Jeffrey's prior stands above the rest. We apply our model to the CESM-LE dataset, demonstrating its ability to better represent the mean function and its uncertainty in climate sensitivity fields.
  This work combines the strengths of GPs for smooth processes with the flexibility of NGPs for abrupt changes. We provide a computationally efficient Gibbs sampler and include additional strategies for accelerating Monte Carlo Markov Chain (MCMC) sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21469v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matthew Hofkes, Douglas Nychka</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Models for Multiple Raters: a General Statistical Framework</title>
      <link>https://arxiv.org/abs/2410.21498</link>
      <description>arXiv:2410.21498v1 Announce Type: new 
Abstract: Rating procedure is crucial in many applied fields (e.g., educational, clinical, emergency). It implies that a rater (e.g., teacher, doctor) rates a subject (e.g., student, doctor) on a rating scale. Given raters variability, several statistical methods have been proposed for assessing and improving the quality of ratings. The analysis and the estimate of the Intraclass Correlation Coefficient (ICC) are major concerns in such cases. As evidenced by the literature, ICC might differ across different subgroups of raters and might be affected by contextual factors and subject heterogeneity. Model estimation in the presence of heterogeneity has been one of the recent challenges in this research line. Consequently, several methods have been proposed to address this issue under a parametric multilevel modelling framework, in which strong distributional assumptions are made. We propose a more flexible model under the Bayesian nonparametric (BNP) framework, in which most of those assumptions are relaxed. By eliciting hierarchical discrete nonparametric priors, the model accommodates clusters among raters and subjects, naturally accounts for heterogeneity, and improves estimate accuracy. We propose a general BNP heteroscedastic framework to analyze rating data and possible latent differences among subjects and raters. The estimated densities are used to make inferences about the rating process and the quality of the ratings. By exploiting a stick-breaking representation of the Dirichlet Process a general class of ICC indices might be derived for these models. Theoretical results about the ICC are provided together with computational strategies. Simulations and a real-world application are presented and possible future directions are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21498v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe Mignemi, Ioanna Manolopoulou</dc:creator>
    </item>
    <item>
      <title>Enhancing parameter estimation in finite mixture of generalized normal distributions</title>
      <link>https://arxiv.org/abs/2410.21559</link>
      <description>arXiv:2410.21559v1 Announce Type: new 
Abstract: Mixtures of generalized normal distributions (MGND) have gained popularity for modelling datasets with complex statistical behaviours. However, the estimation of the shape parameter within the maximum likelihood framework is quite complex, presenting the risk of numerical and degeneracy issues. This study introduced an expectation conditional maximization algorithm that includes an adaptive step size function within Newton-Raphson updates of the shape parameter and a modified criterion for stopping the EM iterations. Through extensive simulations, the effectiveness of the proposed algorithm in overcoming the limitations of existing approaches, especially in scenarios with high shape parameter values, high parameters overalp and low sample sizes, is shown. A detailed comparative analysis with a mixture of normals and Student-t distributions revealed that the MGND model exhibited superior goodness-of-fit performance when used to fit the density of the returns of 50 stocks belonging to the Euro Stoxx index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21559v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierdomenico Duttilo, Stefano Antonio Gattone</dc:creator>
    </item>
    <item>
      <title>Approximate Bayesian Computation with Statistical Distances for Model Selection</title>
      <link>https://arxiv.org/abs/2410.21603</link>
      <description>arXiv:2410.21603v1 Announce Type: new 
Abstract: Model selection is a key task in statistics, playing a critical role across various scientific disciplines. While no model can fully capture the complexities of a real-world data-generating process, identifying the model that best approximates it can provide valuable insights. Bayesian statistics offers a flexible framework for model selection by updating prior beliefs as new data becomes available, allowing for ongoing refinement of candidate models. This is typically achieved by calculating posterior probabilities, which quantify the support for each model given the observed data. However, in cases where likelihood functions are intractable, exact computation of these posterior probabilities becomes infeasible. Approximate Bayesian Computation (ABC) has emerged as a likelihood-free method and it is traditionally used with summary statistics to reduce data dimensionality, however this often results in information loss difficult to quantify, particularly in model selection contexts. Recent advancements propose the use of full data approaches based on statistical distances, offering a promising alternative that bypasses the need for summary statistics and potentially allows recovery of the exact posterior distribution. Despite these developments, full data ABC approaches have not yet been widely applied to model selection problems. This paper seeks to address this gap by investigating the performance of ABC with statistical distances in model selection. Through simulation studies and an application to toad movement models, this work explores whether full data approaches can overcome the limitations of summary statistic-based ABC for model choice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21603v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Clara Grazian, Christian Angelopoulos</dc:creator>
    </item>
    <item>
      <title>Robust Estimation and Model Selection for the Controlled Directed Effect with Unmeasured Mediator-Outcome Confounders</title>
      <link>https://arxiv.org/abs/2410.21832</link>
      <description>arXiv:2410.21832v1 Announce Type: new 
Abstract: Controlled Direct Effect (CDE) is one of the causal estimands used to evaluate both exposure and mediation effects on an outcome. When there are unmeasured confounders existing between the mediator and the outcome, the ordinary identification assumption does not work. In this manuscript, we consider an identification condition to identify CDE in the presence of unmeasured confounders. The key assumptions are: 1) the random allocation of the exposure, and 2) the existence of instrumental variables directly related to the mediator. Under these conditions, we propose a novel doubly robust estimation method, which work well if either the propensity score model or the baseline outcome model is correctly specified. Additionally, we propose a Generalized Information Criterion (GIC)-based model selection criterion for CDE that ensures model selection consistency. Our proposed procedure and related methods are applied to both simulation and real datasets to confirm the performance of these methods. Our proposed method can select the correct model with high probability and accurately estimate CDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21832v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunichiro Orihara, Shinpei Imori, Kosuke Morikawa, Atsushi Goto, Masataka Taguri</dc:creator>
    </item>
    <item>
      <title>Joint Estimation of Conditional Mean and Covariance for Unbalanced Panels</title>
      <link>https://arxiv.org/abs/2410.21858</link>
      <description>arXiv:2410.21858v1 Announce Type: new 
Abstract: We propose a novel nonparametric kernel-based estimator of cross-sectional conditional mean and covariance matrices for large unbalanced panels. We show its consistency and provide finite-sample guarantees. In an empirical application, we estimate conditional mean and covariance matrices for a large unbalanced panel of monthly stock excess returns given macroeconomic and firm-specific covariates from 1962 to 2021.The estimator performs well with respect to statistical measures. It is informative for empirical asset pricing, generating conditional mean-variance efficient portfolios with substantial out-of-sample Sharpe ratios far beyond equal-weighted benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21858v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Damir Filipovic, Paul Schneider</dc:creator>
    </item>
    <item>
      <title>Bayesian Stability Selection and Inference on Inclusion Probabilities</title>
      <link>https://arxiv.org/abs/2410.21914</link>
      <description>arXiv:2410.21914v1 Announce Type: new 
Abstract: Stability selection is a versatile framework for structure estimation and variable selection in high-dimensional setting, primarily grounded in frequentist principles. In this paper, we propose an enhanced methodology that integrates Bayesian analysis to refine the inference of inclusion probabilities within the stability selection framework. Traditional approaches rely on selection frequencies for decision-making, often disregarding domain-specific knowledge and failing to account for the inherent uncertainty in the variable selection process. Our methodology uses prior information to derive posterior distributions of inclusion probabilities, thereby improving both inference and decision-making. We present a two-step process for engaging with domain experts, enabling statisticians to elucidate prior distributions informed by expert knowledge while allowing experts to control the weight of their input on the final results. Using posterior distributions, we offer Bayesian credible intervals to quantify uncertainty in the variable selection process. In addition, we highlight how selection frequencies can be uninformative or even misleading when covariates are correlated with each other, and demonstrate how domain expertise can alleviate such issues. Our approach preserves the versatility of stability selection and is suitable for a broad range of structure estimation challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21914v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Nouraie, Connor Smith, Samuel Muller</dc:creator>
    </item>
    <item>
      <title>Inference of a Susceptible-Infectious stochastic model</title>
      <link>https://arxiv.org/abs/2410.21954</link>
      <description>arXiv:2410.21954v1 Announce Type: new 
Abstract: We consider a time-inhomogeneous diffusion process able to describe the dynamics of infected people in a susceptible-infectious epidemic model in which the transmission intensity function is time-dependent. Such a model is well suited to describe some classes of micro-parasitic infections in which individuals never acquire lasting immunity and over the course of the epidemic everyone eventually becomes infected. The stochastic process related to the deterministic model is transformable into a non homogeneous Wiener process so the probability distribution can be obtained. Here we focus on the inference for such process, by providing an estimation procedure for the involved parameters. We point out that the time dependence in the infinitesimal moments of the diffusion process makes classical inference methods inapplicable. The proposed procedure is based on Generalized Method of Moments in order to find suitable estimate for the infinitesimal drift and variance of the transformed process. Several simulation studies are conduced to test the procedure, these include the time homogeneous case, for which a comparison with the results obtained by applying the MLE is made, and cases in which the intensity function are time dependent with particular attention to periodic cases. Finally, we apply the estimation procedure to a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21954v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3934/mbe.2024310</arxiv:DOI>
      <arxiv:journal_reference>Mathematical Biosciences and Engineering, 21(9), 7067-7083, 2024</arxiv:journal_reference>
      <dc:creator>Giuseppina Albano, Virginia Giorno, Francisco Torres-Ruiz</dc:creator>
    </item>
    <item>
      <title>On the Consistency of Partial Ordering Continual Reassessment Method with Model and Ordering Misspecification</title>
      <link>https://arxiv.org/abs/2410.21989</link>
      <description>arXiv:2410.21989v1 Announce Type: new 
Abstract: One of the aims of Phase I clinical trial designs in oncology is typically to find the maximum tolerated doses. A number of innovative dose-escalation designs were proposed in the literature to achieve this goal efficiently. Although the sample size of Phase I trials is usually small, the asymptotic properties (e.g. consistency) of dose-escalation designs can provide useful guidance on the design parameters and improve fundamental understanding of these designs. For the first proposed model-based monotherapy dose-escalation design, the Continual Reassessment Method (CRM), sufficient consistency conditions have been previously derived and then greatly influenced on how these studies are run in practice. At the same time, there is an increasing interest in Phase I combination-escalation trial in which two or more drugs are combined. The monotherapy dose-escalation design cannot be generally applied in this case due to uncertainty between monotonic ordering between some of the combinations, and, as a result, specialised designs were proposed. However, there were no theoretical or asymptotic properties evaluation of these proposals. In this paper, we derive the consistency conditions of the partial Ordering CRM (POCRM) design when there exists uncertainty in the monotonic ordering with a focus on dual-agent combination-escalation trials. Based on the derived consistency condition, we provide guidance on how the design parameters and ordering of the POCRM should be defined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21989v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weishi Chen, Pavel Mozgunov</dc:creator>
    </item>
    <item>
      <title>Model-free Estimation of Latent Structure via Multiscale Nonparametric Maximum Likelihood</title>
      <link>https://arxiv.org/abs/2410.22248</link>
      <description>arXiv:2410.22248v1 Announce Type: new 
Abstract: Multivariate distributions often carry latent structures that are difficult to identify and estimate, and which better reflect the data generating mechanism than extrinsic structures exhibited simply by the raw data. In this paper, we propose a model-free approach for estimating such latent structures whenever they are present, without assuming they exist a priori. Given an arbitrary density $p_0$, we construct a multiscale representation of the density and propose data-driven methods for selecting representative models that capture meaningful discrete structure. Our approach uses a nonparametric maximum likelihood estimator to estimate the latent structure at different scales and we further characterize their asymptotic limits. By carrying out such a multiscale analysis, we obtain coarseto-fine structures inherent in the original distribution, which are integrated via a model selection procedure to yield an interpretable discrete representation of it. As an application, we design a clustering algorithm based on the proposed procedure and demonstrate its effectiveness in capturing a wide range of latent structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22248v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bryon Aragam, Ruiyi Yang</dc:creator>
    </item>
    <item>
      <title>A Latent Variable Model with Change Points and Its Application to Time Pressure Effects in Educational Assessment</title>
      <link>https://arxiv.org/abs/2410.22300</link>
      <description>arXiv:2410.22300v1 Announce Type: new 
Abstract: Educational assessments are valuable tools for measuring student knowledge and skills, but their validity can be compromised when test takers exhibit changes in response behavior due to factors such as time pressure. To address this issue, we introduce a novel latent factor model with change-points for item response data, designed to detect and account for individual-level shifts in response patterns during testing. This model extends traditional Item Response Theory (IRT) by incorporating person-specific change-points, which enables simultaneous estimation of item parameters, person latent traits, and the location of behavioral changes. We evaluate the proposed model through extensive simulation studies, which demonstrate its ability to accurately recover item parameters, change-point locations, and individual ability estimates under various conditions. Our findings show that accounting for change-points significantly reduces bias in ability estimates, particularly for respondents affected by time pressure. Application of the model to two real-world educational testing datasets reveals distinct patterns of change-point occurrence between high-stakes and lower-stakes tests, providing insights into how test-taking behavior evolves during the tests. This approach offers a more nuanced understanding of test-taking dynamics, with important implications for test design, scoring, and interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22300v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Wallin, Yunxiao Chen, Yi-Hsuan Lee, Xiaoou Li</dc:creator>
    </item>
    <item>
      <title>Hypothesis tests and model parameter estimation on data sets with missing correlation information</title>
      <link>https://arxiv.org/abs/2410.22333</link>
      <description>arXiv:2410.22333v1 Announce Type: new 
Abstract: Ideally, all analyses of normally distributed data should include the full covariance information between all data points. In practice, the full covariance matrix between all data points is not always available. Either because a result was published without a covariance matrix, or because one tries to combine multiple results from separate publications. For simple hypothesis tests, it is possible to define robust test statistics that will behave conservatively in the presence on unknown correlations. For model parameter fits, one can inflate the variance by factor to ensure that things remain conservative at least up to a chosen confidence level. This paper describes a class of robust test statistics for simply hypothesis tests, as well as an algorithm to determine the necessary inflation factor model parameter fits. It then presents some example applications of the methods to real neutrino interaction data and model comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22333v1</guid>
      <category>stat.ME</category>
      <category>hep-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Koch</dc:creator>
    </item>
    <item>
      <title>Reconstructing East Asian Temperatures from 1368 to 1911 Using Historical Documents, Climate Models, and Data Assimilation</title>
      <link>https://arxiv.org/abs/2410.21790</link>
      <description>arXiv:2410.21790v1 Announce Type: cross 
Abstract: We present a novel approach for reconstructing annual temperatures in East Asia from 1368 to 1911, leveraging the Reconstructed East Asian Climate Historical Encoded Series (REACHES). The lack of instrumental data during this period poses significant challenges to understanding past climate conditions. REACHES digitizes historical documents from the Ming and Qing dynasties of China, converting qualitative descriptions into a four-level ordinal temperature scale. However, these index-based data are biased toward abnormal or extreme weather phenomena, leading to data gaps that likely correspond to normal conditions. To address this bias and reconstruct historical temperatures at any point within East Asia, including locations without direct historical data, we employ a three-tiered statistical framework. First, we perform kriging to interpolate temperature data across East Asia, adopting a zero-mean assumption to handle missing information. Next, we utilize the Last Millennium Ensemble (LME) reanalysis data and apply quantile mapping to calibrate the kriged REACHES data to Celsius temperature scales. Finally, we introduce a novel Bayesian data assimilation method that integrates the kriged Celsius data with LME simulations to enhance reconstruction accuracy. We model the LME data at each geographic location using a flexible nonstationary autoregressive time series model and employ regularized maximum likelihood estimation with a fused lasso penalty. The resulting dynamic distribution serves as a prior, which is refined via Kalman filtering by incorporating the kriged Celsius REACHES data to yield posterior temperature estimates. This comprehensive integration of historical documentation, contemporary climate models, and advanced statistical methods improves the accuracy of historical temperature reconstructions and provides a crucial resource for future environmental and climate studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21790v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Sun, Kuan-hui Elaine Lin, Wan-Ling Tseng, Pao K. Wang, Hsin-Cheng Huang</dc:creator>
    </item>
    <item>
      <title>Deep Q-Exponential Processes</title>
      <link>https://arxiv.org/abs/2410.22119</link>
      <description>arXiv:2410.22119v1 Announce Type: cross 
Abstract: Motivated by deep neural networks, the deep Gaussian process (DGP) generalizes the standard GP by stacking multiple layers of GPs. Despite the enhanced expressiveness, GP, as an $L_2$ regularization prior, tends to be over-smooth and sub-optimal for inhomogeneous subjects, such as images with edges. Recently, Q-exponential process (Q-EP) has been proposed as an $L_q$ relaxation to GP and demonstrated with more desirable regularization properties through a parameter $q&gt;0$ with $q=2$ corresponding to GP. Sharing the similar tractability of posterior and predictive distributions with GP, Q-EP can also be stacked to improve its modeling flexibility. In this paper, we generalize Q-EP to deep Q-EP to enjoy both proper regularization and improved expressiveness. The generalization is realized by introducing shallow Q-EP as a latent variable model and then building a hierarchy of the shallow Q-EP layers. Sparse approximation by inducing points and scalable variational strategy are applied to facilitate the inference. We demonstrate the numerical advantages of the proposed deep Q-EP model by comparing with multiple state-of-the-art deep probabilistic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22119v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi Chang, Chukwudi Obite, Shuang Zhou, Shiwei Lan</dc:creator>
    </item>
    <item>
      <title>Measuring Information Transfer Between Nodes in a Brain Network through Spectral Transfer Entropy</title>
      <link>https://arxiv.org/abs/2303.06384</link>
      <description>arXiv:2303.06384v3 Announce Type: replace 
Abstract: Brain connectivity characterizes interactions between different regions of a brain network during resting-state or performance of a cognitive task. In studying brain signals such as electroencephalograms (EEG), one formal approach to investigating connectivity is through an information-theoretic causal measure called transfer entropy (TE). To enhance the functionality of TE in brain signal analysis, we propose a novel methodology that captures cross-channel information transfer in the frequency domain. Specifically, we introduce a new measure, the spectral transfer entropy (STE), to quantify the magnitude and direction of information flow from a band-specific oscillation of one channel to another band-specific oscillation of another channel. The main advantage of our proposed approach is that it formulates TE in a novel way to perform inference on band-specific oscillations while maintaining robustness to the inherent problems associated with filtering. In addition, an advantage of STE is that it allows adjustments for multiple comparisons to control false positive rates. Another novel contribution is a simple yet efficient method for estimating STE using vine copula theory. This method can produce an exact zero estimate of STE (which is the boundary point of the parameter space) without the need for bias adjustments. With the vine copula representation, a null copula model, which exhibits zero STE, is defined, thus enabling straightforward significance testing through standard resampling. Lastly, we demonstrate the advantage of the proposed STE measure through numerical experiments and provide interesting and novel findings on the analysis of EEG data in a visual-memory experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.06384v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paolo Victor Redondo, Raphael Huser, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Externally Valid Policy Evaluation Combining Trial and Observational Data</title>
      <link>https://arxiv.org/abs/2310.14763</link>
      <description>arXiv:2310.14763v3 Announce Type: replace 
Abstract: Randomized trials are widely considered as the gold standard for evaluating the effects of decision policies. Trial data is, however, drawn from a population which may differ from the intended target population and this raises a problem of external validity (aka. generalizability). In this paper we seek to use trial data to draw valid inferences about the outcome of a policy on the target population. Additional covariate data from the target population is used to model the sampling of individuals in the trial study. We develop a method that yields certifiably valid trial-based policy evaluations under any specified range of model miscalibrations. The method is nonparametric and the validity is assured even with finite samples. The certified policy evaluations are illustrated using both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14763v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sofia Ek, Dave Zachariah</dc:creator>
    </item>
    <item>
      <title>Automated threshold selection and associated inference uncertainty for univariate extremes</title>
      <link>https://arxiv.org/abs/2310.17999</link>
      <description>arXiv:2310.17999v5 Announce Type: replace 
Abstract: Threshold selection is a fundamental problem in any threshold-based extreme value analysis. While models are asymptotically motivated, selecting an appropriate threshold for finite samples is difficult and highly subjective through standard methods. Inference for high quantiles can also be highly sensitive to the choice of threshold. Too low a threshold choice leads to bias in the fit of the extreme value model, while too high a choice leads to unnecessary additional uncertainty in the estimation of model parameters. We develop a novel methodology for automated threshold selection that directly tackles this bias-variance trade-off. We also develop a method to account for the uncertainty in the threshold estimation and propagate this uncertainty through to high quantile inference. Through a simulation study, we demonstrate the effectiveness of our method for threshold selection and subsequent extreme quantile estimation, relative to the leading existing methods, and show how the method's effectiveness is not sensitive to the tuning parameters. We apply our method to the well-known, troublesome example of the River Nidd dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17999v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Conor Murphy, Jonathan A. Tawn, Zak Varty</dc:creator>
    </item>
    <item>
      <title>Uncertainty-aware multi-fidelity surrogate modeling with noisy data</title>
      <link>https://arxiv.org/abs/2401.06447</link>
      <description>arXiv:2401.06447v2 Announce Type: replace 
Abstract: Emulating high-accuracy computationally expensive models is crucial for tasks requiring numerous model evaluations, such as uncertainty quantification and optimization. When lower-fidelity models are available, they can be used to improve the predictions of high-fidelity models. Multi-fidelity surrogate models combine information from sources of varying fidelities to construct an efficient surrogate model. However, in real-world applications, uncertainty is present in both high- and low-fidelity models due to measurement or numerical noise, as well as lack of knowledge due to the limited experimental design budget. This paper introduces a comprehensive framework for multi-fidelity surrogate modeling that handles noise-contaminated data and is able to estimate the underlying noise-free high-fidelity model. Our methodology quantitatively incorporates the different types of uncertainty affecting the problem and emphasizes on delivering precise estimates of the uncertainty in its predictions both with respect to the underlying high-fidelity model and unseen noise-contaminated high-fidelity observations, presented through confidence and prediction intervals, respectively. Additionally, the proposed framework offers a natural approach to combining physical experiments and computational models by treating noisy experimental data as high-fidelity sources and white-box computational models as their low-fidelity counterparts. The effectiveness of our methodology is showcased through synthetic examples and a wind turbine application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06447v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katerina Giannoukou, Stefano Marelli, Bruno Sudret</dc:creator>
    </item>
    <item>
      <title>Pretraining and the Lasso</title>
      <link>https://arxiv.org/abs/2401.12911</link>
      <description>arXiv:2401.12911v4 Announce Type: replace 
Abstract: Pretraining is a popular and powerful paradigm in machine learning to pass information from one model to another. As an example, suppose one has a modest-sized dataset of images of cats and dogs, and plans to fit a deep neural network to classify them from the pixel features. With pretraining, we start with a neural network trained on a large corpus of images, consisting of not just cats and dogs but hundreds of other image types. Then we fix all of the network weights except for the top layer(s) (which makes the final classification) and train (or "fine tune") those weights on our dataset. This often results in dramatically better performance than the network trained solely on our smaller dataset. In this paper, we ask the question "Can pretraining help the lasso?".
  We develop a framework for the lasso in which a model is fit to a large dataset, and then fine-tuned using a smaller dataset. This latter dataset can be a subset of the original dataset, or it can be a dataset with a different but related outcome. This framework has a wide variety of applications, including stratified models, multinomial responses, multi-response models, conditional average treatment estimation and even gradient boosting.
  In the stratified model setting, the pretrained lasso pipeline estimates the coefficients common to all groups at the first stage, and then estimates the group-specific coefficients at the second "fine-tuning" stage. We show that under appropriate assumptions, the support recovery rate of the common coefficients is superior to that of the usual lasso trained only on individual groups. This separate identification of common and individual coefficients can also be useful for scientific understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12911v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erin Craig, Mert Pilanci, Thomas Le Menestrel, Balasubramanian Narasimhan, Manuel Rivas, Roozbeh Dehghannasiri, Julia Salzman, Jonathan Taylor, Robert Tibshirani</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal count autoregression</title>
      <link>https://arxiv.org/abs/2404.02982</link>
      <description>arXiv:2404.02982v3 Announce Type: replace 
Abstract: We study the problem of modeling and inference for spatio-temporal count processes. Our approach uses parsimonious parameterisations of multivariate autoregressive count time series models, including possible regression on covariates. We control the number of parameters by specifying spatial neighbourhood structures for possibly huge matrices that take into account spatio-temporal dependencies. This work is motivated by real data applications which call for suitable models. Extensive simulation studies show that our approach yields reliable estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02982v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steffen Maletz, Konstantinos Fokianos, Roland Fried</dc:creator>
    </item>
    <item>
      <title>Semiparametric causal mediation analysis in cluster-randomized experiments</title>
      <link>https://arxiv.org/abs/2404.18256</link>
      <description>arXiv:2404.18256v2 Announce Type: replace 
Abstract: In cluster-randomized experiments, there is emerging interest in exploring the causal mechanism in which a cluster-level treatment affects the outcome through an intermediate outcome. Despite an extensive development of causal mediation methods in the past decade, only a few exceptions have been considered in assessing causal mediation in cluster-randomized studies, all of which depend on parametric model-based estimators. In this article, we develop the formal semiparametric efficiency theory to motivate several doubly-robust methods for addressing several mediation effect estimands corresponding to both the cluster-average and the individual-level treatment effects in cluster-randomized experiments -- the natural indirect effect, natural direct effect, and spillover mediation effect. We derive the efficient influence function for each mediation effect, and carefully parameterize each efficient influence function to motivate practical strategies for operationalizing each estimator. We consider both parametric working models and data-adaptive machine learners to estimate the nuisance functions, and obtain semiparametric efficient causal mediation estimators in the latter case. Our methods are illustrated via extensive simulations and two completed cluster-randomized experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18256v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Cheng, Fan Li</dc:creator>
    </item>
    <item>
      <title>On the posterior property of the Rician distribution</title>
      <link>https://arxiv.org/abs/2410.00142</link>
      <description>arXiv:2410.00142v2 Announce Type: replace 
Abstract: The Rician distribution, a well-known statistical distribution frequently encountered in fields like magnetic resonance imaging and wireless communications, is particularly useful for describing many real phenomena such as signal process data. In this paper, we introduce objective Bayesian inference for the Rician distribution parameters, specifically the Jeffreys rule and Jeffreys prior are derived. We proved that the obtained posterior for the first priors led to an improper posterior while the Jeffreys prior led to a proper distribution. To evaluate the effectiveness of our proposed Bayesian estimation method, we perform extensive numerical simulations and compare the results with those obtained from traditional moment-based and maximum likelihood estimators. Our simulations illustrate that the Bayesian estimators derived from the Jeffreys prior provide nearly unbiased estimates, showcasing the advantages of our approach over classical techniques. Additionally, our framework incorporates the S.A.F.E. principles-Sustainable, Accurate, Fair, and Explainable-ensuring robustness, fairness, and transparency in predictive modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00142v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesus Enrique Achire Quispe, Eduardo Ramos, Pedro Luiz Ramos</dc:creator>
    </item>
    <item>
      <title>Feed-Forward Panel Estimation for Discrete-time Survival Analysis of Recurrent Events with Frailty</title>
      <link>https://arxiv.org/abs/2410.19271</link>
      <description>arXiv:2410.19271v2 Announce Type: replace 
Abstract: In recurrent survival analysis where the event of interest can occur multiple times for each subject, frailty models play a crucial role by capturing unobserved heterogeneity at the subject level within a population. Frailty models traditionally face challenges due to the lack of a closed-form solution for the maximum likelihood estimation that is unconditional on frailty. In this paper, we propose a novel method: Feed-Forward Panel estimation for discrete-time Survival Analysis (FFPSurv). Our model uses variational Bayesian inference to sequentially update the posterior distribution of frailty as recurrent events are observed, and derives a closed form for the panel likelihood, effectively addressing the limitation of existing frailty models. We demonstrate the efficacy of our method through extensive experiments on numerical examples and real-world recurrent survival data. Furthermore, we mathematically prove that our model is identifiable under minor assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19271v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Borna Bateni, Peyman Bateni, Bishwadeep Bhattacharyya, Devin Reeh</dc:creator>
    </item>
    <item>
      <title>trajmsm: An R package for Trajectory Analysis and Causal Modeling</title>
      <link>https://arxiv.org/abs/2410.19682</link>
      <description>arXiv:2410.19682v2 Announce Type: replace 
Abstract: The R package trajmsm provides functions designed to simplify the estimation of the parameters of a model combining latent class growth analysis (LCGA), a trajectory analysis technique, and marginal structural models (MSMs) called LCGA-MSM. LCGA summarizes similar patterns of change over time into a few distinct categories called trajectory groups, which are then included as "treatments" in the MSM. MSMs are a class of causal models that correctly handle treatment-confounder feedback. The parameters of LCGA-MSMs can be consistently estimated using different estimators, such as inverse probability weighting (IPW), g-computation, and pooled longitudinal targeted maximum likelihood estimation (pooled LTMLE). These three estimators of the parameters of LCGA-MSMs are currently implemented in our package. In the context of a time-dependent outcome, we previously proposed a combination of LCGA and history-restricted MSMs (LCGA-HRMSMs). Our package provides additional functions to estimate the parameters of such models. Version 0.1.3 of the package is currently available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19682v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Awa Diop, Caroline Sirois, Jason R. Guertin, Mireille E. Schnitzer, James M. Brophy, Denis Talbot</dc:creator>
    </item>
    <item>
      <title>On the potential benefits of entropic regularization for smoothing Wasserstein estimators</title>
      <link>https://arxiv.org/abs/2210.06934</link>
      <description>arXiv:2210.06934v3 Announce Type: replace-cross 
Abstract: This paper is focused on the study of entropic regularization in optimal transport as a smoothing method for Wasserstein estimators, through the prism of the classical tradeoff between approximation and estimation errors in statistics. Wasserstein estimators are defined as solutions of variational problems whose objective function involves the use of an optimal transport cost between probability measures. Such estimators can be regularized by replacing the optimal transport cost by its regularized version using an entropy penalty on the transport plan. The use of such a regularization has a potentially significant smoothing effect on the resulting estimators. In this work, we investigate its potential benefits on the approximation and estimation properties of regularized Wasserstein estimators. Our main contribution is to discuss how entropic regularization may reach, at a lower computational cost, statistical performances that are comparable to those of un-regularized Wasserstein estimators in statistical learning problems involving distributional data analysis. To this end, we present new theoretical results on the convergence of regularized Wasserstein estimators. We also study their numerical performances using simulated and real data in the supervised learning problem of proportions estimation in mixture models using optimal transport.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.06934v3</guid>
      <category>stat.ML</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J\'er\'emie Bigot, Paul Freulon, Boris P. Hejblum, Arthur Leclaire</dc:creator>
    </item>
    <item>
      <title>Estimating Policy Effects in a Social Network with Independent Set Sampling</title>
      <link>https://arxiv.org/abs/2306.14142</link>
      <description>arXiv:2306.14142v4 Announce Type: replace-cross 
Abstract: Evaluating the impact of policy interventions on respondents who are embedded in a social network is often challenging due to the presence of network interference within the treatment groups, as well as between treatment and non-treatment groups throughout the network. In this paper, we propose a novel empirical strategy that combines network sampling based on the identification of independent sets with a stochastic actor-oriented model (SAOM) to infer the direct and net effects of a policy. By assigning respondents from an independent set to the treatment, we are able to block direct spillover of the treatment among the treated respondents for an extended period of time, during which the direct effect of the treatment can be isolated from the associated network interference. We empirically demonstrate this using a simulation-based evaluation of a fictitious policy implementation using both real-life and generated networks, and use a counterfactual approach to estimate the treatment effect of the policy. Our results highlight the effectiveness of our proposed empirical strategy, and notably, the role of network sampling techniques in influencing the evaluation of policy effects. The findings from this study have the potential to help researchers and policymakers with planning, designing, and anticipating policy responses in a networked society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14142v4</guid>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eugene Ang, Prasanta Bhattacharya, Andrew Lim</dc:creator>
    </item>
    <item>
      <title>On the Computational Complexity of Private High-dimensional Model Selection</title>
      <link>https://arxiv.org/abs/2310.07852</link>
      <description>arXiv:2310.07852v5 Announce Type: replace-cross 
Abstract: We consider the problem of model selection in a high-dimensional sparse linear regression model under privacy constraints. We propose a differentially private (DP) best subset selection method with strong statistical utility properties by adopting the well-known exponential mechanism for selecting the best model. To achieve computational expediency, we propose an efficient Metropolis-Hastings algorithm and under certain regularity conditions, we establish that it enjoys polynomial mixing time to its stationary distribution. As a result, we also establish both approximate differential privacy and statistical utility for the estimates of the mixed Metropolis-Hastings chain. Finally, we perform some illustrative experiments on simulated data showing that our algorithm can quickly identify active features under reasonable privacy budget constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07852v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Roy, Zehua Wang, Ambuj Tewari</dc:creator>
    </item>
    <item>
      <title>Proximal Causal Inference With Text Data</title>
      <link>https://arxiv.org/abs/2401.06687</link>
      <description>arXiv:2401.06687v3 Announce Type: replace-cross 
Abstract: Recent text-based causal methods attempt to mitigate confounding bias by estimating proxies of confounding variables that are partially or imperfectly measured from unstructured text data. These approaches, however, assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is sometimes infeasible due to data privacy or annotation costs. In this work, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that uses two instances of pre-treatment text data, infers two proxies using two zero-shot models on the separate instances, and applies these proxies in the proximal g-formula. We prove, under certain assumptions about the instances of text and accuracy of the zero-shot predictions, that our method of inferring text-based proxies satisfies identification conditions of the proximal g-formula while other seemingly reasonable proposals do not. To address untestable assumptions associated with our method and the proximal g-formula, we further propose an odds ratio falsification heuristic that flags when to proceed with downstream effect estimation using the inferred proxies. We evaluate our method in synthetic and semi-synthetic settings -- the latter with real-world clinical notes from MIMIC-III and open large language models for zero-shot prediction -- and find that our method produces estimates with low bias. We believe that this text-based design of proxies allows for the use of proximal causal inference in a wider range of scenarios, particularly those for which obtaining suitable proxies from structured data is difficult.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06687v3</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob M. Chen, Rohit Bhattacharya, Katherine A. Keith</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Inference in Causal Latent Factor Models</title>
      <link>https://arxiv.org/abs/2402.11652</link>
      <description>arXiv:2402.11652v3 Announce Type: replace-cross 
Abstract: This article introduces a new estimator of average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes. The proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion. We derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero Gaussian distribution at a parametric rate. Simulation results demonstrate the relevance of the formal properties of the estimators analyzed in this article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11652v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Abadie, Anish Agarwal, Raaz Dwivedi, Abhin Shah</dc:creator>
    </item>
    <item>
      <title>A compromise criterion for weighted least squares estimates</title>
      <link>https://arxiv.org/abs/2404.00753</link>
      <description>arXiv:2404.00753v3 Announce Type: replace-cross 
Abstract: In the heteroscedastic linear model, the weighted least squares estimate of the model coefficients is more efficient than the ordinary least squares estimate. However, the practical application of weighted least squares is challenging because it requires knowledge of the error variances. Feasible weighted least squares estimates, which use approximations of the variances when they are unknown, may either be more or less efficient than the ordinary least squares estimate depending on the quality of the approximation. A direct comparison between feasible and ordinary least squares has significant implications for the application of regression analysis in varied fields, yet such a comparison remains an unresolved challenge. In this study, we address this challenge by identifying the conditions under which feasible weighted least squares estimates using fixed weights demonstrate greater efficiency than the ordinary least squares estimate. These conditions provide guidance for the design of feasible estimates using random weights. They also shed light on how certain robust regression estimates behave with respect to the linear model with normal errors of unequal variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00753v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan Bryan, Haibo Zhou, Didong Li</dc:creator>
    </item>
    <item>
      <title>Propensity Score Alignment of Unpaired Multimodal Data</title>
      <link>https://arxiv.org/abs/2404.01595</link>
      <description>arXiv:2404.01595v2 Announce Type: replace-cross 
Abstract: Multimodal representation learning techniques typically rely on paired samples to learn common representations, but paired samples are challenging to collect in fields such as biology where measurement devices often destroy the samples. This paper presents an approach to address the challenge of aligning unpaired samples across disparate modalities in multimodal representation learning. We draw an analogy between potential outcomes in causal inference and potential views in multimodal observations, which allows us to use Rubin's framework to estimate a common space in which to match samples. Our approach assumes we collect samples that are experimentally perturbed by treatments, and uses this to estimate a propensity score from each modality, which encapsulates all shared information between a latent state and treatment and can be used to define a distance between samples. We experiment with two alignment techniques that leverage this distance -- shared nearest neighbours (SNN) and optimal transport (OT) matching -- and find that OT matching results in significant improvements over state-of-the-art alignment approaches in both a synthetic multi-modal setting and in real-world data from NeurIPS Multimodal Single-Cell Integration Challenge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01595v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johnny Xi, Jana Osea, Zuheng Xu, Jason Hartford</dc:creator>
    </item>
    <item>
      <title>Causal Contrastive Learning for Counterfactual Regression Over Time</title>
      <link>https://arxiv.org/abs/2406.00535</link>
      <description>arXiv:2406.00535v3 Announce Type: replace-cross 
Abstract: Estimating treatment effects over time holds significance in various domains, including precision medicine, epidemiology, economy, and marketing. This paper introduces a unique approach to counterfactual regression over time, emphasizing long-term predictions. Distinguishing itself from existing models like Causal Transformer, our approach highlights the efficacy of employing RNNs for long-term forecasting, complemented by Contrastive Predictive Coding (CPC) and Information Maximization (InfoMax). Emphasizing efficiency, we avoid the need for computationally expensive transformers. Leveraging CPC, our method captures long-term dependencies in the presence of time-varying confounders. Notably, recent models have disregarded the importance of invertible representation, compromising identification assumptions. To remedy this, we employ the InfoMax principle, maximizing a lower bound of mutual information between sequence data and its representation. Our method achieves state-of-the-art counterfactual estimation results using both synthetic and real-world data, marking the pioneering incorporation of Contrastive Predictive Encoding in causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00535v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mouad El Bouchattaoui, Myriam Tami, Benoit Lepetit, Paul-Henry Courn\`ede</dc:creator>
    </item>
    <item>
      <title>Efficient Neural Network Training via Subset Pretraining</title>
      <link>https://arxiv.org/abs/2410.16523</link>
      <description>arXiv:2410.16523v2 Announce Type: replace-cross 
Abstract: In training neural networks, it is common practice to use partial gradients computed over batches, mostly very small subsets of the training set. This approach is motivated by the argument that such a partial gradient is close to the true one, with precision growing only with the square root of the batch size. A theoretical justification is with the help of stochastic approximation theory. However, the conditions for the validity of this theory are not satisfied in the usual learning rate schedules. Batch processing is also difficult to combine with efficient second-order optimization methods. This proposal is based on another hypothesis: the loss minimum of the training set can be expected to be well-approximated by the minima of its subsets. Such subset minima can be computed in a fraction of the time necessary for optimizing over the whole training set. This hypothesis has been tested with the help of the MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks, optionally extended by training data augmentation. The experiments have confirmed that results equivalent to conventional training can be reached. In summary, even small subsets are representative if the overdetermination ratio for the given model parameter set sufficiently exceeds unity. The computing expense can be reduced to a tenth or less.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16523v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jan Sp\"orer, Bernhard Bermeitinger, Tomas Hrycej, Niklas Limacher, Siegfried Handschuh</dc:creator>
    </item>
    <item>
      <title>Sacred and Profane: from the Involutive Theory of MCMC to Helpful Hamiltonian Hacks</title>
      <link>https://arxiv.org/abs/2410.17398</link>
      <description>arXiv:2410.17398v2 Announce Type: replace-cross 
Abstract: In the first edition of this Handbook, two remarkable chapters consider seemingly distinct yet deeply connected subjects ...</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17398v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan E. Glatt-Holtz, Andrew J. Holbrook, Justin A. Krometis, Cecilia F. Mondaini, Ami Sheth</dc:creator>
    </item>
  </channel>
</rss>
