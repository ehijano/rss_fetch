<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 May 2025 04:00:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Generative modelling of multivariate geometric extremes using normalising flows</title>
      <link>https://arxiv.org/abs/2505.02957</link>
      <description>arXiv:2505.02957v1 Announce Type: new 
Abstract: Leveraging the recently emerging geometric approach to multivariate extremes and the flexibility of normalising flows on the hypersphere, we propose a principled deep-learning-based methodology that enables accurate joint tail extrapolation in all directions. We exploit theoretical links between intrinsic model parameters defined as functions on hyperspheres to construct models ranging from high flexibility to parsimony, thereby enabling the efficient modelling of multivariate extremes displaying complex dependence structures in higher dimensions with reasonable sample sizes. We use the generative feature of normalising flows to perform fast probability estimation for arbitrary Borel risk regions via an efficient Monte Carlo integration scheme. The good properties of our estimators are demonstrated via a simulation study in up to ten dimensions. We apply our methodology to the analysis of low and high extremes of wind speeds. In particular, we find that our methodology enables probability estimation for non-trivial extreme events in relation to electricity production via wind turbines and reveals interesting structure in the underlying data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02957v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lambert De Monte, Rapha\"el Huser, Ioannis Papastathopoulos, Jordan Richards</dc:creator>
    </item>
    <item>
      <title>Robust High-Dimensional Covariate-Assisted Network Modeling</title>
      <link>https://arxiv.org/abs/2505.02986</link>
      <description>arXiv:2505.02986v1 Announce Type: new 
Abstract: Modern network data analysis often involves analyzing network structures alongside covariate features to gain deeper insights into underlying patterns. However, traditional covariate-assisted statistical network models may not adequately handle cases involving high-dimensional covariates, where some covariates could be uninformative or misleading, or the possible mismatch between network and covariate information. To address this issue, we introduce a novel robust high-dimensional covariate-assisted latent space model. This framework links latent vectors representing network structures with simultaneously sparse and low-rank transformations of the high-dimensional covariates, capturing the mutual dependence between network structures and covariates. To robustly integrate this dependence, we use a shrinkage prior on the discrepancy between latent network vectors and low-rank covariate approximation vectors, allowing for potential mismatches between network and covariate information. For scalable inference, we develop two variational inference algorithms, enabling efficient analysis of large-scale sparse networks. We establish the posterior concentration rate within a suitable parameter space and demonstrate how the proposed model facilitates adaptive information aggregation between networks and high-dimensional covariates. Extensive simulation studies and real-world data analyses confirm the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02986v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Peng Zhao, Yabo Niu</dc:creator>
    </item>
    <item>
      <title>Stratified Regression Analysis of Zero-Truncated Recurrent Event Data</title>
      <link>https://arxiv.org/abs/2505.02996</link>
      <description>arXiv:2505.02996v1 Announce Type: new 
Abstract: This paper is motivated by an ongoing pediatric mental health care (PMHC) program in which records of mental health-related emergency department (MHED) visits are extracted from population-based administrative databases. A particular interest of this paper is to understand how the visit occurrence depends on the occurrences in the past in a general population. Only information on subjects experiencing MHED visits is available within a subject-specific time window. Thus, the MHED visits may be viewed as zero-truncated recurrent events. Some population census information can be utilized as supplementary information on the covariates of subjects without MHED visits during the study period. We consider an innovative stratified Cox regression model, which is an intensity-based model but requiring only a summary of the event history. We propose an estimation procedure with zero-truncated data integrated with some supplementary information. We establish the consistency and asymptotic normality of the proposed estimator. The finite-sample properties of the estimator are evaluated by simulation, which demonstrates improved performance of the proposed estimator over the maximum likelihood estimator based on zero-truncated data only. We use the PMHC program to illustrate the proposed approach throughout the paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02996v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anqi A. Chen, X. Joan Hu, Rhonda J. Rosychuk</dc:creator>
    </item>
    <item>
      <title>An Introduction to Topological Data Analysis Ball Mapper in Python</title>
      <link>https://arxiv.org/abs/2505.03022</link>
      <description>arXiv:2505.03022v1 Announce Type: new 
Abstract: Visualization of data is an important step in the understanding of data and the evaluation of statistical models. Topological Data Analysis Ball Mapper (TDABM) after Dlotko (2019), provides a model free means to visualize multivariate datasets without information loss. To permit the construction of a TDABM graph, each variable must be ordinal and have sufficiently many values to make a scatterplot of interest. Where a scatterplot works with two, or three, axes, the TDABM graph can handle any number of axes simultaneously. The result is a visualization of the structure of data. The TDABM graph also permits coloration by additional variables, enabling the mapping of outcomes across the joint distribution of axes. The strengths of TDABM for understanding data, and evaluating models, lie behind a rapidly expanding literature. This guide provides an introduction to TDABM with code in Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03022v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Rudkin</dc:creator>
    </item>
    <item>
      <title>Enhancing Seasonal Adjustment Space Models: Constraints and Regularization for Improved Trend and AR Decomposition</title>
      <link>https://arxiv.org/abs/2505.03110</link>
      <description>arXiv:2505.03110v1 Announce Type: new 
Abstract: This paper investigates enhancements to model-based methods for seasonal adjustment, with a particular focus on the state space modeling framework. It addresses limitations of the standard Decomp model; specifically, the tendency to produce overly smooth trend components and the misattribution of long-term variation to the AR component when the eigenvalues of the AR model are close to unity. To mitigate these issues, the paper proposes imposing constraints on the modulus and argument of the AR eigenvalues, as well as applying regularization techniques ($L_1$ and $L_2$). These approaches are evaluated using real-world datasets. The paper is structured as follows: an overview of the Decomp model, a comparison with its noise-free variant, empirical assessment of constrained AR models, an exploration of regularization methods, and a concluding discussion of key insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03110v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Genshiro Kitagawa (Tokyo University of Marine Science,Technology,The Institute of Statistical Mathmatics)</dc:creator>
    </item>
    <item>
      <title>Nonparametric learning of covariate-based Markov jump processes using RKHS techniques</title>
      <link>https://arxiv.org/abs/2505.03119</link>
      <description>arXiv:2505.03119v1 Announce Type: new 
Abstract: We propose a novel nonparametric approach for linking covariates to Continuous Time Markov Chains (CTMCs) using the mathematical framework of Reproducing Kernel Hilbert Spaces (RKHS). CTMCs provide a robust framework for modeling transitions across clinical or behavioral states, but traditional multistate models often rely on linear relationships. In contrast, we use a generalized Representer Theorem to enable tractable inference in functional space. For the Frequentist version, we apply normed square penalties, while for the Bayesian version, we explore sparsity inducing spike and slab priors. Due to the computational challenges posed by high-dimensional spaces, we successfully adapt the Expectation Maximization Variable Selection (EMVS) algorithm to efficiently identify the posterior mode. We demonstrate the effectiveness of our method through extensive simulation studies and an application to follicular cell lymphoma data. Our performance metrics include the normalized difference between estimated and true nonlinear transition functions, as well as the difference in the probability of getting absorbed in one the final states, capturing the ability of our approach to predict long-term behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03119v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Han, Arnab Ganguly, Riten Mitra</dc:creator>
    </item>
    <item>
      <title>Metacountregressor: A python package for extensive analysis and assisted estimation of count data models</title>
      <link>https://arxiv.org/abs/2505.03133</link>
      <description>arXiv:2505.03133v1 Announce Type: new 
Abstract: {Analyzing and modeling rare events in count data presents significant challenges due to the scarcity of observations and the complexity of underlying processes, which are often overlooked by analysts due to limitations in time, resources, knowledge, and the influence of biases. This paper introduces MetaCountRegressor, a Python package designed to facilitate predictive count modeling of rare events guided by metaheuristics. The MetaCountRegressor package offers a wide range of functionalities specifically tailored for the unique characteristics of rare event prediction. This package offers a collection of metaheuristic algorithms that efficiently explore the solution space, facilitating effective optimisation and parameter tuning. These algorithms are specifically engineered to deal with the inherent challenges of modeling rare events for predictive purposes, and capturing causative effects that are easily interpretable. State-of-the-art models are produced by the decision-based optimization framework. This includes the ability to capture unobserved heterogeneity through random parameters and allows for correlated and grouped random parameters. It also supports a range of distributions for the random parameters, and can capture heterogeneity in the means. The package also supports panel data, among other features, and serves as a systematic framework for analysts to discover optimization-driven results, saving time, reducing biases, and minimizing the need for extensive prior knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03133v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeke Ahern, Paul Corry, Alexander Paz</dc:creator>
    </item>
    <item>
      <title>Univariate and multivariate tests of equality of quantiles with right-censored data</title>
      <link>https://arxiv.org/abs/2505.03234</link>
      <description>arXiv:2505.03234v1 Announce Type: new 
Abstract: A nonparametric test for equality of quantiles in the presence of right-censored data is studied. We propose to construct an asymptotic test statistic for the comparison of one quantile between two treatment groups, as well as for the comparison of a collection of quantiles. Under the null hypothesis of equality of quantiles, the test statistic follows asymptotically a normal distribution in the univariate case and a chi-square with J degrees of freedom in the multivariate case, with J the number of quantiles compared. Deriving the variance of the test statistic requires the estimation of the probability density function of the distribution of failure times at the quantile being tested. A resampling method is presented as an alternative to kernel density estimation to perform such task. Extensive simulation studies are performed to show that the proposed approach provides reasonable type I probabilities and powers. We illustrate the proposed test in a phase III randomized clinical trial where the proportional hazards assumption between treatment arms does not hold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03234v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beatriz Farah (ICSC, MAP5), Olivier Bouaziz (LPP), Aur\'elien Latouche (ICSC, CNAM Paris)</dc:creator>
    </item>
    <item>
      <title>Simultaneous global and local clustering in multiplex networks with covariate information</title>
      <link>https://arxiv.org/abs/2505.03441</link>
      <description>arXiv:2505.03441v1 Announce Type: new 
Abstract: Understanding both global and layer-specific group structures is useful for uncovering complex patterns in networks with multiple interaction types. In this work, we introduce a new model, the hierarchical multiplex stochastic blockmodel (HMPSBM), that simultaneously detects communities within individual layers of a multiplex network while inferring a global node clustering across the layers. A stochastic blockmodel is assumed in each layer, with probabilities of layer-level group memberships determined by a node's global group assignment. Our model uses a Bayesian framework, employing a probit stick-breaking process to construct node-specific mixing proportions over a set of shared Griffiths-Engen-McCloseky (GEM) distributions. These proportions determine layer-level community assignment, allowing for an unknown and varying number of groups across layers, while incorporating nodal covariate information to inform the global clustering. We propose a scalable variational inference procedure with parallelisable updates for application to large networks. Extensive simulation studies demonstrate our model's ability to accurately recover both global and layer-level clusters in complicated settings, and applications to real data showcase the model's effectiveness in uncovering interesting latent network structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03441v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Corneck, Edward A. K. Cohen, James S. Martin, Lekha Patel, Kurtis W. Shuler, Francesco Sanna Passino</dc:creator>
    </item>
    <item>
      <title>Using causal diagrams to assess parallel trends in difference-in-differences studies</title>
      <link>https://arxiv.org/abs/2505.03526</link>
      <description>arXiv:2505.03526v1 Announce Type: new 
Abstract: Difference-in-differences (DID) is popular because it can allow for unmeasured confounding when the key assumption of parallel trends holds. However, there exists little guidance on how to decide a priori whether this assumption is reasonable. We attempt to develop such guidance by considering the relationship between a causal diagram and the parallel trends assumption. This is challenging because parallel trends is scale-dependent and causal diagrams are generally scale-independent. We develop conditions under which, given a nonparametric causal diagram, one can reject or fail to reject parallel trends. In particular, we adopt a linear faithfulness assumption, which states that all graphically connected variables are correlated, and which is often reasonable in practice. We show that parallel trends can be rejected if either (i) the treatment is affected by pre-treatment outcomes, or (ii) there exist unmeasured confounders for the effect of treatment on pre-treatment outcomes that are not confounders for the post-treatment outcome, or vice versa (more precisely, the two outcomes possess distinct minimally sufficient sets). We also argue that parallel trends should be strongly questioned if (iii) the pre-treatment outcomes affect the post-treatment outcomes (though the two can be correlated) since there exist reasonable semiparametric models in which such an effect violates parallel trends. When (i-iii) are absent, a necessary and sufficient condition for parallel trends is that the association between the common set of confounders and the potential outcomes is constant on an additive scale, pre- and post-treatment. These conditions are similar to, but more general than, those previously derived in linear structural equations models. We discuss our approach in the context of the effect of Medicaid expansion under the U.S. Affordable Care Act on health insurance coverage rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03526v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Audrey Renson, Oliver Dukes, Zach Shahn</dc:creator>
    </item>
    <item>
      <title>New Bounds and Truncation Boundaries for Importance Sampling</title>
      <link>https://arxiv.org/abs/2505.03607</link>
      <description>arXiv:2505.03607v1 Announce Type: new 
Abstract: Importance sampling (IS) is a technique that enables statistical estimation of output performance at multiple input distributions from a single nominal input distribution. IS is commonly used in Monte Carlo simulation for variance reduction and in machine learning applications for reusing historical data, but its effectiveness can be challenging to quantify. In this work, we establish a new result showing the tightness of polynomial concentration bounds for classical IS likelihood ratio (LR) estimators in certain settings. Then, to address a practical statistical challenge that IS faces regarding potentially high variance, we propose new truncation boundaries when using a truncated LR estimator, for which we establish upper concentration bounds that imply an exponential convergence rate. Simulation experiments illustrate the contrasting convergence rates of the various LR estimators and the effectiveness of the newly proposed truncation-boundary LR estimators for examples from finance and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03607v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijuan Liang, Guangxin Jiang, Michael C. Fu</dc:creator>
    </item>
    <item>
      <title>CUI-MET: Clinical Utility Index Dose Optimization Approach for Multiple-Dose, Multiple-Outcome Randomized Trial Designs</title>
      <link>https://arxiv.org/abs/2505.03633</link>
      <description>arXiv:2505.03633v1 Announce Type: new 
Abstract: Dose optimization in oncology clinical trials has shifted from seeking the maximum tolerated dose to identifying the Optimal Biological Dose (OBD) that balances therapeutic benefits and risks across multiple clinical attributes. Existing advanced dose-finding methods can integrate multiple endpoints and compare dose levels but are often complex or computationally intensive, limiting their use in early-phase trials. To address these challenges, we propose the Clinical Utility Index Dose Optimization Approach for Multiple-dose Multiple-Outcome Randomized Trial Designs (CUI-MET). This framework integrates multiple binary endpoints using a clinical utility-based approach, calculating a combined clinical utility index (CUI) for each dose level by weighting endpoint responses. Both empirical and modeling methods can estimate marginal probabilities for each endpoint. These estimated probabilities are then combined using endpoint-specific weights to compute a utility score for each dose, and the dose with the highest score is selected as optimal. To enhance usability, we implemented these methods in an interactive R Shiny application and demonstrated their functionality through case examples. The framework's flexibility allows for different model selections and endpoint weighting schemes to reflect specific clinical priorities. Bootstrap analysis provides confidence intervals for the CUI and estimates the probability that each dose is selected as optimal, thereby evaluating the robustness of dose selection. By integrating multiple endpoints into a single utility index and incorporating user-friendly visualizations, CUI-MET offers a flexible and accessible solution for dose optimization in early-phase oncology trials, supporting informed decision-making and advancing patient-centered care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03633v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fanni Zhang, Kristine Broglio, Michael Sweeting, Gina D'Angelo</dc:creator>
    </item>
    <item>
      <title>GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds</title>
      <link>https://arxiv.org/abs/2505.02972</link>
      <description>arXiv:2505.02972v1 Announce Type: cross 
Abstract: Multi-Task Learning (MTL) seeks to boost statistical power and learning efficiency by discovering structure shared across related tasks. State-of-the-art MTL representation methods, however, usually treat the latent representation matrix as a point in ordinary Euclidean space, ignoring its often non-Euclidean geometry, thus sacrificing robustness when tasks are heterogeneous or even adversarial. We propose GeoERM, a geometry-aware MTL framework that embeds the shared representation on its natural Riemannian manifold and optimizes it via explicit manifold operations. Each training cycle performs (i) a Riemannian gradient step that respects the intrinsic curvature of the search space, followed by (ii) an efficient polar retraction to remain on the manifold, guaranteeing geometric fidelity at every iteration. The procedure applies to a broad class of matrix-factorized MTL models and retains the same per-iteration cost as Euclidean baselines. Across a set of synthetic experiments with task heterogeneity and on a wearable-sensor activity-recognition benchmark, GeoERM consistently improves estimation accuracy, reduces negative transfer, and remains stable under adversarial label noise, outperforming leading MTL and single-task alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02972v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aoran Chen, Yang Feng</dc:creator>
    </item>
    <item>
      <title>Multicategory Matched Learning for Estimating Optimal Individualized Treatment Rules in Observational Studies with Application to a Hepatocellular Carcinoma Study</title>
      <link>https://arxiv.org/abs/2302.05287</link>
      <description>arXiv:2302.05287v2 Announce Type: replace 
Abstract: One primary goal of precision medicine is to estimate the individualized treatment rules (ITRs) that optimize patients' health outcomes based on individual characteristics. Health studies with multiple treatments are commonly seen in practice. However, most existing ITR estimation methods were developed for the studies with binary treatments. Many require that the outcomes are fully observed. In this paper, we propose a matching-based machine learning method to estimate the optimal ITRs in observational studies with multiple treatments when the outcomes are fully observed or right-censored. We establish theoretical property for the proposed method. It is compared with the existing competitive methods in simulation studies and a hepatocellular carcinoma study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.05287v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuqiao Li, Qiuyan Zhou, Ying Wu, Ying Yan</dc:creator>
    </item>
    <item>
      <title>A Debiased Estimator for the Mediation Functional in Ultra-High-Dimensional Setting in the Presence of Interaction Effects</title>
      <link>https://arxiv.org/abs/2412.08827</link>
      <description>arXiv:2412.08827v2 Announce Type: replace 
Abstract: Mediation analysis is a crucial tool for uncovering the mechanisms through which a treatment affects the outcome, providing deeper causal insights and guiding effective interventions. Despite advances in analyzing the mediation effect with fixed/low-dimensional mediators and covariates, our understanding of estimation and inference of mediation functional in the presence of (ultra)-high-dimensional mediators and covariates is still limited. In this paper, we present an estimator for mediation functional in a high-dimensional setting that accommodates the interaction between covariates and treatment in generating mediators, as well as interactions between both covariates and treatment and mediators and treatment in generating the response. We demonstrate that our estimator is $\sqrt{n}$-consistent and asymptotically normal, thus enabling reliable inference on direct and indirect treatment effects with asymptotically valid confidence intervals. A key technical contribution of our work is to develop a multi-step debiasing technique, which may also be valuable in other statistical settings with similar structural complexities where accurate estimation depends on debiasing. We evaluate our proposed methodology through extensive simulation studies and apply it to the TCGA lung cancer dataset to estimate the effect of smoking, mediated by DNA methylation, on the survival time of lung cancer patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08827v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shi Bo, AmirEmad Ghassami, Debarghya Mukherjee</dc:creator>
    </item>
    <item>
      <title>Transfer Learning of CATE with Kernel Ridge Regression</title>
      <link>https://arxiv.org/abs/2502.11331</link>
      <description>arXiv:2502.11331v2 Announce Type: replace 
Abstract: The proliferation of data has sparked significant interest in leveraging findings from one study to estimate treatment effects in a different target population without direct outcome observations. However, the transfer learning process is frequently hindered by substantial covariate shift and limited overlap between (i) the source and target populations, as well as (ii) the treatment and control groups within the source. We propose a novel method for overlap-adaptive transfer learning of conditional average treatment effect (CATE) using kernel ridge regression (KRR). Our approach involves partitioning the labeled source data into two subsets. The first one is used to train candidate CATE models based on regression adjustment and pseudo-outcomes. An optimal model is then selected using the second subset and unlabeled target data, employing another pseudo-outcome-based strategy. We provide a theoretical justification for our method through sharp non-asymptotic MSE bounds, highlighting its adaptivity to both weak overlaps and the complexity of CATE function. Extensive numerical studies confirm that our method achieves superior finite-sample efficiency and adaptability. We conclude by demonstrating the effectiveness of our approach using a 401(k) eligibility dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11331v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seok-Jin Kim, Hongjie Liu, Molei Liu, Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>On Bessel's Correction: Unbiased Sample Variance, the "Bariance", and a Novel Runtime-Optimized Estimator</title>
      <link>https://arxiv.org/abs/2503.22333</link>
      <description>arXiv:2503.22333v3 Announce Type: replace 
Abstract: Bessel's correction adjusts the denominator in the sample variance formula from n to n-1 to produce an unbiased estimator for the population variance. This paper includes rigorous derivations, geometric interpretations, and visualizations. It then introduces the concept of "bariance", an alternative pairwise distances based intuition of sample dispersion without an arithmetic mean. Finally, we address practical concerns raised in Rosenthal's article (Rosenthal, 2015) advocating the use of n-based estimates from a more holistic MSE-based viewpoint for pedagogical reasons and in certain practical contexts. Finally, the empirical part using simulation reveals that the run-time of estimating population variance can be shortened when using an algebraically optimized "bariance" approach to estimate an unbiased population sample variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22333v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Reichel</dc:creator>
    </item>
    <item>
      <title>Bayesian Rao test for distributed target detection in interference and noise with limited training data</title>
      <link>https://arxiv.org/abs/2504.13235</link>
      <description>arXiv:2504.13235v2 Announce Type: replace 
Abstract: This paper has studied the problem of detecting a range-spread target in interference and noise when the number of training data is limited. The interference is located within a certain subspace with an unknown coordinate, while the noise follows a Gaussian distribution with an unknown covariance matrix. We concentrate on the scenarios where the training data are limited and employ a Bayesian framework to ffnd a solution. Speciffcally, the covariance matrix is assumed to follow an inverse Wishart distribution. Then, we introduce the Bayesian detector according to the Rao test, which, demonstrated by both simulation experiment and real data, has superior detection performance to the existing detectors in certain situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13235v2</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11432-024-4422-3</arxiv:DOI>
      <dc:creator>Daipeng Xiao, Weijian Liu, Jun Liu, Yuntao Wu, Qinglei Du, Xiaoqiang Hua</dc:creator>
    </item>
    <item>
      <title>Two-parameter superposable S-curves</title>
      <link>https://arxiv.org/abs/2504.19488</link>
      <description>arXiv:2504.19488v3 Announce Type: replace 
Abstract: Straight line equation $y=mx$ with slope $m$, when singularly perturbed as $ay^3+y=mx$ with a positive parameter $a$, results in S-shaped curves or S-curves on a real plane. As $a\rightarrow 0$, we get back $y=mx$ which is a cumulative distribution function of a continuous uniform distribution that describes the occurrence of every event in an interval to be equally probable. As $a\rightarrow\infty$, the derivative of $y$ has finite support only at $y=0$ resembling a degenerate distribution. Based on these arguments, in this work, we propose that these S-curves can represent maximum entropy uniform distribution to a zero entropy single value. We also argue that these S-curves are superposable as they are only parametrically nonlinear but fundamentally linear. So far, the superposed forms have been used to capture the patterns of natural systems such as nonlinear dynamics of biological growth and kinetics of enzyme reactions. Here, we attempt to use the S-curve and its superposed form as statistical models. We fit the models on a classical dataset containing flower measurements of iris plants and analyze their usefulness in pattern recognition. Based on these models, we claim that any non-uniform pattern can be represented as a singular perturbation to uniform distribution. However, our parametric estimation procedure have some limitations such as sensitivity to initial conditions depending on the data at hand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19488v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vijay Prakash S</dc:creator>
    </item>
    <item>
      <title>Relaxed Gaussian process interpolation: a goal-oriented approach to Bayesian optimization</title>
      <link>https://arxiv.org/abs/2206.03034</link>
      <description>arXiv:2206.03034v3 Announce Type: replace-cross 
Abstract: This work presents a new procedure for obtaining predictive distributions in the context of Gaussian process (GP) modeling, with a relaxation of the interpolation constraints outside ranges of interest: the mean of the predictive distributions no longer necessarily interpolates the observed values when they are outside ranges of interest, but are simply constrained to remain outside. This method called relaxed Gaussian process (reGP) interpolation provides better predictive distributions in ranges of interest, especially in cases where a stationarity assumption for the GP model is not appropriate. It can be viewed as a goal-oriented method and becomes particularly interesting in Bayesian optimization, for example, for the minimization of an objective function, where good predictive distributions for low function values are important. When the expected improvement criterion and reGP are used for sequentially choosing evaluation points, the convergence of the resulting optimization algorithm is theoretically guaranteed (provided that the function to be optimized lies in the reproducing kernel Hilbert space attached to the known covariance of the underlying Gaussian process). Experiments indicate that using reGP instead of stationary GP models in Bayesian optimization is beneficial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.03034v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'ebastien Petit (LNE, RT-UQ), Julien Bect (L2S, RT-UQ), Emmanuel Vazquez (L2S, RT-UQ)</dc:creator>
    </item>
    <item>
      <title>Graph Neural Networks for Causal Inference Under Network Confounding</title>
      <link>https://arxiv.org/abs/2211.07823</link>
      <description>arXiv:2211.07823v4 Announce Type: replace-cross 
Abstract: This paper studies causal inference with observational data from a single large network. We consider a nonparametric model with interference in potential outcomes and selection into treatment. Both stages may be the outcomes of simultaneous equation models, which allow for endogenous peer effects. This results in high-dimensional network confounding where the network and covariates of all units constitute sources of selection bias. In contrast, the existing literature assumes that confounding can be summarized by a known, low-dimensional function of these objects. We propose to use graph neural networks (GNNs) to adjust for network confounding. When interference decays with network distance, we argue that the model has low-dimensional structure that makes estimation feasible and justifies the use of shallow GNN architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.07823v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael P. Leung, Pantelis Loupos</dc:creator>
    </item>
    <item>
      <title>Causal Structure Representation Learning of Confounders in Latent Space for Recommendation</title>
      <link>https://arxiv.org/abs/2311.03382</link>
      <description>arXiv:2311.03382v2 Announce Type: replace-cross 
Abstract: Inferring user preferences from the historical feedback of users is a valuable problem in recommender systems. Conventional approaches often rely on the assumption that user preferences in the feedback data are equivalent to the real user preferences without additional noise, which simplifies the problem modeling. However, there are various confounders during user-item interactions, such as weather and even the recommendation system itself. Therefore, neglecting the influence of confounders will result in inaccurate user preferences and suboptimal performance of the model. Furthermore, the unobservability of confounders poses a challenge in further addressing the problem. To address these issues, we refine the problem and propose a more rational solution. Specifically, we consider the influence of confounders, disentangle them from user preferences in the latent space, and employ causal graphs to model their interdependencies without specific labels. By cleverly combining local and global causal graphs, we capture the user-specificity of confounders on user preferences. We theoretically demonstrate the identifiability of the obtained causal graph. Finally, we propose our model based on Variational Autoencoders, named Causal Structure representation learning of Confounders in latent space (CSC). We conducted extensive experiments on one synthetic dataset and five real-world datasets, demonstrating the superiority of our model. Furthermore, we demonstrate that the learned causal representations of confounders are controllable, potentially offering users fine-grained control over the objectives of their recommendation lists with the learned causal graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03382v2</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hangtong Xu, Yuanbo Xu, Chaozhuo Li, Fuzhen Zhuang</dc:creator>
    </item>
    <item>
      <title>Strong Screening Rules for Group-based SLOPE Models</title>
      <link>https://arxiv.org/abs/2405.15357</link>
      <description>arXiv:2405.15357v2 Announce Type: replace-cross 
Abstract: Tuning the regularization parameter in penalized regression models is an expensive task, requiring multiple models to be fit along a path of parameters. Strong screening rules drastically reduce computational costs by lowering the dimensionality of the input prior to fitting. We develop strong screening rules for group-based Sorted L-One Penalized Estimation (SLOPE) models: Group SLOPE and Sparse-group SLOPE. The developed rules are applicable to the wider family of group-based OWL models, including OSCAR. Our experiments on both synthetic and real data show that the screening rules significantly accelerate the fitting process. The screening rules make it accessible for group SLOPE and sparse-group SLOPE to be applied to high-dimensional datasets, particularly those encountered in genetics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15357v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 28th International Conference on Artificial Intelligence and Statistics, PMLR 258:352-360, 2025</arxiv:journal_reference>
      <dc:creator>Fabio Feser, Marina Evangelou</dc:creator>
    </item>
    <item>
      <title>Method-of-Moments Inference for GLMs and Doubly Robust Functionals under Proportional Asymptotics</title>
      <link>https://arxiv.org/abs/2408.06103</link>
      <description>arXiv:2408.06103v3 Announce Type: replace-cross 
Abstract: In this paper, we consider the estimation of regression coefficients and signal-to-noise (SNR) ratio in high-dimensional Generalized Linear Models (GLMs), and explore their implications in inferring popular estimands such as average treatment effects in high-dimensional observational studies. Under the ``proportional asymptotic'' regime and Gaussian covariates with known (population) covariance $\Sigma$, we derive Consistent and Asymptotically Normal (CAN) estimators of our targets of inference through a Method-of-Moments type of estimators that bypasses estimation of high dimensional nuisance functions and hyperparameter tuning altogether. Additionally, under non-Gaussian covariates, we demonstrate universality of our results under certain additional assumptions on the regression coefficients and $\Sigma$. We also demonstrate that knowing $\Sigma$ is not essential to our proposed methodology when the sample covariance matrix estimator is invertible. Finally, we complement our theoretical results with numerical experiments and comparisons with existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06103v3</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingyu Chen, Lin Liu, Rajarshi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Nonparametric IPSS: Fast, flexible feature selection with false discovery control</title>
      <link>https://arxiv.org/abs/2410.02208</link>
      <description>arXiv:2410.02208v2 Announce Type: replace-cross 
Abstract: Feature selection is a critical task in machine learning and statistics. However, existing feature selection methods either (i) rely on parametric methods such as linear or generalized linear models, (ii) lack theoretical false discovery control, or (iii) identify few true positives. Here, we introduce a general feature selection method with finite-sample false discovery control based on applying integrated path stability selection (IPSS) to arbitrary feature importance scores. The method is nonparametric whenever the importance scores are nonparametric, and it estimates q-values, which are better suited to high-dimensional data than p-values. We focus on two special cases using importance scores from gradient boosting (IPSSGB) and random forests (IPSSRF). Extensive nonlinear simulations with RNA sequencing data show that both methods accurately control the false discovery rate and detect more true positives than existing methods. Both methods are also efficient, running in under 20 seconds when there are 500 samples and 5000 features. We apply IPSSGB and IPSSRF to detect microRNAs and genes related to cancer, finding that they yield better predictions with fewer features than existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02208v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Melikechi, David B. Dunson, Jeffrey W. Miller</dc:creator>
    </item>
  </channel>
</rss>
