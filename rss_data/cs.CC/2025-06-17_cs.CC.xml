<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jun 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>From Complexity to Simplicity: A Polynomial Time Solution To The Subset Sum Problem And A Milestone In Proving P Equal NP</title>
      <link>https://arxiv.org/abs/2506.12019</link>
      <description>arXiv:2506.12019v1 Announce Type: new 
Abstract: The Subset Sum Problem being one of the NP-complete problems, till present, has no polynomial time solution. The aim of this research is to present a way to solve all instances of the Subset Sum Problem in quadratic time worst case. With showing that the solution functions within polynomial time, it would hence prove that indeed P equals NP, theoretically that is. When it comes to the practical extension of the solution to the NP class, reductions would have to be tailored and standardized due to the diverse constraints and structures the NP problems come with. All these diverse constraints cannot be practically piped to Subset Sum form the same way. Since some NP problems have similarities in constraints, standardized reductions can be reused with little to no adaptation. The greater our pool of constraint-standard-reduction, the easier it will be to learn from them and reduce NP problems coming with similar constraints to Subset Sum, and solve them efficiently by the method to be presented. We also present a way of reducing k-SAT and general CNF-SAT instances to Subset Sum in linear time and only touch base on how to solve the SAT instances reduced to Subset Sum. As this paper focuses on efficiently solving the Subset Sum, a follow up paper will extensively detail the extension of the solution to k-SAT and CNF-SAT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12019v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thami Nkosi</dc:creator>
    </item>
    <item>
      <title>The Limits of Tractable Marginalization</title>
      <link>https://arxiv.org/abs/2506.12020</link>
      <description>arXiv:2506.12020v1 Announce Type: new 
Abstract: Marginalization -- summing a function over all assignments to a subset of its inputs -- is a fundamental computational problem with applications from probabilistic inference to formal verification. Despite its computational hardness in general, there exist many classes of functions (e.g., probabilistic models) for which marginalization remains tractable, and they can be commonly expressed by polynomial size arithmetic circuits computing multilinear polynomials. This raises the question, can all functions with polynomial time marginalization algorithms be succinctly expressed by such circuits? We give a negative answer, exhibiting simple functions with tractable marginalization yet no efficient representation by known models, assuming $\textsf{FP}\neq\#\textsf{P}$ (an assumption implied by $\textsf{P} \neq \textsf{NP}$). To this end, we identify a hierarchy of complexity classes corresponding to stronger forms of marginalization, all of which are efficiently computable on the known circuit models. We conclude with a completeness result, showing that whenever there is an efficient real RAM performing virtual evidence marginalization for a function, then there are small circuits for that function's multilinear representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12020v1</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver Broadrick, Sanyam Agarwal, Guy Van den Broeck, Markus Bl\"aser</dc:creator>
    </item>
    <item>
      <title>Monitoring graph edges via shortest paths: computational complexity and approximation algorithms</title>
      <link>https://arxiv.org/abs/2506.12021</link>
      <description>arXiv:2506.12021v1 Announce Type: new 
Abstract: Edge-Geodetic Sets play a crucial role in network monitoring and optimization, wherein the goal is to strategically place monitoring stations on vertices of a network, represented as a graph, to ensure complete coverage of edges and mitigate faults by monitoring lines of communication. This paper illustrates and explores the Monitoring Edge-Geodetic Set (MEG-set) problem, which involves determining the minimum set of vertices that need to be monitored to achieve geodetic coverage for a given network. The significance of this problem lies in its potential to facilitate efficient network monitoring, enhancing the overall reliability and performance of various applications. In this work, we prove the $\mathcal{NP}$-completeness of the MEG-set optimization problem by showing a reduction from the well-known Vertex Cover problem. Furthermore, we present inapproximability results, proving that the MEG-set optimization problem is $\mathcal{APX}$-Hard and that, if the unique games conjecture holds, the problem is not approximable within a factor of $2-\epsilon$ for any constant $\epsilon &gt; 0$. Despite its $\mathcal{NP}$-hardness, we propose an efficient approximation algorithm achieving an approximation ratio of $O(\sqrt{|V(G)| \cdot \ln{|V(G)|})}$ for the MEG-set optimization problem, based on the well-known Set Cover approximation algorithm, where $|V(G)|$ is the number of nodes of the MEG-set instance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12021v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giordano Colli</dc:creator>
    </item>
    <item>
      <title>Sign-Rank of $k$-Hamming Distance is Constant</title>
      <link>https://arxiv.org/abs/2506.12022</link>
      <description>arXiv:2506.12022v1 Announce Type: new 
Abstract: We prove that the sign-rank of the $k$-Hamming Distance matrix on $n$ bits is $2^{O(k)}$, independent of the number of bits $n$. This strongly refutes the conjecture of Hatami, Hatami, Pires, Tao, and Zhao (RANDOM 2022), and Hatami, Hosseini, and Meng (STOC 2023), repeated in several other papers, that the sign-rank should depend on $n$. This conjecture would have qualitatively separated margin from sign-rank (or, equivalently, bounded-error from unbounded-error randomized communication). In fact, our technique gives constant sign-rank upper bounds for all matrices which reduce to $k$-Hamming Distance, as well as large-margin matrices recently shown to be irreducible to $k$-Hamming Distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12022v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mika G\"o\"os, Nathaniel Harms, Valentin Imbach, Dmitry Sokolov</dc:creator>
    </item>
    <item>
      <title>Constant Bit-size Transformers Are Turing Complete</title>
      <link>https://arxiv.org/abs/2506.12027</link>
      <description>arXiv:2506.12027v1 Announce Type: new 
Abstract: We prove that any Turing machine running on inputs of arbitrary length can be simulated by a constant bit-size transformer, as long as the context window is sufficiently long. This improves previous works, which require scaling up either the model's precision or the number of parameters on longer inputs. Furthermore, we prove that the complexity class SPACE$[s(n)]$ exactly characterizes the expressive power of a constant bit-size transformer with a context window of length $s(n)$. Our approach relies on simulating Post machines, a Turing-complete computational model. Post machines can be modeled as automata equipped with a queue, exhibiting computational behaviors naturally aligned with those of transformers. The behavioral similarity between transformers and Post machines may offer new insights into the mechanisms underlying the reasoning abilities of transformers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12027v1</guid>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qian Li, Yuyi Wang</dc:creator>
    </item>
    <item>
      <title>Arithmetic Circuits with Division</title>
      <link>https://arxiv.org/abs/2506.12246</link>
      <description>arXiv:2506.12246v1 Announce Type: new 
Abstract: We study the computational complexity of the membership problem for arithmetic circuits over natural numbers with division. We consider different subsets of the operations {intersection,union,complement,+,x,/}, where / is the element-wise integer division (without remainder and without rounding). Results for the subsets without division have been studied before, in particular by McKenzie and Wagner and Yang. The division is expressive because it makes it possible to describe the set of factors of a given number as a circuit. Surprisingly, the cases {intersection,union,complement,+,/} and {intersection,union,complement,x,/} are PSPACE-complete and therefore equivalent to the corresponding cases without division. The case {union,/} is NP-hard in contrast to the case {union} which is NL-complete. Further upper bounds, lower bounds and completeness results are given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12246v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Silas Cato Sacher</dc:creator>
    </item>
    <item>
      <title>A Compendium of Subset Search Problems and Reductions relating to the Parsimonious Property</title>
      <link>https://arxiv.org/abs/2506.12255</link>
      <description>arXiv:2506.12255v1 Announce Type: new 
Abstract: This thesis centers around the concept of Subset Search Problems (SSP), a type of computational problem introduced by Gr\"une and Wulf to analyze the complexity of more intricate optimization problems. These problems are given an input set, a so-called universe, and their solution lies within their own universe, e.g. the shortest path between two point is a subset of all possible paths. Due to this, reductions upholding the SSP property require an injective embedding from the universe of the first problem into that of the second. This, however, appears inherently similar to the concept of a Parsimonious reduction, a reduction type requiring a bijective function between the solution spaces of the two problems. Parsimonious reductions are mainly used within the complexity class #P, as this class of problems concerns itself with the number of possible solutions in a given problem. These two concepts, SSP and Parsimonious reductions, are inherently similar but, crucially, not equivalent. We therefore explore the interplay between reductions upholding the SSP and Parsimonious properties, highlighting both the similarities and differences by providing a comprehensive theorem delineating the properties required for reductions to uphold both attributes. We also compile and evaluate 46 reductions between 30 subset search variants of computational problems, including those of classic NP-complete problems such as Satisfiability, Vertex Cover, Hamiltonian Cycle, the Traveling Salesman Problem and Subset Sum, providing reduction proofs, illustrative examples and insights as to where the SSP and Parsimonious properties coexist or diverge. With this compendium we contribute to the understanding of the computational complexity of bilevel and robust optimization problems, by contributing a vast collection of proven SSP- and #P-complete problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12255v1</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Celina Janet Bartlett</dc:creator>
    </item>
    <item>
      <title>Deterministic Lifting Theorems for One-Way Number-on-Forehead Communication</title>
      <link>https://arxiv.org/abs/2506.12420</link>
      <description>arXiv:2506.12420v1 Announce Type: new 
Abstract: Lifting theorems are one of the most powerful tools for proving communication lower bounds, with numerous downstream applications in proof complexity, monotone circuit lower bounds, data structures, and combinatorial optimization. However, to the best of our knowledge, prior lifting theorems have primarily focused on the two-party communication.
  In this paper, we propose a new lifting theorem that establishes connections between two-party communication and the Number-on-Forehead (NOF) communication model. Specifically, we present a deterministic lifting theorem that translates one-way two-party communication lower bounds into one-way NOF lower bounds.
  Our lifting theorem yields two applications. First, we obtain an optimal explicit separation between randomized and deterministic one-way NOF communication, even in the multi-player setting. This improves the prior square-root vs. constant separation for three players established by Kelley and Lyu (arXiv 2025). Second, we achieve optimal separations between one-round and two-round deterministic NOF communication, improving upon the previous separation of $\Omega(\frac{n^{1/(k-1)}}{k^k})$ vs. $O(\log n)$ for $k$ players, as shown by Viola and Wigderson (FOCS 2007).
  Beyond the lifting theorems, we also apply our techniques to the disjointness problem. In particular, we provide a new proof that the deterministic one-way three-party NOF communication complexity of set disjointness is $\Omega(n)$, further demonstrating the broader applicability of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12420v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangxu Yang, Jiapeng Zhang</dc:creator>
    </item>
    <item>
      <title>Polynomial Prenexing of QBFs with Non-Monotone Boolean Operators</title>
      <link>https://arxiv.org/abs/2506.12562</link>
      <description>arXiv:2506.12562v1 Announce Type: new 
Abstract: It is well-known that every quantified boolean formula (QBF) can be transformed into a prenex QBF whose only boolean operators are negation, conjunction, and disjunction. It is also well-known that the transformation is polynomial if the boolean operators of the original QBF are restricted to negation, conjunction, and disjunction. In contrast, up to now no polynomial transformation has been found when the original QBF contains other boolean operators such as biconditionals or exclusive disjunction. We define such a transformation and show that it is polynomial and preserves quantifier depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12562v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdallah Saffidine, Andreas Herzig</dc:creator>
    </item>
    <item>
      <title>Leakage-Resilient Extractors against Number-on-Forehead Protocols</title>
      <link>https://arxiv.org/abs/2506.12595</link>
      <description>arXiv:2506.12595v1 Announce Type: new 
Abstract: Given a sequence of $N$ independent sources $\mathbf{X}_1,\mathbf{X}_2,\dots,\mathbf{X}_N\sim\{0,1\}^n$, how many of them must be good (i.e., contain some min-entropy) in order to extract a uniformly random string? This question was first raised by Chattopadhyay, Goodman, Goyal and Li (STOC '20), motivated by applications in cryptography, distributed computing, and the unreliable nature of real-world sources of randomness. In their paper, they showed how to construct explicit low-error extractors for just $K \geq N^{1/2}$ good sources of polylogarithmic min-entropy. In a follow-up, Chattopadhyay and Goodman improved the number of good sources required to just $K \geq N^{0.01}$ (FOCS '21). In this paper, we finally achieve $K=3$.
  Our key ingredient is a near-optimal explicit construction of a new pseudorandom primitive, called a leakage-resilient extractor (LRE) against number-on-forehead (NOF) protocols. Our LRE can be viewed as a significantly more robust version of Li's low-error three-source extractor (FOCS '15), and resolves an open question put forth by Kumar, Meka, and Sahai (FOCS '19) and Chattopadhyay, Goodman, Goyal, Kumar, Li, Meka, and Zuckerman (FOCS '20). Our LRE construction is based on a simple new connection we discover between multiparty communication complexity and non-malleable extractors, which shows that such extractors exhibit strong average-case lower bounds against NOF protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12595v1</guid>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eshan Chattopadhyay, Jesse Goodman</dc:creator>
    </item>
    <item>
      <title>The Word Problem for Products of Symmetric Groups</title>
      <link>https://arxiv.org/abs/2506.13655</link>
      <description>arXiv:2506.13655v1 Announce Type: new 
Abstract: The word problem for products of symmetric groups (WPPSG) is a well-known NP-complete problem. An input instance of this problem consists of ``specification sets'' $X_1,\ldots,X_m \seq \{1,\ldots,n\}$ and a permutation $\tau$ on $\{1,\ldots,n\}$. The sets $X_1,\ldots,X_m$ specify a subset of the symmetric group $\cS_n$ and the question is whether the given permutation $\tau$ is a member of this subset. We discuss three subproblems of WPPSG and show that they can be solved efficiently. The subproblem WPPSG$_0$ is the restriction of WPPSG to specification sets all of which are sets of consecutive integers. The subproblem WPPSG$_1$ is the restriction of WPPSG to specification sets which have the Consecutive Ones Property. The subproblem WPPSG$_2$ is the restriction of WPPSG to specification sets which have what we call the Weak Consecutive Ones Property. WPPSG$_1$ is more general than WPPSG$_0$ and WPPSG$_2$ is more general than WPPSG$_1$. But the efficient algorithms that we use for solving WPPSG$_1$ and WPPSG$_2$ have, as a sub-routine, the efficient algorithm for solving WPPSG$_0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13655v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hans U. Simon</dc:creator>
    </item>
    <item>
      <title>Optimized Amplitude Amplification for Quantum State Preparation</title>
      <link>https://arxiv.org/abs/2506.12017</link>
      <description>arXiv:2506.12017v1 Announce Type: cross 
Abstract: In this paper, we present an algorithm for preparing quantum states of the form $\sum_{i=0}^{n-1} \alpha_i |i\rangle$, where the coefficients $\alpha_i$ are specified by a quantum oracle. Our method achieves this task twice as fast as the best existing algorithm known to the authors. Such state preparation is essential for quantum algorithms that process large classical inputs, including matrix inversion and linear system solvers. The standard approach relies on amplitude amplification, a process that may require multiple, time-consuming oracle queries. Consequently, reducing the number of queries - and thereby the overall time complexity can lead to significant performance improvements in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12017v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Artem Chernikov, Karina Zakharova, Sergey Sysoev</dc:creator>
    </item>
    <item>
      <title>Avoiding Obfuscation with Prover-Estimator Debate</title>
      <link>https://arxiv.org/abs/2506.13609</link>
      <description>arXiv:2506.13609v1 Announce Type: cross 
Abstract: Training powerful AI systems to exhibit desired behaviors hinges on the ability to provide accurate human supervision on increasingly complex tasks. A promising approach to this problem is to amplify human judgement by leveraging the power of two competing AIs in a debate about the correct solution to a given problem. Prior theoretical work has provided a complexity-theoretic formalization of AI debate, and posed the problem of designing protocols for AI debate that guarantee the correctness of human judgements for as complex a class of problems as possible. Recursive debates, in which debaters decompose a complex problem into simpler subproblems, hold promise for growing the class of problems that can be accurately judged in a debate. However, existing protocols for recursive debate run into the obfuscated arguments problem: a dishonest debater can use a computationally efficient strategy that forces an honest opponent to solve a computationally intractable problem to win. We mitigate this problem with a new recursive debate protocol that, under certain stability assumptions, ensures that an honest debater can win with a strategy requiring computational efficiency comparable to their opponent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13609v1</guid>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonah Brown-Cohen, Geoffrey Irving, Georgios Piliouras</dc:creator>
    </item>
    <item>
      <title>Hardness of monadic second-order formulae over succinct graphs</title>
      <link>https://arxiv.org/abs/2302.04522</link>
      <description>arXiv:2302.04522v3 Announce Type: replace 
Abstract: Our main result is a succinct counterpoint to Courcelle's meta-theorem as follows: every cw-nontrivial monadic second-order (MSO) property is either NP-hard or coNP-hard over graphs given by succinct representations. Succint representations are Boolean circuits computing the adjacency relation. Cw-nontrivial properties are those which have infinitely many models and infinitely many countermodels with bounded cliquewidth. Moreover, we explore what happens when the cw-nontriviality condition is dropped and show that, under a reasonable complexity assumption, the previous dichotomy fails, even for questions expressible in first-order logic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.04522v3</guid>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guilhem Gamard (MOCQUA), Ali\'enor Goubault-Larrecq (LIS), Pierre Guillon (I2M), Pierre Ohlmann (LIS), K\'evin Perrot (LIS), Guillaume Theyssier (I2M)</dc:creator>
    </item>
    <item>
      <title>Delta-modular ILP Problems of Bounded Codimension, Discrepancy, and Convolution (new version)</title>
      <link>https://arxiv.org/abs/2405.17001</link>
      <description>arXiv:2405.17001v5 Announce Type: replace 
Abstract: For integers $k,n \geq 0$ and a cost vector $c \in Z^n$, we study two fundamental integer linear programming (ILP) problems: \[
  \text{(Standard Form)} \quad \max\bigl\{c^\top x \colon Ax = b,\ x \in Z^n_{\geq 0}\bigr\} \text{ with } A \in Z^{k \times n}, \text{rank}(A) = k, b \in Z^k, \] \[
  \text{(Canonical Form)} \quad \max\bigl\{c^\top x \colon Ax \leq b,\ x \in Z^n\bigr\} \text{ with } A \in Z^{(n+k) \times n}, \text{rank}(A) = n, b \in Z^{n+k}. \] We present improved algorithms for both problems and their feasibility versions, parameterized by $k$ and $\Delta$, where $\Delta$ denotes the maximum absolute value of $\text{rank}(A) \times \text{rank}(A)$ subdeterminants of $A$. Our main complexity results, stated in terms of required arithmetic operations, are: \[ \text{Optimization:}\quad O(\log k)^{2k} \cdot \Delta^2 / 2^{\Omega(\sqrt{\log \Delta})} + 2^{O(k)} \cdot \text{poly}(\varphi), \] \[ \text{Feasibility:} \quad O(\log k)^k \cdot \Delta \cdot (\log \Delta)^3 + 2^{O(k)} \cdot \text{poly}(\varphi), \] where $\varphi$ represents the input size measured by the bit-encoding length of $(A,b,c)$. We also examine several special cases when $k \in \{0,1\}$, which have important applications in: expected computational complexity of ILP with varying right-hand side $b$, ILP problems with generic constraint matrices, ILP problems on simplices. Our results yield improved complexity bounds for these specific scenarios.
  As independent contributions, we present: An $n^2/2^{\Omega(\sqrt{\log n})}$-time algorithm for the tropical convolution problem on sequences indexed by elements of a finite Abelian group of order $n$; A complete and self-contained error analysis of the generalized DFT over Abelian groups in the Word-RAM model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17001v5</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.AC</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Cherniavskii, D. Gribanov, D. Malyshev, P. M. Pardalos</dc:creator>
    </item>
    <item>
      <title>Complexity of Injectivity and Verification of ReLU Neural Networks</title>
      <link>https://arxiv.org/abs/2405.19805</link>
      <description>arXiv:2405.19805v2 Announce Type: replace 
Abstract: Neural networks with ReLU activation play a key role in modern machine learning. Understanding the functions represented by ReLU networks is a major topic in current research as this enables a better interpretability of learning processes. Injectivity of a function computed by a ReLU network, that is, the question if different inputs to the network always lead to different outputs, plays a crucial role whenever invertibility of the function is required, such as, e.g., for inverse problems or generative models. The exact computational complexity of deciding injectivity was recently posed as an open problem (Puthawala et al. [JMLR 2022]). We answer this question by proving coNP-completeness. On the positive side, we show that the problem for a single ReLU-layer is still tractable for small input dimension; more precisely, we present a parameterized algorithm which yields fixed-parameter tractability with respect to the input dimension. In addition, we study the network verification problem which is to verify that certain inputs only yield specific outputs. This is of great importance since neural networks are increasingly used in safety-critical systems. We prove that network verification is coNP-hard for a general class of input domains. Our results also exclude constant-factor polynomial-time approximations for the maximum of a function computed by a ReLU network. In this context, we also characterize surjectivity of functions computed by ReLU networks with one-dimensional output which turns out to be the complement of a basic network verification task. We reveal interesting connections to computational convexity by formulating the surjectivity problem as a zonotope containment problem</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19805v2</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Froese, Moritz Grillo, Martin Skutella</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Hazard-Free Formulas</title>
      <link>https://arxiv.org/abs/2411.09026</link>
      <description>arXiv:2411.09026v3 Announce Type: replace 
Abstract: This paper studies the hazard-free formula complexity of Boolean functions.
  Our first result shows that unate functions are the only Boolean functions for which the monotone formula complexity of the hazard-derivative equals the hazard-free formula complexity of the function itself. Consequently, they are the only functions for which the hazard-derivative approach of Ikenmeyer et al. (J. ACM, 2019) yields optimal bounds.
  Our second result proves that the hazard-free formula complexity of random Boolean functions is at most $2^{(1+o(1))n}$. Prior to this, no better upper bound than $O(3^n)$ was known. Notably, unlike in the general case of Boolean circuits and formulas, where the typical complexity is derived from that of the multiplexer function with $n$-bit selector, the hazard-free formula complexity of a random function is smaller than the optimal hazard-free formula for the multiplexer by an exponential factor in $n$.
  We provide two proofs of this fact. The first is direct, bounding the number of prime implicants of a random Boolean function and using this bound to construct a DNF of the claimed size. The second introduces a new and independently interesting result: a weak converse to the hazard-derivative lower bound method, which gives an upper bound on the hazard-free complexity of a function in terms of the monotone complexity of a subset of its hazard-derivatives.
  Additionally, we explore the hazard-free formula complexity of block composition of Boolean functions and obtain a result in the hazard-free setting that is analogous to a result of Karchmer, Raz, and Wigderson (Computational Complexity, 1995) in the monotone setting. We show that our result implies a stronger lower bound on the hazard-free formula depth of the block composition of the set covering function with the multiplexer function than the bound obtained via the hazard-derivative method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09026v3</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leah London Arazi, Amir Shpilka</dc:creator>
    </item>
    <item>
      <title>Strong XOR Lemma for Information Complexity</title>
      <link>https://arxiv.org/abs/2411.13015</link>
      <description>arXiv:2411.13015v2 Announce Type: replace 
Abstract: For any $\{0,1\}$-valued function $f$, its \emph{$n$-folded XOR} is the function $f^{\oplus n}$ where $f^{\oplus n}(X_1, \ldots, X_n) = f(X_1) \oplus \cdots \oplus f(X_n)$. Given a procedure for computing the function $f$, one can apply a ``naive" approach to compute $f^{\oplus n}$ by computing each $f(X_i)$ independently, followed by XORing the outputs. This approach uses $n$ times the resources required for computing $f$.
  In this paper, we prove a strong XOR lemma for \emph{information complexity} in the two-player randomized communication model: if computing $f$ with an error probability of $O(n^{-1})$ requires revealing $I$ bits of information about the players' inputs, then computing $f^{\oplus n}$ with a constant error requires revealing $\Omega(n) \cdot (I - 1 - o_n(1))$ bits of information about the players' inputs. Our result demonstrates that the naive protocol for computing $f^{\oplus n}$ is both information-theoretically optimal and asymptotically tight in error trade-offs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13015v2</guid>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pachara Sawettamalya, Huacheng Yu</dc:creator>
    </item>
    <item>
      <title>Deterministic factorization of constant-depth algebraic circuits in subexponential time</title>
      <link>https://arxiv.org/abs/2504.08063</link>
      <description>arXiv:2504.08063v2 Announce Type: replace 
Abstract: While efficient randomized algorithms for factorization of polynomials given by algebraic circuits have been known for decades, obtaining an even slightly non-trivial deterministic algorithm for this problem has remained an open question of great interest. This is true even when the input algebraic circuit has additional structure, for instance, when it is a constant-depth circuit. Indeed, no efficient deterministic algorithms are known even for the seemingly easier problem of factoring sparse polynomials or even the problem of testing the irreducibility of sparse polynomials.
  In this work, we make progress on these questions: we design a deterministic algorithm that runs in subexponential time, and when given as input a constant-depth algebraic circuit $C$ over the field of rational numbers, it outputs algebraic circuits (of potentially unbounded depth) for all the irreducible factors of $C$, together with their multiplicities. In particular, we give the first subexponential time deterministic algorithm for factoring sparse polynomials.
  For our proofs, we rely on a finer understanding of the structure of power series roots of constant-depth circuits and the analysis of the Kabanets-Impagliazzo generator. In particular, we show that the Kabanets-Impagliazzo generator constructed using low-degree hard polynomials (explicitly constructed in the work of Limaye, Srinivasan &amp; Tavenas) preserves not only the non-zeroness of small constant-depth circuits (as shown by Chou, Kumar &amp; Solomon), but also their irreducibility and the irreducibility of their factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08063v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Somnath Bhattacharjee, Mrinal Kumar, Varun Ramanathan, Ramprasad Saptharishi, Shubhangi Saraf</dc:creator>
    </item>
    <item>
      <title>Improved Quantum Query Upper Bounds Based on Classical Decision Trees</title>
      <link>https://arxiv.org/abs/2203.02968</link>
      <description>arXiv:2203.02968v3 Announce Type: replace-cross 
Abstract: Given a classical query algorithm as a decision tree, when does there exist a quantum query algorithm with a speed-up over the classical one? We provide a general construction based on the structure of the underlying decision tree, and prove that this can give us an up-to-quadratic quantum speed-up. In particular, we obtain a bounded-error quantum query algorithm of cost $O(\sqrt{s})$ to compute a Boolean function (more generally, a relation) that can be computed by a classical (even randomized) decision tree of size $s$.
  Lin and Lin [ToC'16] and Beigi and Taghavi [Quantum'20] showed results of a similar flavor, and gave upper bounds in terms of a quantity which we call the "guessing complexity" of a decision tree. We identify that the guessing complexity of a decision tree equals its rank, a notion introduced by Ehrenfeucht and Haussler [Inf. Comp.'89] in the context of learning theory. This answers a question posed by Lin and Lin, who asked whether the guessing complexity of a decision tree is related to any complexity-theoretic measure. We also show a polynomial separation between rank and randomized rank for the complete binary AND-OR tree.
  Beigi and Taghavi constructed span programs and dual adversary solutions for Boolean functions given classical decision trees computing them and an assignment of non-negative weights to its edges. We explore the effect of changing these weights on the resulting span program complexity and objective value of the dual adversary bound, and capture the best possible weighting scheme by an optimization program. We exhibit a solution to this program and argue its optimality from first principles. We also exhibit decision trees for which our bounds are asymptotically stronger than those of Lin and Lin, and Beigi and Taghavi. This answers a question of Beigi and Taghavi, who asked whether different weighting schemes could yield better upper bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.02968v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.FSTTCS.2022.15</arxiv:DOI>
      <dc:creator>Arjan Cornelissen, Nikhil S. Mande, Subhasree Patro</dc:creator>
    </item>
    <item>
      <title>Computational Equivalence of Spiked Covariance and Spiked Wigner Models via Gram-Schmidt Perturbation</title>
      <link>https://arxiv.org/abs/2503.02802</link>
      <description>arXiv:2503.02802v2 Announce Type: replace-cross 
Abstract: In this work, we show the first average-case reduction transforming the sparse Spiked Covariance Model into the sparse Spiked Wigner Model and as a consequence obtain the first computational equivalence result between two well-studied high-dimensional statistics models. Our approach leverages a new perturbation equivariance property for Gram-Schmidt orthogonalization, enabling removal of dependence in the noise while preserving the signal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02802v2</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Bresler, Alina Harbuzova</dc:creator>
    </item>
    <item>
      <title>On the Equivalence of Gaussian Graphical Models Defined on Complete Bipartite Graphs</title>
      <link>https://arxiv.org/abs/2505.02384</link>
      <description>arXiv:2505.02384v2 Announce Type: replace-cross 
Abstract: This paper introduces two Gaussian graphical models defined on complete bipartite graphs. We show that the determinants of the precision matrices associated with the models are equal up to scale, where the scale factor only depends on model parameters. In this context, we will introduce a notion of ``equivalence" between the two Gaussian graphical models. This equivalence has two key applications: first, it can significantly reduce the complexity of computing the exact value of the determinant, and second, it enables the derivation of closed-form expressions for the determinants in certain special cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02384v2</guid>
      <category>cs.IT</category>
      <category>cs.CC</category>
      <category>math.IT</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mehdi Molkaraie</dc:creator>
    </item>
  </channel>
</rss>
