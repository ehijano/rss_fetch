<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Jul 2024 01:41:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Determination of emissivity profiles using a Bayesian data-driven approach</title>
      <link>https://arxiv.org/abs/2407.07113</link>
      <description>arXiv:2407.07113v1 Announce Type: new 
Abstract: In this paper, we explore the determination of a spectral emissivity profile that closely matches real data, intended for use as an initial guess and/or a-priori information in a retrieval code. Our approach employs a Bayesian method that integrates the CAMEL (Combined ASTER MODIS Emissivity over Land) emissivity database with a land cover map. The solution is derived as a convex combination of high-resolution Huang profiles using the Bayesian framework. We test our method on IASI (Infrared Atmospheric Sounding Interferometer) data and find that it outperforms the linear spline interpolation of the CAMEL data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07113v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Sgheri, Cristina Sgattoni, Chiara Zugarini</dc:creator>
    </item>
    <item>
      <title>Research on dynamic analysis and prediction model of tennis match based on Bayesian probability and analytic Hierarchy process</title>
      <link>https://arxiv.org/abs/2407.07116</link>
      <description>arXiv:2407.07116v1 Announce Type: new 
Abstract: In the 2023 Wimbledon Gentlemen's final, Carlos Alcaraz defeated Novak Djokovic. This study aims to predict athletes' performance through five key aspects: first, a mul-ti-classification model based on logistic regression was established, yielding probabilities of winning and losing the first serve at 0.6734 and 0.3266, respectively, and validated with match 1701. Second, an unsupervised "momentum" evaluation model using AHP analytic hierarchy process showed a strong correlation between "momentum" score and winning rate. Third, a trend analysis model identified psychological factors as significantly impacting re-sults. Fourth, the model's generalization was tested with additional competition data, in-cluding women's tennis matches. Finally, data analysis suggested that coaches should focus on improving mental toughness and serve-receive skills, as these significantly affect mo-mentum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07116v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuangqi Li</dc:creator>
    </item>
    <item>
      <title>An Analysis of Pacing Profiles in Sprint Kayak Racing Using Functional Principal Components and Hidden Markov Models</title>
      <link>https://arxiv.org/abs/2407.07120</link>
      <description>arXiv:2407.07120v1 Announce Type: new 
Abstract: This study analysed sprint kayak pacing profiles in order to categorise and compare an athlete's race profile throughout their career. We used functional principal component analysis of normalised velocity data for 500m and 1000m races to quantify pacing. The first four principal components explained 90.77% of the variation over 500m and 78.80% over 1000m. These principal components were then associated with unique pacing characteristics with the first component defined as a dropoff in velocity and the second component defined as a kick. We then applied a Hidden Markov model to categorise each profile over an athlete's career, using the PC scores, into different types of race profiles. This model included age and event type and we identified a trend for a higher dropoff in development pathway athletes. Using the four different race profile types, four athletes had all their race profiles throughout their careers analysed. It was identified that an athlete's pacing profile can and does change throughout their career as an athlete matures. This information provides coaches, practitioners and athletes with expectations as to how pacing profiles can be expected to change across the course of an athlete's career.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07120v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harry Estreich, Nicola Bullock, Mark Osborne, Edgar Santos-Fernandez, Paul Pao-Yen Wu</dc:creator>
    </item>
    <item>
      <title>Differential Equations and Applications to COVID-19</title>
      <link>https://arxiv.org/abs/2407.07123</link>
      <description>arXiv:2407.07123v1 Announce Type: new 
Abstract: This paper focuses on the application of the Verhulst logistic equation to model in retrospective the total COVID-19 cases in Senegal during the period from April 2022 to April 2023. Our predictions for April 2023 are compared with the real COVID-19 data for April 2023 to assess the accuracy of the model. The data analysis is conducted using Python programming language, which allows for efficient data processing and prediction generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07123v1</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mitonsou Tierry Hounkonnou, Laure Gouba</dc:creator>
    </item>
    <item>
      <title>Calibrating satellite maps with field data for improved predictions of forest biomass</title>
      <link>https://arxiv.org/abs/2407.07134</link>
      <description>arXiv:2407.07134v1 Announce Type: new 
Abstract: Spatially explicit quantification of forest biomass is important for forest-health monitoring and carbon accounting. Direct field measurements of biomass are laborious and expensive, typically limiting their spatial and temporal sampling density and therefore the precision and resolution of the resulting inference. Satellites can provide biomass predictions at a far greater density, but these predictions are often biased relative to field measurements and exhibit heterogeneous errors. We developed and implemented a coregionalization model between sparse field measurements and a predictive satellite map to deliver improved predictions of biomass density at a 1-by-1 km resolution throughout the Pacific states of California, Oregon and Washington. The model accounts for zero-inflation in the field measurements and the heterogeneous errors in the satellite predictions. A stochastic partial differential equation approach to spatial modeling is applied to handle the magnitude of the satellite data. The spatial detail rendered by the model is much finer than would be possible with the field measurements alone, and the model provides substantial noise-filtering and bias-correction to the satellite map.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07134v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul B. May, Andrew O. Finley</dc:creator>
    </item>
    <item>
      <title>Question-Score Identity Detection (Q-SID): A Statistical Algorithm to Detect Collusion Groups with Error Quantification from Exam Question Scores</title>
      <link>https://arxiv.org/abs/2407.07420</link>
      <description>arXiv:2407.07420v1 Announce Type: new 
Abstract: Collusion between students in online exams is a major problem that undermines the integrity of the exam results. Although there exist methods that use exam data to identify pairs of students who have likely copied each other's answers, these methods are restricted to specific formats of multiple-choice exams. Here we present a statistical algorithm, Q-SID, that efficiently detects groups of students who likely have colluded, i.e., collusion groups, with error quantification. Q-SID uses graded numeric question scores only, so it works for many formats of multiple-choice and non-multiple-choice exams. Q-SID reports two false-positive rates (FPRs) for each collusion group: (1) empirical FPR, whose null data are from 36 strictly proctored exam datasets independent of the user-input exam data and (2) synthetic FPR, whose null data are simulated from a copula-based probabilistic model, which is first fitted to the user-input exam data and then modified to have no collusion. On 34 unproctored exam datasets, including two benchmark datasets with true positives and negatives verified by textural analysis, we demonstrate that Q-SID is a collusion detection algorithm with powerful and robust performance across exam formats, numbers of questions and students, and exam complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07420v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guanao Yan, Jingyi Jessica Li, Mark D. Biggin</dc:creator>
    </item>
    <item>
      <title>Spectral Bridges</title>
      <link>https://arxiv.org/abs/2407.07430</link>
      <description>arXiv:2407.07430v1 Announce Type: new 
Abstract: In this paper, Spectral Bridges, a novel clustering algorithm, is introduced. This algorithm builds upon the traditional k-means and spectral clustering frameworks by subdividing data into small Vorono\"i regions, which are subsequently merged according to a connectivity measure. Drawing inspiration from Support Vector Machine's margin concept, a non-parametric clustering approach is proposed, building an affinity margin between each pair of Vorono\"i regions. This approach is characterized by minimal hyperparameters and delineation of intricate, non-convex cluster structures.
  The numerical experiments underscore Spectral Bridges as a fast, robust, and versatile tool for sophisticated clustering tasks spanning diverse domains. Its efficacy extends to large-scale scenarios encompassing both real-world and synthetic datasets.
  The Spectral Bridge algorithm is implemented both in Python (&lt;https://pypi.org/project/spectral-bridges&gt;) and R &lt;https://github.com/cambroise/spectral-bridges-Rpackage&gt;).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07430v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F\'elix Laplante, Christophe Ambroise</dc:creator>
    </item>
    <item>
      <title>Relative Sensitivities and Correlation of Factors Introducing Uncertainty in Radiotherapy Dosimetry Audits</title>
      <link>https://arxiv.org/abs/2407.07714</link>
      <description>arXiv:2407.07714v1 Announce Type: new 
Abstract: Dosimetry audits are carried out to determine how well radiotherapy is delivered to the patient. It is also used to understand the uncertainty introduced into the measurement result when using different computational models. As measurement procedures are becoming increasingly complex with technological advancements, it is harder to establish sources of variability in measurements and understand if they stem from true differences in measurands or in the measurement pipelines themselves. The gamma index calculation is a widely accepted metric used for the comparison of measured and predicted doses in radiotherapy. However, various steps in the measurement pipeline can introduce variation in the measurement result. In this paper, we perform a sensitivity and correlation analysis to investigate the influence of various input factors (i.e. setting) in gamma index calculations on the uncertainty introduced in dosimetry audits. We identify a number of factors where standardization will improve measurements by reducing variability in outputs. Furthermore, we also compare gamma index metrics and similarities across audit sites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07714v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Padmini Krishnadas, Spencer Angus Thomas, Jessica Goldring, Nadia A. S. Smith, Mohammad Hussein</dc:creator>
    </item>
    <item>
      <title>Another reason why normalized gain should continue to be used to analyze concept inventories (and estimate learning rates)</title>
      <link>https://arxiv.org/abs/2407.07730</link>
      <description>arXiv:2407.07730v1 Announce Type: new 
Abstract: A transformation called normalized gain (ngain) has been acknowledged as one of the most common measures of knowledge growth in pretest-posttest contexts in physics education research. Recent studies in math education have shown that ngains can also be applied to assess learners' ability to acquire unfamiliar knowledge, that is, to estimate their "learning rate". This quantity is estimated from learning data through two well-known methods: computing the average ngain of the group or computing the ngain of the average learner. These two methods commonly yield different results, and prior research has concluded that the difference between them is associated with a pretest-ngains correlation. Such a correlation would suggest a bias of this learning measurement because it implies its favoring of certain subgroups of students according to their performance in pretest measurements. The present study analyzes these two estimation methods by drawing on statistical models. Our results show that the two estimation methods are equivalent when no measurement errors exist. In contrast, when there are measurement errors, the first method provides a biased estimator, whereas the second one provides an unbiased estimator. Furthermore, these measurement errors induce a spurious correlation between the pretest and ngain scores. Our results seem consistent with prior research, except they show that measurement errors in pretest and posttest scores are the source of a spurious pretest-ngain correlation. Consequently, estimating learning rates might effectively provide unbiased estimates of knowledge change that control for the effect of prior knowledge even in the presence of pretest-ngain correlations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07730v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jairo Navarrete, Valentina Giaconi, Gonzalo Contador, Mariano Vazquez</dc:creator>
    </item>
    <item>
      <title>Fast Revenue Maximization</title>
      <link>https://arxiv.org/abs/2407.07316</link>
      <description>arXiv:2407.07316v1 Announce Type: cross 
Abstract: We study a data-driven problem pricing problem in which a seller offers a price for a single item based on demand observed at a small finite number of historical prices. Our goal is to derive precise evaluation procedures of the value of the historical information gathered by the seller, along with prescriptions for more efficient price experimentation. Our main methodological result is an exact characterization of the maximin ratio (defined as the worst-case revenue garnered by a seller who only relies on past data divided by the optimal revenue achievable with full knowledge of the distribution of values). This result allows to measure the value of any historical data consisting of prices and corresponding conversion rates. We leverage this central reduction to provide new insights about price experimentation. Motivated by practical constraints that impede the seller from changing prices abruptly, we first illustrate our framework by evaluating the value of local information and show that the mere sign of the gradient may sometimes provide significant information to the seller. We then showcase how our framework can be used to run efficient price experiments. On the one hand, we develop a method to select the next price experiment that the seller should use to maximize the information obtained. On the other hand, we demonstrate that our result allows to considerably reduce the price experimentation needed to reach preset revenue guarantees through dynamic pricing algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07316v1</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Achraf Bahamou, Omar Besbes, Omar Mouchtaki</dc:creator>
    </item>
    <item>
      <title>Bayesian weighted time-lapse full-waveform inversion using a receiver-extension strategy</title>
      <link>https://arxiv.org/abs/2407.07467</link>
      <description>arXiv:2407.07467v1 Announce Type: cross 
Abstract: Time-lapse full-waveform inversion (FWI) has become a powerful tool for characterizing and monitoring subsurface changes in various geophysical applications. However, non-repeatability (NR) issues caused, for instance, by GPS inaccuracies, often make it difficult to obtain unbiased time-lapse models. In this work we explore the portability of combining a receiver-extension FWI approach and Bayesian analysis to mitigate time-lapse noises arising from NR issues. The receiver-extension scheme introduces an artificial degree of freedom in positioning receivers, intending to minimize kinematic mismatches between modeled and observed data. Bayesian analysis systematically explores several potential solutions to mitigate time-lapse changes not associated with reservoir responses, assigning probabilities to each scenario based on prior information and available evidence. We consider two different subsurface models to demonstrate the potential of proposed approaches. First, using the Marmousi model, we investigate two NR scenarios associated with background noise in seismic data. Second, using a challenging deep-water Brazilian pre-salt setting, we investigate several NR scenarios to simulate real-world challenges. Our results demonstrate that combining Bayesian analysis with the receiver-extension FWI strategy can mitigate adverse NR effects successfully, producing cleaner and more reliable time-lapse models than conventional approaches. The results also reveal that the proposed Bayesian weighted procedure is a valuable tool for determining time-lapse estimates through statistical analysis of pre-existing models, allowing its application in ongoing time-lapse (4D) projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07467v1</guid>
      <category>physics.geo-ph</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergio Luiz E. F. da Silva, Ammir Karsou, Roger M. Moreira, Marco Cetale</dc:creator>
    </item>
    <item>
      <title>Function-valued marked spatial point processes on linear networks: application to urban cycling profiles</title>
      <link>https://arxiv.org/abs/2407.07637</link>
      <description>arXiv:2407.07637v1 Announce Type: cross 
Abstract: In the literature on spatial point processes, there is an emerging challenge in studying marked point processes with points being labelled by functions. In this paper, we focus on point processes living on linear networks and, from distinct points of view, propose several marked summary characteristics that are of great use in studying the average association and dispersion of the function-valued marks. Through a simulation study, we evaluate the performance of our proposed marked summary characteristics, both when marks are independent and when some sort of spatial dependence is evident among them. Finally, we employ our proposed mark summary characteristics to study the spatial structure of urban cycling profiles in Vancouver, Canada.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07637v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Eckardt, Jorge Mateu, Mehdi Moradi</dc:creator>
    </item>
    <item>
      <title>Two Stage Least Squares with Time-Varying Instruments: An Application to an Evaluation of Treatment Intensification for Type-2 Diabetes</title>
      <link>https://arxiv.org/abs/2407.07647</link>
      <description>arXiv:2407.07647v1 Announce Type: cross 
Abstract: As longitudinal data becomes more available in many settings, policy makers are increasingly interested in the effect of time-varying treatments (e.g. sustained treatment strategies). In settings such as this, the preferred analysis techniques are the g-methods, however these require the untestable assumption of no unmeasured confounding. Instrumental variable analyses can minimise bias through unmeasured confounding. Of these methods, the Two Stage Least Squares technique is one of the most well used in Econometrics, but it has not been fully extended, and evaluated, in full time-varying settings. This paper proposes a robust two stage least squares method for the econometric evaluation of time-varying treatment. Using a simulation study we found that, unlike standard two stage least squares, it performs relatively well across a wide range of circumstances, including model misspecification. It compares well with recent time-varying instrument approaches via g-estimation. We illustrate the methods in an evaluation of treatment intensification for Type-2 Diabetes Mellitus, exploring the exogeneity in prescribing preferences to operationalise a time-varying instrument.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07647v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Tompsett, Stijn Vansteelandt, Richard Grieve, Irene Petersen, Manuel Gomes</dc:creator>
    </item>
    <item>
      <title>Anomaly detection using data depth: multivariate case</title>
      <link>https://arxiv.org/abs/2210.02851</link>
      <description>arXiv:2210.02851v2 Announce Type: replace-cross 
Abstract: Anomaly detection is a branch of data analysis and machine learning which aims at identifying observations that exhibit abnormal behaviour. Be it measurement errors, disease development, severe weather, production quality default(s) (items) or failed equipment, financial frauds or crisis events, their on-time identification, isolation and explanation constitute an important task in almost any branch of science and industry. By providing a robust ordering, data depth - statistical function that measures belongingness of any point of the space to a data set - becomes a particularly useful tool for detection of anomalies. Already known for its theoretical properties, data depth has undergone substantial computational developments in the last decade and particularly recent years, which has made it applicable for contemporary-sized problems of data analysis and machine learning.
  In this article, data depth is studied as an efficient anomaly detection tool, assigning abnormality labels to observations with lower depth values, in a multivariate setting. Practical questions of necessity and reasonability of invariances and shape of the depth function, its robustness and computational complexity, choice of the threshold are discussed. Illustrations include use-cases that underline advantageous behaviour of data depth in various settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.02851v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Pavlo Mozharovskyi, Romain Valla</dc:creator>
    </item>
    <item>
      <title>Digital twin with automatic disturbance detection for real-time optimization of a semi-autogenous grinding (SAG) mill</title>
      <link>https://arxiv.org/abs/2407.06216</link>
      <description>arXiv:2407.06216v2 Announce Type: replace-cross 
Abstract: This work describes the development and validation of a digital twin for a semi-autogenous grinding (SAG) mill controlled by an expert system. The digital twin consists of three modules emulating a closed-loop system: fuzzy logic for the expert control, a state-space model for regulatory control, and a recurrent neural network for the SAG mill process. The model was trained with 68 hours of data and validated with 8 hours of test data. It predicts the mill's behavior within a 2.5-minute horizon with a 30-second sampling time. The disturbance detection evaluates the need for retraining, and the digital twin shows promise for supervising the SAG mill with the expert control system. Future work will focus on integrating this digital twin into real-time optimization strategies with industrial validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06216v2</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paulina Quintanilla, Francisco Fern\'andez, Cristobal Mancilla, Mat\'ias Rojas, Mauricio Estrada, Daniel Navia</dc:creator>
    </item>
  </channel>
</rss>
