<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 May 2025 04:00:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A computationally efficient framework for realistic epidemic modelling through Gaussian Markov random fields</title>
      <link>https://arxiv.org/abs/2505.03938</link>
      <description>arXiv:2505.03938v1 Announce Type: new 
Abstract: We tackle limitations of ordinary differential equation-driven Susceptible-Infections-Removed (SIR) models and their extensions that have recently be employed for epidemic nowcasting and forecasting. In particular, we deal with challenges related to the extension of SIR-type models to account for the so-called \textit{environmental stochasticity}, i.e., external factors, such as seasonal forcing, social cycles and vaccinations that can dramatically affect outbreaks of infectious diseases. Typically, in SIR-type models environmental stochasticity is modelled through stochastic processes. However, this stochastic extension of epidemic models leads to models with large dimension that increases over time. Here we propose a Bayesian approach to build an efficient modelling and inferential framework for epidemic nowcasting and forecasting by using Gaussian Markov random fields to model the evolution of these stochastic processes over time and across population strata. Importantly, we also develop a bespoke and computationally efficient Markov chain Monte Carlo algorithm to estimate the large number of parameters and latent states of the proposed model. We test our approach on simulated data and we apply it to real data from the Covid-19 pandemic in the United Kingdom.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03938v1</guid>
      <category>stat.CO</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angelos Alexopoulos, Paul Birrell, Daniela De Angelis</dc:creator>
    </item>
    <item>
      <title>Multilevel Sampling in Algebraic Statistics</title>
      <link>https://arxiv.org/abs/2505.04062</link>
      <description>arXiv:2505.04062v1 Announce Type: new 
Abstract: This paper proposes a multilevel sampling algorithm for fiber sampling problems in algebraic statistics, inspired by Henry Wynn's suggestion to adapt multilevel Monte Carlo (MLMC) ideas to discrete models. Focusing on log-linear models, we sample from high-dimensional lattice fibers defined by algebraic constraints. Building on Markov basis methods and results from Diaconis and Sturmfels, our algorithm uses variable step sizes to accelerate exploration and reduce the need for long burn-in. We introduce a novel Fiber Coverage Score (FCS) based on Voronoi partitioning to assess sample quality, and highlight the utility of the Maximum Mean Discrepancy (MMD) quality metric. Simulations on benchmark fibers show that multilevel sampling outperforms naive MCMC approaches. Our results demonstrate that multilevel methods, when properly applied, provide practical benefits for discrete sampling in algebraic statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04062v1</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Kirk, Ivan Gvozdanovi\'c, Sonja Petrovi\'c</dc:creator>
    </item>
    <item>
      <title>Particle Gibbs without the Gibbs bit</title>
      <link>https://arxiv.org/abs/2505.04611</link>
      <description>arXiv:2505.04611v1 Announce Type: new 
Abstract: Exact parameter and trajectory inference in state-space models is typically achieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or particle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly proposes a new trajectory and parameter, and accepts or rejects both at once. PGibbs instead alternates between sampling from the trajectory, using an algorithm known as conditional sequential Monte Carlo (CSMC) and the parameter in a Hastings-within-Gibbs fashion. While particle independent Metropolis Hastings (PIMH), the parameter-free version of PMMH, is known to be statistically worse than CSMC, PGibbs can induce a slow mixing if the parameter and the state trajectory are very correlated. This has made PMMH the method of choice for many practitioners, despite theory and experiments favouring CSMC over PIMH for the parameter-free problem. In this article, we describe a formulation of PGibbs which bypasses the Gibbs step, essentially marginalizing over the trajectory distribution in a fashion similar to PMMH. This is achieved by considering the implementation of a CSMC algortihm for the state-space model integrated over the joint distribution of the current parameter and the parameter proposal. We illustrate the benefits of method on a simple example known to be challenging for PMMH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04611v1</guid>
      <category>stat.CO</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos</dc:creator>
    </item>
    <item>
      <title>Likelihood-Free Adaptive Bayesian Inference via Nonparametric Distribution Matching</title>
      <link>https://arxiv.org/abs/2505.04603</link>
      <description>arXiv:2505.04603v1 Announce Type: cross 
Abstract: When the likelihood is analytically unavailable and computationally intractable, approximate Bayesian computation (ABC) has emerged as a widely used methodology for approximate posterior inference; however, it suffers from severe computational inefficiency in high-dimensional settings or under diffuse priors. To overcome these limitations, we propose Adaptive Bayesian Inference (ABI), a framework that bypasses traditional data-space discrepancies and instead compares distributions directly in posterior space through nonparametric distribution matching. By leveraging a novel Marginally-augmented Sliced Wasserstein (MSW) distance on posterior measures and exploiting its quantile representation, ABI transforms the challenging problem of measuring divergence between posterior distributions into a tractable sequence of one-dimensional conditional quantile regression tasks. Moreover, we introduce a new adaptive rejection sampling scheme that iteratively refines the posterior approximation by updating the proposal distribution via generative density estimation. Theoretically, we establish parametric convergence rates for the trimmed MSW distance and prove that the ABI posterior converges to the true posterior as the tolerance threshold vanishes. Through extensive empirical evaluation, we demonstrate that ABI significantly outperforms data-based Wasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free simulators, especially in high-dimensional or dependent observation regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04603v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhui Sophia Lu, Wing Hung Wong</dc:creator>
    </item>
    <item>
      <title>collapse: Advanced and Fast Statistical Computing and Data Transformation in R</title>
      <link>https://arxiv.org/abs/2403.05038</link>
      <description>arXiv:2403.05038v4 Announce Type: replace 
Abstract: collapse is a large C/C++-based infrastructure package facilitating complex statistical computing, data transformation, and exploration tasks in R - at outstanding levels of performance and memory efficiency. It also implements a class-agnostic approach to R programming, supporting vector, matrix and data frame-like objects and their popular extensions (units, integer64, xts, tibble, data.table, sf, pdata.frame), enabling its seamless integration with large parts of the R ecosystem. This article introduces the package's key components and design principles in a structured way, supported by a rich set of examples. A small benchmark demonstrates its computational performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05038v4</guid>
      <category>stat.CO</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Krantz</dc:creator>
    </item>
    <item>
      <title>Bayesian computation with generative diffusion models by Multilevel Monte Carlo</title>
      <link>https://arxiv.org/abs/2409.15511</link>
      <description>arXiv:2409.15511v3 Announce Type: replace 
Abstract: Generative diffusion models have recently emerged as a powerful strategy to perform stochastic sampling in Bayesian inverse problems, delivering remarkably accurate solutions for a wide range of challenging applications. However, diffusion models often require a large number of neural function evaluations per sample in order to deliver accurate posterior samples. As a result, using diffusion models as stochastic samplers for Monte Carlo integration in Bayesian computation can be highly computationally expensive, particularly in applications that require a substantial number of Monte Carlo samples for conducting uncertainty quantification analyses. This cost is especially high in large-scale inverse problems such as computational imaging, which rely on large neural networks that are expensive to evaluate. With quantitative imaging applications in mind, this paper presents a Multilevel Monte Carlo strategy that significantly reduces the cost of Bayesian computation with diffusion models. This is achieved by exploiting cost-accuracy trade-offs inherent to diffusion models to carefully couple models of different levels of accuracy in a manner that significantly reduces the overall cost of the calculation, without reducing the final accuracy. The proposed approach achieves a $4\times$-to-$8\times$ reduction in computational cost w.r.t. standard techniques across three benchmark imaging problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15511v3</guid>
      <category>stat.CO</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdul-Lateef Haji-Ali, Marcelo Pereyra, Luke Shaw, Konstantinos Zygalakis</dc:creator>
    </item>
    <item>
      <title>Functional Partial Least-Squares: Adaptive Estimation and Inference</title>
      <link>https://arxiv.org/abs/2402.11134</link>
      <description>arXiv:2402.11134v2 Announce Type: replace-cross 
Abstract: We study the functional linear regression model with a scalar response and a Hilbert space-valued predictor, a canonical example of an ill-posed inverse problem. We show that the functional partial least squares (PLS) estimator attains nearly minimax-optimal convergence rates over a class of ellipsoids and propose an adaptive early stopping procedure for selecting the number of PLS components. In addition, we develop new test that can detect local alternatives converging at the parametric rate which can be inverted to construct confidence sets. Simulation results demonstrate that the estimator performs favorably relative to several existing methods and the proposed test exhibits good power properties. We apply our methodology to evaluate the nonlinear effects of temperature on corn and soybean yields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11134v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrii Babii, Marine Carrasco, Idriss Tsafack</dc:creator>
    </item>
    <item>
      <title>Dynamic Bayesian Networks with Conditional Dynamics in Edge Addition and Deletion</title>
      <link>https://arxiv.org/abs/2409.08965</link>
      <description>arXiv:2409.08965v2 Announce Type: replace-cross 
Abstract: This study presents a dynamic Bayesian network framework that facilitates intuitive gradual edge changes. We use two conditional dynamics to model the edge addition and deletion, and edge selection separately. Unlike previous research that uses a mixture network approach, which restricts the number of possible edge changes, or structural priors to induce gradual changes, which can lead to unclear network evolution, our model induces more frequent and intuitive edge change dynamics. We employ Markov chain Monte Carlo (MCMC) sampling to estimate the model structures and parameters and demonstrate the model's effectiveness in a portfolio selection application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08965v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lupe S. H. Chan, Amanda M. Y. Chu, Mike K. P. So</dc:creator>
    </item>
  </channel>
</rss>
