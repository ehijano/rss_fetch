<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Jul 2025 02:20:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian Deep Gaussian Processes for Correlated Functional Data: A Case Study in Cosmological Matter Power Spectra</title>
      <link>https://arxiv.org/abs/2507.18683</link>
      <description>arXiv:2507.18683v1 Announce Type: new 
Abstract: Understanding the structure of our universe and the distribution of matter is an area of active research. As cosmological surveys grow in complexity, the development of emulators to efficiently and effectively predict matter power spectra is essential. We are particularly motivated by the Mira-Titan Universe simulation suite that, for a specified cosmological parameterization (termed a "cosmology"), provides multiple response curves of various fidelities, including correlated functional realizations. Our objective is two-fold. First, we estimate the underlying true matter power spectra, with appropriate uncertainty quantification (UQ), from all of the provided curves. To this end, we propose a novel Bayesian deep Gaussian process (DGP) hierarchical model which synthesizes all the simulation information to estimate the underlying matter power spectra while providing effective UQ. Our model extends previous work on Bayesian DGPs from scalar responses to correlated functional outputs. Second, we leverage our predicted power spectra from various cosmologies in order to accurately predict the entire matter power spectra for an unobserved cosmology. For this task, we use basis function representations of the functional spectra to train a separate Gaussian process emulator. Our method performs well in synthetic exercises and against the benchmark cosmological emulator (Cosmic Emu).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18683v1</guid>
      <category>stat.AP</category>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen A. Walsh, Annie S. Booth, David Higdon, Jared Clark, Kelly R. Moran, Katrin Heitmann</dc:creator>
    </item>
    <item>
      <title>Cure Rate Joint Model for Time-to-Event Data and Longitudinal Tumor Burden with Potential Change Points</title>
      <link>https://arxiv.org/abs/2507.18773</link>
      <description>arXiv:2507.18773v1 Announce Type: cross 
Abstract: In non-small cell lung cancer (NSCLC) clinical trials, tumor burden (TB) is a key longitudinal biomarker for assessing treatment effects. Typically, standard-of-care (SOC) therapies and some novel interventions initially decrease TB; however, many patients subsequently experience an increase-indicating disease progression-while others show a continuous decline. In patients with an eventual TB increase, the change point marks the onset of progression and must occur before the time of the event. To capture these distinct dynamics, we propose a novel joint model that integrates time-to-event and longitudinal TB data, classifying patients into a change-point group or a stable group. For the change-point group, our approach flexibly estimates an individualized change point by leveraging time-to-event information. We use a Monte Carlo Expectation-Maximization (MCEM) algorithm for efficient parameter estimation. Simulation studies demonstrate that our model outperforms traditional approaches by accurately capturing diverse disease progression patterns and handling censoring complexities, leading to robust marginal TB outcome estimates. When applied to a Phase 3 NSCLC trial comparing cemiplimab monotherapy to SOC, the treatment group shows prolonged TB reduction and consistently lower TB over time, highlighting the clinical utility of our approach. The implementation code is publicly available on https://github.com/quyixiang/JoCuR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18773v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yixiang Qu, Ethan M. Alt, Weibin Zhong, Jeen Liu, Chenguang Wang, Joseph G. Ibrahim</dc:creator>
    </item>
    <item>
      <title>Deep Neural Network Driven Simulation Based Inference Method for Pole Position Estimation under Model Misspecification</title>
      <link>https://arxiv.org/abs/2507.18824</link>
      <description>arXiv:2507.18824v1 Announce Type: cross 
Abstract: Simulation Based Inference (SBI) is shown to yield more accurate resonance parameter estimates than traditional chi-squared minimization in certain cases of model misspecification, demonstrated through a case study of pi-pi scattering and the rho(770) resonance. Models fit to some data sets using chi-squared minimization can predict inaccurate pole positions for the rho(770), while SBI provides more robust predictions across the same models and data. This result is significant both as a proof of concept that SBI can handle model misspecification, and because accurate modeling of pi-pi scattering is essential in the study of many contemporary physical systems (e.g., a1(1260), omega(782)).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18824v1</guid>
      <category>hep-ph</category>
      <category>nucl-th</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Sadasivan, Isaac Cordero, Andrew Graham, Cecilia Marsh, Daniel Kupcho, Melana Mourad, Maxim Mai</dc:creator>
    </item>
    <item>
      <title>Nonparametric Linear Discriminant Analysis for High Dimensional Matrix-Valued Data</title>
      <link>https://arxiv.org/abs/2507.19028</link>
      <description>arXiv:2507.19028v2 Announce Type: cross 
Abstract: This paper addresses classification problems with matrix-valued data, which commonly arises in applications such as neuroimaging and signal processing. Building on the assumption that the data from each class follows a matrix normal distribution, we propose a novel extension of Fisher's Linear Discriminant Analysis (LDA) tailored for matrix-valued observations. To effectively capture structural information while maintaining estimation flexibility, we adopt a nonparametric empirical Bayes framework based on Nonparametric Maximum Likelihood Estimation (NPMLE), applied to vectorized and scaled matrices. The NPMLE method has been shown to provide robust, flexible, and accurate estimates for vector-valued data with various structures in the mean vector or covariance matrix. By leveraging its strengths, our method is effectively generalized to the matrix setting, thereby improving classification performance. Through extensive simulation studies and real data applications, including electroencephalography (EEG) and magnetic resonance imaging (MRI) analysis, we demonstrate that the proposed method consistently outperforms existing approaches across a variety of data structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19028v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seungyeon Oh, Seongoh Park, Hoyoung Park</dc:creator>
    </item>
    <item>
      <title>Demystifying AI in Criminal Justice</title>
      <link>https://arxiv.org/abs/2507.19305</link>
      <description>arXiv:2507.19305v1 Announce Type: cross 
Abstract: There is widespread confusion among criminal justice practitioners and legal scholars about the use of artificial intelligence in criminal justice. This didactic review is written for readers with little or no background in statistics or computer science. It is not intended to replace more technical treatments. It is intended to supplement them and encourage readers to dig more deeply into topics that strike their fancy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19305v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard Berk</dc:creator>
    </item>
    <item>
      <title>Bernoulli amputation</title>
      <link>https://arxiv.org/abs/2407.18572</link>
      <description>arXiv:2407.18572v2 Announce Type: replace 
Abstract: An approach to amputation, the process of introducing missing values to a complete dataset, is presented. It allows to construct missingness indicators in a flexible and principled way via copulas and Bernoulli margins and to incorporate dependence in missingness patterns. Besides more classical missingness models such as missing completely at random, missing at random, and missing not at random, the approach is able to model structured missingness such as block missingness and, via mixtures, monotone missingness, which are patterns of missing data frequently found in real-life datasets. Properties such as joint missingness probabilities or missingness correlation are derived mathematically. The approach is demonstrated with mathematical examples and empirical illustrations in terms of a well-known dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18572v2</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Hofert, James Jackson, Niels Hagenbuch</dc:creator>
    </item>
    <item>
      <title>VISTA-SSM: Varying and Irregular Sampling Time-series Analysis via State Space Models</title>
      <link>https://arxiv.org/abs/2410.21527</link>
      <description>arXiv:2410.21527v3 Announce Type: replace 
Abstract: We introduce VISTA, a clustering approach for multivariate and irregularly sampled time series based on a parametric state space mixture model. VISTA is specifically designed for the unsupervised identification of groups in datasets originating from healthcare and psychology where such sampling issues are commonplace. Our approach adapts linear Gaussian state space models (LGSSMs) to provide a flexible parametric framework for fitting a wide range of time series dynamics. The clustering approach itself is based on the assumption that the population can be represented as a mixture of a fixed number of LGSSMs. VISTA's model formulation allows for an explicit derivation of the log-likelihood function, from which we develop an expectation-maximization scheme for fitting model parameters to the observed data samples. Our algorithmic implementation is designed to handle populations of multivariate time series that can exhibit large changes in sampling rate as well as irregular sampling. We evaluate the versatility and accuracy of our approach on simulated and real-world datasets, including demographic trends, wearable sensor data, epidemiological time series, and ecological momentary assessments. Our results indicate that VISTA outperforms most comparable standard times series clustering methods. We provide an open-source implementation of VISTA in Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21527v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Brindle, Thomas Derrick Hull, Matteo Malgaroli, Nicolas Charon</dc:creator>
    </item>
    <item>
      <title>Start from the End: A Framework for Computational Policy Exploration to Inform Effective and Geospatially Consistent Interventions applied to COVID-19 in St. Louis</title>
      <link>https://arxiv.org/abs/2507.10870</link>
      <description>arXiv:2507.10870v2 Announce Type: replace 
Abstract: Mathematical models are a powerful tool to study infectious disease dynamics and intervention strategies against them in social systems. However, due to their detailed implementation and steep computational requirements, practitioners and stakeholders are typically only able to explore a small subset of all possible intervention scenarios, a severe limitation when preparing for disease outbreaks. In this work, we propose a parameter exploration framework utilizing emulator models to make uncertainty-aware predictions of high-dimensional parameter spaces and identify large numbers of feasible response strategies. We apply our framework to a case study of a large-scale agent-based disease model of the COVID-19 ``Omicron wave'' in St. Louis, Missouri that took place from December 2021 to February 2022. We identify large numbers of response strategies that would have been estimated to have reduced disease spread by a substantial amount. We also identify policy interventions that would have been able to reduce the geospatial variation in disease spread, which has additional implications for designing thoughtful response strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10870v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David O'Gara, Matt Kasman, Matthew D. Haslam, Ross A. Hammond</dc:creator>
    </item>
    <item>
      <title>Generalizability with ignorance in mind: learning what we do (not) know for archetypes discovery</title>
      <link>https://arxiv.org/abs/2501.13355</link>
      <description>arXiv:2501.13355v2 Announce Type: replace-cross 
Abstract: When studying policy interventions, researchers often pursue two goals: i) identifying for whom the program has the largest effects (heterogeneity) and ii) determining whether those patterns of treatment effects have predictive power across environments (generalizability). We develop a framework to learn when and how to partition observations into groups of individual and environmental characterstics within which treatment effects are predictively stable, and when instead extrapolation is unwarranted and further evidence is needed. Our procedure determines in which contexts effects are generalizable and when, instead, researchers should admit ignorance and collect more data. We provide a decision-theoretic foundation, derive finite-sample regret guarantees, and establish asymptotic inference results. We illustrate the benefits of our approach by reanalyzing a multifaceted anti-poverty program across six countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13355v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Emily Breza, Arun G. Chandrasekhar, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>R2 priors for Grouped Variance Decomposition in High-dimensional Regression</title>
      <link>https://arxiv.org/abs/2507.11833</link>
      <description>arXiv:2507.11833v2 Announce Type: replace-cross 
Abstract: We introduce the Group-R2 decomposition prior, a hierarchical shrinkage prior that extends R2-based priors to structured regression settings with known groups of predictors. By decomposing the prior distribution of the coefficient of determination R2 in two stages, first across groups, then within groups, the prior enables interpretable control over model complexity and sparsity. We derive theoretical properties of the prior, including marginal distributions of coefficients, tail behavior, and connections to effective model complexity. Through simulation studies, we evaluate the conditions under which grouping improves predictive performance and parameter recovery compared to priors that do not account for groups. Our results provide practical guidance for prior specification and highlight both the strengths and limitations of incorporating grouping into R2-based shrinkage priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11833v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javier Enrique Aguilar, David Kohns, Aki Vehtari, Paul-Christian B\"urkner</dc:creator>
    </item>
  </channel>
</rss>
