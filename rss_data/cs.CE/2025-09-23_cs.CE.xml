<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Sep 2025 01:52:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Toward Engineering AGI: Benchmarking the Engineering Design Capabilities of LLMs</title>
      <link>https://arxiv.org/abs/2509.16204</link>
      <description>arXiv:2509.16204v1 Announce Type: new 
Abstract: Today, industry pioneers dream of developing general-purpose AI engineers capable of designing and building humanity's most ambitious projects--from starships that will carry us to distant worlds to Dyson spheres that harness stellar energy. Yet engineering design represents a fundamentally different challenge for large language models (LLMs) compared to traditional textbook-style problem solving or factual question answering. Real-world engineering design demands the synthesis of domain knowledge, navigation of complex trade-offs, and management of the tedious processes that consume much of practicing engineers' time. Despite these shared challenges across engineering disciplines, no benchmark currently captures the unique demands of engineering design work. In this work, we introduce ENGDESIGN, an Engineering Design benchmark that evaluates LLMs' abilities to perform practical design tasks across nine engineering domains: Operating System Design, Computer Architecture Design, Control System Design, Mechanical Systems, Structural Design, Digital Hardware Design, Analog Integrated Circuit Design, Robotics, and Signal Processing. Unlike existing benchmarks that focus on factual recall or question answering, ENGDESIGN uniquely emphasizes LLMs' ability to synthesize domain knowledge, reason under constraints, and generate functional, objective-oriented designs. Each task in ENGDESIGN represents a real-world engineering design problem, accompanied by a detailed task description specifying design goals, constraints, and performance requirements. We pioneer a simulation-based evaluation paradigm where LLM-generated designs undergo rigorous testing through executable, domain-specific simulations-from circuit SPICE simulations to structural finite element analysis, from control system validation to robotic motion planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16204v1</guid>
      <category>cs.CE</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingang Guo, Yaxin Li, Xiangyi Kong, Yilan Jiang, Xiayu Zhao, Zhihua Gong, Yufan Zhang, Daixuan Li, Tianle Sang, Beixiao Zhu, Gregory Jun, Yingbing Huang, Yiqi Liu, Yuqi Xue, Rahul Dev Kundu, Qi Jian Lim, Yizhou Zhao, Luke Alexander Granger, Mohamed Badr Younis, Darioush Keivan, Nippun Sabharwal, Shreyanka Sinha, Prakhar Agarwal, Kojo Vandyck, Hanlin Mai, Zichen Wang, Aditya Venkatesh, Ayush Barik, Jiankun Yang, Chongying Yue, Jingjie He, Libin Wang, Licheng Xu, Hao Chen, Jinwen Wang, Liujun Xu, Rushabh Shetty, Ziheng Guo, Dahui Song, Manvi Jha, Weijie Liang, Weiman Yan, Bryan Zhang, Sahil Bhandary Karnoor, Jialiang Zhang, Rutva Pandya, Xinyi Gong, Mithesh Ballae Ganesh, Feize Shi, Ruiling Xu, Yifan Zhang, Yanfeng Ouyang, Lianhui Qin, Elyse Rosenbaum, Corey Snyder, Peter Seiler, Geir Dullerud, Xiaojia Shelly Zhang, Zuofu Cheng, Pavan Kumar Hanumolu, Jian Huang, Mayank Kulkarni, Mahdi Namazifar, Huan Zhang, Bin Hu</dc:creator>
    </item>
    <item>
      <title>Deep Reinforcement Learning in Factor Investment</title>
      <link>https://arxiv.org/abs/2509.16206</link>
      <description>arXiv:2509.16206v1 Announce Type: new 
Abstract: Deep reinforcement learning has shown promise in trade execution, yet its use in low-frequency factor portfolio construction remains under-explored. A key obstacle is the high-dimensional, unbalanced state space created by stocks that enter and exit the investable universe. We introduce Conditional Auto-encoded Factor-based Portfolio Optimisation (CAFPO), which compresses stock-level returns into a small set of latent factors conditioned on 94 firm-specific characteristics. The factors feed a DRL agent implemented with both PPO and DDPG to generate continuous long-short weights. On 20 years of U.S. equity data (2000--2020), CAFPO outperforms equal-weight, value-weight, Markowitz, vanilla DRL, and Fama--French-driven DRL, delivering a 24.6\% compound return and a Sharpe ratio of 0.94 out of sample. SHAP analysis further reveals economically intuitive factor attributions. Our results demonstrate that factor-aware representation learning can make DRL practical for institutional, low-turnover portfolio management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16206v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Junlin Liu</dc:creator>
    </item>
    <item>
      <title>Predictive Machine Learning to Increase the Throughput of Container Yards</title>
      <link>https://arxiv.org/abs/2509.16207</link>
      <description>arXiv:2509.16207v1 Announce Type: new 
Abstract: This study seeks to improve the throughput rates for shipping container terminals. In the United States, shipping ports link the domestic economy to global markets and are vital to sustain supply chain flow and economic stability. Maritime shipping accounts for nearly half of the U.S.'s annual international trade, two thirds of which are represented by container shipping. Previous studies highlighted the capability of automation in enhancing container processing; however, unlike in European and East Asian ports, full automation is limited in U.S. ports due to legal protections for human labor. Consequently, there is a need for alternative methods that deliver automation level efficiencies while maintaining the terms of cooperative agreements. This paper proposes an Intelligent Planning System (IPS) that applies the concept of Pareto Optimization to container yards through a mixed integer linear programming (MILP) based recursive appointment system. The results show an improvement from baseline for both daily terminal throughput volumes and processing times. The generated IPS can be employed to provide recommendations for container positioning and truck pickup appointments to optimize container yard layout and flow resulting in reduced realtime congestion and predictively mitigated future congestion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16207v1</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Austin Ford Cooper</dc:creator>
    </item>
    <item>
      <title>Synthesis of Service Life Prediction for Bridges in Texas</title>
      <link>https://arxiv.org/abs/2509.16208</link>
      <description>arXiv:2509.16208v1 Announce Type: new 
Abstract: Design-build bridge contracts often include long-term service life requirements, but there are no clear technical guidelines or standardized methods to achieve or verify these goals. While durability practices are commonly applied, they lack quantitative validation. With many aging bridges and limited financial resources, accurately estimating remaining service life is essential for prioritizing repair and rehabilitation needs. This research reviews current practices and recent advancements in bridge service life prediction, providing practical guidance for evaluating and extending the lifespan of both existing and new structures. The findings support more efficient use of maintenance funds, better understanding of deterioration models and inspection methods, and informed strategies to ensure long-term structural performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16208v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lu Gao, Yi-lung Mo, Shalaka Dhonde, Daisy Saldarriaga, Lingguang Song, Ahmed Senouci</dc:creator>
    </item>
    <item>
      <title>Scaling Digital Twin Models</title>
      <link>https://arxiv.org/abs/2509.16209</link>
      <description>arXiv:2509.16209v1 Announce Type: new 
Abstract: In many industries, the scale and complexity of systems can present significant barriers to the development of accurate digital twin models. This paper introduces a novel methodology and a modular computational tool utilizing machine learning and dimensional analysis to establish a framework for scaling digital twin models. Scaling techniques have not yet been applied to digital twin technology, but they can eliminate the need for repetitive physical calibration of such models in industries where product lines include a variety of sizes of the same or similar products. In many cases, it may be easier or more cost-effective to perform physical calibration of the digital twin model on smaller units of a product line. Scaling techniques can then allow adapting the calibration data from the smaller units to other sizes of the product line without the need for additional data collection and experimentation for calibration. Conventional application of dimensional analysis for scaling in this context introduces several challenges due to distortion of scaling factors. This paper addresses these challenges and introduces a framework for proper scaling of digital twin models. The results are applied to scaling the models between an industrial-size wheel loader vehicle used in construction to a miniaturized system instrumented in a laboratory setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16209v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deniz Karanfil, Bahram Ravani</dc:creator>
    </item>
    <item>
      <title>Strain localization in reduced order asymptotic homogenization</title>
      <link>https://arxiv.org/abs/2509.16210</link>
      <description>arXiv:2509.16210v1 Announce Type: new 
Abstract: A reduced order asymptotic homogenization based multiscale technique which can capture damage and inelastic effects in composite materials is proposed. This technique is based on two scale homogenization procedure where eigen strain representation accounts for the inelastic response and the computational efforts are alleviated by reduction of order technique. Macroscale stress is derived by calculating the influence tensors from the analysis of representative volume element (RVE). At microscale, the damage in the material is modeled using continuum damage mechanics (CDM) based framework. To solve the problem of strain localization a method of the alteration of stress-strain relation of micro con- stituents based on the dissipated fracture energy in a crack band is implemented. The issue of spurious post failure artificial stiffness at macroscale is discussed and effect of increasing the order to alleviate this problem is checked. Verification studies demonstrated the proposed formulation predicts the macroscale response and also captures the damage and plasticity induced inelastic strains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16210v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1177/1081286519892655</arxiv:DOI>
      <arxiv:journal_reference>Singh, H., &amp; Mahajan, P. (2020). Strain localization in reduced-order asymptotic homogenization. Mathematics and Mechanics of Solids, 25(4), 913-936</arxiv:journal_reference>
      <dc:creator>Harpreet Singh, Puneet Mahajan</dc:creator>
    </item>
    <item>
      <title>E$^2$-TFA based multiscale analysis of failure in elasto-plastic composites</title>
      <link>https://arxiv.org/abs/2509.16211</link>
      <description>arXiv:2509.16211v1 Announce Type: new 
Abstract: This paper describes a novel homogenization methodology for analyzing the failure of elastoplastic composite materials based on elastic and eigen influence tensors-driven transformation field analysis ($\mathtt{E}^2$-TFA). The proposed technique considers the microscopic eigenstrain field accounting for intra-phase damage and inelastic strains. This results in realistic computations by alleviating the post-damage stiffness response, which is a drawback of TFA-based methods. We attain computational efficiency by identifying the preprocessing data solely from the elastic and eigen transformation functions and adopting a reduced order modelling technique with a piecewise constant eigenstrain field throughout the subdomains. The performance of the model is assessed by simulating the response for (a) the representative volume element (RVE) as a homogenized continuum and (b) the various composites under complex load histories with intricate macroscale morphologies. Furthermore, the nonlinear shear stress-strain response of a glass fiber composite is calculated and compared to experimentally measured fracture initiation parameters, failure plane orientation, and strain histories. Finally, we show that $\mathtt{E}^2$-TFA can accurately and efficiently capture damage and inelastic deformations in order to estimate the mechanical response of composite materials in a better way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16211v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.engfracmech.2023.109787</arxiv:DOI>
      <arxiv:journal_reference>Singh, H. (2024). E2-TFA based multiscale analysis of failure in elasto-plastic composites. Engineering Fracture Mechanics, 295, 109787</arxiv:journal_reference>
      <dc:creator>Harpreet Singh</dc:creator>
    </item>
    <item>
      <title>An efficient framework for computing sensitivity of modal-related structural dynamic characteristics with multi-parameters</title>
      <link>https://arxiv.org/abs/2509.16214</link>
      <description>arXiv:2509.16214v1 Announce Type: new 
Abstract: The sensitivity of structural dynamic characteristics related to eigenmode (such as modal assurance criteria, modal flexibility, and modal mass etc.) has become a crucial and widely applied tool across various engineering fields. In this paper, a novel strategy is proposed for solving the sensitivity of structural dynamic characteristics related to eigenmode with respect to multiple variables. First, an algebraic method for computing the sensitivity of eigenvectors is developed to simplify the expression for sensitivity calculations. Subsequently, based on this new expression for eigenmode sensitivity, a framework for sensitivity analysis of structural dynamic characteristics related to eigenmodes with multiple parameters is established. With the incorporation of a preconditioning iterative method, the new computational framework effectively enhances the computational efficiency of sensitivity analysis for structural characteristics related to eigenmodes with multiple parameters. This framework is easy to operate and effectively reduces the "Fill-in" operations of sparse matrices. Three numerical examples are given to illustrate the effectiveness of the algorithm. The result shows that the novel strategy can significantly reduce central processing unit (CPU) computational time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16214v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Huang, Zhengguang Li, Xiuli Wang</dc:creator>
    </item>
    <item>
      <title>On the Detection of Internal Defects in Structured Media</title>
      <link>https://arxiv.org/abs/2509.16216</link>
      <description>arXiv:2509.16216v1 Announce Type: new 
Abstract: A critical issue that affects engineers trying to assess the structural integrity of various infrastructures, such as metal rods or acoustic ducts, is the challenge of detecting internal fractures (defects). Traditionally, engineers depend on audible and visual aids to identify these fractures, as they do not physically dissect the object in question into multiple pieces to check for inconsistencies. This research introduces ideas towards the development of a robust strategy to image such defects using only a small set of minimal, non-invasive measurements.
  Assuming a one dimensional model (e.g. longitudinal waves in long and thin rods/acoustic ducts or transverse vibrations of strings), we make use of the continuous one-dimensional wave equation to model these physical phenomena and then employ specialized mathematical analysis tools (the Laplace transform and optimization) to introduce our defect imaging ideas. In particular, we will focus on the case of a long bar which is homogeneous throughout except in a small area where a defect in its Young's modulus is present. We will first demonstrate how the problem is equivalent to a spring-mass vibrational system, and then show how our imaging strategy makes use of the Laplace domain analytic map between the characteristics of the respective defect and the measurement data.
  More explicitly, we will utilize MATLAB (a platform for numerical computations) to collect synthetic data (computational alternative to real world measurements) for several scenarios with one defect of arbitrary location and stiffness. Subsequently, we will use this data along with our analytically developed map (between defect characteristics and measurements) to construct a residual function which, once optimized, will reveal the location and magnitude of the stiffness defect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16216v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bryl Nico M. Ong, Aarush Borker, Neil Jerome A. Egarguin, Daniel Onofrei</dc:creator>
    </item>
    <item>
      <title>An Efficient Transient Nonlinear Circuit Simulator Using Exponential Integration and Block-Jacobi Precondition</title>
      <link>https://arxiv.org/abs/2509.16219</link>
      <description>arXiv:2509.16219v1 Announce Type: new 
Abstract: Transient simulation of linear and nonlinear circuits remains an important task in modern EDA tools. At present, SPICE-like simulators face challenges in parallelization, nonlinear convergence and linear efficiency, especially when applied to large-scale circuits. To address the limitations of simulators in handling various nonlinear circuits, we adopt a generalized row-echelon regularization approach, which extends the applicability of exponential integrators to a broader class of differential algebraic equations. The proposed method employs matrix exponential vector products to integrate the regularized system, allowing for a larger time step size while preserving accuracy and stability. Furthermore, in order to accelerate GMRES-based solvers within Newton-Raphson iterations, a structured block-Jacobi preconditioner is designed for linear systems. For locally coupled circuits, Additive Schwarz overlapping strategy is adopted to enhance the solution performance. Numerical experiments of various nonlinear circuit models show that under same hardware environment, our method achieves a speedup of 1.95$\times$-- 3.27$\times$ in total computation time compared to Backward Euler with Inexact Newton iterations, and time steps have decreased by an average of 60.70\% (up to 74.59\%). Compared with EI-NK method, total computing time of our method has a speedup of 1.08$\times$-- 1.79$\times$. These results highlight the potential of proposed method for scalable and nonlinear circuit simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16219v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zijian Zhang, Yuanmiao Lin, Xuesong Chen, Shuting Cai</dc:creator>
    </item>
    <item>
      <title>An Open Dataset for Temperature Modelling in Machine Tools</title>
      <link>https://arxiv.org/abs/2509.16222</link>
      <description>arXiv:2509.16222v1 Announce Type: new 
Abstract: This data set descriptor introduces a structured, high-resolution dataset of transient thermal simulations for a vertical axis of a machine tool test rig. The data set includes temperature and heat flux values recorded at 29 probe locations at 1800 time steps, sampled every second over a 30-minute range, across 17 simulation runs derived from a fractional factorial design. First, a computer-aided design model was de-featured, segmented, and optimized, followed by finite element (FE) modelling. Detailed information on material, mesh, and boundary conditions is included. To support research and model development, the dataset provides summary statistics, thermal evolution plots, correlation matrix analyses, and a reproducible Jupyter notebook. The data set is designed to support machine learning and deep learning applications in thermal modelling for prediction, correction, and compensation of thermally induced deviations in mechanical systems, and aims to support researchers without FE expertise by providing ready-to-use simulation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16222v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>C. Coelho, D. Fern\'andez, M. Hohmann, L. Penter, S. Ihlenfeldt, O. Niggemann</dc:creator>
    </item>
    <item>
      <title>Increasing Inter-Fiber Contact in the Altendorf-Jeulin Model</title>
      <link>https://arxiv.org/abs/2509.16225</link>
      <description>arXiv:2509.16225v1 Announce Type: new 
Abstract: In fields such as material design or biomedicine, fiber materials play an important role. Fiber simulations, also called digital twins, provide a basis for testing and optimizing the material's physical behavior digitally. Inter-fiber contacts can influence the thermal and mechanical behavior of a fiber system; to our knowledge, however, there exist no parametric fiber models allowing for explicit modeling of the number of inter-fiber contacts. Therefore, this paper proposes an extension of the iterative force-biased fiber packing by Altendorf \&amp; Jeulin. In this extension, we model the inter-fiber contacts explicitly and add another force to the force-biased packing to increase the number of contacts. We successfully validate the packing with respect to its parameter accuracy. Moreover, we show that the extension indeed increases the number of contacts, even exceeding theoretical values. Hence, this packing scheme has the potential to achieve higher accuracy in physical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16225v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Keilmann, Claudia Redenbach, Francois Willot</dc:creator>
    </item>
    <item>
      <title>Learn to Rank Risky Investors: A Case Study of Predicting Retail Traders' Behaviour and Profitability</title>
      <link>https://arxiv.org/abs/2509.16616</link>
      <description>arXiv:2509.16616v1 Announce Type: new 
Abstract: Identifying risky traders with high profits in financial markets is crucial for market makers, such as trading exchanges, to ensure effective risk management through real-time decisions on regulation compliance and hedging. However, capturing the complex and dynamic behaviours of individual traders poses significant challenges. Traditional classification and anomaly detection methods often establish a fixed risk boundary, failing to account for this complexity and dynamism. To tackle this issue, we propose a profit-aware risk ranker (PA-RiskRanker) that reframes the problem of identifying risky traders as a ranking task using Learning-to-Rank (LETOR) algorithms. Our approach features a Profit-Aware binary cross entropy (PA-BCE) loss function and a transformer-based ranker enhanced with a self-cross-trader attention pipeline. These components effectively integrate profit and loss (P&amp;L) considerations into the training process while capturing intra- and inter-trader relationships. Our research critically examines the limitations of existing deep learning-based LETOR algorithms in trading risk management, which often overlook the importance of P&amp;L in financial scenarios. By prioritising P&amp;L, our method improves risky trader identification, achieving an 8.4% increase in F1 score compared to state-of-the-art (SOTA) ranking models like Rankformer. Additionally, it demonstrates a 10%-17% increase in average profit compared to all benchmark models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16616v1</guid>
      <category>cs.CE</category>
      <category>cs.IR</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3768623</arxiv:DOI>
      <arxiv:journal_reference>ACM Transactions on Information Systems 2025</arxiv:journal_reference>
      <dc:creator>Weixian Waylon Li, Tiejun Ma</dc:creator>
    </item>
    <item>
      <title>Rational Multi-Modal Transformers for TCR-pMHC Prediction</title>
      <link>https://arxiv.org/abs/2509.17305</link>
      <description>arXiv:2509.17305v1 Announce Type: new 
Abstract: T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is fundamental to adaptive immunity and central to the development of T cell-based immunotherapies. While transformer-based models have shown promise in predicting TCR-pMHC interactions, most lack a systematic and explainable approach to architecture design. We present an approach that uses a new post-hoc explainability method to inform the construction of a novel encoder-decoder transformer model. By identifying the most informative combinations of TCR and epitope sequence inputs, we optimize cross-attention strategies, incorporate auxiliary training objectives, and introduce a novel early-stopping criterion based on explanation quality. Our framework achieves state-of-the-art predictive performance while simultaneously improving explainability, robustness, and generalization. This work establishes a principled, explanation-driven strategy for modeling TCR-pMHC binding and offers mechanistic insights into sequence-level binding behavior through the lens of deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17305v1</guid>
      <category>cs.CE</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiarui Li, Zixiang Yin, Zhengming Ding, Samuel J. Landry, Ramgopal R. Mettu</dc:creator>
    </item>
    <item>
      <title>$i$MIND: Insightful Multi-subject Invariant Neural Decoding</title>
      <link>https://arxiv.org/abs/2509.17313</link>
      <description>arXiv:2509.17313v1 Announce Type: new 
Abstract: Decoding visual signals holds the tantalizing potential to unravel the complexities of cognition and perception. While recent studies have focused on reconstructing visual stimuli from neural recordings to bridge brain activity with visual imagery, existing methods offer limited insights into the underlying mechanisms of visual processing in the brain. To mitigate this gap, we present an \textit{i}nsightful \textbf{M}ulti-subject \textbf{I}nvariant \textbf{N}eural \textbf{D}ecoding ($i$MIND) model, which employs a novel dual-decoding framework--both biometric and semantic decoding--to offer neural interpretability in a data-driven manner and deepen our understanding of brain-based visual functionalities. Our $i$MIND model operates through three core steps: establishing a shared neural representation space across subjects using a ViT-based masked autoencoder, disentangling neural features into complementary subject-specific and object-specific components, and performing dual decoding to support both biometric and semantic classification tasks. Experimental results demonstrate that $i$MIND achieves state-of-the-art decoding performance with minimal scalability limitations. Furthermore, $i$MIND empirically generates voxel-object activation fingerprints that reveal object-specific neural patterns and enable investigation of subject-specific variations in attention to identical stimuli. These findings provide a foundation for more interpretable and generalizable subject-invariant neural decoding, advancing our understanding of the voxel semantic selectivity as well as the neural vision processing dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17313v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zixiang Yin, Jiarui Li, Zhengming Ding</dc:creator>
    </item>
    <item>
      <title>Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach to Knowledge Retrieval and Hallucination Reduction</title>
      <link>https://arxiv.org/abs/2509.16369</link>
      <description>arXiv:2509.16369v1 Announce Type: cross 
Abstract: Accurate and reliable knowledge retrieval is vital for financial question-answering, where continually updated data sources and complex, high-stakes contexts demand precision. Traditional retrieval systems rely on a single database and retriever, but financial applications require more sophisticated approaches to handle intricate regulatory filings, market analyses, and extensive multi-year reports. We introduce a framework for financial Retrieval Augmented Generation (RAG) that leverages agentic AI and the Multi-HyDE system, an approach that generates multiple, nonequivalent queries to boost the effectiveness and coverage of retrieval from large, structured financial corpora. Our pipeline is optimized for token efficiency and multi-step financial reasoning, and we demonstrate that their combination improves accuracy by 11.2% and reduces hallucinations by 15%. Our method is evaluated on standard financial QA benchmarks, showing that integrating domain-specific retrieval mechanisms such as Multi-HyDE with robust toolsets, including keyword and table-based retrieval, significantly enhances both the accuracy and reliability of answers. This research not only delivers a modular, adaptable retrieval framework for finance but also highlights the importance of structured agent workflows and multi-perspective retrieval for trustworthy deployment of AI in high-stakes financial applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16369v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Akshay Govind Srinivasan, Ryan Jacob George, Jayden Koshy Joe, Hrushikesh Kant, Harshith M R, Sachin Sundar, Sudharshan Suresh, Rahul Vimalkanth,  Vijayavallabh</dc:creator>
    </item>
    <item>
      <title>FairTune: A Bias-Aware Fine-Tuning Framework Towards Fair Heart Rate Prediction from PPG</title>
      <link>https://arxiv.org/abs/2509.16491</link>
      <description>arXiv:2509.16491v1 Announce Type: cross 
Abstract: Foundation models pretrained on physiological data such as photoplethysmography (PPG) signals are increasingly used to improve heart rate (HR) prediction across diverse settings. Fine-tuning these models for local deployment is often seen as a practical and scalable strategy. However, its impact on demographic fairness particularly under domain shifts remains underexplored. We fine-tune PPG-GPT a transformer-based foundation model pretrained on intensive care unit (ICU) data across three heterogeneous datasets (ICU, wearable, smartphone) and systematically evaluate the effects on HR prediction accuracy and gender fairness. While fine-tuning substantially reduces mean absolute error (up to 80%), it can simultaneously widen fairness gaps, especially in larger models and under significant distributional characteristics shifts. To address this, we introduce FairTune, a bias-aware fine-tuning framework in which we benchmark three mitigation strategies: class weighting based on inverse group frequency (IF), Group Distributionally Robust Optimization (GroupDRO), and adversarial debiasing (ADV). We find that IF and GroupDRO significantly reduce fairness gaps without compromising accuracy, with effectiveness varying by deployment domain. Representation analyses further reveal that mitigation techniques reshape internal embeddings to reduce demographic clustering. Our findings highlight that fairness does not emerge as a natural byproduct of fine-tuning and that explicit mitigation is essential for equitable deployment of physiological foundation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16491v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lovely Yeswanth Panchumarthi, Saurabh Kataria, Yi Wu, Xiao Hu, Alex Fedorov, Hyunjung Gloria Kwak</dc:creator>
    </item>
    <item>
      <title>Towards Cost-Effective ZK-Rollups: Modeling and Optimization of Proving Infrastructure</title>
      <link>https://arxiv.org/abs/2509.16581</link>
      <description>arXiv:2509.16581v1 Announce Type: cross 
Abstract: Zero-knowledge rollups rely on provers to generate multi-step state transition proofs under strict finality and availability constraints. These steps require expensive hardware (e.g., GPUs), and finality is reached only once all stages complete and results are posted on-chain. As rollups scale, staying economically viable becomes increasingly difficult due to rising throughput, fast finality demands, volatile gas prices, and dynamic resource needs. We base our study on Halo2-based proving systems and identify transactions per second (TPS), average gas usage, and finality time as key cost drivers. To address this, we propose a parametric cost model that captures rollup-specific constraints and ensures provers can keep up with incoming transaction load. We formulate this model as a constraint system and solve it using the Z3 SMT solver to find cost-optimal configurations. To validate our approach, we implement a simulator that detects lag and estimates operational costs. Our method shows a potential cost reduction of up to 70\%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16581v1</guid>
      <category>cs.CR</category>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Ahmadvand, Pedro Souto</dc:creator>
    </item>
    <item>
      <title>KANO: Kolmogorov-Arnold Neural Operator</title>
      <link>https://arxiv.org/abs/2509.16825</link>
      <description>arXiv:2509.16825v1 Announce Type: cross 
Abstract: We introduce Kolmogorov--Arnold Neural Operator (KANO), a dual-domain neural operator jointly parameterized by both spectral and spatial bases with intrinsic symbolic interpretability. We theoretically demonstrate that KANO overcomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO remains expressive over generic position-dependent dynamics for any physical input, whereas FNO stays practical only for spectrally sparse operators and strictly imposes a fast-decaying input Fourier tail. We verify our claims empirically on position-dependent differential operators, for which KANO robustly generalizes but FNO fails to. In the quantum Hamiltonian learning benchmark, KANO reconstructs ground-truth Hamiltonians in closed-form symbolic representations accurate to the fourth decimal place in coefficients and attains $\approx 6\times10^{-6}$ state infidelity from projective measurement data, substantially outperforming that of the FNO trained with ideal full wave function data, $\approx 1.5\times10^{-2}$, by orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16825v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jin Lee, Ziming Liu, Xinling Yu, Yixuan Wang, Haewon Jeong, Murphy Yuezhen Niu, Zheng Zhang</dc:creator>
    </item>
    <item>
      <title>Explainability matters: The effect of liability rules on the healthcare sector</title>
      <link>https://arxiv.org/abs/2509.17334</link>
      <description>arXiv:2509.17334v1 Announce Type: cross 
Abstract: Explainability, the capability of an artificial intelligence system (AIS) to explain its outcomes in a manner that is comprehensible to human beings at an acceptable level, has been deemed essential for critical sectors, such as healthcare. Is it really the case? In this perspective, we consider two extreme cases, ``Oracle'' (without explainability) versus ``AI Colleague'' (with explainability) for a thorough analysis. We discuss how the level of automation and explainability of AIS can affect the determination of liability among the medical practitioner/facility and manufacturer of AIS. We argue that explainability plays a crucial role in setting a responsibility framework in healthcare, from a legal standpoint, to shape the behavior of all involved parties and mitigate the risk of potential defensive medicine practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17334v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawen Wei, Elena Verona, Andrea Bertolini, Gianmarco Mengaldo</dc:creator>
    </item>
    <item>
      <title>An AutoML Framework using AutoGluonTS for Forecasting Seasonal Extreme Temperatures</title>
      <link>https://arxiv.org/abs/2509.17734</link>
      <description>arXiv:2509.17734v1 Announce Type: cross 
Abstract: In recent years, great progress has been made in the field of forecasting meteorological variables. Recently, deep learning architectures have made a major breakthrough in forecasting the daily average temperature over a ten-day horizon. However, advances in forecasting events related to the maximum temperature over short horizons remain a challenge for the community. A problem that is even more complex consists in making predictions of the maximum daily temperatures in the short, medium, and long term. In this work, we focus on forecasting events related to the maximum daily temperature over medium-term periods (90 days). Therefore, instead of addressing the problem from a meteorological point of view, this article tackles it from a climatological point of view. Due to the complexity of this problem, a common approach is to frame the study as a temporal classification problem with the classes: maximum temperature "above normal", "normal" or "below normal". From a practical point of view, we created a large historical dataset (from 1981 to 2018) collecting information from weather stations located in South America. In addition, we also integrated exogenous information from the Pacific, Atlantic, and Indian Ocean basins. We applied the AutoGluonTS platform to solve the above-mentioned problem. This AutoML tool shows competitive forecasting performance with respect to large operational platforms dedicated to tackling this climatological problem; but with a "relatively" low computational cost in terms of time and resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17734v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pablo Rodr\'iguez-Bocca, Guillermo Pereira, Diego Kiedanski, Soledad Collazo, Sebasti\'an Basterrech, Gerardo Rubino</dc:creator>
    </item>
    <item>
      <title>HRFT: Mining High-Frequency Risk Factor Collections End-to-End via Transformer</title>
      <link>https://arxiv.org/abs/2408.01271</link>
      <description>arXiv:2408.01271v5 Announce Type: replace 
Abstract: In quantitative trading, transforming historical stock data into interpretable, formulaic risk factors enhances the identification of market volatility and risk. Despite recent advancements in neural networks for extracting latent risk factors, these models remain limited to feature extraction and lack explicit, formulaic risk factor designs. By viewing symbolic mathematics as a language where valid mathematical expressions serve as meaningful "sentences" we propose framing the task of mining formulaic risk factors as a language modeling problem. In this paper, we introduce an end to end methodology, Intraday Risk Factor Transformer (IRFT), to directly generate complete formulaic risk factors, including constants. We use a hybrid symbolic numeric vocabulary where symbolic tokens represent operators and stock features, and numeric tokens represent constants. We train a Transformer model on high frequency trading (HFT) datasets to generate risk factors without relying on a predefined skeleton of operators. It determines the general form of the stock volatility law, including constants. We refine the predicted constants using the Broyden Fletcher Goldfarb Shanno (BFGS) algorithm to mitigate non linear issues. Compared to the ten approaches in SRBench, an active benchmark for symbolic regression (SR), IRFT achieves a 30% higher investment return on the HS300 and SP500 datasets, while achieving inference times that are orders of magnitude faster than existing methods in HF risk factor mining tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01271v5</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenyan Xu, Rundong Wang, Chen Li, Yonghong Hu, Zhonghua Lu</dc:creator>
    </item>
    <item>
      <title>StockGenChaR: A Study on the Evaluation of Large Vision-Language Models on Stock Chart Captioning</title>
      <link>https://arxiv.org/abs/2412.04041</link>
      <description>arXiv:2412.04041v2 Announce Type: replace 
Abstract: Technical analysis in finance, which aims at forecasting price movements in the future by analyzing past market data, relies on the in- sights that can be gained from the interpretation of stock charts; therefore, non-expert investors could greatly benefit from AI tools that can assist with the captioning of such charts. In our work, we introduce a new dataset StockGenChaR to evaluate large vision-language models in image captioning with stock charts. The purpose of the proposed task is to generate informative descriptions of the depicted charts and help to read the sentiment of the market regarding specific stocks, thus providing useful information for investors</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04041v2</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Le Qiu, Emmanuele Chersoni</dc:creator>
    </item>
    <item>
      <title>Mining Intraday Risk Factor Collections via Hierarchical Reinforcement Learning based on Transferred Options</title>
      <link>https://arxiv.org/abs/2501.07274</link>
      <description>arXiv:2501.07274v4 Announce Type: replace 
Abstract: Traditional risk factors like beta, size/value, and momentum often lag behind market dynamics in measuring and predicting stock return volatility. Statistical models like PCA and factor analysis fail to capture hidden nonlinear relationships. Genetic programming (GP) can identify nonlinear factors but often lacks mechanisms for evaluating factor quality, and the resulting formulas are complex. To address these challenges, we propose a Hierarchical Proximal Policy Optimization (HPPO) framework for automated factor generation and evaluation. HPPO uses two PPO models: a high-level policy assigns weights to stock features, and a low-level policy identifies latent nonlinear relationships. The Pearson correlation between generated factors and return volatility serves as the reward signal. Transfer learning pre-trains the high-level policy on large-scale historical data, fine-tuning it with the latest data to adapt to new features and shifts. Experiments show the HPPO-TO algorithm achieves a 25\% excess return in HFT markets across China (CSI 300/800), India (Nifty 100), and the US (S\&amp;P 500). Code and data are available at https://github.com/wencyxu/HRL-HF_risk_factor_set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07274v4</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenyan Xu, Jiayu Chen, Dawei Xiang, Chen Li, Yonghong Hu, Zhonghua Lu</dc:creator>
    </item>
    <item>
      <title>Multi-Objective Loss Balancing in Physics-Informed Neural Networks for Fluid Flow Applications</title>
      <link>https://arxiv.org/abs/2509.14437</link>
      <description>arXiv:2509.14437v2 Announce Type: replace 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising machine learning approach for solving partial differential equations (PDEs). However, PINNs face significant challenges in balancing multi-objective losses, as multiple competing loss terms such as physics residuals, boundary conditions, and initial conditions must be appropriately weighted. While various loss balancing schemes have been proposed, they have been implemented within neural network architectures with fixed activation functions, and their effectiveness has been assessed using simpler PDEs. We hypothesize that the effectiveness of loss balancing schemes depends not only on the balancing strategy itself, but also on the neural network's inherent function approximation capabilities, which are influenced by the choice of activation function. In this paper, we extend existing solutions by incorporating trainable activation functions within the neural network architecture and evaluate the proposed approach on complex fluid flow applications modeled by the Navier-Stokes equations. Our evaluation across diverse Navier-Stokes problems demonstrates that this proposed solution achieves root mean square error (RMSE) improvements ranging from 7.4% to 95.2% across different scenarios. These findings underscore the importance of carefully considering the interaction between activation function selection and balancing algorithms when designing loss balancing strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14437v2</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Afrah Farea, Saiful Khan, Mustafa Serdar Celebi</dc:creator>
    </item>
    <item>
      <title>Lagrangian-Eulerian Multiscale Data Assimilation in Physical Domain based on Conditional Gaussian Nonlinear System</title>
      <link>https://arxiv.org/abs/2509.14586</link>
      <description>arXiv:2509.14586v2 Announce Type: replace 
Abstract: This research aims to further investigate the process of Lagrangian-Eulerian Multiscale Data Assimilation (LEMDA) by replacing the Fourier space with the physical domain. Such change in the perspective of domain introduces the advantages of being able to deal in non-periodic system and more intuitive representation of localised phenomena or time-dependent problems. The context of the domain for this paper was set as sea ice floe trajectories to recover the ocean eddies in the Arctic regions, which led the model to be derived from two-layer Quasi geostrophic (QG) model. The numerical solution to this model utilises the Conditional Gaussian Nonlinear System (CGNS) to accommodate the inherent non-linearity in analytical and continuous manner. The normalised root mean square error (RMSE) and pattern correlation (Corr) are used to evaluate the performance of the posterior mean of the model. The results corroborate the effectiveness of exploiting the two-layer QG model in physical domain. Nonetheless, the paper still discusses opportunities of improvement, such as deploying neural network (NN) to accelerate the recovery of local particle of Lagrangian DA for the fine scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14586v2</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyeonggeun Yun, Quanling Deng</dc:creator>
    </item>
    <item>
      <title>TinyDef-DETR: A DETR-based Framework for Defect Detection in Transmission Lines from UAV Imagery</title>
      <link>https://arxiv.org/abs/2509.06035</link>
      <description>arXiv:2509.06035v5 Announce Type: replace-cross 
Abstract: Automated defect detection from UAV imagery of transmission lines is a challenging task due to the small size, ambiguity, and complex backgrounds of defects. This paper proposes TinyDef-DETR, a DETR-based framework designed to achieve accurate and efficient detection of transmission line defects from UAV-acquired images. The model integrates four major components: an edge-enhanced ResNet backbone to strengthen boundary-sensitive representations, a stride-free space-to-depth module to enable detail-preserving downsampling, a cross-stage dual-domain multi-scale attention mechanism to jointly model global context and local cues, and a Focaler-Wise-SIoU regression loss to improve the localization of small and difficult targets. Together, these designs effectively mitigate the limitations of conventional detectors. Extensive experiments on both public and real-world datasets demonstrate that TinyDef-DETR achieves superior detection performance and strong generalization capability, while maintaining modest computational overhead. The accuracy and efficiency of TinyDef-DETR make it a suitable method for UAV-based transmission line defect detection, particularly in scenarios involving small and ambiguous targets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06035v5</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Shen, Jiaming Cui, Shuai Zhou, Wenqiang Li, Ruifeng Qin</dc:creator>
    </item>
    <item>
      <title>Towards deep-learning based detection and quantification of intestinal metaplasia on digitized gastric biopsies: a multi-expert comparative study</title>
      <link>https://arxiv.org/abs/2509.06991</link>
      <description>arXiv:2509.06991v2 Announce Type: replace-cross 
Abstract: Current gastric cancer (GCa) risk systems are prone to errors since they evaluate a visual estimation of intestinal metaplasia percentages in histopathology images of gastric mucosa to assign a risk. This study presents an automated method to detect and quantify intestinal metaplasia using deep convolutional neural networks as well as a comparative analysis with visual estimations of three experienced pathologists. Gastric samples were collected from two different cohorts: 149 asymptomatic volunteers from a region with a high prevalence of GCa in Colombia and 56 patients from a tertiary hospital. Deep learning models were trained to classify intestinal metaplasia, and predictions were used to estimate the percentage of intestinal metaplasia and assign the OLGIM risk score. Results were compared with independent blinded metaplastic assessments performed by three experienced pathologists. The best-performing deep learning architecture classified intestinal metaplasia with F1-Score of 0.80 +- 0.01 and AUC of 0.91 +- 0.01. Among pathologists, inter-observer agreement by a Fleiss's Kappa score ranged from 0.20 to 0.48. In comparison, agreement between the pathologists and the best-performing model ranged from 0.12 to 0.35. Deep learning models show potential to reliably detect and quantify the percentage of intestinal metaplasia, achieving high classification performance. Visual estimation of intestinal metaplasia remains highly dependent on individual expertise, resulting in inter-observer variability. Deep learning models provide consistent estimates that could help reduce this subjectivity in risk stratification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06991v2</guid>
      <category>q-bio.TO</category>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fabian Cano, Mauricio Caviedes, Andres Siabatto, Jesus Villarreal, Jose Quijano, \'Alvaro Bedoya-Urresta, Marino Coral Bedoya, Yomaira Yepez Caicedo, Angel Cruz-Roa, Fabio A. Gonz\'alez, Satish E. Viswanath, Eduardo Romero</dc:creator>
    </item>
  </channel>
</rss>
