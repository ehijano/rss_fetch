<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Mar 2024 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Private graphon estimation via sum-of-squares</title>
      <link>https://arxiv.org/abs/2403.12213</link>
      <description>arXiv:2403.12213v1 Announce Type: new 
Abstract: We develop the first pure node-differentially-private algorithms for learning stochastic block models and for graphon estimation with polynomial running time for any constant number of blocks. The statistical utility guarantees match those of the previous best information-theoretic (exponential-time) node-private mechanisms for these problems. The algorithm is based on an exponential mechanism for a score function defined in terms of a sum-of-squares relaxation whose level depends on the number of blocks. The key ingredients of our results are (1) a characterization of the distance between the block graphons in terms of a quadratic optimization over the polytope of doubly stochastic matrices, (2) a general sum-of-squares convergence result for polynomial optimization over arbitrary polytopes, and (3) a general approach to perform Lipschitz extensions of score functions as part of the sum-of-squares algorithmic paradigm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12213v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongjie Chen, Jingqiu Ding, Tommaso d'Orsi, Yiding Hua, Chih-Hung Liu, David Steurer</dc:creator>
    </item>
    <item>
      <title>Revisiting Local Computation of PageRank: Simple and Optimal</title>
      <link>https://arxiv.org/abs/2403.12648</link>
      <description>arXiv:2403.12648v1 Announce Type: new 
Abstract: We revisit the classic local graph exploration algorithm ApproxContributions proposed by Andersen, Borgs, Chayes, Hopcroft, Mirrokni, and Teng (WAW '07, Internet Math. '08) for computing an $\epsilon$-approximation of the PageRank contribution vector for a target node $t$ on a graph with $n$ nodes and $m$ edges. We give a worst-case complexity bound of ApproxContributions as $O(n\pi(t)/\epsilon\cdot\min(\Delta_{in},\Delta_{out},\sqrt{m}))$, where $\pi(t)$ is the PageRank score of $t$, and $\Delta_{in}$ and $\Delta_{out}$ are the maximum in-degree and out-degree of the graph, resp. We also give a lower bound of $\Omega(\min(\Delta_{in}/\delta,\Delta_{out}/\delta,\sqrt{m}/\delta,m))$ for detecting the $\delta$-contributing set of $t$, showing that the simple ApproxContributions algorithm is already optimal.
  We also investigate the computational complexity of locally estimating a node's PageRank centrality. We improve the best-known upper bound of $\widetilde{O}(n^{2/3}\cdot\min(\Delta_{out}^{1/3},m^{1/6}))$ given by Bressan, Peserico, and Pretto (SICOMP '23) to $O(n^{1/2}\cdot\min(\Delta_{in}^{1/2},\Delta_{out}^{1/2},m^{1/4}))$ by simply combining ApproxContributions with the Monte Carlo simulation method. We also improve their lower bound of $\Omega(\min(n^{1/2}\Delta_{out}^{1/2},n^{1/3}m^{1/3}))$ to $\Omega(n^{1/2}\cdot\min(\Delta_{in}^{1/2},\Delta_{out}^{1/2},m^{1/4}))$ if $\min(\Delta_{in},\Delta_{out})=\Omega(n^{1/3})$, and to $\Omega(n^{1/2-\gamma}(\min(\Delta_{in},\Delta_{out}))^{1/2+\gamma})$ if $\min(\Delta_{in},\Delta_{out})=o(n^{1/3})$, where $\gamma&gt;0$ is an arbitrarily small constant. Our matching upper and lower bounds resolve the open problem of whether one can tighten the bounds given by Bressan, Peserico, and Pretto (FOCS '18, SICOMP '23). Remarkably, the techniques and analyses for proving all our results are surprisingly simple.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12648v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanzhi Wang, Zhewei Wei, Ji-Rong Wen, Mingji Yang</dc:creator>
    </item>
    <item>
      <title>Exact and Heuristic Computation of the Scanwidth of Directed Acyclic Graphs</title>
      <link>https://arxiv.org/abs/2403.12734</link>
      <description>arXiv:2403.12734v1 Announce Type: new 
Abstract: To measure the tree-likeness of a directed acyclic graph (DAG), a new width parameter that considers the directions of the arcs was recently introduced: scanwidth. We present the first algorithm that efficiently computes the exact scanwidth of general DAGs. For DAGs with one root and scanwidth $k$ it runs in $O(k \cdot n^k \cdot m)$ time. The algorithm also functions as an FPT algorithm with complexity $O(2^{4 \ell - 1} \cdot \ell \cdot n + n^2)$ for phylogenetic networks of level-$\ell$, a type of DAG used to depict evolutionary relationships among species. Our algorithm performs well in practice, being able to compute the scanwidth of synthetic networks up to 30 reticulations and 100 leaves within 500 seconds. Furthermore, we propose a heuristic that obtains an average practical approximation ratio of 1.5 on these networks. While we prove that the scanwidth is bounded from below by the treewidth of the underlying undirected graph, experiments suggest that for networks the parameters are close in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12734v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niels Holtgrefe, Leo van Iersel, Mark Jones</dc:creator>
    </item>
    <item>
      <title>PackIt! Gamified Rectangle Packing</title>
      <link>https://arxiv.org/abs/2403.12195</link>
      <description>arXiv:2403.12195v1 Announce Type: cross 
Abstract: We present and analyze PackIt!, a turn-based game consisting of packing rectangles on an $n \times n$ grid. PackIt! can be easily played on paper, either as a competitive two-player game or in \emph{solitaire} fashion. On the $t$-th turn, a rectangle of area $t$ or $t+1$ must be placed in the grid. In the two-player format of PackIt! whichever player places a rectangle last wins, whereas the goal in the solitaire variant is to perfectly pack the $n \times n$ grid. We analyze conditions for the existence of a perfect packing over $n \times n$, then present an automated reasoning approach that allows finding perfect games of PackIt! up to $n = 50$ which includes a novel SAT-encoding technique of independent interest, and conclude by proving an NP-hardness result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12195v1</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Garrison, Marijn J. H. Heule, Bernardo Subercaseaux</dc:creator>
    </item>
    <item>
      <title>Closing the Gap Between Directed Hopsets and Shortcut Sets</title>
      <link>https://arxiv.org/abs/2207.04507</link>
      <description>arXiv:2207.04507v4 Announce Type: replace 
Abstract: For an n-vertex directed graph $G = (V,E)$, a $\beta$-\emph{shortcut set} $H$ is a set of additional edges $H \subseteq V \times V$ such that $G \cup H$ has the same transitive closure as $G$, and for every pair $u,v \in V$, there is a $uv$-path in $G \cup H$ with at most $\beta$ edges. A natural generalization of shortcut sets to distances is a $(\beta,\epsilon)$-\emph{hopset} $H \subseteq V \times V$, where the requirement is that $H$ and $G \cup H$ have the same shortest-path distances, and for every $u,v \in V$, there is a $(1+\epsilon)$-approximate shortest path in $G \cup H$ with at most $\beta$ edges.
  There is a large literature on the tradeoff between the size of a shortcut set / hopset and the value of $\beta$. We highlight the most natural point on this tradeoff: what is the minimum value of $\beta$, such that for any graph $G$, there exists a $\beta$-shortcut set (or a $(\beta,\epsilon)$-hopset) with $O(n)$ edges? Not only is this a natural structural question in its own right, but shortcuts sets / hopsets form the core of many distributed, parallel, and dynamic algorithms for reachability / shortest paths. Until very recently the best known upper bound was a folklore construction showing $\beta = O(n^{1/2})$, but in a breakthrough result Kogan and Parter [SODA 2022] improve this to $\beta = \tilde{O}(n^{1/3})$ for shortcut sets and $\tilde{O}(n^{2/5})$ for hopsets.
  Our result is to close the gap between shortcut sets and hopsets. That is, we show that for any graph $G$ and any fixed $\epsilon$ there is a $(\tilde{O}(n^{1/3}),\epsilon)$ hopset with $O(n)$ edges. More generally, we achieve a smooth tradeoff between hopset size and $\beta$ which exactly matches the tradeoff of Kogan and Parter for shortcut sets (up to polylog factors).
  Using a very recent black-box reduction of Kogan and Parter, our new hopset implies improved bounds for approximate distance preservers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.04507v4</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaron Bernstein, Nicole Wein</dc:creator>
    </item>
    <item>
      <title>Attribute-Efficient PAC Learning of Low-Degree Polynomial Threshold Functions with Nasty Noise</title>
      <link>https://arxiv.org/abs/2306.00673</link>
      <description>arXiv:2306.00673v2 Announce Type: replace 
Abstract: The concept class of low-degree polynomial threshold functions (PTFs) plays a fundamental role in machine learning. In this paper, we study PAC learning of $K$-sparse degree-$d$ PTFs on $\mathbb{R}^n$, where any such concept depends only on $K$ out of $n$ attributes of the input. Our main contribution is a new algorithm that runs in time $({nd}/{\epsilon})^{O(d)}$ and under the Gaussian marginal distribution, PAC learns the class up to error rate $\epsilon$ with $O(\frac{K^{4d}}{\epsilon^{2d}} \cdot \log^{5d} n)$ samples even when an $\eta \leq O(\epsilon^d)$ fraction of them are corrupted by the nasty noise of Bshouty et al. (2002), possibly the strongest corruption model. Prior to this work, attribute-efficient robust algorithms are established only for the special case of sparse homogeneous halfspaces. Our key ingredients are: 1) a structural result that translates the attribute sparsity to a sparsity pattern of the Chow vector under the basis of Hermite polynomials, and 2) a novel attribute-efficient robust Chow vector estimation algorithm which uses exclusively a restricted Frobenius norm to either certify a good approximation or to validate a sparsity-induced degree-$2d$ polynomial as a filter to detect corrupted samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.00673v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiwei Zeng, Jie Shen</dc:creator>
    </item>
    <item>
      <title>Hyper-distance Oracles in Hypergraphs</title>
      <link>https://arxiv.org/abs/2306.02696</link>
      <description>arXiv:2306.02696v2 Announce Type: replace 
Abstract: We study point-to-point distance estimation in hypergraphs, where the query is parameterized by a positive integer s, which defines the required level of overlap for two hyperedges to be considered adjacent. To answer s-distance queries, we first explore an oracle based on the line graph of the given hypergraph and discuss its limitations: the main one is that the line graph is typically orders of magnitude larger than the original hypergraph. We then introduce HypED, a landmark-based oracle with a predefined size, built directly on the hypergraph, thus avoiding constructing the line graph. Our framework allows to approximately answer vertex-to-vertex, vertex-to-hyperedge, and hyperedge-to-hyperedge s-distance queries for any value of s. A key observation at the basis of our framework is that, as s increases, the hypergraph becomes more fragmented. We show how this can be exploited to improve the placement of landmarks, by identifying the s-connected components of the hypergraph. For this task, we devise an efficient algorithm based on the union-find technique and a dynamic inverted index. We experimentally evaluate HypED on several real-world hypergraphs and prove its versatility in answering s-distance queries for different values of s. Our framework allows answering such queries in fractions of a millisecond, while allowing fine-grained control of the trade-off between index size and approximation error at creation time. Finally, we prove the usefulness of the s-distance oracle in two applications, namely, hypergraph-based recommendation and the approximation of the s-closeness centrality of vertices and hyper-edges in the context of protein-to-protein interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.02696v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giulia Preti, Gianmarco De Francisci Morales, Francesco Bonchi</dc:creator>
    </item>
    <item>
      <title>Grafite: Taming Adversarial Queries with Optimal Range Filters</title>
      <link>https://arxiv.org/abs/2311.15380</link>
      <description>arXiv:2311.15380v2 Announce Type: replace 
Abstract: Range filters allow checking whether a query range intersects a given set of keys with a chance of returning a false positive answer, thus generalising the functionality of Bloom filters from point to range queries. Existing practical range filters have addressed this problem heuristically, resulting in high false positive rates and query times when dealing with adversarial inputs, such as in the common scenario where queries are correlated with the keys.
  We introduce Grafite, a novel range filter that solves these issues with a simple design and clear theoretical guarantees that hold regardless of the input data and query distribution: given a fixed space budget of $B$ bits per key, the query time is $O(1)$, and the false positive probability is upper bounded by $\ell/2^{B-2}$, where $\ell$ is the query range size. Our experimental evaluation shows that Grafite is the only range filter to date to achieve robust and predictable false positive rates across all combinations of datasets, query workloads, and range sizes, while providing faster queries and construction times, and dominating all competitors in the case of correlated queries.
  As a further contribution, we introduce a very simple heuristic range filter whose performance on uncorrelated queries is very close to or better than the one achieved by the best heuristic range filters proposed in the literature so far.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15380v2</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marco Costa, Paolo Ferragina, Giorgio Vinciguerra</dc:creator>
    </item>
    <item>
      <title>Perfect Zero-Knowledge PCPs for #P</title>
      <link>https://arxiv.org/abs/2403.11941</link>
      <description>arXiv:2403.11941v2 Announce Type: replace-cross 
Abstract: We construct perfect zero-knowledge probabilistically checkable proofs (PZK-PCPs) for every language in #P. This is the first construction of a PZK-PCP for any language outside BPP. Furthermore, unlike previous constructions of (statistical) zero-knowledge PCPs, our construction simultaneously achieves non-adaptivity and zero knowledge against arbitrary (adaptive) polynomial-time malicious verifiers.
  Our construction consists of a novel masked sumcheck PCP, which uses the combinatorial nullstellensatz to obtain antisymmetric structure within the hypercube and randomness outside of it. To prove zero knowledge, we introduce the notion of locally simulatable encodings: randomised encodings in which every local view of the encoding can be efficiently sampled given a local view of the message. We show that the code arising from the sumcheck protocol (the Reed-Muller code augmented with subcube sums) admits a locally simulatable encoding. This reduces the algebraic problem of simulating our masked sumcheck to a combinatorial property of antisymmetric functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11941v2</guid>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Gur, Jack O'Connor, Nicholas Spooner</dc:creator>
    </item>
  </channel>
</rss>
