<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Apr 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>MMformer with Adaptive Transferable Attention: Advancing Multivariate Time Series Forecasting for Environmental Applications</title>
      <link>https://arxiv.org/abs/2504.14050</link>
      <description>arXiv:2504.14050v1 Announce Type: new 
Abstract: Environmental crisis remains a global challenge that affects public health and environmental quality. Despite extensive research, accurately forecasting environmental change trends to inform targeted policies and assess prediction efficiency remains elusive. Conventional methods for multivariate time series (MTS) analysis often fail to capture the complex dynamics of environmental change. To address this, we introduce an innovative meta-learning MTS model, MMformer with Adaptive Transferable Multi-head Attention (ATMA), which combines self-attention and meta-learning for enhanced MTS forecasting. Specifically, MMformer is used to model and predict the time series of seven air quality indicators across 331 cities in China from January 2018 to June 2021 and the time series of precipitation and temperature at 2415 monitoring sites during the summer (276 days) from 2012 to 2014, validating the network's ability to perform and forecast MTS data successfully. Experimental results demonstrate that in these datasets, the MMformer model reaching SOTA outperforms iTransformer, Transformer, and the widely used traditional time series prediction algorithm SARIMAX in the prediction of MTS, reducing by 50\% in MSE, 20\% in MAE as compared to others in air quality datasets, reducing by 20\% in MAPE except SARIMAX. Compared with Transformer and SARIMAX in the climate datasets, MSE, MAE, and MAPE are decreased by 30\%, and there is an improvement compared to iTransformer. This approach represents a significant advance in our ability to forecast and respond to dynamic environmental quality challenges in diverse urban and rural environments. Its predictive capabilities provide valuable public health and environmental quality information, informing targeted interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14050v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ning Xin, Jionglong Su, Md Maruf Hasan</dc:creator>
    </item>
    <item>
      <title>Dose optimization design accounting for unknown patient heterogeneity in cancer clinical trials</title>
      <link>https://arxiv.org/abs/2504.14622</link>
      <description>arXiv:2504.14622v1 Announce Type: new 
Abstract: Project Optimus, an initiative by the FDA's Oncology Center of Excellence, seeks to reform the dose-optimization and dose-selection paradigm in oncology. We propose a dose-optimization design that considers plateau efficacy profiles, integrates pharmacokinetic data to inform the exposure-toxicity curve, and accounts for patient characteristics that may contribute to heterogeneity in response. The dose-optimization design is carried out in two stages. First, a toxicity-driven stage estimates a safe set of doses. Then, a dose-ranging efficacy-driven stage explores the set using response and patient characteristic data, employing Bayesian Sparse Group Selection to understand patient heterogeneity. Between stages, the design integrates pharmacokinetic data and uses futility assessments to identify the target population among the general phase I patient population. An optimal dose is recommended for each identified subpopulation within the target population. The simulation study demonstrates that a model-based approach to identifying the target population can be effective; patient characteristics relating to heterogeneity were identified and different optimal doses were recommended for each identified target subpopulation. Most designs that account for patient heterogeneity are intended for trials where heterogeneity is known and pre-defined subpopulations are specified. However, given the limited information at such an early stage, subpopulations should be learned through the design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14622v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rebecca B. Silva, Bin Cheng, Shing M. Lee</dc:creator>
    </item>
    <item>
      <title>Integrating Response Time and Attention Duration in Bayesian Preference Learning for Multiple Criteria Decision Aiding</title>
      <link>https://arxiv.org/abs/2504.14938</link>
      <description>arXiv:2504.14938v1 Announce Type: new 
Abstract: We introduce a multiple criteria Bayesian preference learning framework incorporating behavioral cues for decision aiding. The framework integrates pairwise comparisons, response time, and attention duration to deepen insights into decision-making processes. The approach employs an additive value function model and utilizes a Bayesian framework to derive the posterior distribution of potential ranking models by defining the likelihood of observed preference data and specifying a prior on the preference structure. This distribution highlights each model's ability to reconstruct Decision-Makers' holistic pairwise comparisons. By leveraging both response time as a proxy for cognitive effort and alternative discriminability as well as attention duration as an indicator of criterion importance, the proposed model surpasses traditional methods by uncovering richer behavioral patterns. We report the results of a laboratory experiment on mobile phone contract selection involving 30 real subjects using a dedicated application with time-, eye-, and mouse-tracking components. We validate the novel method's ability to reconstruct complete preferences. The detailed ablation studies reveal time- and attention-related behavioral patterns, confirming that integrating comprehensive data leads to developing models that better align with the DM's actual preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14938v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxuan Jiang, Jiapeng Liu, Mi{\l}osz Kadzi\'nski, Xiuwu Liao, Jingyu Dong</dc:creator>
    </item>
    <item>
      <title>AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience</title>
      <link>https://arxiv.org/abs/2504.13908</link>
      <description>arXiv:2504.13908v1 Announce Type: cross 
Abstract: Standardized surveys scale efficiently but sacrifice depth, while conversational interviews improve response quality at the cost of scalability and consistency. This study bridges the gap between these methods by introducing a framework for AI-assisted conversational interviewing. To evaluate this framework, we conducted a web survey experiment where 1,800 participants were randomly assigned to text-based conversational AI agents, or "textbots", to dynamically probe respondents for elaboration and interactively code open-ended responses. We assessed textbot performance in terms of coding accuracy, response quality, and respondent experience. Our findings reveal that textbots perform moderately well in live coding even without survey-specific fine-tuning, despite slightly inflated false positive errors due to respondent acquiescence bias. Open-ended responses were more detailed and informative, but this came at a slight cost to respondent experience. Our findings highlight the feasibility of using AI methods to enhance open-ended data collection in web surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13908v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soubhik Barari, Jarret Angbazo, Natalie Wang, Leah M. Christian, Elizabeth Dean, Zoe Slowinski, Brandon Sepulvado</dc:creator>
    </item>
    <item>
      <title>Prognosis Of Lithium-Ion Battery Health with Hybrid EKF-CNN+LSTM Model Using Differential Capacity</title>
      <link>https://arxiv.org/abs/2504.13956</link>
      <description>arXiv:2504.13956v1 Announce Type: cross 
Abstract: Battery degradation is a major challenge in electric vehicles (EV) and energy storage systems (ESS). However, most degradation investigations focus mainly on estimating the state of charge (SOC), which fails to accurately interpret the cells' internal degradation mechanisms. Differential capacity analysis (DCA) focuses on the rate of change of cell voltage about the change in cell capacity, under various charge/discharge rates. This paper developed a battery cell degradation testing model that used two types of lithium-ions (Li-ion) battery cells, namely lithium nickel cobalt aluminium oxides (LiNiCoAlO2) and lithium iron phosphate (LiFePO4), to evaluate internal degradation during loading conditions. The proposed battery degradation model contains distinct charge rates (DCR) of 0.2C, 0.5C, 1C, and 1.5C, as well as discharge rates (DDR) of 0.5C, 0.9C, 1.3C, and 1.6C to analyze the internal health and performance of battery cells during slow, moderate, and fast loading conditions. Besides, this research proposed a model that incorporates the Extended Kalman Filter (EKF), Convolutional Neural Network (CNN), and Long Short-Term Memory (LSTM) networks to validate experimental data. The proposed model yields excellent modelling results based on mean squared error (MSE), and root mean squared error (RMSE), with errors of less than 0.001% at DCR and DDR. The peak identification technique (PIM) has been utilized to investigate battery health based on the number of peaks, peak position, peak height, peak area, and peak width. At last, the PIM method has discovered that the cell aged gradually under normal loading rates but deteriorated rapidly under fast loading conditions. Overall, LiFePO4 batteries perform more robustly and consistently than (LiNiCoAlO2) cells under varying loading conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13956v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Azizul Hoque, Babul Salam, Mohd Khair Hassan, Abdulkabir Aliyu, Abedalmuhdi Almomany, Muhammed Sutcu</dc:creator>
    </item>
    <item>
      <title>Statistical Analysis and End-to-End Performance Evaluation of Traffic Models for Automotive Data</title>
      <link>https://arxiv.org/abs/2504.14017</link>
      <description>arXiv:2504.14017v1 Announce Type: cross 
Abstract: Autonomous driving is a major paradigm shift in transportation, with the potential to enhance safety, optimize traffic congestion, and reduce fuel consumption. Although autonomous vehicles rely on advanced sensors and on-board computing systems to navigate without human control, full awareness of the driving environment also requires a cooperative effort via Vehicle-To-Everything (V2X) communication. Specifically, vehicles send and receive sensor perceptions to/from other vehicles to extend perception beyond their own sensing range. However, transmitting large volumes of data can be challenging for current V2X communication technologies, so data compression represents a crucial solution to reduce the message size and link congestion. In this paper, we present a statistical characterization of automotive data, focusing on LiDAR sensors. Notably, we provide models for the size of both raw and compressed point clouds. The use of statistical traffic models offers several advantages compared to using real data, such as faster simulations, reduced storage requirements, and greater flexibility in the application design. Furthermore, statistical models can be used for understanding traffic patterns and analyzing statistics, which is crucial to design and optimize wireless networks. We validate our statistical models via a Kolmogorov-Smirnoff test implementing a Bootstrap Resampling scheme. Moreover, we show via ns-3 simulations that using statistical models yields comparable results in terms of latency and throughput compared to real data, which also demonstrates the accuracy of the models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14017v1</guid>
      <category>cs.NI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcello Bullo, Amir Ashtari Gargari, Paolo Testolina, Michele Zorzi, Marco Giordani</dc:creator>
    </item>
    <item>
      <title>A New Generalized Fisk distribution: Its Properties, Characterizations and Applications</title>
      <link>https://arxiv.org/abs/2504.14179</link>
      <description>arXiv:2504.14179v1 Announce Type: cross 
Abstract: The shortcomings of the traditional univariate distributions in the past greatly encouraged mathematical statisticians to develop new generalizations of distributions. The New Generalized Fisk distribution, a unique distribution presented in this study, is thoroughly examined. There is a thorough discussion of a few fundamental statistical traits and attributes, such as the quantile function, order statistics, skewness and kurtosis, moments, and moment-generating functions. The new distribution incorporates additional parameters, enhancing its ability to capture a wide range of skewness and kurtosis behaviors, making it applicable to diverse fields such as economics, reliability engineering, and environmental sciences. Both numerical and graphical evaluations are used to evaluate the performance of the recently proposed model. Additionally, the performance of the maximum likelihood estimators is assessed by a simulation study. Real-world applications are analyzed, and the model parameters are estimated using the maximum likelihood estimation technique. It is contrasted with the popular models of computing. According to model adequacy and discrimination approaches, the suggested model performs the best. The models are compared to choose the model that best fits the data with the essential characteristics. The graphical and model comparison approaches suggested an outstanding improvement in the combined distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14179v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Veeranna Banoth</dc:creator>
    </item>
    <item>
      <title>Evidence of Donor Bias in Chicago Police Stops</title>
      <link>https://arxiv.org/abs/2504.15140</link>
      <description>arXiv:2504.15140v1 Announce Type: cross 
Abstract: This study provides the first empirical evidence that private donations to police departments can influence officer behavior. Drawing on the psychology of reciprocity bias, we theorize that public donations create social debts that shape discretionary enforcement. Using quasi-experimental data from Chicago, we find that after 7-Eleven sponsored a police foundation gala, investigatory stops, particularly of Black pedestrians, increased around its stores. These findings reveal a racialized pattern of donor bias in policing and call into question the consequences of private donations to public law enforcement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15140v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angela Zorro Medina, David Hackett, Devin Green, Robert Vargas</dc:creator>
    </item>
    <item>
      <title>Position: Bayesian Statistics Facilitates Stakeholder Participation in Evaluation of Generative AI</title>
      <link>https://arxiv.org/abs/2504.15211</link>
      <description>arXiv:2504.15211v1 Announce Type: cross 
Abstract: The evaluation of Generative AI (GenAI) systems plays a critical role in public policy and decision-making, yet existing methods are often limited by reliance on benchmark-driven, point-estimate comparisons that fail to capture uncertainty and broader societal impacts. This paper argues for the use of Bayesian statistics as a principled framework to address these challenges. Bayesian methods enable the integration of domain expertise through prior elicitation, allow for continuous learning from new data, and provide robust uncertainty quantification via posterior inference. We demonstrate how Bayesian inference can be applied to GenAI evaluation, particularly in incorporating stakeholder perspectives to enhance fairness, transparency, and reliability. Furthermore, we discuss Bayesian workflows as an iterative process for model validation and refinement, ensuring robust assessments of GenAI systems in dynamic, real-world contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15211v1</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yanan Long</dc:creator>
    </item>
    <item>
      <title>Beating the Correlation Breakdown: Robust Inference, Flexible Scenarios, and Stress Testing for Financial Portfolios</title>
      <link>https://arxiv.org/abs/2504.15268</link>
      <description>arXiv:2504.15268v1 Announce Type: cross 
Abstract: We live in a multivariate world, and effective modeling of financial portfolios, including their construction, allocation, forecasting, and risk analysis, simply is not possible without explicitly modeling the dependence structure of their assets. Dependence structure can drive portfolio results more than many other parameters in investment and risk models, sometimes even more than their combined effects, but the literature provides relatively little to define the finite-sample distributions of dependence measures in useable and useful ways under challenging, real-world financial data conditions. Yet this is exactly what is needed to make valid inferences about their estimates, and to use these inferences for a myriad of essential purposes, such as hypothesis testing, dynamic monitoring, realistic and granular scenario and reverse scenario analyses, and mitigating the effects of correlation breakdowns during market upheavals (which is when we need valid inferences the most). This work develops a new and straightforward method, Nonparametric Angles-based Correlation (NAbC), for defining the finite-sample distributions of any dependence measure whose matrix of pairwise associations is positive definite (e.g. Pearsons, Kendalls Tau, Spearmans Rho, Chatterjees, Lancasters, Szekelys, and their many variants). The solution remains valid under marginal asset distributions characterized by notably different and varying degrees of serial correlation, non-stationarity, heavy-tailedness, and asymmetry. Notably, NAbCs p-values and confidence intervals remain analytically consistent at both the matrix level and the pairwise cell level. Finally, NAbC maintains validity even when selected cells in the matrix are frozen for a given scenario or stress test, that is, unaffected by the scenario, thus enabling flexible, granular, and realistic scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15268v1</guid>
      <category>q-fin.RM</category>
      <category>q-fin.PM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>JD Opdyke</dc:creator>
    </item>
    <item>
      <title>Early Detection of Treatments Side Effect: A Sequential Approach</title>
      <link>https://arxiv.org/abs/2401.13760</link>
      <description>arXiv:2401.13760v2 Announce Type: replace 
Abstract: With the emergence and spread of infectious diseases with pandemic potential, such as COVID- 19, the urgency for vaccine development have led to unprecedented compressed and accelerated schedules that shortened the standard development timeline. In a relatively short time, the leading pharmaceutical companies1, received an Emergency Use Authorization (EUA) for vaccine\prime s en-mass deployment To monitor the potential side effect(s) of the vaccine during the (initial) vaccination campaign, we developed an optimal sequential test that allows for the early detection of potential side effect(s). This test employs a rule to stop the vaccination process once the observed number of side effect incidents exceeds a certain (pre-determined) threshold. The optimality of the proposed sequential test is justified when compared with the ({\alpha}, {\beta}) optimality of the non-randomized fixed-sample Uniformly Most Powerful (UMP) test. In the case of a single side effect, we study the properties of the sequential test and derive the exact expressions of the Average Sample Number (ASN) curve of the stopping time (and its variance) via the regularized incomplete beta function. Additionally, we derive the asymptotic distribution of the relative savings in ASN as compared to maximal sample size. Moreover, we construct the post-test parameter estimate and studied its sampling properties, including its asymptotic behavior under local-type alternatives. These limiting behavior results are the consistency and asymptotic normality of the post-test parameter estimator. We conclude the paper with a small simulation study illustrating the asymptotic performance of the point and interval estimation and provide a detailed example, based on COVID-19 side effect data (see Beatty et al. (2021)) of our suggested testing procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13760v2</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayue Wang, Ben Boukai</dc:creator>
    </item>
    <item>
      <title>Devising PoPStat: A Metric Bridging Population Pyramids with Global Disease Mortality</title>
      <link>https://arxiv.org/abs/2501.11514</link>
      <description>arXiv:2501.11514v2 Announce Type: replace 
Abstract: Understanding the relationship between population dynamics and disease-specific mortality is central to evidence-based health policy. This study introduces two novel metrics, PoPDivergence and PoPStat, one to quantify the difference between population pyramids and the other to assess the strength and nature of their association with the mortality of a given disease. PoPDivergence, based on Kullback-Leibler divergence, measures deviations between a countrys population pyramid and a reference pyramid. PoPStat is the correlation between these deviations and the log form of disease-specific mortality rates. The reference population is selected by a brute-force optimization that maximizes this correlation. Utilizing mortality data from the Global Burden of Disease 2021 and population statistics from the United Nations, we applied these metrics to 371 diseases across 204 countries. Results reveal that PoPStat outperforms traditional indicators such as median age, GDP per capita, and Human Development Index in explaining the mortality of most diseases. Noncommunicable diseases (NCDs) like neurological disorders and cancers, communicable diseases (CDs) like neglected tropical diseases, and maternal and neonatal diseases were tightly bound to the underlying demographic attributes whereas NCDs like diabetes, CDs like respiratory infections and injuries including self-harm and interpersonal violence were weakly associated with population pyramid shapes. Notably, except for diabetes, the NCD mortality burden was shared by constrictive population pyramids, while mortality of communicable diseases, maternal and neonatal causes and injuries were largely borne by expansive pyramids. Therefore, PoPStat provides insights into demographic determinants of health and empirical support for models on epidemiological transition. Code and scripts: https://github.com/Buddhi19/DevisingPoPStat.git</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11514v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tharaka Fonseka, Buddhi Wijenayake, Athulya Ratnayake, Inosha Alwis, Supun Manathunga, Roshan Godaliyadda, Samath Dharmarathne, Vijitha Herath, Parakrama Ekanayake, Isuru Pamuditha</dc:creator>
    </item>
    <item>
      <title>A Spatiotemporal, Quasi-experimental Causal Inference Approach to Characterize the Effects of Global Plastic Waste Export and Burning on Air Quality Using Remotely Sensed Data</title>
      <link>https://arxiv.org/abs/2503.04491</link>
      <description>arXiv:2503.04491v2 Announce Type: replace 
Abstract: Open burning of plastic waste may pose a significant threat to global health by degrading air quality, but quantitative research on this problem -- crucial for policy making -- has been stunted by lack of data. Critically, many low- and middle-income countries, where open burning is of greatest concern, have little to no air quality monitoring. Here, we propose an approach to leverage remotely sensed data products combined with spatiotemporal causal analytic techniques to evaluate the impact of large-scale plastic waste policies on air quality. Throughout, we use the case study of Indonesia before and after 2018, when China halted its import of plastic waste, resulting in diversion of this massive waste stream to other countries. We tailor cutting-edge statistical methods to this setting, estimating effects of the increase in plastic waste imports on fine particulate matter (PM$_{2.5}$) near waste dump sites in Indonesia as a function of proximity to ports, which serves as an induced continuous exposure. We observe that dump sites above the 20th quantile of port proximity experienced a statistically significant increase in monthly PM$_{2.5}$ concentrations after China's ban took effect (2018-2019) compared to concentrations expected under business-as-usual (2012-2017), with increases ranging from 0.76--1.72$\mu$g/m$^3$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04491v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ellen M. Considine, Rachel C. Nethery</dc:creator>
    </item>
    <item>
      <title>Aggregating Dependent Signals with Heavy-Tailed Combination Tests</title>
      <link>https://arxiv.org/abs/2310.20460</link>
      <description>arXiv:2310.20460v3 Announce Type: replace-cross 
Abstract: Combining dependent p-values poses a long-standing challenge in statistical inference, particularly when aggregating findings from multiple methods to enhance signal detection. Recently, p-value combination tests based on regularly varying-tailed distributions, such as the Cauchy combination test and harmonic mean p-value, have attracted attention for their robustness to unknown dependence. This paper provides a theoretical and empirical evaluation of these methods under an asymptotic regime where the number of p-values is fixed and the global test significance level approaches zero. We examine two types of dependence among the p-values. First, when p-values are pairwise asymptotically independent, such as with bivariate normal test statistics with no perfect correlation, we prove that these combination tests are asymptotically valid. However, they become equivalent to the Bonferroni test as the significance level tends to zero for both one-sided and two-sided p-values. Empirical investigations suggest that this equivalence can emerge at moderately small significance levels. Second, under pairwise quasi-asymptotic dependence, such as with bivariate t-distributed test statistics, our simulations suggest that these combination tests can remain valid and exhibit notable power gains over Bonferroni, even as the significance level diminishes. These findings highlight the potential advantages of these combination tests in scenarios where p-values exhibit substantial dependence. Our simulations also examine how test performance depends on the support and tail heaviness of the underlying distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.20460v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Gui, Yuchao Jiang, Jingshu Wang</dc:creator>
    </item>
    <item>
      <title>The general linear hypothesis testing problem for multivariate functional data with applications</title>
      <link>https://arxiv.org/abs/2312.02518</link>
      <description>arXiv:2312.02518v3 Announce Type: replace-cross 
Abstract: As technology continues to advance at a rapid pace, the prevalence of multivariate functional data (MFD) has expanded across diverse disciplines, spanning biology, climatology, finance, and numerous other fields of study. Although MFD are encountered in various fields, the development of methods for hypotheses on mean functions, especially the general linear hypothesis testing (GLHT) problem for such data has been limited. In this study, we propose and study a new global test for the GLHT problem for MFD, which includes the one-way FMANOVA, post hoc, and contrast analysis as special cases. The asymptotic null distribution of the test statistic is shown to be a chi-squared-type mixture dependent of eigenvalues of the heteroscedastic covariance functions. The distribution of the chi-squared-type mixture can be well approximated by a three-cumulant matched chi-squared-approximation with its approximation parameters estimated from the data. By incorporating an adjustment coefficient, the proposed test performs effectively irrespective of the correlation structure in the functional data, even when dealing with a relatively small sample size. Additionally, the proposed test is shown to be root-n consistent, that is, it has a nontrivial power against a local alternative. Simulation studies and a real data example demonstrate finite-sample performance and broad applicability of the proposed test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02518v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianming Zhu</dc:creator>
    </item>
    <item>
      <title>Constructing a T-test for Value Function Comparison of Individualized Treatment Regimes in the Presence of Multiple Imputation for Missing Data</title>
      <link>https://arxiv.org/abs/2312.15217</link>
      <description>arXiv:2312.15217v3 Announce Type: replace-cross 
Abstract: Optimal individualized treatment decision-making has improved health outcomes in recent years. The value function is commonly used to evaluate the goodness of an individualized treatment decision rule. Despite recent advances, comparing value functions between different treatment decision rules or constructing confidence intervals around value functions remains difficult. We propose a t-test based method applied to a test set that generates valid p-values to compare value functions between a given pair of treatment decision rules when some of the data are missing. We demonstrate the ease in use of this method and evaluate its performance via simulation studies and apply it to the China Health and Nutrition Survey data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15217v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minxin Lu, Annie Green Howard, Penny Gordon-Larsen, Katie A. Meyer, Hsiao-Chuan Tien, Shufa Du, Huijun Wang, Bing Zhang, Michael R. Kosorok</dc:creator>
    </item>
    <item>
      <title>Bayesian Mapping of Mortality Clusters</title>
      <link>https://arxiv.org/abs/2407.19135</link>
      <description>arXiv:2407.19135v2 Announce Type: replace-cross 
Abstract: Disease mapping analyses the distribution of several disease outcomes within a territory. Primary goals include identifying areas with unexpected changes in mortality rates, studying the relation among multiple diseases, and dividing the analysed territory into clusters based on the observed levels of disease incidence or mortality. In this work, we focus on detecting spatial mortality clusters, that occur when neighbouring areas within a territory exhibit similar mortality levels due to one or more diseases. When multiple causes of death are examined together, it is relevant to identify not only the spatial boundaries of the clusters but also the diseases that lead to their formation. However, existing methods in literature struggle to address this dual problem effectively and simultaneously. To overcome these limitations, we introduce Perla, a multivariate Bayesian model that clusters areas in a territory according to the observed mortality rates of multiple causes of death, also exploiting the information of external covariates. Our model incorporates the spatial structure of data directly into the clustering probabilities by leveraging the stick-breaking formulation of the multinomial distribution. Additionally, it exploits suitable global-local shrinkage priors to ensure that the detection of clusters depends on diseases showing concrete increases or decreases in mortality levels, while excluding uninformative diseases. We propose an MCMC algorithm for posterior inference that consists of closed-form Gibbs sampling moves for nearly every model parameter. To demonstrate the flexibility and effectiveness of our methodology, we validate Perla with a series of simulation experiments and two extensive case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19135v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrea Sottosanti, Pietro Belloni, Enrico Bovo, Giovanna Boccuzzo</dc:creator>
    </item>
    <item>
      <title>PK-YOLO: Pretrained Knowledge Guided YOLO for Brain Tumor Detection in Multiplanar MRI Slices</title>
      <link>https://arxiv.org/abs/2410.21822</link>
      <description>arXiv:2410.21822v2 Announce Type: replace-cross 
Abstract: Brain tumor detection in multiplane Magnetic Resonance Imaging (MRI) slices is a challenging task due to the various appearances and relationships in the structure of the multiplane images. In this paper, we propose a new You Only Look Once (YOLO)-based detection model that incorporates Pretrained Knowledge (PK), called PK-YOLO, to improve the performance for brain tumor detection in multiplane MRI slices. To our best knowledge, PK-YOLO is the first pretrained knowledge guided YOLO-based object detector. The main components of the new method are a pretrained pure lightweight convolutional neural network-based backbone via sparse masked modeling, a YOLO architecture with the pretrained backbone, and a regression loss function for improving small object detection. The pretrained backbone allows for feature transferability of object queries on individual plane MRI slices into the model encoders, and the learned domain knowledge base can improve in-domain detection. The improved loss function can further boost detection performance on small-size brain tumors in multiplanar two-dimensional MRI slices. Experimental results show that the proposed PK-YOLO achieves competitive performance on the multiplanar MRI brain tumor detection datasets compared to state-of-the-art YOLO-like and DETR-like object detectors. The code is available at https://github.com/mkang315/PK-YOLO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21822v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/WACV61041.2025.00367</arxiv:DOI>
      <arxiv:journal_reference>In WACV (2025) 3732--3741</arxiv:journal_reference>
      <dc:creator>Ming Kang, Fung Fung Ting, Rapha\"el C. -W. Phan, Chee-Ming Ting</dc:creator>
    </item>
    <item>
      <title>VACT: A Video Automatic Causal Testing System and a Benchmark</title>
      <link>https://arxiv.org/abs/2503.06163</link>
      <description>arXiv:2503.06163v2 Announce Type: replace-cross 
Abstract: With the rapid advancement of text-conditioned Video Generation Models (VGMs), the quality of generated videos has significantly improved, bringing these models closer to functioning as ``*world simulators*'' and making real-world-level video generation more accessible and cost-effective. However, the generated videos often contain factual inaccuracies and lack understanding of fundamental physical laws. While some previous studies have highlighted this issue in limited domains through manual analysis, a comprehensive solution has not yet been established, primarily due to the absence of a generalized, automated approach for modeling and assessing the causal reasoning of these models across diverse scenarios. To address this gap, we propose VACT: an **automated** framework for modeling, evaluating, and measuring the causal understanding of VGMs in real-world scenarios. By combining causal analysis techniques with a carefully designed large language model assistant, our system can assess the causal behavior of models in various contexts without human annotation, which offers strong generalization and scalability. Additionally, we introduce multi-level causal evaluation metrics to provide a detailed analysis of the causal performance of VGMs. As a demonstration, we use our framework to benchmark several prevailing VGMs, offering insight into their causal reasoning capabilities. Our work lays the foundation for systematically addressing the causal understanding deficiencies in VGMs and contributes to advancing their reliability and real-world applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06163v2</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haotong Yang, Qingyuan Zheng, Yunjian Gao, Yongkun Yang, Yangbo He, Zhouchen Lin, Muhan Zhang</dc:creator>
    </item>
    <item>
      <title>On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction</title>
      <link>https://arxiv.org/abs/2504.08169</link>
      <description>arXiv:2504.08169v2 Announce Type: replace-cross 
Abstract: The predictions of click through rate (CTR) and conversion rate (CVR) play a crucial role in the success of ad-recommendation systems. A Deep Hierarchical Ensemble Network (DHEN) has been proposed to integrate multiple feature crossing modules and has achieved great success in CTR prediction. However, its performance for CVR prediction is unclear in the conversion ads setting, where an ad bids for the probability of a user's off-site actions on a third party website or app, including purchase, add to cart, sign up, etc. A few challenges in DHEN: 1) What feature-crossing modules (MLP, DCN, Transformer, to name a few) should be included in DHEN? 2) How deep and wide should DHEN be to achieve the best trade-off between efficiency and efficacy? 3) What hyper-parameters to choose in each feature-crossing module? Orthogonal to the model architecture, the input personalization features also significantly impact model performance with a high degree of freedom. In this paper, we attack this problem and present our contributions biased to the applied data science side, including:
  First, we propose a multitask learning framework with DHEN as the single backbone model architecture to predict all CVR tasks, with a detailed study on how to make DHEN work effectively in practice; Second, we build both on-site real-time user behavior sequences and off-site conversion event sequences for CVR prediction purposes, and conduct ablation study on its importance; Last but not least, we propose a self-supervised auxiliary loss to predict future actions in the input sequence, to help resolve the label sparseness issue in CVR prediction.
  Our method achieves state-of-the-art performance compared to previous single feature crossing modules with pre-trained user personalization features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08169v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinfeng Zhuang, Yinrui Li, Runze Su, Ke Xu, Zhixuan Shao, Kungang Li, Ling Leng, Han Sun, Meng Qi, Yixiong Meng, Yang Tang, Zhifang Liu, Qifei Shen, Aayush Mudgal</dc:creator>
    </item>
  </channel>
</rss>
