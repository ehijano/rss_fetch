<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Aug 2024 01:39:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 22 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Potential Enabling Technologies for 7G Networks: Survey</title>
      <link>https://arxiv.org/abs/2408.11072</link>
      <description>arXiv:2408.11072v1 Announce Type: new 
Abstract: Every new generation of mobile networks brings significant advances in two segments, enhancement of the network parameters within the legacy technologies and introduction of new technologies enabling new paradigms in designing the networks. In the first class of enhancements the effort is to increase data rates, improve energy efficiency, enhance connectivity, reduce data transmission latency etc. In the second class of innovations for 6G and 7G, we anticipate focus on optimum integration of advanced ML and AI in general, and quantum computing with the continuous interest in the satellite networks for optimal quantum key distribution . By introducing quantum technology 7G will be able to speed up computing processes in the net, enhance network security as well as to enable distributed QC, which is a new paradigm in computer sciences.
  Using advanced networks as a basic ingredient of inter system integration, here we focus only on the second segment of anticipated innovations in networking and present a survey of the subset of potential technology enablers for the above concept with special emphasis on the inter dependency of the solutions chosen in different segments of the network. In Section II, we present several anticipated 6G/7G (system of systems type) network optimization examples resulting in a new paradigm of network optimization indicating a need for quantum computing and quantum computing based optimization algorithms. In Section III we survey work on quantum cryptography and QKD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11072v1</guid>
      <category>cs.ET</category>
      <category>quant-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Savo Glisic</dc:creator>
    </item>
    <item>
      <title>Green Probabilistic Semantic Communication over Wireless Networks</title>
      <link>https://arxiv.org/abs/2408.11446</link>
      <description>arXiv:2408.11446v1 Announce Type: new 
Abstract: In this paper, we propose a multi-user green semantic communication system facilitated by a probabilistic knowledge graph (PKG). By integrating probability into the knowledge graph, we enable probabilistic semantic communication (PSC) and represent semantic information accordingly. On this basis, a semantic compression model designed for multi-user downlink task-oriented communication is introduced, utilizing the semantic compression ratio (SCR) as a parameter to connect the computation and communication processes of information transmission. Based on the rate-splitting multiple access (RSMA) technology, we derive mathematical expressions for system transmission energy consumption and related formulations. Subsequently, the multi-user green semantic communication system is modeled and the optimal problem with the goal of minimizing system energy consumption comprehensively considering the computation and communication process under given constrains is formulated. In order to address the optimal problem, we propose an alternating optimization algorithm that tackles sub-problems of power allocation and beamforming design, semantic compression ratio, and computation capacity allocation. Simulation results validate the effectiveness of our approach, demonstrating the superiority of our system over methods using Space Division Multiple Access (SDMA) and non-orthogonal multiple access (NOMA) instead of RSMA, and highlighting the benefits of our PSC compression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11446v1</guid>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruopeng Xu, Zhaohui Yang, Yijie Mao, Chongwen Huang, Qianqian Yang, Lexi Xu, Wei Xu, Zhaoyang Zhang</dc:creator>
    </item>
    <item>
      <title>Sustainable Volunteer Engagement: Ensuring Potential Retention and Skill Diversity for Balanced Workforce Composition in Crowdsourcing Paradigm</title>
      <link>https://arxiv.org/abs/2408.11498</link>
      <description>arXiv:2408.11498v1 Announce Type: new 
Abstract: Crowdsourcing (CS) faces the challenge of managing complex, skill-demanding tasks, which requires effective task assignment and retention strategies to sustain a balanced workforce. This challenge has become more significant in Volunteer Crowdsourcing Services (VCS). This study introduces Workforce Composition Balance (WCB), a novel framework designed to maintain workforce diversity in VCS by dynamically adjusting retention decisions. The WCB framework integrates the Volunteer Retention and Value Enhancement (VRAVE) algorithm with advanced skill-based task assignment methods. It ensures efficient remuneration policy for both assigned and unassigned potential volunteers by incorporating their potential levels, participation dividends, and satisfaction scores. Comparative analysis with three state-of-the-art baselines on real dataset shows that our WCB framework achieves 1.4 times better volunteer satisfaction and a 20% higher task retention rate, with only a 12% increase in remuneration. The effectiveness of the proposed WCB approach is to enhance the volunteer engagement and their long-term retention, thus making it suitable for functioning of social good applications where a potential and skilled volunteer workforce is crucial for sustainable community services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11498v1</guid>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Riya Samanta, Soumya K Ghosh</dc:creator>
    </item>
    <item>
      <title>Empowering Volunteer Crowdsourcing Services: A Serverless-assisted, Skill and Willingness Aware Task Assignment Approach for Amicable Volunteer Involvement</title>
      <link>https://arxiv.org/abs/2408.11510</link>
      <description>arXiv:2408.11510v1 Announce Type: new 
Abstract: Volunteer crowdsourcing (VCS) leverages citizen interaction to address challenges by utilizing individuals' knowledge and skills. Complex social tasks often require collaboration among volunteers with diverse skill sets, and their willingness to engage is crucial. Matching tasks with the most suitable volunteers remains a significant challenge. VCS platforms face unpredictable demands in terms of tasks and volunteer requests, complicating the prediction of resource requirements for the volunteer-to-task assignment process. To address these challenges, we introduce the Skill and Willingness-Aware Volunteer Matching (SWAM) algorithm, which allocates volunteers to tasks based on skills, willingness, and task requirements. We also developed a serverless framework to deploy SWAM. Our method outperforms conventional solutions, achieving a 71% improvement in end-to-end latency efficiency. We achieved a 92% task completion ratio and reduced task waiting time by 56%, with an overall utility gain 30% higher than state-of-the-art baseline methods. This framework contributes to generating effective volunteer and task matches, supporting grassroots community coordination and fostering citizen involvement, ultimately contributing to social good.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11510v1</guid>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Riya Samanta, Biswajeet Sethi, Soumya K Ghosh</dc:creator>
    </item>
    <item>
      <title>A Systematic Literature Review on the Use of Blockchain Technology in Transition to a Circular Economy</title>
      <link>https://arxiv.org/abs/2408.11664</link>
      <description>arXiv:2408.11664v1 Announce Type: new 
Abstract: The circular economy has the potential to increase resource efficiency and minimize waste through the 4R framework of reducing, reusing, recycling, and recovering. Blockchain technology is currently considered a valuable aid in the transition to a circular economy. Its decentralized and tamper-resistant nature enables the construction of transparent and secure supply chain management systems, thereby improving product accountability and traceability. However, the full potential of blockchain technology in circular economy models will not be realized until a number of concerns, including scalability, interoperability, data protection, and regulatory and legal issues, are addressed. More research and stakeholder participation are required to overcome these limitations and achieve the benefits of blockchain technology in promoting a circular economy. This article presents a systematic literature review (SLR) that identified industry use cases for blockchain-driven circular economy models and offered architectures to minimize resource consumption, prices, and inefficiencies while encouraging the reuse, recycling, and recovery of end-of-life products. Three main outcomes emerged from our review of 41 documents, which included scholarly publications, Twitter-linked information, and Google results. The relationship between blockchain and the 4R framework for circular economy; discussion the terminology and various forms of blockchain and circular economy; and identification of the challenges and obstacles that blockchain technology may face in enabling a circular economy. This research shows how blockchain technology can help with the transition to a circular economy. Yet, it emphasizes the importance of additional study and stakeholder participation to overcome potential hurdles and obstacles in implementing blockchain-driven circular economy models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11664v1</guid>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ishmam Abid, S. M. Zuhayer Anzum Fuad, Mohammad Jabed Morshed Chowdhury, Mehruba Sharmin Chowdhury, Md Sadek Ferdous</dc:creator>
    </item>
    <item>
      <title>Phase-Based Approaches for Rapid Construction of Magnetic Fields in NV Magnetometry</title>
      <link>https://arxiv.org/abs/2408.11069</link>
      <description>arXiv:2408.11069v2 Announce Type: cross 
Abstract: With the second quantum revolution underway, quantum-enhanced sensors are moving from laboratory demonstrations to field deployments, providing enhanced and even new capabilities. Signal processing and operational software is becoming integral parts of these emerging sensing systems to reap the benefits of this progress. This paper looks into widefield Nitrogen Vacancy Center-based magnetometry and focuses on estimating the magnetic field from the Optically Detected Magnetic Resonances (ODMR) signal, a crucial output for various applications. Mapping the shifts of ODMR signals to phase estimation, a computationally efficient approaches are proposed. Involving Fourier Transform and Filtering as pre-processing steps, the suggested approaches involve linear curve fit or complex frequency estimation based on well-known super-resolution technique Estimation of Signal Parameters via Rotational Invariant Techniques (ESPRIT). The existing methods in the quantum sensing literature take different routes based on Lorentzian fitting for determining magnetic field maps. To showcase the functionality and effectiveness of the suggested techniques, relevant results, based on experimental data are provided, which shows a significant reduction in computational time with the proposed method over existing methods</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11069v2</guid>
      <category>physics.ins-det</category>
      <category>cs.ET</category>
      <category>quant-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Prabhat Anand, Ankit Khandelwal, Achanna Anil Kumar, M Girish Chandra, Pavan K Reddy, Anuj Bathla, Dasika Shishir, Kasturi Saha</dc:creator>
    </item>
    <item>
      <title>Carbon Connect: An Ecosystem for Sustainable Computing</title>
      <link>https://arxiv.org/abs/2405.13858</link>
      <description>arXiv:2405.13858v2 Announce Type: replace-cross 
Abstract: Computing is at a moment of profound opportunity. Emerging applications -- such as capable artificial intelligence, immersive virtual realities, and pervasive sensor systems -- drive unprecedented demand for computer. Despite recent advances toward net zero carbon emissions, the computing industry's gross energy usage continues to rise at an alarming rate, outpacing the growth of new energy installations and renewable energy deployments. A shift towards sustainability is needed to spark a transformation in how computer systems are manufactured, allocated, and consumed. Carbon Connect envisions coordinated research thrusts that produce design and management strategies for sustainable, next-generation computer systems. These strategies must flatten and then reverse growth trajectories for computing power and carbon for society's most rapidly growing applications such as artificial intelligence and virtual spaces. We will require accurate models for carbon accounting in computing technology. For embodied carbon, we must re-think conventional design strategies -- over-provisioned monolithic servers, frequent hardware refresh cycles, custom silicon -- and adopt life-cycle design strategies that more effectively reduce, reuse and recycle hardware at scale. For operational carbon, we must not only embrace renewable energy but also design systems to use that energy more efficiently. Finally, new hardware design and management strategies must be cognizant of economic policy and regulatory landscape, aligning private initiatives with societal goals. Many of these broader goals will require computer scientists to develop deep, enduring collaborations with researchers in economics, law, and industrial ecology to spark change in broader practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13858v2</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin C. Lee, David Brooks, Arthur van Benthem, Udit Gupta, Gage Hills, Vincent Liu, Benjamin Pierce, Christopher Stewart, Emma Strubell, Gu-Yeon Wei, Adam Wierman, Yuan Yao, Minlan Yu</dc:creator>
    </item>
    <item>
      <title>TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on OGD portals</title>
      <link>https://arxiv.org/abs/2407.18764</link>
      <description>arXiv:2407.18764v2 Announce Type: replace-cross 
Abstract: Efforts directed towards promoting Open Government Data (OGD) have gained significant traction across various governmental tiers since the mid-2000s. As more datasets are published on OGD portals, finding specific data becomes harder, leading to information overload. Complete and accurate documentation of datasets, including association of proper tags with datasets is key to improving dataset findability and accessibility. Analysis conducted on the Estonian Open Data Portal, revealed that 11% datasets have no associated tags, while 26% had only one tag assigned to them, which underscores challenges in data findability and accessibility within the portal, which, according to the recent Open Data Maturity Report, is considered trend-setter. The aim of this study is to propose an automated solution to tagging datasets to improve data findability on OGD portals. This paper presents Tagify - a prototype of tagging interface that employs large language models (LLM) such as GPT-3.5-turbo and GPT-4 to automate dataset tagging, generating tags for datasets in English and Estonian, thereby augmenting metadata preparation by data publishers and improving data findability on OGD portals by data users. The developed solution was evaluated by users and their feedback was collected to define an agenda for future prototype improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18764v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Kliimask, Anastasija Nikiforova</dc:creator>
    </item>
    <item>
      <title>Large Model Strategic Thinking, Small Model Efficiency: Transferring Theory of Mind in Large Language Models</title>
      <link>https://arxiv.org/abs/2408.05241</link>
      <description>arXiv:2408.05241v3 Announce Type: replace-cross 
Abstract: As the performance of larger, newer Large Language Models continues to improve for strategic Theory of Mind (ToM) tasks, the demand for these state-of-the-art models increases commensurately. However, their deployment is costly both in terms of processing power and time. In this paper, we investigate the feasibility of creating smaller, highly-performing specialized algorithms by way of fine-tuning. To do this, we first present a large pre-trained model with 20 unique scenarios that combine different social contexts with games of varying social dilemmas, record its answers, and use them for Q&amp;A fine-tuning on a smaller model of the same family. Our focus is on in-context game-theoretic decision-making, the same domain within which human interaction occurs and that requires both a theory of mind (or a semblance thereof) and an understanding of social dynamics. The smaller model is therefore trained not just on the answers provided, but also on the motivations provided by the larger model, which should contain advice and guidelines to navigate both strategic dilemmas and social cues. We find that the fine-tuned smaller language model consistently bridged the gap in performance between the smaller pre-trained version of the model and its larger relative and that its improvements extended in areas and contexts beyond the ones provided in the training examples, including on out-of-sample scenarios that include completely different game structures. On average for all games, through fine-tuning, the smaller model showed a 46% improvement measured as alignment towards the behavior of the larger model, with 100% representing indistinguishable behavior. When presented with out-of-sample social contexts and games, the fine-tuned model still displays remarkable levels of alignment, reaching an improvement of 18% and 28% respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05241v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nunzio Lore, Alireza Sepehr Ilami, Babak Heydari</dc:creator>
    </item>
    <item>
      <title>V-RoAst: A New Dataset for Visual Road Assessment</title>
      <link>https://arxiv.org/abs/2408.10872</link>
      <description>arXiv:2408.10872v2 Announce Type: replace-cross 
Abstract: Road traffic crashes cause millions of deaths annually and have a significant economic impact, particularly in low- and middle-income countries (LMICs). This paper presents an approach using Vision Language Models (VLMs) for road safety assessment, overcoming the limitations of traditional Convolutional Neural Networks (CNNs). We introduce a new task ,V-RoAst (Visual question answering for Road Assessment), with a real-world dataset. Our approach optimizes prompt engineering and evaluates advanced VLMs, including Gemini-1.5-flash and GPT-4o-mini. The models effectively examine attributes for road assessment. Using crowdsourced imagery from Mapillary, our scalable solution influentially estimates road safety levels. In addition, this approach is designed for local stakeholders who lack resources, as it does not require training data. It offers a cost-effective and automated methods for global road safety assessments, potentially saving lives and reducing economic burdens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10872v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Natchapon Jongwiriyanurak, Zichao Zeng, June Moh Goo, Xinglei Wang, Ilya Ilyankou, Kerkritt Srirrongvikrai, Meihui Wang, James Haworth</dc:creator>
    </item>
  </channel>
</rss>
