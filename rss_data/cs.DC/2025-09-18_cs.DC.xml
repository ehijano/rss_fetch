<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Sep 2025 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A User-centric Kubernetes-based Architecture for Green Cloud Computing</title>
      <link>https://arxiv.org/abs/2509.13325</link>
      <description>arXiv:2509.13325v1 Announce Type: new 
Abstract: To meet the increasing demand for cloud computing services, the scale and number of data centers keeps increasing worldwide. This growth comes at the cost of increased electricity consumption, which directly correlates to CO2 emissions, the main driver of climate change. As such, researching ways to reduce cloud computing emissions is more relevant than ever. However, although cloud providers are reportedly already working near optimal power efficiency, they fail in providing precise sustainability reporting. This calls for further improvements on the cloud computing consumer's side. To this end, in this paper we propose a user-centric, Kubernetes-based architecture for green cloud computing. We implement a carbon intensity forecaster and we use it to schedule workloads based on the availability of green energy, exploiting both regional and temporal variations to minimize emissions. We evaluate our system using real-world traces of cloud workloads execution comparing the achieved carbon emission savings against a baseline round-robin scheduler. Our findings indicate that our system can achieve up to a 13% reduction in emissions in a strict scenario with heavy limitations on the available resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13325v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Zanotto, Leonardo Vicentini, Redi Vreto, Francesco Lumpp, Diego Braga, Sandro Fiore</dc:creator>
    </item>
    <item>
      <title>Testing and benchmarking emerging supercomputers via the MFC flow solver</title>
      <link>https://arxiv.org/abs/2509.13575</link>
      <description>arXiv:2509.13575v1 Announce Type: new 
Abstract: Deploying new supercomputers requires testing and evaluation via application codes. Portable, user-friendly tools enable evaluation, and the Multicomponent Flow Code (MFC), a computational fluid dynamics (CFD) code, addresses this need. MFC is adorned with a toolchain that automates input generation, compilation, batch job submission, regression testing, and benchmarking. The toolchain design enables users to evaluate compiler-hardware combinations for correctness and performance with limited software engineering experience. As with other PDE solvers, wall time per spatially discretized grid point serves as a figure of merit. We present MFC benchmarking results for five generations of NVIDIA GPUs, three generations of AMD GPUs, and various CPU architectures, utilizing Intel, Cray, NVIDIA, AMD, and GNU compilers. These tests have revealed compiler bugs and regressions on recent machines such as Frontier and El Capitan. MFC has benchmarked approximately 50 compute devices and 5 flagship supercomputers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13575v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Wilfong, Anand Radhakrishnan, Henry A. Le Berre, Tanush Prathi, Stephen Abbott, Spencer H. Bryngelson</dc:creator>
    </item>
    <item>
      <title>Modeling the Carbon Footprint of HPC: The Top 500 and EasyC</title>
      <link>https://arxiv.org/abs/2509.13583</link>
      <description>arXiv:2509.13583v1 Announce Type: new 
Abstract: Climate change is a critical concern for HPC systems, but GHG protocol carbon-emission accounting methodologies are difficult for a single system, and effectively infeasible for a collection of systems. As a result, there is no HPC-wide carbon reporting, and even the largest HPC sites do not do GHG protocol reporting.
  We assess the carbon footprint of HPC, focusing on the Top 500 systems. The key challenge lies in modeling the carbon footprint with limited data availability.
  With the disclosed Top500.org data, and using a new tool, EasyC, we were able to model the operational carbon of 391 HPC systems and the embodied carbon of 283 HPC systems. We further show how this coverage can be enhanced by exploiting additional public information. With improved coverage, then interpolation is used to produce the first carbon footprint estimates of the Top 500 HPC systems. They are 1,393.7 million MT CO2e operational carbon (1 Year) and 1,881.8 million MT CO2e embodied carbon. We also project how the Top 500's carbon footprint will increase through 2030.
  A key enabler is the EasyC tool which models carbon footprint with only a few data metrics. We explore availability of data and enhancement, showing that coverage can be increased to 98% of Top 500 systems for operational and 80.8% of the systems for embodied emissions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13583v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3731599.3767567</arxiv:DOI>
      <arxiv:journal_reference>Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC Workshops 2025)</arxiv:journal_reference>
      <dc:creator>Varsha Rao, Andrew A. Chien</dc:creator>
    </item>
    <item>
      <title>GPU Programming for AI Workflow Development on AWS SageMaker: An Instructional Approach</title>
      <link>https://arxiv.org/abs/2509.13703</link>
      <description>arXiv:2509.13703v1 Announce Type: new 
Abstract: We present the design, implementation, and comprehensive evaluation of a specialized course on GPU architecture, GPU programming, and how these are used for developing AI agents. This course is offered to undergraduate and graduate students during Fall 2024 and Spring 2025. The course began with foundational concepts in GPU/CPU hardware and parallel computing and progressed to develop RAG and optimizing them using GPUs. Students gained experience provisioning and configuring cloud-based GPU instances, implementing parallel algorithms, and deploying scalable AI solutions. We evaluated learning outcomes through assessments, course evaluations, and anonymous surveys. The results reveal that (1) AWS served as an effective and economical platform for practical GPU programming, (2) experiential learning significantly enhanced technical proficiency and engagement, and (3) the course strengthened students' problem-solving and critical thinking skills through tools such as TensorBoard and HPC profilers, which exposed performance bottlenecks and scaling issues. Our findings underscore the pedagogical value of integrating parallel computing into STEM education. We advocate for broader adoption of similar electives across STEM curricula to prepare students for the demands of modern, compute-intensive fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13703v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sriram Srinivasan, Hamdan Alabsi, Rand Obeidat, Nithisha Ponnala, Azene Zenebe</dc:creator>
    </item>
    <item>
      <title>LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology</title>
      <link>https://arxiv.org/abs/2509.13978</link>
      <description>arXiv:2509.13978v1 Announce Type: new 
Abstract: Modern scientific discovery increasingly relies on workflows that process data across the Edge, Cloud, and High Performance Computing (HPC) continuum. Comprehensive and in-depth analyses of these data are critical for hypothesis validation, anomaly detection, reproducibility, and impactful findings. Although workflow provenance techniques support such analyses, at large scale, the provenance data become complex and difficult to analyze. Existing systems depend on custom scripts, structured queries, or static dashboards, limiting data interaction. In this work, we introduce an evaluation methodology, reference architecture, and open-source implementation that leverages interactive Large Language Model (LLM) agents for runtime data analysis. Our approach uses a lightweight, metadata-driven design that translates natural language into structured provenance queries. Evaluations across LLaMA, GPT, Gemini, and Claude, covering diverse query classes and a real-world chemistry workflow, show that modular design, prompt tuning, and Retrieval-Augmented Generation (RAG) enable accurate and insightful LLM agent responses beyond recorded provenance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13978v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3731599.3767582</arxiv:DOI>
      <dc:creator>Renan Souza, Timothy Poteet, Brian Etz, Daniel Rosendo, Amal Gueroudji, Woong Shin, Prasanna Balaprakash, Rafael Ferreira da Silva</dc:creator>
    </item>
    <item>
      <title>AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions</title>
      <link>https://arxiv.org/abs/2509.13523</link>
      <description>arXiv:2509.13523v1 Announce Type: cross 
Abstract: Generative machine learning offers new opportunities to better understand complex Earth system dynamics. Recent diffusion-based methods address spectral biases and improve ensemble calibration in weather forecasting compared to deterministic methods, yet have so far proven difficult to scale stably at high resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin diffusion transformer to address this gap, and SWiPe, a generalizable technique that composes window parallelism with sequence and pipeline parallelism to shard window-based transformers without added communication cost or increased global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS (mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \times 1$ patch size on the 0.25{\deg} ERA5 dataset, achieving 95.5% weak scaling efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS and remains stable on seasonal scales to 90 days, highlighting the potential of billion-parameter diffusion models for weather and climate prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13523v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V\"ain\"o Hatanp\"a\"a, Eugene Ku, Jason Stock, Murali Emani, Sam Foreman, Chunyong Jung, Sandeep Madireddy, Tung Nguyen, Varuni Sastry, Ray A. O. Sinurat, Sam Wheeler, Huihuo Zheng, Troy Arcomano, Venkatram Vishwanath, Rao Kotamarthi</dc:creator>
    </item>
    <item>
      <title>Secure, Scalable and Privacy Aware Data Strategy in Cloud</title>
      <link>https://arxiv.org/abs/2509.13627</link>
      <description>arXiv:2509.13627v1 Announce Type: cross 
Abstract: The enterprises today are faced with the tough challenge of processing, storing large amounts of data in a secure, scalable manner and enabling decision makers to make quick, informed data driven decisions. This paper addresses this challenge and develops an effective enterprise data strategy in the cloud. Various components of an effective data strategy are discussed and architectures addressing security, scalability and privacy aspects are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13627v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICAISS55157.2022.10011063</arxiv:DOI>
      <arxiv:journal_reference>Butte, Vijay Kumar, and Sujata Butte. "Secure, scalable and privacy aware data strategy in cloud." 2022 International Conference on Augmented Intelligence and Sustainable Systems (ICAISS). IEEE, 2022</arxiv:journal_reference>
      <dc:creator>Vijay Kumar Butte, Sujata Butte</dc:creator>
    </item>
    <item>
      <title>Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery</title>
      <link>https://arxiv.org/abs/2509.13631</link>
      <description>arXiv:2509.13631v1 Announce Type: cross 
Abstract: Accurate identification of deforestation from satellite images is essential in order to understand the geographical situation of an area. This paper introduces a new distributed approach to identify as well as locate deforestation across different clients using Federated Learning (FL). Federated Learning enables distributed network clients to collaboratively train a model while maintaining data privacy and security of the active users. In our framework, a client corresponds to an edge satellite center responsible for local data processing. Moreover, FL provides an advantage over centralized training method which requires combining data, thereby compromising with data security of the clients. Our framework leverages the FLOWER framework with RAY framework to execute the distributed learning workload. Furthermore, efficient client spawning is ensured by RAY as it can select definite amount of users to create an emulation environment. Our FL framework uses YOLOS-small (a Vision Transformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN with a MobileNetV3 backbone models trained and tested on publicly available datasets. Our approach provides us a different view for image segmentation-based tasks on satellite imagery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13631v1</guid>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuvraj Dutta, Aaditya Sikder, Basabdatta Palit</dc:creator>
    </item>
    <item>
      <title>ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated Learning</title>
      <link>https://arxiv.org/abs/2509.13739</link>
      <description>arXiv:2509.13739v1 Announce Type: cross 
Abstract: Federated learning (FL) faces a critical dilemma: existing protection mechanisms like differential privacy (DP) and homomorphic encryption (HE) enforce a rigid trade-off, forcing a choice between model utility and computational efficiency. This lack of flexibility hinders the practical implementation. To address this, we introduce ParaAegis, a parallel protection framework designed to give practitioners flexible control over the privacy-utility-efficiency balance. Our core innovation is a strategic model partitioning scheme. By applying lightweight DP to the less critical, low norm portion of the model while protecting the remainder with HE, we create a tunable system. A distributed voting mechanism ensures consensus on this partitioning. Theoretical analysis confirms the adjustments between efficiency and utility with the same privacy. Crucially, the experimental results demonstrate that by adjusting the hyperparameters, our method enables flexible prioritization between model accuracy and training time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13739v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihou Wu (School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China), Yuecheng Li (School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China), Tianchi Liao (School of Software Engineering, Sun Yat-sen University, Zhuhai, China), Jian Lou (School of Software Engineering, Sun Yat-sen University, Zhuhai, China), Chuan Chen (School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China)</dc:creator>
    </item>
    <item>
      <title>Graph-Regularized Learning of Gaussian Mixture Models</title>
      <link>https://arxiv.org/abs/2509.13855</link>
      <description>arXiv:2509.13855v1 Announce Type: cross 
Abstract: We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in distributed settings with heterogeneous and limited local data. The method exploits a provided similarity graph to guide parameter sharing among nodes, avoiding the transfer of raw data. The resulting model allows for flexible aggregation of neighbors' parameters and outperforms both centralized and locally trained GMMs in heterogeneous, low-sample regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13855v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shamsiiat Abdurakhmanova, Alex Jung</dc:creator>
    </item>
    <item>
      <title>Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless Federated Learning</title>
      <link>https://arxiv.org/abs/2509.13933</link>
      <description>arXiv:2509.13933v1 Announce Type: cross 
Abstract: We consider the client selection problem in wireless Federated Learning (FL), with the objective of reducing the total required time to achieve a certain level of learning accuracy. Since the server cannot observe the clients' dynamic states that can change their computation and communication efficiency, we formulate client selection as a restless multi-armed bandit problem. We propose a scalable and efficient approach called the Whittle Index Learning in Federated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and update an approximated Whittle index associated with each client, and then selects the clients with the highest indices. Compared to existing approaches, WILF-Q does not require explicit knowledge of client state transitions or data distributions, making it well-suited for deployment in practical FL settings. Experiment results demonstrate that WILF-Q significantly outperforms existing baseline policies in terms of learning efficiency, providing a robust and efficient approach to client selection in wireless FL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13933v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiyue Li, Yingxin Liu, Hang Qi, Jieping Luo, Zhizhang Liu, Jingjin Wu</dc:creator>
    </item>
    <item>
      <title>A Closeness Centrality-based Circuit Partitioner for Quantum Simulations</title>
      <link>https://arxiv.org/abs/2509.14098</link>
      <description>arXiv:2509.14098v1 Announce Type: cross 
Abstract: Simulating quantum circuits (QC) on high-performance computing (HPC) systems has become an essential method to benchmark algorithms and probe the potential of large-scale quantum computation despite the limitations of current quantum hardware. However, these simulations often require large amounts of resources, necessitating the use of large clusters with thousands of compute nodes and large memory footprints. In this work, we introduce an end-to-end framework that provides an efficient partitioning scheme for large-scale QCs alongside a flexible code generator to offer a portable solution that minimizes data movement between compute nodes. By formulating the distribution of quantum states and circuits as a graph problem, we apply closeness centrality to assess gate importance and design a fast, scalable partitioning method. The resulting partitions are compiled into highly optimized codes that run seamlessly on a wide range of supercomputers, providing critical insights into the performance and scalability of quantum algorithm simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14098v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Doru Thom Popovici, Harlin Lee, Mauro Del Ben, Naoki Yoshioka, Nobuyasu Ito, Katherine Klymko, Daan Camps, Anastasiia Butko</dc:creator>
    </item>
    <item>
      <title>Julia GraphBLAS with Nonblocking Execution</title>
      <link>https://arxiv.org/abs/2509.14211</link>
      <description>arXiv:2509.14211v1 Announce Type: cross 
Abstract: From the beginning, the GraphBLAS were designed for ``nonblocking execution''; i.e., calls to GraphBLAS methods return as soon as the arguments to the methods are validated and define a directed acyclic graph (DAG) of GraphBLAS operations. This lets GraphBLAS implementations fuse functions, elide unneeded objects, exploit parallelism, plus any additional DAG-preserving transformations. GraphBLAS implementations exist that utilize nonblocking execution but with limited scope. In this paper, we describe our work to implement GraphBLAS with support for aggressive nonblocking execution. We show how features of the Julia programming language greatly simplify implementation of nonblocking execution. This is \emph{work-in-progress} sufficient to show the potential for nonblocking execution and is limited to GraphBLAS methods required to support PageRank.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14211v1</guid>
      <category>cs.MS</category>
      <category>cs.DC</category>
      <category>cs.PL</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pascal Costanza, Timothy G. Mattson, Raye Kimmerer, Benjamin Brock</dc:creator>
    </item>
    <item>
      <title>EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model</title>
      <link>https://arxiv.org/abs/2506.09061</link>
      <description>arXiv:2506.09061v3 Announce Type: replace 
Abstract: This paper introduces EdgeProfiler, a fast profiling framework designed for evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs offer remarkable capabilities in natural language understanding and generation, their high computational, memory, and power requirements often confine them to cloud environments. EdgeProfiler addresses these challenges by providing a systematic methodology for assessing LLM performance in resource-constrained edge settings. The framework profiles compact LLMs, including TinyLLaMA, Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization techniques and strict memory constraints. Analytical modeling is used to estimate latency, FLOPs, and energy consumption. The profiling reveals that 4-bit quantization reduces model memory usage by approximately 60-70%, while maintaining accuracy within 2-5% of full-precision baselines. Inference speeds are observed to improve by 2-3x compared to FP16 baselines across various edge devices. Power modeling estimates a 35-50% reduction in energy consumption for INT4 configurations, enabling practical deployment on hardware such as Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the importance of efficient profiling tailored to lightweight LLMs in edge environments, balancing accuracy, energy efficiency, and computational feasibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09061v3</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alyssa Pinnock, Shakya Jayakody, Kawsher A Roxy, Md Rubel Ahmed</dc:creator>
    </item>
    <item>
      <title>Bridging Cache-Friendliness and Concurrency: A Locality-Optimized In-Memory B-Skiplist</title>
      <link>https://arxiv.org/abs/2507.21492</link>
      <description>arXiv:2507.21492v3 Announce Type: replace 
Abstract: Skiplists are widely used for in-memory indexing in many key-value stores, such as RocksDB and LevelDB, due to their ease of implementation and simple concurrency control mechanisms. However, traditional skiplists suffer from poor cache locality, as they store only a single element per node, leaving performance on the table. Minimizing last-level cache misses is key to maximizing in-memory index performance, making high cache locality essential. In this paper, we present a practical concurrent B-skiplist that enhances cache locality and performance while preserving the simplicity of traditional skiplist structures and concurrency control schemes. Our key contributions include a top-down, single-pass insertion algorithm for B-skiplists and a corresponding simple and efficient top-down concurrency control scheme. On 128 threads, the proposed concurrent B-skiplist achieves between 2x-9x higher throughput compared to state-of-the-art concurrent skiplist implementations, including Facebook's concurrent skiplist from Folly and the Java ConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves competitive (0.9x-1.7x) throughput on point workloads compared to state-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a more complete picture of the performance, we also measure the latency of skiplist and tree-based indices and find that the B-skiplist achieves between 3.5x-103x lower 99% latency compared to other concurrent skiplists and between 0.85x-64x lower 99% latency compared to tree-based indices on point workloads with inserts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21492v3</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3754598.3754655</arxiv:DOI>
      <dc:creator>Yicong Luo, Senhe Hao, Brian Wheatman, Prashant Pandey, Helen Xu</dc:creator>
    </item>
    <item>
      <title>Performance measurements of modern Fortran MPI applications with Score-P</title>
      <link>https://arxiv.org/abs/2508.16592</link>
      <description>arXiv:2508.16592v2 Announce Type: replace 
Abstract: Version 3.0 of the Message-Passing Interface (MPI) standard, released in 2012, introduced a new set of language bindings for Fortran 2008. By making use of modern language features and the enhanced interoperability with C, there was finally a type safe and standard conforming method to call MPI from Fortran. This highly recommended use mpi_f08 language binding has since then been widely adopted among developers of modern Fortran applications. However, tool support for the F08 bindings is still lacking almost a decade later, forcing users to recede to the less safe and convenient interfaces. Full support for the F08 bindings was added to the performance measurement infrastructure Score-P by implementing MPI wrappers in Fortran. Wrappers cover the latest MPI standard version 4.1 in its entirety, matching the features of the C wrappers. By implementing the wrappers in modern Fortran, we can provide full support for MPI procedures passing attributes, info objects, or callbacks. The implementation is regularly tested under the MPICH test suite. The new F08 wrappers were already used by two fluid dynamics simulation codes -- Neko, a spectral finite-element code derived from Nek5000, and EPIC (Elliptical Parcel-In-Cell) -- to successfully generate performance measurements. In this work, we additionally present our design considerations and sketch out the implementation, discussing the challenges we faced in the process. The key component of the implementation is a code generator that produces approximately 50k lines of MPI wrapper code to be used by Score-P, relying on the Python pympistandard module to provide programmatic access to the extracted data from the MPI standard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16592v2</guid>
      <category>cs.DC</category>
      <category>cs.MS</category>
      <category>cs.PF</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregor Corbin</dc:creator>
    </item>
    <item>
      <title>Scalable hybrid quantum Monte Carlo simulation of U(1) gauge field coupled to fermions on GPU</title>
      <link>https://arxiv.org/abs/2508.16298</link>
      <description>arXiv:2508.16298v2 Announce Type: replace-cross 
Abstract: We develop a GPU-accelerated hybrid quantum Monte Carlo (QMC) algorithm to solve the fundamental yet difficult problem of $U(1)$ gauge field coupled to fermions, which gives rise to a $U(1)$ Dirac spin liquid state under the description of (2+1)d quantum electrodynamics QED$_3$. The algorithm renders a good acceptance rate and, more importantly, nearly linear space-time volume scaling in computational complexity $O(N_{\tau} V_s)$, where $N_\tau$ is the imaginary time dimension and $V_s$ is spatial volume, which is much more efficient than determinant QMC with scaling behavior of $O(N_\tau V_s^3)$. Such acceleration is achieved via a collection of technical improvements, including (i) the design of the efficient problem-specific preconditioner, (ii) customized CUDA kernel for matrix-vector multiplication, and (iii) CUDA Graph implementation on the GPU. These advances allow us to simulate the $U(1)$ Dirac spin liquid state with unprecedentedly large system sizes, which is up to $N_\tau\times L\times L = 660\times66\times66$, and reveal its novel properties. With these technical improvements, we see the asymptotic convergence in the scaling dimensions of various fermion bilinear operators and the conserved current operator when approaching the thermodynamic limit. The scaling dimensions find good agreement with field-theoretical expectation, which provides supporting evidence for the conformal nature of the $U(1)$ Dirac spin liquid state in the \qed. Our technical advancements open an avenue to study the Dirac spin liquid state and its transition towards symmetry-breaking phases at larger system sizes and with less computational burden.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16298v2</guid>
      <category>cond-mat.str-el</category>
      <category>cs.DC</category>
      <category>hep-th</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kexin Feng, Chuang Chen, Zi Yang Meng</dc:creator>
    </item>
  </channel>
</rss>
