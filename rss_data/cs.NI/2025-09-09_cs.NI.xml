<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Sep 2025 01:32:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)</title>
      <link>https://arxiv.org/abs/2509.05447</link>
      <description>arXiv:2509.05447v1 Announce Type: new 
Abstract: In wireless networks characterized by dense connectivity, the significant signaling overhead generated by distributed link scheduling algorithms can exacerbate issues like congestion, energy consumption, and radio footprint expansion. To mitigate these challenges, we propose a distributed link sparsification scheme employing graph neural networks (GNNs) to reduce scheduling overhead for delay-tolerant traffic while maintaining network capacity. A GNN module is trained to adjust contention thresholds for individual links based on traffic statistics and network topology, enabling links to withdraw from scheduling contention when they are unlikely to succeed. Our approach is facilitated by a novel offline constrained {unsupervised} learning algorithm capable of balancing two competing objectives: minimizing scheduling overhead while ensuring that total utility meets the required level. In simulated wireless multi-hop networks with up to 500 links, our link sparsification technique effectively alleviates network congestion and reduces radio footprints across four distinct distributed link scheduling protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05447v1</guid>
      <category>cs.NI</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TWC.2025.3606741</arxiv:DOI>
      <dc:creator>Zhongyuan Zhao, Gunjan Verma, Ananthram Swami, Santiago Segarra</dc:creator>
    </item>
    <item>
      <title>Joint Routing, Resource Allocation, and Energy Optimization for Integrated Access and Backhaul with Open RAN</title>
      <link>https://arxiv.org/abs/2509.05467</link>
      <description>arXiv:2509.05467v1 Announce Type: new 
Abstract: As networks evolve towards 6G, Mobile Network Operators (MNOs) must accommodate diverse requirements and at the same time manage rising energy consumption. Integrated Access and Backhaul (IAB) networks facilitate dense cellular deployments with reduced infrastructure complexity. However, the multi-hop wireless backhauling in IAB networks necessitates proper routing and resource allocation decisions to meet the performance requirements. At the same time, cell densification makes energy optimization crucial. This paper addresses the joint optimization of routing and resource allocation in IAB networks through two distinct objectives: energy minimization and throughput maximization. We develop a novel capacity model that links power levels to achievable data rates. We propose two practical large-scale approaches to solve the optimization problems and leverage the closed-loop control framework introduced by the Open Radio Access Network (O-RAN) architecture to integrate the solutions. The approaches are evaluated on diverse scenarios built upon open data of two months of traffic collected by network operators in the city of Milan, Italy. Results show that the proposed approaches effectively reduces number of activated nodes to save energy and achieves approximately 100 Mbps of minimum data rate per User Equipment (UE) during peak hours of the day using spectrum within the Frequency Range (FR) 3, or upper midband. The results validate the practical applicability of our framework for next-generation IAB network deployment and optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05467v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reshma Prasad, Maxime Elkael, Gabriele Gemmi, Osama M. Bushnaq, Debashisha Mishra, Prasanna Raut, Jennifer Simonjan, Michele Polese, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]</title>
      <link>https://arxiv.org/abs/2509.05759</link>
      <description>arXiv:2509.05759v1 Announce Type: new 
Abstract: This paper presents Tiga, a new design for geo-replicated and scalable transactional databases such as Google Spanner. Tiga aims to commit transactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of scenarios, while maintaining high throughput with minimal computational overhead. Tiga consolidates concurrency control and consensus, completing both strictly serializable execution and consistent replication in a single round. It uses synchronized clocks to proactively order transactions by assigning each a future timestamp at submission. In most cases, transactions arrive at servers before their future timestamps and are serialized according to the designated timestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed and proactive ordering fails, in which case Tiga falls back to a slow path, committing in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can commit more transactions at 1-WRTT latency, and incurs much less throughput overhead. Evaluation results show that Tiga outperforms all baselines, achieving 1.3--7.2$\times$ higher throughput and 1.4--4.6$\times$ lower latency. Tiga is open-sourced at https://github.com/New-Consensus-Concurrency-Control/Tiga.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05759v1</guid>
      <category>cs.NI</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3731569.3764854</arxiv:DOI>
      <dc:creator>Jinkun Geng, Shuai Mu, Anirudh Sivaraman, Balaji Prabhakar</dc:creator>
    </item>
    <item>
      <title>On-Dyn-CDA: A Real-Time Cost-Driven Task Offloading Algorithm for Vehicular Networks with Reduced Latency and Task Loss</title>
      <link>https://arxiv.org/abs/2509.05889</link>
      <description>arXiv:2509.05889v1 Announce Type: new 
Abstract: Real-time task processing is a critical challenge in vehicular networks, where achieving low latency and minimizing dropped task ratio depend on efficient task execution. Our primary objective is to maximize the number of completed tasks while minimizing overall latency, with a particular focus on reducing number of dropped tasks. To this end, we investigate both static and dynamic versions of an optimization algorithm. The static version assumes full task availability, while the dynamic version manages tasks as they arrive. We also distinguish between online and offline cases: the online version incorporates execution time into the offloading decision process, whereas the offline version excludes it, serving as a theoretical benchmark for optimal performance. We evaluate our proposed Online Dynamic Cost-Driven Algorithm (On-Dyn-CDA) against these baselines. Notably, the static Particle Swarm Optimization (PSO) baseline assumes all tasks are transferred to the RSU and processed by the MEC, and its offline version disregards execution time, making it infeasible for real-time applications despite its optimal performance in theory. Our novel On-Dyn-CDA completes execution in just 0.05 seconds under the most complex scenario, compared to 1330.05 seconds required by Dynamic PSO. It also outperforms Dynamic PSO by 3.42% in task loss and achieves a 29.22% reduction in average latency in complex scenarios. Furthermore, it requires neither a dataset nor a training phase, and its low computational complexity ensures efficiency and scalability in dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05889v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/JIOT.2025.3602667</arxiv:DOI>
      <dc:creator>Mahsa Paknejad, Parisa Fard Moshiri, Murat Simsek, Burak Kantarci, Hussein T. Mouftah</dc:creator>
    </item>
    <item>
      <title>ALPHA: LLM-Enabled Active Learning for Human-Free Network Anomaly Detection</title>
      <link>https://arxiv.org/abs/2509.05936</link>
      <description>arXiv:2509.05936v1 Announce Type: new 
Abstract: Network log data analysis plays a critical role in detecting security threats and operational anomalies. Traditional log analysis methods for anomaly detection and root cause analysis rely heavily on expert knowledge or fully supervised learning models, both of which require extensive labeled data and significant human effort. To address these challenges, we propose ALPHA, the first Active Learning Pipeline for Human-free log Analysis. ALPHA integrates semantic embedding, clustering-based representative sampling, and large language model (LLM)-assisted few-shot annotation to automate the anomaly detection process. The LLM annotated labels are propagated across clusters, enabling large-scale training of an anomaly detector with minimal supervision. To enhance the annotation accuracy, we propose a two-step few-shot refinement strategy that adaptively selects informative prompts based on the LLM's observed error patterns. Extensive experiments on real-world log datasets demonstrate that ALPHA achieves detection accuracy comparable to fully supervised methods while mitigating human efforts in the loop. ALPHA also supports interpretable analysis through LLM-driven root cause explanations in the post-detection stage. These capabilities make ALPHA a scalable and cost-efficient solution for truly automated log-based anomaly detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05936v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanhao Luo, Shivesh Madan Nath Jha, Akruti Sinha, Zhizhen Li, Yuchen Liu</dc:creator>
    </item>
    <item>
      <title>An Axiomatic Analysis of Path Selection Strategies for Multipath Transport in Path-Aware Networks</title>
      <link>https://arxiv.org/abs/2509.05938</link>
      <description>arXiv:2509.05938v1 Announce Type: new 
Abstract: Path-aware networking architectures like SCION provide end-hosts with explicit control over inter-domain routing, while multipath transport protocols like MPTCP and MPQUIC enable the concurrent use of multiple paths. This combination promises significant gains in performance and policy enforcement, but it also creates a stark trade-off between individual performance optimization and overall network stability. This paper quantifies this trade-off through a rigorous axiomatic analysis. We evaluate a spectrum of algorithms, from greedy (Min-RTT) and cooperative (Round-Robin) to hybrid approaches (Epsilon-Greedy), against axioms of Efficiency, Loss Avoidance, Stability, and Fairness in a simulated path-aware environment.
  Our simulations reveal that purely greedy strategies, while efficient under low contention, induce catastrophic packet loss, increasing by over &gt;18,000% as the number of competing agents grow, due to herd effects that cause severe network instability. Conversely, cooperative strategies ensure fairness and stability but at the cost of underutilizing high-capacity paths. Crucially, we demonstrate that hybrid strategies resolve this conflict. The Epsilon-Greedy algorithm, for instance, achieves the highest efficiency of all tested strategies in high-contention scenarios while mitigating the instability inherent to the greedy approach. Our axiomatic analysis suggests that tunable, hybrid algorithms are essential for designing robust and high-performance path selection mechanisms for next-generation networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05938v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alissa Baumeister, Sina Keshvadi</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Next-Generation Wireless Network Management: A Survey and Tutorial</title>
      <link>https://arxiv.org/abs/2509.05946</link>
      <description>arXiv:2509.05946v1 Announce Type: new 
Abstract: The rapid advancement toward sixth-generation (6G) wireless networks has significantly intensified the complexity and scale of optimization problems, including resource allocation and trajectory design, often formulated as combinatorial problems in large discrete decision spaces. However, traditional optimization methods, such as heuristics and deep reinforcement learning (DRL), struggle to meet the demanding requirements of real-time adaptability, scalability, and dynamic handling of user intents in increasingly heterogeneous and resource-constrained network environments. Large language models (LLMs) present a transformative paradigm by enabling natural language-driven problem formulation, context-aware reasoning, and adaptive solution refinement through advanced semantic understanding and structured reasoning capabilities. This paper provides a systematic and comprehensive survey of LLM-enabled optimization frameworks tailored for wireless networks. We first introduce foundational design concepts and distinguish LLM-enabled methods from conventional optimization paradigms. Subsequently, we critically analyze key enabling methodologies, including natural language modeling, solver collaboration, and solution verification processes. Moreover, we explore representative case studies to demonstrate LLMs' transformative potential in practical scenarios such as optimization formulation, low-altitude economy networking, and intent networking. Finally, we discuss current research challenges, examine prominent open-source frameworks and datasets, and identify promising future directions to facilitate robust, scalable, and trustworthy LLM-enabled optimization solutions for next-generation wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05946v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bisheng Wei, Ruihong Jiang, Ruichen Zhang, Yinqiu Liu, Dusit Niyato, Yaohua Sun, Yang Lu, Yonghui Li, Shiwen Mao, Chau Yuen, Marco Di Renzo, Mugen Peng</dc:creator>
    </item>
    <item>
      <title>Optimized Split Computing Framework for Edge and Core Devices</title>
      <link>https://arxiv.org/abs/2509.06049</link>
      <description>arXiv:2509.06049v1 Announce Type: new 
Abstract: With mobile networks expected to support services with stringent requirements that ensure high-quality user experience, the ability to apply Feed-Forward Neural Network (FFNN) models to User Equipment (UE) use cases has become critical. Given that UEs have limited resources, running FFNNs directly on UEs is an intrinsically challenging problem. This letter proposes an optimization framework for split computing applications where an FFNN model is partitioned into multiple sections, and executed by UEs, edge- and core-located nodes to reduce the required UE computational footprint while containing the inference time. An efficient heuristic strategy for solving the optimization problem is also provided. The proposed framework is shown to be robust in heterogeneous settings, eliminating the need for retraining and reducing the UE's memory (CPU) footprint by over 33.6% (60%).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06049v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Andrea Tassi, Oluwatayo Yetunde Kolawole, Joan Pujol Roig, Daniel Warren</dc:creator>
    </item>
    <item>
      <title>Understanding BBRv3 Performance in AQM-Enabled WiFi Networks</title>
      <link>https://arxiv.org/abs/2509.06245</link>
      <description>arXiv:2509.06245v1 Announce Type: new 
Abstract: We present a modular experimental testbed and lightweight visualization tool for evaluating TCP congestion control performance in wireless networks. We compare Google's latest Bottleneck Bandwidth and Round-trip time version 3 (BBRv3) algorithm with loss-based CUBIC under varying Active Queue Management (AQM) schemes, namely PFIFO, FQ-CoDel, and CAKE, on a Wi-Fi link using a commercial MikroTik router. Our real-time dashboard visualizes metrics such as throughput, latency, and fairness across competing flows. Results show that BBRv3 significantly improves fairness and convergence under AQM, especially with FQ-CoDel. Our visualization tool and modular testbed provide a practical foundation for evaluating next-generation TCP variants in real-world AQM-enabled home wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06245v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>IEEE LCN Conference 2025</arxiv:journal_reference>
      <dc:creator>Shyam Kumar Shrestha, Jonathan Kua, Shiva Raj Pokhrel</dc:creator>
    </item>
    <item>
      <title>Network-Aware Control of AGVs in an Industrial Scenario: A Simulation Study Based on ROS 2 and Gazebo</title>
      <link>https://arxiv.org/abs/2509.06451</link>
      <description>arXiv:2509.06451v1 Announce Type: new 
Abstract: Networked Control System (NCS) is a paradigm where sensors, controllers, and actuators communicate over a shared network. One promising application of NCS is the control of Automated Guided Vehicles (AGVs) in the industrial environment, for example to transport goods efficiently and to autonomously follow predefined paths or routes. In this context, communication and control are tightly correlated, a paradigm referred to as Joint Communication and Control (JCC), since network issues such as delays or errors can lead to significant deviations of the AGVs from the planned trajectory. In this paper, we present a simulation framework based on Gazebo and Robot Operating System 2 (ROS 2) to simulate and visualize, respectively, the complex interaction between the control of AGVs and the underlying communication network. This framework explicitly incorporates communication metrics, such as delay and packet loss, and control metrics, especially the Mean Squared Error (MSE) between the optimal/desired and actual path of the AGV in response to driving commands. Our results shed light into the correlation between the network performance, particularly Packet Reception Ratio (PRR), and accuracy of control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06451v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filippo Bragato, Tullia Fontana, Marco Giordani, Malte Schellmann, Josef Eichinger, Michele Zorzi</dc:creator>
    </item>
    <item>
      <title>Empirical Evaluation of a 5G Transparent Clock for Time Synchronization in a TSN-5G Network</title>
      <link>https://arxiv.org/abs/2509.06454</link>
      <description>arXiv:2509.06454v1 Announce Type: new 
Abstract: Time synchronization is essential for industrial IoT and Industry 4.0/5.0 applications, but achieving high synchronization accuracy in Time-Sensitive Networking (TSN)-5G networks is challenging due to jitter and asymmetric delays. 3GPP TS 23.501 defines three 5G synchronization modes: time-aware system, boundary clock (BC), and transparent clock (TC), where TC offers a promising solution. However, to the best of our knowledge, there is no empirical evaluation of TC in a TSN-5G network. This paper empirically evaluates an 5G end-to-end TC in a TSN-5G network, implemented on commercial TSN switches with a single clock. For TC development, we compute the residence time in 5G and recover the clock domain at the slave node. We deploy a TSN-5G testbed with commercial equipment for synchronization evaluation by modifying the Precision Timing Protocol (PTP) message transmission rates. Experimental results show a peak-to-peak synchronization of 500 ns, meeting the industrial requirement of &lt; 1 us, with minimal synchronization offsets for specific PTP message transmission rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06454v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Caleya-Sanchez, Pablo Mu\~noz, Jorge S\'anchez-Garrido, Emilio Florent\'in, Felix Delgado-Ferro, Pablo Rodriguez-Martin, Pablo Ameigeiras</dc:creator>
    </item>
    <item>
      <title>Five Blind Men and the Internet: Towards an Understanding of Internet Traffic</title>
      <link>https://arxiv.org/abs/2509.06515</link>
      <description>arXiv:2509.06515v1 Announce Type: new 
Abstract: The Internet, the world's largest and most pervasive network, lacks a transparent, granular view of its traffic patterns, volumes, and growth trends, hindering the networking community's understanding of its dynamics. This paper leverages publicly available Internet Exchange Point traffic statistics to address this gap, presenting a comprehensive two-year study (2023-2024) from 472 IXPs worldwide, capturing approximately 300 Tbps of peak daily aggregate traffic by late 2024. Our analysis reveals a 49.2% global traffic increase (24.5% annualized), uncovers regionally distinct diurnal patterns and event-driven anomalies, and demonstrates stable utilization rates, reflecting predictable infrastructure scaling. By analyzing biases and confirming high self-similarity, we establish IXP traffic as a robust proxy for overall Internet growth and usage behavior. With transparent, replicable data--covering 87% of the worldwide IXP port capacity--and plans to release our dataset, this study offers a verifiable foundation for long-term Internet traffic monitoring. In particular, our findings shed light on the interplay between network design and function, providing an accessible framework for researchers and operators to explore the Internet's evolving ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06515v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ege Cem Kirci, Ayush Mishra, Laurent Vanbever</dc:creator>
    </item>
    <item>
      <title>Ghost Points Matter: Far-Range Vehicle Detection with a Single mmWave Radar in Tunnel</title>
      <link>https://arxiv.org/abs/2509.06639</link>
      <description>arXiv:2509.06639v1 Announce Type: new 
Abstract: Vehicle detection in tunnels is crucial for traffic monitoring and accident response, yet remains underexplored. In this paper, we develop mmTunnel, a millimeter-wave radar system that achieves far-range vehicle detection in tunnels. The main challenge here is coping with ghost points caused by multi-path reflections, which lead to severe localization errors and false alarms. Instead of merely removing ghost points, we propose correcting them to true vehicle positions by recovering their signal reflection paths, thus reserving more data points and improving detection performance, even in occlusion scenarios. However, recovering complex 3D reflection paths from limited 2D radar points is highly challenging. To address this problem, we develop a multi-path ray tracing algorithm that leverages the ground plane constraint and identifies the most probable reflection path based on signal path loss and spatial distance. We also introduce a curve-to-plane segmentation method to simplify tunnel surface modeling such that we can significantly reduce the computational delay and achieve real-time processing.
  We have evaluated mmTunnel with comprehensive experiments. In two test tunnels, we conducted controlled experiments in various scenarios with cars and trucks. Our system achieves an average F1 score of 93.7% for vehicle detection while maintaining real-time processing. Even in the challenging occlusion scenarios, the F1 score remains above 91%. Moreover, we collected extensive data from a public tunnel with heavy traffic at times and show our method could achieve an F1 score of 91.5% in real-world traffic conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06639v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenming He, Rui Xia, Chengzhen Meng, Xiaoran Fan, Dequan Wang, Haojie Ren, Jianmin Ji, Yanyong Zhang</dc:creator>
    </item>
    <item>
      <title>Sovereign AI for 6G: Towards the Future of AI-Native Networks</title>
      <link>https://arxiv.org/abs/2509.06700</link>
      <description>arXiv:2509.06700v1 Announce Type: new 
Abstract: The advent of Generative Artificial Intelligence (GenAI), Large Language Models (LLMs), and Large Telecom Models (LTM) significantly reshapes mobile networks, especially as the telecom industry transitions from 5G's cloud-centric to AI-native 6G architectures. This transition unlocks unprecedented capabilities in real-time automation, semantic networking, and autonomous service orchestration. However, it introduces critical risks related to data sovereignty, security, explainability, and regulatory compliance especially when AI models are trained, deployed, or governed externally. This paper introduces the concept of `Sovereign AI' as a strategic imperative for 6G, proposing architectural, operational, and governance frameworks that enable national or operator-level control over AI development, deployment, and life-cycle management. Focusing on O-RAN architecture, we explore how sovereign AI-based xApps and rApps can be deployed Near-RT and Non-RT RICs to ensure policy-aligned control, secure model updates, and federated learning across trusted infrastructure. We analyse global strategies, technical enablers, and challenges across safety, talent, and model governance. Our findings underscore that Sovereign AI is not just a regulatory necessity but a foundational pillar for secure, resilient, and ethically-aligned 6G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06700v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swarna Bindu Chetty, David Grace, Simon Saunders, Paul Harris, Eirini Eleni Tsiropoulou, Tony Quek, Hamed Ahmadi</dc:creator>
    </item>
    <item>
      <title>VariSAC: V2X Assured Connectivity in RIS-Aided ISAC via GNN-Augmented Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2509.06763</link>
      <description>arXiv:2509.06763v2 Announce Type: new 
Abstract: The integration of Reconfigurable Intelligent Surfaces (RIS) and Integrated Sensing and Communication (ISAC) in vehicular networks enables dynamic spatial resource management and real-time adaptation to environmental changes. However, the coexistence of distinct vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) connectivity requirements, together with highly dynamic and heterogeneous network topologies, presents significant challenges for unified reliability modeling and resource optimization. To address these issues, we propose VariSAC, a graph neural network (GNN)-augmented deep reinforcement learning framework for assured, time-continuous connectivity in RIS-assisted, ISAC-enabled vehicle-to-everything (V2X) systems. Specifically, we introduce the Continuous Connectivity Ratio (CCR), a unified metric that characterizes the sustained temporal reliability of V2I connections and the probabilistic delivery guarantees of V2V links, thus unifying their continuous reliability semantics. Next, we employ a GNN with residual adapters to encode complex, high-dimensional system states, capturing spatial dependencies among vehicles, base stations (BS), and RIS nodes. These representations are then processed by a Soft Actor-Critic (SAC) agent, which jointly optimizes channel allocation, power control, and RIS configurations to maximize CCR-driven long-term rewards. Extensive experiments on real-world urban datasets demonstrate that VariSAC consistently outperforms existing baselines in terms of continuous V2I ISAC connectivity and V2V delivery reliability, enabling persistent connectivity in highly dynamic vehicular environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06763v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Huijun Tang, Wang Zeng, Ming Du, Pinlong Zhao, Pengfei Jiao, Huaming Wu, Hongjian Sun</dc:creator>
    </item>
    <item>
      <title>Resilience of Mega-Satellite Constellations: How Node Failures Impact Inter-Satellite Networking Over Time?</title>
      <link>https://arxiv.org/abs/2509.06766</link>
      <description>arXiv:2509.06766v1 Announce Type: new 
Abstract: Mega-satellite constellations have the potential to leverage inter-satellite links to deliver low-latency end-to-end communication services globally, thereby extending connectivity to underserved regions. However, harsh space environments make satellites vulnerable to failures, leading to node removals that disrupt inter-satellite networking. With the high risk of satellite node failures, understanding their impact on end-to-end services is essential. This study investigates the importance of individual nodes on inter-satellite networking and the resilience of mega satellite constellations against node failures. We represent the mega-satellite constellation as discrete temporal graphs and model node failure events accordingly. To quantify node importance for targeted services over time, we propose a service-aware temporal betweenness metric. Leveraging this metric, we develop an analytical framework to identify critical nodes and assess the impact of node failures. The framework takes node failure events as input and efficiently evaluates their impacts across current and subsequent time windows. Simulations on the Starlink constellation setting reveal that satellite networks inherently exhibit resilience to node failures, as their dynamic topology partially restore connectivity and mitigate the long-term impact. Furthermore, we find that the integration of rerouting mechanisms is crucial for unleashing the full resilience potential to ensure rapid recovery of inter-satellite networking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06766v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Binquan Guo, Zehui Xiong, Zhou Zhang, Baosheng Li, Dusit Niyato, Chau Yuen, Zhu Han</dc:creator>
    </item>
    <item>
      <title>BatStation: Toward In-Situ Radar Sensing on 5G Base Stations with Zero-Shot Template Generation</title>
      <link>https://arxiv.org/abs/2509.06898</link>
      <description>arXiv:2509.06898v1 Announce Type: new 
Abstract: The coexistence between incumbent radar signals and commercial 5G signals necessitates a versatile and ubiquitous radar sensing for efficient and adaptive spectrum sharing. In this context, leveraging the densely deployed 5G base stations (BS) for radar sensing is particularly promising, offering both wide coverage and immediate feedback to 5G scheduling. However, the targeting radar signals are superimposed with concurrent 5G uplink transmissions received by the BS, and practical deployment also demands a lightweight, portable radar sensing model. This paper presents BatStation, a lightweight, in-situ radar sensing framework seamlessly integrated into 5G BSs. BatStation leverages uplink resource grids to extract radar signals through three key components: (i) radar signal separation to cancel concurrent 5G transmissions and reveal the radar signals, (ii) resource grid reshaping to align time-frequency resolution with radar pulse characteristics, and (iii) zero-shot template correlation based on a portable model trained purely on synthetic data that supports detection, classification, and localization of radar pulses without fine-tuning using experimental data. We implement BatStation on a software-defined radio (SDR) testbed and evaluate its performance with real 5G traffic in the CBRS band. Results show robust performance across diverse radar types, achieving detection probabilities of 97.02% (PUCCH) and 79.23% (PUSCH), classification accuracy up to 97.00%, and median localization errors of 2.68-6.20 MHz (frequency) and 24.6-32.4 microseconds (time). Notably, BatStation achieves this performance with a runtime latency of only 0.11/0.94 ms on GPU/CPU, meeting the real-time requirement of 5G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06898v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihui Gao, Zhecun Liu, Tingjun Chen</dc:creator>
    </item>
    <item>
      <title>A Framework for Detection and Classification of Attacks on Surveillance Cameras under IoT Networks</title>
      <link>https://arxiv.org/abs/2509.05366</link>
      <description>arXiv:2509.05366v1 Announce Type: cross 
Abstract: The increasing use of Internet of Things (IoT) devices has led to a rise in security related concerns regarding IoT Networks. The surveillance cameras in IoT networks are vulnerable to security threats such as brute force and zero-day attacks which can lead to unauthorized access by hackers and potential spying on the users activities. Moreover, these cameras can be targeted by Denial of Service (DOS) attacks, which will make it unavailable for the user. The proposed AI based framework will leverage machine learning algorithms to analyze network traffic and detect anomalous behavior, allowing for quick detection and response to potential intrusions. The framework will be trained and evaluated using real-world datasets to learn from past security incidents and improve its ability to detect potential intrusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05366v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Umair Amjid, M. Umar Khan, S. A. Manan Kirmani</dc:creator>
    </item>
    <item>
      <title>A Dynamic Programming Framework for Vehicular Task Offloading with Successive Action Improvement</title>
      <link>https://arxiv.org/abs/2509.05907</link>
      <description>arXiv:2509.05907v1 Announce Type: cross 
Abstract: In this paper, task offloading from vehicles with random velocities is optimized via a novel dynamic programming framework. Particularly, in a vehicular network with multiple vehicles and base stations (BSs), computing tasks of vehicles are offloaded via BSs to an edge server. Due to the random velocities, the exact locations of vehicles versus time, namely trajectories, cannot be determined in advance. Hence, instead of deterministic optimization, the cell association, uplink time, and throughput allocation of multiple vehicles during a period of task offloading are formulated as a finite-horizon Markov decision process. In order to derive a low-complexity solution algorithm, a two-time-scale framework is proposed. The scheduling period is divided into super slots, each super slot is further divided into a number of time slots. At the beginning of each super slot, we first obtain a reference scheduling scheme of cell association, uplink time and throughput allocation via deterministic optimization, yielding an approximation of the optimal value function. Within the super slot, the actual scheduling action of each time slot is determined by making improvement to the approximate value function according to the system state. Due to the successive improvement framework, a non-trivial average cost upper bound could be derived. In the simulation, the random trajectories of vehicles are generated from a high-fidelity traffic simulator. It is shown that the performance gain of the proposed scheduling framework over the baselines is significant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05907v1</guid>
      <category>eess.SY</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qianren Li, Yuncong Hong, Bojie Lv, Rui Wang</dc:creator>
    </item>
    <item>
      <title>Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs</title>
      <link>https://arxiv.org/abs/2509.06550</link>
      <description>arXiv:2509.06550v1 Announce Type: cross 
Abstract: Network intrusion detection remains a critical challenge in cybersecurity. While supervised machine learning models achieve state-of-the-art performance, their reliance on large labelled datasets makes them impractical for many real-world applications. Anomaly detection methods, which train exclusively on benign traffic to identify malicious activity, suffer from high false positive rates, limiting their usability. Recently, self-supervised learning techniques have demonstrated improved performance with lower false positive rates by learning discriminative latent representations of benign traffic. In particular, contrastive self-supervised models achieve this by minimizing the distance between similar (positive) views of benign traffic while maximizing it between dissimilar (negative) views. Existing approaches generate positive views through data augmentation and treat other samples as negative. In contrast, this work introduces Contrastive Learning using Augmented Negative pairs (CLAN), a novel paradigm for network intrusion detection where augmented samples are treated as negative views - representing potentially malicious distributions - while other benign samples serve as positive views. This approach enhances both classification accuracy and inference efficiency after pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset demonstrates that the proposed method surpasses existing self-supervised and anomaly detection techniques in a binary classification task. Furthermore, when fine-tuned on a limited labelled dataset, the proposed approach achieves superior multi-class classification performance compared to existing self-supervised models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06550v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/CSR64739.2025.11129979</arxiv:DOI>
      <arxiv:journal_reference>2025 IEEE International Conference on Cyber Security and Resilience (CSR), Chania, Crete, Greece, 2025, pp. 206-213</arxiv:journal_reference>
      <dc:creator>Jack Wilkie, Hanan Hindy, Christos Tachtatzis, Robert Atkinson</dc:creator>
    </item>
    <item>
      <title>Network-level Censorship Attacks in the InterPlanetary File System</title>
      <link>https://arxiv.org/abs/2509.06626</link>
      <description>arXiv:2509.06626v1 Announce Type: cross 
Abstract: The InterPlanetary File System (IPFS) has been successfully established as the de facto standard for decentralized data storage in the emerging Web3. Despite its decentralized nature, IPFS nodes, as well as IPFS content providers, have converged to centralization in large public clouds. Centralization introduces BGP routing-based attacks, such as passive interception and BGP hijacking, as potential threats. Although this attack vector has been investigated for many other Web3 protocols, such as Bitcoin and Ethereum, to the best of our knowledge, it has not been analyzed for the IPFS network. In our work, we bridge this gap and demonstrate that BGP routing attacks can be effectively leveraged to censor content in IPFS. For the analysis, we collected 3,000 content blocks called CIDs and conducted a simulation of BGP hijacking and passive interception against them. We find that a single malicious AS can censor 75% of the IPFS content for more than 57% of all requester nodes. Furthermore, we show that even with a small set of only 62 hijacked prefixes, 70% of the full attack effectiveness can already be reached. We further propose and validate countermeasures based on global collaborative content replication among all nodes in the IPFS network, together with additional robust backup content provider nodes that are well-hardened against BGP hijacking. We hope this work raises awareness about the threat BGP routing-based attacks pose to IPFS and triggers further efforts to harden the live IPFS network against them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06626v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Matter, Muoi Tran</dc:creator>
    </item>
    <item>
      <title>Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing</title>
      <link>https://arxiv.org/abs/2509.06640</link>
      <description>arXiv:2509.06640v1 Announce Type: cross 
Abstract: We propose a simple algorithm that needs only a few data samples from a single graph for learning local routing policies that generalize across a rich class of geometric random graphs in Euclidean metric spaces. We thus solve the all-pairs near-shortest path problem by training deep neural networks (DNNs) that let each graph node efficiently and scalably route (i.e., forward) packets by considering only the node's state and the state of the neighboring nodes. Our algorithm design exploits network domain knowledge in the selection of input features and design of the policy function for learning an approximately optimal policy. Domain knowledge also provides theoretical assurance that the choice of a ``seed graph'' and its node data sampling suffices for generalizable learning. Remarkably, one of these DNNs we train -- using distance-to-destination as the only input feature -- learns a policy that exactly matches the well-known Greedy Forwarding policy, which forwards packets to the neighbor with the shortest distance to the destination. We also learn a new policy, which we call GreedyTensile routing -- using both distance-to-destination and node stretch as the input features -- that almost always outperforms greedy forwarding. We demonstrate the explainability and ultra-low latency run-time operation of Greedy Tensile routing by symbolically interpreting its DNN in low-complexity terms of two linear actions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06640v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yung-Fu Chen, Sen Lin, Anish Arora</dc:creator>
    </item>
    <item>
      <title>Zeropod: Simplifying Datacenter Networking with Future-Proof Zero-Buffer Packet Switches</title>
      <link>https://arxiv.org/abs/2109.13065</link>
      <description>arXiv:2109.13065v2 Announce Type: replace 
Abstract: With the rapid growth of traffic volume in datacenter networks (DCNs), packet switches suffer from insufficient switching chip capacity and difficulties in transmission control, making it challenging to provide high goodput and low latency for emerging cloud applications.
  We present Zeropod, a future-proof DCN architecture featuring simplified zero-buffer packet switches inside the point-of-delivery (pod). Within each pod, traffic transmission is scheduled by a per-pod centralized scheduler for collision avoidance, enabling a highly simplified data plane, facilitating benefits like higher switching capacity and precise transmission control. Among the pods, buffered Core switches work as barriers and relay inter-pod data, limiting the scope of centralized scheduling and thus simplifying the control plane. Zeropod combines host-level and flow-level scheduling for high performance with low overhead. Evaluation results show that Zeropod consistently performs better or equivalent to traditional buffered DCN, particularly regarding flow completion time (FCT). When accounting for the increased switching capacity due to the removal of buffers, its performance is further improved. Zeropod explores an extreme end of the design spectrum, and we hope it can encourage further exploration in the DCN community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.13065v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cong Liang, Jing Cheng, Mowei Wang, Yashe Liu, Zhenhua Liu, Yong Cui</dc:creator>
    </item>
    <item>
      <title>Timely and Energy-Efficient Information Delivery in Heterogeneous Correlated Random Access Networks</title>
      <link>https://arxiv.org/abs/2503.03097</link>
      <description>arXiv:2503.03097v2 Announce Type: replace 
Abstract: This paper characterizes and jointly optimizes Age of Information (AoI) and energy efficiency in heterogeneous correlated random access networks, where each sensor adopts a distinct transmission probability and its observations are correlated with those of other sensors. An analytical model is proposed to analyze AoI and energy efficiency for each sensor. Closed-form expressions for long-term average AoI and energy efficiency are derived, explicitly accounting for spatial correlation and state-dependent power consumption. By constraining sensors to adopt the same transmission probability, three unified transmission strategies are derived: the age-optimal strategy (q_A^), the energy-efficiency optimal strategy (q_E^), and the Pareto-optimal strategy (q^), which jointly optimizes AoI and energy efficiency. A bounded exhaustive search with O(1/(n q_epsilon)) complexity guarantees efficient computation of q^. Theoretically, the correlation gain is proven to significantly enhance both metrics under spatial correlation. To exploit sensor heterogeneity, a gradient-based iterative algorithm, Multi-Start Projected Adaptive Moment Estimation (MS-PAdam), is proposed to jointly optimize all sensors' transmission probabilities, efficiently converging to the optimal AoI-energy-efficiency tradeoff. Crucially, MS-PAdam adaptively suppresses transmissions where marginal gains are outweighed by correlated neighbors' contributions, substantially alleviating competition. Numerical results show MS-PAdam outperforms unified strategies, achieving harmonious operation that mitigates AoI/energy degradation in contention-intensive scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03097v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anshan Yuan, Xinghua Sun, Yayu Gao, Wen Zhan, Xiang Chen</dc:creator>
    </item>
    <item>
      <title>A QoS Framework for Service Provision in Multi-Infrastructure-Sharing Networks</title>
      <link>https://arxiv.org/abs/2509.01694</link>
      <description>arXiv:2509.01694v2 Announce Type: replace 
Abstract: We propose a framework for resource provisioning with QoS guarantees in shared infrastructure networks. Our novel framework provides tunable probabilistic service guarantees for throughput and delay. Key to our approach is a Modified Dirft-plus-Penalty (MDP) policy that ensures long-term stability while capturing short-term probabilistic service guarantees using linearized upper-confidence bounds. We characterize the feasible region of service guarantees and show that our MDP procedure achieves mean rate stability and an optimality gap that vanishes with the frame size over which service guarantees are provided. Finally, empirical simulations validate our theory and demonstrate the favorable performance of our algorithm in handling QoS in multi-infrastructure networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01694v2</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3704413.3764414</arxiv:DOI>
      <dc:creator>Quang Minh Nguyen, Eytan Modiano</dc:creator>
    </item>
    <item>
      <title>Grassroots Consensus</title>
      <link>https://arxiv.org/abs/2505.19216</link>
      <description>arXiv:2505.19216v3 Announce Type: replace-cross 
Abstract: Consider people with smartphones operating without external authorities or global resources other than the network itself. In this setting, high-end applications supporting sovereign democratic digital communities, community banks, and digital cooperatives require consensus executed by community members, which must be reconfigurable to support community dynamics.
  The Constitutional Consensus protocol aims to address this need by introducing constitutional self-governance to consensus: participants dynamically amend the participant set, supermajority threshold, and timeout parameter through the consensus protocol itself. We achieve this by enhancing a DAG-based protocol (like Cordial Miners) with participant-controlled reconfiguration, while also supporting both high- and low-throughput operation (like Morpheus), remaining quiescent when idle. This three-way synthesis uniquely combines: (1) constitutional amendments for self-governance, (2) a cryptographic DAG structure for simplicity, parallelism, and throughput, and (3) both high- and low-throughput operation. The protocol achieves consensus in $3\delta$, maintains O(n) amortized communication complexity during high throughput, and seamlessly transitions between modes. The basic protocol (without constitutional amendments) realizes these features in 25 lines of pseudocode, making it one of the most concise consensus protocols for eventual synchrony.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19216v3</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Idit Keidar, Andrew Lewis-Pye, Ehud Shapiro</dc:creator>
    </item>
    <item>
      <title>Online Identification of IT Systems through Active Causal Learning</title>
      <link>https://arxiv.org/abs/2509.02130</link>
      <description>arXiv:2509.02130v2 Announce Type: replace-cross 
Abstract: Identifying a causal model of an IT system is fundamental to many branches of systems engineering and operation. Such a model can be used to predict the effects of control actions, optimize operations, diagnose failures, detect intrusions, etc., which is central to achieving the longstanding goal of automating network and system management tasks. Traditionally, causal models have been designed and maintained by domain experts. This, however, proves increasingly challenging with the growing complexity and dynamism of modern IT systems. In this paper, we present the first principled method for online, data-driven identification of an IT system in the form of a causal model. The method, which we call active causal learning, estimates causal functions that capture the dependencies among system variables in an iterative fashion using Gaussian process regression based on system measurements, which are collected through a rollout-based intervention policy. We prove that this method is optimal in the Bayesian sense and that it produces effective interventions. Experimental validation on a testbed shows that our method enables accurate identification of a causal system model while inducing low interference with system operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02130v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kim Hammar, Rolf Stadler</dc:creator>
    </item>
  </channel>
</rss>
