<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Jun 2024 04:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Effectiveness of denoising diffusion probabilistic models for fast and high-fidelity whole-event simulation in high-energy heavy-ion experiments</title>
      <link>https://arxiv.org/abs/2406.01602</link>
      <description>arXiv:2406.01602v1 Announce Type: new 
Abstract: Artificial intelligence (AI) generative models, such as generative adversarial networks (GANs), variational auto-encoders, and normalizing flows, have been widely used and studied as efficient alternatives for traditional scientific simulations. However, they have several drawbacks, including training instability and inability to cover the entire data distribution, especially for regions where data are rare. This is particularly challenging for whole-event, full-detector simulations in high-energy heavy-ion experiments, such as sPHENIX at the Relativistic Heavy Ion Collider and Large Hadron Collider experiments, where thousands of particles are produced per event and interact with the detector. This work investigates the effectiveness of Denoising Diffusion Probabilistic Models (DDPMs) as an AI-based generative surrogate model for the sPHENIX experiment that includes the heavy-ion event generation and response of the entire calorimeter stack. DDPM performance in sPHENIX simulation data is compared with a popular rival, GANs. Results show that both DDPMs and GANs can reproduce the data distribution where the examples are abundant (low-to-medium calorimeter energies). Nonetheless, DDPMs significantly outperform GANs, especially in high-energy regions where data are rare. Additionally, DDPMs exhibit superior stability compared to GANs. The results are consistent between both central and peripheral centrality heavy-ion collision events. Moreover, DDPMs offer a substantial speedup of approximately a factor of 100 compared to the traditional Geant4 simulation method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01602v1</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <category>nucl-ex</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeonju Go, Dmitrii Torbunov, Timothy Rinn, Yi Huang, Haiwang Yu, Brett Viren, Meifeng Lin, Yihui Ren, Jin Huang</dc:creator>
    </item>
    <item>
      <title>Parnassus: An Automated Approach to Accurate, Precise, and Fast Detector Simulation and Reconstruction</title>
      <link>https://arxiv.org/abs/2406.01620</link>
      <description>arXiv:2406.01620v1 Announce Type: new 
Abstract: Detector simulation and reconstruction are a significant computational bottleneck in particle physics. We develop Particle-flow Neural Assisted Simulations (Parnassus) to address this challenge. Our deep learning model takes as input a point cloud (particles impinging on a detector) and produces a point cloud (reconstructed particles). By combining detector simulations and reconstruction into one step, we aim to minimize resource utilization and enable fast surrogate models suitable for application both inside and outside large collaborations. We demonstrate this approach using a publicly available dataset of jets passed through the full simulation and reconstruction pipeline of the CMS experiment. We show that Parnassus accurately mimics the CMS particle flow algorithm on the (statistically) same events it was trained on and can generalize to jet momentum and type outside of the training distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01620v1</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Etienne Dreyer, Eilam Gross, Dmitrii Kobylianskii, Vinicius Mikuni, Benjamin Nachman, Nathalie Soybelman</dc:creator>
    </item>
    <item>
      <title>SwdFold:A Reweighting and Unfolding method based on Optimal Transport Theory</title>
      <link>https://arxiv.org/abs/2406.01635</link>
      <description>arXiv:2406.01635v1 Announce Type: new 
Abstract: High-energy physics experiments rely heavily on precise measurements of energy and momentum, yet face significant challenges due to detector limitations, calibration errors, and the intrinsic nature of particle interactions. Traditional unfolding techniques have been employed to correct for these distortions, yet they often suffer from model dependency and stability issues. We present a novel method, SwdFold, which utilizes the principles of optimal transport to provide a robust, model-independent framework to estimate the probability density ratio for data unfolding. It not only unfold the toy experimental event by reweighted simulated data distributions closely with true distributions but also maintains the integrity of physical features across various observables. We can expect it can enable more reliable predictions and comprehensive analyses as a high precision reweighting and unfolding tool in high-energy physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01635v1</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chu-Cheng Pan, Xiang Dong, Yu-Chang Sun, Ao-Yan Cheng, Ao-Bo Wang, Yu-Xuan Hu, Hao Cai</dc:creator>
    </item>
    <item>
      <title>Markov Chain Monte Carlo with Gaussian Process Emulation for a 1D Hemodynamics Model of CTEPH</title>
      <link>https://arxiv.org/abs/2406.01599</link>
      <description>arXiv:2406.01599v1 Announce Type: cross 
Abstract: Microvascular disease is a contributor to persistent pulmonary hypertension in those with chronic thromboembolic pulmonary hypertension (CTEPH). The heterogenous nature of the micro and macrovascular defects motivates the use of personalized computational models, which can predict flow dynamics within multiple generations of the arterial tree and into the microvasculature. Our study uses computational hemodynamics models and Gaussian processes for rapid, subject-specific calibration using retrospective data from a large animal model of CTEPH. Our subject-specific predictions shed light on microvascular dysfunction and arterial wall shear stress changes in CTEPH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01599v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amirreza Kachabi, Mitchel J. Colebank, Sofia Altieri Correa, Naomi C. Chesler</dc:creator>
    </item>
    <item>
      <title>Learning dynamical models from stochastic trajectories</title>
      <link>https://arxiv.org/abs/2406.02363</link>
      <description>arXiv:2406.02363v1 Announce Type: cross 
Abstract: The dynamics of biological systems, from proteins to cells to organisms, is complex and stochastic. To decipher their physical laws, we need to bridge between experimental observations and theoretical modeling. Thanks to progress in microscopy and tracking, there is today an abundance of experimental trajectories reflecting these dynamical laws. Inferring physical models from noisy and imperfect experimental data, however, is challenging. Because there are no inference methods that are robust and efficient, model reconstruction from experimental trajectories is a bottleneck to data-driven biophysics. In this Thesis, I present a set of tools developed to bridge this gap and permit robust and universal inference of stochastic dynamical models from experimental trajectories. These methods are rooted in an information-theoretical framework that quantifies how much can be inferred from trajectories that are short, partial and noisy. They permit the efficient inference of dynamical models for overdamped and underdamped Langevin systems, as well as the inference of entropy production rates. I finally present early applications of these techniques, as well as future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02363v1</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Ronceray</dc:creator>
    </item>
  </channel>
</rss>
