<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 Jan 2026 02:39:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Graph structure learning for stable processes</title>
      <link>https://arxiv.org/abs/2601.06264</link>
      <description>arXiv:2601.06264v1 Announce Type: new 
Abstract: We introduce Ising-H\"usler-Reiss processes, a new class of multivariate L\'evy processes that allows for sparse modeling of the path-wise conditional independence structure between marginal stable processes with different stability indices. The underlying conditional independence graph is encoded as zeroes in a suitable precision matrix. An Ising-type parametrization of the weights for each orthant of the L\'evy measure allows for data-driven modeling of asymmetry of the jumps while retaining an arbitrary sparse graph. We develop consistent estimators for the graphical structure and asymmetry parameters, relying on a new uniform small-time approximation for L\'evy processes. The methodology is illustrated in simulations and a real data application to modeling dependence of stock returns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06264v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Br\"uck, Sebastian Engelke, Stanislav Volgushev</dc:creator>
    </item>
    <item>
      <title>A Framework for Estimating Restricted Mean Survival Time Difference using Pseudo-observations</title>
      <link>https://arxiv.org/abs/2601.06296</link>
      <description>arXiv:2601.06296v1 Announce Type: new 
Abstract: A targeted learning (TL) framework is developed to estimate the difference in the restricted mean survival time (RMST) for a clinical trial with time-to-event outcomes. The approach starts by defining the target estimand as the RMST difference between investigational and control treatments. Next, an efficient estimation method is introduced: a targeted minimum loss estimator (TMLE) utilizing pseudo-observations. Moreover, a version of the copy reference (CR) approach is developed to perform a sensitivity analysis for right-censoring. The proposed TL framework is demonstrated using a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06296v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Man Jin, Yixin Fang</dc:creator>
    </item>
    <item>
      <title>Empirical Likelihood Test for Common Invariant Subspace of Multilayer Networks based on Monte Carlo Approximation</title>
      <link>https://arxiv.org/abs/2601.06390</link>
      <description>arXiv:2601.06390v1 Announce Type: new 
Abstract: Multilayer (or multiple) networks are widely used to represent diverse patterns of relationships among objects in increasingly complex real-world systems. Identifying a common invariant subspace across network layers has become an active area of research, as such a subspace can filter out layer-specific noise, facilitate cross-network comparisons, reduce dimensionality, and extract shared structural features of scientific interest. One statistical approach to detecting a common subspace is hypothesis testing, which evaluates whether the observed networks share a common latent structure. In this paper, we propose an empirical likelihood (EL) based test for this purpose. The null hypothesis states that all network layers share the same invariant subspace, whereas under the alternative hypothesis at least two layers differ in their subspaces. We study the asymptotic behavior of the proposed test via Monte Carlo approximation and assess its finite-sample performance through extensive simulations. The simulation results demonstrate that the proposed method achieves satisfactory size and power, and its practical utility is further illustrated with a real-data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06390v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianqian Yao</dc:creator>
    </item>
    <item>
      <title>Triple-dyad ratio estimation for the $p_1$ model</title>
      <link>https://arxiv.org/abs/2601.06481</link>
      <description>arXiv:2601.06481v1 Announce Type: new 
Abstract: Although the $p_1$ model was proposed 40 years ago, little progress has been made to address asymptotic theories in this model, that is, neither consistency of the maximum likelihood estimator (MLE) nor other parameter estimation with statistical guarantees is understood. This problem has been acknowledged as a long-standing open problem. To address it, we propose a novel parametric estimation method based on the ratios of the sum of a sequence of triple-dyad indicators to another one, where a triple-dyad indicator means the product of three dyad indicators. Our proposed estimators, called \emph{triple-dyad ratio estimator}, have explicit expressions and can be scaled to very large networks with millions of nodes. We establish the consistency and asymptotic normality of the triple-dyad ratio estimator when the number of nodes reaches infinity. Based on the asymptotic results, we develop a test statistic for evaluating whether is a reciprocity effect in directed networks. The estimators for the density and reciprocity parameters contain bias terms, where analytical bias correction formulas are proposed to make valid inference. Numerical studies demonstrate the findings of our theories and show that the estimator is comparable to the MLE in large networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06481v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qunqiang Feng, Yaru Tian, Ting Yan</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization of Noisy Log-Likelihoods Evaluated by Particle Filters -- One Parameter Case --</title>
      <link>https://arxiv.org/abs/2601.06545</link>
      <description>arXiv:2601.06545v1 Announce Type: new 
Abstract: Likelihood functions evaluated using particle filters are typically noisy, computationally expensive, and non-differentiable due to Monte Carlo variability. These characteristics make conventional optimization methods difficult to apply directly or potentially unreliable. This paper investigates the use of Bayesian optimization for maximizing log-likelihood functions estimated by particle filters. By modeling the noisy log-likelihood surface with a Gaussian process surrogate and employing an acquisition function that balances exploration and exploitation, the proposed approach identifies the maximizer using a limited number of likelihood evaluations. Through numerical experiments, we demonstrate that Bayesian optimization provides robust and stable estimation in the presence of observation noise. The results suggest that Bayesian optimization is a promising alternative for likelihood maximization problems where exhaustive search or gradient-based methods are impractical. The estimation accuracy is quantitatively assessed using mean squared error metrics by comparison with the exact maximum likelihood solution obtained via the Kalman filter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06545v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Genshiro Kitagawa (Tokyo University of Marine Science,Technology,The Institute of Statistical Mathematics)</dc:creator>
    </item>
    <item>
      <title>Mittag Leffler Distributions Estimation and Autoregressive Framework</title>
      <link>https://arxiv.org/abs/2601.06610</link>
      <description>arXiv:2601.06610v1 Announce Type: new 
Abstract: This work deals with the estimation of parameters of Mittag-Leffler (ML($\alpha, \sigma$)) distribution. We estimate the parameters of ML($\alpha, \sigma$) using empirical Laplace transform method. The simulation study indicates that the proposed method provides satisfactory results. The real life application of ML($\alpha, \sigma$) distribution on high frequency trading data is also demonstrated. We also provide the estimation of three-parameter Mittag-Leffler distribution using empirical Laplace transform. Additionally, we establish an autoregressive model of order 1, incorporating the Mittag-Leffler distribution as marginals in one scenario and as innovation terms in another. We apply empirical Laplace transform method to estimate the model parameters and provide the simulation study for the same.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06610v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monika S. Dhull</dc:creator>
    </item>
    <item>
      <title>Censored Graphical Horseshoe: Bayesian sparse precision matrix estimation with censored and missing data</title>
      <link>https://arxiv.org/abs/2601.06671</link>
      <description>arXiv:2601.06671v1 Announce Type: new 
Abstract: Gaussian graphical models provide a powerful framework for studying conditional dependencies in multivariate data, with widespread applications spanning biomedical, environmental sciences, and other data-rich scientific domains. While the Graphical Horseshoe (GHS) method has emerged as a state-of-the-art Bayesian method for sparse precision matrix estimation, existing approaches assume fully observed data and thus fail in the presence of censoring or missingness, which are pervasive in real-world studies. In this paper, we develop the Censored Graphical Horseshoe (CGHS), a novel Bayesian framework that extends the GHS to censored and arbitrarily missing Gaussian data. By introducing a latent-variable representation, CGHS accommodates incomplete observations while retaining the adaptive global-local shrinkage properties of the Horseshoe prior. We derive efficient Gibbs samplers for posterior computation and establish new theoretical results on posterior behavior under censoring and missingness, filling a gap not addressed by frequentist Lasso-based methods. Through extensive simulations, we demonstrate that CGHS consistently improves estimation accuracy compared to penalized likelihood approaches. Our methods are implemented in the package GHScenmis available on Github: https://github.com/tienmt/ghscenmis .</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06671v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>The Tien Mai, Sayantan Banerjee</dc:creator>
    </item>
    <item>
      <title>R-Estimation with Right-Censored Data</title>
      <link>https://arxiv.org/abs/2601.06685</link>
      <description>arXiv:2601.06685v1 Announce Type: new 
Abstract: This paper considers the problem of directly generalizing the R-estimator under a linear model formulation with right-censored outcomes. We propose a natural generalization of the rank and corresponding estimating equation for the R-estimator in the case of the Wilcoxon (i.e., linear-in-ranks) score function, and show how it can respectively be exactly represented as members of the classes of estimating equations proposed in Ritov (1990) and Tsiatis (1990). We then establish analogous results for a large class of bounded nonlinear-in-ranks score functions. Asymptotics and variance estimation are obtained as straightforward consequences of these representation results. The self-consistent estimator of the residual distribution function, and the mid-cumulative distribution function (and, where needed, a generalization of it), play critical roles in these developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06685v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Glen A. Satten, Mo Li, Ni Zhao, Robert L. Strawderman</dc:creator>
    </item>
    <item>
      <title>Nonparametric contaminated Gaussian mixture of regressions</title>
      <link>https://arxiv.org/abs/2601.06695</link>
      <description>arXiv:2601.06695v1 Announce Type: new 
Abstract: Semi- and non-parametric mixture of regressions are a very useful flexible class of mixture of regressions in which some or all of the parameters are non-parametric functions of the covariates. These models are, however, based on the Gaussian assumption of the component error distributions. Thus, their estimation is sensitive to outliers and heavy-tailed error distributions. In this paper, we propose semi- and non-parametric contaminated Gaussian mixture of regressions to robustly estimate the parametric and/or non-parametric terms of the models in the presence of mild outliers. The virtue of using a contaminated Gaussian error distribution is that we can simultaneously perform model-based clustering of observations and model-based outlier detection. We propose two algorithms, an expectation-maximization (EM)-type algorithm and an expectation-conditional-maximization (ECM)-type algorithm, to perform maximum likelihood and local-likelihood kernel estimation of the parametric and non-parametric of the proposed models, respectively. The robustness of the proposed models is examined using an extensive simulation study. The practical utility of the proposed models is demonstrated using real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06695v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sphiwe B. Skhosana, Weixin Yao</dc:creator>
    </item>
    <item>
      <title>Adversarially Perturbed Precision Matrix Estimation</title>
      <link>https://arxiv.org/abs/2601.06807</link>
      <description>arXiv:2601.06807v1 Announce Type: new 
Abstract: Precision matrix estimation is a fundamental topic in multivariate statistics and modern machine learning. This paper proposes an adversarially perturbed precision matrix estimation framework, motivated by recent developments in adversarial training. The proposed framework is versatile for the precision matrix problem since, by adapting to different perturbation geometries, the proposed framework can not only recover the existing distributionally robust method but also inspire a novel moment-adaptive approach to precision matrix estimation, proven capable of sparsity recovery and adversarial robustness. Notably, the proposed perturbed precision matrix framework is proven to be asymptotically equivalent to regularized precision matrix estimation, and the asymptotic normality can be established accordingly. The resulting asymptotic distribution highlights the asymptotic bias introduced by perturbation and identifies conditions under which the perturbed estimation can be unbiased in the asymptotic sense. Numerical experiments on both synthetic and real data demonstrate the desirable performance of the proposed adversarially perturbed approach in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06807v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiling Xie</dc:creator>
    </item>
    <item>
      <title>Likelihood-Based Regression for Weibull Accelerated Life Testing Model Under Censored Data</title>
      <link>https://arxiv.org/abs/2601.06890</link>
      <description>arXiv:2601.06890v1 Announce Type: new 
Abstract: In this paper, we investigate accelerated life testing (ALT) models based on the Weibull distribution with stress-dependent shape and scale parameters. Temperature and voltage are treated as stress variables influencing the lifetime distribution. Data are assumed to be collected under Progressive Hybrid Censoring (PHC) and Adaptive Progressive Hybrid Censoring (APHC). A two-step estimation framework is developed. First, the Weibull parameters are estimated via maximum likelihood, and the consistency and asymptotic normality of the estimators are established under both censoring schemes. Second, the resulting parameter estimates are linked to the stress variables through a regression model to quantify the stress-lifetime relationship. Extensive simulations are conducted to examine finite-sample performance under a range of parameter settings, and a data illustration is also presented to showcase practical relevance. The proposed framework provides a flexible approach for modeling stress-dependent reliability behavior in ALT studies under complex censoring schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06890v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rahul Konar, Ramnivas Jat, Neeraj Joshi, Raghu Nandan Sengupta</dc:creator>
    </item>
    <item>
      <title>Minimum information Markov model</title>
      <link>https://arxiv.org/abs/2601.06900</link>
      <description>arXiv:2601.06900v1 Announce Type: new 
Abstract: The analysis of high-dimensional time series data has become increasingly important across a wide range of fields. Recently, a method for constructing the minimum information Markov kernel on finite state spaces was established. In this study, we propose a statistical model based on a parametrization of its dependence function, which we call the \textit{Minimum Information Markov Model}. We show that its parametrization induces an orthogonal structure between the stationary distribution and the dependence function, and that the model arises as the optimal solution to a divergence rate minimization problem. In particular, for the Gaussian autoregressive case, we establish the existence of the optimal solution to this minimization problem, a nontrivial result requiring a rigorous proof. For parameter estimation, our approach exploits the conditional independence structure inherent in the model, which is supported by the orthogonality. Specifically, we develop several estimators, including conditional likelihood and pseudo likelihood estimators, for the minimum information Markov model in both univariate and multivariate settings. We demonstrate their practical performance through simulation studies and applications to real-world time series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06900v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Issey Sukeda, Tomonari Sei</dc:creator>
    </item>
    <item>
      <title>Localization Estimator for High Dimensional Tensor Covariance Matrices</title>
      <link>https://arxiv.org/abs/2601.06989</link>
      <description>arXiv:2601.06989v1 Announce Type: new 
Abstract: This paper considers covariance matrix estimation of tensor data under high dimensionality. A multi-bandable covariance class is established to accommodate the need for complex covariance structures of multi-layer lattices and general covariance decay patterns. We propose a high dimensional covariance localization estimator for tensor data, which regulates the sample covariance matrix through a localization function. The statistical properties of the proposed estimator are studied by deriving the minimax rates of convergence under the spectral and the Frobenius norms. Numerical experiments and real data analysis on ocean eddy data are carried out to illustrate the utility of the proposed method in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06989v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao-Xuan Sun, Song Xi Chen, Yumou Qiu</dc:creator>
    </item>
    <item>
      <title>Unity Forests: Improving Interaction Modelling and Interpretability in Random Forests</title>
      <link>https://arxiv.org/abs/2601.07003</link>
      <description>arXiv:2601.07003v1 Announce Type: new 
Abstract: Random forests (RFs) are widely used for prediction and variable importance analysis and are often believed to capture any types of interactions via recursive splitting. However, since the splits are chosen locally, interactions are only reliably captured when at least one involved covariate has a marginal effect. We introduce unity forests (UFOs), an RF variant designed to better exploit interactions involving covariates without marginal effects. In UFOs, the first few splits of each tree are optimized jointly across a random covariate subset to form a "tree root" capturing such interactions; the remainder is grown conventionally. We further propose the unity variable importance measure (VIM), which is based on out-of-bag split criterion values from the tree roots. Here, only a small fraction of tree root splits with the highest in-bag criterion values are considered per covariate, reflecting that covariates with purely interaction-based effects are discriminative only if a split in an interacting covariate occurred earlier in the tree. Finally, we introduce covariate-representative tree roots (CRTRs), which select representative tree roots per covariate and provide interpretable insight into the conditions - marginal or interactive - under which each covariate has its strongest effects. In a simulation study, the unity VIM reliably identified interacting covariates without marginal effects, unlike conventional RF-based VIMs. In a large-scale real-data comparison, UFOs achieved higher discrimination and predictive accuracy than standard RFs, with comparable calibration. The CRTRs reproduced the covariates' true effect types reliably in simulated data and provided interesting insights in a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07003v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Roman Hornung (Institute for Medical Information Processing, Biometry and Epidemiology, Faculty of Medicine, Ludwig Maximilian University of Munich, Munich Center for Machine Learning), Alexander Hapfelmeier (Institute of General Practice and Health Services Research, Department Clinical Medicine, TUM School of Medicine and Health, Technical University of Munich, Institute of AI and Informatics in Medicine, TUM School of Medicine and Health, Technical University of Munich)</dc:creator>
    </item>
    <item>
      <title>Semiparametric Analysis of Interval-Censored Data Subject to Inaccurate Diagnoses with A Terminal Event</title>
      <link>https://arxiv.org/abs/2601.07044</link>
      <description>arXiv:2601.07044v1 Announce Type: new 
Abstract: Interval-censoring frequently occurs in studies of chronic diseases where disease status is inferred from intermittently collected biomarkers. Although many methods have been developed to analyze such data, they typically assume perfect disease diagnosis, which often does not hold in practice due to the inherent imperfect clinical diagnosis of cognitive functions or measurement errors of biomarkers such as cerebrospinal fluid. In this work, we introduce a semiparametric modeling framework using the Cox proportional hazards model to address interval-censored data in the presence of inaccurate disease diagnosis. Our model incorporates sensitivity and specificity of the diagnosis to account for uncertainty in whether the interval truly contains the disease onset. Furthermore, the framework accommodates scenarios involving a terminal event and when diagnosis is accurate, such as through postmortem analysis. We propose a nonparametric maximum likelihood estimation method for inference and develop an efficient EM algorithm to ensure computational feasibility. The regression coefficient estimators are shown to be asymptotically normal, achieving semiparametric efficiency bounds. We further validate our approach through extensive simulation studies and an application assessing Alzheimer's disease (AD) risk. We find that amyloid-beta is significantly associated with AD, but Tau is predictive of both AD and mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07044v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Deng, Donglin Zeng, Yuanjia Wang</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian Optimization via Tempered Posteriors</title>
      <link>https://arxiv.org/abs/2601.07094</link>
      <description>arXiv:2601.07094v1 Announce Type: new 
Abstract: Bayesian optimization (BO) iteratively fits a Gaussian process (GP) surrogate to accumulated evaluations and selects new queries via an acquisition function such as expected improvement (EI). In practice, BO often concentrates evaluations near the current incumbent, causing the surrogate to become overconfident and to understate predictive uncertainty in the region guiding subsequent decisions. We develop a robust GP-based BO via tempered posterior updates, which downweight the likelihood by a power $\alpha \in (0,1]$ to mitigate overconfidence under local misspecification. We establish cumulative regret bounds for tempered BO under a family of generalized improvement rules, including EI, and show that tempering yields strictly sharper worst-case regret guarantees than the standard posterior $(\alpha=1)$, with the most favorable guarantees occurring near the classical EI choice.
  Motivated by our theoretic findings, we propose a prequential procedure for selecting $\alpha$ online: it decreases $\alpha$ when realized prediction errors exceed model-implied uncertainty and returns $\alpha$ toward one as calibration improves. Empirical results demonstrate that tempering provides a practical yet theoretically grounded tool for stabilizing BO surrogates under localized sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07094v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiguang Li, Hengrui Luo</dc:creator>
    </item>
    <item>
      <title>The Bayesian Intransitive Bradley-Terry Model via Combinatorial Hodge Theory</title>
      <link>https://arxiv.org/abs/2601.07158</link>
      <description>arXiv:2601.07158v1 Announce Type: new 
Abstract: Pairwise comparison data are widely used to infer latent rankings in areas such as sports, social choice, and machine learning. The Bradley-Terry model provides a foundational probabilistic framework but inherently assumes transitive preferences, explaining all comparisons solely through subject-specific parameters. In many competitive networks, however, cycle-induced effects are intrinsic, and ignoring them can distort both estimation and uncertainty quantification. To address this limitation, we propose a Bayesian extension of the Bradley-Terry model that explicitly separates the transitive and intransitive components. The proposed Bayesian Intransitive Bradley-Terry model embeds combinatorial Hodge theory into a logistic framework, decomposing paired relationships into a gradient flow representing transitive strength and a curl flow capturing cycle-induced structure. We impose global-local shrinkage priors on the curl component, enabling data-adaptive regularization and ensuring a natural reduction to the classical Bradley-Terry model when intransitivity is absent. Posterior inference is performed using an efficient Gibbs sampler, providing scalable computation and full Bayesian uncertainty quantification. Simulation studies demonstrate improved estimation accuracy, well-calibrated uncertainty, and substantial computational advantages over existing Bayesian models for intransitivity. The proposed framework enables uncertainty-aware quantification of intransitivity at both the global and triad levels, while also characterizing cycle-induced competitive advantages among teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07158v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hisaya Okahara, Tomoyuki Nakagawa, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Principal component-guided sparse reduced-rank regression</title>
      <link>https://arxiv.org/abs/2601.07202</link>
      <description>arXiv:2601.07202v2 Announce Type: new 
Abstract: Reduced-rank regression estimates regression coefficients by imposing a low-rank constraint on the matrix of regression coefficients, thereby accounting for correlations among response variables. To further improve predictive accuracy and model interpretability, several regularized reduced-rank regression methods have been proposed. However, these existing methods cannot bias the regression coefficients toward the leading principal component directions while accounting for the correlation structure among explanatory variables. In addition, when the explanatory variables exhibit a group structure, the correlation structure within each group cannot be adequately incorporated. To overcome these limitations, we propose a new method that introduces pcLasso into the reduced-rank regression framework. The proposed method improves predictive accuracy by accounting for the correlation among response variables while strongly biasing the matrix of regression coefficients toward principal component directions with large variance. Furthermore, even in settings where the explanatory variables possess a group structure, the proposed method is capable of explicitly incorporating this structure into the estimation process. Finally, we illustrate the effectiveness of the proposed method through numerical simulations and real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07202v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kanji Goto, Shintaro Yuki, Kensuke Tanioka, Hiroshi Yadohisa</dc:creator>
    </item>
    <item>
      <title>Compounded Linear Failure Rate Distribution: Properties, Simulation and Analysis</title>
      <link>https://arxiv.org/abs/2601.07249</link>
      <description>arXiv:2601.07249v1 Announce Type: new 
Abstract: This paper proposes a new extension of the linear failure rate (LFR) model to better capture real-world lifetime data. The model incorporates an additional shape parameter to increase flexibility. It helps model the minimum survival time from a set of LFR distributed variables. We define the model, derive certain statistical properties such as the mean residual life, the mean inactivity time, moments, quantile, order statistics and also discuss the results on stochastic orders of the proposed distribution. The proposed model has increasing, bathtub shaped and inverse bathtub shaped hazard rate function. We use the method of maximum likelihood estimation to estimate the unknown parameters. We conduct simulation studies to examine the behavior of the estimators. We also use three real datasets to evaluate the model, which turns out superior compared to classical alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07249v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suchismita Das, Akul Ameya, Cahyani Karunia Putri</dc:creator>
    </item>
    <item>
      <title>Connections as treatment: causal inference with edge interventions in networks</title>
      <link>https://arxiv.org/abs/2601.07267</link>
      <description>arXiv:2601.07267v1 Announce Type: new 
Abstract: Causal inference has traditionally focused on interventions at the unit level. In many applications, however, the central question concerns the causal effects of connections between units, such as transportation links, social relationships, or collaborative ties. We develop a causal framework for edge interventions in networks, where treatments correspond to the presence or absence of edges. Our framework defines causal estimands under stochastic interventions on the network structure and introduces an inverse probability weighting estimator under an unconfoundedness assumption on edge assignment. We estimate edge probabilities using exponential random graph models, a widely used class of network models. We establish consistency and asymptotic normality of the proposed estimator. Finally, we apply our methodology to China's transportation network to estimate the causal impact of railroad connections on regional economic development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07267v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuli Chen, Jie Hu, Zhichao Jiang</dc:creator>
    </item>
    <item>
      <title>Minimum Wasserstein distance estimator under covariate shift: closed-form, super-efficiency and irregularity</title>
      <link>https://arxiv.org/abs/2601.07282</link>
      <description>arXiv:2601.07282v1 Announce Type: new 
Abstract: Covariate shift arises when covariate distributions differ between source and target populations while the conditional distribution of the response remains invariant, and it underlies problems in missing data and causal inference. We propose a minimum Wasserstein distance estimation framework for inference under covariate shift that avoids explicit modeling of outcome regressions or importance weights. The resulting W-estimator admits a closed-form expression and is numerically equivalent to the classical 1-nearest neighbor estimator, yielding a new optimal transport interpretation of nearest neighbor methods. We establish root-$n$ asymptotic normality and show that the estimator is not asymptotically linear, leading to super-efficiency relative to the semiparametric efficient estimator under covariate shift in certain regimes, and uniformly in missing data problems. Numerical simulations, along with an analysis of a rainfall dataset, underscore the exceptional performance of our W-estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07282v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junjun Lang, Qiong Zhang, Yukun Liu</dc:creator>
    </item>
    <item>
      <title>Characterization of multi-way binary tables with uniform margins and fixed correlations</title>
      <link>https://arxiv.org/abs/2601.07369</link>
      <description>arXiv:2601.07369v1 Announce Type: new 
Abstract: In many applications involving binary variables, only pairwise dependence measures, such as correlations, are available. However, for multi-way tables involving more than two variables, these quantities do not uniquely determine the joint distribution, but instead define a family of admissible distributions that share the same pairwise dependence while potentially differing in higher-order interactions. In this paper, we introduce a geometric framework to describe the entire feasible set of such joint distributions with uniform margins. We show that this admissible set forms a convex polytope, analyze its symmetry properties, and characterize its extreme rays. These extremal distributions provide fundamental insights into how higher-order dependence structures may vary while preserving the prescribed pairwise information. Unlike traditional methods for table generation, which return a single table, our framework makes it possible to explore and understand the full admissible space of dependence structures, enabling more flexible choices for modeling and simulation. We illustrate the usefulness of our theoretical results through examples and a real case study on rater agreement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07369v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Fontana, Elisa Perrone, Fabio Rapallo</dc:creator>
    </item>
    <item>
      <title>Inference for Multiple Change-points in Piecewise Locally Stationary Time Series</title>
      <link>https://arxiv.org/abs/2601.07400</link>
      <description>arXiv:2601.07400v1 Announce Type: new 
Abstract: Change-point detection and locally stationary time series modeling are two major approaches for the analysis of non-stationary data. The former aims to identify stationary phases by detecting abrupt changes in the dynamics of a time series model, while the latter employs (locally) time-varying models to describe smooth changes in dependence structure of a time series. However, in some applications, abrupt and smooth changes can co-exist, and neither of the two approaches alone can model the data adequately. In this paper, we propose a novel likelihood-based procedure for the inference of multiple change-points in locally stationary time series. In contrast to traditional change-point analysis where an abrupt change occurs in a real-valued parameter, a change in locally stationary time series occurs in a parameter curve, and can be classified as a jump or a kink depending on whether the curve is discontinuous or not. We show that the proposed method can consistently estimate the number, locations, and the types of change-points. Two different asymptotic distributions corresponding respectively to jump and kink estimators are also established.Extensive simulation studies and a real data application to financial time series are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07400v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wai Leong Ng, Xinyi Tang, Mun Lau Cheung, Jiacheng Gao, Chun Yip Yau, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Ridge-penalised spectral least-squares estimation for point processes</title>
      <link>https://arxiv.org/abs/2601.07490</link>
      <description>arXiv:2601.07490v1 Announce Type: new 
Abstract: Penalised estimation methods for point processes usually rely on a large amount of independent repetitions for cross-validation purposes. However, in the case of a single realisation of the process, existing cross-validation methods may be impractical depending on the chosen model. To overcome this issue, this paper presents a Ridge-penalised spectral least-squares estimation method for second-order stationary point processes. This is achieved through two novel approaches: a p-thinning-based cross-validation method to tune the penalisation parameter, relying on the spectral representation of the process; and the introduction of a spectral least-squares contrast based around the asymptotic properties of the periodogram of the sample. The proposed method is then illustrated by a simulation study on linear Hawkes processes in the context of parametric estimation, highlighting its performances against more traditional approaches, specifically when working with short observation windows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07490v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miguel Martinez Herrera (CNRS UMR3738), Felix Cheysson (LAMA)</dc:creator>
    </item>
    <item>
      <title>Functional Synthetic Control Methods for Metric Space-Valued Outcomes</title>
      <link>https://arxiv.org/abs/2601.07539</link>
      <description>arXiv:2601.07539v1 Announce Type: new 
Abstract: The synthetic control method (SCM) is a widely used tool for evaluating causal effects of policy changes in panel data settings. Recent studies have extended its framework to accommodate complex outcomes that take values in metric spaces, such as distributions, functions, networks, covariance matrices, and compositional data. However, due to the lack of linear structure in general metric spaces, theoretical guarantees for estimation and inference within these extended frameworks remain underdeveloped. In this study, we propose the functional synthetic control (FSC) method as an extension of the SCM for metric space-valued outcomes. To address challenges arising from the nonlinearlity of metric spaces, we leverage isometric embeddings into Hilbert spaces. Building on this approach, we develop the FSC and augmented FSC estimators for counterfactual outcomes, with the latter being a bias-corrected version of the former. We then derive their finite-sample error bounds to establish theoretical guarantees for estimation, and construct prediction sets based on these estimators to conduct inference on causal effects. We demonstrate the usefulness of the proposed framework through simulation studies and three empirical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07539v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryo Okano, Daisuke Kurisu</dc:creator>
    </item>
    <item>
      <title>Omitted covariates bias and finite mixtures of regression models for longitudinal responses</title>
      <link>https://arxiv.org/abs/2601.07609</link>
      <description>arXiv:2601.07609v1 Announce Type: new 
Abstract: Individual-specific, time-constant, random effects are often used to model dependence and/or to account for omitted covariates in regression models for longitudinal responses. Longitudinal studies have known a huge and widespread use in the last few years as they allow to distinguish between so-called age and cohort effects; these relate to differences that can be observed at the beginning of the study and stay persistent through time, and changes in the response that are due to the temporal dynamics in the observed covariates. While there is a clear and general agreement on this purpose, the random effect approach has been frequently criticized for not being robust to the presence of correlation between the observed (i.e. covariates) and the unobserved (i.e. random effects) heterogeneity. Starting from the so-called correlated effect approach, we argue that the random effect approach may be parametrized to account for potential correlation between observables and unobservables. Specifically, when the random effect distribution is estimated non-parametrically using a discrete distribution on finite number of locations, a further, more general, solution is developed. This is illustrated via a large scale simulation study and the analysis of a benchmark dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07609v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Alfo', Robrto Rocci</dc:creator>
    </item>
    <item>
      <title>Cluster-based name embeddings reduce ethnic disparities in record linkage quality under realistic name corruption: evidence from the North Carolina Voter Registry</title>
      <link>https://arxiv.org/abs/2601.07693</link>
      <description>arXiv:2601.07693v1 Announce Type: new 
Abstract: Differential ethnic-based record linkage errors can bias epidemiologic estimates. Prior evidence often conflates heterogeneity in error mechanisms with unequal exposure to error. Using snapshots of the North Carolina Voter Registry (Oct 2011-Oct 2022), we derived empirical name-discrepancy profiles to parameterise realistic corruptions. From an Oct 2022 extract (n=848,566), we generated five replicate corrupted datasets under three settings that separately varied mechanism heterogeneity and exposure inequality, and linked records back to originals using unadjusted Jaro-Winkler, Term Frequency (TF)-adjusted Jaro-Winkler, and a cluster-based forename-embedding comparator combined with TF-adjusted surname comparison. We evaluated false match rate (FMR), missed match rate (MMR) and white-centric disparities. At a fixed MMR near 0.20, overall error rates and ethnic disparities diverged substantially by model under disproportionate exposure to corruption. Term-frequency (TF)-adjusted Jaro-Winkler achieved very low overall FMR (0.55% (95% CI 0.54-0.57)) at overall MMR 20.34% (20.30-20.39), but large white-centric under-linkage disparities persisted: Hispanic voters had 36.3% (36.1-36.6) and Non-Hispanic Black voters 8.6% (8.6-8.7) higher FMRs compared to Non-Hispanic White groups. Relative to unadjusted string similarity, TF adjustment reduced these disparities (Hispanic: +60.4% (60.1-60.7) to +36.3%; Black: +13.1% (13.0-13.2) to +8.6%). The cluster-based forename-embedding model reduced missed-match disparities further (Hispanic: +10.2% (9.8-10.3); Black: +0.6% (0.4-0.7)), but at a cost of increasing overall FMR (4.28% (4.22-4.35)) at the same threshold. Unequal exposure to identifier error drove substantially larger disparities than mechanism heterogeneity alone; cluster-based embeddings markedly narrowed under-linkage disparities beyond TF adjustment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07693v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joseph Lam, Mario Cortina-Borja, Rob Aldridge, Ruth Blackburn, Katie Harron</dc:creator>
    </item>
    <item>
      <title>Performance of models for monitoring sustainable development goals from remote sensing: A three-level meta-regression</title>
      <link>https://arxiv.org/abs/2601.06178</link>
      <description>arXiv:2601.06178v1 Announce Type: cross 
Abstract: Machine learning (ML) is a tool to exploit remote sensing data for the monitoring and implementation of the United Nations' Sustainable Development Goals (SDGs). In this paper, we report on a meta-analysis to evaluate the performance of ML applied to remote sensing data to monitor SDGs. Specifically, we aim to 1) estimate the average performance; 2) determine the degree of heterogeneity between and within studies; and 3) assess how study features influence model performance. Using PRISMA guidelines, a search was performed across multiple academic databases to identify potentially relevant studies. A random sample of 200 was screened by three reviewers, resulting in 86 trials within 20 studies with 14 study features. Overall accuracy was the most reported performance metric. It was analyzed using double arcsine transformation and a three-level random effects model. The average overall accuracy of the best model was 0.90 [0.86, 0.92]. There was considerable heterogeneity in model performance, 64% of which was between studies. The only significant feature was the prevalence of the majority class, which explained 61% of the between-study heterogeneity. None of the other thirteen features added value to the model. The most important contributions of this paper are the following two insights. 1) Overall accuracy is the most popular performance metric, yet arguably the least insightful. Its sensitivity to class imbalance makes it necessary to normalize it, which is far from common practice. 2) The field needs to standardize the reporting. Reporting of the confusion matrix for independent test sets is the most important ingredient for between-study comparisons of ML classifiers. These findings underscore the need for robust and comparable evaluation metrics in machine learning applications to ensure reliable and actionable insights for effective SDG monitoring and policy formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06178v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Klingwort, Nina M. Leach, Joep Burger</dc:creator>
    </item>
    <item>
      <title>Why are there many equally good models? An Anatomy of the Rashomon Effect</title>
      <link>https://arxiv.org/abs/2601.06730</link>
      <description>arXiv:2601.06730v1 Announce Type: cross 
Abstract: The Rashomon effect -- the existence of multiple, distinct models that achieve nearly equivalent predictive performance -- has emerged as a fundamental phenomenon in modern machine learning and statistics. In this paper, we explore the causes underlying the Rashomon effect, organizing them into three categories: statistical sources arising from finite samples and noise in the data-generating process; structural sources arising from non-convexity of optimization objectives and unobserved variables that create fundamental non-identifiability; and procedural sources arising from limitations of optimization algorithms and deliberate restrictions to suboptimal model classes. We synthesize insights from machine learning, statistics, and optimization literature to provide a unified framework for understanding why the multiplicity of good models arises. A key distinction emerges: statistical multiplicity diminishes with more data, structural multiplicity persists asymptotically and cannot be resolved without different data or additional assumptions, and procedural multiplicity reflects choices made by practitioners. Beyond characterizing causes, we discuss both the challenges and opportunities presented by the Rashomon effect, including implications for inference, interpretability, fairness, and decision-making under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06730v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Harsh Parikh</dc:creator>
    </item>
    <item>
      <title>Dimension-reduced outcome-weighted learning for estimating individualized treatment regimes in observational studies</title>
      <link>https://arxiv.org/abs/2601.06782</link>
      <description>arXiv:2601.06782v1 Announce Type: cross 
Abstract: Individualized treatment regimes (ITRs) aim to improve clinical outcomes by assigning treatment based on patient-specific characteristics. However, existing methods often struggle with high-dimensional covariates, limiting accuracy, interpretability, and real-world applicability. We propose a novel sufficient dimension reduction approach that directly targets the contrast between potential outcomes and identifies a low-dimensional subspace of the covariates capturing treatment effect heterogeneity. This reduced representation enables more accurate estimation of optimal ITRs through outcome-weighted learning. To accommodate observational data, our method incorporates kernel-based covariate balancing, allowing treatment assignment to depend on the full covariate set and avoiding the restrictive assumption that the subspace sufficient for modeling heterogeneous treatment effects is also sufficient for confounding adjustment. We show that the proposed method achieves universal consistency, i.e., its risk converges to the Bayes risk, under mild regularity conditions. We demonstrate its finite sample performance through simulations and an analysis of intensive care unit sepsis patient data to determine who should receive transthoracic echocardiography.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06782v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sungtaek Son, Eardi Lila, Kwun Chuen Gary Chan</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes Estimation in Heterogeneous Coefficient Panel Models</title>
      <link>https://arxiv.org/abs/2601.07059</link>
      <description>arXiv:2601.07059v1 Announce Type: cross 
Abstract: We develop an empirical Bayes (EB) G-modeling framework for short-panel linear models with multidimensional heterogeneity and nonparametric prior. Specifically, we allow heterogeneous intercepts, slopes, dynamics, and a non-spherical error covariance structure. We establish identification and consistency of the nonparametric maximum likelihood estimator (NPMLE) under general conditions, and provide low-level sufficient conditions for several models of empirical interest. Conditions for regret consistency of the resulting EB estimators are also established. The NPMLE is computed using a Wasserstein-Fisher-Rao gradient flow algorithm adapted to panel regressions. Using data from the Panel Study of Income Dynamics, we find that the slope coefficient for potential experience is substantially heterogeneous and negatively correlated with the random intercept, and that error variances and autoregressive coefficients vary significantly across individuals. The EB estimates reduce mean squared prediction errors relative to individual maximum likelihood estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07059v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Myunghyun Song, Sokbae Lee, Serena Ng</dc:creator>
    </item>
    <item>
      <title>Multi-environment Invariance Learning with Missing Data</title>
      <link>https://arxiv.org/abs/2601.07247</link>
      <description>arXiv:2601.07247v1 Announce Type: cross 
Abstract: Learning models that can handle distribution shifts is a key challenge in domain generalization. Invariance learning, an approach that focuses on identifying features invariant across environments, improves model generalization by capturing stable relationships, which may represent causal effects when the data distribution is encoded within a structural equation model (SEM) and satisfies modularity conditions. This has led to a growing body of work that builds on invariance learning, leveraging the inherent heterogeneity across environments to develop methods that provide causal explanations while enhancing robust prediction. However, in many practical scenarios, obtaining complete outcome data from each environment is challenging due to the high cost or complexity of data collection. This limitation in available data hinders the development of models that fully leverage environmental heterogeneity, making it crucial to address missing outcomes to improve both causal insights and robust prediction. In this work, we derive an estimator from the invariance objective under missing outcomes. We establish non-asymptotic guarantees on variable selection property and $\ell_2$ error convergence rates, which are influenced by the proportion of missing data and the quality of imputation models across environments. We evaluate the performance of the new estimator through extensive simulations and demonstrate its application using the UCI Bike Sharing dataset to predict the count of bike rentals. The results show that despite relying on a biased imputation model, the estimator is efficient and achieves lower prediction error, provided the bias is within a reasonable range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07247v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Jia</dc:creator>
    </item>
    <item>
      <title>Covariance-Driven Regression Trees: Reducing Overfitting in CART</title>
      <link>https://arxiv.org/abs/2601.07281</link>
      <description>arXiv:2601.07281v1 Announce Type: cross 
Abstract: Decision trees are powerful machine learning algorithms, widely used in fields such as economics and medicine for their simplicity and interpretability. However, decision trees such as CART are prone to overfitting, especially when grown deep or the sample size is small. Conventional methods to reduce overfitting include pre-pruning and post-pruning, which constrain the growth of uninformative branches. In this paper, we propose a complementary approach by introducing a covariance-driven splitting criterion for regression trees (CovRT). This method is more robust to overfitting than the empirical risk minimization criterion used in CART, as it produces more balanced and stable splits and more effectively identifies covariates with true signals. We establish an oracle inequality of CovRT and prove that its predictive accuracy is comparable to that of CART in high-dimensional settings. We find that CovRT achieves superior prediction accuracy compared to CART in both simulations and real-world tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07281v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Likun Zhang, Wei Ma</dc:creator>
    </item>
    <item>
      <title>Riesz Representer Fitting under Bregman Divergence: A Unified Framework for Debiased Machine Learning</title>
      <link>https://arxiv.org/abs/2601.07752</link>
      <description>arXiv:2601.07752v1 Announce Type: cross 
Abstract: Estimating the Riesz representer is a central problem in debiased machine learning for causal and structural parameter estimation. Various methods for Riesz representer estimation have been proposed, including Riesz regression and covariate balancing. This study unifies these methods within a single framework. Our framework fits a Riesz representer model to the true Riesz representer under a Bregman divergence, which includes the squared loss and the Kullback--Leibler (KL) divergence as special cases. We show that the squared loss corresponds to Riesz regression, and the KL divergence corresponds to tailored loss minimization, where the dual solutions correspond to stable balancing weights and entropy balancing weights, respectively, under specific model specifications. We refer to our method as generalized Riesz regression, and we refer to the associated duality as automatic covariate balancing. Our framework also generalizes density ratio fitting under a Bregman divergence to Riesz representer estimation, and it includes various applications beyond density ratio estimation. We also provide a convergence analysis for both cases where the model class is a reproducing kernel Hilbert space (RKHS) and where it is a neural network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07752v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Comparing three learn-then-test paradigms in a multivariate normal means problem</title>
      <link>https://arxiv.org/abs/2601.07764</link>
      <description>arXiv:2601.07764v1 Announce Type: cross 
Abstract: Many modern procedures use the data to learn a structure and then leverage it to test many hypotheses. If the entire data is used at both stages, analytical or computational corrections for selection bias are required to ensure validity (post-learning adjustment). Alternatively, one can learn and/or test on masked versions of the data to avoid selection bias, either via information splitting or null augmentation}. Choosing among these three learn-then-test paradigms, and how much masking to employ for the latter two, are critical decisions impacting power that currently lack theoretical guidance. In a multivariate normal means model, we derive asymptotic power formulas for prototypical methods from each paradigm -- variants of sample splitting, conformal-style null augmentation, and resampling-based post-learning adjustment -- quantifying the power losses incurred by masking at each stage. For these paradigm representatives, we find that post-learning adjustment is most powerful, followed by null augmentation, and then information splitting. Moreover, null augmentation can be nearly as powerful as post-learning adjustment, while avoiding its challenges: the power of the former approaches that of the latter if the number of nulls used for augmentation is a vanishing fraction of the number of hypotheses. We also prove for a tractable proxy that the optimal number of nulls scales as the square root of the number of hypotheses, challenging existing heuristics. Finally, we characterize optimal tuning for information splitting by identifying an optimal split fraction and tying it to the difficulty of the learning problem. These results establish a theoretical foundation for key decisions in the deployment of learn-then-test methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07764v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhinav Chakraborty, Junu Lee, Eugene Katsevich</dc:creator>
    </item>
    <item>
      <title>Experiment-selector cross-validated targeted maximum likelihood estimator for hybrid RCT-external data studies</title>
      <link>https://arxiv.org/abs/2210.05802</link>
      <description>arXiv:2210.05802v4 Announce Type: replace 
Abstract: Augmenting a randomized controlled trial (RCT) with external data may increase power at the risk of introducing bias. To select and analyze the experiment (RCT alone or combined with external data) with the optimal bias-variance tradeoff, we develop a novel experiment-selector cross-validated targeted maximum likelihood estimator for randomized-external data studies (ES-CVTMLE). This estimator utilizes two estimates of bias to determine whether to integrate external data based on 1) a function of the difference in conditional mean outcome under control between the RCT and combined experiments and 2) an estimate of the average treatment effect on a negative control outcome (NCO). We define the asymptotic distribution of the ES-CVTMLE under varying magnitudes of bias and construct confidence intervals by Monte Carlo simulation. We evaluate ES-CVTMLE compared to three other data fusion estimators in simulations and demonstrate the ability of ES-CVTMLE to distinguish biased from unbiased external controls in a real data analysis of the effect of liraglutide on glycemic control from the LEADER trial. The ES-CVTMLE has the potential to improve power while providing relatively robust inference for future hybrid RCT-external data studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.05802v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1515/jci-2024-0041</arxiv:DOI>
      <arxiv:journal_reference>Journal of Causal Inference; 13(1), pp. 20240041 (2025)</arxiv:journal_reference>
      <dc:creator>Lauren Eyler Dang, Jens Magelund Tarp, Trine Julie Abrahamsen, Kajsa Kvist, John B Buse, Maya Petersen, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Expectile Periodograms</title>
      <link>https://arxiv.org/abs/2403.02060</link>
      <description>arXiv:2403.02060v4 Announce Type: replace 
Abstract: This paper introduces a novel periodogram-like function, called the expectile periodogram, for modeling spectral features of time series and detecting hidden periodicities. The expectile periodogram is constructed from trigonometric expectile regression, in which a specially designed check function is used to substitute the squared $l_2$ norm that leads to the ordinary periodogram. The expectile periodogram retains the key properties of the ordinary periodogram as a frequency-domain representation of serial dependence in time series, while offering a more comprehensive understanding by examining the data across the entire range of expectile levels. We establish the asymptotic theory and investigate the relationship between the expectile periodogram and the so called expectile spectrum. Simulations demonstrate the efficiency of the expectile periodogram in the presence of hidden periodicities. Finally, by leveraging the inherent two-dimensional nature of the expectile periodogram, we train a deep learning (DL) model to classify earthquake waveform data. Remarkably, our approach outperforms alternative periodogram-based methods in terms of classification accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02060v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.csda.2025.108337</arxiv:DOI>
      <dc:creator>Tianbo Chen, Ta-Hsin Li, Hanbing Zhu, Wenwu Gao</dc:creator>
    </item>
    <item>
      <title>Iterative Methods for Full-Scale Gaussian Process Approximations for Large Spatial Data</title>
      <link>https://arxiv.org/abs/2405.14492</link>
      <description>arXiv:2405.14492v5 Announce Type: replace 
Abstract: Gaussian processes are flexible probabilistic regression models which are widely used in statistics and machine learning. However, a drawback is their limited scalability to large data sets. To alleviate this, full-scale approximations (FSAs) combine predictive process methods and covariance tapering, thus approximating both global and local structures. We show how iterative methods can be used to reduce computational costs in calculating likelihoods, gradients, and predictive distributions with FSAs. In particular, we introduce a novel preconditioner and show theoretically and empirically that it accelerates the conjugate gradient method's convergence speed and mitigates its sensitivity with respect to the FSA parameters and the eigenvalue structure of the original covariance matrix, and we demonstrate empirically that it outperforms a state-of-the-art pivoted Cholesky preconditioner. Furthermore, we introduce an accurate and fast way to calculate predictive variances using stochastic simulation and iterative methods. In addition, we show how our newly proposed fully independent training conditional (FITC) preconditioner can also be used in iterative methods for Vecchia approximations. In our experiments, it outperforms existing state-of-the-art preconditioners for Vecchia approximations. All methods are implemented in a free C++ software library with high-level Python and R packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14492v5</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Gyger, Reinhard Furrer, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>Model-X Change-Point Detection of Conditional Distribution</title>
      <link>https://arxiv.org/abs/2505.12023</link>
      <description>arXiv:2505.12023v3 Announce Type: replace 
Abstract: The dynamic nature of many real-world systems can lead to temporal outcome model shifts, causing a deterioration in model accuracy and reliability over time. This requires change-point detection on the outcome models to guide model retraining and adjustments. However, inferring the change point of conditional models is more prone to loss of validity or power than classic detection problems for marginal distributions. This is due to both the temporal covariate shift and the complexity of the outcome model. Also, the existing method of conditional change points detection both have many limitations including linear assumption and low dimension prerequisite which sometimes is not suitable for real world application. To address these challenges, we propose a novel Model-X changE-point detectioN of conditional Distribution (MEND) method computationally enhanced with distillation function for simultaneous change-point detection and localization of the conditional outcome model. We extend and combine our model with neural network to accommodate complex nonlinear and high dimensional situation, which is proved to be valid in both simulation and real data. Theoretical validity of the proposed method is justified. Extensive simulation studies and two real-world examples demonstrate the statistical effectiveness and computational scalability of our method as well as its significant improvements over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12023v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuofan Dong, Yiwen Huang, Yan Dong, Mengying Yan, Ziye Tian, Chuan Hong, Doudou Zhou, Molei Liu</dc:creator>
    </item>
    <item>
      <title>Testing for sufficient follow-up in cure models with categorical covariates</title>
      <link>https://arxiv.org/abs/2505.13128</link>
      <description>arXiv:2505.13128v2 Announce Type: replace 
Abstract: In survival analysis, estimating the fraction of 'immune' or 'cured' subjects who will never experience the event of interest, requires a sufficiently long follow-up period. A few statistical tests have been proposed to test the assumption of sufficient follow-up, i.e. whether the right extreme of the censoring distribution exceeds that of the survival time of the uncured subjects. However, in practice the problem remains challenging. To address this, a relaxed notion of 'practically' sufficient follow-up has been introduced recently, suggesting that the follow-up would be considered sufficiently long if the probability for the event occurring after the end of the study is very small. All these existing tests do not incorporate covariate information, which might affect the cure rate and the survival times. We extend the test for 'practically' sufficient follow-up to settings with categorical covariates. While a straightforward intersection-union type test could reject the null hypothesis of insufficient follow-up only if such hypothesis is rejected for all covariate values, in practice this approach is overly conservative and lacks power. To improve upon this, we propose a novel test procedure that relies on the test decision for one properly chosen covariate value. Our approach relies on the assumption that the conditional density of the uncured survival time is a non-increasing function of time in the tail region. We show that both methods yield tests of asymptotically level $\alpha$ and investigate their finite sample performance through simulations. The practical application of the methods is illustrated using a skin melanoma dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13128v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tsz Pang Yuen, Eni Musta, Ingrid Van Keilegom</dc:creator>
    </item>
    <item>
      <title>Data-Adaptive Automatic Threshold Calibration for Stability Selection</title>
      <link>https://arxiv.org/abs/2505.22012</link>
      <description>arXiv:2505.22012v2 Announce Type: replace 
Abstract: Stability selection has gained popularity as a method for enhancing the performance of variable selection algorithms while controlling false discovery rates. However, achieving these desirable properties depends on correctly specifying the stable threshold parameter, which can be challenging. An arbitrary choice of this parameter can substantially alter the set of selected variables, as the variables' selection probabilities are inherently data-dependent. To address this issue, we propose Exclusion Automatic Threshold Selection (EATS), a data-adaptive algorithm that streamlines stability selection by automating the threshold specification process. EATS initially filters out potential noise variables using an exclusion probability threshold, derived from applying stability selection to a randomly shuffled version of the dataset. Following this, EATS selects the stable threshold parameter using the elbow method, balancing the marginal utility of including additional variables against the risk of selecting superfluous variables. We evaluate our approach through an extensive simulation study, benchmarking across commonly used variable selection algorithms and static stable threshold values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22012v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Huang, Samuel Muller, Garth Tarr</dc:creator>
    </item>
    <item>
      <title>PCA-Guided Quantile Sampling: Preserving Data Structure in Large-Scale Subsampling</title>
      <link>https://arxiv.org/abs/2506.18249</link>
      <description>arXiv:2506.18249v2 Announce Type: replace 
Abstract: We introduce Principal Component Analysis guided Quantile Sampling (PCA QS), a novel sampling framework designed to preserve both the statistical and geometric structure of large scale datasets. Unlike conventional PCA, which reduces dimensionality at the cost of interpretability, PCA QS retains the original feature space while using leading principal components solely to guide a quantile based stratification scheme. This principled design ensures that sampling remains representative without distorting the underlying data semantics. We establish rigorous theoretical guarantees, deriving convergence rates for empirical quantiles, Kullback Leibler divergence, and Wasserstein distance, thus quantifying the distributional fidelity of PCA QS samples. Practical guidelines for selecting the number of principal components, quantile bins, and sampling rates are provided based on these results. Extensive empirical studies on both synthetic and real-world datasets show that PCA QS consistently outperforms simple random sampling, yielding better structure preservation and improved downstream model performance. Together, these contributions position PCA QS as a scalable, interpretable, and theoretically grounded solution for efficient data summarization in modern machine learning workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18249v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Foo Hui-Mean, Yuan-chin Ivan Chang</dc:creator>
    </item>
    <item>
      <title>Bootstrapped Control Limits for Score-Based Concept Drift Control Charts</title>
      <link>https://arxiv.org/abs/2507.16749</link>
      <description>arXiv:2507.16749v2 Announce Type: replace 
Abstract: Monitoring for changes in a predictive relationship represented by a fitted supervised learning model (i.e., concept drift detection) is a widespread problem in modern data-driven applications. A general and powerful Fisher score-based concept drift approach was recently proposed, in which detecting concept drift reduces to detecting changes in the mean of the model's score vector using a multivariate exponentially weighted moving average (MEWMA). To implement the approach, the initial data must be split into two subsets. The first subset serves as the training sample to which the model is fit, and the second subset serves as an out-of-sample test set from which the MEWMA control limit (CL) is determined. In this paper, we retain the same score-based MEWMA monitoring statistic as the existing method and focus instead on improving the computation of the control limit. We develop a novel nested bootstrap procedure for calibrating the CL that allows the entire initial sample to be used for model fitting, thereby yielding a more accurate baseline model while eliminating the need for a large holdout set. We show that a standard nested bootstrap substantially underestimates the variability of the monitoring statistic and develop a 0.632-like correction that appropriately accounts for this. We demonstrate the advantages with numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16749v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiezhong Wu, Daniel W. Apley</dc:creator>
    </item>
    <item>
      <title>Omnibus goodness-of-fit tests based on trigonometric moments</title>
      <link>https://arxiv.org/abs/2507.18591</link>
      <description>arXiv:2507.18591v2 Announce Type: replace 
Abstract: We propose a new omnibus goodness-of-fit test based on trigonometric moments of probability-integral-transformed data. The test builds on the framework of the LK test introduced by Langholz and Kronmal [J. Amer. Statist. Assoc. 86 (1991), 1077-1084], but fully exploits the covariance structure of the associated trigonometric statistics. As a result, our test statistic converges under the null hypothesis to a $\chi_2^2$ distribution, even in the presence of nuisance parameters, yielding a well-calibrated rejection region. We derive the exact asymptotic covariance matrix required for normalization and propose a unified approach to computing the LK normalizing scalar. The applicability of both the proposed test and the LK test is substantially expanded by providing implementation details for 11 families of continuous distributions, covering most commonly used parametric models. Simulation studies demonstrate accurate empirical size, close to the nominal level, and strong power properties, yielding fully plug-and-play procedures. Further insight is provided by an analysis under local alternatives. The methodology is illustrated using surface temperature forecast errors from a numerical weather prediction model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18591v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain Desgagn\'e, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Bayesian inference of antibody evolutionary dynamics using multitype branching processes</title>
      <link>https://arxiv.org/abs/2508.09519</link>
      <description>arXiv:2508.09519v2 Announce Type: replace 
Abstract: When our immune system encounters foreign antigens (i.e., from pathogens), the B cells that produce our antibodies undergo a cyclic process of proliferation, mutation, and selection, improving their ability to bind to the specific antigen. Immunologists have recently developed powerful experimental techniques to investigate this process in mouse models. In one such experiment, mice are engineered with a monoclonal B-cell precursor and immunized with a model antigen. B cells are sampled from sacrificed mice after the immune response has progressed, and the mutated genetic loci encoding antibodies are sequenced. This experiment allows parallel replay of antibody evolution, but produces data at only one time point; we are unable to observe the evolutionary trajectories that lead to optimized antibody affinity in each mouse. To address this, we model antibody evolution as a multitype branching process and integrate over unobserved histories conditioned on phylogenetic signal in sequence data, leveraging parallel experimental replays for parameter inference. We infer the functional relationship between B-cell fitness and antigen binding affinity in a Bayesian framework, equipped with an efficient likelihood calculation algorithm and Markov chain Monte Carlo posterior approximation. In a simulation study, we demonstrate that a sigmoidal relationship between fitness and binding affinity can be recovered from realizations of the branching process. We then perform inference for experimental data from 52 replayed B-cell lineages sampled 15 days after immunization, yielding a total of 3,758 sampled B cells. The recovered sigmoidal curve indicates that the fitness of high-affinity B cells is over six times larger than that of low-affinity B cells, with a sharp transition from low to high fitness values as affinity increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09519v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Athanasios G. Bakis, Ashni A. Vora, Tatsuya Araki, Tongqiu Jia, Jared G. Galloway, Chris Jennings-Shaffer, Gabriel D. Victora, Yun S. Song, William S. DeWitt, Frederick A. Matsen IV, Volodymyr M. Minin</dc:creator>
    </item>
    <item>
      <title>Approximating Persistent Homology for Large Datasets</title>
      <link>https://arxiv.org/abs/2204.09155</link>
      <description>arXiv:2204.09155v3 Announce Type: replace-cross 
Abstract: Persistent homology is an important methodology in topological data analysis which adapts theory from algebraic topology to data settings. Computing persistent homology produces persistence diagrams, which have been successfully used in diverse domains. Despite its widespread use, persistent homology is simply impossible to compute when a dataset is very large. We study a statistical approach to the problem of computing persistent homology for massive datasets using a multiple subsampling framework and extend it to three summaries of persistent homology: H\"{o}lder continuous vectorizations of persistence diagrams; the alternative representation as persistence measures; and standard persistence diagrams. Specifically, we derive finite sample convergence rates for empirical means for persistent homology and practical guidance on interpreting and tuning parameters. We validate our approach through extensive experiments on both synthetic and real-world data. We demonstrate the performance of multiple subsampling in a permutation test to analyze the topological structure of Poincar\'{e} embeddings of large lexical databases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.09155v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueqi Cao, Anthea Monod</dc:creator>
    </item>
    <item>
      <title>Relaxed Gaussian process interpolation: a goal-oriented approach to Bayesian optimization</title>
      <link>https://arxiv.org/abs/2206.03034</link>
      <description>arXiv:2206.03034v4 Announce Type: replace-cross 
Abstract: This work presents a new procedure for obtaining predictive distributions in the context of Gaussian process (GP) modeling, with a relaxation of the interpolation constraints outside ranges of interest: the mean of the predictive distributions no longer necessarily interpolates the observed values when they are outside ranges of interest, but are simply constrained to remain outside. This method called relaxed Gaussian process (reGP) interpolation provides better predictive distributions in ranges of interest, especially in cases where a stationarity assumption for the GP model is not appropriate. It can be viewed as a goal-oriented method and becomes particularly interesting in Bayesian optimization, for example, for the minimization of an objective function, where good predictive distributions for low function values are important. When the expected improvement criterion and reGP are used for sequentially choosing evaluation points, the convergence of the resulting optimization algorithm is theoretically guaranteed (provided that the function to be optimized lies in the reproducing kernel Hilbert space attached to the known covariance of the underlying Gaussian process). Experiments indicate that using reGP instead of stationary GP models in Bayesian optimization is beneficial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.03034v4</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research, 2025, 26, pp.1-70</arxiv:journal_reference>
      <dc:creator>S\'ebastien Petit (LNE, RT-UQ), Julien Bect (L2S, RT-UQ), Emmanuel Vazquez (L2S, RT-UQ)</dc:creator>
    </item>
    <item>
      <title>Data-Driven Knowledge Transfer in Batch $Q^*$ Learning</title>
      <link>https://arxiv.org/abs/2404.15209</link>
      <description>arXiv:2404.15209v3 Announce Type: replace-cross 
Abstract: In data-driven decision-making in marketing, healthcare, and education, it is desirable to utilize a large amount of data from existing ventures to navigate high-dimensional feature spaces and address data scarcity in new ventures. We explore knowledge transfer in dynamic decision-making by concentrating on batch stationary environments and formally defining task discrepancies through the lens of Markov decision processes (MDPs). We propose a framework of Transferred Fitted $Q$-Iteration algorithm with general function approximation, enabling the direct estimation of the optimal action-state function $Q^*$ using both target and source data. We establish the relationship between statistical performance and MDP task discrepancy under sieve approximation, shedding light on the impact of source and target sample sizes and task discrepancy on the effectiveness of knowledge transfer. We show that the final learning error of the $Q^*$ function is significantly improved from the single task rate both theoretically and empirically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15209v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elynn Chen, Xi Chen, Wenbo Jing</dc:creator>
    </item>
    <item>
      <title>A Kernelization-Based Approach to Nonparametric Binary Choice Models</title>
      <link>https://arxiv.org/abs/2410.15734</link>
      <description>arXiv:2410.15734v2 Announce Type: replace-cross 
Abstract: We propose a new estimator for nonparametric binary choice models that does not impose a parametric structure on either the systematic function of covariates or the distribution of the error term. A key advantage of our approach is its computational scalability in the number of covariates. For instance, even when assuming a normal error distribution as in probit models, commonly used sieves for approximating an unknown function of covariates can lead to a large-dimensional optimization problem when the number of covariates is moderate. Our approach, motivated by kernel methods in machine learning, views certain reproducing kernel Hilbert spaces as special sieve spaces, coupled with spectral cut-off regularization for dimension reduction. We establish the consistency of the proposed estimator and asymptotic normality of the plug-in estimator for weighted average partial derivatives. Simulation studies show that, compared to parametric estimation methods, the proposed method effectively improves finite sample performance in cases of misspecification, and has a rather mild efficiency loss if the model is correctly specified. Using administrative data on the grant decisions of US asylum applications to immigration courts, along with nine case-day variables on weather and pollution, we re-examine the effect of outdoor temperature on court judges' ``mood'', and thus, their grant decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15734v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guo Yan</dc:creator>
    </item>
    <item>
      <title>A Spatiotemporal, Quasi-experimental Causal Inference Approach to Characterize the Effects of Global Plastic Waste Export and Burning on Air Quality Using Remotely Sensed Data</title>
      <link>https://arxiv.org/abs/2503.04491</link>
      <description>arXiv:2503.04491v3 Announce Type: replace-cross 
Abstract: Open burning of plastic waste may pose a significant threat to global health by degrading air quality, but quantitative research on this problem -- crucial for policy making -- has been stunted by lack of data. Many low- and middle-income countries, where open burning is most concerning, have little to no air quality monitoring. Here, we leverage remotely sensed data products combined with spatiotemporal causal analytic techniques to evaluate the impact of large-scale plastic waste policies on air quality. Throughout, we study Indonesia before and after 2018, when China halted its import of plastic waste, resulting in diversion of this massive waste stream to other countries. We tailor cutting-edge statistical methods to this setting, estimating effects of increased plastic waste imports on fine particulate matter (PM$_{2.5}$) near waste dump sites in Indonesia as a function of proximity to ports, an induced continuous exposure. We observe strong evidence that monthly PM$_{2.5}$increased after China's ban (2018-2019) relative to expected business-as-usual (2012-2017), with increases up to 1.68 $\mu$g/m$^3$ (95\% CI = [0.72, 2.48]) at dump sites with medium-high port proximity. Effects were more modest at sites with very high port proximity, possibly reflecting smaller increases in dumping/burning where government oversight is greater.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04491v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ellen M. Considine, Rachel C. Nethery</dc:creator>
    </item>
    <item>
      <title>Asymptotically well-calibrated Bayesian $p$-value using the Kolmogorov-Smirnov statistic</title>
      <link>https://arxiv.org/abs/2504.14077</link>
      <description>arXiv:2504.14077v3 Announce Type: replace-cross 
Abstract: The posterior predictive $p$-value (ppp) is widely used in Bayesian model evaluation. However, due to double use of the data, the ppp may not be a valid $p$-value even in large samples: The asymptotic null distribution of the ppp can be non-uniform unless the underlying test statistic satisfies certain well-calibration conditions. Such conditions have been studied in the literature for asymptotically normal test statistics. We extend this line of work by establishing well-calibration conditions for test statistics that are not necessarily asymptotically normal. In particular, we show that Kolmogorov-Smirnov (KS)-type test statistics satisfy these conditions, such that their ppps are asymptotically well-calibrated Bayesian $p$-values. KS-type statistics are versatile, omnibus, and sensitive to model misspecifications. They apply to i.i.d. real-valued data, as well as non-identically distributed observations under regression models. Numerical experiments demonstrate that such $p$-values are well behaved in finite samples and can effectively detect a wide range of alternative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14077v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yueming Shen, Surya Tokdar</dc:creator>
    </item>
    <item>
      <title>A new measure of dependence: Integrated $R^2$</title>
      <link>https://arxiv.org/abs/2505.18146</link>
      <description>arXiv:2505.18146v5 Announce Type: replace-cross 
Abstract: We introduce a novel measure of dependence that captures the extent to which a random variable $Y$ is determined by a random vector $X$. The measure equals zero precisely when $Y$ and $X$ are independent, and it attains one exactly when $Y$ is almost surely a measurable function of $X$. We further extend this framework to define a measure of conditional dependence between $Y$ and $X$ given $Z$. We propose a simple and interpretable estimator with computational complexity comparable to classical correlation coefficients, including those of Pearson, Spearman, and Chatterjee. Leveraging this dependence measure, we develop a tuning-free, model-agnostic variable selection procedure and establish its consistency under appropriate sparsity conditions. Extensive experiments on synthetic and real datasets highlight the strong empirical performance of our methodology and demonstrate substantial gains over existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18146v5</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mona Azadkia, Pouya Roudaki</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Experiments with Latent Outcomes: Key Results and Their Implications for Design and Analysis</title>
      <link>https://arxiv.org/abs/2505.21909</link>
      <description>arXiv:2505.21909v3 Announce Type: replace-cross 
Abstract: How should researchers analyze randomized experiments in which the main outcome is latent and measured in multiple ways but each measure contains some degree of error? We first identify a critical study-specific noncomparability problem in existing methods for handling multiple measurements, which often rely on strong modeling assumptions or arbitrary standardization. Such approaches render the resulting estimands noncomparable across studies. To address the problem, we describe design-based approaches that enable researchers to identify causal parameters of interest, suggest ways that experimental designs can be augmented so as to make assumptions more credible, and discuss empirical tests of key assumptions. We show that when experimental researchers invest appropriately in multiple outcome measures, an optimally weighted scaled index of these measures enables researchers to obtain efficient and interpretable estimates of causal parameters by applying standard regression. An empirical application illustrates the gains in precision and robustness that multiple outcome measures can provide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21909v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu, Donald P. Green</dc:creator>
    </item>
    <item>
      <title>Precision Dose-Finding Design for Phase I Oncology Trials by Integrating Pharmacology Data</title>
      <link>https://arxiv.org/abs/2509.05120</link>
      <description>arXiv:2509.05120v2 Announce Type: replace-cross 
Abstract: Phase I oncology trials aim to identify a safe dose - often the maximum tolerated dose (MTD) - for subsequent studies. Conventional designs focus on population-level toxicity modeling, with recent attention on leveraging pharmacokinetic (PK) data to improve dose selection. We propose the Precision Dose-Finding (PDF) design, a novel Bayesian phase I framework that integrates individual patient PK profiles into the dose-finding process. By incorporating patient-specific PK parameters (such as volume of distribution and elimination rate), PDF models toxicity risk at the individual level, in contrast to traditional methods that ignore inter-patient variability. The trial is structured in two stages: an initial training stage to update model parameters using cohort-based dose escalation, and a subsequent test stage in which doses for new patients are chosen based on each patient's own PK-predicted toxicity probability. This two-stage approach enables truly personalized dose assignment while maintaining rigorous safety oversight. Extensive simulation studies demonstrate the feasibility of PDF and suggest that it provides improved safety and dosing precision relative to the continual reassessment method (CRM). The PDF design thus offers a refined dose-finding strategy that tailors the MTD to individual patients, aligning phase I trials with the ideals of precision medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05120v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyong Ju Lee, Yuan Ji</dc:creator>
    </item>
    <item>
      <title>Robust Confidence Intervals for a Binomial Proportion: Local Optimality and Adaptivity</title>
      <link>https://arxiv.org/abs/2509.05568</link>
      <description>arXiv:2509.05568v2 Announce Type: replace-cross 
Abstract: This paper revisits the classical problem of interval estimation of a binomial proportion under Huber contamination. Our main result derives the rate of optimal interval length when the contamination proportion is unknown under a local minimax framework, where the performance of an interval is evaluated at each point in the parameter space. By comparing the rate with the optimal length of a confidence interval that is allowed to use the knowledge of contamination proportion, we characterize the exact adaptation cost due to the ignorance of data quality. Our construction of the confidence interval to achieve local length optimality builds on robust hypothesis testing with a new monotonization step, which guarantees valid coverage, boundary-respecting intervals, and an efficient algorithm for computing the endpoints. The general strategy of interval construction can be applied beyond the binomial setting, and leads to optimal interval estimation for Poisson data with contamination as well. We also investigate a closely related Erd\H{o}s--R\'{e}nyi model with node contamination. Though its optimal rate of parameter estimation agrees with that of the binomial setting, we show that adaptation to unknown contamination proportion is provably impossible for interval estimation in that setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05568v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minjun Cho, Yuetian Luo, Chao Gao</dc:creator>
    </item>
    <item>
      <title>Testing Monotonicity in a Finite Population</title>
      <link>https://arxiv.org/abs/2512.25032</link>
      <description>arXiv:2512.25032v2 Announce Type: replace-cross 
Abstract: We consider the extent to which we can learn from a completely randomized experiment whether all individuals have treatment effects that are weakly of the same sign, a condition we call monotonicity. From a classical sampling perspective, it is well-known that monotonicity is not falsifiable. By contrast, we show from the design-based perspective -- in which the units in the population are fixed and only treatment assignment is stochastic -- that the distribution of treatment effects in the finite population (and hence whether monotonicity holds) is formally identified. We argue, however, that the usual definition of identification is unnatural in the design-based setting because it imagines knowing the distribution of outcomes over different treatment assignments for the same units. We thus evaluate the informativeness of the data by the extent to which it enables frequentist testing and Bayesian updating. We show that frequentist tests can have nontrivial power against some alternatives, but power is generically limited. Likewise, we show that there exist (non-degenerate) Bayesian priors that never update about whether monotonicity holds. We conclude that, despite the formal identification result, the ability to learn about monotonicity from data in practice is severely limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.25032v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen, Jonathan Roth, Jann Spiess</dc:creator>
    </item>
  </channel>
</rss>
