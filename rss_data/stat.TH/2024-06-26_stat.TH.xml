<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Jun 2024 08:19:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Causal Inference on Process Graphs, Part II: Causal Structure and Effect Identification</title>
      <link>https://arxiv.org/abs/2406.17422</link>
      <description>arXiv:2406.17422v1 Announce Type: new 
Abstract: A structural vector autoregressive (SVAR) process is a linear causal model for variables that evolve over a discrete set of time points and between which there may be lagged and instantaneous effects. The qualitative causal structure of an SVAR process can be represented by its finite and directed process graph, in which a directed link connects two processes whenever there is a lagged or instantaneous effect between them. At the process graph level, the causal structure of SVAR processes is compactly parameterised in the frequency domain. In this paper, we consider the problem of causal discovery and causal effect estimation from the spectral density, the frequency domain analogue of the auto covariance, of the SVAR process. Causal discovery concerns the recovery of the process graph and causal effect estimation concerns the identification and estimation of causal effects in the frequency domain.
  We show that information about the process graph, in terms of $d$- and $t$-separation statements, can be identified by verifying algebraic constraints on the spectral density. Furthermore, we introduce a notion of rational identifiability for frequency causal effects that may be confounded by exogenous latent processes, and show that the recent graphical latent factor half-trek criterion can be used on the process graph to assess whether a given (confounded) effect can be identified by rational operations on the entries of the spectral density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17422v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas-Domenic Reiter, Jonas Wahl, Andreas Gerhardus, Jakob Runge</dc:creator>
    </item>
    <item>
      <title>Can independent Metropolis beat crude Monte Carlo?</title>
      <link>https://arxiv.org/abs/2406.17699</link>
      <description>arXiv:2406.17699v1 Announce Type: new 
Abstract: Assume that we would like to estimate the expected value of a function $F$ with respect to a density $\pi$. We prove that if $\pi$ is close enough under KL divergence to another density $q$, an independent Metropolis sampler estimator that obtains samples from $\pi$ with proposal density $q$, enriched with a variance reduction computational strategy based on control variates, achieves smaller asymptotic variance than that of the crude Monte Carlo estimator. The control variates construction requires no extra computational effort but assumes that the expected value of $F$ under $q$ is analytically available. We illustrate this result by calculating the marginal likelihood in a linear regression model with prior-likelihood conflict and a non-conjugate prior. Furthermore, we propose an adaptive independent Metropolis algorithm that adapts the proposal density such that its KL divergence with the target is being reduced. We demonstrate its applicability in a Bayesian logistic and Gaussian process regression problems and we rigorously justify our asymptotic arguments under easily verifiable and essentially minimal conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17699v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siran Liu, Petros Dellaportas, Michalis K. Titsias</dc:creator>
    </item>
    <item>
      <title>Exploring Stochastic Mean Curvature Flow on Networks Using Ito Calculus</title>
      <link>https://arxiv.org/abs/2406.16920</link>
      <description>arXiv:2406.16920v1 Announce Type: cross 
Abstract: In this paper, we investigate the stochastic mean curvature flow (SMCF) on networks, a niche area within stochastic processes and geometric analysis. By applying Ito calculus, we analyze the evolution of network structures influenced by random perturbations. We derive a stochastic differential equation (SDE) for the network edges and utilize numerical simulations to study the stability, long-term behavior, and pattern formation in these systems. Our results offer new insights into the dynamics of complex networks under stochastic influences and open pathways for future research in stochastic geometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16920v1</guid>
      <category>math.DG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.21595.37923/1</arxiv:DOI>
      <dc:creator>Roman Bahadursingh</dc:creator>
    </item>
    <item>
      <title>Topological Data Analysis via Undergraduate Linear Algebra</title>
      <link>https://arxiv.org/abs/2406.17045</link>
      <description>arXiv:2406.17045v1 Announce Type: cross 
Abstract: Topological Data Analysis has grown in popularity in recent years as a way to apply tools from algebraic topology to large data sets. One of the main tools in topological data analysis is persistent homology. This paper uses undergraduate linear algebra to provide explicit methods for, and examples of, computing persistent (co)homology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17045v1</guid>
      <category>math.AT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cheyne Glass, Elizabeth Vidaurre</dc:creator>
    </item>
    <item>
      <title>Greedy equivalence search for nonparametric graphical models</title>
      <link>https://arxiv.org/abs/2406.17228</link>
      <description>arXiv:2406.17228v1 Announce Type: cross 
Abstract: One of the hallmark achievements of the theory of graphical models and Bayesian model selection is the celebrated greedy equivalence search (GES) algorithm due to Chickering and Meek. GES is known to consistently estimate the structure of directed acyclic graph (DAG) models in various special cases including Gaussian and discrete models, which are in particular curved exponential families. A general theory that covers general nonparametric DAG models, however, is missing. Here, we establish the consistency of greedy equivalence search for general families of DAG models that satisfy smoothness conditions on the Markov factorization, and hence may not be curved exponential families, or even parametric. The proof leverages recent advances in nonparametric Bayes to construct a test for comparing misspecified DAG models that avoids arguments based on the Laplace approximation. Nonetheless, when the Laplace approximation is valid and a consistent scoring function exists, we recover the classical result. As a result, we obtain a general consistency theorem for GES applied to general DAG models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17228v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bryon Aragam</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference for CP Tensor Factor Models</title>
      <link>https://arxiv.org/abs/2406.17278</link>
      <description>arXiv:2406.17278v1 Announce Type: cross 
Abstract: High-dimensional tensor-valued data have recently gained attention from researchers in economics and finance. We consider the estimation and inference of high-dimensional tensor factor models, where each dimension of the tensor diverges. Our focus is on a factor model that admits CP-type tensor decomposition, which allows for non-orthogonal loading vectors. Based on the contemporary covariance matrix, we propose an iterative simultaneous projection estimation method. Our estimator is robust to weak dependence among factors and weak correlation across different dimensions in the idiosyncratic shocks. We establish an inferential theory, demonstrating both consistency and asymptotic normality under relaxed assumptions. Within a unified framework, we consider two eigenvalue ratio-based estimators for the number of factors in a tensor factor model and justify their consistency. Through a simulation study and two empirical applications featuring sorted portfolios and international trade flows, we illustrate the advantages of our proposed estimator over existing methodologies in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17278v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bin Chen, Yuefeng Han, Qiyang Yu</dc:creator>
    </item>
    <item>
      <title>Generalizability of experimental studies</title>
      <link>https://arxiv.org/abs/2406.17374</link>
      <description>arXiv:2406.17374v1 Announce Type: cross 
Abstract: Experimental studies are a cornerstone of machine learning (ML) research. A common, but often implicit, assumption is that the results of a study will generalize beyond the study itself, e.g. to new data. That is, there is a high probability that repeating the study under different conditions will yield similar results. Despite the importance of the concept, the problem of measuring generalizability remains open. This is probably due to the lack of a mathematical formalization of experimental studies. In this paper, we propose such a formalization and develop a quantifiable notion of generalizability. This notion allows to explore the generalizability of existing studies and to estimate the number of experiments needed to achieve the generalizability of new studies. To demonstrate its usefulness, we apply it to two recently published benchmarks to discern generalizable and non-generalizable results. We also publish a Python module that allows our analysis to be repeated for other experimental studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17374v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Federico Matteucci, Vadim Arzamasov, Jose Cribeiro-Ramallo, Marco Heyden, Konstantin Ntounas, Klemens B\"ohm</dc:creator>
    </item>
    <item>
      <title>Tame sparse exponential random graphs</title>
      <link>https://arxiv.org/abs/2406.17390</link>
      <description>arXiv:2406.17390v1 Announce Type: cross 
Abstract: In this paper, we obtain a precise estimate of the probability that the sparse binomial random graph contains a large number of vertices in a triangle. The estimate of log of this probability is correct up to second order, and enables us to propose an exponential random graph model based on the number of vertices in a triangle. Specifically, by tuning a single parameter, we can with high probability induce any given fraction of vertices in a triangle. Moreover, in the proposed exponential random graph model we derive the large deviation principle for the number of edges. As a byproduct, we propose a consistent estimator of the tuning parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17390v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suman Chakraborty, Remco van der Hofstad, Frank den Hollander</dc:creator>
    </item>
    <item>
      <title>Copula-Based Estimation of Causal Effects in Multiple Linear and Path Analysis Models</title>
      <link>https://arxiv.org/abs/2406.17445</link>
      <description>arXiv:2406.17445v1 Announce Type: cross 
Abstract: Regression analysis is one of the most popularly used statistical technique which only measures the direct effect of independent variables on dependent variable. Path analysis looks for both direct and indirect effects of independent variables and may overcome several hurdles allied with regression models. It utilizes one or more structural regression equations in the model which are used to estimate the unknown parameters. The aim of this work is to study the path analysis models when the endogenous (dependent) variable and exogenous (independent) variables are linked through the elliptical copulas. Using well-organized numerical schemes, we investigate the performance of path models when direct and indirect effects are estimated applying classical ordinary least squares and copula-based regression approaches in different scenarios. Finally, two real data applications are also presented to demonstrate the performance of path analysis using copula approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17445v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alam Ali, Ashok Kumar Pathak, Mohd Arshad, Ayyub Sheikhi</dc:creator>
    </item>
    <item>
      <title>Two-Stage Testing in a high dimensional setting</title>
      <link>https://arxiv.org/abs/2406.17466</link>
      <description>arXiv:2406.17466v1 Announce Type: cross 
Abstract: In a high dimensional regression setting in which the number of variables ($p$) is much larger than the sample size ($n$), the number of possible two-way interactions between the variables is immense. If the number of variables is in the order of one million, which is usually the case in e.g., genetics, the number of two-way interactions is of the order one million squared. In the pursuit of detecting two-way interactions, testing all pairs for interactions one-by-one is computational unfeasible and the multiple testing correction will be severe. In this paper we describe a two-stage testing procedure consisting of a screening and an evaluation stage. It is proven that, under some assumptions, the tests-statistics in the two stages are asymptotically independent. As a result, multiplicity correction in the second stage is only needed for the number of statistical tests that are actually performed in that stage. This increases the power of the testing procedure. Also, since the testing procedure in the first stage is computational simple, the computational burden is lowered. Simulations have been performed for multiple settings and regression models (generalized linear models and Cox PH model) to study the performance of the two-stage testing procedure. The results show type I error control and an increase in power compared to the procedure in which the pairs are tested one-by-one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17466v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marianne A Jonker, Luc van Schijndel, Eric Cator</dc:creator>
    </item>
    <item>
      <title>Learning Dynamic Bayesian Networks from Data: Foundations, First Principles and Numerical Comparisons</title>
      <link>https://arxiv.org/abs/2406.17585</link>
      <description>arXiv:2406.17585v1 Announce Type: cross 
Abstract: In this paper, we present a guide to the foundations of learning Dynamic Bayesian Networks (DBNs) from data in the form of multiple samples of trajectories for some length of time. We present the formalism for a generic as well as a set of common types of DBNs for particular variable distributions. We present the analytical form of the models, with a comprehensive discussion on the interdependence between structure and weights in a DBN model and their implications for learning. Next, we give a broad overview of learning methods and describe and categorize them based on the most important statistical features, and how they treat the interplay between learning structure and weights. We give the analytical form of the likelihood and Bayesian score functions, emphasizing the distinction from the static case. We discuss functions used in optimization to enforce structural requirements. We briefly discuss more complex extensions and representations. Finally we present a set of comparisons in different settings for various distinct but representative algorithms across the variants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17585v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vyacheslav Kungurtsev, Petr Rysavy, Fadwa Idlahcen, Pavel Rytir, Ales Wodecki</dc:creator>
    </item>
    <item>
      <title>Constructing structured tensor priors for Bayesian inverse problems</title>
      <link>https://arxiv.org/abs/2406.17597</link>
      <description>arXiv:2406.17597v1 Announce Type: cross 
Abstract: Specifying a prior distribution is an essential part of solving Bayesian inverse problems. The prior encodes a belief on the nature of the solution and this regularizes the problem. In this article we completely characterize a Gaussian prior that encodes the belief that the solution is a structured tensor. We first define the notion of (A,b)-constrained tensors and show that they describe a large variety of different structures such as Hankel, circulant, triangular, symmetric, and so on. Then we completely characterize the Gaussian probability distribution of such tensors by specifying its mean vector and covariance matrix. Furthermore, explicit expressions are proved for the covariance matrix of tensors whose entries are invariant under a permutation. These results unlock a whole new class of priors for Bayesian inverse problems. We illustrate how new kernel functions can be designed and efficiently computed and apply our results on two particular Bayesian inverse problems: completing a Hankel matrix from a few noisy measurements and learning an image classifier of handwritten digits. The effectiveness of the proposed priors is demonstrated for both problems. All applications have been implemented as reactive Pluto notebooks in Julia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17597v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kim Batselier</dc:creator>
    </item>
    <item>
      <title>Information Geometry of Wasserstein Statistics on Shapes and Affine Deformations</title>
      <link>https://arxiv.org/abs/2307.12508</link>
      <description>arXiv:2307.12508v4 Announce Type: replace 
Abstract: Information geometry and Wasserstein geometry are two main structures introduced in a manifold of probability distributions, and they capture its different characteristics. We study characteristics of Wasserstein geometry in the framework of Li and Zhao (2023) for the affine deformation statistical model, which is a multi-dimensional generalization of the location-scale model. We compare merits and demerits of estimators based on information geometry and Wasserstein geometry. The shape of a probability distribution and its affine deformation are separated in the Wasserstein geometry, showing its robustness against the waveform perturbation in exchange for the loss in Fisher efficiency. We show that the Wasserstein estimator is the moment estimator in the case of the elliptically symmetric affine deformation model. It coincides with the information-geometrical estimator (maximum-likelihood estimator) when the waveform is Gaussian. The role of the Wasserstein efficiency is elucidated in terms of robustness against waveform change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12508v4</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shun-ichi Amari, Takeru Matsuda</dc:creator>
    </item>
    <item>
      <title>Straight-Through meets Sparse Recovery: the Support Exploration Algorithm</title>
      <link>https://arxiv.org/abs/2301.13584</link>
      <description>arXiv:2301.13584v3 Announce Type: replace-cross 
Abstract: The {\it straight-through estimator} (STE) is commonly used to optimize quantized neural networks, yet its contexts of effective performance are still unclear despite empirical successes.To make a step forward in this comprehension, we apply STE to a well-understood problem: {\it sparse support recovery}. We introduce the {\it Support Exploration Algorithm} (SEA), a novel algorithm promoting sparsity, and we analyze its performance in support recovery (a.k.a. model selection) problems. SEA explores more supports than the state-of-the-art, leading to superior performance in experiments, especially when the columns of $A$ are strongly coherent.The theoretical analysis considers recovery guarantees when the linear measurements matrix $A$ satisfies the {\it Restricted Isometry Property} (RIP).The sufficient conditions of recovery are comparable but more stringent than those of the state-of-the-art in sparse support recovery. Their significance lies mainly in their applicability to an instance of the STE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.13584v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>ICML 2024, the 41st International Conference on Machine Learning, Jul 2024, Vienna, Austria</arxiv:journal_reference>
      <dc:creator>Mimoun Mohamed (QARMA, I2M), Fran\c{c}ois Malgouyres (IMT), Valentin Emiya (QARMA), Caroline Chaux (IPAL)</dc:creator>
    </item>
    <item>
      <title>Computerized Tomography and Reproducing Kernels</title>
      <link>https://arxiv.org/abs/2311.07465</link>
      <description>arXiv:2311.07465v2 Announce Type: replace-cross 
Abstract: The X-ray transform is one of the most fundamental integral operators in image processing and reconstruction. In this article, we revisit the formalism of the X-ray transform by considering it as an operator between Reproducing Kernel Hilbert Spaces (RKHS). Within this framework, the X-ray transform can be viewed as a natural analogue of Euclidean projection. The RKHS framework considerably simplifies projection image interpolation, and leads to an analogue of the celebrated representer theorem for the problem of tomographic reconstruction. It leads to methodology that is dimension-free and stands apart from conventional filtered back-projection techniques, as it does not hinge on the Fourier transform. It also allows us to establish sharp stability results at a genuinely functional level (i.e. without recourse to discretization), but in the realistic setting where the data are discrete and noisy. The RKHS framework is versatile, accommodating any reproducing kernel on a unit ball, affording a high level of generality. When the kernel is chosen to be rotation-invariant, explicit spectral representations can be obtained, elucidating the regularity structure of the associated Hilbert spaces. Moreover, the reconstruction problem can be solved at the same computational cost as filtered back-projection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07465v2</guid>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ho Yun, Victor M. Panaretos</dc:creator>
    </item>
    <item>
      <title>Computational-Statistical Gaps for Improper Learning in Sparse Linear Regression</title>
      <link>https://arxiv.org/abs/2402.14103</link>
      <description>arXiv:2402.14103v2 Announce Type: replace-cross 
Abstract: We study computational-statistical gaps for improper learning in sparse linear regression. More specifically, given $n$ samples from a $k$-sparse linear model in dimension $d$, we ask what is the minimum sample complexity to efficiently (in time polynomial in $d$, $k$, and $n$) find a potentially dense estimate for the regression vector that achieves non-trivial prediction error on the $n$ samples. Information-theoretically this can be achieved using $\Theta(k \log (d/k))$ samples. Yet, despite its prominence in the literature, there is no polynomial-time algorithm known to achieve the same guarantees using less than $\Theta(d)$ samples without additional restrictions on the model. Similarly, existing hardness results are either restricted to the proper setting, in which the estimate must be sparse as well, or only apply to specific algorithms.
  We give evidence that efficient algorithms for this task require at least (roughly) $\Omega(k^2)$ samples. In particular, we show that an improper learning algorithm for sparse linear regression can be used to solve sparse PCA problems (with a negative spike) in their Wishart form, in regimes in which efficient algorithms are widely believed to require at least $\Omega(k^2)$ samples. We complement our reduction with low-degree and statistical query lower bounds for the sparse PCA problems from which we reduce.
  Our hardness results apply to the (correlated) random design setting in which the covariates are drawn i.i.d. from a mean-zero Gaussian distribution with unknown covariance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14103v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rares-Darius Buhai, Jingqiu Ding, Stefan Tiegel</dc:creator>
    </item>
    <item>
      <title>On the Convergence of the Sinkhorn-Knopp Algorithm with Sparse Cost Matrices</title>
      <link>https://arxiv.org/abs/2405.20528</link>
      <description>arXiv:2405.20528v4 Announce Type: replace-cross 
Abstract: Matrix scaling problems with sparse cost matrices arise frequently in various domains, such as optimal transport, image processing, and machine learning. The Sinkhorn-Knopp algorithm is a popular iterative method for solving these problems, but its convergence properties in the presence of sparsity have not been thoroughly analyzed. This paper presents a theoretical analysis of the convergence rate of the Sinkhorn-Knopp algorithm specifically for sparse cost matrices. We derive novel bounds on the convergence rate that explicitly depend on the sparsity pattern and the degree of nonsparsity of the cost matrix. These bounds provide new insights into the behavior of the algorithm and highlight the potential for exploiting sparsity to develop more efficient solvers. We also explore connections between our sparse convergence results and existing convergence results for dense matrices, showing that our bounds generalize the dense case. Our analysis reveals that the convergence rate improves as the matrix becomes less sparse and as the minimum entry of the cost matrix increases relative to its maximum entry. These findings have important practical implications, suggesting that the Sinkhorn-Knopp algorithm may be particularly well-suited for large-scale matrix scaling problems with sparse cost matrices arising in real-world applications. Future research directions include investigating tighter bounds based on more sophisticated sparsity patterns, developing algorithm variants that actively exploit sparsity, and empirically validating the benefits of our theoretical results on real-world datasets. This work advances our understanding of the Sinkhorn-Knopp algorithm for an important class of matrix scaling problems and lays the foundation for designing more efficient and scalable solutions in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20528v4</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose Rafael Espinosa Mena</dc:creator>
    </item>
    <item>
      <title>Sampling Strategies in Bayesian Inversion: A Study of RTO and Langevin Methods</title>
      <link>https://arxiv.org/abs/2406.16658</link>
      <description>arXiv:2406.16658v2 Announce Type: replace-cross 
Abstract: This paper studies two classes of sampling methods for the solution of inverse problems, namely Randomize-Then-Optimize (RTO), which is rooted in sensitivity analysis, and Langevin methods, which are rooted in the Bayesian framework. The two classes of methods correspond to different assumptions and yield samples from different target distributions. We highlight the main conceptual and theoretical differences between the two approaches and compare them from a practical point of view by tackling two classical inverse problems in imaging: deblurring and inpainting. We show that the choice of the sampling method has a significant impact on the quality of the reconstruction and that the RTO method is more robust to the choice of the parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16658v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Remi Laumont, Yiqiu Dong, Martin Skovgaard Andersen</dc:creator>
    </item>
  </channel>
</rss>
