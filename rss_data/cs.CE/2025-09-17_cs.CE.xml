<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Sep 2025 04:00:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Physics-based deep kernel learning for parameter estimation in high dimensional PDEs</title>
      <link>https://arxiv.org/abs/2509.14054</link>
      <description>arXiv:2509.14054v1 Announce Type: new 
Abstract: Inferring parameters of high-dimensional partial differential equations (PDEs) poses significant computational and inferential challenges, primarily due to the curse of dimensionality and the inherent limitations of traditional numerical methods. This paper introduces a novel two-stage Bayesian framework that synergistically integrates training, physics-based deep kernel learning (DKL) with Hamiltonian Monte Carlo (HMC) to robustly infer unknown PDE parameters and quantify their uncertainties from sparse, exact observations. The first stage leverages physics-based DKL to train a surrogate model, which jointly yields an optimized neural network feature extractor and robust initial estimates for the PDE parameters. In the second stage, with the neural network weights fixed, HMC is employed within a full Bayesian framework to efficiently sample the joint posterior distribution of the kernel hyperparameters and the PDE parameters. Numerical experiments on canonical and high-dimensional inverse PDE problems demonstrate that our framework accurately estimates parameters, provides reliable uncertainty estimates, and effectively addresses challenges of data sparsity and model complexity, offering a robust and scalable tool for diverse scientific and engineering applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14054v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weihao Yan, Christoph Brune, Mengwu Guo</dc:creator>
    </item>
    <item>
      <title>Programmable Cognitive Bias in Social Agents</title>
      <link>https://arxiv.org/abs/2509.13588</link>
      <description>arXiv:2509.13588v1 Announce Type: cross 
Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying agent behavior in LLM-based social simulation. We found that conventional approaches that specify agent behaviors through implicit natural language descriptions cannot yield consistent behaviors across models, and the produced agent behaviors do not capture the nuances of the descriptions. In contrast, CoBRA presents a new approach to program agents' cognitive biases explicitly, by grounding agents' expected behaviors using classic social science experiments. CoBRA has two components: (1) Cognitive Bias Index that measures the cognitive bias of a social agent, by quantifying the agent's reactions in a set of validated classical social science experiments; (2) Behavioral Regulation Engine that aligns the agent's behavior to demonstrate controlled cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and technical benchmarks. Our results suggest that CoBRA can precisely program the cognitive bias demonstrated in a social agent in a model-agnostic manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13588v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan Liu, Haoyang Shang, Haojian Jin</dc:creator>
    </item>
    <item>
      <title>Transformer-based Topology Optimization</title>
      <link>https://arxiv.org/abs/2509.05800</link>
      <description>arXiv:2509.05800v2 Announce Type: replace 
Abstract: Topology optimization enables the design of highly efficient and complex structures, but conventional iterative methods, such as SIMP-based approaches, often suffer from high computational costs and sensitivity to initial conditions. Although machine learning methods have recently shown promise for accelerating topology generation, existing models either remain iterative or struggle to match ground-truth performance. In this work, we propose a transformer-based machine learning model for topology optimization that embeds critical boundary and loading conditions directly into the tokenized domain representation via a class token mechanism. We implement this model on static and dynamic datasets, using transfer learning and FFT encoding of dynamic loads to improve our performance on the dynamic dataset. Auxiliary loss functions are introduced to promote the realism and manufacturability of the generated designs. We conduct a comprehensive evaluation of the model's performance, including compliance error, volume fraction error, floating material percentage, and load discrepancy error, and benchmark it against state-of-the-art non-iterative and iterative generative models. Our results demonstrate that the proposed model approaches the fidelity of diffusion-based models while remaining iteration-free, offering a significant step toward real-time, high-fidelity topology generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05800v2</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Lutheran, Srijan Das, Alireza Tabarraei</dc:creator>
    </item>
    <item>
      <title>Looking for Attention: Randomized Attention Test Design for Validator Monitoring in Optimistic Rollups</title>
      <link>https://arxiv.org/abs/2505.24393</link>
      <description>arXiv:2505.24393v2 Announce Type: replace-cross 
Abstract: Optimistic Rollups (ORUs) significantly enhance blockchain scalability but inherently suffer from the verifier's dilemma, particularly concerning validator attentiveness. Current systems lack mechanisms to proactively ensure validators are diligently monitoring L2 state transitions, creating a vulnerability where fraudulent states could be finalized. This paper introduces the Randomized Attention Test (RAT), a novel L1-based protocol designed to probabilistically challenge validators in ORUs, thereby verifying their liveness and computational readiness. Our game-theoretic analysis demonstrates that an Ideal Security Equilibrium, where all validators are attentive and proposers are honest, can be achieved with RAT. Notably, this equilibrium is attainable and stable with relatively low economic penalties (under \$1000) for non-responsive validators, a low attention test frequency (under 1\% per epoch), and a minimal operation overhead (monthly under \$30) with 10 validators. RAT thus provides a pivotal, practical mechanism to enforce validator diligence, fortifying the overall security and integrity of ORU systems with minimizing additional costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24393v2</guid>
      <category>cs.CR</category>
      <category>cs.CE</category>
      <category>cs.GT</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suhyeon Lee, Yeongju Bak</dc:creator>
    </item>
  </channel>
</rss>
