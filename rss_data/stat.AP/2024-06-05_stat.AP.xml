<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Jun 2024 04:01:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Democratizing Propensity Score Matching Using Web Application</title>
      <link>https://arxiv.org/abs/2406.02743</link>
      <description>arXiv:2406.02743v1 Announce Type: new 
Abstract: Traditionally, data scientists use exploratory data analysis techniques such as correlation analysis, summary statistics, and regression analysis for identifying the most product enhancements and roadmap planning. However, these conventional approaches often yield biased conclusions and suboptimal solutions, leading to a waste of valuable time and missed opportunities for higher-value outcomes. In contrast, there are alternative techniques that involve the use of causal inference methods. However, these methods suffer from issues of limited accessibility, as they are not easily understandable or effectively utilized by inexperienced practitioners. Additionally, their implementation necessitates a substantial investment of time and effort. To this end, this paper tackles these challenges by democratizing one of the causal inference methods called Propensity Score Matching (PSM) and enhancing its accessibility for less technically inclined users through the automation of the entire workflow using a web application. Our approach not only fills this accessibility gap but also contributes to the existing literature by introducing a more rigorous model selection process and an enhanced sensitivity analysis. By overcoming the limitations of traditional exploratory data analysis methods, our web application has empowered data scientists at Booking.com to make better use of PSM, thereby improving the overall efficacy of their analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02743v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Gajtkowski, Felipe Moraes</dc:creator>
    </item>
    <item>
      <title>Bayesian Adaptive Trials for Social Policy</title>
      <link>https://arxiv.org/abs/2406.02868</link>
      <description>arXiv:2406.02868v1 Announce Type: new 
Abstract: This paper proposes Bayesian Adaptive Trials (BAT) as both an efficient method to conduct trials and a unifying framework for evaluation social policy interventions, addressing limitations inherent in traditional methods such as Randomized Controlled Trials (RCT). Recognizing the crucial need for evidence-based approaches in public policy, the proposal aims to lower barriers to the adoption of evidence-based methods and align evaluation processes more closely with the dynamic nature of policy cycles. BATs, grounded in decision theory, offer a dynamic, ``learning as we go'' approach, enabling the integration of diverse information types and facilitating a continuous, iterative process of policy evaluation. BATs' adaptive nature is particularly advantageous in policy settings, allowing for more timely and context-sensitive decisions. Moreover, BATs' ability to value potential future information sources positions it as an optimal strategy for sequential data acquisition during policy implementation. While acknowledging the assumptions and models intrinsic to BATs, such as prior distributions and likelihood functions, the paper argues that these are advantageous for decision-makers in social policy, effectively merging the best features of various methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02868v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sally Cripps, Anna Lopatnikova, Hadi Mohasel Afshar, Ben Gales, Roman Marchant, Gilad Francis, Catarina Moreira, Alex Fischer</dc:creator>
    </item>
    <item>
      <title>Estimating Disease-Free Life Expectancy based on Clinical Data from the French Hospital Discharge Database</title>
      <link>https://arxiv.org/abs/2406.02934</link>
      <description>arXiv:2406.02934v1 Announce Type: new 
Abstract: The development of health indicators to measure healthy life expectancy (HLE) is an active field of research aimed at summarizing the health of a population. Although many health indicators have emerged in the literature as critical metrics in public health assessments, the methods and data to conduct this evaluation vary considerably in nature and quality. Traditionally, health data collection relies on population surveys. However, these studies, typically of limited size, encompass only a small yet representative segment of the population. This limitation can necessitate the separate estimation of incidence and mortality rates, significantly restricting the available analysis methods. In this article, we leverage an extract from the French National Hospital Discharge database to define health indicators. Our analysis focuses on the resulting Disease-Free Life Expectancy (Dis-FLE) indicator, which provides insights based on the hospital trajectory of each patient admitted to hospital in France during 2008-13. Through this research, we illustrate the advantages and disadvantages of employing large clinical datasets as the foundation for more robust health indicators. We shed light on the opportunities that such data offer for a more comprehensive understanding of the health status of a population. In particular, we estimate age-dependent hazard rates associated with sex, alcohol abuse, tobacco consumption, and obesity, as well as geographic location. Simultaneously, we delve into the challenges and limitations that arise when adopting such a data-driven approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02934v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3390/risks12060092</arxiv:DOI>
      <arxiv:journal_reference>Risks 2024, 12, 92</arxiv:journal_reference>
      <dc:creator>Oleksandr Sorochynskyi, Quentin Guibert, Fr\'ed\'eric Planchet, Micha\"el Schwarzinger</dc:creator>
    </item>
    <item>
      <title>Copula-based semiparametric nonnormal transformed linear model for survival data with dependent censoring</title>
      <link>https://arxiv.org/abs/2406.02948</link>
      <description>arXiv:2406.02948v1 Announce Type: cross 
Abstract: Although the independent censoring assumption is commonly used in survival analysis, it can be violated when the censoring time is related to the survival time, which often happens in many practical applications. To address this issue, we propose a flexible semiparametric method for dependent censored data. Our approach involves fitting the survival time and the censoring time with a joint transformed linear model, where the transformed function is unspecified. This allows for a very general class of models that can account for possible covariate effects, while also accommodating administrative censoring. We assume that the transformed variables have a bivariate nonnormal distribution based on parametric copulas and parametric marginals, which further enhances the flexibility of our method. We demonstrate the identifiability of the proposed model and establish the consistency and asymptotic normality of the model parameters under appropriate regularity conditions and assumptions. Furthermore, we evaluate the performance of our method through extensive simulation studies, and provide a real data example for illustration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02948v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huazhen Yu, Lixin Zhang</dc:creator>
    </item>
    <item>
      <title>Sparse two-stage Bayesian meta-analysis for individualized treatments</title>
      <link>https://arxiv.org/abs/2406.03056</link>
      <description>arXiv:2406.03056v1 Announce Type: cross 
Abstract: Individualized treatment rules tailor treatments to patients based on clinical, demographic, and other characteristics. Estimation of individualized treatment rules requires the identification of individuals who benefit most from the particular treatments and thus the detection of variability in treatment effects. To develop an effective individualized treatment rule, data from multisite studies may be required due to the low power provided by smaller datasets for detecting the often small treatment-covariate interactions. However, sharing of individual-level data is sometimes constrained. Furthermore, sparsity may arise in two senses: different data sites may recruit from different populations, making it infeasible to estimate identical models or all parameters of interest at all sites, and the number of non-zero parameters in the model for the treatment rule may be small. To address these issues, we adopt a two-stage Bayesian meta-analysis approach to estimate individualized treatment rules which optimize expected patient outcomes using multisite data without disclosing individual-level data beyond the sites. Simulation results demonstrate that our approach can provide consistent estimates of the parameters which fully characterize the optimal individualized treatment rule. We estimate the optimal Warfarin dose strategy using data from the International Warfarin Pharmacogenetics Consortium, where data sparsity and small treatment-covariate interaction effects pose additional statistical challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03056v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Junwei Shen, Erica E. M. Moodie, Shirin Golchi</dc:creator>
    </item>
    <item>
      <title>The Impossibility of Fair LLMs</title>
      <link>https://arxiv.org/abs/2406.03198</link>
      <description>arXiv:2406.03198v1 Announce Type: cross 
Abstract: The need for fair AI is increasingly clear in the era of general-purpose systems such as ChatGPT, Gemini, and other large language models (LLMs). However, the increasing complexity of human-AI interaction and its social impacts have raised questions of how fairness standards could be applied. Here, we review the technical frameworks that machine learning researchers have used to evaluate fairness, such as group fairness and fair representations, and find that their application to LLMs faces inherent limitations. We show that each framework either does not logically extend to LLMs or presents a notion of fairness that is intractable for LLMs, primarily due to the multitudes of populations affected, sensitive attributes, and use cases. To address these challenges, we develop guidelines for the more realistic goal of achieving fairness in particular use cases: the criticality of context, the responsibility of LLM developers, and the need for stakeholder participation in an iterative process of design and evaluation. Moreover, it may eventually be possible and even necessary to use the general-purpose capabilities of AI systems to address fairness challenges as a form of scalable AI-assisted alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03198v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacy Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Alexander D'Amour, Chenhao Tan</dc:creator>
    </item>
    <item>
      <title>Cooperative learning of Pl@ntNet's Artificial Intelligence algorithm: how does it work and how can we improve it?</title>
      <link>https://arxiv.org/abs/2406.03356</link>
      <description>arXiv:2406.03356v1 Announce Type: cross 
Abstract: Deep learning models for plant species identification rely on large annotated datasets. The PlantNet system enables global data collection by allowing users to upload and annotate plant observations, leading to noisy labels due to diverse user skills. Achieving consensus is crucial for training, but the vast scale of collected data makes traditional label aggregation strategies challenging. Existing methods either retain all observations, resulting in noisy training data or selectively keep those with sufficient votes, discarding valuable information. Additionally, as many species are rarely observed, user expertise can not be evaluated as an inter-user agreement: otherwise, botanical experts would have a lower weight in the AI training step than the average user. Our proposed label aggregation strategy aims to cooperatively train plant identification AI models. This strategy estimates user expertise as a trust score per user based on their ability to identify plant species from crowdsourced data. The trust score is recursively estimated from correctly identified species given the current estimated labels. This interpretable score exploits botanical experts' knowledge and the heterogeneity of users. Subsequently, our strategy removes unreliable observations but retains those with limited trusted annotations, unlike other approaches. We evaluate PlantNet's strategy on a released large subset of the PlantNet database focused on European flora, comprising over 6M observations and 800K users. We demonstrate that estimating users' skills based on the diversity of their expertise enhances labeling performance. Our findings emphasize the synergy of human annotation and data filtering in improving AI performance for a refined dataset. We explore incorporating AI-based votes alongside human input. This can further enhance human-AI interactions to detect unreliable observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03356v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanguy Lefort, Antoine Affouard, Benjamin Charlier, Jean-Christophe Lombardo, Mathias Chouet, Herv\'e Go\"eau, Joseph Salmon, Pierre Bonnet, Alexis Joly</dc:creator>
    </item>
    <item>
      <title>Gaussian Copula Models for Nonignorable Missing Data Using Auxiliary Marginal Quantiles</title>
      <link>https://arxiv.org/abs/2406.03463</link>
      <description>arXiv:2406.03463v1 Announce Type: cross 
Abstract: We present an approach for modeling and imputation of nonignorable missing data under Gaussian copulas. The analyst posits a set of quantiles of the marginal distributions of the study variables, for example, reflecting information from external data sources or elicited expert opinion. When these quantiles are accurately specified, we prove it is possible to consistently estimate the copula correlation and perform multiple imputation in the presence of nonignorable missing data. We develop algorithms for estimation and imputation that are computationally efficient, which we evaluate in simulation studies of multiple imputation inferences. We apply the model to analyze associations between lead exposure levels and end-of-grade test scores for 170,000 students in North Carolina. These measurements are not missing at random, as children deemed at-risk for high lead exposure are more likely to be measured. We construct plausible marginal quantiles for lead exposure using national statistics provided by the Centers for Disease Control and Prevention. Complete cases and missing at random analyses appear to underestimate the relationships between certain variables and end-of-grade test scores, while multiple imputation inferences under our model support stronger adverse associations between lead exposure and educational outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03463v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Feldman, Jerome P. Reiter, Daniel R. Kowal</dc:creator>
    </item>
    <item>
      <title>COVID-19 incidence in the Republic of Ireland: A case study for network-based time series models</title>
      <link>https://arxiv.org/abs/2307.06199</link>
      <description>arXiv:2307.06199v3 Announce Type: replace 
Abstract: The generalised network autoregressive (GNAR) model conceptualises time series on the vertices of a network; it has an autoregressive component for temporal dependence and a spatial autoregressive component for dependence between neighbouring vertices in the network. Consequently, the choice of underlying network is essential. This paper assesses the performance of GNAR models on different networks in predicting COVID-19 cases for the 26 counties in the Republic of Ireland, over two distinct pandemic phases (restricted and unrestricted), characterised by inter-county movement restrictions. Ten static networks are constructed, in which vertices represent counties, and edges are built upon neighbourhood relations, such as railway lines. We find that a GNAR model based on the fairly sparse Economic hub network explains the data best for the restricted pandemic phase while the fairly dense 21-nearest neighbour network performs best for the unrestricted phase. Across phases, GNAR models have higher predictive accuracy than standard ARIMA models which ignore the network structure. For county-specific predictions, in pandemic phases with more lenient or no COVID-19 regulation, the network effect is not quite as pronounced. The results indicate some robustness to the precise network architecture as long as the densities of the networks are similar. An analysis of the residuals justifies the model assumptions for the restricted phase but raises questions regarding their validity for the unrestricted phase. While generally performing better than ARIMA models which ignore network effects, there is scope for further development of the GNAR model to better model complex infectious diseases, including COVID-19.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.06199v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephanie Armbruster, Gesine Reinert</dc:creator>
    </item>
    <item>
      <title>Privacy-Preserving Line Outage Detection in Distribution Grids: An Efficient Approach with Uncompromised Performance</title>
      <link>https://arxiv.org/abs/2309.05140</link>
      <description>arXiv:2309.05140v2 Announce Type: replace 
Abstract: Recent advancements in research have shown the efficacy of employing sensor measurements, such as voltage and power data, in identifying line outages within distribution grids. However, these measurements inadvertently pose privacy risks to electricity customers by potentially revealing their sensitive information, such as household occupancy and economic status, to adversaries. To safeguard raw data from direct exposure to third-party adversaries, this paper proposes a novel decentralized data encryption scheme. The effectiveness of this encryption strategy is validated via demonstration of its differential privacy attributes by studying the Gaussian differential privacy. Recognizing that the encryption of raw data could affect the efficacy of outage detection, this paper analyzes the performance degradation by examining the Kullback-Leibler divergence between data distributions before and after the line outage. This analysis allows us to further alleviate the performance degradation by designing an innovative detection statistic that accurately approximates the optimal one. Manipulating the variance of this statistic, we demonstrate its ability to approach the optimal detection performance. The proposed privacy-aware detection procedure is evaluated using representative distribution grids and real load profiles, covering 17 distinct outage configurations. Our empirical results confirm the privacy-preserving nature of our approach and show that it achieves comparable detection performance to the optimal baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.05140v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenhan Xiao, Yizheng Liao, Yang Weng</dc:creator>
    </item>
    <item>
      <title>Inferring the Long-Term Causal Effects of Long-Term Treatments from Short-Term Experiments</title>
      <link>https://arxiv.org/abs/2311.08527</link>
      <description>arXiv:2311.08527v3 Announce Type: replace 
Abstract: We study inference on the long-term causal effect of a continual exposure to a novel intervention, which we term a long-term treatment, based on an experiment involving only short-term observations. Key examples include the long-term health effects of regularly-taken medicine or of environmental hazards and the long-term effects on users of changes to an online platform. This stands in contrast to short-term treatments or "shocks," whose long-term effect can reasonably be mediated by short-term observations, enabling the use of surrogate methods. Long-term treatments by definition have direct effects on long-term outcomes via continual exposure, so surrogacy conditions cannot reasonably hold. We connect the problem with offline reinforcement learning, leveraging doubly-robust estimators to estimate long-term causal effects for long-term treatments and construct confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08527v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Allen Tran, Aur\'elien Bibaut, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Relational hyperevent models for the coevolution of coauthoring and citation networks</title>
      <link>https://arxiv.org/abs/2308.01722</link>
      <description>arXiv:2308.01722v3 Announce Type: replace-cross 
Abstract: The development of suitable statistical models for the analysis of bibliographic networks has trailed behind the empirical ambitions expressed by recent studies of science of science. Extant research typically restricts the analytical focus to either paper citation networks, or author collaboration networks. These networks involve not only direct relationships between papers or authors, but also a broader system of dependencies between the references of papers connected through multiple simultaneous citation links. In this work, we extend recently developed relational hyperevent models (RHEM) to analyze scientific networks - systems of scientific publications connected by citations and authorship. We introduce new covariates that represent theoretically relevant and empirically meaningful sub-network configurations. The new model specification supports testing of hypotheses that align with the polyadic nature of scientific publication events and the multiple interdependencies between authors and references of current and prior papers. We implement the model using open-source software to analyze a large, publicly available scientific network dataset. A significant finding of the study is the tendency for subsets of papers to be repeatedly cited together across publications. This result is crucial as it suggests that the papers' impact may be partly due to endogenous network processes. More broadly, the study shows that models accounting for both the hyperedge structure of publication events and the interconnections between authors and references significantly enhance our understanding of the network mechanisms that drive scientific production, productivity, and impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01722v3</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\"urgen Lerner, Marian-Gabriel H\^ancean, Alessandro Lomi</dc:creator>
    </item>
  </channel>
</rss>
