<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Apr 2025 04:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Demographic Factors Associated with Triage Acuity, Admission and Length of Stay During Adult Emergency Department Visits</title>
      <link>https://arxiv.org/abs/2503.22781</link>
      <description>arXiv:2503.22781v1 Announce Type: new 
Abstract: Objective: To describe the association of demographic factors with triage acuity, hospital admission rates, and length of stay (LOS) for adult patients in the emergency department (ED). Methods: We performed a retrospective cross-sectional analysis using publicly available electronic health records describing visits to the ED of a single US medical center during 2011-2019. The primary exposures of interest were self-reported gender, race/ethnicity, and age. The outcomes studied were triage acuity, admission to hospital, and LOS in the ED. Odds ratios were calculated using propensity-score matching. Analyses were adjusted for confounding variables, including vital signs and diagnoses. Key Results: Black patients were more likely than White patients to experience long stays before admission but not before discharge. Men were more likely than women to be triaged as urgent or admitted, and had shorter stays. Patients over 30 were likelier to be triaged as urgent or admitted, but had longer stays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22781v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helena Coggan, Pradip Chaudhari, Yuval Barak-Corren, Andrew M. Fine, Ben Y. Reis, Jaya Aysola, William G. La Cava</dc:creator>
    </item>
    <item>
      <title>Inference on the Miss Distance in a Conjunction</title>
      <link>https://arxiv.org/abs/2503.22836</link>
      <description>arXiv:2503.22836v1 Announce Type: new 
Abstract: Over the last quarter-century, spacecraft conjunction assessment has focused on a quantity associated by its advocates with collision probability. This quantity has a well-known dilution feature, where it is small when uncertainty is large, giving rise to false confidence that a conjunction is safe when it is not. An alternative approach to conjunction assessment is to assess the missed detection probability that the best available information indicates the conjunction to be safe, when it is actually unsafe. In other words, the alternative seeks to answer the question of whether unknowable errors in the best available data might be especially unlucky. A proper implementation of this alternative avoids dilution and false confidence. Implementations of the alternative use either significance probabilities (p-values) associated with a null hypothesis that the miss distance is small, or confidence intervals on the miss distance. Both approaches rely on maximum likelihood principles to deal with nuisance variables, rather than marginalization. This paper discusses the problems with the traditional approach, and summarizes other work that developed the alternative approach. The paper presents examples of application of the alternatives using data from actual conjunctions experienced in operations, including synthetic scaling to highlight contrasts between the alternative and the traditional approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22836v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. Russell Carpenter, Anthony C. Davison, Soumaya Elkantassi, Matthew D. Hejduk</dc:creator>
    </item>
    <item>
      <title>Parameter estimation for fractional autoregressive process with seasonal structure</title>
      <link>https://arxiv.org/abs/2503.23089</link>
      <description>arXiv:2503.23089v1 Announce Type: new 
Abstract: This paper introduces a new kind of seasonal fractional autoregressive process (SFAR) driven by fractional Gaussian noise (fGn). The new model includes a standard seasonal AR model and fGn. {The estimation of the parameters of this new model has to solve two problems: nonstationarity from the seasonal structure and long memory from fGn. We innovatively solve these by getting a stationary subsequence, making a stationary additive sequence, and then obtaining their spectral density. Then, we use one-step procedure for Generalized Least Squares Estimator (GLSE) and the Geweke Porter-Hudak (GPH) method to get better results. We prove that both the initial and one-step estimators are consistent and asymptotically normal. Finally, we use Monte Carlo simulations with finite-sized samples to demonstrate the performance of these estimators. Moreover, through empirical analysis, it is shown that the SFAR model can simulate some real world phenomena better than general models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23089v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunhao Cai, Yiwu Shang</dc:creator>
    </item>
    <item>
      <title>USE-LFA: A Data-Driven Framework for Urban Air Mobility Site Evaluation using Latent Factor Analysis</title>
      <link>https://arxiv.org/abs/2503.23090</link>
      <description>arXiv:2503.23090v1 Announce Type: new 
Abstract: This study introduces the Urban Site Evaluation using Latent Factor Analysis (USE-LFA) framework, a data-driven methodology for selecting optimal urban port locations, demonstrated through a case study in Seoul. USE-LFA identifies six critical urban factors categorized into suitability (operational feasibility) and attractiveness (operational merits), enabling a holistic evaluation of potential sites. The framework employs latent factor analysis to uncover hidden patterns in urban data, integrating diverse attributes such as floating population, economic activity, and infrastructure connectivity. A composite metric, the v-score, balances suitability and attractiveness, offering flexibility to prioritize specific strategic goals. Spatial analysis reveals distinct regional typologies-balanced, suitability-biased, and attractiveness-biased regions-highlighting the adaptability of USE-LFA across varying urban contexts. Beyond its application to vertiports, this versatile framework provides valuable insights for integrating emerging mobility technologies and transit hubs into complex urban environments, supporting sustainable urban planning and informed policymaking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23090v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sungmin Sohn, Namwoo Kim, Mark Hansen, Yoonjin Yoon</dc:creator>
    </item>
    <item>
      <title>Scenario-Based Optimization of Network Resilience: Integrating Vulnerability Assessments and Traffic Flow</title>
      <link>https://arxiv.org/abs/2503.23251</link>
      <description>arXiv:2503.23251v1 Announce Type: new 
Abstract: Infrastructure networks are increasingly vulnerable to natural hazards and design flaws, making resilience assessment essential. This paper presents a scenario-based framework to evaluate network vulnerability by combining local measures and topological analysis, assessing each node's role in maintaining network integrity during disruptions. The framework identifies optimization opportunities by comparing structural properties with established standards. Traffic flow is modeled using the Bureau of Public Roads (BPR) function to improve disruption resilience. A two-stage stochastic model captures uncertainties, ensuring robust network performance across diverse scenarios. The approach balances risk-neutral and risk-averse strategies, emphasizing the importance of strengthening critical nodes to prevent cascading failures. The proposed method enhances resilience by minimizing undelivered demand and optimizing overall performance under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23251v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Saei, N. Tajik</dc:creator>
    </item>
    <item>
      <title>Rethinking Structural Equation Modeling in Political Science: Challenges, Best Practices, and Future Directions</title>
      <link>https://arxiv.org/abs/2503.23551</link>
      <description>arXiv:2503.23551v1 Announce Type: new 
Abstract: Structural Equation Modeling (SEM) or Covariance Structure Analysis (CSA) is a versatile and powerful method in the social and behavioral sciences, providing a framework for modeling complex relationships, testing mediation, accounting for measurement error, and analyzing latent constructs. However, SEM remains underutilized in in political science; its application is often marred by misunderstandings, misinterpretations, and methodological pitfalls that can compromise the validity and interpretability of findings. This article examines key challenges in SEM applications within political science, including test statistics and fit indices, model specification, estimator selection, and causal inference. It offers practical recommendations for enhancing methodological rigor and introduces recent advancements in causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23551v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bang Quan Zheng</dc:creator>
    </item>
    <item>
      <title>Bot Identification in Social Media</title>
      <link>https://arxiv.org/abs/2503.23629</link>
      <description>arXiv:2503.23629v1 Announce Type: new 
Abstract: Escalating proliferation of inorganic accounts, commonly known as bots, within the digital ecosystem represents an ongoing and multifaceted challenge to online security, trustworthiness, and user experience. These bots, often employed for the dissemination of malicious propaganda and manipulation of public opinion, wield significant influence in social media spheres with far-reaching implications for electoral processes, political campaigns and international conflicts. Swift and accurate identification of inorganic accounts is of paramount importance in mitigating their detrimental effects. This research paper focuses on the identification of such accounts and explores various effective methods for their detection through machine learning techniques. In response to the pervasive presence of bots in the contemporary digital landscape, this study extracts temporal and semantic features from tweet behaviors and proposes a bot detection algorithm utilizing fundamental machine learning approaches, including Support Vector Machines (SVM) and k-means clustering. Furthermore, the research ranks the importance of these extracted features for each detection technique and also provides uncertainty quantification using a distribution free method, called the conformal prediction, thereby contributing to the development of effective strategies for combating the prevalence of inorganic accounts in social media platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23629v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, William Boettcher, Rob Johnston, Soumendra Lahiri</dc:creator>
    </item>
    <item>
      <title>Seasonal bias-correction of daily precipitation over France using a stitch model designed for robust extremes representation</title>
      <link>https://arxiv.org/abs/2503.23773</link>
      <description>arXiv:2503.23773v1 Announce Type: new 
Abstract: Highly resoluted and accurate daily precipitation data are required for impact models to perform adequately and to correctly measure high-risk events' impact. In order to produce such data, bias-correction is often needed. Most of those statistical methods correct the probability distributions of daily precipitation by modeling them using either empirical or parametric distributions. A recent semi-parametric model based on a penalized Berk-Jones (BJ) statistical test which allows for an automatic and personalized splicing of parametric and nonparametric has been developed. This method, called Stitch-BJ model, was found to be able to model daily precipitation correctly and showed interesting potential in a bias-correction setting. In the present study, we will consolidate these results by taking into account the seasonal properties of daily precipitation in an out-of-sample context, and by considering dry days probabilities in our methodology. We evaluate the performance of the Stitch-BJ method in this seasonal bias-correction setting against more classical models such as the Gamma, Exponentiated Weibull (ExpW), Extended Generalized Pareto (EGP) or empirical distributions. The Stitch-BJ distribution was able to consistently perform as well or better than all the other models over the validation set, including the empirical distribution, which is often used due to its robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23773v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ear Philippe (LJAD), Di Bernardino Elena (LJAD), Lalo\"e Thomas (LJAD), Troin Magali, Lambert Adrien</dc:creator>
    </item>
    <item>
      <title>Joint Modeling of Multiple Longitudinal Biomarkers and Survival Outcomes via Threshold Regression: Variability as a Predictor</title>
      <link>https://arxiv.org/abs/2503.24146</link>
      <description>arXiv:2503.24146v1 Announce Type: new 
Abstract: Longitudinal biomarker data and health outcomes are routinely collected in many studies to assess how biomarker trajectories predict health outcomes. Existing methods primarily focus on mean biomarker profiles, treating variability as a nuisance. However, excess variability may indicate system dysregulations that may be associated with poor outcomes. In this paper, we address the long-standing problem of using variability information of multiple longitudinal biomarkers in time-to-event analyses by formulating and studying a Bayesian joint model. We first model multiple longitudinal biomarkers, some of which are subject to limit-of-detection censoring. We then model the survival times by incorporating random effects and variances from the longitudinal component as predictors through threshold regression that admits non-proportional hazards. We demonstrate the operating characteristics of the proposed joint model through simulations and apply it to data from the Study of Women's Health Across the Nation (SWAN) to investigate the impact of the mean and variability of follicle-stimulating hormone (FSH) and anti-Mullerian hormone (AMH) on age at the final menstrual period (FMP).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24146v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyan Yu, Zhenke Wu, Michelle M. Hood, Carrie A. Karvonen-Gutierrez, Sioban D. Harlow, Michael R. Elliott</dc:creator>
    </item>
    <item>
      <title>Johnson's contribution to the Discussion of `Statistical aspects of the Covid-19 response' by Wood et al</title>
      <link>https://arxiv.org/abs/2503.24308</link>
      <description>arXiv:2503.24308v1 Announce Type: new 
Abstract: This is a response to the paper "Some statistical aspects of the Covid-19 response" by Wood et al, submitted to the discussion at the read paper meeting of the Royal Statistical Society on 10th April 2025.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24308v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver Johnson</dc:creator>
    </item>
    <item>
      <title>Predicting and Mitigating Agricultural Price Volatility Using Climate Scenarios and Risk Models</title>
      <link>https://arxiv.org/abs/2503.24324</link>
      <description>arXiv:2503.24324v1 Announce Type: new 
Abstract: Agricultural price volatility challenges sustainable finance, planning, and policy, driven by market dynamics and meteorological factors such as temperature and precipitation. In India, the Minimum Support Price (MSP) system acts as implicit crop insurance, shielding farmers from price drops without premium payments. We analyze the impact of climate on price volatility for soybean (Madhya Pradesh), rice (Assam), and cotton (Gujarat). Using ERA5-Land reanalysis data from the Copernicus Climate Change Service, we analyze historical climate patterns and evaluate two scenarios: SSP2.4.5 (moderate case) and SSP5.8.5 (severe case). Our findings show that weather conditions strongly influence price fluctuations and that integrating meteorological data into volatility models enhances risk-hedging. Using the Exponential Generalized Autoregressive Conditional Heteroskedasticity (EGARCH) model, we estimate conditional price volatility and identify cross-correlations between weather and price volatility movements. Recognizing MSP's equivalence to a European put option, we apply the Black-Scholes model to estimate its implicit premium, quantifying its fiscal cost. We propose this novel market-based risk-hedging mechanism wherein the government purchases insurance equivalent to MSP, leveraging Black-Scholes for accurate premium estimation. Our results underscore the importance of meteorological data in agricultural risk modeling, supporting targeted insurance and strengthening resilience in agricultural finance. This climate-informed financial framework enhances risk-sharing, stabilizes prices, and informs sustainable agricultural policy under growing climate uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24324v1</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourish Das, Sudeep Shukla, Abbinav Sankar Kailasam, Anish Rai, Anirban Chakraborti</dc:creator>
    </item>
    <item>
      <title>From Occurrence to Consequence: A Comprehensive Data-driven Analysis of Building Fire Risk</title>
      <link>https://arxiv.org/abs/2503.22689</link>
      <description>arXiv:2503.22689v1 Announce Type: cross 
Abstract: Building fires pose a persistent threat to life, property, and infrastructure, emphasizing the need for advanced risk mitigation strategies. This study presents a data-driven framework analyzing U.S. fire risks by integrating over one million fire incident reports with diverse fire-relevant datasets, including social determinants, building inventories, weather conditions, and incident-specific factors. By adapting machine learning models, we identify key risk factors influencing fire occurrence and consequences. Our findings show that vulnerable communities, characterized by socioeconomic disparities or the prevalence of outdated or vacant buildings, face higher fire risks. Incident-specific factors, such as fire origins and safety features, strongly influence fire consequences. Buildings equipped with fire detectors and automatic extinguishing systems experience significantly lower fire spread and injury risks. By pinpointing high-risk areas and populations, this research supports targeted interventions, including mandating fire safety systems and providing subsidies for disadvantaged communities. These measures can enhance fire prevention, protect vulnerable groups, and promote safer, more equitable communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22689v1</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chenzhi Ma, Hongru Du, Shengzhi Luan, Ensheng Dong, Lauren M. Gardner, Thomas Gernay</dc:creator>
    </item>
    <item>
      <title>Association in Facial Phenotype, Gene, Disease: A Dataset for Explainable Rare Genetic Diseases Diagnosis</title>
      <link>https://arxiv.org/abs/2503.22716</link>
      <description>arXiv:2503.22716v1 Announce Type: cross 
Abstract: Many rare genetic diseases exhibit recognizable facial phenotypes, which are often used as diagnostic clues. However, current facial phenotype diagnostic models, which are trained on image datasets, have high accuracy but often suffer from an inability to explain their predictions, which reduces physicians' confidence in the model output.In this paper, we constructed a dataset, called FGDD, which was collected from 509 publications and contains 1147 data records, in which each data record represents a patient group and contains patient information, variation information, and facial phenotype information. To verify the availability of the dataset, we evaluated the performance of commonly used classification algorithms on the dataset and analyzed the explainability from global and local perspectives. FGDD aims to support the training of disease diagnostic models, provide explainable results, and increase physicians' confidence with solid evidence. It also allows us to explore the complex relationship between genes, diseases, and facial phenotypes, to gain a deeper understanding of the pathogenesis and clinical manifestations of rare genetic diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22716v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Song, Mengqiao He, Shumin Ren, Bairong Shen</dc:creator>
    </item>
    <item>
      <title>The "Days of Learning" Metric for Education Evaluations</title>
      <link>https://arxiv.org/abs/2503.22739</link>
      <description>arXiv:2503.22739v1 Announce Type: cross 
Abstract: The third National Charter School Study (NCSS III) aimed to test whether charter school were effective and to highlight outcomes on academic progress. The results showed that typical charter school students outperformed similar students in noncharter public schools by 6 days in mathematics and 16 days in reading. The key metric used to claim better performance in charter schools is "days of learning." In this report, the origin of the days-of-learning metric is explored with NAEP data, and the interpretation of days of learning as within-year growth is considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22739v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregory Camilli</dc:creator>
    </item>
    <item>
      <title>Combating the Bullwhip Effect in Rival Online Food Delivery Platforms Using Deep Learning</title>
      <link>https://arxiv.org/abs/2503.22753</link>
      <description>arXiv:2503.22753v1 Announce Type: cross 
Abstract: The wastage of perishable items has led to significant health and economic crises, increasing business uncertainty and fluctuating customer demand. This issue is worsened by online food delivery services, where frequent and unpredictable orders create inefficiencies in supply chain management, contributing to the bullwhip effect. This effect results in stockouts, excess inventory, and inefficiencies. Accurate demand forecasting helps stabilize inventory, optimize supplier orders, and reduce waste. This paper presents a Third-Party Logistics (3PL) supply chain model involving restaurants, online food apps, and customers, along with a deep learning-based demand forecasting model using a two-phase Long Short-Term Memory (LSTM) network.
  Phase one, intra-day forecasting, captures short-term variations, while phase two, daily forecasting, predicts overall demand. A two-year dataset from January 2023 to January 2025 from Swiggy and Zomato is used, employing discrete event simulation and grid search for optimal LSTM hyperparameters. The proposed method is evaluated using RMSE, MAE, and R-squared score, with R-squared as the primary accuracy measure. Phase one achieves an R-squared score of 0.69 for Zomato and 0.71 for Swiggy with a training time of 12 minutes, while phase two improves to 0.88 for Zomato and 0.90 for Swiggy with a training time of 8 minutes.
  To mitigate demand fluctuations, restaurant inventory is dynamically managed using the newsvendor model, adjusted based on forecasted demand. The proposed framework significantly reduces the bullwhip effect, improving forecasting accuracy and supply chain efficiency. For phase one, supply chain instability decreases from 2.61 to 0.96, and for phase two, from 2.19 to 0.80. This demonstrates the model's effectiveness in minimizing food waste and maintaining optimal restaurant inventory levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22753v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tisha Ghosh</dc:creator>
    </item>
    <item>
      <title>Density-valued time series: Nonparametric density-on-density regression</title>
      <link>https://arxiv.org/abs/2503.22904</link>
      <description>arXiv:2503.22904v1 Announce Type: cross 
Abstract: This paper is concerned with forecasting probability density functions. Density functions are nonnegative and have a constrained integral; they thus do not constitute a vector space. Implementing unconstrained functional time-series forecasting methods is problematic for such nonlinear and constrained data. A novel forecasting method is developed based on a nonparametric function-on-function regression, where both the response and the predictor are probability density functions. Through a series of Monte-Carlo simulation studies, we evaluate the finite-sample performance of our nonparametric regression estimator. Using French departmental COVID19 data and age-specific period life tables in the United States, we assess and compare finite-sample forecast accuracy between the proposed and several existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22904v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Ferraty, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Optimal Change Point Detection and Inference in the Spectral Density of General Time Series Models</title>
      <link>https://arxiv.org/abs/2503.23211</link>
      <description>arXiv:2503.23211v1 Announce Type: cross 
Abstract: This paper addresses the problem of detecting change points in the spectral density of time series, motivated by EEG analysis of seizure patients. Seizures disrupt coherence and functional connectivity, necessitating precise detection. Departing from traditional parametric approaches, we utilize the Wold decomposition, representing general time series as autoregressive processes with infinite lags, which are truncated and estimated around the change point. Our detection procedure employs an initial estimator that systematically searches across time points. We examine the localization error and its dependence on time series properties and sample size. To enhance accuracy, we introduce an optimal rate method with an asymptotic distribution, facilitating the construction of confidence intervals. The proposed method effectively identifies seizure onset in EEG data and extends to event detection in video data. Comprehensive numerical experiments demonstrate its superior performance compared to existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23211v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepideh Mosaferi, Abolfazl Safikhani, Peiliang Bai</dc:creator>
    </item>
    <item>
      <title>Modeling Maximum drawdown Records with Piecewise Deterministic Markov Processe in Capital Markets</title>
      <link>https://arxiv.org/abs/2503.23221</link>
      <description>arXiv:2503.23221v1 Announce Type: cross 
Abstract: We propose to model the records of the maximum Drawdown in capital markets by means a Piecewise Deterministic Markov Process (PDMP). We derive statistical results such as the mean and variance that describes the sequence of maximum Drawdown records. In addition, we developed a simulation study and techniques for estimating the parameters governing the stochastic process, using a practical example in the capital market to illustrate the procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23221v1</guid>
      <category>q-fin.RM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rolando Rubilar-Torrealba, Lisandro Fermin, Soledad Torres</dc:creator>
    </item>
    <item>
      <title>DGSAM: Domain Generalization via Individual Sharpness-Aware Minimization</title>
      <link>https://arxiv.org/abs/2503.23430</link>
      <description>arXiv:2503.23430v1 Announce Type: cross 
Abstract: Domain generalization (DG) aims to learn models that can generalize well to unseen domains by training only on a set of source domains. Sharpness-Aware Minimization (SAM) has been a popular approach for this, aiming to find flat minima in the total loss landscape. However, we show that minimizing the total loss sharpness does not guarantee sharpness across individual domains. In particular, SAM can converge to fake flat minima, where the total loss may exhibit flat minima, but sharp minima are present in individual domains. Moreover, the current perturbation update in gradient ascent steps is ineffective in directly updating the sharpness of individual domains. Motivated by these findings, we introduce a novel DG algorithm, Decreased-overhead Gradual Sharpness-Aware Minimization (DGSAM), that applies gradual domain-wise perturbation to reduce sharpness consistently across domains while maintaining computational efficiency. Our experiments demonstrate that DGSAM outperforms state-of-the-art DG methods, achieving improved robustness to domain shifts and better performance across various benchmarks, while reducing computational overhead compared to SAM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23430v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youngjun Song, Youngsik Hwang, Jonghun Lee, Heechang Lee, Dong-Young Lim</dc:creator>
    </item>
    <item>
      <title>A Kolmogorov-Zurbenko Fourier Transform Band-pass Filter Extension for Time Series Analysis</title>
      <link>https://arxiv.org/abs/2503.23493</link>
      <description>arXiv:2503.23493v1 Announce Type: cross 
Abstract: This research introduces a novel extension, called the Extended Kolmogorov-Zurbenko Fourier Transform (EKZFT), to an existing class of band-pass filters first introduced by Kolmogorov and Zurbenko. Their original Kolmogorov-Zurbenko Fourier Transform (KZFT) is a useful tool in time series and spatio-temporal analysis, Fourier analysis, and related statistical analysis fields. Example uses of these filters include separating frequencies, filtering portions of the spectra, reconstructing seasonality, investigating periodic signals, and reducing noise. KZFT filters have many practical applications across a wide range of disciplines including health, social, natural, and physical sciences. KZFT filters are band-pass filters defined by three arguments: the length of the filter window; the number of iterations; and the central frequency of the band-pass filter. However, the KZFT filter is limited in design to only positive odd integer widow lengths inherited from the time series. Therefore, for any combination of the other KZFT filter arguments, there is only a relatively small, discrete, selection of possible filter window lengths in a range determined by the size of the dataset. This limits the utility of KZFT filters for many of the stated uses. The proposed EKZFT filter allows a continuous selection of filter window length arguments over the same range, offering improved control, increased functionality, and wider practical use of this band-pass filter. An example application of the EKZFT in a data simulation is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23493v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Multi-Objective Optimization and Hyperparameter Tuning With Desirability Functions</title>
      <link>https://arxiv.org/abs/2503.23595</link>
      <description>arXiv:2503.23595v1 Announce Type: cross 
Abstract: The goal of this article is to provide an introduction to the desirability function approach to multi-objective optimization (direct and surrogate model-based), and multi-objective hyperparameter tuning. This work is based on the paper by Kuhn (2016). It presents a `Python` implementation of Kuhn's `R` package `desirability`. The `Python` package `spotdesirability` is available as part of the `sequential parameter optimization` framework. After a brief introduction to the desirability function approach is presented, three examples are given that demonstrate how to use the desirability functions for classical optimization, surrogate-model based optimization, and hyperparameter tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23595v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Thomas Bartz-Beielstein</dc:creator>
    </item>
    <item>
      <title>AutoML Algorithms for Online Generalized Additive Model Selection: Application to Electricity Demand Forecasting</title>
      <link>https://arxiv.org/abs/2503.24019</link>
      <description>arXiv:2503.24019v1 Announce Type: cross 
Abstract: Electricity demand forecasting is key to ensuring that supply meets demand lest the grid would blackout. Reliable short-term forecasts may be obtained by combining a Generalized Additive Models (GAM) with a State-Space model (Obst et al., 2021), leading to an adaptive (or online) model. A GAM is an over-parameterized linear model defined by a formula and a state-space model involves hyperparameters. Both the formula and adaptation parameters have to be fixed before model training and have a huge impact on the model's predictive performance. We propose optimizing them using the DRAGON package of Keisler (2025), originally designed for neural architecture search. This work generalizes it for automated online generalized additive model selection by defining an efficient modeling of the search space (namely, the space of the GAM formulae and adaptation parameters). Its application to short-term French electricity demand forecasting demonstrates the relevance of the approach</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24019v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keshav Das, Julie Keisler, Margaux Br\'eg\`ere, Amaury Durand</dc:creator>
    </item>
    <item>
      <title>Selective Inference in Graphical Models via Maximum Likelihood</title>
      <link>https://arxiv.org/abs/2503.24311</link>
      <description>arXiv:2503.24311v1 Announce Type: cross 
Abstract: The graphical lasso is a widely used algorithm for fitting undirected Gaussian graphical models. However, for inference on functionals of edge values in the learned graph, standard tools lack formal statistical guarantees, such as control of the type I error rate. In this paper, we introduce a selective inference method for asymptotically valid inference after graphical lasso selection with added randomization. We obtain a selective likelihood, conditional on the event of selection, through a change of variable on the known density of the randomization variables. Our method enables interval estimation and hypothesis testing for a wide range of functionals of edge values in the learned graph using the conditional maximum likelihood estimate. Our numerical studies show that introducing a small amount of randomization: (i) greatly increases power and yields substantially shorter intervals compared to other conditional inference methods, including data splitting; (ii) ensures intervals of bounded length in high-dimensional settings where data splitting is infeasible due to insufficient samples for inference; (iii) enables inference for a wide range of inferential targets in the learned graph, including measures of node influence and connectivity between nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24311v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sofia Guglielmini, Gerda Claeskens, Snigdha Panigrahi</dc:creator>
    </item>
    <item>
      <title>Correlation Improves Group Testing: Modeling Concentration-Dependent Test Errors</title>
      <link>https://arxiv.org/abs/2111.07517</link>
      <description>arXiv:2111.07517v5 Announce Type: replace 
Abstract: Population-wide screening is a powerful tool for controlling infectious diseases. Group testing enables such screening despite limited resources. Viral concentration of pooled samples are often positively correlated, either because prevalence and sample collection are influenced by location, or through intentional enhancement via pooling samples according to risk/household. Such correlation is known to improve efficiency under fixed test sensitivity. However, in reality, a test's sensitivity depends on the concentration of the analyte (e.g., viral RNA), as in the so-called dilution effect, where sensitivity decreases for larger pools. We show that concentration-dependent test error alters correlation's effect under the most widely-used group testing procedure, the two-stage Dorfman procedure. We prove that when test sensitivity increases with concentration, pooling correlated samples together (correlated pooling) achieves asymptotically higher sensitivity than independently pooling the samples (naive pooling). In contrast, in the concentration-independent case, correlation does not affect sensitivity. Moreover, with concentration-dependent errors, correlation can degrade test efficiency compared to naive pooling whereas under concentration-independent errors, correlation always improves efficiency. We propose an alternative measure of test resource usage, the number of positives found per test consumed, which we argue is better aligned with infection control, and show that correlated pooling outperforms naive pooling on this measure. In simulation, we show that the effect of correlation under realistic concentration-dependent test error meaningfully differs from correlation's effect assuming fixed sensitivity. Our findings underscore the importance for policy-makers of using models that incorporate naturally-occurring correlation and of considering ways of strengthening this correlation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.07517v5</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jiayue Wan, Yujia Zhang, Peter I. Frazier</dc:creator>
    </item>
    <item>
      <title>Estimated Roadway Segment Traffic Data by Vehicle Class for the United States: A Machine Learning Approach</title>
      <link>https://arxiv.org/abs/2502.05161</link>
      <description>arXiv:2502.05161v2 Announce Type: replace 
Abstract: The Highway Performance Monitoring System, managed by the Federal Highway Administration, provides essential data on average annual daily traffic across U.S. roadways, but it has limited representation of medium- and heavy-duty vehicles on non-interstate roads. This gap limits research and policy analysis on the impacts of truck traffic, especially concerning air quality and public health. To address this, we use random forest regression to estimate medium- and heavy-duty vehicle traffic volumes in areas with sparse data. This results in a more comprehensive dataset, which enables the estimation of traffic density at the census block level as a proxy for traffic-related air pollution exposure. Our high-resolution spatial data products, rigorously validated, provide a more accurate representation of truck traffic and its environmental and health impacts. These datasets are valuable for transportation planning, public health research, and policy decisions aimed at mitigating the effects of truck traffic on vulnerable communities exposed to air pollution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05161v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brittany Antonczak, Meg Fay, Aviral Chawla, Gregory Rowangould</dc:creator>
    </item>
    <item>
      <title>Rapid updating of multivariate resource models based on new information using EnKF-MDA and multi-Gaussian transformation</title>
      <link>https://arxiv.org/abs/2503.04694</link>
      <description>arXiv:2503.04694v2 Announce Type: replace 
Abstract: Rapid resource model updating with real-time data is important for making timely decisions in resource management and mining operations. This requires optimal merging of models and observations, which can be achieved through data assimilation, and the ensemble Kalman filter (EnKF) has become a popular method for this task. However, the modelled resources in mining usually consist of multiple variables of interest with multivariate relationships of varying complexity. EnKF is not a multivariate approach, and even for univariate cases, there may be slight deviations between its outcomes and observations. This study presents a methodology for rapidly updating multivariate resource models using the EnKF with multiple data assimilations (EnKF-MDA) combined with rotation based iterative Gaussianisation (RBIG). EnKF-MDA improves the updating by assimilating the same data multiple times with an inflated measurement error, while RBIG quickly transforms the data into multi-Gaussian factors. The application of the proposed algorithm is validated by a real case study with nine cross-correlated variables. The combination of EnKF-MDA and RBIG successfully improves the accuracy of resource model updates, minimises uncertainty, and preserves the multivariate relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04694v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sultan Abulkhair, Peter Dowd, Chaoshui Xu, Penny Stewart</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of restricted mean survival time adjusted for covariates using pseudo-observations</title>
      <link>https://arxiv.org/abs/2503.05225</link>
      <description>arXiv:2503.05225v3 Announce Type: replace 
Abstract: The difference in restricted mean survival time (RMST) is a clinically meaningful measure to quantify treatment effect in randomized controlled trials, especially when the proportional hazards assumption does not hold. Several frequentist methods exist to estimate RMST adjusted for covariates based on modeling and integrating the survival function. A more natural approach may be a regression model on RMST using pseudo-observations, which allows for a direct estimation without modeling the survival function. Only a few Bayesian methods exist, and each requires a model of the survival function. We developed a new Bayesian method that combines the use of pseudo-observations with the generalized method of moments. This offers RMST estimation adjusted for covariates without the need to model the survival function, making it more attractive than existing Bayesian methods. A simulation study was conducted with different time-dependent treatment effects (early, delayed, and crossing survival) and covariate effects, showing that our approach provides valid results, aligns with existing methods, and shows improved precision after covariate adjustment. For illustration, we applied our approach to a phase III trial in prostate cancer, providing estimates of the treatment effect on RMST, comparable to existing methods. In addition, our approach provided the effect of other covariates on RMST and determined the posterior probability of the difference in RMST exceeds any given time threshold for any covariate, allowing for nuanced and interpretable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05225v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'ea Orsini (CESP, U1018), Emmanuel Lesaffre (KU Leuven), Guosheng Yin (DSAS), Caroline Brard (U1018), David Dejardin (U1018), Gw\'ena\"el Le Teuff (U1018)</dc:creator>
    </item>
    <item>
      <title>Generalized Multilevel Functional Principal Component Analysis with Application to NHANES Active Inactive Patterns</title>
      <link>https://arxiv.org/abs/2311.14054</link>
      <description>arXiv:2311.14054v4 Announce Type: replace-cross 
Abstract: Between 2011 and 2014 NHANES collected objectively measured physical activity data using wrist-worn accelerometers for tens of thousands of individuals for up to seven days. In this study, we analyze minute-level indicators of being active, which can be viewed as binary (since each minute is either active or inactive), multilevel (because there are multiple days of data for each participant), and functional data (because the within-day measurements can be viewed as a function of time). To identify both within- and between-participant directions of variation in these data, we introduce Generalized Multilevel Functional Principal Component Analysis (GM-FPCA), an approach based on the dimension reduction of the linear predictor. Our results indicate that specific activity patterns captured by GM-FPCA are strongly associated with mortality risk. Extensive simulation studies demonstrate that GM-FPCA accurately estimates model parameters, is computationally stable, and scales up with the number of study participants, visits, and observations per visit. R code for implementing the method is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14054v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinkai Zhou, Julia Wrobel, Ciprian M. Crainiceanu, Andrew Leroux</dc:creator>
    </item>
    <item>
      <title>Quantification of Interdependent Emotion Dynamics in Online Interactions</title>
      <link>https://arxiv.org/abs/2408.05700</link>
      <description>arXiv:2408.05700v2 Announce Type: replace-cross 
Abstract: A growing share of human interactions now occurs online, where the expression and perception of emotions are often amplified and distorted. Yet, the interplay between different emotions and the extent to which they are driven by external stimuli or social feedback remains poorly understood. We calibrate a multivariate Hawkes self-exciting point process to model the temporal expression of six basic emotions in YouTube Live chats. This framework captures both temporal and cross-emotional dependencies while allowing us to disentangle the influence of video content (exogenous) from peer interactions (endogenous). We find that emotional expressions are up to four times more strongly driven by peer interaction than by video content. Positivity is more contagious, spreading three times more readily, whereas negativity is more memorable, lingering nearly twice as long. Moreover, we observe asymmetric cross-excitation, with negative emotions frequently triggering positive ones, a pattern consistent with trolling dynamics, but not the reverse. These findings highlight the central role of social interaction in shaping emotional dynamics online and the risks of emotional manipulation as human-chatbot interactions become increasingly realistic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05700v2</guid>
      <category>cs.SI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yishan Luo, Didier Sornette, Sandro Claudio Lera</dc:creator>
    </item>
    <item>
      <title>Highest Posterior Density Intervals As Analogues to Profile Likelihood Ratio Confidence Intervals for Modes of Unimodal Distributions</title>
      <link>https://arxiv.org/abs/2412.06528</link>
      <description>arXiv:2412.06528v3 Announce Type: replace-cross 
Abstract: In Bayesian statistics, the highest posterior density (HPD) interval is often used to describe properties of a posterior distribution. As a method for estimating confidence intervals (CIs), the HPD has two main desirable properties. Firstly, it is the shortest interval to have a specified coverage probability. Secondly, every point inside the HPD interval has a density greater than every point outside the interval. However, it is sometimes criticized for being transformation invariant.
  We make the case that the HPD interval is a natural analog to the frequentist profile likelihood ratio confidence interval (LRCI). First we provide background on the HPD interval as well as the Likelihood Ratio Test statistic and its inversion to generate asymptotically-correct CIs. Our main result is to show that the HPD interval has similar desirable properties as the profile LRCI, such as transformation invariance with respect to the mode for monotonic functions. We then discuss an application of the main result, an example case which compares the profile LRCI for the binomial probability parameter p with the Bayesian HPD interval for the beta distribution density function, both of which are used to estimate population proportions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06528v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. X. Venu</dc:creator>
    </item>
  </channel>
</rss>
