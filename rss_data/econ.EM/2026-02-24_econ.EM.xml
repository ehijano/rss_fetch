<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Feb 2026 05:00:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Roof Over Risk: A House Price-at-Risk Framework for Hungary</title>
      <link>https://arxiv.org/abs/2602.18592</link>
      <description>arXiv:2602.18592v1 Announce Type: new 
Abstract: This paper develops a House Price-at-Risk framework to examine how housing subsidies, credit conditions, and supply factors influence the distribution of house price growth in Hungary. Using quantile regression with adaptive LASSO variable selection, we identify variables driving downside versus upside risks across multiple horizons. Financial stress dominates the lower tail at short horizons, while unemployment and affordability constraints become the primary drivers of downside risk at longer horizons. Housing subsidies exhibit pro-cyclical characteristics, concentrating significant positive effects on the upper quantiles while leaving the lower tail largely unaffected. Supply-side variables display horizon-dependent sign reversals, with construction permits exerting upward pressure on prices in the short run but moderating them as supply materialises. Uncertainty decomposition reveals persistent left-tail dominance across all horizons. These findings suggest that macroprudential frameworks should account for the distributional effects of housing subsidies, particularly their pro-cyclical influence on house price growth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18592v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tibor Szendrei, Nikolett V\'ag\'o, Katalin Varga</dc:creator>
    </item>
    <item>
      <title>Demand estimation without outside good shares</title>
      <link>https://arxiv.org/abs/2602.19154</link>
      <description>arXiv:2602.19154v1 Announce Type: new 
Abstract: The BLP model is the workhorse framework in empirical IO and enables estimation of demand models for differentiated products using aggregate product shares. In practice, however, the share of the outside good is often unobserved. This paper studies identification and inference in the BLP model when the share of the outside good is unobserved. We show that the model is partially identified, and we derive sharp identified sets for structural parameters and equilibrium objects. We also develop inference procedures based on moment inequalities that deliver valid confidence sets for these structural parameters and equilibrium objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19154v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico A. Bugni, Joel L. Horowitz, Linqi Zhang</dc:creator>
    </item>
    <item>
      <title>Panel Quantile Regression with Common Shocks</title>
      <link>https://arxiv.org/abs/2602.19201</link>
      <description>arXiv:2602.19201v1 Announce Type: new 
Abstract: This paper develops an asymptotic and inferential theory for fixed-effects panel quantile regression (FEQR) that delivers inference robust to pervasive common shocks. Such shocks induce cross-sectional dependence that is central in many economic and financial panels but largely ignored in existing FEQR theory, which typically assumes cross-sectional independence and requires $T \gg N$. We show that the standard FEQR estimator remains asymptotically normal under the mild condition $(\log N)^2/T \to 0$, thereby accommodating empirically relevant regimes, including those with $T \ll N$. We further show that common shocks fundamentally alter the asymptotic covariance structure, rendering conventional covariance estimators inconsistent, and we propose a simple covariance estimator that remains consistent both in the presence and absence of common shocks. The proposed procedure therefore provides valid robust inference without requiring prior knowledge of the dependence structure, substantially expanding the applicability of FEQR methods in realistic panel data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19201v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harold D. Chiang, Antonio F. Galvao, Chia-Min Wei</dc:creator>
    </item>
    <item>
      <title>Distributional Effects in Censored Quantile Regressions with Endogeneity and Heteroskedasticity</title>
      <link>https://arxiv.org/abs/2602.19279</link>
      <description>arXiv:2602.19279v1 Announce Type: new 
Abstract: Distributional effects, characterized by quantile frameworks, are well-known to capture heterogeneous impacts of economic factors across the unobserved relative ranks. Censored outcome, endogenous regressor and heteroskedastic error are prevalent in empirical work, yet challenge the consistency of existing quantile estimation methods. This paper develops a Sequential Control Function Censored Quantile (SCFCQ) estimator for distributional effects in censored quantile models with unbounded endogenous regressors. Our method combines the sequential analysis with the control function approach, particularly adapting for conditional heteroskedasticity in the endogenous regressor. The estimation algorithm is a two-step procedure composed of series quantile regressions, thereby providing applied researchers with a computationally tractable and practically feasible tool. We apply the SCFCQ method to estimate heterogeneous income elasticities over household preferences using data from the UK Family Expenditure Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19279v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Wang</dc:creator>
    </item>
    <item>
      <title>Asymptotic theory of range-based multipower variation</title>
      <link>https://arxiv.org/abs/2602.19287</link>
      <description>arXiv:2602.19287v1 Announce Type: new 
Abstract: In this paper, we present a realized range-based multipower variation theory, which can be used to estimate return variation and draw jump-robust inference about the diffusive volatility component, when a high-frequency record of asset prices is available. The standard range-statistic -- routinely used in financial economics to estimate the variance of securities prices -- is shown to be biased when the price process contains jumps. We outline how the new theory can be applied to remove this bias by constructing a hybrid range-based estimator. Our asymptotic theory also reveals that when high-frequency data are sparsely sampled, as is often done in practice due to the presence of microstructure noise, the range-based multipower variations can produce significant efficiency gains over comparable subsampled return-based estimators. The analysis is supported by a simulation study and we illustrate the practical use of our framework on some recent TAQ equity data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19287v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/jjfinec/nbr019</arxiv:DOI>
      <dc:creator>Kim Christensen, Mark Podolskij</dc:creator>
    </item>
    <item>
      <title>How Robust are Robustness Checks?</title>
      <link>https://arxiv.org/abs/2602.19384</link>
      <description>arXiv:2602.19384v1 Announce Type: new 
Abstract: Robustness checks are routine in empirical work, but there is no standard statistical procedure to formally measure what one can learn from them. I propose a "robustness radius" measure to quantify the amount by which the robustness checks estimands differ from the main specification estimand. I do so by framing robustness checks as explicitly biased regressions, clarifying what exactly the estimands are when comparing multiple regressions with slightly different samples, and applying a test from the moment inequalities literature. The robustness radius is easily interpretable and adapts to sampling uncertainty and correlation across regressions. An application shows that, although assessing overall robustness is context-specific, the robustness radius guides those judgments and improves transparency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19384v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brenda Prallon</dc:creator>
    </item>
    <item>
      <title>Pre-averaging estimators of the ex-post covariance matrix in noisy diffusion models with non-synchronous data</title>
      <link>https://arxiv.org/abs/2602.19645</link>
      <description>arXiv:2602.19645v1 Announce Type: new 
Abstract: We show how pre-averaging can be applied to the problem of measuring the ex-post covariance of financial asset returns under microstructure noise and non-synchronous trading. A pre-averaged realised covariance is proposed, and we present an asymptotic theory for this new estimator, which can be configured to possess an optimal convergence rate or to ensure positive semi-definite covariance matrix estimates. We also derive a noise-robust Hayashi-Yoshida estimator that can be implemented on the original data without prior alignment of prices. We uncover the finite sample properties of our estimators with simulations and illustrate their practical use on high-frequency equity data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19645v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jeconom.2010.05.001</arxiv:DOI>
      <dc:creator>Kim Christensen, Silja Kinnebrock, Mark Podolskij</dc:creator>
    </item>
    <item>
      <title>On covariation estimation for multivariate continuous It\^o semimartingales with noise in non-synchronous observation schemes</title>
      <link>https://arxiv.org/abs/2602.19658</link>
      <description>arXiv:2602.19658v1 Announce Type: new 
Abstract: This paper presents a Hayashi-Yoshida type estimator for the covariation matrix of continuous It\^o semimartingales observed with noise. The coordinates of the multivariate process are assumed to be observed at highly frequent non-synchronous points. The estimator of the covariation matrix is designed via a certain combination of the local averages and the Hayashi-Yoshida estimator. Our method does not require any synchronization of the observation scheme (as e.g. previous tick method or refreshing time method) and it is robust to some dependence structure of the noise process. We show the associated central limit theorem for the proposed estimator and provide a feasible asymptotic result. Our proofs are based on a blocking technique and a stable convergence theorem for semimartingales. Finally, we show simulation results for the proposed estimator to illustrate its finite sample properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19658v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jmva.2013.05.002</arxiv:DOI>
      <dc:creator>Kim Christensen, Mark Podolskij, Mathias Vetter</dc:creator>
    </item>
    <item>
      <title>Testing Effect Homogeneity and Confounding in High-Dimensional Experimental and Observational Studies</title>
      <link>https://arxiv.org/abs/2602.19703</link>
      <description>arXiv:2602.19703v1 Announce Type: new 
Abstract: We propose a framework for testing the homogeneity of conditional average treatment effects (CATEs) across multiple experimental and observational studies. Our approach leverages multiple randomized trials to assess whether treatment effects vary with unobserved heterogeneity that differs across trials: if CATEs are homogeneous, this indicates the absence of interactions between treatment and unobservables in the mean effect. Comparing CATEs between experimental and observational data further allows evaluation of potential confounding: if the estimands coincide, there is no unobserved confounding; if they differ, deviations may arise from unobserved confounding, effect heterogeneity, or both. We extend the framework to settings with alternative identification strategies, namely instrumental variable settings and panel data with parallel trends assumptions based on differences in differences, where effects are identified only locally for subpopulations such as compliers or treated units. In these contexts, testing homogeneity is useful for assessing whether local effects can be extrapolated to the total population. We suggest a test based on double machine learning that accommodates high-dimensional covariates in a data-driven way and investigate its finite-sample performance through a simulation study. Finally, we apply the test to the International Stroke Trial (IST), a large multi-country randomized controlled trial in patients with acute ischaemic stroke that evaluated whether early treatment with aspirin altered subsequent clinical outcomes. Our methodology provides a flexible tool for both validating identification assumptions and understanding the generalizability of estimated treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19703v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ana Armendariz, Martin Huber</dc:creator>
    </item>
    <item>
      <title>Model Selection in High-Dimensional Linear Regression using Boosting with Multiple Testing</title>
      <link>https://arxiv.org/abs/2602.19705</link>
      <description>arXiv:2602.19705v1 Announce Type: new 
Abstract: High-dimensional regression specification and analysis is a complex and active area of research in statistics, machine learning, and econometrics. This paper proposes a new approach, Boosting with Multiple Testing (BMT), which combines forward stepwise variable selection with the multiple testing framework of Chudik et al (2018). At each stage, the model is updated by adding only the most significant regressor conditional on those already included, while a family-wise multiple testing filter is applied to the remaining candidates. In this way, the method retains the strong screening properties of Chudik et al (2018) while operating in a less greedy manner with respect to proxy and noise variables. Using sharp probability inequalities for heterogeneous strongly mixing processes from Dendramis et al (2022), we show that BMT enjoys oracle type properties relative to an approximating model that includes all true signals and excludes pure noise variables: this model is selected with probability tending to one, and the resulting estimator achieves standard parametric rates for prediction error and coefficient estimation. Additional results establish conditions under which BMT recovers the exact true model and avoids selection of proxy signals. Monte Carlo experiments indicate that BMT performs very well relative to OCMT and Lasso type procedures, delivering higher model selection accuracy and smaller RMSE for the estimated coefficients, especially under strong multicollinearity of the regressors. Two empirical illustrations based on a large set of macro-financial indicators as covariates, show that BMT yields sparse, interpretable specifications with favourable out-of-sample performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19705v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Kapetanios, Vasilis Sarafidis, Alexia Ventouri</dc:creator>
    </item>
    <item>
      <title>Volatility Spillovers in China's Real Estate Crisis: A Network Approach</title>
      <link>https://arxiv.org/abs/2602.19740</link>
      <description>arXiv:2602.19740v1 Announce Type: new 
Abstract: Sentiment towards the Chinese real estate sector has deteriorated following the introduction of financing constraints in 2020 with the ''three red lines." Forcing developers to restructure their debt, the policy triggered a cascade of financing troubles, defaults, and reduced housing demand, ultimately culminating in a prolonged real estate crisis. This paper utilizes a network approach in line with Demirer et al. (2018) and Diebold and Yilmaz (2014) to measure daily time-varying connectedness in the stock return volatilities of major Chinese real estate developers throughout the crisis. Focusing on spillover between companies as reflected by market perception, this paper examines how connectedness evolves over time across firms with different regional exposures and state-ownership statuses, filling a gap in the literature to elucidate where property demand and real estate firm trustworthiness have deteriorated most. An event-study analysis of four key moments of the crisis outlines distinct phases of market sentiment: with the introduction of the three red lines, connectedness primarily reflects shared exposure and a uniform shock to the market. Then, the early unrest surrounding Evergrande exposes strong regional differentiation, with firms concentrated in less developed regions receiving significant spillover. By one year into the crisis, previously stable regions receive higher levels of spillover, and there is evidence of a substitution effect towards private developers. Two years into the crisis, the market has much less homogeneity in effects across regions and state-ownership status: major shocks induce minimal network changes, reflecting how investors have already priced in their beliefs. This paper also offers one of the most extensive timelines of the Chinese real estate crisis to date, and a new R package, GephiForR, was created for the network visualization in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19740v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Julia Manso</dc:creator>
    </item>
    <item>
      <title>Using Invalid Instruments on Purpose: Focused Moment Selection and Averaging for GMM</title>
      <link>https://arxiv.org/abs/1408.0705</link>
      <description>arXiv:1408.0705v3 Announce Type: cross 
Abstract: In finite samples, the use of a slightly endogenous but highly relevant instrument can reduce mean-squared error (MSE). Building on this observation, I propose a novel moment selection procedure for GMM -- the Focused Moment Selection Criterion (FMSC) -- in which moment conditions are chosen not based on their validity but on the MSE of their associated estimator of a user-specified target parameter. The FMSC mimics the situation faced by an applied researcher who begins with a set of relatively mild "baseline" assumptions and must decide whether to impose any of a collection of stronger but more controversial "suspect" assumptions. When the (correctly specified) baseline moment conditions identify the model, the FMSC provides an asymptotically unbiased estimator of asymptotic MSE, allowing us to select over the suspect moment conditions. I go on to show how the framework used to derive the FMSC can address the problem of inference post-moment selection. Treating post-selection estimators as a special case of moment-averaging, in which estimators based on different moment sets are given data-dependent weights, I propose simulation-based procedures for inference that can be applied to a variety of formal and informal moment-selection and averaging procedures. Both the FMSC and confidence interval procedures perform well in simulations. I conclude with an empirical example examining the effect of instrument selection on the estimated relationship between malaria and income per capita.</description>
      <guid isPermaLink="false">oai:arXiv.org:1408.0705v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jeconom.2016.07.006</arxiv:DOI>
      <arxiv:journal_reference>Journal of Econometrics, Volume 195, Issue 2, December 2016, Pages 187-208</arxiv:journal_reference>
      <dc:creator>Francis J. DiTraglia</dc:creator>
    </item>
    <item>
      <title>Distributional Discontinuity Design</title>
      <link>https://arxiv.org/abs/2602.19290</link>
      <description>arXiv:2602.19290v1 Announce Type: cross 
Abstract: Regression discontinuity and kink designs are typically analyzed through mean effects, even when treatment changes the shape of the entire outcome distribution. To address this, we introduce distributional discontinuity designs, a framework for estimating causal effects for a scalar outcome at the boundary of a discontinuity in treatment assignment. Our estimand is the Wasserstein distance between limiting conditional outcome distributions; a single scale-interpretable measure of distribution shift. We show that this weakly bounds the average treatment effect, where equality holds if and only if the treatment effect is purely additive; thus, departure from equality measures effect heterogeneity. To further encode effect heterogeneity we show that the Wasserstein distance admits an orthogonal decomposition into squared differences in $L$-moments, thereby quantifying the contribution from location, scale, skewness, and higher-order shape components to the overall distributional distance. Next, we extend this framework to distributional kink designs by evaluating the Wasserstein derivative at a policy kink; this describes the flow of probability mass through the kink. In the case of fuzzy kink designs, we derive new identification results. Finally, we apply our methods on real data by re-analyzing two natural experiments to compare our distributional effects to traditional causal estimands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19290v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Schindl, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Compound decisions and empirical Bayes via Bayesian nonparametrics</title>
      <link>https://arxiv.org/abs/2602.20115</link>
      <description>arXiv:2602.20115v1 Announce Type: cross 
Abstract: We study the Gaussian sequence compound decision problem and analyze a Bayesian nonparametric estimator from an empirical Bayes, regret-based perspective. Motivated by sharp results for the classical nonparametric maximum likelihood estimator (NPMLE), we ask whether an analogous guarantee can be obtained using a standard Bayesian nonparametric prior. We show that a Dirichlet-process-based Bayesian procedure achieves near-optimal regret bounds. Our main results are stated in the compound decision framework, where the mean vector is treated as fixed, while we also provide parallel guarantees under a hierarchical model in which the means are drawn from a true unknown prior distribution. The posterior mean Bayes rule is, a fortiori, admissible, whereas we show that the NPMLE plug-in rule is inadmissible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20115v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolaos Ignatiadis, Sid Kankanala</dc:creator>
    </item>
    <item>
      <title>Dynamic covariate balancing: estimating treatment effects over time with potential local projections</title>
      <link>https://arxiv.org/abs/2103.01280</link>
      <description>arXiv:2103.01280v5 Announce Type: replace 
Abstract: This paper studies the estimation and inference of treatment effects in panel data settings when treatments change dynamically over time.
  We propose a balancing method that allows for (i) treatments to be assigned dynamically over time based on high-dimensional covariates, past outcomes, and treatments; (ii) outcomes and time-varying covariates to depend on the trajectory of all past treatments; (iii) heterogeneity of treatment effects.
  Our approach recursively projects potential outcomes' expectations on past histories. It then controls the bias arising from the non-experimental and sequential nature of this setting by balancing dynamically observable characteristics over time. We establish inferential guarantees of the proposed method even when the number of observable characteristics significantly exceeds the sample size. We study numerical properties of the estimator and illustrate the benefits of the procedure in an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.01280v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Davide Viviano, Jelena Bradic</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation in a Time-Varying Endogenous Random Coefficient Panel Data Model</title>
      <link>https://arxiv.org/abs/2110.00982</link>
      <description>arXiv:2110.00982v3 Announce Type: replace 
Abstract: This paper proposes a correlated random coefficient linear panel data model, where regressors can be correlated with time-varying and individual-specific random coefficients through both a fixed effect and a time-varying random shock. I develop a new panel data-based method to identify the average partial effect and the local average response function. The identification strategy employs a sufficient statistic to control for the fixed effect and a control variable for the random shock. Conditional on these two controls, the residual variation in the regressors is driven solely by the exogenous instrumental variables, and thus can be exploited to identify the parameters of interest. The constructive identification analysis leads to three-step series estimators, for which I establish rates of convergence and asymptotic normality. To illustrate the method, I estimate a heterogeneous Cobb-Douglas production function for manufacturing firms in China, finding substantial variations in output elasticities across firms that can be related to various firm characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.00982v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Li</dc:creator>
    </item>
    <item>
      <title>Statistical Inference of Optimal Allocations I: Regularities and their Implications</title>
      <link>https://arxiv.org/abs/2403.18248</link>
      <description>arXiv:2403.18248v5 Announce Type: replace 
Abstract: In this paper, we develop a functional differentiability approach for solving statistical optimal allocation problems. We derive Hadamard differentiability of the value functions through analyzing the properties of the sorting operator using tools from geometric measure theory. Building on our Hadamard differentiability results, we apply the functional delta method to obtain the asymptotic properties of the value function process for the binary constrained optimal allocation problem and the plug-in ROC curve estimator. Moreover, the convexity of the optimal allocation value functions facilitates demonstrating the degeneracy of first order derivatives with respect to the policy. We then present a double / debiased estimator for the value functions. Importantly, the conditions that validate Hadamard differentiability justify the margin assumption from the statistical classification literature for the fast convergence rate of plug-in methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18248v5</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Feng, Han Hong, Denis Nekipelov</dc:creator>
    </item>
    <item>
      <title>Prediction Sets and Conformal Inference with Interval Outcomes</title>
      <link>https://arxiv.org/abs/2501.10117</link>
      <description>arXiv:2501.10117v4 Announce Type: replace 
Abstract: Given data on a random variable \(Y\), a prediction set with miscoverage level \(\alpha \in (0,1)\) is a set that contains a new draw of \(Y\) with probability \(1-\alpha\). Among all prediction sets satisfying this coverage property, the oracle prediction set is the one with minimal volume. The oracle prediction set offers a complementary view of the distribution of \(Y\), beyond point estimators such as the mean and quantiles, and has attracted considerable interest recently. This paper develops methods for estimating such prediction sets conditional on observed covariates when \(Y\) is \textit{censored} or \textit{interval-valued}. We characterise the oracle prediction set under partial identification induced by interval censoring and propose consistent estimators for both oracle prediction intervals and more general oracle prediction sets consisting of multiple disjoint intervals. In addition, we apply conformal inference to construct finite-sample valid prediction sets for interval outcomes that remain consistent as the sample size grows, using a conformity score tailored to interval data. The proposed procedure accounts for irreducible prediction uncertainty due to the stochastic nature of outcomes, modelling uncertainty arising from partial identification, and sampling uncertainty that vanishes as sample size increases. We conduct Monte Carlo simulations and two empirical applications using UK job postings data and the US Current Population Survey. The results demonstrate the robustness and efficiency of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10117v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weiguang Liu, \'Aureo de Paula, Elie Tamer</dc:creator>
    </item>
    <item>
      <title>On the Wisdom of Crowds (of Economists)</title>
      <link>https://arxiv.org/abs/2503.09287</link>
      <description>arXiv:2503.09287v3 Announce Type: replace 
Abstract: We study the properties of macroeconomic survey forecast response averages as the number of survey respondents grows. Such averages are ``portfolios" of forecasts. We characterize the speed and pattern of the gains from diversification as a function of portfolio size (the number of survey respondents) in both (1) the key real-world data-based environment of the U.S. Survey of Professional Forecasters, and (2) the theoretical model-based environment of equicorrelated forecast errors. We proceed by proposing and comparing various direct and model-based ``crowd size signature plots", which summarize the forecasting performance of $k$-average forecasts as a function of $k$, where $k$ is the number of forecasts in the average. We then estimate the equicorrelation model for growth and inflation forecast errors by choosing model parameters to minimize the divergence between direct and model-based signature plots. The results indicate near-perfect equicorrelation model fit for both growth and inflation, which we explicate by showing analytically that, under very weak conditions, the direct and fitted equicorrelation model-based signature plots are identical at a particular model parameter configuration. That parameter configuration immediately suggests an analytic closed-form estimator for the direct signature plot, so that equicorrelation ultimately emerges as a device for convenient calculation of direct signature plots, rather than a separate ``model" producing separate signature plots. In any event we find that the gains from survey diversification are greater for inflation forecasts than for growth forecasts, and that they are largely exhausted with inclusion of 5-10 representative forecasters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09287v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Francis X. Diebold, Aaron Mora, Minchul Shin</dc:creator>
    </item>
    <item>
      <title>Robust Time Series Causal Discovery for Agent-Based Model Validation</title>
      <link>https://arxiv.org/abs/2410.19412</link>
      <description>arXiv:2410.19412v3 Announce Type: replace-cross 
Abstract: Agent-Based Model (ABM) validation is crucial as it helps ensuring the reliability of simulations, and causal discovery has become a powerful tool in this context. However, current causal discovery methods often face accuracy and robustness challenges when applied to complex and noisy time series data, which is typical in ABM scenarios. This study addresses these issues by proposing a Robust Cross-Validation (RCV) approach to enhance causal structure learning for ABM validation. We develop RCV-VarLiNGAM and RCV-PCMCI, novel extensions of two prominent causal discovery algorithms. These aim to reduce the impact of noise better and give more reliable causal relation results, even with high-dimensional, time-dependent data. The proposed approach is then integrated into an enhanced ABM validation framework, which is designed to handle diverse data and model structures.
  The approach is evaluated using synthetic datasets and a complex simulated fMRI dataset. The results demonstrate greater reliability in causal structure identification. The study examines how various characteristics of datasets affect the performance of established causal discovery methods. These characteristics include linearity, noise distribution, stationarity, and causal structure density. This analysis is then extended to the RCV method to see how it compares in these different situations. This examination helps confirm whether the results are consistent with existing literature and also reveals the strengths and weaknesses of the novel approaches.
  By tackling key methodological challenges, the study aims to enhance ABM validation with a more resilient valuation framework presented. These improvements increase the reliability of model-driven decision making processes in complex systems analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19412v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gene Yu, Ce Guo, Wayne Luk</dc:creator>
    </item>
  </channel>
</rss>
