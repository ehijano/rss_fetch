<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Nov 2024 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A computational framework for integrating Predictive processes with evidence Accumulation Models (PAM)</title>
      <link>https://arxiv.org/abs/2411.13203</link>
      <description>arXiv:2411.13203v1 Announce Type: new 
Abstract: Evidence Accumulation Models (EAMs) have been widely used to investigate speeded decision-making processes, but they have largely neglected the role of predictive processes emphasized by theories of the predictive brain. In this paper, we present the Predictive evidence Accumulation Models (PAM), a novel computational framework that integrates predictive processes into EAMs. Grounded in the "observing the observer" framework, PAM combines models of Bayesian perceptual inference, such as the Hierarchical Gaussian Filter, with three established EAMs (the Diffusion Decision Model, Lognormal Race Model, and Race Diffusion Model) to model decision-making under uncertainty. We validate PAM through parameter recovery simulations, demonstrating its accuracy and computational efficiency across various decision-making scenarios. Additionally, we provide a step-by-step tutorial using real data to illustrate PAM's application and discuss its theoretical implications. PAM represents a significant advancement in the computational modeling of decision-making, bridging the gap between predictive brain theories and EAMs, and offers a promising tool for future empirical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13203v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonino Visalli, Francesco Maria Calistroni, Margherita Calderan, Francesco Donnarumma, Marco Zorzi, Ettore Ambrosini</dc:creator>
    </item>
    <item>
      <title>Optimal Designs for Spherical Harmonic Regression</title>
      <link>https://arxiv.org/abs/2411.13356</link>
      <description>arXiv:2411.13356v1 Announce Type: new 
Abstract: This short paper is concerned with the use of spherical t-designs as optimal designs for the spherical harmonic regression model in three dimensions over a range of specified criteria. The nature of the designs is explored and their availability and suitability is reviewed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13356v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linda M. Haines</dc:creator>
    </item>
    <item>
      <title>Analysis of Higher Education Dropouts Dynamics through Multilevel Functional Decomposition of Recurrent Events in Counting Processes</title>
      <link>https://arxiv.org/abs/2411.13370</link>
      <description>arXiv:2411.13370v1 Announce Type: new 
Abstract: This paper analyzes the dynamics of higher education dropouts through an innovative approach that integrates recurrent events modeling and point process theory with functional data analysis. We propose a novel methodology that extends existing frameworks to accommodate hierarchical data structures, demonstrating its potential through a simulation study. Using administrative data from student careers at Politecnico di Milano, we explore dropout patterns during the first year across different bachelor's degree programs and schools. Specifically, we employ Cox-based recurrent event models, treating dropouts as repeated occurrences within both programs and schools. Additionally, we apply functional modeling of recurrent events and multilevel principal component analysis to disentangle latent effects associated with degree programs and schools, identifying critical periods of dropout risk and providing valuable insights for institutions seeking to implement strategies aimed at reducing dropout rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13370v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandra Ragni, Chiara Masci, Anna Maria Paganoni</dc:creator>
    </item>
    <item>
      <title>Underlying Core Inflation with Multiple Regimes</title>
      <link>https://arxiv.org/abs/2411.12845</link>
      <description>arXiv:2411.12845v1 Announce Type: cross 
Abstract: This paper introduces a new approach for estimating core inflation indicators based on common factors across a broad range of price indices. Specifically, by utilizing procedures for detecting multiple regimes in high-dimensional factor models, we propose two types of core inflation indicators: one incorporating multiple structural breaks and another based on Markov switching. The structural breaks approach can eliminate revisions for past regimes, though it functions as an offline indicator, as real-time detection of breaks is not feasible with this method. On the other hand, the Markov switching approach can reduce revisions while being useful in real time, making it a simple and robust core inflation indicator suitable for real-time monitoring and as a short-term guide for monetary policy. Additionally, this approach allows us to estimate the probability of being in different inflationary regimes. To demonstrate the effectiveness of these indicators, we apply them to Canadian price data. To compare the real-time performance of the Markov switching approach to the benchmark model without regime-switching, we assess their abilities to forecast headline inflation and minimize revisions. We find that the Markov switching model delivers superior predictive accuracy and significantly reduces revisions during periods of substantial inflation changes. Hence, our findings suggest that accounting for time-varying factors and parameters enhances inflation signal accuracy and reduces data requirements, especially following sudden economic shifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12845v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Rodriguez-Rondon</dc:creator>
    </item>
    <item>
      <title>Integration of Active Learning and MCMC Sampling for Efficient Bayesian Calibration of Mechanical Properties</title>
      <link>https://arxiv.org/abs/2411.13361</link>
      <description>arXiv:2411.13361v1 Announce Type: cross 
Abstract: Recent advancements in Markov chain Monte Carlo (MCMC) sampling and surrogate modelling have significantly enhanced the feasibility of Bayesian analysis across engineering fields. However, the selection and integration of surrogate models and cutting-edge MCMC algorithms, often depend on ad-hoc decisions. A systematic assessment of their combined influence on analytical accuracy and efficiency is notably lacking. The present work offers a comprehensive comparative study, employing a scalable case study in computational mechanics focused on the inference of spatially varying material parameters, that sheds light on the impact of methodological choices for surrogate modelling and sampling. We show that a priori training of the surrogate model introduces large errors in the posterior estimation even in low to moderate dimensions. We introduce a simple active learning strategy based on the path of the MCMC algorithm that is superior to all a priori trained models, and determine its training data requirements. We demonstrate that the choice of the MCMC algorithm has only a small influence on the amount of training data but no significant influence on the accuracy of the resulting surrogate model. Further, we show that the accuracy of the posterior estimation largely depends on the surrogate model, but not even a tailored surrogate guarantees convergence of the MCMC.Finally, we identify the forward model as the bottleneck in the inference process, not the MCMC algorithm. While related works focus on employing advanced MCMC algorithms, we demonstrate that the training data requirements render the surrogate modelling approach infeasible before the benefits of these gradient-based MCMC algorithms on cheap models can be reaped.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13361v1</guid>
      <category>physics.comp-ph</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leon Riccius, Iuri B. C. M. Rocha, Joris Bierkens, Hanne Kekkonen, Frans P. van der Meer</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction for Hierarchical Data</title>
      <link>https://arxiv.org/abs/2411.13479</link>
      <description>arXiv:2411.13479v1 Announce Type: cross 
Abstract: Reconciliation has become an essential tool in multivariate point forecasting for hierarchical time series. However, there is still a lack of understanding of the theoretical properties of probabilistic Forecast Reconciliation techniques. Meanwhile, Conformal Prediction is a general framework with growing appeal that provides prediction sets with probabilistic guarantees in finite sample. In this paper, we propose a first step towards combining Conformal Prediction and Forecast Reconciliation by analyzing how including a reconciliation step in the Split Conformal Prediction (SCP) procedure enhances the resulting prediction sets. In particular, we show that the validity granted by SCP remains while improving the efficiency of the prediction sets. We also advocate a variation of the theoretical procedure for practical use. Finally, we illustrate these results with simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13479v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillaume Principato, Yvenn Amara-Ouali, Yannig Goude, Bachir Hamrouche, Jean-Michel Poggi, Gilles Stoltz</dc:creator>
    </item>
    <item>
      <title>Assumption Smuggling in Intermediate Outcome Tests of Causal Mechanisms</title>
      <link>https://arxiv.org/abs/2407.07072</link>
      <description>arXiv:2407.07072v2 Announce Type: replace 
Abstract: Political scientists are increasingly interested in assessing causal mechanisms, or determining not just if a causal effect exists but also why it occurs. Even so, many researchers avoid formal causal mediation analyses due to their stringent assumptions, instead opting to explore causal mechanisms through what we call intermediate outcome tests. These tests estimate the effect of the treatment on one or more mediators and view such effects as suggestive evidence of a causal mechanism. In this paper, we use nonparametric bounding analysis to show that, without further assumptions, these tests can neither establish nor rule out the existence of a causal mechanism. To use intermediate outcome tests as a falsification test of causal mechanisms, researchers must make a very strong but rarely discussed monotonicity assumption. We develop a way to assess the plausibility of this monotonicity assumption and estimate our bounds for two recent experiments that use these tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07072v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Matthew Blackwell, Ruofan Ma, Aleksei Opacic</dc:creator>
    </item>
    <item>
      <title>Clustering multivariate functional data using the epigraph and hypograph indices: a case study on Madrid air quality</title>
      <link>https://arxiv.org/abs/2307.16720</link>
      <description>arXiv:2307.16720v4 Announce Type: replace-cross 
Abstract: With the rapid growth of data generation, advancements in functional data analysis (FDA) have become essential, especially for approaches that handle multiple variables at the same time. This paper introduces a novel formulation of the epigraph and hypograph indices, along with their generalized expressions, specifically designed for multivariate functional data (MFD). These new definitions account for interrelationships between variables, enabling effective clustering of MFD based on the original data curves and their first two derivatives. The methodology developed here has been tested on simulated datasets, demonstrating strong performance compared to state-of-the-art methods. Its practical utility is further illustrated with two environmental datasets: the Canadian weather dataset and a 2023 air quality study in Madrid. These applications highlight the potential of the method as a great tool for analyzing complex environmental data, offering valuable insights for researchers and policymakers in climate and environmental research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16720v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bel\'en Pulido, Alba M. Franco-Pereira, Rosa E. Lillo</dc:creator>
    </item>
    <item>
      <title>Stochastic Volatility in Mean: Efficient Analysis by a Generalized Mixture Sampler</title>
      <link>https://arxiv.org/abs/2404.13986</link>
      <description>arXiv:2404.13986v2 Announce Type: replace-cross 
Abstract: In this paper we consider the simulation-based Bayesian analysis of stochastic volatility in mean (SVM) models. Extending the highly efficient Markov chain Monte Carlo mixture sampler for the SV model proposed in Kim et al. (1998) and Omori et al. (2007), we develop an accurate approximation of the non-central chi-squared distribution as a mixture of thirty normal distributions. Under this mixture representation, we sample the parameters and latent volatilities in one block. We also detail a correction of the small approximation error by using additional Metropolis-Hastings steps. The proposed method is extended to the SVM model with leverage. The methodology and models are applied to excess holding yields and S&amp;P500 returns in empirical studies, and the SVM models are shown to outperform other volatility models based on marginal likelihoods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13986v2</guid>
      <category>econ.EM</category>
      <category>q-fin.MF</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daichi Hiraki, Siddhartha Chib, Yasuhiro Omori</dc:creator>
    </item>
    <item>
      <title>HAL-based Plugin Estimation of the Causal Dose-Response Curve</title>
      <link>https://arxiv.org/abs/2406.05607</link>
      <description>arXiv:2406.05607v2 Announce Type: replace-cross 
Abstract: Estimating the marginally adjusted dose-response curve for continuous treatments is a longstanding statistical challenge critical across multiple fields. In the context of parametric models, mis-specification may result in substantial bias, hindering the accurate discernment of the true data generating distribution and the associated dose-response curve. In contrast, non-parametric models face difficulties as the dose-response curve isn't pathwise differentiable, and then there is no $\sqrt{n}$-consistent estimator. The emergence of the Highly Adaptive Lasso (HAL) MLE by van der Laan [2015] and van der Laan [2017] and the subsequent theoretical evidence by van der Laan [2023] regarding its pointwise asymptotic normality and uniform convergence rates, have highlighted the asymptotic efficacy of the HAL-based plug-in estimator for this intricate problem. This paper delves into the HAL-based plug-in estimators, including those with cross-validation and undersmoothing selectors, and introduces the undersmoothed smoothness-adaptive HAL-based plug-in estimator. We assess these estimators through extensive simulations, employing detailed evaluation metrics. Building upon the theoretical proofs in van der Laan [2023], our empirical findings underscore the asymptotic effectiveness of the undersmoothed smoothness-adaptive HAL-based plug-in estimator in estimating the marginally adjusted dose-response curve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05607v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junming Shi, Wenxin Zhang, Alan E. Hubbard, Mar van der Laan</dc:creator>
    </item>
    <item>
      <title>Classification of Buried Objects from Ground Penetrating Radar Images by using Second Order Deep Learning Models</title>
      <link>https://arxiv.org/abs/2410.07117</link>
      <description>arXiv:2410.07117v2 Announce Type: replace-cross 
Abstract: In this paper, a new classification model based on covariance matrices is built in order to classify buried objects. The inputs of the proposed models are the hyperbola thumbnails obtained with a classical Ground Penetrating Radar (GPR) system. These thumbnails are then inputs to the first layers of a classical CNN, which then produces a covariance matrix using the outputs of the convolutional filters. Next, the covariance matrix is given to a network composed of specific layers to classify Symmetric Positive Definite (SPD) matrices. We show in a large database that our approach outperform shallow networks designed for GPR data and conventional CNNs typically used in computer vision applications, particularly when the number of training data decreases and in the presence of mislabeled data. We also illustrate the interest of our models when training data and test sets are obtained from different weather modes or considerations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07117v2</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Douba Jafuno, Ammar Mian, Guillaume Ginolhac, Nickolas Stelzenmuller</dc:creator>
    </item>
    <item>
      <title>The VIX as Stochastic Volatility for Corporate Bonds</title>
      <link>https://arxiv.org/abs/2410.22498</link>
      <description>arXiv:2410.22498v4 Announce Type: replace-cross 
Abstract: Classic stochastic volatility models assume volatility is unobservable. We use the Volatility Index: S\&amp;P 500 VIX to observe it, to easier fit the model. We apply it to corporate bonds. We fit autoregression for corporate rates and for risk spreads between these rates and Treasury rates. Next, we divide residuals by VIX. Our main idea is such division makes residuals closer to the ideal case of a Gaussian white noise. This is remarkable, since these residuals and VIX come from separate market segments. Similarly, we model corporate bond returns as a linear function of rates and rate changes. Our article has two main parts: Moody's AAA and BAA spreads; Bank of America investment-grade and high-yield rates, spreads, and returns. We analyze long-term stability of these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22498v4</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jihyun Park, Andrey Sarantsev</dc:creator>
    </item>
    <item>
      <title>Zero-Coupon Treasury Yield Curve with VIX as Stochastic Volatility</title>
      <link>https://arxiv.org/abs/2411.03699</link>
      <description>arXiv:2411.03699v3 Announce Type: replace-cross 
Abstract: We study a multivariate autoregressive stochastic volatility model for the first 3 principal components (level, slope, curvature) of 10 series of zero-coupon Treasury bond rates with maturities from 1 to 10 years. We fit this model using monthly data from 1990. Next, we prove long-term stability for this discrete-time model and its continuous-time version. Unlike classic models with hidden stochastic volatility, here it is observed as VIX: the volatility index for the S\&amp;P 500 stock market index. It is surprising that this volatility, created for the stock market, also works for Treasury bonds. Since total returns of zero-coupon bonds can be easily found from these principal components, we prove long-term stability for total returns in discrete time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03699v3</guid>
      <category>q-fin.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jihyun Park, Andrey Sarantsev</dc:creator>
    </item>
    <item>
      <title>Generation of synthetic gait data: application to multiple sclerosis patients' gait patterns</title>
      <link>https://arxiv.org/abs/2411.10377</link>
      <description>arXiv:2411.10377v2 Announce Type: replace-cross 
Abstract: Multiple sclerosis (MS) is the leading cause of severe non-traumatic disability in young adults and its incidence is increasing worldwide. The variability of gait impairment in MS necessitates the development of a non-invasive, sensitive, and cost-effective tool for quantitative gait evaluation. The eGait movement sensor, designed to characterize human gait through unit quaternion time series (QTS) representing hip rotations, is a promising approach. However, the small sample sizes typical of clinical studies pose challenges for the stability of gait data analysis tools. To address these challenges, this article presents two key scientific contributions. First, a comprehensive framework is proposed for transforming QTS data into a form that preserves the essential geometric properties of gait while enabling the use of any tabular synthetic data generation method. Second, a synthetic data generation method is introduced, based on nearest neighbors weighting, which produces high-fidelity synthetic QTS data suitable for small datasets and private data environments. The effectiveness of the proposed method, is demonstrated through its application to MS gait data, showing very good fidelity and respect of the initial geometry of the data. Thanks to this work, we are able to produce synthetic data sets and work on the stability of clustering methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10377v2</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Klervi Le Gall, Lise Bellanger, David Laplaud, Aymeric Stamm</dc:creator>
    </item>
    <item>
      <title>The Noisy Work of Uncertainty Visualisation Research: A Review</title>
      <link>https://arxiv.org/abs/2411.10482</link>
      <description>arXiv:2411.10482v2 Announce Type: replace-cross 
Abstract: Uncertainty visualisation is quickly becomming a hot topic in information visualisation. Exisiting reviews in the field take the definition and purpose of an uncertainty visualisation to be self evident which results in a large amout of conflicting information. This conflict largely stems from a conflation between uncertainty visualisations designed for decision making and those designed to prevent false conclusions. We coin the term "signal suppression" to describe a visualisation that is designed for preventing false conclusions, as the approach demands that the signal (i.e. the collective take away of the estimates) is suppressed by the noise (i.e. the variance on those estimates). We argue that the current standards in visualisation suggest that uncertainty visualisations designed for decision making should not be considered uncertainty visualisations at all. Therefore, future work should focus on signal suppression. Effective signal suppression requires us to communicate the signal and the noise as a single "validity of signal" variable, and doing so proves to be difficult with current methods. We illustrate current approaches to uncertainty visualisation by showing how they would change the visual apprearance of a choropleth map. These maps allow us to see why some methods succeed at signal suppression, while others fall short. Evaluating visualisations on how well they perform signal suppression also proves to be difficult, as it involves measuring the effect of noise, a variable we typically try to ignore. We suggest authors use qualitative studies or compare uncertainty visualisations to the relevant hypothesis tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10482v2</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harriet Mason, Dianne Cook, Sarah Goodwin, Emi Tanaka, Susan VanderPlas</dc:creator>
    </item>
  </channel>
</rss>
