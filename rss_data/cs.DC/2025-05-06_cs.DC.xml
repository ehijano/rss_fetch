<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 May 2025 01:46:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Unlocking True Elasticity for the Cloud-Native Era with Dandelion</title>
      <link>https://arxiv.org/abs/2505.01603</link>
      <description>arXiv:2505.01603v1 Announce Type: new 
Abstract: Elasticity is fundamental to cloud computing, as it enables quickly allocating resources to match the demand of each workload as it arrives, rather than pre-provisioning resources to meet performance objectives. However, even serverless platforms -- which boot sandboxes in 10s to 100s of milliseconds -- are not sufficiently elastic to avoid over-provisioning expensive resources. Today's FaaS platforms rely on pre-provisioning many idle sandboxes in memory to reduce the occurrence of slow, cold starts. A key obstacle for high elasticity is booting a guest OS and configuring features like networking in sandboxes, which are required to expose an isolated POSIX-like interface to user functions. Our key insight is that redesigning the interface for applications in the cloud-native era enables co-designing a much more efficient and elastic execution system. Now is a good time to rethink cloud abstractions as developers are building applications to be cloud-native. Cloud-native applications typically consist of user-provided compute logic interacting with cloud services (for storage, AI inference, query processing, etc) exposed over REST APIs. Hence, we propose Dandelion, an elastic cloud platform with a declarative programming model that expresses applications as DAGs of pure compute functions and higher-level communication functions. Dandelion can securely execute untrusted user compute functions in lightweight sandboxes that cold start in hundreds of microseconds, since pure functions do not rely on extra software environments such as a guest OS. Dandelion makes it practical to boot a sandbox on-demand for each request, decreasing performance variability by two to three orders of magnitude compared to Firecracker and reducing committed memory by 96% on average when running the Azure Functions trace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01603v1</guid>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tom Kuchler, Pinghe Li, Yazhuo Zhang, Lazar Cvetkovi\'c, Boris Goranov, Tobias Stocker, Leon Thomm, Simone Kalbermatter, Tim Notter, Andrea Lattuada, Ana Klimovic</dc:creator>
    </item>
    <item>
      <title>Phantora: Live GPU Cluster Simulation for Machine Learning System Performance Estimation</title>
      <link>https://arxiv.org/abs/2505.01616</link>
      <description>arXiv:2505.01616v1 Announce Type: new 
Abstract: To accommodate ever-increasing model complexity, modern machine learning (ML) systems have to scale to large GPU clusters. Changes in ML model architecture, ML system implementation, and cluster configuration can significantly affect overall ML system performance. However, quantifying the performance impact before deployment is challenging. Existing performance estimation methods use performance modeling or static workload simulation. These techniques are not general: they requires significant human effort and computation capacity to generate training data or a workload. It is also difficult to adapt ML systems to use these techniques. This paper introduces, Phantora, a live GPU cluster simulator for performance estimation. Phantora runs minimally modified ML models and frameworks, intercepting and simulating GPU-related operations to enable high-fidelity performance estimation. Phantora overcomes several research challenges in integrating an event-driven network simulator with live system execution, and introduces a set of techniques to improve simulation speed, scalability, and accuracy. Our evaluation results show that Phantora can deliver similar estimation accuracy to the state-of-the-art workload simulation approach with only one GPU, while reducing human effort and increasing generalizability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01616v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianxing Qin, Jingrong Chen, Xinhao Kong, Yongji Wu, Liang Luo, Zhaodong Wang, Ying Zhang, Tingjun Chen, Alvin R. Lebeck, Danyang Zhuo</dc:creator>
    </item>
    <item>
      <title>Report on Challenges of Practical Reproducibility for Systems and HPC Computer Science</title>
      <link>https://arxiv.org/abs/2505.01671</link>
      <description>arXiv:2505.01671v1 Announce Type: new 
Abstract: This report synthesizes findings from the November 2024 Community Workshop on Practical Reproducibility in HPC, which convened researchers, artifact authors, reviewers, and chairs of reproducibility initiatives to address the critical challenge of making computational experiments reproducible in a cost-effective manner. The workshop deliberately focused on systems and HPC computer science research due to its unique requirements, including specialized hardware access and deep system reconfigurability. Through structured discussions, lightning talks, and panel sessions, participants identified key barriers to practical reproducibility and formulated actionable recommendations for the community.
  The report presents a dual framework of challenges and recommendations organized by target audience (authors, reviewers, organizations, and community). It characterizes technical obstacles in experiment packaging and review, including completeness of artifact descriptions, acquisition of specialized hardware, and establishing reproducibility conditions. The recommendations range from immediate practical tools (comprehensive checklists for artifact packaging) to ecosystem-level improvements (refining badge systems, creating artifact digital libraries, and developing AI-assisted environment creation). Rather than advocating for reproducibility regardless of cost, the report emphasizes striking an appropriate balance between reproducibility rigor and practical feasibility, positioning reproducibility as an integral component of scientific exploration rather than a burdensome afterthought. Appendices provide detailed, immediately actionable checklists for authors and reviewers to improve reproducibility practices across the HPC community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01671v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.15306609</arxiv:DOI>
      <dc:creator>Kate Keahey, Marc Richardson, Rafael Tolosana Calasanz, Sascha Hunold, Jay Lofstead, Tanu Malik, Christian Perez</dc:creator>
    </item>
    <item>
      <title>The consensus number of a shift register equals its width</title>
      <link>https://arxiv.org/abs/2505.01691</link>
      <description>arXiv:2505.01691v1 Announce Type: new 
Abstract: The consensus number of a w-bit register supporting logical left shift and right shift operations is exactly w, giving an example of a class of types, widely implemented in practice, that populates all levels of the consensus hierarchy. This result generalizes to w-wide shift registers over larger alphabets. In contrast, a register providing arithmetic right shift, which replicates the most significant bit instead of replacing it with zero, is shown to solve consensus for any fixed number of processes as long as its width is at least two.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01691v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>James Aspnes</dc:creator>
    </item>
    <item>
      <title>Byzantine Agreement with Predictions</title>
      <link>https://arxiv.org/abs/2505.01793</link>
      <description>arXiv:2505.01793v1 Announce Type: new 
Abstract: In this paper, we study the problem of \emph{Byzantine Agreement with predictions}. Along with a proposal, each process is also given a prediction, i.e., extra information which is not guaranteed to be true. For example, one might imagine that the prediction is produced by a network security monitoring service that looks for patterns of malicious behavior.
  Our goal is to design an algorithm that is more efficient when the predictions are accurate, degrades in performance as predictions decrease in accuracy, and still in the worst case performs as well as any algorithm without predictions even when the predictions are completely inaccurate.
  On the negative side, we show that Byzantine Agreement with predictions still requires $\Omega(n^2)$ messages, even in executions where the predictions are completely accurate. On the positive side, we show that \emph{classification predictions} can help improve the time complexity. For (synchronous) Byzantine Agreement with classification predictions, we present new algorithms that leverage predictions to yield better time complexity, and we show that the time complexity achieved is optimal as a function of the prediction quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01793v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naama Ben-David, Muhammad Ayaz Dzulfikar, Faith Ellen, Seth Gilbert</dc:creator>
    </item>
    <item>
      <title>Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey</title>
      <link>https://arxiv.org/abs/2505.01821</link>
      <description>arXiv:2505.01821v1 Announce Type: new 
Abstract: Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm for addressing the computational demands of modern intelligent applications, integrating cloud resources with edge devices to enable efficient, low-latency processing. Recent advancements in AI, particularly deep learning and large language models (LLMs), have dramatically enhanced the capabilities of these distributed systems, yet introduce significant challenges in model deployment and resource management. In this survey, we comprehensive examine the intersection of distributed intelligence and model optimization within edge-cloud environments, providing a structured tutorial on fundamental architectures, enabling technologies, and emerging applications. Additionally, we systematically analyze model optimization approaches, including compression, adaptation, and neural architecture search, alongside AI-driven resource management strategies that balance performance, energy efficiency, and latency requirements. We further explore critical aspects of privacy protection and security enhancement within ECCC systems and examines practical deployments through diverse applications, spanning autonomous driving, healthcare, and industrial automation. Performance analysis and benchmarking techniques are also thoroughly explored to establish evaluation standards for these complex systems. Furthermore, the review identifies critical research directions including LLMs deployment, 6G integration, neuromorphic computing, and quantum computing, offering a roadmap for addressing persistent challenges in heterogeneity management, real-time processing, and scalability. By bridging theoretical advancements and practical deployments, this survey offers researchers and practitioners a holistic perspective on leveraging AI to optimize distributed computing environments, fostering innovation in next-generation intelligent systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01821v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Liu, Yao Du, Kun Yang, Yan Wang, Xiping Hu, Zehua Wang, Yang Liu, Peng Sun, Azzedine Boukerche, Victor C. M. Leung</dc:creator>
    </item>
    <item>
      <title>LCI: a Lightweight Communication Interface for Efficient Asynchronous Multithreaded Communication</title>
      <link>https://arxiv.org/abs/2505.01864</link>
      <description>arXiv:2505.01864v1 Announce Type: new 
Abstract: The evolution of architectures, programming models, and algorithms is driving communication towards greater asynchrony and concurrency, usually in multithreaded environments. We present LCI, a communication library designed for efficient asynchronous multithreaded communication. LCI provides a concise interface that supports common point-to-point primitives and diverse completion mechanisms, along with flexible controls for incrementally fine-tuning communication resources and runtime behavior. It features a threading-efficient runtime built on atomic data structures, fine-grained non-blocking locks, and low-level network insights. We evaluate LCI on both Inifiniband and Slingshot-11 clusters with microbenchmarks and two application-level benchmarks. Experiment results show that LCI significantly outperforms existing communication libraries in various multithreaded scenarios, achieving performance that exceeds the traditional multi-process execution mode and unlocking new possibilities for emerging programming models and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01864v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiakun Yan, Marc Snir</dc:creator>
    </item>
    <item>
      <title>HAS-GPU: Efficient Hybrid Auto-scaling with Fine-grained GPU Allocation for SLO-aware Serverless Inferences</title>
      <link>https://arxiv.org/abs/2505.01968</link>
      <description>arXiv:2505.01968v1 Announce Type: new 
Abstract: Serverless Computing (FaaS) has become a popular paradigm for deep learning inference due to the ease of deployment and pay-per-use benefits. However, current serverless inference platforms encounter the coarse-grained and static GPU resource allocation problems during scaling, which leads to high costs and Service Level Objective (SLO) violations in fluctuating workloads. Meanwhile, current platforms only support horizontal scaling for GPU inferences, thus the cold start problem further exacerbates the problems. In this paper, we propose HAS-GPU, an efficient Hybrid Auto-scaling Serverless architecture with fine-grained GPU allocation for deep learning inferences. HAS-GPU proposes an agile scheduler capable of allocating GPU Streaming Multiprocessor (SM) partitions and time quotas with arbitrary granularity and enables significant vertical quota scalability at runtime. To resolve performance uncertainty introduced by massive fine-grained resource configuration spaces, we propose the Resource-aware Performance Predictor (RaPP). Furthermore, we present an adaptive hybrid auto-scaling algorithm with both horizontal and vertical scaling to ensure inference SLOs and minimize GPU costs. The experiments demonstrated that compared to the mainstream serverless inference platform, HAS-GPU reduces function costs by an average of 10.8x with better SLO guarantees. Compared to state-of-the-art spatio-temporal GPU sharing serverless framework, HAS-GPU reduces function SLO violation by 4.8x and cost by 1.72x on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01968v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianfeng Gu, Puxuan Wang, Isaac Nunezand, Kai Huang, Michael Gerndt</dc:creator>
    </item>
    <item>
      <title>Scalable Genomic Context Analysis with GCsnap2 on HPC Clusters</title>
      <link>https://arxiv.org/abs/2505.02195</link>
      <description>arXiv:2505.02195v1 Announce Type: new 
Abstract: GCsnap2 Cluster is a scalable, high performance tool for genomic context analysis, developed to overcome the limitations of its predecessor, GCsnap1 Desktop. Leveraging distributed computing with mpi4py.futures, GCsnap2 Cluster achieved a 22x improvement in execution time and can now perform genomic context analysis for hundreds of thousands of input sequences in HPC clusters. Its modular architecture enables the creation of task-specific workflows and flexible deployment in various computational environments, making it well suited for bioinformatics studies of large-scale datasets. This work highlights the potential for applying similar approaches to solve scalability challenges in other scientific domains that rely on large-scale data analysis pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02195v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reto Krummenacher, Osman Seckin Simsek, Mich\`ele Leemann, Leila T. Alexander, Torsten Schwede, Florina M. Ciorba, Joana Pereira</dc:creator>
    </item>
    <item>
      <title>Grassroots Democratic Federation: Fair Governance of Large-Scale, Decentralized, Sovereign Digital Communities</title>
      <link>https://arxiv.org/abs/2505.02208</link>
      <description>arXiv:2505.02208v2 Announce Type: new 
Abstract: Grassroots Democratic Federation aims to address the egalitarian formation and the fair democratic governance of large-scale, decentralized, sovereign digital communities, the size of the EU, the US, existing social networks, and even humanity at large. A grassroots democratic federation evolves via the grassroots formation of digital communities and their consensual federation. Such digital communities may form according to geography, jurisdiction, affiliations, relations, interests, or causes. Small communities (say up to 100 members) govern themselves; larger communities -- no matter how large -- are governed by a small assembly elected by sortition among its members. Earlier work on Grassroots Democratic Federation explored the fair sortition of the assemblies of a federation in a static setting: Given a federation, populate its assemblies with members satisfying ex ante and ex post fairness conditions on the participation of members of a community in its assembly, and on the representation of child communities in the assembly of their parent community.
  In practice, we expect a grassroots democratic federation to grow and evolve dynamically and in all directions -- bottom-up, top-down, and middle-out. To address that, we formally specify this dynamic setting and adapt the static fairness conditions to it: The ex post condition on the fair representation of a child community becomes a condition that must always hold; the ex ante conditions in expectation on the fair participation of an individual and on the fair representation of a child community become conditions satisfied in actuality in the limit, provided the federation structure eventually stabilizes. We then present a protocol that satisfies these fairness conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02208v2</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ehud Shapiro, Nimrod Talmon</dc:creator>
    </item>
    <item>
      <title>Opt-GPTQ: An Optimized GPTQ Combining Sparse Attention and Quantization Techniques</title>
      <link>https://arxiv.org/abs/2505.02351</link>
      <description>arXiv:2505.02351v1 Announce Type: new 
Abstract: In the field of deep learning, traditional attention mechanisms face significant challenges related to high computational complexity and large memory consumption when processing long sequence data. To address these limitations, we propose Opt-GPTQ, an optimized Gradient-based Post Training Quantization (GPTQ) combining the Grouped Query Attention (GQA) mechanism with paging memory management, optimizing the traditional Multi-Head Attention (MHA) mechanism by grouping query heads and sharing key-value vectors. Optimized GQA (Opt-GQA) effectively reduces computational complexity, minimizes memory fragmentation, and enhances memory utilization for large-scale models. Opt-GPTQ is optimized for Data Center Units (DCUs) and integrated into the vLLM model to maximize hardware efficiency. It customizes GPU kernels to further enhance attention computation by reducing memory access latency and boosting parallel computing capabilities. Opt-GQA integrates Attention with Linear Biases (ALiBi) to reduce overhead and enhance long-sequence processing. Experimental results show that Opt?GPTQ significantly reduces computation time and memory usage while improving model performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02351v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Kong, Junxiang Zhang, Jiheng Xu, Yalong Li, Shouhua Zhang, Jiehan Zhou, Yuhai Liu, Peng Liang, Quan Zhang, Luohan Jiang</dc:creator>
    </item>
    <item>
      <title>Model Checking and Synthesis for Optimal Use of Knowledge in Consensus Protocols</title>
      <link>https://arxiv.org/abs/2505.02353</link>
      <description>arXiv:2505.02353v1 Announce Type: new 
Abstract: Logics of knowledge and knowledge-based programs provide a way to give abstract descriptions of solutions to problems in fault-tolerant distributed computing, and have been used to derive optimal protocols for these problems with respect to a variety of failure models. Generally, these results have involved complex pencil and paper analyses with respect to the theoretical "full-information protocol" model of information exchange between network nodes. It is equally of interest to be able to establish the optimality of protocols using weaker, but more practical, models of information exchange, or else identify opportunities to improve their performance. Over the last 20 years, automated verification and synthesis tools for the logic of knowledge have been developed, such as the model checker MCK, that can be applied to this problem. This paper concerns the application of MCK to automated analyses of this kind. A number of information-exchange models are considered, for Simultaneous and Eventual variants of Byzantine Agreement under a range of failure types. MCK is used to automatically analyze these models. The results demonstrate that it is possible to automatically identify optimization opportunities, and to automatically synthesize optimal protocols. The paper provides performance measurements for the automated analysis, establishing a benchmark for epistemic model checking and synthesis tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02353v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaya Alpturer, Gerald Huang, Ron van der Meyden</dc:creator>
    </item>
    <item>
      <title>Large Language Model Partitioning for Low-Latency Inference at the Edge</title>
      <link>https://arxiv.org/abs/2505.02533</link>
      <description>arXiv:2505.02533v1 Announce Type: new 
Abstract: Large Language Models (LLMs) based on autoregressive, decoder-only Transformers generate text one token at a time, where a token represents a discrete unit of text. As each newly produced token is appended to the partial output sequence, the length grows and so does the memory and compute load, due to the expanding key-value caches, which store intermediate representations of all previously generated tokens in the multi-head attention (MHA) layer. As this iterative process steadily increases memory and compute demands, layer-based partitioning in resource-constrained edge environments often results in memory overload or high inference latency. To address this and reduce inference latency, we propose a resource-aware Transformer architecture partitioning algorithm, where the partitioning decision is updated at regular intervals during token generation. The approach is myopic in that it is based on instantaneous information about device resource availability and network link bandwidths. When first executed, the algorithm places blocks on devices, and in later executions, it migrates these blocks among devices so that the sum of migration delay and inference delay remains low. Our approach partitions the decoder at the attention head level, co-locating each attention head with its key-value cache and allowing dynamic migrations whenever resources become tight. By allocating different attention heads to different devices, we exploit parallel execution of attention heads and thus achieve substantial reductions in inference delays. Our experiments show that in small-scale settings (3-5 devices), the proposed method achieves within 15 to 20 percent of an exact optimal solver's latency, while in larger-scale tests it achieves notable improvements in inference speed and memory usage compared to state-of-the-art layer-based partitioning approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02533v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dimitrios Kafetzis, Ramin Khalili, Iordanis Koutsopoulos</dc:creator>
    </item>
    <item>
      <title>Tight Bounds on Channel Reliability via Generalized Quorum Systems (Extended Version)</title>
      <link>https://arxiv.org/abs/2505.02646</link>
      <description>arXiv:2505.02646v1 Announce Type: new 
Abstract: Communication channel failures are a major concern for the developers of modern fault-tolerant systems. However, while tight bounds for process failures are well-established, extending them to include channel failures has remained an open problem. We introduce \emph{generalized quorum systems} - a framework that characterizes the necessary and sufficient conditions for implementing atomic registers, atomic snapshots, lattice agreement and consensus under arbitrary patterns of process-channel failures. Generalized quorum systems relax the connectivity constraints of classical quorum systems: instead of requiring bidirectional reachability for every pair of write and read quorums, they only require some write quorum to be \emph{unidirectionally} reachable from some read quorum. This weak connectivity makes implementing registers particularly challenging, because it precludes the traditional request/response pattern of quorum access, making classical solutions like ABD inapplicable. To address this, we introduce novel logical clocks that allow write and read quorums to reliably track state updates without relying on bi-directional connectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02646v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Naser-Pastoriza, Gregory Chockler, Alexey Gotsman, Fedor Ryabinin</dc:creator>
    </item>
    <item>
      <title>Open Challenges for a Production-ready Cloud Environment on top of RISC-V hardware</title>
      <link>https://arxiv.org/abs/2505.02650</link>
      <description>arXiv:2505.02650v1 Announce Type: new 
Abstract: As part of the Vitamin-V European project, we have built a prototype of a RISC-V cluster managed by OpenStack, with the goal of realizing a functional RISC-V cloud ecosystem. In this poster we explain the hardware and software challenges encountered while porting some elements of OpenStack. We also discuss the current performance gaps that challenge a performance-ready cloud environment over such new ISA, an essential element to fulfill in order to achieve european technological sovereignty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02650v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Call, Ramon Nou, Guillem Senabre</dc:creator>
    </item>
    <item>
      <title>A Unifying Framework to Enable Artificial Intelligence in High Performance Computing Workflows</title>
      <link>https://arxiv.org/abs/2505.02738</link>
      <description>arXiv:2505.02738v1 Announce Type: new 
Abstract: Current trends point to a future where large-scale scientific applications are tightly-coupled HPC/AI hybrids. Hence, we urgently need to invest in creating a seamless, scalable framework where HPC and AI/ML can efficiently work together and adapt to novel hardware and vendor libraries without starting from scratch every few years. The current ecosystem and sparsely-connected community are not sufficient to tackle these challenges, and we require a breakthrough catalyst for science similar to what PyTorch enabled for AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02738v1</guid>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/MCSE.2025.3543940</arxiv:DOI>
      <dc:creator>Jens Domke, Mohamed Wahib, Anshu Dubey, Tal Ben-Nun, Erik W. Draeger</dc:creator>
    </item>
    <item>
      <title>Optimistic, Signature-Free Reliable Broadcast and Its Applications</title>
      <link>https://arxiv.org/abs/2505.02761</link>
      <description>arXiv:2505.02761v1 Announce Type: new 
Abstract: Reliable broadcast (RBC) is a key primitive in fault-tolerant distributed systems, and improving its efficiency can benefit a wide range of applications. This work focuses on signature-free RBC protocols, which are particularly attractive due to their computational efficiency. Existing protocols in this setting incur an optimal 3 steps to reach a decision while tolerating up to $f &lt; n/3$ Byzantine faults, where $n$ is the number of parties. In this work, we propose an optimistic RBC protocol that maintains the $f &lt; n/3$ fault tolerance but achieves termination in just 2 steps under certain optimistic conditions--when at least $\lceil \frac{n+2f-2}{2} \rceil$ non-broadcaster parties behave honestly. We also prove a matching lower bound on the number of honest parties required for 2-step termination.
  We show that our latency-reduction technique generalizes beyond RBC and applies to other primitives such as asynchronous verifiable secret sharing (AVSS) and asynchronous verifiable information dispersal (AVID), enabling them to complete in 2 steps under similar optimistic conditions.
  To highlight the practical impact of our RBC protocol, we integrate it into Sailfish++, a new signature-free, post-quantum secure DAG-based Byzantine fault-tolerant (BFT) consensus protocol. Under optimistic conditions, this protocol achieves a commit latency of 3 steps--matching the performance of the best signature-based protocols. Our experimental evaluation shows that our protocol significantly outperforms existing post-quantum secure and signature-based protocols, even on machines with limited CPU resources. In contrast, signature-based protocols require high CPU capacity to achieve comparable performance. We have open-sourced our Rust implementation of Sailfish++ to facilitate reproducible results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02761v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nibesh Shrestha, Qianyu Yu, Aniket Kate, Giuliano Losa, Kartik Nayak, Xuechao Wang</dc:creator>
    </item>
    <item>
      <title>An Almost Tight Lower Bound for Plurality Consensus with Undecided State Dynamics in the Population Protocol Model</title>
      <link>https://arxiv.org/abs/2505.02765</link>
      <description>arXiv:2505.02765v1 Announce Type: new 
Abstract: We revisit the majority problem in the population protocol communication model, as first studied by Angluin et al. (Distributed Computing 2008). We consider a more general version of this problem known as plurality consensus, which has already been studied intensively in the literature. In this problem, each node in a system of $n$ nodes, has initially one of $k$ different opinions, and they need to agree on the (relative) majority opinion. In particular, we consider the important and intensively studied model of Undecided State Dynamics.
  Our main contribution is an almost tight lower bound on the stabilization time: we prove that there exists an initial configuration, even with bias $\Delta = \omega(\sqrt{n\log n})$, where stabilization requires $\Omega(kn\log \frac {\sqrt n} {k \log n})$ interactions, or equivalently, $\Omega(k\log \frac {\sqrt n} {k \log n})$ parallel time for any $k = o\left(\frac {\sqrt n}{\log n}\right)$. This bound is tight for any $ k \le n^{\frac 1 2 - \epsilon}$, where $\epsilon &gt;0$ can be any small constant, as Amir et al.~(PODC'23) gave a $O(k\log n)$ parallel time upper bound for $k = O\left(\frac {\sqrt n} {\log ^2 n}\right)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02765v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine El-Hayek, Robert Els\"asser, Stefan Schmid</dc:creator>
    </item>
    <item>
      <title>Brief Announcement: Minimizing Energy Solves Relative Majority with a Cubic Number of States in Population Protocols</title>
      <link>https://arxiv.org/abs/2505.02785</link>
      <description>arXiv:2505.02785v1 Announce Type: new 
Abstract: This paper revisits a fundamental distributed computing problem in the population protocol model.
  Provided $n$ agents each starting with an input color in $[k]$, the relative majority problem asks to find the predominant color.
  In the population protocol model, at each time step, a scheduler selects two agents that first learn each other's states and then update their states based on what they learned.
  We present the \textsc{Circles} protocol that solves the relative majority problem with $k^3$ states. It is always-correct under weakly fair scheduling.
  Not only does it improve upon the best known upper bound of $O(k^7)$, but it also shows a strikingly simpler design inspired by energy minimization in chemical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02785v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom-Lukas Breitkopf, Julien Dallot, Antoine El-Hayek, Stefan Schmid</dc:creator>
    </item>
    <item>
      <title>Recolorable Graph Exploration by an Oblivious Agent with Fewer Colors</title>
      <link>https://arxiv.org/abs/2505.02789</link>
      <description>arXiv:2505.02789v1 Announce Type: new 
Abstract: Recently, B\"ockenhauer, Frei, Unger, and Wehner (SIROCCO 2023) introduced a novel variant of the graph exploration problem in which a single memoryless agent must visit all nodes of an unknown, undirected, and connected graph before returning to its starting node. Unlike the standard model for mobile agents, edges are not labeled with port numbers. Instead, the agent can color its current node and observe the color of each neighboring node. To move, it specifies a target color and then moves to an adversarially chosen neighbor of that color. B\"ockenhauer~et al.~analyzed the minimum number of colors required for successful exploration and proposed an elegant algorithm that enables the agent to explore an arbitrary graph using only eight colors. In this paper, we present a novel graph exploration algorithm that requires only six colors. Furthermore, we prove that five colors are sufficient if we consider only a restricted class of graphs, which we call the $\varphi$-free graphs, a class that includes every graph with maximum degree at most three and every cactus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02789v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shota Takahashi, Haruki Kanaya, Shoma Hiraoka, Ryota Eguchi, Yuichi Sudo</dc:creator>
    </item>
    <item>
      <title>AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine</title>
      <link>https://arxiv.org/abs/2505.01435</link>
      <description>arXiv:2505.01435v1 Announce Type: cross 
Abstract: Language models for scientific tasks are trained on text from scientific publications, most distributed as PDFs that require parsing. PDF parsing approaches range from inexpensive heuristics (for simple documents) to computationally intensive ML-driven systems (for complex or degraded ones). The choice of the "best" parser for a particular document depends on its computational cost and the accuracy of its output. To address these issues, we introduce an Adaptive Parallel PDF Parsing and Resource Scaling Engine (AdaParse), a data-driven strategy for assigning an appropriate parser to each document. We enlist scientists to select preferred parser outputs and incorporate this information through direct preference optimization (DPO) into AdaParse, thereby aligning its selection process with human judgment. AdaParse then incorporates hardware requirements and predicted accuracy of each parser to orchestrate computational resources efficiently for large-scale parsing campaigns. We demonstrate that AdaParse, when compared to state-of-the-art parsers, improves throughput by $17\times$ while still achieving comparable accuracy (0.2 percent better) on a benchmark set of 1000 scientific documents. AdaParse's combination of high accuracy and parallel scalability makes it feasible to parse large-scale scientific document corpora to support the development of high-quality, trillion-token-scale text datasets. The implementation is available at https://github.com/7shoe/AdaParse/</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01435v1</guid>
      <category>cs.IR</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlo Siebenschuh, Kyle Hippe, Ozan Gokdemir, Alexander Brace, Arham Khan, Khalid Hossain, Yadu Babuji, Nicholas Chia, Venkatram Vishwanath, Rick Stevens, Arvind Ramanathan, Ian Foster, Robert Underwood</dc:creator>
    </item>
    <item>
      <title>PipeSpec: Breaking Stage Dependencies in Hierarchical LLM Decoding</title>
      <link>https://arxiv.org/abs/2505.01572</link>
      <description>arXiv:2505.01572v1 Announce Type: cross 
Abstract: Speculative decoding accelerates large language model inference by using smaller draft models to generate candidate tokens for parallel verification. However, current approaches are limited by sequential stage dependencies that prevent full hardware utilization. We present PipeSpec, a framework that generalizes speculative decoding to $k$ models arranged in a hierarchical pipeline, enabling asynchronous execution with lightweight coordination for prediction verification and rollback. Our analytical model characterizes token generation rates across pipeline stages and proves guaranteed throughput improvements over traditional decoding for any non-zero acceptance rate. We further derive closed-form expressions for steady-state verification probabilities that explain the empirical benefits of pipeline depth. Experimental results show that PipeSpec achieves up to 2.54$\times$ speedup while outperforming state-of-the-art methods. We validate PipeSpec across text summarization and code generation tasks using LLaMA 2 and 3 models, demonstrating that pipeline efficiency increases with model depth, providing a scalable approach to accelerating LLM inference on multi-device systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01572v1</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bradley McDanel, Sai Qian Zhang, Yunhai Hu, Zining Liu</dc:creator>
    </item>
    <item>
      <title>On the Design of Resilient Distributed Single Time-Scale Estimators: A Graph-Theoretic Approach</title>
      <link>https://arxiv.org/abs/2505.01757</link>
      <description>arXiv:2505.01757v1 Announce Type: cross 
Abstract: Distributed estimation in interconnected systems has gained increasing attention due to its relevance in diverse applications such as sensor networks, autonomous vehicles, and cloud computing. In real practice, the sensor network may suffer from communication and/or sensor failures. This might be due to cyber-attacks, faults, or environmental conditions. Distributed estimation resilient to such conditions is the topic of this paper. By representing the sensor network as a graph and exploiting its inherent structural properties, we introduce novel techniques that enhance the robustness of distributed estimators. As compared to the literature, the proposed estimator (i) relaxes the network connectivity of most existing single time-scale estimators and (ii) reduces the communication load of the existing double time-scale estimators by avoiding the inner consensus loop.
  On the other hand, the sensors might be subject to faults or attacks, resulting in biased measurements. Removing these sensor data may result in observability loss. Therefore, we propose resilient design on the definitions of $q$-node-connectivity and $q$-link-connectivity, which capture robust strong-connectivity under link or sensor node failure. By proper design of the sensor network, we prove Schur stability of the proposed distributed estimation protocol under failure of up to $q$ sensors or $q$ communication links.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01757v1</guid>
      <category>eess.SY</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadreza Doostmohammadian, Mohammad Pirani</dc:creator>
    </item>
    <item>
      <title>Privacy Preserving Machine Learning Model Personalization through Federated Personalized Learning</title>
      <link>https://arxiv.org/abs/2505.01788</link>
      <description>arXiv:2505.01788v1 Announce Type: cross 
Abstract: The widespread adoption of Artificial Intelligence (AI) has been driven by significant advances in intelligent system research. However, this progress has raised concerns about data privacy, leading to a growing awareness of the need for privacy-preserving AI. In response, there has been a seismic shift in interest towards the leading paradigm for training Machine Learning (ML) models on decentralized data silos while maintaining data privacy, Federated Learning (FL). This research paper presents a comprehensive performance analysis of a cutting-edge approach to personalize ML model while preserving privacy achieved through Privacy Preserving Machine Learning with the innovative framework of Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns about data privacy, this study evaluates the effectiveness of PPMLFPL addressing the critical balance between personalized model refinement and maintaining the confidentiality of individual user data. According to our analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption (APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated personalized learning settings is strongly suggested. The results offer valuable insights creating it a promising scope for future advancements in the field of privacy-conscious data-driven technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01788v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Md. Tanzib Hosain, Asif Zaman, Md. Shahriar Sajid, Shadman Sakeeb Khan, Shanjida Akter</dc:creator>
    </item>
    <item>
      <title>Towards Trustworthy Federated Learning with Untrusted Participants</title>
      <link>https://arxiv.org/abs/2505.01874</link>
      <description>arXiv:2505.01874v1 Announce Type: cross 
Abstract: Resilience against malicious parties and data privacy are essential for trustworthy distributed learning, yet achieving both with good utility typically requires the strong assumption of a trusted central server. This paper shows that a significantly weaker assumption suffices: each pair of workers shares a randomness seed unknown to others. In a setting where malicious workers may collude with an untrusted server, we propose CafCor, an algorithm that integrates robust gradient aggregation with correlated noise injection, leveraging shared randomness between workers. We prove that CafCor achieves strong privacy-utility trade-offs, significantly outperforming local differential privacy (DP) methods, which do not make any trust assumption, while approaching central DP utility, where the server is fully trusted. Empirical results on standard benchmarks validate CafCor's practicality, showing that privacy and robustness can coexist in distributed systems without sacrificing utility or trusting the server.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01874v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youssef Allouah, Rachid Guerraoui, John Stephan</dc:creator>
    </item>
    <item>
      <title>Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes</title>
      <link>https://arxiv.org/abs/2505.02184</link>
      <description>arXiv:2505.02184v1 Announce Type: cross 
Abstract: While large language models (LLMs) are increasingly used for generating parallel scientific code, most current efforts emphasize functional correctness, often overlooking performance and energy considerations. In this work, we propose LASSI-EE, an automated LLM-based refactoring framework that generates energy-efficient parallel code on a target parallel system for a given parallel code as input. Through a multi-stage, iterative pipeline process, LASSI-EE achieved an average energy reduction of 47% across 85% of the 20 HeCBench benchmarks tested on NVIDIA A100 GPUs. Our findings demonstrate the broader potential of LLMs, not only for generating correct code but also for enabling energy-aware programming. We also address key insights and limitations within the framework, offering valuable guidance for future improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02184v1</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Matthew T. Dearing, Yiheng Tao, Xingfu Wu, Zhiling Lan, Valerie Taylor</dc:creator>
    </item>
    <item>
      <title>Towards One-shot Federated Learning: Advances, Challenges, and Future Directions</title>
      <link>https://arxiv.org/abs/2505.02426</link>
      <description>arXiv:2505.02426v1 Announce Type: cross 
Abstract: One-shot FL enables collaborative training in a single round, eliminating the need for iterative communication, making it particularly suitable for use in resource-constrained and privacy-sensitive applications. This survey offers a thorough examination of One-shot FL, highlighting its distinct operational framework compared to traditional federated approaches. One-shot FL supports resource-limited devices by enabling single-round model aggregation while maintaining data locality. The survey systematically categorizes existing methodologies, emphasizing advancements in client model initialization, aggregation techniques, and strategies for managing heterogeneous data distributions. Furthermore, we analyze the limitations of current approaches, particularly in terms of scalability and generalization in non-IID settings. By analyzing cutting-edge techniques and outlining open challenges, this survey aspires to provide a comprehensive reference for researchers and practitioners aiming to design and implement One-shot FL systems, advancing the development and adoption of One-shot FL solutions in a real-world, resource-constrained scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02426v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flora Amato, Lingyu Qiu, Mohammad Tanveer, Salvatore Cuomo, Fabio Giampaolo, Francesco Piccialli</dc:creator>
    </item>
    <item>
      <title>HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models</title>
      <link>https://arxiv.org/abs/2505.02795</link>
      <description>arXiv:2505.02795v1 Announce Type: cross 
Abstract: Recently, large language models (LLMs) have achieved remarkable breakthroughs, revolutionizing the natural language processing domain and beyond. Due to immense parameter sizes, fine-tuning these models with private data for diverse downstream tasks has become mainstream. Though federated learning (FL) offers a promising solution for fine-tuning LLMs without sharing raw data, substantial computing costs hinder its democratization. Moreover, in real-world scenarios, private client devices often possess heterogeneous computing resources, further complicating LLM fine-tuning. To combat these challenges, we propose HSplitLoRA, a heterogeneous parameter-efficient fine-tuning (PEFT) framework built on split learning (SL) and low-rank adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on heterogeneous client devices. HSplitLoRA first identifies important weights based on their contributions to LLM training. It then dynamically configures the decomposition ranks of LoRA adapters for selected weights and determines the model split point according to varying computing budgets of client devices. Finally, a noise-free adapter aggregation mechanism is devised to support heterogeneous adapter aggregation without introducing noise. Extensive experiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks in training accuracy and convergence speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02795v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Lin, Yuxin Zhang, Zhe Chen, Zihan Fang, Xianhao Chen, Praneeth Vepakomma, Wei Ni, Jun Luo, Yue Gao</dc:creator>
    </item>
    <item>
      <title>OptiReduce: Resilient and Tail-Optimal AllReduce for Distributed Deep Learning in the Cloud</title>
      <link>https://arxiv.org/abs/2310.06993</link>
      <description>arXiv:2310.06993v2 Announce Type: replace 
Abstract: We present OptiReduce, a new collective-communication system for the cloud with bounded, predictable completion times for deep-learning jobs in the presence of varying computation (stragglers) and communication (congestion and gradient drops) variabilities. OptiReduce exploits the inherent resiliency and the stochastic nature of distributed deep-learning (DDL) training and fine-tuning to work with approximated (or lost) gradients -- providing an efficient balance between (tail) performance and the resulting accuracy of the trained models.
  Exploiting this domain-specific characteristic of DDL, OptiReduce introduces (1) mechanisms (e.g., unreliable bounded transport with adaptive timeout) to improve the DDL jobs' tail execution time, and (2) strategies (e.g., Transpose AllReduce and Hadamard Transform) to mitigate the impact of gradient drops on model accuracy. Our evaluation shows that OptiReduce achieves 70% and 30% faster time-to-accuracy (TTA), on average, when operating in shared, cloud environments (e.g., CloudLab) compared to Gloo and NCCL, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06993v2</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>In 22nd USENIX Symposium on Networked Systems Design and Implementation (NSDI 25), pp. 685-703. 2025</arxiv:journal_reference>
      <dc:creator>Ertza Warraich, Omer Shabtai, Khalid Manaa, Shay Vargaftik, Yonatan Piasetzky, Matty Kadosh, Lalith Suresh, Muhammad Shahbaz</dc:creator>
    </item>
    <item>
      <title>DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers</title>
      <link>https://arxiv.org/abs/2403.10266</link>
      <description>arXiv:2403.10266v4 Announce Type: replace 
Abstract: Scaling multi-dimensional transformers to long sequences is indispensable across various domains. However, the challenges of large memory requirements and slow speeds of such sequences necessitate sequence parallelism. All existing approaches fall under the category of embedded sequence parallelism, which are limited to shard along a single sequence dimension, thereby introducing significant communication overhead. However, the nature of multi-dimensional transformers involves independent calculations across multiple sequence dimensions. To this end, we propose Dynamic Sequence Parallelism (DSP) as a novel abstraction of sequence parallelism. DSP dynamically switches the parallel dimension among all sequences according to the computation stage with efficient resharding strategy. DSP offers significant reductions in communication costs, adaptability across modules, and ease of implementation with minimal constraints. Experimental evaluations demonstrate DSP's superiority over state-of-the-art embedded sequence parallelism methods by remarkable throughput improvements ranging from 32.2% to 10x, with less than 25% communication volume.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10266v4</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanlei Zhao, Shenggan Cheng, Chang Chen, Zangwei Zheng, Ziming Liu, Zheming Yang, Yang You</dc:creator>
    </item>
    <item>
      <title>Balancing Pipeline Parallelism with Vocabulary Parallelism</title>
      <link>https://arxiv.org/abs/2411.05288</link>
      <description>arXiv:2411.05288v2 Announce Type: replace 
Abstract: Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize a generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only a small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in a 5% to 51% improvement in throughput compared to naive approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism .</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05288v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Man Tsung Yeung, Penghui Qi, Min Lin, Xinyi Wan</dc:creator>
    </item>
    <item>
      <title>Distributed Maximum Flow in Planar Graphs</title>
      <link>https://arxiv.org/abs/2411.11718</link>
      <description>arXiv:2411.11718v3 Announce Type: replace 
Abstract: The dual of a planar graph $G$ is a planar graph $G^*$ that has a vertex for each face of $G$ and an edge for each pair of adjacent faces of $G$. The profound relationship between a planar graph and its dual has been the algorithmic basis for solving numerous (centralized) classical problems on planar graphs. In the distributed setting however, the only use of planar duality is for finding a recursive decomposition of $G$ [DISC 2017, STOC 2019].
  We extend the distributed algorithmic toolkit to work on the dual graph $G^*$. These tools can then facilitate various algorithms on $G$ by solving a suitable dual problem on $G^*$.
  Given a directed planar graph $G$ with positive and negative edge-lengths and hop-diameter $D$, our key result is an $\tilde{O}(D^2)$-round algorithm for Single Source Shortest Paths on $G^*$, which then implies $\tilde{O}(D^2)$-round algorithms for Maximum $st$-Flow and Directed Global Min-Cut on $G$. Prior to our work, no $\tilde{O}(\text{poly}(D))$-round algorithm was known for those problems. We further obtain a $D\cdot n^{o(1)}$-rounds $(1-\epsilon)$-approximation algorithm for Maximum $st$-Flow on $G$ when $G$ is undirected and $st$-planar. Finally, we give a near optimal $\tilde O(D)$-round algorithm for computing the weighted girth of $G$. The main challenges in our work are that $G^*$ is not the communication graph (e.g., a vertex of $G$ is mapped to multiple vertices of $G^*$), and that the diameter of $G^*$ can be much larger than $D$ (i.e., possibly by a linear factor). We overcome these challenges by carefully defining and maintaining subgraphs of the dual graph $G^*$ while applying the recursive decomposition on the primal graph $G$. The main technical difficulty, is that along the recursive decomposition, a face of $G$ gets shattered into (disconnected) components yet we still need to treat it as a dual node.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11718v3</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yaseen Abd-Elhaleem (University of Haifa), Michal Dory (University of Haifa), Merav Parter (Weizmann Institute of Science), Oren Weimann (University of Haifa)</dc:creator>
    </item>
    <item>
      <title>Decouple and Decompose: Scaling Resource Allocation with DeDe</title>
      <link>https://arxiv.org/abs/2412.11447</link>
      <description>arXiv:2412.11447v2 Announce Type: replace 
Abstract: Resource allocation is fundamental for cloud systems to ensure efficient resource sharing among tenants. However, the scale of such optimization problems has outgrown the capabilities of commercial solvers traditionally employed in production. To scale up resource allocation, prior approaches either tailor solutions to specific problems or rely on assumptions tied to particular workloads. In this work, we revisit real-world resource allocation problems and uncover a common underlying structure: a vast majority of these problems are inherently separable, i.e., they optimize the aggregate utility of individual resource and demand allocations, under separate constraints for each resource and each demand. Building on this insight, we develop DeDe, a general, scalable, and theoretically grounded framework for accelerating resource allocation through a "decouple and decompose" approach. DeDe systematically decouples entangled resource and demand constraints, thereby decomposing the overall optimization into alternating per-resource and per-demand allocations, which can then be solved efficiently and in parallel. We have implemented DeDe as a library extension to an open-source solver, maintaining a familiar user interface. Experimental results across three prominent resource allocation tasks -- traffic engineering, cluster scheduling, and load balancing -- demonstrate DeDe's substantial speedups and robust allocation quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11447v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiying Xu, Minlan Yu, Francis Y. Yan</dc:creator>
    </item>
    <item>
      <title>DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient MoE Inference</title>
      <link>https://arxiv.org/abs/2501.10375</link>
      <description>arXiv:2501.10375v2 Announce Type: replace 
Abstract: Mixture-of-Experts (MoE) models, though highly effective for various machine learning tasks, face significant deployment challenges on memory-constrained devices. While GPUs offer fast inference, their limited memory compared to CPUs means not all experts can be stored on the GPU simultaneously, necessitating frequent, costly data transfers from CPU memory, often negating GPU speed advantages. To address this, we present DAOP, an on-device MoE inference engine to optimize parallel GPU-CPU execution. DAOP dynamically allocates experts between CPU and GPU based on per-sequence activation patterns, and selectively pre-calculates predicted experts on CPUs to minimize transfer latency. This approach enables efficient resource utilization across various expert cache ratios while maintaining model accuracy through a novel graceful degradation mechanism. Comprehensive evaluations across various datasets show that DAOP outperforms traditional expert caching and prefetching methods by up to 8.20x and offloading techniques by 1.35x while maintaining accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10375v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujie Zhang, Shivam Aggarwal, Tulika Mitra</dc:creator>
    </item>
    <item>
      <title>Improving Efficiency in Near-State and State-Optimal Self-Stabilising Leader Election Population Protocols</title>
      <link>https://arxiv.org/abs/2502.01227</link>
      <description>arXiv:2502.01227v2 Announce Type: replace 
Abstract: We investigate leader election problem via ranking within self-stabilising population protocols. In this scenario, the agent's state space comprises $n$ rank states and $x$ extra states. The initial configuration of $n$ agents consists of arbitrary arrangements of rank and extra states, with the objective of self-ranking. Specifically, each agent is tasked with stabilising in a unique rank state silently, implying that after stabilisation, each agent remains in its designated state indefinitely.
  In this paper, we present several new self-stabilising ranking protocols, greatly enriching our comprehension of these intricate problems. All protocols ensure self-stabilisation time with high probability (whp), defined as $1-n^{-\eta},$ for a constant $\eta&gt;0.$ We delve into three scenarios, from which we derive stable (always correct), either state-optimal or almost state-optimal, silent ranking protocols that self-stabilise within a time frame of $o(n^2)$ whp, including:
  - Utilising a novel concept of an agent trap, we derive a state-optimal ranking protocol that achieves self-stabilisation in time $O(min(kn^{3/2},n^2\log^2 n)),$ for any $k$-distant starting configuration.
  - Furthermore, we show that the incorporation of a single extra state ($x=1$) ensures a ranking protocol that self-stabilises in time $O(n^{7/4}\log^2 n)=o(n^2)$, regardless of the initial configuration.
  - Lastly, we show that extra $x=O(\log n)$ states admit self-stabilising ranking with the best currently known stabilisation time $O(n\log n)$, when whp and $x=O(\log n)$ guarantees are imposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01227v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leszek G\k{a}sieniec, Tytus Grodzicki, Grzegorz Stachowiak</dc:creator>
    </item>
    <item>
      <title>Extending Asynchronous Byzantine Agreement with Crusader Agreement</title>
      <link>https://arxiv.org/abs/2502.02320</link>
      <description>arXiv:2502.02320v3 Announce Type: replace 
Abstract: In this work, we study multivalued byzantine agreement (BA) in an asynchronous network of $n$ parties where up to $t &lt; \frac{n}{3}$ parties are byzantine. We present a new reduction from multivalued BA to binary BA. It allows one to achieve BA on $\ell$-bit inputs with one instance of binary BA, one instance of crusader agreement (CA) on $\ell$-bit inputs and $\Theta(\ell n + n^2)$ bits of additional communication.
  As our reduction uses multivalued CA, we also design two new information-theoretic CA protocols for $\ell$-bit inputs. In the first one, we use almost-universal hashing to achieve statistical security with probability $1 - 2^{-\lambda}$ against $t &lt; \frac{n}{3}$ faults with $\Theta(\ell n + n^2(\lambda + \log n))$ bits of communication. Following this, we replace the hashes with error correcting code symbols and add a preliminary step based on the synchronous multivalued BA protocol COOL [DISC '21] to obtain a second, perfectly secure CA protocol that can for any $\varepsilon &gt; 0$ be set to tolerate $t \leq \frac{n}{3 + \varepsilon}$ faults with $\mathcal{O}\bigl(\frac{\ell n}{\min(1, \varepsilon^2)} + n^2\max\bigl(1, \log \frac{1}{\varepsilon}\bigr) \bigr)$ bits of communication. Our CA protocols allow one to extend binary BA to multivalued BA with a constant round overhead, a quadratic-in-$n$ communication overhead, and information-theoretic security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02320v3</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mose Mizrahi Erbes, Roger Wattenhofer</dc:creator>
    </item>
    <item>
      <title>Performance Trade-offs of High Order Meshless Approximation on Distributed Memory Systems</title>
      <link>https://arxiv.org/abs/2502.12878</link>
      <description>arXiv:2502.12878v2 Announce Type: replace 
Abstract: Meshless methods approximate operators in a specific node as a weighted sum of values in its neighbours. Higher order approximations of derivatives provide more accurate solutions with better convergence characteristics, but they come at the cost of including more neighbours. On the accuracy-per-compute time basis we know that increasing the approximation order is beneficial for a shared memory computer, but there is additional communication overhead when problems become too large and we have to resort to distributed memory systems. Meshless nodes are divided between systems in spatially coherent subdomains with approximations at their edges requiring neighbouring value exchange. Performance optimization is then a balancing act between minimizing the required number of communicated neighbours by lowering the approximation order or increasing it to enable faster convergence. We use the radial basis function-generated finite difference method (RBF-FD) to approximate the derivatives that we use to solve the Poisson equation with an explicit iterative scheme. Inter-system communication is provided by Open MPI, while OpenMP is used for intra-system parallelisation. We perform the analysis on a homogenous CPU-based cluster where we examine the behaviour and attempt to determine the optimal parameterisation with the goal of minimizing the computational time to reach a desired accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12878v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jon Vehovar, Miha Rot, Gregor Kosec</dc:creator>
    </item>
    <item>
      <title>SPD: Sync-Point Drop for efficient tensor parallelism of Large Language Models</title>
      <link>https://arxiv.org/abs/2502.20727</link>
      <description>arXiv:2502.20727v2 Announce Type: replace 
Abstract: With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from popular distributed inference techniques such as Tensor Parallelism pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization technique, Sync-Point Drop (SPD), to reduce communication overheads in tensor parallelism by selectively dropping synchronization on attention outputs. In detail, we first propose a block design that allows execution to proceed without communication through SPD. Second, we apply different SPD strategies to attention blocks based on their sensitivity to the model accuracy. The proposed methods effectively alleviate communication bottlenecks while minimizing accuracy degradation during LLM inference, offering a scalable solution for diverse distributed environments: SPD offered about 20% overall inference latency reduction with &lt; 1% accuracy regression for LLaMA2-70B inference over 8 GPUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20727v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Han-Byul Kim, Duc Hoang, Arnav Kundu, Mohammad Samragh, Minsik Cho</dc:creator>
    </item>
    <item>
      <title>A Survey on the Landscape of Self-adaptive Cloud Design and Operations Patterns: Goals, Strategies, Tooling, Evaluation and Dataset Perspectives</title>
      <link>https://arxiv.org/abs/2503.06705</link>
      <description>arXiv:2503.06705v2 Announce Type: replace 
Abstract: Cloud-native applications have significantly advanced the development and scalability of online services through the use of microservices and modular architectures. However, achieving adaptability, resilience, and efficient performance management within cloud environments remains a key challenge. This survey provides an overview of self-adaptive cloud design and operations patterns published over the last seven years, focusing on a taxonomy of their objectives, scope of control, decision-making mechanisms approach, automation level and validation methodologies. Overall, 96 papers have been taken under consideration, indicating a significant increase in the years since 2023 in the produced output. The analysis highlights the prevalence of feedback loop structures, with both reactive and proactive implementations, and underscores the increasing role of machine learning techniques in predictive management, especially when it comes to resource provisioning and management of the executed applications. On the other hand, adaptive application architectures through direct application-level pattern-based management seem significantly underrepresented in the current field of research, thus serving as an uninvestigated area for future research. Furthermore, the current work highlights practical aspects such as validation datasets per category (application, resource, network, etc.), tools, technologies and frameworks usage during the experimentation, in order to guide researchers in the validation process for comparative and robust experimentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06705v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Apostolos Angelis, George Kousiouris</dc:creator>
    </item>
    <item>
      <title>Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler</title>
      <link>https://arxiv.org/abs/2504.19442</link>
      <description>arXiv:2504.19442v2 Announce Type: replace 
Abstract: In this report, we propose Triton-distributed, an extension of existing Triton compiler, to overcome the programming challenges in distributed AI systems. Triton-distributed is the first compiler that supports native overlapping optimizations for distributed AI workloads, providing a good coverage of existing optimizations from different frameworks. First, we integrate communication primitives compliant with the OpenSHMEM standard into the compiler. This enables programmers to utilize these primitives with a higher-level Python programming model. Second, we illustrate how to achieve complex joint optimization of computation, memory access, and communication with the assistance of the compiler. In particular, we show how to use overlapping techniques to hide latency and present our compiler-based programming methods in both single-node and multi-node scenarios. Finally, we showcase the performance of the code generated by our compiler. In a test environment with up to 64 devices, our compiler can fully utilize heterogeneous communication and computation resources to provide effective overlapping and high performance. In many cases, the performance of the generated code can even outperform hand-optimized code. Moreover, the development difficulty and the time cost for development using our compiler are far less than those of low-level programming such as CUDA/C++, which clearly demonstrates significant productivity advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19442v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Size Zheng, Wenlei Bao, Qi Hou, Xuegui Zheng, Jin Fang, Chenhui Huang, Tianqi Li, Haojie Duanmu, Renze Chen, Ruifan Xu, Yifan Guo, Ningxin Zheng, Ziheng Jiang, Xinyi Di, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Liqiang Lu, Yun Liang, Jidong Zhai, Xin Liu</dc:creator>
    </item>
    <item>
      <title>Distributed Quantum Circuit Cutting for Hybrid Quantum-Classical High-Performance Computing</title>
      <link>https://arxiv.org/abs/2505.01184</link>
      <description>arXiv:2505.01184v2 Announce Type: replace 
Abstract: Most quantum computers today are constrained by hardware limitations, particularly the number of available qubits, causing significant challenges for executing large-scale quantum algorithms. Circuit cutting has emerged as a key technique to overcome these limitations by decomposing large quantum circuits into smaller subcircuits that can be executed independently and later reconstructed. In this work, we introduce Qdislib, a distributed and flexible library for quantum circuit cutting, designed to seamlessly integrate with hybrid quantum-classical high-performance computing (HPC) systems. Qdislib employs a graph-based representation of quantum circuits to enable efficient partitioning, manipulation and execution, supporting both wire cutting and gate cutting techniques. The library is compatible with multiple quantum computing libraries, including Qiskit and Qibo, and leverages distributed computing frameworks to execute subcircuits across CPUs, GPUs, and quantum processing units (QPUs) in a fully parallelized manner. We present a proof of concept demonstrating how Qdislib enables the distributed execution of quantum circuits across heterogeneous computing resources, showcasing its potential for scalable quantum-classical workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01184v2</guid>
      <category>cs.DC</category>
      <category>quant-ph</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mar Tejedor, Berta Casas, Javier Conejero, Alba Cervera-Lierta, Rosa M. Badia</dc:creator>
    </item>
    <item>
      <title>Towards Optimal Deterministic LOCAL Algorithms on Trees</title>
      <link>https://arxiv.org/abs/2505.01410</link>
      <description>arXiv:2505.01410v2 Announce Type: replace 
Abstract: While obtaining optimal algorithms for the most important problems in the LOCAL model has been one of the central goals in the area of distributed algorithms since its infancy, tight complexity bounds are elusive for many problems even when considering \emph{deterministic} complexities on \emph{trees}. We take a step towards remedying this issue by providing a way to relate the complexity of a problem $\Pi$ on trees to its truly local complexity, which is the (asymptotically) smallest function $f$ such that $\Pi$ can be solved in $O(f(\Delta)+\log^*n)$ rounds. More specifically, we develop a transformation that takes an algorithm $\mathcal A$ for $\Pi$ with a runtime of $O(f(\Delta)+\log^*n)$ rounds as input and transforms it into an $O(f(g(n))+\log^* n)$-round algorithm $\mathcal{A}'$ on trees, where $g$ is the function that satisfies $g(n)^{f(g(n))}=n$. If $f$ is the truly local complexity of $\Pi$ (i.e., if $\mathcal{A}$ is asymptotically optimal), then $\mathcal{A}'$ is an asymptotically optimal algorithm on trees, conditioned on a natural assumption on the nature of the worst-case instances of $\Pi$. Our transformation works for any member of a wide class of problems, including the most important symmetry-breaking problems. As an example of our transformation we obtain the first strongly sublogarithmic algorithm for $(\text{edge-degree+1})$-edge coloring (and therefore also $(2\Delta-1)$-edge coloring) on trees, exhibiting a runtime of $O(\log^{12/13} n)$ rounds. This breaks through the $\Omega(\log n/\log\log n)$-barrier that is a fundamental lower bound for other symmetry-breaking problems such as maximal independent set or maximal matching (that already holds on trees), and proves a separation between these problems and the aforementioned edge coloring problems on trees. We extend a subset of our results to graphs of bounded arboricity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01410v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Brandt, Ananth Narayanan</dc:creator>
    </item>
    <item>
      <title>When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions</title>
      <link>https://arxiv.org/abs/2306.15546</link>
      <description>arXiv:2306.15546v3 Announce Type: replace-cross 
Abstract: The intersection of Foundation Model (FM) and Federated Learning (FL) presents a unique opportunity to unlock new possibilities for real-world applications. On the one hand, FL, as a collaborative learning paradigm, help address challenges in FM development by expanding data availability, enabling computation sharing, facilitating the collaborative development of FMs, tackling continuous data update, avoiding FM monopoly, response delay and FM service down. On the other hand, FM, equipped with pre-trained knowledge and exceptional performance, can serve as a robust starting point for FL. It can also generate synthetic data to enrich data diversity and enhance overall performance of FL. Meanwhile, FM unlocks new sharing paradigm and multi-task and multi-modality capabilities for FL. By examining the interplay between FL and FM, this paper presents the motivations, challenges, and future directions of empowering FL with FM and empowering FM with FL. We hope that this work provides a good foundation to inspire future research efforts to drive advancements in both fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15546v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weiming Zhuang, Chen Chen, Jingtao Li, Chaochao Chen, Yaochu Jin, Lingjuan Lyu</dc:creator>
    </item>
    <item>
      <title>Bandwidth Efficient Livestreaming in Mobile Wireless Networks: A Peer-to-Peer ACIDE Solution</title>
      <link>https://arxiv.org/abs/2310.14283</link>
      <description>arXiv:2310.14283v3 Announce Type: replace-cross 
Abstract: In mobile wireless networks, livestreaming in high user density areas presents two typical challenges: the wireless bandwidth is depleted and the number of users is limited. In this study, a media distribution model utilizing peer to peer communications, Active Control in an Intelligent and Distributed Environment, is proposed for bandwidth efficient livestreaming. The basic idea is to group users with identical livestream interest in a cluster of n peers. Instead of sending n copies of a livestream package, only one copy is sent to the cluster. A package is divided into n blocks. Each user receives one block from the base station and the remaining n-1 blocks from the other peers. Two optimization problems are addressed. The first problem is minimizing the bandwidth needed to guarantee a continuous live media play on all peers. A solution is proposed to find the optimal block sizes such that the wireless bandwidth is minimized. The second problem is maximizing the number of peers admitted to a cluster, given a fixed wireless bandwidth. This problem is NP-complete and a greedy strategy is proposed to calculate a feasible solution for peer selection. The proposed model improves the bandwidth efficiency and allows more users to be served.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14283v3</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrei Negulescu, Weijia Shang</dc:creator>
    </item>
    <item>
      <title>LASSI: An LLM-based Automated Self-Correcting Pipeline for Translating Parallel Scientific Codes</title>
      <link>https://arxiv.org/abs/2407.01638</link>
      <description>arXiv:2407.01638v2 Announce Type: replace-cross 
Abstract: This paper addresses the problem of providing a novel approach to sourcing significant training data for LLMs focused on science and engineering. In particular, a crucial challenge is sourcing parallel scientific codes in the ranges of millions to billions of codes. To tackle this problem, we propose an automated pipeline framework called LASSI, designed to translate between parallel programming languages by bootstrapping existing closed- or open-source LLMs. LASSI incorporates autonomous enhancement through self-correcting loops where errors encountered during the compilation and execution of generated code are fed back to the LLM through guided prompting for debugging and refactoring. We highlight the bi-directional translation of existing GPU benchmarks between OpenMP target offload and CUDA to validate LASSI. The results of evaluating LASSI with different application codes across four LLMs demonstrate the effectiveness of LASSI for generating executable parallel codes, with 80% of OpenMP to CUDA translations and 85% of CUDA to OpenMP translations producing the expected output. We also observe approximately 78% of OpenMP to CUDA translations and 62% of CUDA to OpenMP translations execute within 10% of or at a faster runtime than the original benchmark code in the same language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01638v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.PL</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/CLUSTERWorkshops61563.2024.00029</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE International Conference on Cluster Computing Workshops (CLUSTER Workshops), pp. 136-143</arxiv:journal_reference>
      <dc:creator>Matthew T. Dearing, Yiheng Tao, Xingfu Wu, Zhiling Lan, Valerie Taylor</dc:creator>
    </item>
    <item>
      <title>Parallel Split Learning with Global Sampling</title>
      <link>https://arxiv.org/abs/2407.15738</link>
      <description>arXiv:2407.15738v3 Announce Type: replace-cross 
Abstract: Distributed deep learning in resource-constrained environments faces scalability and generalization challenges due to large effective batch sizes and non-identically distributed client data. We introduce a server-driven sampling strategy that maintains a fixed global batch size by dynamically adjusting client-side batch sizes. This decouples the effective batch size from the number of participating devices and ensures that global batches better reflect the overall data distribution. Using standard concentration bounds, we establish tighter deviation guarantees compared to existing approaches. Empirical results on a benchmark dataset confirm that the proposed method improves model accuracy, training efficiency, and convergence stability, offering a scalable solution for learning at the network edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15738v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Kohankhaki, Ahmad Ayad, Mahdi Barhoush, Anke Schmeink</dc:creator>
    </item>
  </channel>
</rss>
