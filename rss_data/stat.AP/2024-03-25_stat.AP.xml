<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Mar 2024 04:00:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Reducing Investigator Bias in Sampling-Based Land Cover Classification by Integrating Multiple Investigators' Maps Using a Multiple Classifier System</title>
      <link>https://arxiv.org/abs/2403.15720</link>
      <description>arXiv:2403.15720v1 Announce Type: new 
Abstract: Land cover classification plays a pivotal role in describing Earth's surface characteristics. However, these thematic classifications can be affected by uncertainties introduced by an investigator's bias. While land cover classification mapping is becoming easier for us due to the emergence of cloud geospatial platforms such as Google Earth Engine, such uncertainty is often overlooked. Thus, this study aimed to create a robust land cover classification map by reducing investigator-induced uncertainty from independent investigators' maps using a multiple classifier system. In Saitama City, Japan, as a case study, 44 investigators used a point-based visual interpretation method via Google Earth Engine to collect stratified reference samples across four different land cover classes: forest, agriculture, urban, and water. These samples were then used to train a random forest classifier, ultimately resulting in the creation of individual classification maps. We quantified pixel-level discrepancies in these maps, which came from inherent investigator-induced variability. To tackle these uncertainties, we developed a multiple classifier system incorporating K-Medoids to group the most reliable maps and minimize discrepancies. We further applied Bayesian analysis to these grouped maps to produce a unified, accurate classification map. This yielded an overall accuracy of 92.5\% for 400 independent validation samples. We discuss how our approach can also reduce salt-and-pepper noise, which is often found in individual classification maps. This research underscores the intrinsic uncertainties present in land cover classification maps attributable to investigator variations and introduces a potential solution to attenuate these variations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15720v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Narumasa Tsutsumida, Akira Kato</dc:creator>
    </item>
    <item>
      <title>A comparison of graphical methods in the case of the murder of Meredith Kercher</title>
      <link>https://arxiv.org/abs/2403.16628</link>
      <description>arXiv:2403.16628v1 Announce Type: new 
Abstract: We compare three graphical methods for displaying evidence in a legal case: Wigmore Charts, Bayesian Networks and Chain Event Graphs. We find that these methods are aimed at three distinct audiences, respectively lawyers, forensic scientists and the police. The methods are illustrated using part of the evidence in the case of the murder of Meredith Kercher. More specifically, we focus on representing the list of propositions, evidence, testimony and facts given in the first trial against Raffaele Sollecito and Amanda Knox with these graphical methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16628v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Philip Dawid, Francesco Dotto, Maxine Graves, Jay B. Kadane, Julia Mortera, Gail Robertson, Jim Q. Smith, Amy L. Wilson</dc:creator>
    </item>
    <item>
      <title>Efficient Weighting Schemes for Auditing Instant-Runoff Voting Elections</title>
      <link>https://arxiv.org/abs/2403.15400</link>
      <description>arXiv:2403.15400v1 Announce Type: cross 
Abstract: Various risk-limiting audit (RLA) methods have been developed for instant-runoff voting (IRV) elections. A recent method, AWAIRE, is the first efficient approach that does not require cast vote records (CVRs). AWAIRE involves adaptively weighted averages of test statistics, essentially "learning" an effective set of hypotheses to test. However, the initial paper on AWAIRE only examined a few weighting schemes and parameter settings.
  We provide an extensive exploration of schemes and settings, to identify and recommend efficient choices for practical use. We focus only on the (hardest) case where CVRs are not available, using simulations based on real election data to assess performance.
  Across our comparisons, the most effective schemes are often those that place most or all of the weight on the apparent "best" hypotheses based on already seen data. Conversely, the optimal tuning parameters tended to vary based on the election margin. Nonetheless, we quantify the performance trade-offs for different choices across varying election margins, aiding in selecting the most desirable trade-off if a default option is needed.
  A limitation of the current AWAIRE implementation is its restriction to handling a small number of candidates (previously demonstrated up to six candidates). One path to a more computationally efficient implementation would be to use lazy evaluation and avoid considering all possible hypotheses. Our findings suggest that such an approach could be done without substantially comprising statistical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15400v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Ek, Philip B. Stark, Peter J. Stuckey, Damjan Vukcevic</dc:creator>
    </item>
    <item>
      <title>Agile gesture recognition for low-power applications: customisation for generalisation</title>
      <link>https://arxiv.org/abs/2403.15421</link>
      <description>arXiv:2403.15421v1 Announce Type: cross 
Abstract: Automated hand gesture recognition has long been a focal point in the AI community. Traditionally, research in this field has predominantly focused on scenarios with access to a continuous flow of hand's images. This focus has been driven by the widespread use of cameras and the abundant availability of image data. However, there is an increasing demand for gesture recognition technologies that operate on low-power sensor devices. This is due to the rising concerns for data leakage and end-user privacy, as well as the limited battery capacity and the computing power in low-cost devices. Moreover, the challenge in data collection for individually designed hardware also hinders the generalisation of a gesture recognition model.
  In this study, we unveil a novel methodology for pattern recognition systems using adaptive and agile error correction, designed to enhance the performance of legacy gesture recognition models on devices with limited battery capacity and computing power. This system comprises a compact Support Vector Machine as the base model for live gesture recognition. Additionally, it features an adaptive agile error corrector that employs few-shot learning within the feature space induced by high-dimensional kernel mappings. The error corrector can be customised for each user, allowing for dynamic adjustments to the gesture prediction based on their movement patterns while maintaining the agile performance of its base model on a low-cost and low-power micro-controller. This proposed system is distinguished by its compact size, rapid processing speed, and low power consumption, making it ideal for a wide range of embedded systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15421v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ying Liu, Liucheng Guo, Valeri A. Makarovc, Alexander Gorbana, Evgeny Mirkesa, Ivan Y. Tyukin</dc:creator>
    </item>
    <item>
      <title>Emotion Detection with Transformers: A Comparative Study</title>
      <link>https://arxiv.org/abs/2403.15454</link>
      <description>arXiv:2403.15454v1 Announce Type: cross 
Abstract: In this study, we explore the application of transformer-based models for emotion classification on text data. We train and evaluate several pre-trained transformer models, on the Emotion dataset using different variants of transformers. The paper also analyzes some factors that in-fluence the performance of the model, such as the fine-tuning of the transformer layer, the trainability of the layer, and the preprocessing of the text data. Our analysis reveals that commonly applied techniques like removing punctuation and stop words can hinder model performance. This might be because transformers strength lies in understanding contextual relationships within text. Elements like punctuation and stop words can still convey sentiment or emphasis and removing them might disrupt this context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15454v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mahdi Rezapour</dc:creator>
    </item>
    <item>
      <title>Computationally Scalable Bayesian SPDE Modeling for Censored Spatial Responses</title>
      <link>https://arxiv.org/abs/2403.15670</link>
      <description>arXiv:2403.15670v1 Announce Type: cross 
Abstract: Observations of groundwater pollutants, such as arsenic or Perfluorooctane sulfonate (PFOS), are riddled with left censoring. These measurements have impact on the health and lifestyle of the populace. Left censoring of these spatially correlated observations are usually addressed by applying Gaussian processes (GPs), which have theoretical advantages. However, this comes with a challenging computational complexity of $\mathcal{O}(n^3)$, which is impractical for large datasets. Additionally, a sizable proportion of the data being left-censored creates further bottlenecks, since the likelihood computation now involves an intractable high-dimensional integral of the multivariate Gaussian density. In this article, we tackle these two problems simultaneously by approximating the GP with a Gaussian Markov random field (GMRF) approach that exploits an explicit link between a GP with Mat\'ern correlation function and a GMRF using stochastic partial differential equations (SPDEs). We introduce a GMRF-based measurement error into the model, which alleviates the likelihood computation for the censored data, drastically improving the speed of the model while maintaining admirable accuracy. Our approach demonstrates robustness and substantial computational scalability, compared to state-of-the-art methods for censored spatial responses across various simulation settings. Finally, the fit of this fully Bayesian model to the concentration of PFOS in groundwater available at 24,959 sites across California, where 46.62\% responses are censored, produces prediction surface and uncertainty quantification in real time, thereby substantiating the applicability and scalability of the proposed method. Code for implementation is made available via GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15670v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Indranil Sahoo, Suman Majumder, Arnab Hazra, Ana G. Rappold, Dipankar Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Optimized Model Selection for Estimating Treatment Effects from Costly Simulations of the US Opioid Epidemic</title>
      <link>https://arxiv.org/abs/2403.15755</link>
      <description>arXiv:2403.15755v1 Announce Type: cross 
Abstract: Agent-based simulation with a synthetic population can help us compare different treatment conditions while keeping everything else constant within the same population (i.e., as digital twins). Such population-scale simulations require large computational power (i.e., CPU resources) to get accurate estimates for treatment effects. We can use meta models of the simulation results to circumvent the need to simulate every treatment condition. Selecting the best estimating model at a given sample size (number of simulation runs) is a crucial problem. Depending on the sample size, the ability of the method to estimate accurately can change significantly. In this paper, we discuss different methods to explore what model works best at a specific sample size. In addition to the empirical results, we provide a mathematical analysis of the MSE equation and how its components decide which model to select and why a specific method behaves that way in a range of sample sizes. The analysis showed why the direction estimation method is better than model-based methods in larger sample sizes and how the between-group variation and the within-group variation affect the MSE equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15755v1</guid>
      <category>stat.ME</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdulrahman A. Ahmed, M. Amin Rahimian, Mark S. Roberts</dc:creator>
    </item>
    <item>
      <title>Augmented Doubly Robust Post-Imputation Inference for Proteomic Data</title>
      <link>https://arxiv.org/abs/2403.15802</link>
      <description>arXiv:2403.15802v1 Announce Type: cross 
Abstract: Quantitative measurements produced by mass spectrometry proteomics experiments offer a direct way to explore the role of proteins in molecular mechanisms. However, analysis of such data is challenging due to the large proportion of missing values. A common strategy to address this issue is to utilize an imputed dataset, which often introduces systematic bias into downstream analyses if the imputation errors are ignored. In this paper, we propose a statistical framework inspired by doubly robust estimators that offers valid and efficient inference for proteomic data. Our framework combines powerful machine learning tools, such as variational autoencoders, to augment the imputation quality with high-dimensional peptide data, and a parametric model to estimate the propensity score for debiasing imputed outcomes. Our estimator is compatible with the double machine learning framework and has provable properties. In application to both single-cell and bulk-cell proteomic data our method utilizes the imputed data to gain additional, meaningful discoveries and yet maintains good control of false positives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15802v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haeun Moon, Jin-Hong Du, Jing Lei, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Electron-Tunnelling-Noise Programmable Random Variate Accelerator for Monte Carlo Sampling</title>
      <link>https://arxiv.org/abs/2403.16421</link>
      <description>arXiv:2403.16421v1 Announce Type: cross 
Abstract: This article presents an electron tunneling noise programmable random variate accelerator for accelerating the sampling stage of Monte Carlo simulations. We used the LiteX framework to generate a Petitbateau FemtoRV RISC-V instruction set soft processor and deploy it on a Digilent Arty-100T FPGA development board. The RISC-V soft processor augmented with our programmable random variate accelerator achieves an average speedup of 8.70 times and a median speedup of 8.68 times for a suite of twelve different benchmark applications when compared to GNU Scientific Library software random number generation. These speedups are achievable because the benchmarks spend an average of 90.0 % of their execution time generating random samples. The results of the Monte Carlo benchmark programs run over the programmable random variate accelerator have an average Wasserstein distance of 1.48 times and a median Wasserstein distance of 1.41 times$that of the results produced by the GNU Scientific Library random number generators. The soft processor samples the electron tunneling noise source using the hardened XADC block in the FPGA. The flexibility of the LiteX framework allows for the deployment of any LiteX-supported soft processor with an electron tunneling noise programmable random variate accelerator on any LiteX-supported development board that contains an FPGA with an XADC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16421v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>physics.comp-ph</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James T. Meech, Vasileios Tsoutsouras, Phillip Stanley-Marbell</dc:creator>
    </item>
    <item>
      <title>Extremal properties of max-autoregressive moving average processes for modelling extreme river flows</title>
      <link>https://arxiv.org/abs/2403.16590</link>
      <description>arXiv:2403.16590v1 Announce Type: cross 
Abstract: Max-autogressive moving average (Max-ARMA) processes are powerful tools for modelling time series data with heavy-tailed behaviour; these are a non-linear version of the popular autoregressive moving average models. River flow data typically have features of heavy tails and non-linearity, as large precipitation events cause sudden spikes in the data that then exponentially decay. Therefore, stationary Max-ARMA models are a suitable candidate for capturing the unique temporal dependence structure exhibited by river flows. This paper contributes to advancing our understanding of the extremal properties of stationary Max-ARMA processes. We detail the first approach for deriving the extremal index, the lagged asymptotic dependence coefficient, and an efficient simulation for a general Max-ARMA process. We use the extremal properties, coupled with the belief that Max-ARMA processes provide only an approximation to extreme river flow, to fit such a model which can broadly capture river flow behaviour over a high threshold. We make our inference under a reparametrisation which gives a simpler parameter space that excludes cases where any parameter is non-identifiable. We illustrate results for river flow data from the UK River Thames.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16590v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleanor D'Arcy, Jonathan A Tawn</dc:creator>
    </item>
    <item>
      <title>Use of mobile phone sensing data to estimate residence and mobility times in urban patches during the COVID-19 epidemic: The case of the 2020 outbreak in Hermosillo, Mexico</title>
      <link>https://arxiv.org/abs/2212.09963</link>
      <description>arXiv:2212.09963v2 Announce Type: replace 
Abstract: It is often necessary to introduce the main characteristics of population mobility dynamics to model critical social phenomena such as the economy, violence, transmission of information, or infectious diseases. In this work, we focus on modeling and inferring urban population mobility using the geospatial data of its inhabitants. The objective is to estimate mobility and times inhabitants spend in the areas of interest, such as zip codes and census geographical areas. The proposed method uses the Brownian bridge model for animal movement in ecology. We illustrate its possible applications using mobile phone GPS data in 2020 from the city of Hermosillo, Sonora, in Mexico. We incorporate the estimated residence-mobility matrix into a multi-patch compartmental SEIR model to assess the effect of mobility changes due to governmental interventions</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.09963v2</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L. Leticia Ram\'irez-Ram\'irez, Jos\'e A. Montoya, Jes\'us F. Espinoza, Chahak Mehta, Albert Orwa Akuno, Tan Bui-Thanh</dc:creator>
    </item>
    <item>
      <title>Estimating and Correcting Degree Ratio Bias in the Network Scale-up Method</title>
      <link>https://arxiv.org/abs/2305.04381</link>
      <description>arXiv:2305.04381v3 Announce Type: replace 
Abstract: The Network Scale-up Method (NSUM) uses social networks and answers to "How many X's do you know?" questions to estimate sizes of groups excluded by standard surveys. This paper addresses the bias caused by varying average social network sizes across populations, commonly referred to as the degree ratio bias. This bias is especially important for marginalized populations like sex workers and drug users, where members tend to have smaller social networks than the average person. We show how the degree ratio affects size estimates and provide a method to estimate degree ratios without collecting additional data. We demonstrate that our adjustment procedure improves the accuracy of NSUM size estimates using simulations and data from two data sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04381v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian Laga, Jessica P. Kunke, Tyler H. McCormick, Xiaoyue Niu</dc:creator>
    </item>
    <item>
      <title>Automated Reminders Reduce Incarceration for Missed Court Dates: Evidence from a Text Message Experiment</title>
      <link>https://arxiv.org/abs/2306.12389</link>
      <description>arXiv:2306.12389v3 Announce Type: replace 
Abstract: Millions of Americans must attend mandatory court dates every year. To boost appearance rates, jurisdictions nationwide are increasingly turning to automated reminders, but previous research offers mixed evidence on their effectiveness. In partnership with the Santa Clara County Public Defender Office, we randomly assigned 5,709 public defender clients to either receive automated text message reminders (treatment) or not receive reminders (control). We found that reminders reduced warrants issued for missed court dates by approximately 20%, with 12.1% of clients in the control condition issued a warrant compared to 9.7% of clients in the treatment condition. We further found that incarceration from missed court dates dropped by a similar amount, from 6.2% in the control condition to 4.8% in the treatment condition. Our results provide evidence that automated reminders can help people avoid the negative consequences of missing court.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.12389v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Chohlas-Wood, Madison Coots, Joe Nudell, Julian Nyarko, Emma Brunskill, Todd Rogers, Sharad Goel</dc:creator>
    </item>
    <item>
      <title>A Plot is Worth a Thousand Tests: Assessing Residual Diagnostics with the Lineup Protocol</title>
      <link>https://arxiv.org/abs/2308.05964</link>
      <description>arXiv:2308.05964v2 Announce Type: replace 
Abstract: Regression experts consistently recommend plotting residuals for model diagnosis, despite the availability of many numerical hypothesis test procedures designed to use residuals to assess problems with a model fit. Here we provide evidence for why this is good advice using data from a visual inference experiment. We show how conventional tests are too sensitive, which means that too often the conclusion would be that the model fit is inadequate. The experiment uses the lineup protocol which puts a residual plot in the context of null plots. This helps generate reliable and consistent reading of residual plots for better model diagnosis. It can also help in an obverse situation where a conventional test would fail to detect a problem with a model due to contaminated data. The lineup protocol also detects a range of departures from good residuals simultaneously. Supplemental materials for the article are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05964v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weihao Li, Dianne Cook, Emi Tanaka, Susan VanderPlas</dc:creator>
    </item>
    <item>
      <title>Individual claims reserving using the Aalen--Johansen estimator</title>
      <link>https://arxiv.org/abs/2311.07384</link>
      <description>arXiv:2311.07384v2 Announce Type: replace 
Abstract: We propose an individual claims reserving model based on the conditional Aalen-Johansen estimator, as developed in Bladt and Furrer (2023b). In our approach, we formulate a multi-state problem, where the underlying variable is the individual claim size, rather than time. The states in this model represent development periods, and we estimate the cumulative density function of individual claim sizes using the conditional Aalen-Johansen method as transition probabilities to an absorbing state. Our methodology reinterprets the concept of multi-state models and offers a strategy for modeling the complete curve of individual claim sizes. To illustrate our approach, we apply our model to both simulated and real datasets. Having access to the entire dataset enables us to support the use of our approach by comparing the predicted total final cost with the actual amount, as well as evaluating it in terms of the continuously ranked probability score.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07384v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Gabriele Pittarello</dc:creator>
    </item>
    <item>
      <title>Faster identification of faster Formula 1 drivers via time-rank duality</title>
      <link>https://arxiv.org/abs/2312.14637</link>
      <description>arXiv:2312.14637v2 Announce Type: replace 
Abstract: Two natural ways of modelling Formula 1 race outcomes are a probabilistic approach, based on the exponential distribution, and econometric modelling of the ranks. Both approaches lead to exactly soluble race-winning probabilities. Equating race-winning probabilities leads to a set of equivalent parametrisations. This time-rank duality is attractive theoretically and leads to quicker ways of dis-entangling driver and car level effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14637v2</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.econlet.2024.111671</arxiv:DOI>
      <arxiv:journal_reference>Economics Letters, Online First, 2024</arxiv:journal_reference>
      <dc:creator>John Fry, Tom Brighton, Silvio Fanzon</dc:creator>
    </item>
    <item>
      <title>Bias-Free Estimation of Signals on Top of Unknown Backgrounds</title>
      <link>https://arxiv.org/abs/2306.17667</link>
      <description>arXiv:2306.17667v2 Announce Type: replace-cross 
Abstract: We present a method for obtaining unbiased signal estimates in the presence of a significant unknown background, eliminating the need for a parametric model for the background itself. Our approach is based on a minimal set of conditions for observation and background estimators, which are typically satisfied in practical scenarios. To showcase the effectiveness of our method, we apply it to simulated data from the planned dielectric axion haloscope MADMAX.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.17667v2</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.CO</category>
      <category>hep-ex</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.nima.2024.169259</arxiv:DOI>
      <arxiv:journal_reference>NIM A 1063 (2024) 169259</arxiv:journal_reference>
      <dc:creator>Johannes Diehl, Jakob Knollm\"uller, Oliver Schulz</dc:creator>
    </item>
    <item>
      <title>Nonparametric Estimation and Comparison of Distance Distributions from Censored Data</title>
      <link>https://arxiv.org/abs/2311.02658</link>
      <description>arXiv:2311.02658v4 Announce Type: replace-cross 
Abstract: Transportation distance information is a powerful resource, but location records are often censored due to privacy concerns or regulatory mandates. We outline methods to approximate, sample from, and compare distributions of distances between censored location pairs, a task with applications to public health informatics, logistics, and more. We validate empirically via simulation and demonstrate applicability to practical geospatial data analysis tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02658v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas H. McCabe</dc:creator>
    </item>
    <item>
      <title>Bayesian Functional Analysis for Untargeted Metabolomics Data with Matching Uncertainty and Small Sample Sizes</title>
      <link>https://arxiv.org/abs/2312.03257</link>
      <description>arXiv:2312.03257v3 Announce Type: replace-cross 
Abstract: Untargeted metabolomics based on liquid chromatography-mass spectrometry technology is quickly gaining widespread application given its ability to depict the global metabolic pattern in biological samples. However, the data is noisy and plagued by the lack of clear identity of data features measured from samples. Multiple potential matchings exist between data features and known metabolites, while the truth can only be one-to-one matches. Some existing methods attempt to reduce the matching uncertainty, but are far from being able to remove the uncertainty for most features. The existence of the uncertainty causes major difficulty in downstream functional analysis. To address these issues, we develop a novel approach for Bayesian Analysis of Untargeted Metabolomics data (BAUM) to integrate previously separate tasks into a single framework, including matching uncertainty inference, metabolite selection, and functional analysis. By incorporating the knowledge graph between variables and using relatively simple assumptions, BAUM can analyze datasets with small sample sizes. By allowing different confidence levels of feature-metabolite matching, the method is applicable to datasets in which feature identities are partially known. Simulation studies demonstrate that, compared with other existing methods, BAUM achieves better accuracy in selecting important metabolites that tend to be functionally consistent and assigning confidence scores to feature-metabolite matches. We analyze a COVID-19 metabolomics dataset and a mouse brain metabolomics dataset using BAUM. Even with a very small sample size of 16 mice per group, BAUM is robust and stable. It finds pathways that conform to existing knowledge, as well as novel pathways that are biologically plausible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03257v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guoxuan Ma, Jian Kang, Tianwei Yu</dc:creator>
    </item>
    <item>
      <title>Extracting Emotion Phrases from Tweets using BART</title>
      <link>https://arxiv.org/abs/2403.14050</link>
      <description>arXiv:2403.14050v2 Announce Type: replace-cross 
Abstract: Sentiment analysis is a natural language processing task that aims to identify and extract the emotional aspects of a text. However, many existing sentiment analysis methods primarily classify the overall polarity of a text, overlooking the specific phrases that convey sentiment. In this paper, we applied an approach to sentiment analysis based on a question-answering framework. Our approach leverages the power of Bidirectional Autoregressive Transformer (BART), a pre-trained sequence-to-sequence model, to extract a phrase from a given text that amplifies a given sentiment polarity. We create a natural language question that identifies the specific emotion to extract and then guide BART to pay attention to the relevant emotional cues in the text. We use a classifier within BART to predict the start and end positions of the answer span within the text, which helps to identify the precise boundaries of the extracted emotion phrase. Our approach offers several advantages over most sentiment analysis studies, including capturing the complete context and meaning of the text and extracting precise token spans that highlight the intended sentiment. We achieved an end loss of 87% and Jaccard score of 0.61.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14050v2</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mahdi Rezapour</dc:creator>
    </item>
  </channel>
</rss>
