<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Feb 2025 02:54:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On interpolation problem for multidimensional harmonizable stable sequences with noise observations</title>
      <link>https://arxiv.org/abs/2502.15717</link>
      <description>arXiv:2502.15717v1 Announce Type: new 
Abstract: We consider the problem of optimal linear estimation of the functional $$A_N \vec{\xi} =\sum_{j = 0}^{N} (\vec{a}(j))^{\top} \vec{\xi}(j)$$ that depends on the unknown values $\vec{\xi}(j),j=0,1,\dots,N,$ of a vector-valued harmonizable symmetric $\alpha$-stable random sequence $\vec{\xi}(j)=\left \{ \xi_ {k} (j) \right \}_{k = 1} ^ {T}$, from observations of the sequence $\vec{\xi}(j)+\vec{\eta}(j)$ at points $j\in\mathbb Z\setminus\{0,1,\dots,N\}$. We consider the problem for mutually independent vector-valued harmonizable symmetric $\alpha$-stable random sequences $\vec{\xi}(j)=\left \{ \xi_ {k} (j) \right \}_{k = 1} ^ {T}$ and $\vec{\eta}(j)=\left \{ \xi_ {k} (j) \right \}_{k = 1} ^ {T}$ which have absolutely continuous spectral measures and the spectral densities $f(\theta)$ and $g(\theta)$ satisfying the minimality condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15717v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikhail Moklyachuk</dc:creator>
    </item>
    <item>
      <title>Universality of High-Dimensional Logistic Regression and a Novel CGMT under Block Dependence with Applications to Data Augmentation</title>
      <link>https://arxiv.org/abs/2502.15752</link>
      <description>arXiv:2502.15752v1 Announce Type: new 
Abstract: Over the last decade, a wave of research has characterized the exact asymptotic risk of many high-dimensional models in the proportional regime. Two foundational results have driven this progress: Gaussian universality, which shows that the asymptotic risk of estimators trained on non-Gaussian and Gaussian data is equivalent, and the convex Gaussian min-max theorem (CGMT), which characterizes the risk under Gaussian settings. However, these results rely on the assumption that the data consists of independent random vectors, an assumption that significantly limits its applicability to many practical setups. In this paper, we address this limitation by generalizing both results to the dependent setting. More precisely, we prove that Gaussian universality still holds for high-dimensional logistic regression under block dependence, and establish a novel CGMT framework that accommodates for correlation across both the covariates and observations. Using these results, we establish the impact of data augmentation, a widespread practice in deep learning, on the asymptotic risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15752v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Esmaili Mallory, Kevin Han Huang, Morgane Austern</dc:creator>
    </item>
    <item>
      <title>Counting communities in weighted Stochastic Block Models via semidefinite programming</title>
      <link>https://arxiv.org/abs/2502.15891</link>
      <description>arXiv:2502.15891v1 Announce Type: new 
Abstract: We consider the problem of estimating the number of communities in a weighted balanced Stochastic Block Model. We construct hypothesis tests based on semidefinite programming and with a statistic coming from a GOE matrix to distinguish between any two candidate numbers of communities. This is possible due to a universality result for a semidefinite programming-based function that we also prove. The tests are then used to form a sequential test to estimate the number of communities. Furthermore, we also construct estimators of the communities themselves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15891v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deborah Oliveira, Andressa Cerqueira, Roberto Oliveira</dc:creator>
    </item>
    <item>
      <title>A frequentist local false discovery rate</title>
      <link>https://arxiv.org/abs/2502.16005</link>
      <description>arXiv:2502.16005v1 Announce Type: new 
Abstract: The local false discovery rate (lfdr) of Efron et al. (2001) enjoys major conceptual and decision-theoretic advantages over the false discovery rate (FDR) as an error criterion in multiple testing, but is only well-defined in Bayesian models where the truth status of each null hypothesis is random. We define a frequentist counterpart to the lfdr based on the relative frequency of nulls at each point in the sample space. The frequentist lfdr is defined without reference to any prior, but preserves several important properties of the Bayesian lfdr: For continuous test statistics, $\text{lfdr}(t)$ gives the probability, conditional on observing some statistic equal to $t$, that the corresponding null hypothesis is true. Evaluating the lfdr at an individual test statistic also yields a calibrated forecast of whether its null hypothesis is true. Finally, thresholding the lfdr at $\frac{1}{1+\lambda}$ gives the best separable rejection rule under the weighted classification loss where Type I errors are $\lambda$ times as costly as Type II errors. The lfdr can be estimated efficiently using parametric or non-parametric methods, and a closely related error criterion can be provably controlled in finite samples under independence assumptions. Whereas the FDR measures the average quality of all discoveries in a given rejection region, our lfdr measures how the quality of discoveries varies across the rejection region, allowing for a more fine-grained analysis without requiring the introduction of a prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16005v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Xiang, Jake A. Soloff, William Fithian</dc:creator>
    </item>
    <item>
      <title>A Generalisation of Ville's Inequality to Monotonic Lower Bounds and Thresholds</title>
      <link>https://arxiv.org/abs/2502.16019</link>
      <description>arXiv:2502.16019v1 Announce Type: new 
Abstract: Essentially all anytime-valid methods hinge on Ville's inequality to gain validity across time without incurring a union bound. Ville's inequality is a proper generalisation of Markov's inequality. It states that a non-negative supermartingale will only ever reach a multiple of its initial value with small probability. In the classic rendering both the lower bound (of zero) and the threshold are constant in time. We generalise both to monotonic curves. That is, we bound the probability that a supermartingale which remains above a given decreasing curve exceeds a given increasing threshold curve. We show our bound is tight by exhibiting a supermartingale for which the bound is an equality. Using our generalisation, we derive a clean finite-time version of the law of the iterated logarithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16019v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wouter M. Koolen, Muriel Felipe P\'erez-Ortiz, Tyron Lardy</dc:creator>
    </item>
    <item>
      <title>Monotonicity Testing of High-Dimensional Distributions with Subcube Conditioning</title>
      <link>https://arxiv.org/abs/2502.16355</link>
      <description>arXiv:2502.16355v1 Announce Type: new 
Abstract: We study monotonicity testing of high-dimensional distributions on $\{-1,1\}^n$ in the model of subcube conditioning, suggested and studied by Canonne, Ron, and Servedio~\cite{CRS15} and Bhattacharyya and Chakraborty~\cite{BC18}. Previous work shows that the \emph{sample complexity} of monotonicity testing must be exponential in $n$ (Rubinfeld, Vasilian~\cite{RV20}, and Aliakbarpour, Gouleakis, Peebles, Rubinfeld, Yodpinyanee~\cite{AGPRY19}). We show that the subcube \emph{query complexity} is $\tilde{\Theta}(n/\varepsilon^2)$, by proving nearly matching upper and lower bounds. Our work is the first to use directed isoperimetric inequalities (developed for function monotonicity testing) for analyzing a distribution testing algorithm. Along the way, we generalize an inequality of Khot, Minzer, and Safra~\cite{KMS18} to real-valued functions on $\{-1,1\}^n$.
  We also study uniformity testing of distributions that are promised to be monotone, a problem introduced by Rubinfeld, Servedio~\cite{RS09} , using subcube conditioning. We show that the query complexity is $\tilde{\Theta}(\sqrt{n}/\varepsilon^2)$. Our work proves the lower bound, which matches (up to poly-logarithmic factors) the uniformity testing upper bound for general distributions (Canonne, Chen, Kamath, Levi, Waingarten~\cite{CCKLW21}). Hence, we show that monotonicity does not help, beyond logarithmic factors, in testing uniformity of distributions with subcube conditional queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16355v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deeparnab Chakrabarty, Xi Chen, Simeon Ristic, C. Seshadhri, Erik Waingarten</dc:creator>
    </item>
    <item>
      <title>On the asymptotic validity of confidence sets for linear functionals of solutions to integral equations</title>
      <link>https://arxiv.org/abs/2502.16673</link>
      <description>arXiv:2502.16673v1 Announce Type: new 
Abstract: This paper examines the construction of confidence sets for parameters defined as a linear functional of the solution to an integral equation involving conditional expectations. We show that any confidence set uniformly valid over a broad class of probability laws, allowing the integral equation to be arbitrarily ill-posed must have, with high probability under some laws, a diameter at least as large as the diameter of the parameter's range over the model. Additionally, we establish that uniformly consistent estimators of the parameter do not exist. We show that, consistent with the weak instruments literature, Wald confidence intervals are not uniformly valid. Furthermore, we argue that inverting the score test, a successful approach in that literature, does not extend to the broader class of parameters considered here. We present a method for constructing uniformly valid confidence sets in the special case where all variables are binary and discuss its limitations. Finally, we emphasize that developing uniformly valid confidence sets for the general class of parameters considered in this paper remains an open problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16673v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ezequiel Smucler, James M. Robins, Andrea Rotnitzky</dc:creator>
    </item>
    <item>
      <title>Minimax Decision Trees via Martingale Approximations</title>
      <link>https://arxiv.org/abs/2502.16758</link>
      <description>arXiv:2502.16758v1 Announce Type: new 
Abstract: We develop a martingale-based approach to constructing decision trees that efficiently approximate a target variable through recursive conditioning. We introduce MinimaxSplit, a novel splitting criterion that minimizes the worst-case variance at each step, and analyze its cyclic variant, proving an exponential error decay rate under mild conditions. Our analysis builds upon partition-based martingale approximations, providing new insights into their convergence behavior. Unlike traditional variance-based methods, MinimaxSplit avoids end-cut preference and performs well in noisy settings. We derive empirical risk bounds and also explore its integration into random forests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16758v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenyuan Zhang, Hengrui Luo</dc:creator>
    </item>
    <item>
      <title>Quasi-likelihood ratio test for jump-diffusion processes based on adaptive maximum likelihood inference</title>
      <link>https://arxiv.org/abs/2502.17058</link>
      <description>arXiv:2502.17058v1 Announce Type: new 
Abstract: In this paper, we consider parameter estimation and quasi-likelihood ratio tests for multidimensional jump-diffusion processes defined by stochastic differential equations. In general, simultaneous estimation faces challenges such as an increase of computational time for optimization and instability of estimation accuracy as the dimensionality of parameters grows. To address these issues, we propose an adaptive quasi-log likelihood function based on the joint quasi-log likelihood function introduced by Shimizu and Yoshida (2003, 2006) and Ogihara and Yoshida (2011). We then show that the resulting adaptive estimators possess consistency and asymptotic normality. Furthermore, we extend the joint quasi-log likelihood function proposed by Shimizu and Yoshida (2003, 2006) and Ogihara and Yoshida (2011) and construct a test statistic using the proposed adaptive estimators. We prove that the proposed test statistic converges in distribution to a $\chi^2$-distribution under the null hypothesis and that the associated test is consistent. Finally, we conduct numerical simulations using a specific jump-diffusion process model to examine the asymptotic behavior of the proposed adaptive estimators and test statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17058v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiromasa Nishikawa, Tetsuya Kawai, Masayuki Uchida</dc:creator>
    </item>
    <item>
      <title>The feasibility of multi-graph alignment: a Bayesian approach</title>
      <link>https://arxiv.org/abs/2502.17142</link>
      <description>arXiv:2502.17142v1 Announce Type: new 
Abstract: We establish thresholds for the feasibility of random multi-graph alignment in two models. In the Gaussian model, we demonstrate an "all-or-nothing" phenomenon: above a critical threshold, exact alignment is achievable with high probability, while below it, even partial alignment is statistically impossible. In the sparse Erd\H{o}s-R\'enyi model, we rigorously identify a threshold below which no meaningful partial alignment is possible and conjecture that above this threshold, partial alignment can be achieved. To prove these results, we develop a general Bayesian estimation framework over metric spaces, which provides insight into a broader class of high-dimensional statistical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17142v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louis Vassaux, Laurent Massouli\'e</dc:creator>
    </item>
    <item>
      <title>Multivariate R\'enyi inaccuracy measures based on copulas: properties and application</title>
      <link>https://arxiv.org/abs/2502.17215</link>
      <description>arXiv:2502.17215v1 Announce Type: new 
Abstract: We propose R\'enyi inaccuracy measure based on multivariate copula and multivariate survival copula, respectively dubbed as multivariate cumulative copula R\'enyi inaccuracy measure and multivariate survival copula R\'enyi inaccuracy measure. Bounds of multivariate cumulative copula R\'enyi inaccuracy and multivariate survival copula R\'enyi inaccuracy measures have been obtained using Fr\'echet-Hoeffding bound. We discuss the comparison studies of the multivariate cumulative copula R\'enyi inaccuracy and multivariate survival copula R\'enyi inaccuracy measures based on lower orthant and upper orthant orders. We have also proposed multivariate co-copula R\'enyi inaccuracy and multivariate dual copula R\'enyi inaccuracy measures based on multivariate co-copula and dual copula. Similar properties have been explored. Further, we propose semiparametric estimator of multivariate cumulative copula R\'enyi inaccuracy measure. A simulation study is performed to compute standard deviation, absolute bias and mean squared error of the proposed estimator. Finally, a data set is considered to show that the multivariate cumulative copula R\'enyi inaccuracy measure can be applied as a model (copula) selection criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17215v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shital Saha, Suchandan Kayal</dc:creator>
    </item>
    <item>
      <title>On the admissibility of bounds on the mean of discrete, scalar probability distributions from an iid sample</title>
      <link>https://arxiv.org/abs/2502.17223</link>
      <description>arXiv:2502.17223v1 Announce Type: new 
Abstract: We address the problem of producing a lower bound for the mean of a discrete probability distribution, with known support over a finite set of real numbers, from an iid sample of that distribution. Up to a constant, this is equivalent to bounding the mean of a multinomial distribution (with known support) from a sample of that distribution. Our main contribution is to characterize the complete set of admissible bound functions for any sample space, and to show that certain previously published bounds are admissible. We prove that the solution to each one of a set of simple-to-state optimization problems yields such an admissible bound. Single examples of such bounds, such as the trinomial bound by Miratrix and Stark [2009] have been previously published, but without an analysis of admissibility, and without a discussion of the full set of alternative admissible bounds. In addition to a variety of results about admissible bounds, we prove the non-existence of optimal bounds for sample spaces with supports of size greater than 1 and samples sizes greater than 1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17223v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erik Learned-Miller</dc:creator>
    </item>
    <item>
      <title>On a class of high dimensional linear regression methods with debiasing and thresholding</title>
      <link>https://arxiv.org/abs/2502.17261</link>
      <description>arXiv:2502.17261v1 Announce Type: new 
Abstract: In this paper, we introduce a unified framework, inspired by classical regularization theory, for designing and analyzing a broad class of linear regression approaches. Our framework encompasses traditional methods like least squares regression and Ridge regression, as well as innovative techniques, including seven novel regression methods such as Landweber and Showalter regressions. Within this framework, we further propose a class of debiased and thresholded regression methods to promote feature selection, particularly in terms of sparsity. These methods may offer advantages over conventional regression techniques, including Lasso, due to their ease of computation via a closed-form expression. Theoretically, we establish consistency results and Gaussian approximation theorems for this new class of regularization methods. Extensive numerical simulations further demonstrate that the debiased and thresholded counterparts of linear regression methods exhibit favorable finite sample performance and may be preferable in certain settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17261v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying-Ao Wang, Yunyi Zhang, Ye Zhang</dc:creator>
    </item>
    <item>
      <title>Exponential dimensional dependence in high-dimensional Hermite method of moments</title>
      <link>https://arxiv.org/abs/2502.17431</link>
      <description>arXiv:2502.17431v1 Announce Type: new 
Abstract: In this paper, we show exponential dimensional dependence for the Hermite method of moments as a statistical test for Gaussianity in the case of i.i.d. Gaussian variables, by constructing a lower bound for the the Kolmogorov-Smirnov distance and an upper bound for the convex distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17431v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Basse-O'Connor, David Kramer-Bang</dc:creator>
    </item>
    <item>
      <title>Bankruptcy analysis using images and convolutional neural networks (CNN)</title>
      <link>https://arxiv.org/abs/2502.15726</link>
      <description>arXiv:2502.15726v1 Announce Type: cross 
Abstract: The marketing departments of financial institutions strive to craft products and services that cater to the diverse needs of businesses of all sizes. However, it is evident upon analysis that larger corporations often receive a more substantial portion of available funds. This disparity arises from the relative ease of assessing the risk of default and bankruptcy in these more prominent companies. Historically, risk analysis studies have focused on data from publicly traded or stock exchange-listed companies, leaving a gap in knowledge about small and medium-sized enterprises (SMEs). Addressing this gap, this study introduces a method for evaluating SMEs by generating images for processing via a convolutional neural network (CNN). To this end, more than 10,000 images, one for each company in the sample, were created to identify scenarios in which the CNN can operate with higher assertiveness and reduced training error probability. The findings demonstrate a significant predictive capacity, achieving 97.8% accuracy, when a substantial number of images are utilized. Moreover, the image creation method paves the way for potential applications of this technique in various sectors and for different analytical purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15726v1</guid>
      <category>q-fin.RM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luiz Tavares, Jose Mazzon, Francisco Paletta, Fabio Barros</dc:creator>
    </item>
    <item>
      <title>PLS-based approach for fair representation learning</title>
      <link>https://arxiv.org/abs/2502.16263</link>
      <description>arXiv:2502.16263v1 Announce Type: cross 
Abstract: We revisit the problem of fair representation learning by proposing Fair Partial Least Squares (PLS) components. PLS is widely used in statistics to efficiently reduce the dimension of the data by providing representation tailored for the prediction. We propose a novel method to incorporate fairness constraints in the construction of PLS components. This new algorithm provides a feasible way to construct such features both in the linear and the non linear case using kernel embeddings. The efficiency of our method is evaluated on different datasets, and we prove its superiority with respect to standard fair PCA method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16263v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena M. De-Diego, Adri\'an Perez-Suay, Paula Gordaliza, Jean-Michel Loubes</dc:creator>
    </item>
    <item>
      <title>Rectifying Conformity Scores for Better Conditional Coverage</title>
      <link>https://arxiv.org/abs/2502.16336</link>
      <description>arXiv:2502.16336v1 Announce Type: cross 
Abstract: We present a new method for generating confidence sets within the split conformal prediction framework. Our method performs a trainable transformation of any given conformity score to improve conditional coverage while ensuring exact marginal coverage. The transformation is based on an estimate of the conditional quantile of conformity scores. The resulting method is particularly beneficial for constructing adaptive confidence sets in multi-output problems where standard conformal quantile regression approaches have limited applicability. We develop a theoretical bound that captures the influence of the accuracy of the quantile estimate on the approximate conditional validity, unlike classical bounds for conformal prediction methods that only offer marginal coverage. We experimentally show that our method is highly adaptive to the local data structure and outperforms existing methods in terms of conditional coverage, improving the reliability of statistical inference in various applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16336v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Plassier, Alexander Fishkov, Victor Dheur, Mohsen Guizani, Souhaib Ben Taieb, Maxim Panov, Eric Moulines</dc:creator>
    </item>
    <item>
      <title>Improved Margin Generalization Bounds for Voting Classifiers</title>
      <link>https://arxiv.org/abs/2502.16462</link>
      <description>arXiv:2502.16462v1 Announce Type: cross 
Abstract: In this paper we establish a new margin-based generalization bound for voting classifiers, refining existing results and yielding tighter generalization guarantees for widely used boosting algorithms such as AdaBoost (Freund and Schapire, 1997). Furthermore, the new margin-based generalization bound enables the derivation of an optimal weak-to-strong learner: a Majority-of-3 large-margin classifiers with an expected error matching the theoretical lower bound. This result provides a more natural alternative to the Majority-of-5 algorithm by (H\o gsgaard et al. 2024) , and matches the Majority-of-3 result by (Aden-Ali et al. 2024) for the realizable prediction model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16462v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikael M{\o}ller H{\o}gsgaard, Kasper Green Larsen</dc:creator>
    </item>
    <item>
      <title>Regularized zero-inflated Bernoulli regression model</title>
      <link>https://arxiv.org/abs/2502.16574</link>
      <description>arXiv:2502.16574v1 Announce Type: cross 
Abstract: Logistic regression model is widely used in many studies to investigate the relationship between a binary response variable Y and a set of potential predictors $X_1,\ldots, X_p$ (for example: $Y = 1$ if the outcome occurred and $Y = 0$ otherwise). One problem arising then is that, a proportion of the study subjects cannot experience the outcome of interest. This leads to an excessive presence of zeros in the study sample. This article is interested in estimating parameters of the zero-inflated Bernouilli regression model in a high-dimensional setting, i.e. with a large number of regressors. We use particulary Ridge regression and the Lasso which are typically achieved by constraining the weights of the model. and are useful when the number of predictors is much bigger than the number of observations. We establish the existency, consistency and asymptotic normality of the proposed regularized estimator. Then, we conduct a simulation study to investigate its finite-sample behavior, and application to real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16574v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mouhamed Ndoye, Aba Diop</dc:creator>
    </item>
    <item>
      <title>Sharp Concentration of Simple Random Tensors</title>
      <link>https://arxiv.org/abs/2502.16916</link>
      <description>arXiv:2502.16916v1 Announce Type: cross 
Abstract: This paper establishes sharp dimension-free concentration inequalities and expectation bounds for the deviation of the sum of simple random tensors from its expectation. As part of our analysis, we use generic chaining techniques to obtain a sharp high-probability upper bound on the suprema of multi-product empirical processes. In so doing, we generalize classical results for quadratic and product empirical processes to higher-order settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16916v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Al-Ghattas, Jiaheng Chen, Daniel Sanz-Alonso</dc:creator>
    </item>
    <item>
      <title>Convergence of Shallow ReLU Networks on Weakly Interacting Data</title>
      <link>https://arxiv.org/abs/2502.16977</link>
      <description>arXiv:2502.16977v1 Announce Type: cross 
Abstract: We analyse the convergence of one-hidden-layer ReLU networks trained by gradient flow on $n$ data points. Our main contribution leverages the high dimensionality of the ambient space, which implies low correlation of the input samples, to demonstrate that a network with width of order $\log(n)$ neurons suffices for global convergence with high probability. Our analysis uses a Polyak-{\L}ojasiewicz viewpoint along the gradient-flow trajectory, which provides an exponential rate of convergence of $\frac{1}{n}$. When the data are exactly orthogonal, we give further refined characterizations of the convergence speed, proving its asymptotic behavior lies between the orders $\frac{1}{n}$ and $\frac{1}{\sqrt{n}}$, and exhibiting a phase-transition phenomenon in the convergence rate, during which it evolves from the lower bound to the upper, and in a relative time of order $\frac{1}{\log(n)}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16977v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'eo Dana (SIERRA), Francis Bach (SIERRA), Loucas Pillaud-Vivien (ENPC, CERMICS)</dc:creator>
    </item>
    <item>
      <title>Invariance principle for the Gaussian Multiplicative Chaos via a high dimensional CLT with low rank increments</title>
      <link>https://arxiv.org/abs/2502.17412</link>
      <description>arXiv:2502.17412v1 Announce Type: cross 
Abstract: Gaussian multiplicative chaos (GMC) is a canonical random fractal measure obtained by exponentiating log-correlated Gaussian processes, first constructed in the seminal work of Kahane (1985). Since then it has served as an important building block in constructions of quantum field theories and Liouville quantum gravity. However, in many natural settings, non-Gaussian log-correlated processes arise. In this paper, we investigate the universality of GMC through an invariance principle. We consider the model of a random Fourier series, a process known to be log-correlated. While the Gaussian Fourier series has been a classical object of study, recently, the non-Gaussian counterpart was investigated and the associated multiplicative chaos constructed by Junnila in 2016. We show that the Gaussian and non-Gaussian variables can be coupled so that the associated chaos measures are almost surely mutually absolutely continuous throughout the entire sub-critical regime. This solves the main open problem from Kim and Kriechbaum (2024) who had earlier established such a result for a part of the regime. The main ingredient is a new high dimensional CLT for a sum of independent (but not i.i.d.) random vectors belonging to rank one subspaces with error bounds involving the isotropic properties of the covariance matrix of the sum, which we expect will find other applications. The proof relies on a path-wise analysis of Skorokhod embeddings as well as a perturbative result about square roots of positive semi-definite matrices which, surprisingly, appears to be new.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17412v1</guid>
      <category>math.PR</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mriganka Basu Roy Chowdhury, Shirshendu Ganguly</dc:creator>
    </item>
    <item>
      <title>Stronger Neyman Regret Guarantees for Adaptive Experimental Design</title>
      <link>https://arxiv.org/abs/2502.17427</link>
      <description>arXiv:2502.17427v1 Announce Type: cross 
Abstract: We study the design of adaptive, sequential experiments for unbiased average treatment effect (ATE) estimation in the design-based potential outcomes setting. Our goal is to develop adaptive designs offering sublinear Neyman regret, meaning their efficiency must approach that of the hindsight-optimal nonadaptive design. Recent work [Dai et al, 2023] introduced ClipOGD, the first method achieving $\widetilde{O}(\sqrt{T})$ expected Neyman regret under mild conditions. In this work, we propose adaptive designs with substantially stronger Neyman regret guarantees. In particular, we modify ClipOGD to obtain anytime $\widetilde{O}(\log T)$ Neyman regret under natural boundedness assumptions. Further, in the setting where experimental units have pre-treatment covariates, we introduce and study a class of contextual "multigroup" Neyman regret guarantees: Given any set of possibly overlapping groups based on the covariates, the adaptive design outperforms each group's best non-adaptive designs. In particular, we develop a contextual adaptive design with $\widetilde{O}(\sqrt{T})$ anytime multigroup Neyman regret. We empirically validate the proposed designs through an array of experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17427v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgy Noarov, Riccardo Fogliato, Martin Bertran, Aaron Roth</dc:creator>
    </item>
    <item>
      <title>Causal Structure Learning in Directed, Possibly Cyclic, Graphical Models</title>
      <link>https://arxiv.org/abs/2305.06127</link>
      <description>arXiv:2305.06127v2 Announce Type: replace 
Abstract: We consider the problem of learning a directed graph $G^\star$ from observational data. We assume that the distribution which gives rise to the samples is Markov and faithful to the graph $G^\star$ and that there are no unobserved variables. We do not rely on any further assumptions regarding the graph or the distribution of the variables. Particularly, we allow for directed cycles in $G^\star$ and work in the fully non-parametric setting. Given the set of conditional independence statements satisfied by the distribution, we aim to find a directed graph which satisfies the same $d$-separation statements as $G^\star$. We propose a hybrid approach consisting of two steps. We first find a partially ordered partition of the vertices of $G^\star$ by optimizing a certain score in a greedy fashion. We prove that any optimal partition uniquely characterizes the Markov equivalence class of $G^\star$. Given an optimal partition, we propose an algorithm for constructing a graph in the Markov equivalence class of $G^\star$ whose strongly connected components correspond to the elements of the partition, and which are partially ordered according to the partial order of the partition. Our algorithm comes in two versions -- one which is provably correct and another one which performs fast in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.06127v2</guid>
      <category>math.ST</category>
      <category>math.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pardis Semnani, Elina Robeva</dc:creator>
    </item>
    <item>
      <title>Some Generalized Information and Divergence Generating Functions: Properties, Estimation, Validation and Applications</title>
      <link>https://arxiv.org/abs/2401.04418</link>
      <description>arXiv:2401.04418v3 Announce Type: replace 
Abstract: We propose R\'enyi information generating function and discuss its properties. A connection between the R\'enyi information generating function and the diversity index is proposed for discrete type random variables. The relation between the R\'enyi information generating function and Shannon entropy of order $q&gt;0$ is established and several bounds are obtained. The R\'enyi information generating function of escort distribution is derived. Furthermore, we introduce R\'enyi divergence information generating function and discuss its effect under monotone transformations. We present non-parametric and parametric estimators of the R\'enyi information generating function. A simulation study is carried out and a real data relating to the failure times of electronic components is analyzed. A comparison study between the non-parametric and parametric estimators is made in terms of the standard deviation, absolute bias, and mean square error. We have observed superior performance for the newly proposed estimators. Some applications of the proposed R\'enyi information generating function and R\'enyi divergence information generating function are provided. For three coherent systems, we calculate the values of the R\'enyi information generating function and other well-established uncertainty measures and similar behaviour of the R\'enyi information generating function is observed. Further, a study regarding the usefulness of the R\'enyi divergence information generating function and R\'enyi information generating function as model selection criteria is conducted. Finally, three chaotic maps are considered and then used to establish a validation of the proposed information generating function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04418v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shital Saha, Suchandan Kayal, N. Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Gaussian Approximation for Lag-Window Estimators and the Construction of Confidence bands for the Spectral Density</title>
      <link>https://arxiv.org/abs/2407.12316</link>
      <description>arXiv:2407.12316v2 Announce Type: replace 
Abstract: In this paper we consider the construction of simultaneous confidence bands for the spectral density of a stationary time series using a Gaussian approximation for classical lag-window spectral density estimators evaluated at the set of all positive Fourier frequencies. The Gaussian approximation opens up the possibility to verify asymptotic validity of a multiplier bootstrap procedure and, even further, to derive the corresponding rate of convergence. A small simulation study sheds light on the finite sample properties of this bootstrap proposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12316v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jens-Peter Kreiss, Anne Leucht, Efstathios Paparoditis</dc:creator>
    </item>
    <item>
      <title>Rates of convergence of a binary classification procedure for time-homogeneous S.D.E paths</title>
      <link>https://arxiv.org/abs/2501.15926</link>
      <description>arXiv:2501.15926v2 Announce Type: replace 
Abstract: In the context of binary classification of trajectories generated by time-homogeneous stochastic differential equations, we consider a mixture model of two diffusion processes characterized by a stochastic differential equation whose drift coefficient depends on the class or label, which is modeled as a discrete random variable taking two possible values and whose diffusion coefficient is independent of the class. We assume that the drift and diffusion coefficients are unknown as well as the law of the discrete random variable that models the class. In this paper, we study the minimax convergence rate of the resulting nonparametric plug-in classifier under different sets of assumptions on the mixture model considered. As the plug-in classifier is based on nonparametric estimators of the coefficients of the mixture model, we also study a minimax convergence rate of the risk of estimation of the drift coefficients on the real line.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15926v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eddy Michel Ella Mintsa</dc:creator>
    </item>
    <item>
      <title>Minimax rates of convergence for the nonparametric estimation of the diffusion coefficient from time-homogeneous SDE paths</title>
      <link>https://arxiv.org/abs/2501.15933</link>
      <description>arXiv:2501.15933v2 Announce Type: replace 
Abstract: Consider a diffusion process X, solution of a time-homogeneous stochastic differential equation. We assume that the diffusion process X is observed at discrete times, at high frequency, which means that the time step tends toward zero. In addition, the drift and diffusion coefficients of the process X are assumed to be unknown. In this paper, we study the minimax rates of convergence of the nonparametric estimators of the square of the diffusion coefficient. Two observation schemes are considered depending on the estimation interval. The square of the diffusion coefficient is estimated on the real line from repeated observations of the process X, where the number of diffusion paths tends to infinity. For the case of a compact estimation interval, we study the nonparametric estimation of the square of the diffusion coefficient constructed from a single diffusion path on one side and from repeated observations on the other side, where the number of trajectories tends to infinity. In each of these cases, we establish minimax convergence rates of the risk of estimation of the diffusion coefficient over a space of Holder functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15933v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eddy Michel Ella Mintsa</dc:creator>
    </item>
    <item>
      <title>Finite sample bounds for barycenter estimation in geodesic spaces</title>
      <link>https://arxiv.org/abs/2502.14069</link>
      <description>arXiv:2502.14069v2 Announce Type: replace 
Abstract: We study the problem of estimating the barycenter of a distribution given i.i.d. data in a geodesic space. Assuming an upper curvature bound in Alexandrov's sense and a support condition ensuring the strong geodesic convexity of the barycenter problem, we establish finite-sample error bounds in expectation and with high probability. Our results generalize Hoeffding- and Bernstein-type concentration inequalities from Euclidean to geodesic spaces. Building on these concentration inequalities, we derive statistical guarantees for two efficient algorithms for the computation of barycenters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14069v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Victor-Emmanuel Brunel, Jordan Serres</dc:creator>
    </item>
    <item>
      <title>Exact Phase Transitions for Stochastic Block Models and Reconstruction on Trees</title>
      <link>https://arxiv.org/abs/2212.03362</link>
      <description>arXiv:2212.03362v2 Announce Type: replace-cross 
Abstract: In this paper we continue to rigorously establish the predictions in ground breaking work in statistical physics by Decelle, Krzakala, Moore, Zdeborov\'a (2011) regarding the block model, in particular in the case of $q=3$ and $q=4$ communities.
  We prove that for $q=3$ and $q=4$ there is no computational-statistical gap if the average degree is above some constant by showing it is information theoretically impossible to detect below the Kesten-Stigum bound. The proof is based on showing that for the broadcast process on Galton-Watson trees, reconstruction is impossible for $q=3$ and $q=4$ if the average degree is sufficiently large. This improves on the result of Sly (2009), who proved similar results for regular trees for $q=3$. Our analysis of the critical case $q=4$ provides a detailed picture showing that the tightness of the Kesten-Stigum bound in the antiferromagnetic case depends on the average degree of the tree. We also prove that for $q\geq 5$, the Kestin-Stigum bound is not sharp.
  Our results prove conjectures of Decelle, Krzakala, Moore, Zdeborov\'a (2011), Moore (2017), Abbe and Sandon (2018) and Ricci-Tersenghi, Semerjian, and Zdeborov\'a (2019). Our proofs are based on a new general coupling of the tree and graph processes and on a refined analysis of the broadcast process on the tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.03362v2</guid>
      <category>math.PR</category>
      <category>cs.SI</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elchanan Mossel, Allan Sly, Youngtak Sohn</dc:creator>
    </item>
    <item>
      <title>Detection and inference of changes in high-dimensional linear regression with non-sparse structures</title>
      <link>https://arxiv.org/abs/2402.06915</link>
      <description>arXiv:2402.06915v4 Announce Type: replace-cross 
Abstract: For data segmentation in high-dimensional linear regression settings, the regression parameters are often assumed to be sparse segment-wise, which enables many existing methods to estimate the parameters locally via $\ell_1$-regularised maximum likelihood-type estimation and then contrast them for change point detection. Contrary to this common practice, we show that the exact sparsity of neither regression parameters nor their differences, a.k.a.\ differential parameters, is necessary for consistency in multiple change point detection. In fact, both statistically and computationally, better efficiency is attained by a simple strategy that scans for large discrepancies in local covariance between the regressors and the response. We go a step further and propose a suite of tools for directly inferring about the differential parameters post-segmentation, which are applicable even when the regression parameters themselves are non-sparse. Theoretical investigations are conducted under general conditions permitting non-Gaussianity, temporal dependence and ultra-high dimensionality. Numerical results from simulated and macroeconomic datasets demonstrate the competitiveness and efficacy of the proposed methods. Implementation of all methods is provided in the R package \texttt{inferchange} on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06915v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haeran Cho, Tobias Kley, Housen Li</dc:creator>
    </item>
    <item>
      <title>Conditioning of Banach Space Valued Gaussian Random Variables: An Approximation Approach Based on Martingales</title>
      <link>https://arxiv.org/abs/2404.03453</link>
      <description>arXiv:2404.03453v4 Announce Type: replace-cross 
Abstract: We investigate the conditional distributions of two Banach space valued, jointly Gaussian random variables. In particular, we show that these conditional distributions are again Gaussian and that their means and covariances can be determined by a general finite dimensional approximation scheme. Here, it turns out that the covariance operators occurring in this scheme converge with respect to the nuclear norm and that the conditional probabilities converge weakly. Furthermore, we discuss how our approximation scheme can be implemented in several classes of important Banach spaces such as (reproducing kernel) Hilbert spaces, spaces of continuous functions, and other spaces consisting of functions. As an example, we then apply our general results to the case of continuous Gaussian processes that are conditioned to partial but infinite observations of their paths. Here we show that conditioning on sufficiently rich, increasing sets of finitely many observations leads to consistent approximations, that is, both the mean and covariance functions converge uniformly and the conditional probabilities converge weakly. Moreover, we discuss how these results improve our understanding of the popular Gaussian processes for machine learning. From a technical perspective our results are based upon a Banach space valued martingale approach for regular conditional probabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03453v4</guid>
      <category>math.PR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ingo Steinwart</dc:creator>
    </item>
    <item>
      <title>Reassessing How to Compare and Improve the Calibration of Machine Learning Models</title>
      <link>https://arxiv.org/abs/2406.04068</link>
      <description>arXiv:2406.04068v2 Announce Type: replace-cross 
Abstract: A machine learning model is calibrated if its predicted probability for an outcome matches the observed frequency for that outcome conditional on the model prediction. This property has become increasingly important as the impact of machine learning models has continued to spread to various domains. As a result, there are now a dizzying number of recent papers on measuring and improving the calibration of (specifically deep learning) models. In this work, we reassess the reporting of calibration metrics in the recent literature. We show that there exist trivial recalibration approaches that can appear seemingly state-of-the-art unless calibration and prediction metrics (i.e. test accuracy) are accompanied by additional generalization metrics such as negative log-likelihood. We then use a calibration-based decomposition of Bregman divergences to develop a new extension to reliability diagrams that jointly visualizes calibration and generalization error, and show how our visualization can be used to detect trade-offs between calibration and generalization. Along the way, we prove novel results regarding the relationship between full calibration error and confidence calibration error for Bregman divergences. We also establish the consistency of the kernel regression estimator for calibration error used in our visualization approach, which generalizes existing consistency results in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04068v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muthu Chidambaram, Rong Ge</dc:creator>
    </item>
    <item>
      <title>Information-Theoretic Thresholds for the Alignments of Partially Correlated Graphs</title>
      <link>https://arxiv.org/abs/2406.05428</link>
      <description>arXiv:2406.05428v2 Announce Type: replace-cross 
Abstract: This paper studies the problem of recovering the hidden vertex correspondence between two correlated random graphs. We propose the partially correlated Erd\H{o}s-R\'enyi graphs model, wherein a pair of induced subgraphs with a certain number are correlated. We investigate the information-theoretic thresholds for recovering the latent correlated subgraphs and the hidden vertex correspondence. We prove that there exists an optimal rate for partial recovery for the number of correlated nodes, above which one can correctly match a fraction of vertices and below which correctly matching any positive fraction is impossible, and we also derive an optimal rate for exact recovery. In the proof of possibility results, we propose correlated functional digraphs, which partition the edges of the intersection graph into two types of components, and bound the error probability by lower-order cumulant generating functions. The proof of impossibility results build upon the generalized Fano's inequality and the recovery thresholds settled in correlated Erd\H{o}s-R\'enyi graphs model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05428v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dong Huang, Xianwen Song, Pengkun Yang</dc:creator>
    </item>
    <item>
      <title>Weak recovery, hypothesis testing, and mutual information in stochastic block models and planted factor graphs</title>
      <link>https://arxiv.org/abs/2406.15957</link>
      <description>arXiv:2406.15957v2 Announce Type: replace-cross 
Abstract: The stochastic block model is a canonical model of communities in random graphs. It was introduced in the social sciences and statistics as a model of communities, and in theoretical computer science as an average case model for graph partitioning problems under the name of the ``planted partition model.'' Given a sparse stochastic block model, the two standard inference tasks are: (i) Weak recovery: can we estimate the communities with non trivial overlap with the true communities? (ii) Detection/Hypothesis testing: can we distinguish if the sample was drawn from the block model or from a random graph with no community structure with probability tending to $1$ as the graph size tends to infinity?
  In this work, we show that for sparse stochastic block models, the two inference tasks are equivalent except at a critical point. That is, weak recovery is information theoretically possible if and only if detection is possible. We thus find a strong connection between these two notions of inference for the model. We further prove that when detection is impossible, an explicit hypothesis test based on low degree polynomials in the adjacency matrix of the observed graph achieves the optimal statistical power. This low degree test is efficient as opposed to the likelihood ratio test, which is not known to be efficient. Moreover, we prove that the asymptotic mutual information between the observed network and the community structure exhibits a phase transition at the weak recovery threshold.
  Our results are proven in much broader settings including the hypergraph stochastic block models and general planted factor graphs. In these settings we prove that the impossibility of weak recovery implies contiguity and provide a condition which guarantees the equivalence of weak recovery and detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15957v2</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>cs.SI</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elchanan Mossel, Allan Sly, Youngtak Sohn</dc:creator>
    </item>
    <item>
      <title>Sampling from mixture distributions based on regime-switching diffusions</title>
      <link>https://arxiv.org/abs/2407.13389</link>
      <description>arXiv:2407.13389v2 Announce Type: replace-cross 
Abstract: It is proposed to use stochastic differential equations with state-dependent switching rates (SDEwS) for sampling from finite mixture distributions. An Euler scheme with constant time step for SDEwS is considered. It is shown that the scheme converges with order one in weak sense and also in the ergodic limit. Numerical experiments illustrate the use of SDEwS for sampling from mixture distributions and confirm the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13389v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. V. Tretyakov</dc:creator>
    </item>
    <item>
      <title>A generalized Bayesian approach for high-dimensional robust regression with serially correlated errors and predictors</title>
      <link>https://arxiv.org/abs/2412.05673</link>
      <description>arXiv:2412.05673v2 Announce Type: replace-cross 
Abstract: This paper introduces a loss-based generalized Bayesian methodology for high-dimensional robust regression with serially correlated errors and predictors. The proposed framework employs a novel scaled pseudo-Huber (SPH) loss function, which smooths the well-known Huber loss, effectively balancing quadratic ($\ell_2$) and absolute linear ($\ell_1$) loss behaviors. This flexibility enables the framework to accommodate both thin-tailed and heavy-tailed data efficiently. The generalized Bayesian approach constructs a working likelihood based on the SPH loss, facilitating efficient and stable estimation while providing rigorous uncertainty quantification for all model parameters. Notably, this approach allows formal statistical inference without requiring ad hoc tuning parameter selection while adaptively addressing a wide range of tail behavior in the errors. By specifying appropriate prior distributions for the regression coefficients--such as ridge priors for small or moderate-dimensional settings and spike-and-slab priors for high-dimensional settings--the framework ensures principled inference. We establish rigorous theoretical guarantees for accurate parameter estimation and correct predictor selection under sparsity assumptions for a wide range of data generating setups. Extensive simulation studies demonstrate the superior performance of our approach compared to traditional Bayesian regression methods based on $\ell_2$ and $\ell_1$-loss functions. The results highlight its flexibility and robustness, particularly in challenging high-dimensional settings characterized by data contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05673v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Chakraborty, Kshitij Khare, George Michailidis</dc:creator>
    </item>
    <item>
      <title>Tensor Product Neural Networks for Functional ANOVA Model</title>
      <link>https://arxiv.org/abs/2502.15215</link>
      <description>arXiv:2502.15215v2 Announce Type: replace-cross 
Abstract: Interpretability for machine learning models is becoming more and more important as machine learning models become more complex. The functional ANOVA model, which decomposes a high-dimensional function into a sum of lower dimensional functions so called components, is one of the most popular tools for interpretable AI, and recently, various neural network models have been developed for estimating each component in the functional ANOVA model. However, such neural networks are highly unstable when estimating components since the components themselves are not uniquely defined. That is, there are multiple functional ANOVA decompositions for a given function. In this paper, we propose a novel interpretable model which guarantees a unique functional ANOVA decomposition and thus is able to estimate each component stably. We call our proposed model ANOVA-NODE since it is a modification of Neural Oblivious Decision Ensembles (NODE) for the functional ANOVA model. Theoretically, we prove that ANOVA-NODE can approximate a smooth function well. Additionally, we experimentally show that ANOVA-NODE provides much more stable estimation of each component and thus much more stable interpretation when training data and initial values of the model parameters vary than existing neural network models do.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15215v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seokhun Park, Insung Kong, Yongchan Choi, Chanmoo Park, Yongdai Kim</dc:creator>
    </item>
  </channel>
</rss>
