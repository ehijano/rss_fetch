<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 05:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Triple Difference Designs with Heterogeneous Treatment Effects</title>
      <link>https://arxiv.org/abs/2502.19620</link>
      <description>arXiv:2502.19620v1 Announce Type: new 
Abstract: Triple difference designs have become increasingly popular in empirical economics. The advantage of a triple difference design is that, within treatment group, it allows for another subgroup of the population -- potentially less impacted by the treatment -- to serve as a control for the subgroup of interest. While literature on difference-in-differences has discussed heterogeneity in treatment effects between treated and control groups or over time, little attention has been given to the implications of heterogeneity in treatment effects between subgroups. In this paper, I show that interpretation of the usual triple difference parameter of interest, the difference in average treatment effects on the treated between subgroups, may be affected by this kind of heterogeneity. I propose a new parameter of interest, the causal difference in average treatment effects on the treated, which makes causal comparisons between subgroups. I discuss assumptions for identification and derive the semiparametric efficiency bounds for this parameter. I then propose doubly-robust, efficient estimators for this parameter. I use a simulation study to highlight the desirable finite-sample properties of these estimators, as well as to show the difference between this parameter and the usual triple difference parameter of interest. An empirical application shows the importance of considering treatment effect heterogeneity in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19620v1</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Caron</dc:creator>
    </item>
    <item>
      <title>Time-Varying Identification of Structural Vector Autoregressions</title>
      <link>https://arxiv.org/abs/2502.19659</link>
      <description>arXiv:2502.19659v1 Announce Type: new 
Abstract: We propose a novel Bayesian heteroskedastic Markov-switching structural vector autoregression with data-driven time-varying identification. The model selects among alternative patterns of exclusion restrictions to identify structural shocks within the Markov process regimes. We implement the selection through a multinomial prior distribution over these patterns, which is a spike'n'slab prior for individual parameters. By combining a Markov-switching structural matrix with heteroskedastic structural shocks following a stochastic volatility process, the model enables shock identification through time-varying volatility within a regime. As a result, the exclusion restrictions become over-identifying, and their selection is driven by the signal from the data. Our empirical application shows that data support time variation in the US monetary policy shock identification. We also verify that time-varying volatility identifies the monetary policy shock within the regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19659v1</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annika Camehl (Erasmus University Rotterdam), Tomasz Wo\'zniak (University of Melbourne)</dc:creator>
    </item>
    <item>
      <title>Semiparametric Triple Difference Estimators</title>
      <link>https://arxiv.org/abs/2502.19788</link>
      <description>arXiv:2502.19788v1 Announce Type: new 
Abstract: The triple difference causal inference framework is an extension of the well-known difference-in-differences framework. It relaxes the parallel trends assumption of the difference-in-differences framework through leveraging data from an auxiliary domain. Despite being commonly applied in empirical research, the triple difference framework has received relatively limited attention in the statistics literature. Specifically, investigating the intricacies of identification and the design of robust and efficient estimators for this framework has remained largely unexplored. This work aims to address these gaps in the literature. From the identification standpoint, we present outcome regression and weighting methods to identify the average treatment effect on the treated in both panel data and repeated cross-section settings. For the latter, we relax the commonly made assumption of time-invariant covariates. From the estimation perspective, we consider semiparametric estimators for the triple difference framework in both panel data and repeated cross-sections settings. We demonstrate that our proposed estimators are doubly robust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19788v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sina Akbari, Negar Kiyavash, AmirEmad Ghassami</dc:creator>
    </item>
    <item>
      <title>Economic Causal Inference Based on DML Framework: Python Implementation of Binary and Continuous Treatment Variables</title>
      <link>https://arxiv.org/abs/2502.19898</link>
      <description>arXiv:2502.19898v1 Announce Type: new 
Abstract: This study utilizes a simulated dataset to establish Python code for Double Machine Learning (DML) using Anaconda's Jupyter Notebook and the DML software package from GitHub. The research focuses on causal inference experiments for both binary and continuous treatment variables. The findings reveal that the DML model demonstrates relatively stable performance in calculating the Average Treatment Effect (ATE) and its robustness metrics. However, the study also highlights that the computation of Conditional Average Treatment Effect (CATE) remains a significant challenge for future DML modeling, particularly in the context of continuous treatment variables. This underscores the need for further research and development in this area to enhance the model's applicability and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19898v1</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Shunxin Yao</dc:creator>
    </item>
    <item>
      <title>The Canonical Decomposition of Factor Models: Weak Factors are Everywhere</title>
      <link>https://arxiv.org/abs/2307.10067</link>
      <description>arXiv:2307.10067v3 Announce Type: replace 
Abstract: There are two approaches to time series approximate factor models: the static factor model, where the factors are loaded contemporaneously by the common component, and the Generalised Dynamic Factor Model, where the factors are loaded with lags. In this paper we derive a canonical decomposition which nests both models by introducing the weak common component which is the difference between the dynamic- and the static common component. Such component is driven by potentially infinitely many non-pervasive weak factors which live in the dynamically common space (not to be confused with rate-weak factors, being pervasive but associated with a slower rate). Our result shows that the relation between the two approaches is far more rich and complex than what usually assumed. We exemplify why the weak common component shall not be neglected by means of theoretical and empirical examples. Furthermore, we propose a simple estimation procedure for the canonical decomposition. Our empirical estimates on US macroeconomic data reveal that the weak common component can account for a large part of the variation of individual variables. Furthermore in a pseudo real-time forecasting evaluation for industrial production and inflation, we show that gains can be obtained from considering the dynamic approach over the static approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10067v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Gersing, Matteo Barigozzi, Christoph Rust, Manfred Deistler</dc:creator>
    </item>
    <item>
      <title>Invalid proxies and volatility changes</title>
      <link>https://arxiv.org/abs/2403.08753</link>
      <description>arXiv:2403.08753v2 Announce Type: replace 
Abstract: When in proxy-SVARs the covariance matrix of VAR disturbances is subject to exogenous, permanent breaks that cause IRFs to change across volatility regimes, even strong, exogenous external instruments yield inconsistent estimates of the dynamic causal effects. However, if these volatility shifts are properly incorporated into the analysis through (testable) "stability restrictions", we demonstrate that the target IRFs are point-identified and can be estimated consistently under a necessary and sufficient rank condition. If the shifts in volatility are sufficiently informative, standard asymptotic inference remains valid even with (i) local-to-zero covariance between the proxies and the instrumented structural shocks, and (ii) potential failures of instrument exogeneity. Intuitively, shifts in volatility act similarly to strong instruments that are correlated with both the target and non-target shocks. We illustrate the effectiveness of our approach by revisiting a seminal fiscal proxy-SVAR for the US economy. We detect a sharp change in the size of the tax multiplier when the narrative tax instrument is complemented with the decline in unconditional volatility observed during the transition from the Great Inflation to the Great Moderation. The narrative tax instrument contributes to identify the tax shock in both regimes, despite our empirical analysis raises concerns about its "statistical" validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08753v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni Angelini, Luca Fanelli, Luca Neri</dc:creator>
    </item>
    <item>
      <title>Matching $\leq$ Hybrid $\leq$ Difference in Differences</title>
      <link>https://arxiv.org/abs/2411.07952</link>
      <description>arXiv:2411.07952v2 Announce Type: replace 
Abstract: Since LaLonde's (1986) seminal paper, there has been ongoing interest in estimating treatment effects using pre- and post-intervention data. Scholars have traditionally used experimental benchmarks to evaluate the accuracy of alternative econometric methods, including Matching, Difference-in-Differences (DID), and their hybrid forms (e.g., Heckman et al., 1998b; Dehejia and Wahba, 2002; Smith and Todd, 2005). We revisit these methodologies in the evaluation of job training and educational programs using four datasets (LaLonde, 1986; Heckman et al., 1998a; Smith and Todd, 2005; Chetty et al., 2014a; Athey et al., 2020), and show that the inequality relationship, Matching $\leq$ Hybrid $\leq$ DID, appears as a consistent norm, rather than a mere coincidence. We provide a formal theoretical justification for this puzzling phenomenon under plausible conditions such as negative selection, by generalizing the classical bracketing (Angrist and Pischke, 2009, Section 5). Consequently, when treatments are expected to be non-negative, DID tends to provide optimistic estimates, while Matching offers more conservative ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07952v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yechan Park, Yuya Sasaki</dc:creator>
    </item>
    <item>
      <title>Transformers Handle Endogeneity in In-Context Linear Regression</title>
      <link>https://arxiv.org/abs/2410.01265</link>
      <description>arXiv:2410.01265v2 Announce Type: replace-cross 
Abstract: We explore the capability of transformers to address endogeneity in in-context linear regression. Our main finding is that transformers inherently possess a mechanism to handle endogeneity effectively using instrumental variables (IV). First, we demonstrate that the transformer architecture can emulate a gradient-based bi-level optimization procedure that converges to the widely used two-stage least squares $(\textsf{2SLS})$ solution at an exponential rate. Next, we propose an in-context pretraining scheme and provide theoretical guarantees showing that the global minimizer of the pre-training loss achieves a small excess loss. Our extensive experiments validate these theoretical findings, showing that the trained transformer provides more robust and reliable in-context predictions and coefficient estimates than the $\textsf{2SLS}$ method, in the presence of endogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01265v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haodong Liang, Krishnakumar Balasubramanian, Lifeng Lai</dc:creator>
    </item>
  </channel>
</rss>
