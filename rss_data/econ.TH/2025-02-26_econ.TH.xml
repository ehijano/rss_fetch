<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.TH</link>
    <description>econ.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Feb 2025 02:55:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Pareto-undominated strategy-proof rules in economies with multidimensional single-peaked preferences</title>
      <link>https://arxiv.org/abs/2502.17682</link>
      <description>arXiv:2502.17682v1 Announce Type: new 
Abstract: In the problem of fully allocating a social endowment of perfectly divisible commodities among a group of agents with multidimensional single-peaked preferences, we study strategy-proof rules that are not Pareto-dominated by other strategy-proof rules. Specifically, we: (i) establish a sufficient condition for a rule to be Pareto-undominated strategy-proof; (ii) introduce a broad class of rules satisfying this property by extending the family of "sequential allotment rules" to the multidimensional setting; and (iii) provide a new characterization of the "multidimensional uniform rule" involving Pareto-undominated strategy-proofness. Results (i) and (iii) generalize previous findings that were only applicable to the two-agent case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17682v1</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agustin G. Bonifacio</dc:creator>
    </item>
    <item>
      <title>Certified Decisions</title>
      <link>https://arxiv.org/abs/2502.17830</link>
      <description>arXiv:2502.17830v1 Announce Type: cross 
Abstract: Hypothesis tests and confidence intervals are ubiquitous in empirical research, yet their connection to subsequent decision-making is often unclear. We develop a theory of certified decisions that pairs recommended decisions with inferential guarantees. Specifically, we attach P-certificates -- upper bounds on loss that hold with probability at least $1-\alpha$ -- to recommended actions. We show that such certificates allow "safe," risk-controlling adoption decisions for ambiguity-averse downstream decision-makers. We further prove that it is without loss to limit attention to P-certificates arising as minimax decisions over confidence sets, or what Manski (2021) terms "as-if decisions with a set estimate." A parallel argument applies to E-certified decisions obtained from e-values in settings with unbounded loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17830v1</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaiah Andrews, Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>FACT or Fiction: Can Truthful Mechanisms Eliminate Federated Free Riding?</title>
      <link>https://arxiv.org/abs/2405.13879</link>
      <description>arXiv:2405.13879v3 Announce Type: replace-cross 
Abstract: Standard federated learning (FL) approaches are vulnerable to the free-rider dilemma: participating agents can contribute little to nothing yet receive a well-trained aggregated model. While prior mechanisms attempt to solve the free-rider dilemma, none have addressed the issue of truthfulness. In practice, adversarial agents can provide false information to the server in order to cheat its way out of contributing to federated training. In an effort to make free-riding-averse federated mechanisms truthful, and consequently less prone to breaking down in practice, we propose FACT. FACT is the first federated mechanism that: (1) eliminates federated free riding by using a penalty system, (2) ensures agents provide truthful information by creating a competitive environment, and (3) encourages agent participation by offering better performance than training alone. Empirically, FACT avoids free-riding when agents are untruthful, and reduces agent loss by over 4x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13879v3</guid>
      <category>cs.GT</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>econ.TH</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Bornstein, Amrit Singh Bedi, Abdirisak Mohamed, Furong Huang</dc:creator>
    </item>
    <item>
      <title>Non-maximizing policies that fulfill multi-criterion aspirations in expectation</title>
      <link>https://arxiv.org/abs/2408.04385</link>
      <description>arXiv:2408.04385v2 Announce Type: replace-cross 
Abstract: In dynamic programming and reinforcement learning, the policy for the sequential decision making of an agent in a stochastic environment is usually determined by expressing the goal as a scalar reward function and seeking a policy that maximizes the expected total reward. However, many goals that humans care about naturally concern multiple aspects of the world, and it may not be obvious how to condense those into a single reward function. Furthermore, maximization suffers from specification gaming, where the obtained policy achieves a high expected total reward in an unintended way, often taking extreme or nonsensical actions.
  Here we consider finite acyclic Markov Decision Processes with multiple distinct evaluation metrics, which do not necessarily represent quantities that the user wants to be maximized. We assume the task of the agent is to ensure that the vector of expected totals of the evaluation metrics falls into some given convex set, called the aspiration set. Our algorithm guarantees that this task is fulfilled by using simplices to approximate feasibility sets and propagate aspirations forward while ensuring they remain feasible. It has complexity linear in the number of possible state-action-successor triples and polynomial in the number of evaluation metrics. Moreover, the explicitly non-maximizing nature of the chosen policy and goals yields additional degrees of freedom, which can be used to apply heuristic safety criteria to the choice of actions. We discuss several such safety criteria that aim to steer the agent towards more conservative behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04385v2</guid>
      <category>cs.AI</category>
      <category>econ.TH</category>
      <category>math.OC</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-73903-3_8</arxiv:DOI>
      <arxiv:journal_reference>In: Freeman, R., Mattei, N. (eds) Algorithmic Decision Theory. ADT 2024. Lecture Notes in Computer Science(), vol 15248. Springer, Cham (2025)</arxiv:journal_reference>
      <dc:creator>Simon Dima, Simon Fischer, Jobst Heitzig, Joss Oliver</dc:creator>
    </item>
  </channel>
</rss>
