<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Aug 2025 04:04:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Weak Identification in Peer Effects Estimation</title>
      <link>https://arxiv.org/abs/2508.04897</link>
      <description>arXiv:2508.04897v1 Announce Type: new 
Abstract: It is commonly accepted that some phenomena are social: for example, individuals' smoking habits often correlate with those of their peers. Such correlations can have a variety of explanations, such as direct contagion or shared socioeconomic circumstances. The network linear-in-means model is a workhorse statistical model which incorporates these peer effects by including average neighborhood characteristics as regressors. Although the model's parameters are identifiable under mild structural conditions on the network, it remains unclear whether identification ensures reliable estimation in the "infill" asymptotic setting, where a single network grows in size. We show that when covariates are i.i.d. and the average network degree of nodes increases with the population size, standard estimators suffer from bias or slow convergence rates due to asymptotic collinearity induced by network averaging. As an alternative, we demonstrate that linear-in-sums models, which are based on aggregate rather than average neighborhood characteristics, do not exhibit such issues as long as the network degrees have some nontrivial variation, a condition satisfied by most network models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04897v1</guid>
      <category>math.ST</category>
      <category>cs.SI</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William W. Wang, Ali Jadbabaie</dc:creator>
    </item>
    <item>
      <title>General asymptotic representations of indexes based on the functional empirical process and the residual functional empirical process and applications</title>
      <link>https://arxiv.org/abs/2508.04905</link>
      <description>arXiv:2508.04905v1 Announce Type: new 
Abstract: The objective of this paper is to establish a general asymptotic representation (\textit{GAR}) for a wide range of statistics, employing two fundamental processes: the functional empirical process (\textit{fep}) and the residual functional empirical process introduced by Lo and Sall (2010a, 2010b), denoted as \textit{lrfep}. The functional empirical process (\textit{fep}) is defined as follows:
  $$ \mathbb{G}_n(h)=\frac{1}{\sqrt{n}} \sum_{j=1}^{n} \{h(X_j)-\mathbb{E}h(X_j)\}, $$
  \Bin [where $X$, $X_1$, $\cdots$, $X_n$ is a sample from a random $d$-vectors $X$ of size $(n+1)$ with $n\geq 1$ and $h$ is a measurable function defined on $\mathbb{R}^d$ such that $\mathbb{E}h(X)^2&lt;+\infty$]. It is a powerful tool for deriving asymptotic laws. An earlier and simpler version of this paper focused on the application of the (\textit{fep}) to statistics $J_n$ that can be turned into an asymptotic algebraic expression of empirical functions of the form
  $$ J_n=\mathbb{E}h(X) + n^{-1/2} \mathbb{G}_n(h) + o_{\mathbb{P}}(n^{-1/2}). \ \ \ \textit{SGAR} $$
  \Bin However, not all statistics, in particular welfare indexes, conform to this form. In many scenarios, functions of the order statistics $X_{1,n}\leq$, $\cdots$, $\leq X_{n,n}$ are involved, resulting in $L$-statistics. In such cases, the (\textit{fep}) can still be utilized, but in combination with the related residual functional empirical process introduced by Lo and Sall (2010a, 2010b). This combination leads to general asymptotic representations (GAR) for a wide range of statistical indexes
  $$ J_n=\mathbb{E}h(X) + n^{-1/2} \biggr(\mathbb{G}_n(h) + \int_{0}^{1} \mathbb{G}_n(\tilde{f}_s) \ell(s) \ ds + o_{\mathbb{P}}(1)\biggr), \ \ \textit{FGAR} $$</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04905v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Gane Samb Lo, Tchilabalo Abozou Kpanzou, Gandasor Bonyiri Onesiphore Da</dc:creator>
    </item>
    <item>
      <title>Kaminsky Type Functional Equations and Bivariate Residual Lifetimes Distributions</title>
      <link>https://arxiv.org/abs/2508.05079</link>
      <description>arXiv:2508.05079v1 Announce Type: new 
Abstract: This paper considers generalizations of the functional equations that characterize the lack-of-memory properties at univariate and bivariate levels. Specifically, we extend the univariate functional equation introduced by Kaminsky (1983) (that characterizes the Gompertz distribution) and the corresponding bivariate strong and weak versions later studied in Marshall and Olkin (2015) by allowing the conditional survival distribution to be a fully general time dependent distortion of the unconditional one: in particular, we show that the solutions of these generalized functional equations coincide with the solutions of the functional equations studied in Ricci (2024). Since the univariate functional equation leads only to a trivial case and the solutions of the strong bivariate functional equation have been already studied in the literature, the analysis is focused on the weak bivariate case, where joint residual lifetimes are conditioned on survival beyond a common threshold t. In view of potential applications to insurance risk analysis, the impact of the time dependent distortion on the aging properties of the associated distribution is analized as well as the time dependent dependence structure of the residual lifetimes through time-varying versions of the Kendall's function and of the tail dependence coefficients. Many examples are provided and a wide family of bivariate survival distributions satisfying the generalized weak functional equation is constructed through a mixing approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05079v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sabrina Mulinacci, Massimo Ricci</dc:creator>
    </item>
    <item>
      <title>Nonparametric Estimation from Correlated Copies of a Drifted 2nd-Order Process</title>
      <link>https://arxiv.org/abs/2508.05259</link>
      <description>arXiv:2508.05259v1 Announce Type: new 
Abstract: This paper presents several situations leading to the observation of multiple - possibly correlated - copies of a drifted 2nd-order process, and then non-asymptotic risk bounds are established on nonparametric estimators of the drift function $b_0$ and its derivative. For drifted Gaussian processes with a regular enough covariance function, a sharper risk bound is established on the estimator of $b_0'$, and a model selection procedure is provided with theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05259v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Marie</dc:creator>
    </item>
    <item>
      <title>Conditional validity and a fast approximation formula of full conformal prediction sets</title>
      <link>https://arxiv.org/abs/2508.05272</link>
      <description>arXiv:2508.05272v1 Announce Type: new 
Abstract: Prediction sets based on full conformal prediction have seen an increasing interest in statistical learning due to their universal marginal coverage guarantees. However, practitioners have refrained from using it in applications for two reasons: Firstly, it comes at very high computational costs, exceeding even that of cross-validation. Secondly, an applicant is typically not interested in a marginal coverage guarantee which averages over all possible (but not available) training data sets, but rather in a guarantee conditional on the specific training data. This work tackles these problems by, firstly, showing that full conformal prediction sets are conditionally conservative given the training data if the conformity score is stochastically bounded and satisfies a stability condition. Secondly, we propose an approximation for the full conformal prediction set that has asymptotically the same training conditional coverage as full conformal prediction under the stability assumption derived before, and can be computed more easily. Furthermore, we show that under the stability assumption, $n$-fold cross-conformal prediction also has the same asymptotic training conditional coverage guarantees as full conformal prediction. If the conformity score is defined as the out-of-sample prediction error, our approximation of the full conformal set coincides with the symmetrized Jackknife. We conclude that for this conformity score, if based on a stable prediction algorithm, full-conformal, $n$-fold cross-conformal, the Jackknife+, our approximation formula, and hence also the Jackknife, all yield the same asymptotic training conditional coverage guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05272v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolai Amann</dc:creator>
    </item>
    <item>
      <title>Differentially Private Model-X Knockoffs via Johnson-Lindenstrauss Transform</title>
      <link>https://arxiv.org/abs/2508.04800</link>
      <description>arXiv:2508.04800v1 Announce Type: cross 
Abstract: We introduce a novel privatization framework for high-dimensional controlled variable selection. Our framework enables rigorous False Discovery Rate (FDR) control under differential privacy constraints. While the Model-X knockoff procedure provides FDR guarantees by constructing provably exchangeable ``negative control" features, existing privacy mechanisms like Laplace or Gaussian noise injection disrupt its core exchangeability conditions. Our key innovation lies in privatizing the data knockoff matrix through the Gaussian Johnson-Lindenstrauss Transformation (JLT), a dimension reduction technique that simultaneously preserves covariate relationships through approximate isometry for $(\epsilon,\delta)$-differential privacy.
  We theoretically characterize both FDR and the power of the proposed private variable selection procedure, in an asymptotic regime. Our theoretical analysis characterizes the role of different factors, such as the JLT's dimension reduction ratio, signal-to-noise ratio, differential privacy parameters, sample size and feature dimension, in shaping the privacy-power trade-off. Our analysis is based on a novel `debiasing technique' for high-dimensional private knockoff procedure. We further establish sufficient conditions under which the power of the proposed procedure converges to one. This work bridges two critical paradigms -- knockoff-based FDR control and private data release -- enabling reliable variable selection in sensitive domains. Our analysis demonstrates that structural privacy preservation through random projections outperforms the classical noise addition mechanism, maintaining statistical power even under strict privacy budgets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04800v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuxuan Tao, Adel Javanmard</dc:creator>
    </item>
    <item>
      <title>Gaussian mixture layers for neural networks</title>
      <link>https://arxiv.org/abs/2508.04883</link>
      <description>arXiv:2508.04883v1 Announce Type: cross 
Abstract: The mean-field theory for two-layer neural networks considers infinitely wide networks that are linearly parameterized by a probability measure over the parameter space. This nonparametric perspective has significantly advanced both the theoretical and conceptual understanding of neural networks, with substantial efforts made to validate its applicability to networks of moderate width. In this work, we explore the opposite direction, investigating whether dynamics can be directly implemented over probability measures. Specifically, we employ Gaussian mixture models as a flexible and expressive parametric family of distributions together with the theory of Wasserstein gradient flows to derive training dynamics for such measures. Our approach introduces a new type of layer -- the Gaussian mixture (GM) layer -- that can be integrated into neural network architectures. As a proof of concept, we validate our proposal through experiments on simple classification tasks, where a GM layer achieves test performance comparable to that of a two-layer fully connected network. Furthermore, we examine the behavior of these dynamics and demonstrate numerically that GM layers exhibit markedly different behavior compared to classical fully connected layers, even when the latter are large enough to be considered in the mean-field regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04883v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sinho Chewi, Philippe Rigollet, Yuling Yan</dc:creator>
    </item>
    <item>
      <title>Parameter-free entropy-regularized multi-view clustering with hierarchical feature selection</title>
      <link>https://arxiv.org/abs/2508.05504</link>
      <description>arXiv:2508.05504v1 Announce Type: cross 
Abstract: Multi-view clustering faces critical challenges in automatically discovering patterns across heterogeneous data while managing high-dimensional features and eliminating irrelevant information. Traditional approaches suffer from manual parameter tuning and lack principled cross-view integration mechanisms. This work introduces two complementary algorithms: AMVFCM-U and AAMVFCM-U, providing a unified parameter-free framework. Our approach replaces fuzzification parameters with entropy regularization terms that enforce adaptive cross-view consensus. The core innovation employs signal-to-noise ratio based regularization ($\delta_j^h = \frac{\bar{x}_j^h}{(\sigma_j^h)^2}$) for principled feature weighting with convergence guarantees, coupled with dual-level entropy terms that automatically balance view and feature contributions. AAMVFCM-U extends this with hierarchical dimensionality reduction operating at feature and view levels through adaptive thresholding ($\theta^{h^{(t)}} = \frac{d_h^{(t)}}{n}$). Evaluation across five diverse benchmarks demonstrates superiority over 15 state-of-the-art methods. AAMVFCM-U achieves up to 97% computational efficiency gains, reduces dimensionality to 0.45% of original size, and automatically identifies critical view combinations for optimal pattern discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05504v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristina P. Sinaga, Sara Colantonio, Miin-Shen Yang</dc:creator>
    </item>
    <item>
      <title>High-Order Error Bounds for Markovian LSA with Richardson-Romberg Extrapolation</title>
      <link>https://arxiv.org/abs/2508.05570</link>
      <description>arXiv:2508.05570v1 Announce Type: cross 
Abstract: In this paper, we study the bias and high-order error bounds of the Linear Stochastic Approximation (LSA) algorithm with Polyak-Ruppert (PR) averaging under Markovian noise. We focus on the version of the algorithm with constant step size $\alpha$ and propose a novel decomposition of the bias via a linearization technique. We analyze the structure of the bias and show that the leading-order term is linear in $\alpha$ and cannot be eliminated by PR averaging. To address this, we apply the Richardson-Romberg (RR) extrapolation procedure, which effectively cancels the leading bias term. We derive high-order moment bounds for the RR iterates and show that the leading error term aligns with the asymptotically optimal covariance matrix of the vanilla averaged LSA iterates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05570v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilya Levin, Alexey Naumov, Sergey Samsonov</dc:creator>
    </item>
    <item>
      <title>Consistency of an Intercept-Shifted Synthetic-Control Estimator under Weighted Parallel Trends</title>
      <link>https://arxiv.org/abs/2508.05604</link>
      <description>arXiv:2508.05604v1 Announce Type: cross 
Abstract: The average treatment effect on the treated (ATT) in a staggered-adoption panel is estimated using an intercept-augmented synthetic-control (SCM) estimator. A weighted parallel trends plus an intercept shift, together with mild regularity on the weight vectors (non-degenerate dispersion) and expanding pre-treatment length, are sufficient for consistency allowing for heavy-tailed shocks. These conditions can be more interpretable than the autoregressive or low-rank factor models with light tails assumed by Ben-Michael, Feller, and Rothstein (2022) and expand the valid DGP pool from the same paper. Practical diagnostics to support the assumptions are discussed and situate these results within the recent literature on SC + DiD hybrids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05604v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Guggisberg</dc:creator>
    </item>
    <item>
      <title>The many routes to the ubiquitous Bradley-Terry model</title>
      <link>https://arxiv.org/abs/2312.13619</link>
      <description>arXiv:2312.13619v2 Announce Type: replace 
Abstract: The rating of items based on pairwise comparisons has been a topic of statistical investigation for many decades. Numerous approaches have been proposed. One of the best known is the Bradley-Terry model. This paper seeks to assemble and explain a variety of motivations for its use. Some are based on principles or on maximising an objective function; others are derived from well-known statistical models, or stylised game scenarios. They include both examples well-known in the literature as well as what are believed to be novel presentations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13619v2</guid>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Hamilton, Nick Tawn, David Firth</dc:creator>
    </item>
    <item>
      <title>On the asymptotic validity of confidence sets for linear functionals of solutions to integral equations</title>
      <link>https://arxiv.org/abs/2502.16673</link>
      <description>arXiv:2502.16673v3 Announce Type: replace 
Abstract: This paper examines the construction of confidence sets for parameters defined as linear functionals of a function of W and X whose conditional mean given Z and X equals the conditional mean of another variable Y given Z and X. Many estimands of interest in causal inference can be expressed in this form, including the average treatment effect in proximal causal inference and treatment effect contrasts in instrumental variable models. We derive a necessary condition for a confidence set to be uniformly valid over a model that allows for the dependence between W and Z given X to be arbitrarily weak. Specifically, we show that for any such confidence set, there must exist some laws in the model under which, with high probability, the confidence set has a diameter greater than or equal to the diameter of the parameter's range. In particular, consistent with the weak instruments literature, Wald confidence intervals are not uniformly valid over the aforementioned model when the parameter's range is infinite. Furthermore, we argue that inverting the score test, a successful approach in that literature, generally fails for the broader class of parameters considered here. We present a method for constructing uniformly valid confidence sets in the special case where all variables, but possibly Y, are binary and discuss its limitations. Finally, we emphasize that developing uniformly valid confidence sets for the class of parameters considered in this paper remains an open problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16673v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ezequiel Smucler, James M. Robins, Andrea Rotnitzky</dc:creator>
    </item>
    <item>
      <title>Shift-Dispersion Decompositions of Wasserstein and Cram\'er Distances</title>
      <link>https://arxiv.org/abs/2408.09770</link>
      <description>arXiv:2408.09770v3 Announce Type: replace-cross 
Abstract: Divergence functions are measures of distance or dissimilarity between probability distributions that serve various purposes in statistics and applications. We propose decompositions of Wasserstein and Cram\'er distances$-$which compare two distributions by integrating over their differences in distribution or quantile functions$-$into directed shift and dispersion components. These components are obtained by dividing the differences between the quantile functions into contributions arising from shift and dispersion, respectively. Our decompositions add information on how the distributions differ in a condensed form and consequently enhance the interpretability of the underlying divergences. We show that our decompositions satisfy a number of natural properties and are unique in doing so in location-scale families. The decompositions allow to derive sensitivities of the divergence measures to changes in location and dispersion, and they give rise to weak stochastic order relations that are linked to the usual stochastic and the dispersive order. Our theoretical developments are illustrated in two applications, where we focus on forecast evaluation of temperature extremes and on the design of probabilistic surveys in economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09770v3</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Resin, Daniel Wolffram, Johannes Bracher, Timo Dimitriadis</dc:creator>
    </item>
    <item>
      <title>A Stein Gradient Descent Approach for Doubly Intractable Distributions</title>
      <link>https://arxiv.org/abs/2410.21021</link>
      <description>arXiv:2410.21021v2 Announce Type: replace-cross 
Abstract: Bayesian inference for doubly intractable distributions is challenging because they include intractable terms, which are functions of parameters of interest. Although several alternatives have been developed for such models, they are computationally intensive due to repeated auxiliary variable simulations. We propose a novel Monte Carlo Stein variational gradient descent (MC-SVGD) approach for inference for doubly intractable distributions. Through an efficient gradient approximation, our MC-SVGD approach rapidly transforms an arbitrary reference distribution to approximate the posterior distribution of interest, without necessitating any predefined variational distribution class for the posterior. Such a transport map is obtained by minimizing Kullback-Leibler divergence between the transformed and posterior distributions in a reproducing kernel Hilbert space (RKHS). We also investigate the convergence rate of the proposed method. We illustrate the application of the method to challenging examples, including a Potts model, an exponential random graph model, and a Conway--Maxwell--Poisson regression model. The proposed method achieves substantial computational gains over existing algorithms, while providing comparable inferential performance for the posterior distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21021v2</guid>
      <category>stat.ML</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heesang Lee, Songhee Kim, Bokgyeong Kang, Jaewoo Park</dc:creator>
    </item>
    <item>
      <title>GRAND: Graph Release with Assured Node Differential Privacy</title>
      <link>https://arxiv.org/abs/2507.00402</link>
      <description>arXiv:2507.00402v2 Announce Type: replace-cross 
Abstract: Differential privacy is a well-established framework for safeguarding sensitive information in data. While extensively applied across various domains, its application to network data -- particularly at the node level -- remains underexplored. Existing methods for node-level privacy either focus exclusively on query-based approaches, which restrict output to pre-specified network statistics, or fail to preserve key structural properties of the network. In this work, we propose GRAND (Graph Release with Assured Node Differential privacy), which is, to the best of our knowledge, the first network release mechanism that releases entire networks while ensuring node-level differential privacy and preserving structural properties. Under a broad class of latent space models, we show that the released network asymptotically follows the same distribution as the original network. The effectiveness of the approach is evaluated through extensive experiments on both synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00402v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suqing Liu, Xuan Bi, Tianxi Li</dc:creator>
    </item>
    <item>
      <title>Cholesky decomposition for symmetric matrices, Riemannian geometry, and random matrices</title>
      <link>https://arxiv.org/abs/2508.02715</link>
      <description>arXiv:2508.02715v2 Announce Type: replace-cross 
Abstract: For each $n \geq 1$ and sign pattern $\epsilon \in \{ \pm 1 \}^n$, we introduce a cone of real symmetric matrices $LPM_n(\epsilon)$: those with leading principal $k \times k$ minors of signs $\epsilon_k$. These cones are pairwise disjoint and their union $LPM_n$ is an open dense cone in all symmetric matrices; they subsume positive and negative definite matrices, and symmetric (P-,) N-, PN-, almost P-, and almost N- matrices. We show that each $LPM_n$ matrix $A$ admits an uncountable family of Cholesky-type factorizations - yielding a unique lower triangular matrix $L$ with positive diagonals - with additional attractive properties: (i) each such factorization is algorithmic; and (ii) each such Cholesky map $A \mapsto L$ is a smooth diffeomorphism from $LPM_n(\epsilon)$ onto an open Euclidean ball.
  We then show that (iii) the (diffeomorphic) balls $LPM_n(\epsilon)$ are isometric Riemannian manifolds as well as isomorphic abelian Lie groups, each equipped with a translation-invariant Riemannian metric (and hence Riemannian means/barycentres). Moreover, (iv) this abelian metric group structure on each $LPM_n(\epsilon)$ - and hence the log-Cholesky metric on Cholesky space - yields an isometric isomorphism onto a finite-dimensional Euclidean space. The complex version of this also holds.
  In the latter part, we show that the abelian group $PD_n$ of positive definite matrices, with its bi-invariant log-Cholesky metric, is precisely the identity-component of a larger group with an alternate metric: the open dense cone $LPM_n$. This also holds for Hermitian matrices over several subfields $\mathbb{F} \subseteq \mathbb{C}$. As a result, (v) the groups $LPM_n^{\mathbb{F}}$ and $LPM_\infty^{\mathbb{F}}$ admit a rich probability theory, and the cones $LPM_n(\epsilon), TPM_n(\epsilon)$ admit Wishart densities with signed Bartlett decompositions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02715v2</guid>
      <category>math.RA</category>
      <category>math.DG</category>
      <category>math.PR</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Apoorva Khare, Prateek Kumar Vishwakarma</dc:creator>
    </item>
  </channel>
</rss>
