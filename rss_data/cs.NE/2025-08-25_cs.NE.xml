<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Aug 2025 04:00:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Spike Agreement Dependent Plasticity: A scalable Bio-Inspired learning paradigm for Spiking Neural Networks</title>
      <link>https://arxiv.org/abs/2508.16216</link>
      <description>arXiv:2508.16216v1 Announce Type: new 
Abstract: We introduce Spike Agreement Dependent Plasticity (SADP), a biologically inspired synaptic learning rule for Spiking Neural Networks (SNNs) that relies on the agreement between pre- and post-synaptic spike trains rather than precise spike-pair timing. SADP generalizes classical Spike-Timing-Dependent Plasticity (STDP) by replacing pairwise temporal updates with population-level correlation metrics such as Cohen's kappa. The SADP update rule admits linear-time complexity and supports efficient hardware implementation via bitwise logic. Empirical results on MNIST and Fashion-MNIST show that SADP, especially when equipped with spline-based kernels derived from our experimental iontronic organic memtransistor device data, outperforms classical STDP in both accuracy and runtime. Our framework bridges the gap between biological plausibility and computational scalability, offering a viable learning mechanism for neuromorphic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16216v1</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Bej, Muhammed Sahad E, Gouri Lakshmi, Harshit Kumar, Pritam Kar, Bikas C Das</dc:creator>
    </item>
    <item>
      <title>Competition and Attraction Improve Model Fusion</title>
      <link>https://arxiv.org/abs/2508.16204</link>
      <description>arXiv:2508.16204v1 Announce Type: cross 
Abstract: Model merging is a powerful technique for integrating the specialized knowledge of multiple machine learning models into a single model. However, existing methods require manually partitioning model parameters into fixed groups for merging, which restricts the exploration of potential combinations and limits performance. To overcome these limitations, we propose Model Merging of Natural Niches (M2N2), an evolutionary algorithm with three key features: (1) dynamic adjustment of merging boundaries to progressively explore a broader range of parameter combinations; (2) a diversity preservation mechanism inspired by the competition for resources in nature, to maintain a population of diverse, high-performing models that are particularly well-suited for merging; and (3) a heuristicbased attraction metric to identify the most promising pairs of models for fusion. Our experimental results demonstrate, for the first time, that model merging can be used to evolve models entirely from scratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch and achieve performance comparable to CMA-ES, while being computationally more efficient. Furthermore, M2N2 scales to merge specialized language and image generation models, achieving state-of-the-art performance. Notably, it preserves crucial model capabilities beyond those explicitly optimized by the fitness function, highlighting its robustness and versatility. Our code is available at https://github.com/SakanaAI/natural_niches</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16204v1</guid>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3712256.3726329</arxiv:DOI>
      <dc:creator>Jo\~ao Abrantes, Robert Tjarko Lange, Yujin Tang</dc:creator>
    </item>
    <item>
      <title>Energy-Information Trade-Off in Self-Directed Channel Memristors</title>
      <link>https://arxiv.org/abs/2508.16236</link>
      <description>arXiv:2508.16236v1 Announce Type: cross 
Abstract: Understanding the nature of information storage on memristors is vital to enable their use in novel data storage and neuromorphic applications. One key consideration in information storage is the energy cost of storage and what impact the available energy has on the information capacity of the devices. In this paper, we propose and study an energy-information trade-off for a particular kind of memristive device - Self-Directed Channel (SDC) memristors. We perform experiments to model the energy required to set the devices into various states, as well as assessing the stability of these states over time. Based on these results, we employ a generative modelling approach, using a conditional Generative Adversarial Network (cGAN) to characterise the storage conditional distribution, allowing us to estimate energy-information curves for a range of storage delays, showing the graceful trade-off between energy consumed and the effective capacity of the devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16236v1</guid>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <category>physics.app-ph</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Waleed El-Geresy, D\'aniel Hajt\'o, Gy\"orgy Cserey, Deniz G\"und\"uz</dc:creator>
    </item>
  </channel>
</rss>
