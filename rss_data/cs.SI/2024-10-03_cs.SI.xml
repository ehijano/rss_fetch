<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SI</link>
    <description>cs.SI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Oct 2024 05:04:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Detecting Viral Social Events through Censored Observation with Deep Survival Analysis</title>
      <link>https://arxiv.org/abs/2410.01320</link>
      <description>arXiv:2410.01320v1 Announce Type: new 
Abstract: Users increasing activity across various social networks made it the most widely used platform for exchanging and propagating information among individuals. To spread information within a network, a user initially shared information on a social network, and then other users in direct contact with him might have shared that information. Information expanded throughout the network by repeatedly following this process. A set of information that became popular and was repeatedly shared by different individuals was called viral events. Identifying and analyzing viral social events led to valuable insights into the dynamics of information dissemination within a network. However, more importantly, proactive approaches emerged. In other words, by observing the dissemination pattern of a piece of information in the early stages of expansion, it became possible to determine whether this cascade would become viral in the future. This research aimed to predict and detect viral events in social networks by observing granular information and using a deep survival analysis-based method. This model could play a significant role in identifying rumors, predicting the impact of information, and assisting in optimal decision-making in information management and marketing. Ultimately, the proposed method was tested on various real-world datasets from Twitter, Weibo, and Digg.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01320v1</guid>
      <category>cs.SI</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maryam Ramezani, Hossein Goli, AmirMohammad Izad, Hamid R. Rabiee</dc:creator>
    </item>
    <item>
      <title>Does Graph Prompt Work? A Data Operation Perspective with Theoretical Analysis</title>
      <link>https://arxiv.org/abs/2410.01635</link>
      <description>arXiv:2410.01635v1 Announce Type: cross 
Abstract: In recent years, graph prompting has emerged as a promising research direction, enabling the learning of additional tokens or subgraphs appended to the original graphs without requiring retraining of pre-trained graph models across various applications. This novel paradigm, shifting from the traditional pretraining and finetuning to pretraining and prompting has shown significant empirical success in simulating graph data operations, with applications ranging from recommendation systems to biological networks and graph transferring. However, despite its potential, the theoretical underpinnings of graph prompting remain underexplored, raising critical questions about its fundamental effectiveness. The lack of rigorous theoretical proof of why and how much it works is more like a dark cloud over the graph prompt area to go further. To fill this gap, this paper introduces a theoretical framework that rigorously analyzes graph prompting from a data operation perspective. Our contributions are threefold: First, we provide a formal guarantee theorem, demonstrating graph prompts capacity to approximate graph transformation operators, effectively linking upstream and downstream tasks. Second, we derive upper bounds on the error of these data operations by graph prompts for a single graph and extend this discussion to batches of graphs, which are common in graph model training. Third, we analyze the distribution of data operation errors, extending our theoretical findings from linear graph models (e.g., GCN) to non-linear graph models (e.g., GAT). Extensive experiments support our theoretical results and confirm the practical implications of these guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01635v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qunzhong Wang, Xiangguo Sun, Hong Cheng</dc:creator>
    </item>
    <item>
      <title>Examining the Role of Relationship Alignment in Large Language Models</title>
      <link>https://arxiv.org/abs/2410.01708</link>
      <description>arXiv:2410.01708v1 Announce Type: cross 
Abstract: The rapid development and deployment of Generative AI in social settings raise important questions about how to optimally personalize them for users while maintaining accuracy and realism. Based on a Facebook public post-comment dataset, this study evaluates the ability of Llama 3.0 (70B) to predict the semantic tones across different combinations of a commenter's and poster's gender, age, and friendship closeness and to replicate these differences in LLM-generated comments.
  The study consists of two parts: Part I assesses differences in semantic tones across social relationship categories, and Part II examines the similarity between comments generated by Llama 3.0 (70B) and human comments from Part I given public Facebook posts as input. Part I results show that including social relationship information improves the ability of a model to predict the semantic tone of human comments. However, Part II results show that even without including social context information in the prompt, LLM-generated comments and human comments are equally sensitive to social context, suggesting that LLMs can comprehend semantics from the original post alone. When we include all social relationship information in the prompt, the similarity between human comments and LLM-generated comments decreases. This inconsistency may occur because LLMs did not include social context information as part of their training data. Together these results demonstrate the ability of LLMs to comprehend semantics from the original post and respond similarly to human comments, but also highlights their limitations in generalizing personalized comments through prompting alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01708v1</guid>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristen M. Altenburger, Hongda Jiang, Robert E. Kraut, Yi-Chia Wang, Jane Dwivedi-Yu</dc:creator>
    </item>
    <item>
      <title>Triangle Centrality</title>
      <link>https://arxiv.org/abs/2105.00110</link>
      <description>arXiv:2105.00110v3 Announce Type: replace-cross 
Abstract: Triangle centrality is introduced for finding important vertices in a graph based on the concentration of triangles surrounding each vertex. It has the distinct feature of allowing a vertex to be central if it is in many triangles or none at all.
  We show experimentally that triangle centrality is broadly applicable to many different types of networks. Our empirical results demonstrate that 30% of the time triangle centrality identified central vertices that differed with those found by five well-known centrality measures, which suggests novelty without being overly specialized. It is also asymptotically faster to compute on sparse graphs than all but the most trivial of these other measures.
  We introduce optimal algorithms that compute triangle centrality in $O(m\bar\delta)$ time and $O(m+n)$ space, where $\bar\delta\le O(\sqrt{m})$ is the $\textit{average degeneracy}$ introduced by Burkhardt, Faber, and Harris (2020). In practical applications, $\bar\delta$ is much smaller than $\sqrt{m}$ so triangle centrality can be computed in nearly linear time. On a Concurrent Read Exclusive Write (CREW) Parallel Random Access Machine (PRAM), we give a near work-optimal parallel algorithm that takes $O(\log n)$ time using $O(m\sqrt{m})$ CREW PRAM processors. In MapReduce, we show it takes four rounds using $O(m\sqrt{m})$ communication bits and is therefore optimal. We also derive a linear algebraic formulation of triangle centrality which can be computed in $O(m\bar\delta)$ time on sparse graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.00110v3</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>cs.SI</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Burkhardt</dc:creator>
    </item>
    <item>
      <title>Polynomial growth in degree-dependent first passage percolation on spatial random graphs</title>
      <link>https://arxiv.org/abs/2309.11880</link>
      <description>arXiv:2309.11880v3 Announce Type: replace-cross 
Abstract: In this paper we study a version of (non-Markovian) first passage percolation on graphs, where the transmission time between two connected vertices is non-iid, but increases by a penalty factor polynomial in their expected degrees. Based on the exponent of the penalty-polynomial, this makes it increasingly harder to transmit to and from high-degree vertices. This choice is motivated by awareness or time-limitations. For the iid part of the transmission times we allow any nonnegative distribution with regularly varying behaviour at $0$. For the underlying graph models we choose spatial random graphs that have power-law degree distributions, so that the effect of the penalisation becomes visible: (finite and infinite) Geometric Inhomogeneous Random Graphs, and Scale-Free Percolation. In these spatial models, the connection probability between two vertices depends on their spatial distance and on their expected degrees. We prove that upon increasing the penalty exponent, the transmission time between two far away vertices $x,y$ sweeps through four universal phases even for a single underlying graph: explosive (tight transmission times), polylogarithmic, polynomial but sublinear ($|x-y|^{\eta_0+o(1)}$ for an explicit $\eta_0&lt;1$), and linear ($\Theta(|x-y|)$) in their Euclidean distance. Further, none of these phases are restricted to phase boundaries, and those are non-trivial in the main model parameters: the tail of the degree-distribution, a long-range parameter, and the exponent of regular variation of the iid part of the transmission times. In this paper we present proofs of lower bounds for the latter two phases and the upper bound for the linear phase. These complement the matching upper bounds for the polynomial regime in our companion paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.11880v3</guid>
      <category>math.PR</category>
      <category>cs.SI</category>
      <category>math.CO</category>
      <category>q-bio.PE</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\'ulia Komj\'athy, John Lapinskas, Johannes Lengler, Ulysse Schaller</dc:creator>
    </item>
    <item>
      <title>Open-Set Graph Anomaly Detection via Normal Structure Regularisation</title>
      <link>https://arxiv.org/abs/2311.06835</link>
      <description>arXiv:2311.06835v4 Announce Type: replace-cross 
Abstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to train a detection model using a small number of normal and anomaly nodes (referred to as seen anomalies) to detect both seen anomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the training anomalies). Those labelled training data provide crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current supervised GAD methods tend to over-emphasise fitting the seen anomalies, leading to many errors of detecting the unseen anomalies as normal nodes. Further, existing open-set AD models were introduced to handle Euclidean data, failing to effectively capture discriminative features from graph structure and node attributes for GAD. In this work, we propose a novel open-set GAD approach, namely normal structure regularisation (NSReg), to achieve generalised detection ability to unseen anomalies, while maintaining its effectiveness on detecting seen anomalies. The key idea in NSReg is to introduce a regularisation term that enforces the learning of compact, semantically-rich representations of normal nodes based on their structural relations to other nodes. When being optimised with supervised anomaly detection losses, the regularisation term helps incorporate strong normality into the modelling, and thus, it effectively avoids over-fitting the seen anomalies and learns a better normality decision boundary, largely reducing the false negatives of detecting unseen anomalies as normal. Extensive empirical results on seven real-world datasets show that NSReg significantly outperforms state-of-the-art competing methods by at least 14% AUC-ROC on the unseen anomaly classes and by 10% AUC-ROC on all anomaly classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06835v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qizhou Wang, Guansong Pang, Mahsa Salehi, Xiaokun Xia, Christopher Leckie</dc:creator>
    </item>
    <item>
      <title>Joint Graph Rewiring and Feature Denoising via Spectral Resonance</title>
      <link>https://arxiv.org/abs/2408.07191</link>
      <description>arXiv:2408.07191v2 Announce Type: replace-cross 
Abstract: In graph learning the graph and the node features both contain noisy information about the node labels. In this paper we propose joint denoising and rewiring (JDR)--an algorithm to jointly rewire the graph and denoise the features, which improves the performance of downstream node classification graph neural nets (GNNs). JDR improves the alignment between the leading eigenspaces of graph and feature matrices. To approximately solve the associated non-convex optimization problem we propose a heuristic that efficiently handles real-world graph datasets with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and verify the effectiveness of our approach through extensive experiments on synthetic and real-world graph datasets. The results show that JDR consistently outperforms existing rewiring methods on node classification using GNNs as downstream models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07191v2</guid>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Linkerh\"agner, Cheng Shi, Ivan Dokmani\'c</dc:creator>
    </item>
  </channel>
</rss>
