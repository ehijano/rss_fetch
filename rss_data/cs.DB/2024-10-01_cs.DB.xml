<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Oct 2024 02:01:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>VecLSTM: Trajectory Data Processing and Management for Activity Recognition through LSTM Vectorization and Database Integration</title>
      <link>https://arxiv.org/abs/2409.19258</link>
      <description>arXiv:2409.19258v1 Announce Type: cross 
Abstract: Activity recognition is a challenging task due to the large scale of trajectory data and the need for prompt and efficient processing. Existing methods have attempted to mitigate this problem by employing traditional LSTM architectures, but these approaches often suffer from inefficiencies in processing large datasets. In response to this challenge, we propose VecLSTM, a novel framework that enhances the performance and efficiency of LSTM-based neural networks. Unlike conventional approaches, VecLSTM incorporates vectorization layers, leveraging optimized mathematical operations to process input sequences more efficiently. We have implemented VecLSTM and incorporated it into the MySQL database. To evaluate the effectiveness of VecLSTM, we compare its performance against a conventional LSTM model using a dataset comprising 1,467,652 samples with seven unique labels. Experimental results demonstrate superior accuracy and efficiency compared to the state-of-the-art, with VecLSTM achieving a validation accuracy of 85.57\%, a test accuracy of 85.47\%, and a weighted F1-score of 0.86. Furthermore, VecLSTM significantly reduces training time, offering a 26.2\% reduction compared to traditional LSTM models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19258v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.NE</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Solmaz Seyed Monir, Dongfang Zhao</dc:creator>
    </item>
    <item>
      <title>Efficient Approximation of Fractional Hypertree Width</title>
      <link>https://arxiv.org/abs/2409.20172</link>
      <description>arXiv:2409.20172v1 Announce Type: cross 
Abstract: We give two new approximation algorithms to compute the fractional hypertree width of an input hypergraph. The first algorithm takes as input $n$-vertex $m$-edge hypergraph $H$ of fractional hypertree width at most $\omega$, runs in polynomial time and produces a tree decomposition of $H$ of fractional hypertree width $O(\omega \log n \log \omega)$. As an immediate corollary this yields polynomial time $O(\log^2 n \log \omega)$-approximation algorithms for (generalized) hypertree width as well. To the best of our knowledge our algorithm is the first non-trivial polynomial-time approximation algorithm for fractional hypertree width and (generalized) hypertree width, as opposed to algorithms that run in polynomial time only when $\omega$ is considered a constant. For hypergraphs with the bounded intersection property we get better bounds, comparable with that recent algorithm of Lanzinger and Razgon [STACS 2024].
  The second algorithm runs in time $n^{\omega}m^{O(1)}$ and produces a tree decomposition of $H$ of fractional hypertree width $O(\omega \log^2 \omega)$. This significantly improves over the $(n+m)^{O(\omega^3)}$ time algorithm of Marx [ACM TALG 2010], which produces a tree decomposition of fractional hypertree width $O(\omega^3)$, both in terms of running time and the approximation ratio.
  Our main technical contribution, and the key insight behind both algorithms, is a variant of the classic Menger's Theorem for clique separators in graphs: For every graph $G$, vertex sets $A$ and $B$, family ${\cal F}$ of cliques in $G$, and positive rational $f$, either there exists a sub-family of $O(f \cdot \log^2 n)$ cliques in ${\cal F}$ whose union separates $A$ from $B$, or there exist $f \cdot \log |{\cal F}|$ paths from $A$ to $B$ such that no clique in ${\cal F}$ intersects more than $\log |{\cal F}|$ paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20172v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.DM</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Viktoriia Korchemna, Daniel Lokshtanov, Saket Saurabh, Vaishali Surianarayanan, Jie Xue</dc:creator>
    </item>
    <item>
      <title>Mixed-Precision Embeddings for Large-Scale Recommendation Models</title>
      <link>https://arxiv.org/abs/2409.20305</link>
      <description>arXiv:2409.20305v1 Announce Type: cross 
Abstract: Embedding techniques have become essential components of large databases in the deep learning era. By encoding discrete entities, such as words, items, or graph nodes, into continuous vector spaces, embeddings facilitate more efficient storage, retrieval, and processing in large databases. Especially in the domain of recommender systems, millions of categorical features are encoded as unique embedding vectors, which facilitates the modeling of similarities and interactions among features. However, numerous embedding vectors can result in significant storage overhead. In this paper, we aim to compress the embedding table through quantization techniques. Given that features vary in importance levels, we seek to identify an appropriate precision for each feature to balance model accuracy and memory usage. To this end, we propose a novel embedding compression method, termed Mixed-Precision Embeddings (MPE). Specifically, to reduce the size of the search space, we first group features by frequency and then search precision for each feature group. MPE further learns the probability distribution over precision levels for each feature group, which can be used to identify the most suitable precision with a specially designed sampling strategy. Extensive experiments on three public datasets demonstrate that MPE significantly outperforms existing embedding compression methods. Remarkably, MPE achieves about 200x compression on the Criteo dataset without comprising the prediction accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20305v1</guid>
      <category>cs.IR</category>
      <category>cs.DB</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiwei Li, Zhuoqi Hu, Fuyuan Lyu, Xing Tang, Haozhao Wang, Shijie Xu, Weihong Luo, Yuhua Li, Xue Liu, Xiuqiang He, Ruixuan Li</dc:creator>
    </item>
    <item>
      <title>Multi-Relational Algebra and Its Applications to Data Insights</title>
      <link>https://arxiv.org/abs/2311.04824</link>
      <description>arXiv:2311.04824v5 Announce Type: replace 
Abstract: A range of data insight analytical tasks involves analyzing a large set of tables of different schemas, possibly induced by various groupings, to find salient patterns. This paper presents Multi-Relational Algebra, an extension of the classic Relational Algebra, to facilitate such transformations and their compositions. Multi-Relational Algebra has two main characteristics: (1) Information Unit. The information unit is a slice $(r, X)$, where $r$ is a (region) tuple, and $X$ is a (feature) table. Specifically, a slice can encompass multiple columns, which surpasses the information unit of "a single tuple" or "a group of tuples of one column" in the classic relational algebra, (2) Schema Flexibility. Slices can have varying schemas, not constrained to a single schema. This flexibility further expands the expressive power of the algebra. Through various examples, we show that multi-relational algebra can effortlessly express many complex analytic problems, some of which are beyond the scope of traditional relational analytics. We have implemented and deployed a service for multi-relational analytics. Due to a unified logical design, we are able to conduct systematic optimization for a variety of seemingly different tasks. Our service has garnered interest from numerous internal teams who have developed data-insight applications using it, and serves millions of operators daily.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04824v5</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.PL</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Wu, Zichen Zhu, Xiangyao Yu, Shaleen Deep, Stratis Viglas, John Cieslewicz, Somesh Jha, Jeffrey F. Naughton</dc:creator>
    </item>
    <item>
      <title>Efficient Rare Temporal Pattern Mining in Time Series</title>
      <link>https://arxiv.org/abs/2409.05042</link>
      <description>arXiv:2409.05042v2 Announce Type: replace 
Abstract: Time series data from various domains is continuously growing, and extracting and analyzing temporal patterns within these series can provide valuable insights. Temporal pattern mining (TPM) extends traditional pattern mining by incorporating event time intervals into patterns, making them more expressive but also increasing the computational complexity in terms of time and space. One important type of temporal pattern is the rare temporal pattern (RTP), which occurs infrequently but with high confidence. Mining these rare patterns poses several challenges, for example, the low support threshold can lead to a combinatorial explosion and the generation of many irrelevant patterns. To address this, an efficient approach to mine rare temporal patterns is essential. This paper introduces the Rare Temporal Pattern Mining from Time Series (RTPMfTS) method, designed to discover rare temporal patterns. The key contributions of this work are as follows: (1) An end-to-end RTPMfTS process that takes time series data as input and outputs rare temporal patterns. (2) A highly efficient Rare Temporal Pattern Mining (RTPM) algorithm, which leverages optimized data structures for fast event and pattern retrieval, as well as effective pruning techniques to accelerate the mining process. (3) A comprehensive experimental evaluation of RTPM, demonstrating that it outperforms the baseline in both runtime and memory efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05042v2</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Van Ho Long, Nguyen Ho, Trinh Le Cong, Anh-Vu Dinh-Duc, Tu Nguyen Ngoc</dc:creator>
    </item>
    <item>
      <title>An Intelligent Innovation Dataset on Scientific Research Outcomes</title>
      <link>https://arxiv.org/abs/2409.06936</link>
      <description>arXiv:2409.06936v2 Announce Type: replace 
Abstract: Various stakeholders, such as researchers, government agencies, businesses, and research laboratories require a large volume of reliable scientific research outcomes including research articles and patent data to support their work. These data are crucial for a variety of application, such as advancing scientific research, conducting business evaluations, and undertaking policy analysis. However, collecting such data is often a time-consuming and laborious task. Consequently, many users turn to using openly accessible data for their research. However, these existing open dataset releases typically suffer from lack of relationship between different data sources and a limited temporal coverage. To address this issue, we present a new open dataset, the Intelligent Innovation Dataset (IIDS), which comprises six interrelated datasets spanning nearly 120 years, encompassing paper information, paper citation relationships, patent details, patent legal statuses, and funding information. The extensive contextual and extensive temporal coverage of the IIDS dataset will provide researchers and practitioners and policy maker with comprehensive data support, enabling them to conduct in-depth scientific research and comprehensive data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06936v2</guid>
      <category>cs.DB</category>
      <category>cs.DL</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinran Wu, Hui Zou, Yidan Xing, Jingjing Qu, Qiongxiu Li, Renxia Xue, Xiaoming Fu</dc:creator>
    </item>
  </channel>
</rss>
