<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Jul 2024 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Learning Cellular Network Connection Quality with Conformal</title>
      <link>https://arxiv.org/abs/2407.10976</link>
      <description>arXiv:2407.10976v1 Announce Type: new 
Abstract: In this paper, we address the problem of uncertainty quantification for cellular network speed. It is a well-known fact that the actual internet speed experienced by a mobile phone can fluctuate significantly, even when remaining in a single location. This high degree of variability underscores that mere point estimation of network speed is insufficient. Rather, it is advantageous to establish a prediction interval that can encompass the expected range of speed variations. In order to build an accurate network estimation map, numerous mobile data need to be collected at different locations. Currently, public datasets rely on users to upload data through apps. Although massive data has been collected, the datasets suffer from significant noise due to the nature of cellular networks and various other factors. Additionally, the uneven distribution of population density affects the spatial consistency of data collection, leading to substantial uncertainty in the network quality maps derived from this data. We focus our analysis on large-scale internet-quality datasets provided by Ookla to construct an estimated map of connection quality. To improve the reliability of this map, we introduce a novel conformal prediction technique to build an uncertainty map. We identify regions with heightened uncertainty to prioritize targeted, manual data collection. In addition, the uncertainty map quantifies how reliable the prediction is in different areas. Our method also leads to a sampling strategy that guides researchers to selectively gather high-quality data that best complement the current dataset to improve the overall accuracy of the prediction model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10976v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanyang Jiang, Elizabeth Belding, Ellen Zegure, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Diffusion Model-based Incentive Mechanism with Prospect Theory for Edge AIGC Services in 6G IoT</title>
      <link>https://arxiv.org/abs/2407.10979</link>
      <description>arXiv:2407.10979v1 Announce Type: new 
Abstract: The fusion of Internet of Things (IoT) with Sixth-Generation (6G) technology has significant potential to revolutionize the IoT landscape. Utilizing the ultra-reliable and low-latency communication capabilities of 6G, 6G-IoT networks can transmit high-quality and diverse data to enhance edge learning. Artificial Intelligence-Generated Content (AIGC) harnesses advanced AI algorithms to automatically generate various types of content. The emergence of edge AIGC integrates with edge networks, facilitating real-time provision of customized AIGC services by deploying AIGC models on edge devices. However, the current practice of edge devices as AIGC Service Providers (ASPs) lacks incentives, hindering the sustainable provision of high-quality edge AIGC services amidst information asymmetry. In this paper, we develop a user-centric incentive mechanism framework for edge AIGC services in 6G-IoT networks. Specifically, we first propose a contract theory model for incentivizing ASPs to provide AIGC services to clients. Recognizing the irrationality of clients towards personalized AIGC services, we utilize Prospect Theory (PT) to better capture the subjective utility of clients. Finally, we adopt the generative diffusion model to generate the optimal contract design under PT, outperforming traditional deep reinforcement learning algorithms, i.e., soft actor-critic algorithms. Our numerical results demonstrate the effectiveness of the proposed scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10979v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinbo Wen, Jiangtian Nie, Yue Zhong, Changyan Yi, Xiaohuan Li, Jiangming Jin, Yang Zhang, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>Learning-based Big Data Sharing Incentive in Mobile AIGC Networks</title>
      <link>https://arxiv.org/abs/2407.10980</link>
      <description>arXiv:2407.10980v1 Announce Type: new 
Abstract: Rapid advancements in wireless communication have led to a dramatic upsurge in data volumes within mobile edge networks. These substantial data volumes offer opportunities for training Artificial Intelligence-Generated Content (AIGC) models to possess strong prediction and decision-making capabilities. AIGC represents an innovative approach that utilizes sophisticated generative AI algorithms to automatically generate diverse content based on user inputs. Leveraging mobile edge networks, mobile AIGC networks enable customized and real-time AIGC services for users by deploying AIGC models on edge devices. Nonetheless, several challenges hinder the provision of high-quality AIGC services, including issues related to the quality of sensing data for AIGC model training and the establishment of incentives for big data sharing from mobile devices to edge devices amidst information asymmetry. In this paper, we initially define a Quality of Data (QoD) metric based on the age of information to quantify the quality of sensing data. Subsequently, we propose a contract theoretic model aimed at motivating mobile devices for big data sharing. Furthermore, we employ a Proximal Policy Optimization (PPO) algorithm to determine the optimal contract. Finally, numerical results demonstrate the efficacy and reliability of the proposed PPO-based contract model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10980v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinbo Wen, Yang Zhang, Yulin Chen, Weifeng Zhong, Xumin Huang, Lei Liu, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>Systematic Literature Review of AI-enabled Spectrum Management in 6G and Future Networks</title>
      <link>https://arxiv.org/abs/2407.10981</link>
      <description>arXiv:2407.10981v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) has advanced significantly in various domains like healthcare, finance, and cybersecurity, with successes such as DeepMind's medical imaging and Tesla's autonomous vehicles. As telecommunications transition from 5G to 6G, integrating AI is crucial for complex demands like data processing, network optimization, and security. Despite ongoing research, there's a gap in consolidating AI-enabled Spectrum Management (AISM) advancements. Traditional spectrum management methods are inadequate for 6G due to its dynamic and complex demands, making AI essential for spectrum optimization, security, and network efficiency. This study aims to address this gap by: (i) Conducting a systematic review of AISM methodologies, focusing on learning models, data handling techniques, and performance metrics. (ii) Examining security and privacy concerns related to AI and traditional network threats within AISM contexts. Using the Systematic Literature Review (SLR) methodology, we meticulously analyzed 110 primary studies to: (a) Identify AI's utility in spectrum management. (b) Develop a taxonomy of AI approaches. (c) Classify datasets and performance metrics used. (d) Detail security and privacy threats and countermeasures. Our findings reveal challenges such as under-explored AI usage in critical AISM systems, computational resource demands, transparency issues, the need for real-world datasets, imbalances in security and privacy research, and the absence of testbeds, benchmarks, and security analysis tools. Addressing these challenges is vital for maximizing AI's potential in advancing 6G technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10981v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bushra Sabir, Shuiqiao Yang, David Nguyen, Nan Wu, Alsharif Abuadbba, Hajime Suzuki, Shangqi Lai, Wei Ni, Ding Ming, Surya Nepal</dc:creator>
    </item>
    <item>
      <title>ARA-O-RAN: End-to-End Programmable O-RAN Living Lab for Agriculture and Rural Communities</title>
      <link>https://arxiv.org/abs/2407.10982</link>
      <description>arXiv:2407.10982v1 Announce Type: new 
Abstract: As wireless networks evolve towards open architectures like O-RAN, testing, and integration platforms are crucial to address challenges like interoperability. This paper describes ARA-O-RAN, a novel O-RAN testbed established through the NSF Platforms for Advanced Wireless Research (PAWR) ARA platform. ARA provides an at-scale rural wireless living lab focused on technologies for digital agriculture and rural communities. As an O-RAN Alliance certified Open Testing and Integration Centre (OTIC), ARA launched ARA-O-RAN -- the first public O-RAN testbed tailored to rural and agriculture use cases, together with the end-to-end, whole-stack programmability. ARA-O-RAN uniquely combines support for outdoor testing across a university campus, surrounding farmlands, and rural communities with a 50-node indoor sandbox. The testbed facilitates vital R\&amp;D to implement open architectures that can meet rural connectivity needs. The paper outlines ARA-O-RAN's hardware system design, software architecture, and enabled research experiments. It also discusses plans aligned with national spectrum policy and rural spectrum innovation. ARA-O-RAN exemplifies the value of purpose-built wireless testbeds in accelerating impactful wireless research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10982v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyi Zhang, Joshua Ofori Boateng, Taimoor UI Islam, Arsalan Ahmad, Hongwei Zhang, Daji Qiao</dc:creator>
    </item>
    <item>
      <title>On the Combination of AI and Wireless Technologies: 3GPP Standardization Progress</title>
      <link>https://arxiv.org/abs/2407.10984</link>
      <description>arXiv:2407.10984v1 Announce Type: new 
Abstract: Combing Artificial Intelligence (AI) and wireless communication technologies has become one of the major technologies trends towards 2030. This includes using AI to improve the efficiency of the wireless transmission and supporting AI deployment with wireless networks. In this article, the latest progress of the Third Generation Partnership Project (3GPP) standards development is introduced. Concentrating on AI model distributed transfer and AI for Beam Management (BM) with wireless network, we introduce the latest studies and explain how the existing standards should be modified to incorporate the results from academia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10984v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Sun, Tao Cui, Wenqi Zhang, Yingshuang Bai, Shuo Wang, Haojin Li</dc:creator>
    </item>
    <item>
      <title>Strategies for Tracking Individual IP Packets Towards DDoS</title>
      <link>https://arxiv.org/abs/2407.10985</link>
      <description>arXiv:2407.10985v1 Announce Type: new 
Abstract: The identification of the exact path that packets are routed in the network is quite a challenge. This paper presents a novel, efficient traceback strategy in combination with a defence system against distributed denial of service (DDoS) attacks named Tracemax. A single packets can be directly traced over many more hops than the current existing techniques allow. It let good connections pass while bad ones get thwarted. Initiated by the victim the routers in the network cooperate in tracing and become automatically self-organised and self-managed. The novel concept support analyses of packet flows and transmission paths in a network infrastructure. It can effectively reduce the effect of common bandwidth and resource consumption attacks and foster in addition early warning and prevention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10985v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1515/pik-2015-0010</arxiv:DOI>
      <arxiv:journal_reference>PIK - Praxis der Informationsverarbeitung und Kommunikation 2015</arxiv:journal_reference>
      <dc:creator>Peter Hillmann, Frank Tietze, Gabi Dreo Rodosek</dc:creator>
    </item>
    <item>
      <title>Adaptive Digital Twin and Communication-Efficient Federated Learning Network Slicing for 5G-enabled Internet of Things</title>
      <link>https://arxiv.org/abs/2407.10987</link>
      <description>arXiv:2407.10987v1 Announce Type: new 
Abstract: Network slicing enables industrial Internet of Things (IIoT) networks with multiservice and differentiated resource requirements to meet increasing demands through efficient use and management of network resources. Typically, the network slice orchestrator relies on demand forecasts for each slice to make informed decisions and maximize resource utilization. The new generation of Industry 4.0 has introduced digital twins to map physical systems to digital models for accurate decision-making. In our approach, we first use graph-attention networks to build a digital twin environment for network slices, enabling real-time traffic analysis, monitoring, and demand forecasting. Based on these predictions, we formulate the resource allocation problem as a federated multi-agent reinforcement learning problem and employ a deep deterministic policy gradient to determine the resource allocation policy while preserving the privacy of the slices. Our results demonstrate that the proposed approaches can improve the accuracy of demand prediction for network slices and reduce the communication overhead of dynamic network slicing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10987v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Ayepah-Mensah, Guolin Sun, Yu Pang, Wei Jiang</dc:creator>
    </item>
    <item>
      <title>Selfish Carrier Monitoring in WIFI Using Distributed Sniffers</title>
      <link>https://arxiv.org/abs/2407.10997</link>
      <description>arXiv:2407.10997v1 Announce Type: new 
Abstract: This work proposes a tool to estimate the interference between nodes and links in a live wireless network by passive monitoring of wireless traffic. This approach requires deploying multiple sniffers across the network to capture wireless traffic traces. These traces are then analyzed using a machine learning approach to infer the carrier-sense relationship between network nodes. It also demonstrates an important application of this tool-detection of selfish carrier-sense behavior. This is based on identifying any asymmetry in carrier-sense behavior between node pairs and finding multiple witnesses to raise confidence. Simulation results demonstrate that the proposed approach of estimating interference relations is significantly more accurate than simpler heuristics and quite competitive with active measurements. Minimizing router overhead taken as a main goal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10997v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>U Sinthuja, R Sridevi</dc:creator>
    </item>
    <item>
      <title>Online Multi-Task Offloading for Semantic-Aware Edge Computing Systems</title>
      <link>https://arxiv.org/abs/2407.11018</link>
      <description>arXiv:2407.11018v1 Announce Type: new 
Abstract: Mobile edge computing (MEC) provides low-latency offloading solutions for computationally intensive tasks, effectively improving the computing efficiency and battery life of mobile devices. However, for data-intensive tasks or scenarios with limited uplink bandwidth, network congestion might occur due to massive simultaneous offloading nodes, increasing transmission latency and affecting task performance. In this paper, we propose a semantic-aware multi-modal task offloading framework to address the challenges posed by limited uplink bandwidth. By introducing a semantic extraction factor, we balance the relationship among transmission latency, computation energy consumption, and task performance. To measure the offloading performance of multi-modal tasks, we design a unified and fair quality of experience (QoE) metric that includes execution latency, energy consumption, and task performance. Lastly, we formulate the optimization problem as a Markov decision process (MDP) and exploit the multi-agent proximal policy optimization (MAPPO) reinforcement learning algorithm to jointly optimize the semantic extraction factor, communication resources, and computing resources to maximize overall QoE. Experimental results show that the proposed method achieves a reduction in execution latency and energy consumption of 18.1% and 12.9%, respectively compared with the semantic-unaware approach. Moreover, the proposed approach can be easily extended to models with different user preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11018v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuyang Chen, Qu Luo, Gaojie Chen, Daquan Feng, Yao Sun</dc:creator>
    </item>
    <item>
      <title>Enhancing Vehicular Networks with Generative AI: Opportunities and Challenges</title>
      <link>https://arxiv.org/abs/2407.11020</link>
      <description>arXiv:2407.11020v1 Announce Type: new 
Abstract: In the burgeoning field of intelligent transportation systems, the integration of Generative Artificial Intelligence (AI) into vehicular networks presents a transformative potential for the automotive industry. This paper explores the innovative applications of generative AI in enhancing communication protocols, optimizing traffic management, and bolstering security frameworks within vehicular networks. By examining current technologies and recent advancements, we identify key challenges such as scalability, real-time data processing, and security vulnerabilities that come with AI integration. Additionally, we propose novel applications and methodologies that leverage generative AI to simulate complex network scenarios, generate adaptive communication schemes, and enhance predictive capabilities for traffic conditions. This study not only reviews the state of the art but also highlights significant opportunities where generative AI can lead to groundbreaking improvements in vehicular network efficiency and safety. Through this comprehensive exploration, our findings aim to guide future research directions and foster a deeper understanding of generative AI's role in the next generation of vehicular technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11020v1</guid>
      <category>cs.NI</category>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Teef David, Kassi Muhammad, Kevin Nassisid, Bronny Farus</dc:creator>
    </item>
    <item>
      <title>PCAPVision: PCAP-Based High-Velocity and Large-Volume Network Failure Detection</title>
      <link>https://arxiv.org/abs/2407.11021</link>
      <description>arXiv:2407.11021v1 Announce Type: new 
Abstract: Detecting failures via analysis of Packet Capture (PCAP) files is crucial for maintaining network reliability and performance, especially in large-scale telecommunications networks. Traditional methods, relying on manual inspection and rule-based systems, are often too slow and labor-intensive to meet the demands of modern networks. In this paper, we present PCAPVision, a novel approach that utilizes computer vision and Convolutional Neural Networks (CNNs) to detect failures in PCAP files. By converting PCAP data into images, our method leverages the robust pattern recognition capabilities of CNNs to analyze network traffic efficiently. This transformation process involves encoding packet data into structured images, enabling rapid and accurate failure detection. Additionally, we incorporate a continual learning framework, leveraging automated annotation for the feedback loop, to adapt the model dynamically and ensure sustained performance over time. Our approach significantly reduces the time required for failure detection. The initial training phase uses a Voice Over LTE (VoLTE) dataset, demonstrating the model's effectiveness and generalizability when using transfer learning on Mobility Management services. This work highlights the potential of integrating computer vision techniques in network analysis, offering a scalable and efficient solution for real-time network failure detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11021v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukasz Tulczyjew, Ihor Biruk, Murat Bilgic, Charles Abondo, Nathanael Weill</dc:creator>
    </item>
    <item>
      <title>Exploring the 6G Potentials: Immersive, Hyper Reliable, and Low-Latency Communication</title>
      <link>https://arxiv.org/abs/2407.11051</link>
      <description>arXiv:2407.11051v1 Announce Type: new 
Abstract: The transition towards the sixth-generation (6G) wireless telecommunications networks introduces significant challenges for researchers and industry stakeholders. The 6G technology aims to enhance existing usage scenarios, particularly supporting innovative applications requiring stringent performance metrics. Among the key performance indicators (KPIs) for 6G, immersive throughput, hyper-reliability, and hyper-low latency must be achieved simultaneously in some critical applications to achieve the application requirements. However, this is challenging due to the conflicting nature of these KPIs. This article proposes a new service class of 6G as immersive, hyper reliable, and low-latency communication (IHRLLC), and introduces a potential network architecture to achieve the associated KPIs. Specifically, technologies such as ultra-massive multiple-input multiple-output (umMIMO)-aided terahertz (THz) communications, and reconfigurable intelligent surfaces (RIS) are viewed as the key enablers for achieving immersive data rate and hyper reliability. Given the computational complexity involved in employing these technologies, and the challenges encountered in designing real-time algorithms for efficient resource allocation and management strategies as well as dynamic beamforming and tracking techniques, we also propose the involvement of other potential enabling technologies such as non-terrestrial networks (NTN), learn-to-optimize (L2O) and generative-AI (GenAI) technologies, quantum computing, and network digital twin (NDT) for limiting the latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11051v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Afsoon Alidadi Shamsabadi, Animesh Yadav, Yasser Gadallah, Halim Yanikomeroglu</dc:creator>
    </item>
    <item>
      <title>Joint Optimization of Completion Ratio and Latency of Offloaded Tasks with Multiple Priority Levels in 5G Edge</title>
      <link>https://arxiv.org/abs/2407.11155</link>
      <description>arXiv:2407.11155v1 Announce Type: new 
Abstract: Multi-Access Edge Computing (MEC) is widely recognized as an essential enabler for applications that necessitate minimal latency. However, the dropped task ratio metric has not been studied thoroughly in literature. Neglecting this metric can potentially reduce the system's capability to effectively manage tasks, leading to an increase in the number of eliminated or unprocessed tasks. This paper presents a 5G-MEC task offloading scenario with a focus on minimizing the dropped task ratio, computational latency, and communication latency. We employ Mixed Integer Linear Programming (MILP), Particle Swarm Optimization (PSO), and Genetic Algorithm (GA) to optimize the latency and dropped task ratio. We conduct an analysis on how the quantity of tasks and User Equipment (UE) impacts the ratio of dropped tasks and the latency. The tasks that are generated by UEs are classified into two categories: urgent tasks and non-urgent tasks. The UEs with urgent tasks are prioritized in processing to ensure a zero-dropped task ratio. Our proposed method improves the performance of the baseline methods, First Come First Serve (FCFS) and Shortest Task First (STF), in the context of 5G-MEC task offloading. Under the MILP-based approach, the latency is reduced by approximately 55% compared to GA and 35% compared to PSO. The dropped task ratio under the MILP-based approach is reduced by approximately 70% compared to GA and by 40% compared to PSO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11155v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Parisa Fard Moshiri, Murat Simsek, Burak Kantarci</dc:creator>
    </item>
    <item>
      <title>Spatial-spectral Cell-free Networks: A Large-scale Case Study</title>
      <link>https://arxiv.org/abs/2407.11389</link>
      <description>arXiv:2407.11389v1 Announce Type: new 
Abstract: This paper studies the large-scale cell-free networks where dense distributed access points (APs) serve many users. As a promising next-generation network architecture, cell-free networks enable ultra-reliable connections and minimal fading/blockage, which are much favorable to the millimeter wave and Terahertz transmissions. However, conventional beam management with large phased arrays in a cell is very time-consuming in the higher-frequencies, and could be worsened when deploying a large number of coordinated APs in the cell-free systems. To tackle this challenge, the spatial-spectral cell-free networks with the leaky-wave antennas are established by coupling the propagation angles with frequencies. The beam training overhead in this direction can be significantly reduced through exploiting such spatial-spectral coupling effects. In the considered large-scale spatial-spectral cell-free networks, a novel subchannel allocation solution at sub-terahertz bands is proposed by leveraging the relationship between cross-entropy method and mixture model. Since initial access and AP clustering play a key role in achieving scalable large-scale cell-free networks, a hierarchical AP clustering solution is proposed to make the joint initial access and cluster formation, which is adaptive and has no need to initialize the number of AP clusters. After AP clustering, a subchannel allocation solution is devised to manage the interference between AP clusters. Numerical results are presented to confirm the efficiency of the proposed solutions and indicate that besides subchannel allocation, AP clustering can also have a big impact on the large-scale cell-free network performance at sub-terahertz bands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11389v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zesheng Zhu, Lifeng Wang, Xin Wang, Dongming Wang, Kai-Kit Wong</dc:creator>
    </item>
    <item>
      <title>Performance Analysis of Internet of Vehicles Mesh Networks Based on Actual Switch Models</title>
      <link>https://arxiv.org/abs/2407.11483</link>
      <description>arXiv:2407.11483v1 Announce Type: new 
Abstract: The rapid growth of the automotive industry has exacerbated the conflict between the complex traffic environment, increasing communication demands, and limited resources. Given the imperative to mitigate traffic and network congestion, analyzing the performance of Internet of Vehicles (IoV) mesh networks is of great practical significance. Most studies focus solely on individual performance metrics and influencing factors, and the adopted simulation tools, such as OPNET, cannot achieve the dynamic link generation of IoV mesh networks. To address these problems, a network performance analysis model based on actual switches is proposed. First, a typical IoV mesh network architecture is constructed and abstracted into a mathematical model that describes how the link and topology changes over time. Then, the task generation model and the task forwarding model based on actual switches are proposed to obtain the real traffic distribution of the network. Finally, a scientific network performance indicator system is constructed. Simulation results demonstrate that, with rising task traffic and decreasing node caching capacity, the packet loss rate increases, and the task arrival rate decreases in the network. The proposed model can effectively evaluate the network performance across various traffic states and provide valuable insights for network construction and enhancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11483v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialin Hu, Zhiyuan Ren, Wenchi Cheng, Zhiliang Shuai, Zhao Li</dc:creator>
    </item>
    <item>
      <title>PandORA: Automated Design and Comprehensive Evaluation of Deep Reinforcement Learning Agents for Open RAN</title>
      <link>https://arxiv.org/abs/2407.11747</link>
      <description>arXiv:2407.11747v1 Announce Type: new 
Abstract: The highly heterogeneous ecosystem of NextG wireless communication systems calls for novel networking paradigms where functionalities and operations can be dynamically and optimally reconfigured in real time to adapt to changing traffic conditions and satisfy stringent and diverse QoS demands. Open RAN technologies, and specifically those being standardized by the O-RAN Alliance, make it possible to integrate network intelligence into the once monolithic RAN via intelligent applications, namely, xApps and rApps. These applications enable flexible control of the network resources and functionalities, network management, and orchestration through data-driven intelligent control loops. Recent work has showed how DRL is effective in dynamically controlling O-RAN systems. However, how to design these solutions in a way that manages heterogeneous optimization goals and prevents unfair resource allocation is still an open challenge, with the logic within DRL agents often considered as a black box. In this paper, we introduce PandORA, a framework to automatically design and train DRL agents for Open RAN applications, package them as xApps and evaluate them in the Colosseum wireless network emulator. We benchmark $23$ xApps that embed DRL agents trained using different architectures, reward design, action spaces, and decision-making timescales, and with the ability to hierarchically control different network parameters. We test these agents on the Colosseum testbed under diverse traffic and channel conditions, in static and mobile setups. Our experimental results indicate how suitable fine-tuning of the RAN control timers, as well as proper selection of reward designs and DRL architectures can boost network performance according to the network conditions and demand. Notably, finer decision-making granularities can improve mMTC's performance by ~56% and even increase eMBB Throughput by ~99%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11747v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Tsampazi, Salvatore D'Oro, Michele Polese, Leonardo Bonati, Gwenael Poitau, Michael Healy, Mohammad Alavirad, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>An Overview and Solution for Democratizing AI Workflows at the Network Edge</title>
      <link>https://arxiv.org/abs/2407.11905</link>
      <description>arXiv:2407.11905v1 Announce Type: new 
Abstract: With the process of democratization of the network edge, hardware and software for networks are becoming available to the public, overcoming the confines of traditional cloud providers and network operators. This trend, coupled with the increasing importance of AI in 6G and beyond cellular networks, presents opportunities for innovative AI applications and systems at the network edge. While AI models and services are well-managed in cloud systems, achieving similar maturity for serving network needs remains an open challenge. Existing open solutions are emerging and are yet to consider democratization requirements. In this work, we identify key requirements for democratization and propose NAOMI, a solution for democratizing AI/ML workflows at the network edge designed based on those requirements. Guided by the functionality and overlap analysis of the O-RAN AI/ML workflow architecture and MLOps systems, coupled with the survey of open-source AI/ML tools, we develop a modular, scalable, and distributed hardware architecture-independent solution. NAOMI leverages state-of-the-art open-source tools and can be deployed on distributed clusters of heterogeneous devices. The results show that NAOMI performs up to 40% better in deployment time and up to 73% faster in AI/ML workflow execution for larger datasets compared to AI/ML Framework, a representative open network access solution, while performing inference and utilizing resources on par with its counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11905v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrej \v{C}op, Bla\v{z} Bertalani\v{c}, Carolina Fortuna</dc:creator>
    </item>
    <item>
      <title>Integrating Base Station with Intelligent Surface for 6G Wireless Networks: Architectures, Design Issues, and Future Directions</title>
      <link>https://arxiv.org/abs/2407.10986</link>
      <description>arXiv:2407.10986v1 Announce Type: cross 
Abstract: Intelligent surface (IS) is envisioned as a promising technology for the sixth-generation (6G) wireless networks, which can effectively reconfigure the wireless propagation environment via dynamically controllable signal reflection/transmission. In particular, integrating passive intelligent surface (IS) into the base station (BS) is a novel solution to enhance the wireless network throughput and coverage both cost-effectively and energyefficiently. In this article, we provide an overview of IS-integrated BSs for wireless networks, including their motivations, practical architectures, and main design issues. Moreover, numerical results are presented to compare the performance of different IS-integrated BS architectures as well as the conventional BS without IS. Finally, promising directions are pointed out to stimulate future research on IS-BS/terminal integration in wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10986v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuwei Huang, Lipeng Zhu, Rui Zhang</dc:creator>
    </item>
    <item>
      <title>Hybrid-Generative Diffusion Models for Attack-Oriented Twin Migration in Vehicular Metaverses</title>
      <link>https://arxiv.org/abs/2407.11036</link>
      <description>arXiv:2407.11036v1 Announce Type: cross 
Abstract: The vehicular metaverse is envisioned as a blended immersive domain that promises to bring revolutionary changes to the automotive industry. As a core component of vehicular metaverses, Vehicle Twins (VTs) are digital twins that cover the entire life cycle of vehicles, providing immersive virtual services for Vehicular Metaverse Users (VMUs). Vehicles with limited resources offload the computationally intensive tasks of constructing and updating VTs to edge servers and migrate VTs between these servers, ensuring seamless and immersive experiences for VMUs. However, the high mobility of vehicles, uneven deployment of edge servers, and potential security threats pose challenges to achieving efficient and reliable VT migrations. To address these issues, we propose a secure and reliable VT migration framework in vehicular metaverses. Specifically, we design a two-layer trust evaluation model to comprehensively evaluate the reputation value of edge servers in the network communication and interaction layers. Then, we model the VT migration problem as a partially observable Markov decision process and design a hybrid-Generative Diffusion Model (GDM) algorithm based on deep reinforcement learning to generate optimal migration decisions by taking hybrid actions (i.e., continuous actions and discrete actions). Numerical results demonstrate that the hybrid-GDM algorithm outperforms the baseline algorithms, showing strong adaptability in various settings and highlighting the potential of the hybrid-GDM algorithm for addressing various optimization issues in vehicular metaverses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11036v1</guid>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingkai Kang, Jinbo Wen, Jiawen Kang, Tao Zhang, Hongyang Du, Dusit Niyato, Rong Yu, Shengli Xie</dc:creator>
    </item>
    <item>
      <title>An open source Multi-Agent Deep Reinforcement Learning Routing Simulator for satellite networks</title>
      <link>https://arxiv.org/abs/2407.11047</link>
      <description>arXiv:2407.11047v1 Announce Type: cross 
Abstract: This paper introduces an open source simulator for packet routing in Low Earth Orbit Satellite Constellations (LSatCs) considering the dynamic system uncertainties. The simulator, implemented in Python, supports traditional Dijkstra's based routing as well as more advanced learning solutions, specifically Q-Routing and Multi-Agent Deep Reinforcement Learning (MA-DRL) from our previous work. It uses an event-based approach with the SimPy module to accurately simulate packet creation, routing and queuing, providing real-time tracking of queues and latency. The simulator is highly configurable, allowing adjustments in routing policies, traffic, ground and space layer topologies, communication parameters, and learning hyperparameters. Key features include the ability to visualize system motion and track packet paths. Results highlight significant improvements in end-to-end (E2E) latency using Reinforcement Learning (RL)-based routing policies compared to traditional methods. The source code, the documentation and a Jupyter notebook with post-processing results and analysis are available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11047v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Lozano-Cuadra, Mathias D. Thorsager, Israel Leyva-Mayorga, Beatriz Soret</dc:creator>
    </item>
    <item>
      <title>Digital Twin Vehicular Edge Computing Network: Task Offloading and Resource Allocation</title>
      <link>https://arxiv.org/abs/2407.11310</link>
      <description>arXiv:2407.11310v1 Announce Type: cross 
Abstract: With the increasing demand for multiple applications on internet of vehicles. It requires vehicles to carry out multiple computing tasks in real time. However, due to the insufficient computing capability of vehicles themselves, offloading tasks to vehicular edge computing (VEC) servers and allocating computing resources to tasks becomes a challenge. In this paper, a multi task digital twin (DT) VEC network is established. By using DT to develop offloading strategies and resource allocation strategies for multiple tasks of each vehicle in a single slot, an optimization problem is constructed. To solve it, we propose a multi-agent reinforcement learning method on the task offloading and resource allocation. Numerous experiments demonstrate that our method is effective compared to other benchmark algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11310v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Xie, Qiong Wu, Pingyi Fan</dc:creator>
    </item>
    <item>
      <title>Analytical Performance Estimations for Quantum Repeater Network Scenarios</title>
      <link>https://arxiv.org/abs/2407.11376</link>
      <description>arXiv:2407.11376v1 Announce Type: cross 
Abstract: Quantum repeater chains will form the backbone of future quantum networks that distribute entanglement between network nodes. Therefore, it is important to understand the entanglement distribution performance of quantum repeater chains, especially their throughput and latency. By using Markov chains to model the stochastic dynamics in quantum repeater chains, we offer analytical estimations for long-run throughput and on-demand latency of continuous entanglement distribution. We first study single-link entanglement generation using general multiheralded protocols. We then model entanglement distribution with entanglement swapping over two links, using either a single- or a double-heralded entanglement generation protocol. We also demonstrate how the two-link results offer insights into the performance of general $2^k$-link nested repeater chains. Our results enrich the quantitative understanding of quantum repeater network performance, especially the dependence on system parameters. The analytical formulae themselves are valuable reference resources for the quantum networking community. They can serve as benchmarks for quantum network simulation validation or as examples of quantum network dynamics modeling using the Markov chain formalism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11376v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Allen Zang, Joaquin Chung, Rajkumar Kettimuthu, Martin Suchara, Tian Zhong</dc:creator>
    </item>
    <item>
      <title>Time-Sensitive Networking (TSN) for Industrial Automation: Current Advances and Future Directions</title>
      <link>https://arxiv.org/abs/2306.03691</link>
      <description>arXiv:2306.03691v4 Announce Type: replace 
Abstract: With the introduction of Cyber-Physical Systems (CPS) and Internet of Things (IoT) technologies, the automation industry is undergoing significant changes, particularly in improving production efficiency and reducing maintenance costs. Industrial automation applications often need to transmit time- and safety-critical data to closely monitor and control industrial processes. Several Ethernet-based fieldbus solutions, such as PROFINET IRT, EtherNet/IP, and EtherCAT, are widely used to ensure real-time communications in industrial automation systems. These solutions, however, commonly incorporate additional mechanisms to provide latency guarantees, making their interoperability a grand challenge. The IEEE 802.1 Time Sensitive Networking (TSN) task group was formed to enhance and optimize IEEE 802.1 network standards, particularly for Ethernet-based networks. These solutions can be evolved and adapted for cross-industry scenarios, such as large-scale distributed industrial plants requiring multiple industrial entities to work collaboratively. This paper provides a comprehensive review of current advances in TSN standards for industrial automation. It presents the state-of-the-art IEEE TSN standards and discusses the opportunities and challenges of integrating TSN into the automation industry. Some promising research directions are also highlighted for applying TSN technologies to industrial automation applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.03691v4</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Gang Wang, Chuanyu Xue, Jiachen Wang, Mark Nixon, Song Han</dc:creator>
    </item>
    <item>
      <title>Long-Range Backscatter Connectivity via Spaceborne Synthetic Aperture Radar</title>
      <link>https://arxiv.org/abs/2402.09682</link>
      <description>arXiv:2402.09682v3 Announce Type: replace-cross 
Abstract: SARComm is a novel wireless communication system that enables passive satellite backscatter connectivity using existing spaceborne synthetic aperture radar (SAR) signals. We demonstrate that SAR signals from the European Space Agency's Sentinel-1 satellite, used to image Earth's terrain, can be leveraged to enable low-power ground-to-satellite communication. This paper presents the first cooperative, on-the-ground target that modulates SAR backscatter to send information bits and analyzes how to extract them from publicly available Sentinel-1 datasets. To demonstrate the system, we evaluate the effectiveness of modulating the radar cross section of corner reflectors both mechanically and electronically to encode data bits, develop a deployment algorithm to optimize corner reflector placement, and present a SAR processing pipeline to enable communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09682v3</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geneva Ecola, Bill Yen, Ana Banzer Morgado, Bodhi Priyantha, Ranveer Chandra, Zerina Kapetanovic</dc:creator>
    </item>
    <item>
      <title>Entanglement-Based Artificial Topology: Neighboring Remote Network Nodes</title>
      <link>https://arxiv.org/abs/2404.16204</link>
      <description>arXiv:2404.16204v2 Announce Type: replace-cross 
Abstract: Entanglement is unanimously recognized as the key communication resource of the Quantum Internet. Yet, the possibility of implementing novel network functionalities by exploiting the marvels of entanglement has been poorly investigated so far, by mainly restricting the attention to bipartite entanglement. Conversely, in this paper, we aim at exploiting multipartite entanglement as inter-network resource. Specifically, we consider the interconnection of different Quantum Local Area Networks (QLANs), and we show that multipartite entanglement allows to dynamically generate an inter-QLAN artificial topology, by means of local operations only, that overcomes the limitations of the physical QLAN topologies. To this aim, we first design the multipartite entangled state to be distributed within each QLAN. Then, we show how such a state can be engineered to: i) interconnect nodes belonging to different QLANs, and ii) dynamically adapt to different inter-QLAN traffic patterns. Our contribution aims at providing the network engineering community with a hands-on guideline towards the concept of artificial topology and artificial neighborhood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16204v2</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>SiYi Chen, Jessica Illiano, Angela Sara Cacciapuoti, Marcello Caleffi</dc:creator>
    </item>
  </channel>
</rss>
