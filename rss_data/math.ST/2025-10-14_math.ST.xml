<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Oct 2025 01:46:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>TWIN: Two window inspection for online change point detection</title>
      <link>https://arxiv.org/abs/2510.11348</link>
      <description>arXiv:2510.11348v1 Announce Type: new 
Abstract: We propose a new class of sequential change point tests, both for changes in the mean parameter and in the overall distribution function. The methodology builds on a two-window inspection scheme (TWIN), which aggregates data into symmetric samples and applies strong weighting to enhance statistical performance. The detector yields logarithmic rather than polynomial detection delays, representing a substantial reduction compared to state-of-the-art alternatives. Delays remain short, even for late changes, where existing methods perform worst. Moreover, the new procedure also attains higher power than current methods across broad classes of local alternatives. For mean changes, we further introduce a self-normalized version of the detector that automatically cancels out temporal dependence, eliminating the need to estimate nuisance parameters. The advantages of our approach are supported by asymptotic theory, simulations and an application to monitoring COVID19 data. Here, structural breaks associated with new virus variants are detected almost immediately by our new procedures. This indicates potential value for the real-time monitoring of future epidemics.
  Mathematically, our approach is underpinned by new exponential moment bounds for the global modulus of continuity of the partial sum process, which may be of independent interest beyond change point testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11348v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Bastian, Tim Kutta</dc:creator>
    </item>
    <item>
      <title>Simultaneous Frequentist Calibration of Confidence Regions for Multiple Functionals in Constrained Inverse Problems</title>
      <link>https://arxiv.org/abs/2510.11708</link>
      <description>arXiv:2510.11708v1 Announce Type: new 
Abstract: Many scientific analyses require simultaneous comparison of multiple functionals of an unknown signal at once, calling for multidimensional confidence regions with guaranteed simultaneous frequentist under structural constraints (e.g., non-negativity, shape, or physics-based). This paper unifies and extends many previous optimization-based approaches to constrained confidence region construction in linear inverse problems through the lens of statistical test inversion. We begin by reviewing the historical development of optimization-based confidence intervals for the single-functional setting, from "strict bounds" to the Burrus conjecture and its recent refutation via the aforementioned test inversion framework. We then extend this framework to the multiple-functional setting. This framework can be used to: (i) improve the calibration constants of previous methods, yielding smaller confidence regions that still preserve frequentist coverage, (ii) obtain tractable multidimensional confidence regions that need not be hyper-rectangles to better capture functional dependence structure, and (iii) generalize beyond Gaussian error distributions to generic log-concave error distributions. We provide theory establishing nominal simultaneous coverage of our methods and show quantitative volume improvements relative to prior approaches using numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11708v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pau Batlle, Pratik Patil, Michael Stanley, Javier Ruiz Lupon, Houman Owhadi, Mikael Kuusela</dc:creator>
    </item>
    <item>
      <title>A Unified Information-Theoretic Framework for Meta-Learning Generalization</title>
      <link>https://arxiv.org/abs/2501.15559</link>
      <description>arXiv:2501.15559v2 Announce Type: cross 
Abstract: In recent years, information-theoretic generalization bounds have gained increasing attention for analyzing the generalization capabilities of meta-learning algorithms. However, existing results are confined to two-step bounds, failing to provide a sharper characterization of the meta-generalization gap that simultaneously accounts for environment-level and task-level dependencies. This paper addresses this fundamental limitation by developing a unified information-theoretic framework using a single-step derivation. The resulting meta-generalization bounds, expressed in terms of diverse information measures, exhibit substantial advantages over previous work, particularly in terms of tightness, scaling behavior associated with sampled tasks and samples per task, and computational tractability. Furthermore, through gradient covariance analysis, we provide new theoretical insights into the generalization properties of two classes of noisy and iterative meta-learning algorithms, where the meta-learner uses either the entire meta-training data (e.g., Reptile), or separate training and test data within the task (e.g., model agnostic meta-learning (MAML)). Numerical results validate the effectiveness of the derived bounds in capturing the generalization dynamics of meta-learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15559v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wen Wen, Tieliang Gong, Yuxin Dong, Zeyu Gao, Yong-Jin Liu</dc:creator>
    </item>
    <item>
      <title>Causal Digital Twins for Cyber-Physical Security: A Framework for Robust Anomaly Detection in Industrial Control Systems</title>
      <link>https://arxiv.org/abs/2510.09616</link>
      <description>arXiv:2510.09616v1 Announce Type: cross 
Abstract: Industrial Control Systems (ICS) face growing cyber-physical attacks that exploit both network vulnerabilities and physical processes. Current anomaly detection methods rely on correlation-based analysis, which cannot separate true causal relationships from spurious associations. This limitation results in high false alarm rates and poor root cause analysis. We propose a novel Causal Digital Twin (CDT) framework for cyber-physical security in medium-scale ICS. Our method combines causal inference theory with digital twin modeling. The framework enables three types of causal reasoning: association for pattern detection, intervention for understanding system responses, and counterfactual analysis for attack prevention planning. We evaluate our framework on three industrial datasets: SWaT, WADI, and HAI, with validation through physical constraint compliance (90.8\%) and synthetic ground truth testing (structural Hamming distance 0.13). Results show significant improvements over seven baseline methods. Our CDT achieves F1-scores are $0.944 \pm 0.014$ for SWaT, $0.902 \pm 0.021$ for WADI, and $0.923 \pm 0.018$ for HAI with statistical significance ($p &lt; 0.0024$, Bonferroni corrected). The framework reduces false positives by \SI{74}{\percent} and achieves \SI{78.4}{\percent} root cause analysis accuracy compared to \SI{48.7}{\percent} for existing methods. Counterfactual analysis enables defense strategies that reduce attack success by \SI{73.2}{\percent}. The system keeps real-time performance with \SI{3.2}{ms} latency, which is suitable for industrial deployment, while providing interpretable explanations for operators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09616v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mohammadhossein Homaei, Mehran Tarif, Mar Avilla, Andres Caro</dc:creator>
    </item>
    <item>
      <title>How much can we learn from quantum random circuit sampling?</title>
      <link>https://arxiv.org/abs/2510.09919</link>
      <description>arXiv:2510.09919v1 Announce Type: cross 
Abstract: Benchmarking quantum devices is a foundational task for the sustained development of quantum technologies. However, accurate in situ characterization of large-scale quantum devices remains a formidable challenge: such systems experience many different sources of errors, and cannot be simulated on classical computers. Here, we introduce new benchmarking methods based on random circuit sampling (RCS), that substantially extend the scope of conventional approaches. Unlike existing benchmarks that report only a single quantity--the circuit fidelity--our framework extracts rich diagnostic information, including spatiotemporal error profiles, correlated and contextual errors, and biased readout errors, without requiring any modifications of the experiment. Furthermore, we develop techniques that achieve this task without classically intractable simulations of the quantum circuit, by leveraging side information, in the form of bitstring samples obtained from reference quantum devices. Our approach is based on advanced high-dimensional statistical modeling of RCS data. We sharply characterize the information-theoretic limits of error estimation, deriving matching upper and lower bounds on the sample complexity across all regimes of side information. We identify surprising phase transitions in learnability as the amount of side information varies. We demonstrate our methods using publicly available RCS data from a state-of-the-art superconducting processor, obtaining in situ characterizations that are qualitatively consistent yet quantitatively distinct from component-level calibrations. Our results establish both practical benchmarking protocols for current and future quantum computers and fundamental information-theoretic limits on how much can be learned from RCS data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09919v1</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tudor Manole, Daniel K. Mark, Wenjie Gong, Bingtian Ye, Yury Polyanskiy, Soonwon Choi</dc:creator>
    </item>
    <item>
      <title>An information theorist's tour of differential privacy</title>
      <link>https://arxiv.org/abs/2510.10316</link>
      <description>arXiv:2510.10316v1 Announce Type: cross 
Abstract: Since being proposed in 2006, differential privacy has become a standard method for quantifying certain risks in publishing or sharing analyses of sensitive data. At its heart, differential privacy measures risk in terms of the differences between probability distributions, which is a central topic in information theory. A differentially private algorithm is a channel between the underlying data and the output of the analysis. Seen in this way, the guarantees made by differential privacy can be understood in terms of properties of this channel. In this article we examine a few of the key connections between information theory and the formulation/application of differential privacy, giving an ``operational significance'' for relevant information measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10316v1</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anand D. Sarwate, Flavio P. Calmon, Oliver Kosut, Lalitha Sankar</dc:creator>
    </item>
    <item>
      <title>On extremes for Gaussian subordination</title>
      <link>https://arxiv.org/abs/2510.10578</link>
      <description>arXiv:2510.10578v1 Announce Type: cross 
Abstract: This paper investigates extreme value theory for processes obtained by applying transformations to stationary Gaussian processes, also called subordinated Gaussian processes. The main contributions are as follows. First, we refine the method of \cite{sly2008nonstandard} to allow the covariance of the underlying Gaussian process to decay more slowly than any polynomial rate, nearly matching Berman's condition. Second, we extend the theory to a multivariate setting, where both the subordinated process and the underlying Gaussian process may be vector-valued, and the transformation is finite-dimensional. In particular, we establish the weak convergence of a point process constructed from the subordinated Gaussian process, from which a multivariate extreme value limit theorem follows. A key observation that facilitates our analysis, and may be of independent interest, is the following: any bivariate random vector derived from the transformations of two jointly Gaussian vectors with a non-unity canonical correlation always remains extremally independent. This observation also motivates us to introduce and discuss a notion we call m-extremal-dependence, which extends the classical concept of m-dependence. Moreover, we relax the restriction to finite-dimensional transforms, extending the results to infinite-dimensional settings via an approximation argument. As an illustration, we establish a limit theorem for a multivariate moving maxima process driven by regularly varying innovations that arise from subordinated Gaussian processes with potentially long memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10578v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyang Bai, Marie-Christine D\"uker</dc:creator>
    </item>
    <item>
      <title>Information-Computation Tradeoffs for Noiseless Linear Regression with Oblivious Contamination</title>
      <link>https://arxiv.org/abs/2510.10665</link>
      <description>arXiv:2510.10665v1 Announce Type: cross 
Abstract: We study the task of noiseless linear regression under Gaussian covariates in the presence of additive oblivious contamination. Specifically, we are given i.i.d.\ samples from a distribution $(x, y)$ on $\mathbb{R}^d \times \mathbb{R}$ with $x \sim \mathcal{N}(0,\mathbf{I}_d)$ and $y = x^\top \beta + z$, where $z$ is drawn independently of $x$ from an unknown distribution $E$. Moreover, $z$ satisfies $\mathbb{P}_E[z = 0] = \alpha&gt;0$. The goal is to accurately recover the regressor $\beta$ to small $\ell_2$-error. Ignoring computational considerations, this problem is known to be solvable using $O(d/\alpha)$ samples. On the other hand, the best known polynomial-time algorithms require $\Omega(d/\alpha^2)$ samples. Here we provide formal evidence that the quadratic dependence in $1/\alpha$ is inherent for efficient algorithms. Specifically, we show that any efficient Statistical Query algorithm for this task requires VSTAT complexity at least $\tilde{\Omega}(d^{1/2}/\alpha^2)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10665v1</guid>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Chao Gao, Daniel M. Kane, John Lafferty, Ankit Pensia</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Learning Dynamics of Quantized Models with Straight-Through Estimator</title>
      <link>https://arxiv.org/abs/2510.10693</link>
      <description>arXiv:2510.10693v1 Announce Type: cross 
Abstract: Quantized neural network training optimizes a discrete, non-differentiable objective. The straight-through estimator (STE) enables backpropagation through surrogate gradients and is widely used. While previous studies have primarily focused on the properties of surrogate gradients and their convergence, the influence of quantization hyperparameters, such as bit width and quantization range, on learning dynamics remains largely unexplored. We theoretically show that in the high-dimensional limit, STE dynamics converge to a deterministic ordinary differential equation. This reveals that STE training exhibits a plateau followed by a sharp drop in generalization error, with plateau length depending on the quantization range. A fixed-point analysis quantifies the asymptotic deviation from the unquantized linear model. We also extend analytical techniques for stochastic gradient descent to nonlinear transformations of weights and inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10693v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuma Ichikawa, Shuhei Kashiwamura, Ayaka Sakata</dc:creator>
    </item>
    <item>
      <title>Small Area Estimation of General Indicators in Off-Census Years</title>
      <link>https://arxiv.org/abs/2510.10812</link>
      <description>arXiv:2510.10812v1 Announce Type: cross 
Abstract: We propose small area estimators of general indicators in off-census years, which avoid the use of deprecated census microdata, but are nearly optimal in census years. The procedure is based on replacing the obsolete census file with a larger unit-level survey that adequately covers the areas of interest and contains the values of useful auxiliary variables. However, the minimal data requirement of the proposed method is a single survey with microdata on the target variable and suitable auxiliary variables for the period of interest. We also develop an estimator of the mean squared error (MSE) that accounts for the uncertainty introduced by the large survey used to replace the census of auxiliary information. Our empirical results indicate that the proposed predictors perform clearly better than the alternative predictors when census data are outdated, and are very close to optimal ones when census data are correct. They also illustrate that the proposed total MSE estimator corrects for the bias of purely model-based MSE estimators that do not account for the large survey uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10812v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Acero, Isabel Molina, J. Miguel Mar\'in</dc:creator>
    </item>
    <item>
      <title>Transfer Learning with Distance Covariance for Random Forest: Error Bounds and an EHR Application</title>
      <link>https://arxiv.org/abs/2510.10870</link>
      <description>arXiv:2510.10870v1 Announce Type: cross 
Abstract: Random forest is an important method for ML applications due to its broad outperformance over competing methods for structured tabular data. We propose a method for transfer learning in nonparametric regression using a centered random forest (CRF) with distance covariance-based feature weights, assuming the unknown source and target regression functions are different for a few features (sparsely different). Our method first obtains residuals from predicting the response in the target domain using a source domain-trained CRF. Then, we fit another CRF to the residuals, but with feature splitting probabilities proportional to the sample distance covariance between the features and the residuals in an independent sample. We derive an upper bound on the mean square error rate of the procedure as a function of sample sizes and difference dimension, theoretically demonstrating transfer learning benefits in random forests. In simulations, we show that the results obtained for the CRFs also hold numerically for the standard random forest (SRF) method with data-driven feature split selection. Beyond transfer learning, our results also show the benefit of distance-covariance-based weights on the performance of RF in some situations. Our method shows significant gains in predicting the mortality of ICU patients in smaller-bed target hospitals using a large multi-hospital dataset of electronic health records for 200,000 ICU patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10870v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenze Li, Subhadeep Paul</dc:creator>
    </item>
    <item>
      <title>On the Optimal Representation Efficiency of Barlow Twins: An Information-Geometric Interpretation</title>
      <link>https://arxiv.org/abs/2510.10980</link>
      <description>arXiv:2510.10980v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) has achieved remarkable success by learning meaningful representations without labeled data. However, a unified theoretical framework for understanding and comparing the efficiency of different SSL paradigms remains elusive. In this paper, we introduce a novel information-geometric framework to quantify representation efficiency. We define representation efficiency $\eta$ as the ratio between the effective intrinsic dimension of the learned representation space and its ambient dimension, where the effective dimension is derived from the spectral properties of the Fisher Information Matrix (FIM) on the statistical manifold induced by the encoder. Within this framework, we present a theoretical analysis of the Barlow Twins method. Under specific but natural assumptions, we prove that Barlow Twins achieves optimal representation efficiency ($\eta = 1$) by driving the cross-correlation matrix of representations towards the identity matrix, which in turn induces an isotropic FIM. This work provides a rigorous theoretical foundation for understanding the effectiveness of Barlow Twins and offers a new geometric perspective for analyzing SSL algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10980v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Di Zhang</dc:creator>
    </item>
    <item>
      <title>Spatial and Temporal Boundaries in Difference-in-Differences: A Framework from Navier-Stokes Equation</title>
      <link>https://arxiv.org/abs/2510.11013</link>
      <description>arXiv:2510.11013v1 Announce Type: cross 
Abstract: This paper develops a unified framework for identifying spatial and temporal boundaries of treatment effects in difference-in-differences designs. Starting from fundamental fluid dynamics equations (Navier-Stokes), we derive conditions under which treatment effects decay exponentially in space and time, enabling researchers to calculate explicit boundaries beyond which effects become undetectable. The framework encompasses both linear (pure diffusion) and nonlinear (advection-diffusion with chemical reactions) regimes, with testable scope conditions based on dimensionless numbers from physics (P\'eclet and Reynolds numbers). We demonstrate the framework's diagnostic capability using air pollution from coal-fired power plants. Analyzing 791 ground-based PM$_{2.5}$ monitors and 189,564 satellite-based NO$_2$ grid cells in the Western United States over 2019-2021, we find striking regional heterogeneity: within 100 km of coal plants, both pollutants show positive spatial decay (PM$_{2.5}$: $\kappa_s = 0.00200$, $d^* = 1,153$ km; NO$_2$: $\kappa_s = 0.00112$, $d^* = 2,062$ km), validating the framework. Beyond 100 km, negative decay parameters correctly signal that urban sources dominate and diffusion assumptions fail. Ground-level PM$_{2.5}$ decays approximately twice as fast as satellite column NO$_2$, consistent with atmospheric transport physics. The framework successfully diagnoses its own validity in four of eight analyzed regions, providing researchers with physics-based tools to assess whether their spatial difference-in-differences setting satisfies diffusion assumptions before applying the estimator. Our results demonstrate that rigorous boundary detection requires both theoretical derivation from first principles and empirical validation of underlying physical assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11013v1</guid>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>math.ST</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Efficient Group Lasso Regularized Rank Regression with Data-Driven Parameter Determination</title>
      <link>https://arxiv.org/abs/2510.11546</link>
      <description>arXiv:2510.11546v1 Announce Type: cross 
Abstract: High-dimensional regression often suffers from heavy-tailed noise and outliers, which can severely undermine the reliability of least-squares based methods. To improve robustness, we adopt a non-smooth Wilcoxon score based rank objective and incorporate structured group sparsity regularization, a natural generalization of the lasso, yielding a group lasso regularized rank regression method. By extending the tuning-free parameter selection scheme originally developed for the lasso, we introduce a data-driven, simulation-based tuning rule and further establish a finite-sample error bound for the resulting estimator. On the computational side, we develop a proximal augmented Lagrangian method for solving the associated optimization problem, which eliminates the singularity issues encountered in existing methods, thereby enabling efficient semismooth Newton updates for the subproblems. Extensive numerical experiments demonstrate the robustness and effectiveness of our proposed estimator against alternatives, and showcase the scalability of the algorithm across both simulated and real-data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11546v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meixia Lin, Meijiao Shi, Yunhai Xiao, Qian Zhang</dc:creator>
    </item>
    <item>
      <title>Robust Online Sampling from Possibly Moving Target Distributions</title>
      <link>https://arxiv.org/abs/2510.11571</link>
      <description>arXiv:2510.11571v1 Announce Type: cross 
Abstract: We suppose we are given a list of points $x_1, \dots, x_n \in \mathbb{R}$, a target probability measure $\mu$ and are asked to add additional points $x_{n+1}, \dots, x_{n+m}$ so that $x_1, \dots, x_{n+m}$ is as close as possible to the distribution of $\mu$; additionally, we want this to be true uniformly for all $m$. We propose a simple method that achieves this goal. It selects new points in regions where the existing set is lacking points and avoids regions that are already overly crowded. If we replace $\mu$ by another measure $\mu_2$ in the middle of the computation, the method dynamically adjusts and allows us to keep the original sampling points. $x_{n+1}$ can be computed in $\mathcal{O}(n)$ steps and we obtain state-of-the-art results. It appears to be an interesting dynamical system in its own right; we analyze a continuous mean-field version that reflects much of the same behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11571v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Cl\'ement, Stefan Steinerberger</dc:creator>
    </item>
    <item>
      <title>MLE convergence speed to information projection of exponential family: Criterion for model dimension and sample size -- complete proof version--</title>
      <link>https://arxiv.org/abs/2105.08947</link>
      <description>arXiv:2105.08947v5 Announce Type: replace 
Abstract: For a parametric model of distributions, the closest distribution in the model to the true distribution located outside the model is considered. Measuring the closeness between two distributions with the Kullback-Leibler (K-L) divergence, the closest distribution is called the "information projection." The estimation risk of the maximum likelihood estimator (MLE) is defined as the expectation of K-L divergence between the information projection and the predictive distribution with plugged-in MLE. Here, the asymptotic expansion of the risk is derived up to $n^{-2}$-order, and the sufficient condition on the risk for the Bayes error rate between the true distribution and the information projection to be lower than a specified value is investigated. Combining these results, the "$p-n$ criterion" is proposed, which determines whether the MLE is sufficiently close to the information projection for the given model and sample. In particular, the criterion for an exponential family model is relatively simple and can be used for a complex model with no explicit form of normalizing constant. This criterion can constitute a solution to the sample size or model acceptance problem. Use of the $p-n$ criteria is demonstrated for two practical datasets. The relationship between the results and information criteria is also studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.08947v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yo Sheena</dc:creator>
    </item>
    <item>
      <title>Robust mean change point testing in high-dimensional data with heavy tails</title>
      <link>https://arxiv.org/abs/2305.18987</link>
      <description>arXiv:2305.18987v4 Announce Type: replace 
Abstract: We study mean change point testing problems for high-dimensional data, with exponentially- or polynomially-decaying tails. In each case, depending on the $\ell_0$-norm of the mean change vector, we separately consider dense and sparse regimes. We characterise the boundary between the dense and sparse regimes under the above two tail conditions for the first time in the change point literature and propose novel testing procedures that attain optimal rates in each of the four regimes up to a poly-iterated logarithmic factor. By comparing with previous results under Gaussian assumptions, our results quantify the costs of heavy-tailedness on the fundamental difficulty of change point testing problems for high-dimensional data.
  To be specific, when the error distributions possess exponentially-decaying tails, a CUSUM-type statistic is shown to achieve a minimax testing rate up to $\sqrt{\log\log(8n)}$. As for polynomially-decaying tails, admitting bounded $\alpha$-th moments for some $\alpha \geq 4$, we introduce a median-of-means-type test statistic that achieves a near-optimal testing rate in both dense and sparse regimes. In the sparse regime, we further propose a computationally-efficient test to achieve optimality. Our investigation in the even more challenging case of $2 \leq \alpha &lt; 4$, unveils a new phenomenon that the minimax testing rate has no sparse regime, i.e.\ testing sparse changes is information-theoretically as hard as testing dense changes. Finally, we consider various extensions where we also obtain near-optimal performances, including testing against multiple change points, allowing temporal dependence as well as fewer than two finite moments in the data generating mechanisms. We also show how sub-Gaussian rates can be achieved when an additional minimal spacing condition is imposed under the alternative hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.18987v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengchu Li, Yudong Chen, Tengyao Wang, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Measure estimation on a manifold explored by a diffusion process</title>
      <link>https://arxiv.org/abs/2410.11777</link>
      <description>arXiv:2410.11777v2 Announce Type: replace 
Abstract: From the observation of a diffusion path $(X_t)_{t\in [0,T]}$ on a compact connected $d$-dimensional manifold $\mathcal{M}$ without boundary, we consider the problem of estimating the stationary measure $\mu$ of the process. Wang and Zhu (2023) showed that for the Wasserstein metric $\mathcal{W}_2$ and for $d\geq 5$, the convergence rate of $T^{-1/(d-2)}$ is attained by the occupation measure of the path $(X_t)_{t\in [0,T]}$ when $(X_t)_{t\in [0,T]}$ is a Langevin diffusion. We extend their result in several directions. First, we show that the rate of convergence holds for a large class of diffusion paths, whose generators are uniformly elliptic. Second, the regularity of the density $p$ of the stationary measure $\mu$ with respect to the volume measure of $\mathcal{M}$ can be leveraged to obtain faster estimators: when $p$ belongs to a Sobolev space of order $\ell\geq 2$, smoothing the occupation measure by convolution with a kernel yields an estimator whose rate of convergence is of order $T^{-(\ell+1)/(2\ell+d-2)}$. We further show that this rate is the minimax rate of estimation for this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11777v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Divol, H\'el\`ene Gu\'erin, Dinh-Toan Nguyen, Viet Chi Tran</dc:creator>
    </item>
    <item>
      <title>Universality of estimators for high-dimensional linear models with block dependency</title>
      <link>https://arxiv.org/abs/2410.19244</link>
      <description>arXiv:2410.19244v2 Announce Type: replace 
Abstract: We study the universality property of estimators for high-dimensional linear models, which implies that the distribution of estimators is independent of whether the covariates follow a Gaussian distribution. Recent developments in high-dimensional statistics typically require covariates to strictly follow a Gaussian distribution to precisely characterize the properties of estimators. To relax this Gaussianity requirement, the existing literature has examined conditions under which estimators achieve universality. In particular, independence among the elements of the high-dimensional covariates has played a critical role. In this study, we focus on high-dimensional linear models with covariates exhibiting block dependence, where covariate elements can only be dependent within each block, and show that estimators for such models retain universality. Specifically, we prove that the distribution of estimators with Gaussian covariates can be approximated by the distribution of estimators with non-Gaussian covariates having the same moments under block dependence. To establish this result, we develop a generalized Lindeberg principle suitable for handling block dependencies and derive new error bounds for correlated covariate elements. We further demonstrate the universality result across several different estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19244v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Toshiki Tsuda, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Uniformly most powerful tests in linear models</title>
      <link>https://arxiv.org/abs/2411.18033</link>
      <description>arXiv:2411.18033v2 Announce Type: replace 
Abstract: In the multiple regression model we prove that the coefficient t-test for a variable of interest is uniformly most powerful unbiased, with the other parameters considered nuisance. The proof is based on the theory of tests with Neyman-structure and does not assume unbiasedness or linearity of the test statistic. We further show that the Gram-Schmidt decomposition of the design matrix leads to a family of regression model with potentially more powerful tests for the corresponding transformed regressors. Finally, we discuss interpretation and performance criteria for the Gram-Schmidt regression compared to standard multiple regression, and show how the power differential has major implications for study design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18033v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Razvan G. Romanescu</dc:creator>
    </item>
    <item>
      <title>Geometric Ergodicity of Gibbs Algorithms for a Normal Model With a Global-Local Shrinkage Prior</title>
      <link>https://arxiv.org/abs/2503.00538</link>
      <description>arXiv:2503.00538v4 Announce Type: replace 
Abstract: We consider Gibbs samplers for a normal linear regression model with a global-local shrinkage prior and show that they produce geometrically ergodic Markov chains. First, under the horseshoe local prior and a three-parameter beta global prior under some assumptions, we prove geometric ergodicity for a Gibbs algorithm in which it is relatively easy to update the global shrinkage parameter. Second, we consider a more general class of global-local shrinkage priors. Under milder conditions, geometric ergodicity is proved for two- and three-stage Gibbs samplers based on rejection sampling. We also construct a practical rejection sampling method in the horseshoe case. Finally, a simulation study is performed to compare proposed and existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00538v4</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Hamura</dc:creator>
    </item>
    <item>
      <title>Valid post-selection inference in Robust Q-learning</title>
      <link>https://arxiv.org/abs/2208.03233</link>
      <description>arXiv:2208.03233v2 Announce Type: replace-cross 
Abstract: Q-learning facilitates the development of an optimal adaptive treatment strategy through stagewise regression on a pre-specified set of tailoring variables and confounders. Semiparametric robust Q-learning eliminates the residual confounding that can occur when parametric working models for confounding influences are misspecified. However, in the presence of many potential tailoring variables, constructing an optimal adaptive treatment strategy using either approach may lead to including extraneous variables that contribute little or no benefit while increasing implementation costs, thereby placing an undue burden on patients. Using data-driven selection processes to identify a smaller set of informative prognostic factors is straightforward; however, proper statistical inference must account for this selection process. In this paper, we adapt the Universal Post-Selection Inference (UPoSI) procedure to the semiparametric Robust Q-learning method. UPoSI, introduced for use with linear models, allows for very general variable selection mechanisms. Our approach addresses the unique challenges stemming from the use of UPoSI with semiparametric multistage decision methods. Theoretical and simulation results demonstrate the validity of the proposed confidence regions. We illustrate our proposed methods through an application to adaptive treatment strategy estimation for substance abuse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.03233v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jeremiah Jones, Ashkan Ertefaie, James R. McKay, David W. Oslin, Robert L. Strawderman</dc:creator>
    </item>
    <item>
      <title>Almost sure convergence rates of adaptive increasingly rare Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2402.12122</link>
      <description>arXiv:2402.12122v2 Announce Type: replace-cross 
Abstract: We consider adaptive increasingly rare Markov chain Monte Carlo (MCMC) algorithms, which are adaptive MCMC methods, where the adaptation concerning the "past'' happens less and less frequently over time. Under a contraction assumption with respect to a Wasserstein-like function we deduce upper bounds of the convergence rate of Monte Carlo sums taking a renormalisation factor into account that is "almost'' the one that appears in a law of the iterated logarithm. We demonstrate the applicability of our results by considering different settings, among which are those of simultaneous geometric and uniform ergodicity. All proofs are carried out on an augmented state space, including the classical non-augmented setting as a special case. In contrast to other adaptive MCMC limit theory, some technical assumptions, like diminishing adaptation, are not needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12122v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian Hofstadler, Krzysztof Latuszynski, Gareth O. Roberts, Daniel Rudolf</dc:creator>
    </item>
    <item>
      <title>On a new family of weighted Gaussian processes: an application to bat telemetry data</title>
      <link>https://arxiv.org/abs/2405.19903</link>
      <description>arXiv:2405.19903v2 Announce Type: replace-cross 
Abstract: In this article we use a covariance function that arises from limit of fluctuations of the rescaled occupation time process of a branching particle system, to introduce a family of weighted long-range dependence Gaussian processes. In particular, we consider two subfamilies for which we show that the process is not a semimartingale, that the processes exhibit long-range dependence and have long-range memory of logarithmic order. Finally, we illustrate that this family of processes is useful for modeling real world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19903v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose Hermenegildo Ramirez Gonzalez, Antonio Murillo Salas, Ying Sun</dc:creator>
    </item>
    <item>
      <title>Explaining and Connecting Kriging with Gaussian Process Regression</title>
      <link>https://arxiv.org/abs/2408.02331</link>
      <description>arXiv:2408.02331v2 Announce Type: replace-cross 
Abstract: Kriging and Gaussian Process Regression are statistical methods that allow predicting the outcome of a random process or a random field by using a sample of correlated observations. In other words, the random process or random field is partially observed, and by using a sample a prediction is made, pointwise or as a whole, where the latter can be thought as a reconstruction. In addition, the techniques permit to give a measure of uncertainty of the prediction. The methods have different origins. Kriging comes from geostatistics, a field which started to develop around 1950 oriented to mining valuation problems, whereas Gaussian Process Regression has gained popularity in the area of machine learning in the last decade of the previous century. In the literature, the methods are usually presented as being the same technique. However, beyond this affirmation, the techniques have yet not been compared on a thorough mathematical basis and neither explained why and under which conditions this affirmation holds. Furthermore, Kriging has many variants and this affirmation should be precised. In this paper, this gap is filled. It is shown, step by step how both methods are deduced from the first principles -- with a major focus on Kriging, the mathematical connection between them, and which Kriging variant corresponds to which Gaussian Process Regression set up. The three most widely used versions of Kriging are considered: Simple Kriging, Ordinary Kriging and Universal Kriging. It is found, that despite their closeness, the techniques are different in their approach and assumptions, in a similar way the Least Square method, the Best Linear Unbiased Estimator method and the Likelihood method in regression do. I hope this work deepen the understanding of the relation between Kriging and Gaussian Process Regression, and serves as a cohesive introductory resource for researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02331v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Marinescu</dc:creator>
    </item>
    <item>
      <title>Mixing Times and Privacy Analysis for the Projected Langevin Algorithm under a Modulus of Continuity</title>
      <link>https://arxiv.org/abs/2501.04134</link>
      <description>arXiv:2501.04134v2 Announce Type: replace-cross 
Abstract: We study the mixing time of the projected Langevin algorithm (LA) and the privacy curve of noisy Stochastic Gradient Descent (SGD), beyond nonexpansive iterations. Specifically, we derive new mixing time bounds for the projected LA which are, in some important cases, dimension-free and poly-logarithmic on the accuracy, closely matching the existing results in the smooth convex case. Additionally, we establish new upper bounds for the privacy curve of the subsampled noisy SGD algorithm. These bounds show a crucial dependency on the regularity of gradients, and are useful for a wide range of convex losses beyond the smooth case. Our analysis relies on a suitable extension of the Privacy Amplification by Iteration (PABI) framework (Feldman et al., 2018; Altschuler and Talwar, 2022, 2023) to noisy iterations whose gradient map is not necessarily nonexpansive. This extension is achieved by designing an optimization problem which accounts for the best possible R\'enyi divergence bound obtained by an application of PABI, where the tractability of the problem is crucially related to the modulus of continuity of the associated gradient mapping. We show that, in several interesting cases -- namely the nonsmooth convex, weakly smooth and (strongly) dissipative -- such optimization problem can be solved exactly and explicitly, yielding the tightest possible PABI-based bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04134v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Bravo, Juan P. Flores-Mella, Crist\'obal Guzm\'an</dc:creator>
    </item>
    <item>
      <title>Inference on effect size after multiple hypothesis testing</title>
      <link>https://arxiv.org/abs/2503.22369</link>
      <description>arXiv:2503.22369v2 Announce Type: replace-cross 
Abstract: Significant treatment effects are often emphasized when interpreting and summarizing empirical findings in studies that estimate multiple, possibly many, treatment effects. Under this kind of selective reporting, conventional treatment effect estimates may be biased and their corresponding confidence intervals may undercover the true effect sizes. We propose new estimators and confidence intervals that provide valid inferences on the effect sizes of the significant effects after multiple hypothesis testing. Our methods are based on the principle of selective conditional inference and complement a wide range of tests, including step-up tests and bootstrap-based step-down tests. Our approach is scalable, allowing us to study an application with over 370 estimated effects. We justify our procedure for asymptotically normal treatment effect estimators. We provide two empirical examples that demonstrate bias correction and confidence interval adjustments for significant effects. The magnitude and direction of the bias correction depend on the correlation structure of the estimated effects and whether the interpretation of the significant effects depends on the (in)significance of other effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22369v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andreas Dzemski, Ryo Okui, Wenjie Wang</dc:creator>
    </item>
    <item>
      <title>Query Complexity of Classical and Quantum Channel Discrimination</title>
      <link>https://arxiv.org/abs/2504.12989</link>
      <description>arXiv:2504.12989v3 Announce Type: replace-cross 
Abstract: Quantum channel discrimination has been studied from an information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of unknown channel accesses. In this paper, we study the query complexity of quantum channel discrimination, wherein the goal is to determine the minimum number of channel uses needed to reach a desired error probability. To this end, we show that the query complexity of binary channel discrimination depends logarithmically on the inverse error probability and inversely on the negative logarithm of the (geometric and Holevo) channel fidelity. As a special case of these findings, we precisely characterize the query complexity of discriminating two classical channels and two classical-quantum channels. Furthermore, by obtaining an optimal characterization of the sample complexity of quantum hypothesis testing, including prior probabilities, we provide a more precise characterization of query complexity when the error probability does not exceed a fixed threshold. We also provide lower and upper bounds on the query complexity of binary asymmetric channel discrimination and multiple quantum channel discrimination. For the former, the query complexity depends on the geometric R\'enyi and Petz R\'enyi channel divergences, while for the latter, it depends on the negative logarithm of the (geometric and Uhlmann) channel fidelity. For multiple channel discrimination, the upper bound scales as the logarithm of the number of channels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12989v3</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/2058-9565/ae0a79</arxiv:DOI>
      <arxiv:journal_reference>Quantum Science and Technology, 2025</arxiv:journal_reference>
      <dc:creator>Theshani Nuradha, Mark M. Wilde</dc:creator>
    </item>
    <item>
      <title>On Experiments</title>
      <link>https://arxiv.org/abs/2508.08288</link>
      <description>arXiv:2508.08288v2 Announce Type: replace-cross 
Abstract: The scientific process is a means to turn the results of experiments into knowledge about the world in which we live. Much research effort has been directed toward automating this process. To do this, one needs to formulate the scientific process in a precise mathematical language. This paper outlines one such language. What is presented here is hardly new. The material is based on great thinkers from times past well as more modern contributions. The novel contributions of this paper are: A new general data processing inequality, a bias variance decomposition for canonical losses, streamlined proofs of the Blackwell-Sherman-Stein and Randomization theorems. means of calculating deficiency through linear programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08288v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brendan van Rooyen</dc:creator>
    </item>
    <item>
      <title>Statistical properties of Markov shifts (part I)</title>
      <link>https://arxiv.org/abs/2510.07757</link>
      <description>arXiv:2510.07757v2 Announce Type: replace-cross 
Abstract: We prove central limit theorems, Berry-Esseen type theorems, almost sure invariance principles, large deviations and Livsic type regularity for partial sums of the form $S_n=\sum_{j=0}^{n-1}f_j(...,X_{j-1},X_j,X_{j+1},...)$, where $(X_j)$ is an inhomogeneous Markov chain satisfying some mixing assumptions and $f_j$ is a sequence of sufficiently regular functions. Even though the case of non-stationary chains and time dependent functions $f_j$ is more challenging, our results seem to be new already for stationary Markov chains. They also seem to be new for non-stationary Bernoulli shifts (that is when $(X_j)$ are independent but not identically distributed). This paper is the first one in a series of two papers. In \cite{Work} we will prove local limit theorems including developing the related reduction theory in the sense of \cite{DolgHaf LLT, DS}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07757v2</guid>
      <category>math.PR</category>
      <category>math.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeor Hafouta</dc:creator>
    </item>
  </channel>
</rss>
