<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DM</link>
    <description>cs.DM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Feb 2025 02:53:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimal placement of mobile distance-limited devices for line routing\</title>
      <link>https://arxiv.org/abs/2502.16844</link>
      <description>arXiv:2502.16844v1 Announce Type: new 
Abstract: A segment (barrier) is specified on the plane, as well as depots, where the mobile devices (drones) can be placed. Each drone departs from its depot to the barrier, moves along the barrier and returns to its depot, traveling a path of a limited length. The part of the barrier along which the drone moved is \emph{covered} by this sensor. It is required to place a limited quantity of drones in the depots and determine the trajectory of each drone in such a way that the barrier is covered, and the total length of the paths traveled by the drones is minimal.
  Previously, this problem was considered for an unlimited number of drones. If each drone covers a segment of length at least 1, then the time complexity of the proposed algorithm was $O(mL^3)$, where $m$ is the number of depots and $L$ is the length of the barrier. In this paper, we generalize the problem by introducing an upper bound $n$ on the number of drones, and propose a new algorithm with time complexity equals $O(mnL^2)$. Since each drone covers a segment of length at least 1, then $n\leq L$ and $O(mnL^2)\leq O(mL^3)$. Assuming an unlimited number of drones, as investigated in our prior work, we present an $O(mL^2)$-time algorithm, achieving an $L$-fold reduction compared to previous methods. Here, the algorithm has a time complexity that equals $O(L^2)$, and the most time-consuming is preprocessing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16844v1</guid>
      <category>cs.DM</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adil Erzin, Anzhela Shadrina</dc:creator>
    </item>
    <item>
      <title>Optimality and Renormalization imply Statistical Laws</title>
      <link>https://arxiv.org/abs/2502.16314</link>
      <description>arXiv:2502.16314v1 Announce Type: cross 
Abstract: Benford's Law is an important instance of experimental mathematics that appears to constrain the information-theoretic behavior of numbers. Elias' encoding for integers is a remarkable approach to universality and optimality of codes. In the present analysis we seek to deduce a general law and its particular implications for these two cases from optimality and renormalization as applied to information-theoretical functionals. Both theoretical and experimental results corroborate our conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16314v1</guid>
      <category>cs.IT</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Kolpakov, Aidan Rocke</dc:creator>
    </item>
    <item>
      <title>Monotonicity Testing of High-Dimensional Distributions with Subcube Conditioning</title>
      <link>https://arxiv.org/abs/2502.16355</link>
      <description>arXiv:2502.16355v1 Announce Type: cross 
Abstract: We study monotonicity testing of high-dimensional distributions on $\{-1,1\}^n$ in the model of subcube conditioning, suggested and studied by Canonne, Ron, and Servedio~\cite{CRS15} and Bhattacharyya and Chakraborty~\cite{BC18}. Previous work shows that the \emph{sample complexity} of monotonicity testing must be exponential in $n$ (Rubinfeld, Vasilian~\cite{RV20}, and Aliakbarpour, Gouleakis, Peebles, Rubinfeld, Yodpinyanee~\cite{AGPRY19}). We show that the subcube \emph{query complexity} is $\tilde{\Theta}(n/\varepsilon^2)$, by proving nearly matching upper and lower bounds. Our work is the first to use directed isoperimetric inequalities (developed for function monotonicity testing) for analyzing a distribution testing algorithm. Along the way, we generalize an inequality of Khot, Minzer, and Safra~\cite{KMS18} to real-valued functions on $\{-1,1\}^n$.
  We also study uniformity testing of distributions that are promised to be monotone, a problem introduced by Rubinfeld, Servedio~\cite{RS09} , using subcube conditioning. We show that the query complexity is $\tilde{\Theta}(\sqrt{n}/\varepsilon^2)$. Our work proves the lower bound, which matches (up to poly-logarithmic factors) the uniformity testing upper bound for general distributions (Canonne, Chen, Kamath, Levi, Waingarten~\cite{CCKLW21}). Hence, we show that monotonicity does not help, beyond logarithmic factors, in testing uniformity of distributions with subcube conditional queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16355v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deeparnab Chakrabarty, Xi Chen, Simeon Ristic, C. Seshadhri, Erik Waingarten</dc:creator>
    </item>
    <item>
      <title>Worst-case Error Bounds for Online Learning of Smooth Functions</title>
      <link>https://arxiv.org/abs/2502.16388</link>
      <description>arXiv:2502.16388v1 Announce Type: cross 
Abstract: Online learning is a model of machine learning where the learner is trained on sequential feedback. We investigate worst-case error for the online learning of real functions that have certain smoothness constraints. Suppose that $\mathcal{F}_q$ is the class of all absolutely continuous functions $f: [0, 1] \rightarrow \mathbb{R}$ such that $\|f'\|_q \le 1$, and $\operatorname{opt}_p(\mathcal{F}_q)$ is the best possible upper bound on the sum of the $p^{\text{th}}$ powers of absolute prediction errors for any number of trials guaranteed by any learner. We show that for any $\delta, \epsilon \in (0, 1)$, $\operatorname{opt}_{1+\delta} (\mathcal{F}_{1+\epsilon}) = O(\min(\delta, \epsilon)^{-1})$. Combined with the previous results of Kimber and Long (1995) and Geneson and Zhou (2023), we achieve a complete characterization of the values of $p, q \ge 1$ that result in $\operatorname{opt}_p(\mathcal{F}_q)$ being finite, a problem open for nearly 30 years.
  We study the learning scenarios of smooth functions that also belong to certain special families of functions, such as polynomials. We prove a conjecture by Geneson and Zhou (2023) that it is not any easier to learn a polynomial in $\mathcal{F}_q$ than it is to learn any general function in $\mathcal{F}_q$. We also define a noisy model for the online learning of smooth functions, where the learner may receive incorrect feedback up to $\eta \ge 1$ times, denoting the worst-case error bound as $\operatorname{opt}^{\text{nf}}_{p, \eta} (\mathcal{F}_q)$. We prove that $\operatorname{opt}^{\text{nf}}_{p, \eta} (\mathcal{F}_q)$ is finite if and only if $\operatorname{opt}_p(\mathcal{F}_q)$ is. Moreover, we prove for all $p, q \ge 2$ and $\eta \ge 1$ that $\operatorname{opt}^{\text{nf}}_{p, \eta} (\mathcal{F}_q) = \Theta (\eta)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16388v1</guid>
      <category>cs.LG</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weian Xie</dc:creator>
    </item>
    <item>
      <title>SUperman: Efficient Permanent Computation on GPUs</title>
      <link>https://arxiv.org/abs/2502.16577</link>
      <description>arXiv:2502.16577v2 Announce Type: cross 
Abstract: The permanent is a function, defined for a square matrix, with applications in various domains including quantum computing, statistical physics, complexity theory, combinatorics, and graph theory. Its formula is similar to that of the determinant, however unlike the determinant, its exact computation is #P-complete, i.e., there is no algorithm to compute the permanent in polynomial time unless P=NP. For an $n \times n$ matrix, the fastest algorithm has a time complexity of $O(2^{n-1}n)$. Although supercomputers have been employed for permanent computation before, there is no work and more importantly, no publicly available software that leverages cutting-edge, yet widely accessible, High-Performance Computing accelerators such as GPUs. In this work, we designed, developed, and investigated the performance of SUperman, a complete software suite that can compute matrix permanents on multiple nodes/GPUs on a cluster while handling various matrix types, e.g., real/complex/binary and sparse/dense etc., with a unique treatment for each type. Compared to a state-of-the-art parallel algorithm on 44 cores, SUperman can be $86\times$ faster on a single Nvidia A100 GPU. Combining multiple GPUs, we also showed that SUperman can compute the permanent of a $56 \times 56$ matrix which is the largest reported in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16577v2</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deniz Elbek, Fatih Ta\c{s}yaran, Bora U\c{c}ar, Kamer Kaya</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Disk Inspection</title>
      <link>https://arxiv.org/abs/2411.15391</link>
      <description>arXiv:2411.15391v2 Announce Type: replace 
Abstract: We consider $n$ unit-speed mobile agents initially positioned at the center of a unit disk, tasked with inspecting all points on the disk's perimeter. A perimeter point is considered covered if an agent positioned outside the disk's interior has unobstructed visibility of it, treating the disk itself as an obstacle. For $n=1$, this problem is referred to as the shoreline problem with a known distance. Isbell in 1957 derived an optimal trajectory that minimizes the worst-case inspection time for that problem. The one-agent version of the problem was originally proposed as a more tractable variant of Bellman's famous lost-in-the-forest problem.
  Our contributions are threefold. First, and as a warm-up, we extend Isbell's findings by deriving worst-case optimal trajectories addressing the partial inspection of a section of the disk, hence deriving an alternative proof of optimality for inspecting the disk with $n \geq 2$ agents. Second, we analyze the average-case inspection time, assuming a uniform distribution of perimeter points (equivalent to randomized inspection algorithms). Using spatial discretization and Nonlinear Programming (NLP), we propose feasible solutions to the continuous problem and evaluate their effectiveness compared to NLP solutions. Third, we establish Pareto-optimal bounds for the multi-objective problem of jointly minimizing the worst-case and average-case inspection times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15391v2</guid>
      <category>cs.DM</category>
      <category>cs.CG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Conley, Konstantinos Georgiou</dc:creator>
    </item>
    <item>
      <title>Local geometry of NAE-SAT solutions in the condensation regime</title>
      <link>https://arxiv.org/abs/2305.17334</link>
      <description>arXiv:2305.17334v3 Announce Type: replace-cross 
Abstract: The local behavior of typical solutions of random constraint satisfaction problems (CSP) describes many important phenomena including clustering thresholds, decay of correlations, and the behavior of message passing algorithms. When the constraint density is low, studying the planted model is a powerful technique for determining this local behavior which in many examples has a simple Markovian structure. The work of Coja-Oghlan, Kapetanopoulos, M\"{u}ller (2020) showed that for a wide class of models, this description applies up to the so-called condensation threshold.
  Understanding the local behavior after the condensation threshold is more complex due to long-range correlations. In this work, we revisit the random regular NAE-SAT model in the condensation regime and determine the local weak limit which describes a random solution around a typical variable. This limit exhibits a complicated non-Markovian structure arising from the space of solutions being dominated by a small number of large clusters. This is the first description of the local weak limit in the condensation regime for any sparse random CSPs in the one-step replica symmetry breaking (1RSB) class. Our result is non-asymptotic, and characterizes the tight fluctuation $O(n^{-1/2})$ around the limit. Our proof is based on coupling the local neighborhoods of an infinite spin system, which encodes the structure of the clusters, to a broadcast model on trees whose channel is given by the 1RSB belief-propagation fixed point. We believe that our proof technique has broad applicability to random CSPs in the 1RSB class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17334v3</guid>
      <category>math.PR</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.DM</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Allan Sly, Youngtak Sohn</dc:creator>
    </item>
    <item>
      <title>Characterization of Circular-arc Graphs: III. Chordal Graphs</title>
      <link>https://arxiv.org/abs/2409.02733</link>
      <description>arXiv:2409.02733v2 Announce Type: replace-cross 
Abstract: We identify all minimal chordal graphs that are not circular-arc graphs, thereby resolving one of ``the main open problems'' concerning the structures of circular-arc graphs as posed by Dur{\'{a}}n, Grippo, and Safe in 2011. The problem had been attempted even earlier, and previous efforts have yielded partial results, particularly for claw-free graphs and graphs with an independence number of at most four. The answers turn out to have very simple structures: all the nontrivial ones belong to a single family. Our findings are based on a structural study of McConnell's flipping, which transforms circular-arc graphs into interval graphs with certain representation patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02733v2</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yixin Cao, Tomasz Krawczyk</dc:creator>
    </item>
    <item>
      <title>Hyperplanes Avoiding Problem and Integer Points Counting in Polyhedra</title>
      <link>https://arxiv.org/abs/2411.07030</link>
      <description>arXiv:2411.07030v2 Announce Type: replace-cross 
Abstract: In our work, we consider the problem of computing a vector $x \in Z^n$ of minimum $\|\cdot\|_p$-norm such that $a^\top x \not= a_0$, for any vector $(a,a_0)$ from a given subset of $Z^n$ of size $m$. In other words, we search for a vector of minimum norm that avoids a given finite set of hyperplanes, which is natural to call as the $\textit{Hyperplanes Avoiding Problem}$. This problem naturally appears as a subproblem in Barvinok-type algorithms for counting integer points in polyhedra. We show that:
  1) With respect to $\|\cdot\|_1$, the problem admits a feasible solution $x$ with $\|x\|_1 \leq (m+n)/2$, and show that such solution can be constructed by a deterministic polynomial-time algorithm with $O(n \cdot m)$ operations. Moreover, this inequality is the best possible. This is a significant improvement over the previous randomized algorithm, which computes $x$ with a guaranty $\|x\|_{1} \leq n \cdot m$. The original approach of A.~Barvinok can guarantee only $\|x\|_1 = O\bigl((n \cdot m)^n\bigr)$. To prove this result, we use a newly established algorithmic variant of the Combinatorial Nullstellensatz;
  2) The problem is NP-hard with respect to any norm $\|\cdot\|_p$, for $p \in \bigl(R_{\geq 1} \cup \{\infty\}\bigr)$.
  3) As an application, we show that the problem to count integer points in a polytope $P = \{x \in R^n \colon A x \leq b\}$, for given $A \in Z^{m \times n}$ and $b \in Q^m$, can be solved by an algorithm with $O\bigl(\nu^2 \cdot n^3 \cdot \Delta^3 \bigr)$ operations, where $\nu$ is the maximum size of a normal fan triangulation of $P$, and $\Delta$ is the maximum value of rank-order subdeterminants of $A$. As a further application, it provides a refined complexity bound for the counting problem in polyhedra of bounded codimension. For example, in the polyhedra of the Unbounded Subset-Sum problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07030v2</guid>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigorii Dakhno, Dmitry Gribanov, Nikita Kasianov, Anastasiia Kats, Andrey Kupavskii, Nikita Kuz'min</dc:creator>
    </item>
  </channel>
</rss>
