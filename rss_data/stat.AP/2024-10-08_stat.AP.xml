<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Oct 2024 04:00:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Combining Structural and Unstructured Data: A Topic-based Finite Mixture Model for Insurance Claim Prediction</title>
      <link>https://arxiv.org/abs/2410.04684</link>
      <description>arXiv:2410.04684v1 Announce Type: new 
Abstract: Modeling insurance claim amounts and classifying claims into different risk levels are critical yet challenging tasks. Traditional predictive models for insurance claims often overlook the valuable information embedded in claim descriptions. This paper introduces a novel approach by developing a joint mixture model that integrates both claim descriptions and claim amounts. Our method establishes a probabilistic link between textual descriptions and loss amounts, enhancing the accuracy of claims clustering and prediction. In our proposed model, the latent topic/component indicator serves as a proxy for both the thematic content of the claim description and the component of loss distributions. Specifically, conditioned on the topic/component indicator, the claim description follows a multinomial distribution, while the claim amount follows a component loss distribution. We propose two methods for model calibration: an EM algorithm for maximum a posteriori estimates, and an MH-within-Gibbs sampler algorithm for the posterior distribution. The empirical study demonstrates that the proposed methods work effectively, providing interpretable claims clustering and prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04684v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanxi Hou, Xiaolan Xia, Guangyuan Gao</dc:creator>
    </item>
    <item>
      <title>Post-Quantum Cryptography Anonymous Scheme -- PQCWC: Post-Quantum Cryptography Winternitz-Chen</title>
      <link>https://arxiv.org/abs/2410.03678</link>
      <description>arXiv:2410.03678v1 Announce Type: cross 
Abstract: As quantum computing technology matures, it poses a threat to the security of mainstream asymmetric cryptographic methods. In response, the National Institute of Standards and Technology released the final version of post-quantum cryptographic (PQC) algorithm standards in August 2024. These post-quantum cryptographic algorithms are primarily based on lattice-based and hash-based cryptography. Therefore, this study proposes the Post-Quantum Cryptography Winternitz-Chen (PQCWC) anonymous scheme, aimed at exploring the design of anonymous schemes based on PQC for future applications in privacy protection. The anonymous scheme designed in this study is mainly built on the Winternitz signature scheme, which can prevent the original public key from being exposed in the certificate. Furthermore, the PQCWC anonymous scheme integrates the butterfly key expansion mechanism, proposing the first hash-based butterfly key expansion mechanism in the world, achieving anonymity for both the registration authority and the certificate authority, thereby fully protecting privacy. In the experimental environment, this study compares various hash algorithms, including Secure Hash Algorithm-1 (SHA-1), the SHA-2 series, the SHA-3 series, and the BLAKE series. The results demonstrate that the proposed anonymous scheme can achieve anonymity without increasing key length, signature length, key generation time, signature generation time, or signature verification time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03678v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abel C. H. Chen</dc:creator>
    </item>
    <item>
      <title>Modeling and Analysis of Spatial and Temporal Land Clutter Statistics in SAR Imaging Based on MSTAR Data</title>
      <link>https://arxiv.org/abs/2410.03816</link>
      <description>arXiv:2410.03816v1 Announce Type: cross 
Abstract: The statistical analysis of land clutter for Synthetic Aperture Radar (SAR) imaging has become an increasingly important subject for research and investigation. It is also absolutely necessary for designing robust algorithms capable of performing the task of target detection in the background clutter. Any attempt to extract the energy of the desired targets from the land clutter requires complete knowledge of the statistical properties of the background clutter. In this paper, the spatial as well as the temporal characteristics of the land clutter are studied. Since the data for each image has been collected based on a different aspect angle; therefore, the temporal analysis contains variation in the aspect angle. Consequently, the temporal analysis includes the characteristics of the radar cross section with respect to the aspect angle based on which the data has been collected. In order to perform the statistical analysis, several well-known and relevant distributions, namely, Weibull, Log-normal, Gamma, and Rayleigh are considered as prime candidates to model the land clutter. The goodness-of-fit test is based on the Kullback-Leibler (KL) Divergence metric. The detailed analysis presented in this paper demonstrates that the Weibull distribution is a more accurate fit for the temporal-aspect-angle statistical analysis while the Rayleigh distribution models the spatial characteristics of the background clutter with higher accuracy. Finally, based on the aforementioned statistical analyses and by utilizing the Constant False Alarm Rate (CFAR) algorithm, we perform target detection in land clutter. The overall verification of the analysis is performed by exploiting the Moving and Stationary Target Acquisition and Recognition (MSTAR) data-set, which has been collected in spotlight mode at X-band, and the results are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03816v1</guid>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shahrokh Hamidi</dc:creator>
    </item>
    <item>
      <title>Efficient Bayesian Additive Regression Models For Microbiome Studies</title>
      <link>https://arxiv.org/abs/2410.03911</link>
      <description>arXiv:2410.03911v1 Announce Type: cross 
Abstract: Statistical analysis of microbiome data is challenging. Bayesian multinomial logistic-normal (MLN) models have gained popularity due to their ability to account for the count compositional nature of these data. However, these models are often computationally intractable to infer. Recently, we developed a computationally efficient and accurate approach to inferring MLN models with a Marginally Latent Matrix-T Process (MLTP) form: MLN-MLTPs. Our approach is based on a novel sampler with a marginal Laplace approximation -- called the \textit{Collapse-Uncollapse} (CU) sampler. However, existing work with MLTPs has been limited to linear models or models of a single non-linear process. Moreover, existing methods lack an efficient means of estimating model hyperparameters. This article addresses both deficiencies. We introduce a new class of MLN Additive Gaussian Process models (\textit{MultiAddGPs}) for deconvolution of overlapping linear and non-linear processes. We show that MultiAddGPs are examples of MLN-MLTPs and derive an efficient CU sampler for this model class. Moreover, we derive efficient Maximum Marginal Likelihood estimation for hyperparameters in MLTP models by taking advantage of Laplace approximation in the CU sampler. We demonstrate our approach using simulated and real data studies. Our models produce novel biological insights from a previously published artificial gut study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03911v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tinghua Chen, Michelle Pistner Nixon, Justin D. Silverman</dc:creator>
    </item>
    <item>
      <title>"6 choose 4": A framework to understand and facilitate discussion of strategies for overall survival safety monitoring</title>
      <link>https://arxiv.org/abs/2410.04020</link>
      <description>arXiv:2410.04020v1 Announce Type: cross 
Abstract: Advances in anticancer therapies have significantly contributed to declining death rates in certain disease and clinical settings. However, they have also made it difficult to power a clinical trial in these settings with overall survival (OS) as the primary efficacy endpoint. A number of statistical approaches have therefore been recently proposed for the pre-specified analysis of OS as a safety endpoint (Shan, 2023; Fleming et al., 2024; Rodriguez et al., 2024). In this paper, we provide a simple, unifying framework that includes the aforementioned approaches (and a couple others) as special cases. By highlighting each approach's focus, priority, tolerance for risk, and strengths or challenges for practical implementation, this framework can help to facilitate discussions between stakeholders on "fit-for-purpose OS data collection and assessment of harm" (American Association for Cancer Research, 2024). We apply this framework to a real clinical trial in large B-cell lymphoma to illustrate its application and value. Several recommendations and open questions are also raised.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04020v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Godwin Yung, Kaspar Rufibach, Marcel Wolbers, Mark Yan, Jue Wang</dc:creator>
    </item>
    <item>
      <title>Penalized Sparse Covariance Regression with High Dimensional Covariates</title>
      <link>https://arxiv.org/abs/2410.04028</link>
      <description>arXiv:2410.04028v1 Announce Type: cross 
Abstract: Covariance regression offers an effective way to model the large covariance matrix with the auxiliary similarity matrices. In this work, we propose a sparse covariance regression (SCR) approach to handle the potentially high-dimensional predictors (i.e., similarity matrices). Specifically, we use the penalization method to identify the informative predictors and estimate their associated coefficients simultaneously. We first investigate the Lasso estimator and subsequently consider the folded concave penalized estimation methods (e.g., SCAD and MCP). However, the theoretical analysis of the existing penalization methods is primarily based on i.i.d. data, which is not directly applicable to our scenario. To address this difficulty, we establish the non-asymptotic error bounds by exploiting the spectral properties of the covariance matrix and similarity matrices. Then, we derive the estimation error bound for the Lasso estimator and establish the desirable oracle property of the folded concave penalized estimator. Extensive simulation studies are conducted to corroborate our theoretical results. We also illustrate the usefulness of the proposed method by applying it to a Chinese stock market dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04028v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Gao, Zhiyuan Zhang, Zhanrui Cai, Xuening Zhu, Tao Zou, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Approximate Maximum Likelihood Inference for Acoustic Spatial Capture-Recapture with Unknown Identities, Using Monte Carlo Expectation Maximization</title>
      <link>https://arxiv.org/abs/2410.04390</link>
      <description>arXiv:2410.04390v1 Announce Type: cross 
Abstract: Acoustic spatial capture-recapture (ASCR) surveys with an array of synchronized acoustic detectors can be an effective way of estimating animal density or call density. However, constructing the capture histories required for ASCR analysis is challenging, as recognizing which detections at different detectors are of which calls is not a trivial task. Because calls from different distances take different times to arrive at detectors, the order in which calls are detected is not necessarily the same as the order in which they are made, and without knowing which detections are of the same call, we do not know how many different calls are detected. We propose a Monte Carlo expectation-maximization (MCEM) estimation method to resolve this unknown call identity problem. To implement the MCEM method in this context, we sample the latent variables from a complete-data likelihood model in the expectation step and use a semi-complete-data likelihood or conditional likelihood in the maximization step. We use a parametric bootstrap to obtain confidence intervals. When we apply our method to a survey of moss frogs, it gives an estimate within 15% of the estimate obtained using data with call capture histories constructed by experts, and unlike this latter estimate, our confidence interval incorporates the uncertainty about call identities. Simulations show it to have a low bias (6%) and coverage probabilities close to the nominal 95% value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04390v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuheng Wang, Juan Ye, Weiye Li, David L. Borchers</dc:creator>
    </item>
    <item>
      <title>A Reflection on the Impact of Misspecifying Unidentifiable Causal Inference Models in Surrogate Endpoint Evaluation</title>
      <link>https://arxiv.org/abs/2410.04438</link>
      <description>arXiv:2410.04438v1 Announce Type: cross 
Abstract: Surrogate endpoints are often used in place of expensive, delayed, or rare true endpoints in clinical trials. However, regulatory authorities require thorough evaluation to accept these surrogate endpoints as reliable substitutes. One evaluation approach is the information-theoretic causal inference framework, which quantifies surrogacy using the individual causal association (ICA). Like most causal inference methods, this approach relies on models that are only partially identifiable. For continuous outcomes, a normal model is often used. Based on theoretical elements and a Monte Carlo procedure we studied the impact of model misspecification across two scenarios: 1) the true model is based on a multivariate t-distribution, and 2) the true model is based on a multivariate log-normal distribution. In the first scenario, the misspecification has a negligible impact on the results, while in the second, it has a significant impact when the misspecification is detectable using the observed data. Finally, we analyzed two data sets using the normal model and several D-vine copula models that were indistinguishable from the normal model based on the data at hand. We observed that the results may vary when different models are used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04438v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gokce Deliorman, Florian Stijven, Wim Van der Elst, Maria del Carmen Pardo, Ariel Alonso</dc:creator>
    </item>
    <item>
      <title>A Bayesian Method for Adverse Effects Estimation in Observational Studies with Truncation by Death</title>
      <link>https://arxiv.org/abs/2410.04561</link>
      <description>arXiv:2410.04561v1 Announce Type: cross 
Abstract: Death among subjects is common in observational studies evaluating the causal effects of interventions among geriatric or severely ill patients. High mortality rates complicate the comparison of the prevalence of adverse events (AEs) between interventions. This problem is often referred to as outcome "truncation" by death. A possible solution is to estimate the survivor average causal effect (SACE), an estimand that evaluates the effects of interventions among those who would have survived under both treatment assignments. However, because the SACE does not include subjects who would have died under one or both arms, it does not consider the relationship between AEs and death. We propose a Bayesian method which imputes the unobserved mortality and AE outcomes for each participant under the intervention they did not receive. Using the imputed outcomes we define a composite ordinal outcome for each patient, combining the occurrence of death and the AE in an increasing scale of severity. This allows for the comparison of the effects of the interventions on death and the AE simultaneously among the entire sample. We implement the procedure to analyze the incidence of heart failure among geriatric patients being treated for Type II diabetes with sulfonylureas or dipeptidyl peptidase-4 inhibitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04561v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Sisti, Andrew Zullo, Roee Gutman</dc:creator>
    </item>
    <item>
      <title>Ranking Policy Learning via Marketplace Expected Value Estimation From Observational Data</title>
      <link>https://arxiv.org/abs/2410.04568</link>
      <description>arXiv:2410.04568v1 Announce Type: cross 
Abstract: We develop a decision making framework to cast the problem of learning a ranking policy for search or recommendation engines in a two-sided e-commerce marketplace as an expected reward optimization problem using observational data. As a value allocation mechanism, the ranking policy allocates retrieved items to the designated slots so as to maximize the user utility from the slotted items, at any given stage of the shopping journey. The objective of this allocation can in turn be defined with respect to the underlying probabilistic user browsing model as the expected number of interaction events on presented items matching the user intent, given the ranking context. Through recognizing the effect of ranking as an intervention action to inform users' interactions with slotted items and the corresponding economic value of the interaction events for the marketplace, we formulate the expected reward of the marketplace as the collective value from all presented ranking actions. The key element in this formulation is a notion of context value distribution, which signifies not only the attribution of value to ranking interventions within a session but also the distribution of marketplace reward across user sessions. We build empirical estimates for the expected reward of the marketplace from observational data that account for the heterogeneity of economic value across session contexts as well as the distribution shifts in learning from observational user activity data. The ranking policy can then be trained by optimizing the empirical expected reward estimates via standard Bayesian inference techniques. We report empirical results for a product search ranking task in a major e-commerce platform demonstrating the fundamental trade-offs governed by ranking polices trained on empirical reward estimates with respect to extreme choices of the context value distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04568v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ehsan Ebrahimzadeh, Nikhil Monga, Hang Gao, Alex Cozzi, Abraham Bagherjeiran</dc:creator>
    </item>
    <item>
      <title>A Finite Mixture Hidden Markov Model for Intermittently Observed Disease Process with Heterogeneity and Partially Known Disease Type</title>
      <link>https://arxiv.org/abs/2410.04667</link>
      <description>arXiv:2410.04667v1 Announce Type: cross 
Abstract: Continuous-time multistate models are widely used for analyzing interval-censored data on disease progression over time. Sometimes, diseases manifest differently and what appears to be a coherent collection of symptoms is the expression of multiple distinct disease subtypes. To address this complexity, we propose a mixture hidden Markov model, where the observation process encompasses states representing common symptomatic stages across these diseases, and each underlying process corresponds to a distinct disease subtype. Our method models both the overall and the type-specific disease incidence/prevalence accounting for sampling conditions and exactly observed death times. Additionally, it can utilize partially available disease-type information, which offers insights into the pathway through specific hidden states in the disease process, to aid in the estimation. We present both a frequentist and a Bayesian way to obtain the estimates. The finite sample performance is evaluated through simulation studies. We demonstrate our method using the Nun Study and model the development and progression of dementia, encompassing both Alzheimer's disease (AD) and non-AD dementia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04667v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yidan Shi (Department of Population Health, New York University Grossman School of Medicine), Leilei Zeng (Department of Statistics and Actuarial Science, University of Waterloo), Mary E. Thompson (Department of Statistics and Actuarial Science, University of Waterloo), Suzanne L. Tyas (School of Public Health Sciences, University of Waterloo)</dc:creator>
    </item>
    <item>
      <title>Economic growth of cities: Does resource allocation matter?</title>
      <link>https://arxiv.org/abs/2410.04918</link>
      <description>arXiv:2410.04918v1 Announce Type: cross 
Abstract: We study how efficient resource reallocation across cities affects potential aggregate growth. Using optimal resource allocation models and data on 284 China's prefecture-level cities in the years 2003--2019, we quantitatively measure the cost of misallocation of resources. We show that average aggregate output gains from reallocating resources across nationwide cities to their efficient use are 1.349- and 1.287-fold in the perfect and imperfect allocation scenarios. We further provide evidence on the effects of administrative division adjustments and local allocation. This suggests that city-level adjustments can yield more aggregate gain and that the output gain from nationwide allocation is likely to be more substantial than that from local allocation. Policy implications are proposed to improve the resource allocation efficiency in China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04918v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sheng Dai, Timo Kuosmanen, Zhiqiang Liao</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Post-Integrated Inference with Negative Control Outcomes</title>
      <link>https://arxiv.org/abs/2410.04996</link>
      <description>arXiv:2410.04996v1 Announce Type: cross 
Abstract: Data integration has become increasingly common in aligning multiple heterogeneous datasets. With high-dimensional outcomes, data integration methods aim to extract low-dimensional embeddings of observations to remove unwanted variations, such as batch effects and unmeasured covariates, inherent in data collected from different sources. However, multiple hypothesis testing after data integration can be substantially biased due to the data-dependent integration processes. To address this challenge, we introduce a robust post-integrated inference (PII) method that adjusts for latent heterogeneity using negative control outcomes. By leveraging causal interpretations, we derive nonparametric identification conditions that form the basis of our PII approach.
  Our assumption-lean semiparametric inference method extends robustness and generality to projected direct effect estimands that account for mediators, confounders, and moderators. These estimands remain statistically meaningful under model misspecifications and with error-prone embeddings. We provide deterministic quantifications of the bias of target estimands induced by estimated embeddings and finite-sample linear expansions of the estimators with uniform concentration bounds on the residuals for all outcomes.
  The proposed doubly robust estimators are consistent and efficient under minimal assumptions, facilitating data-adaptive estimation with machine learning algorithms. Using random forests, we evaluate empirical statistical errors in simulations and analyze single-cell CRISPR perturbed datasets with potential unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04996v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Kathryn Roeder, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Precise Model Benchmarking with Only a Few Observations</title>
      <link>https://arxiv.org/abs/2410.05222</link>
      <description>arXiv:2410.05222v1 Announce Type: cross 
Abstract: How can we precisely estimate a large language model's (LLM) accuracy on questions belonging to a specific topic within a larger question-answering dataset? The standard direct estimator, which averages the model's accuracy on the questions in each subgroup, may exhibit high variance for subgroups (topics) with small sample sizes. Synthetic regression modeling, which leverages the model's accuracy on questions about other topics, may yield biased estimates that are too unreliable for large subgroups. We prescribe a simple yet effective solution: an empirical Bayes (EB) estimator that balances direct and regression estimates for each subgroup separately, improving the precision of subgroup-level estimates of model performance. Our experiments on multiple datasets show that this approach consistently provides more precise estimates of the LLM performance compared to the direct and regression approaches, achieving substantial reductions in the mean squared error. Confidence intervals for EB estimates also have near-nominal coverage and are narrower compared to those for the direct estimator. Additional experiments on tabular and vision data validate the benefits of this EB approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05222v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riccardo Fogliato, Pratik Patil, Nil-Jana Akpinar, Mathew Monfort</dc:creator>
    </item>
    <item>
      <title>Using spatial extreme-value theory with machine learning to model and understand spatially compounding weather extremes</title>
      <link>https://arxiv.org/abs/2401.12195</link>
      <description>arXiv:2401.12195v4 Announce Type: replace 
Abstract: When extreme weather events affect large areas, their regional to sub-continental spatial scale is important for their impacts. We propose a novel machine learning (ML) framework that integrates spatial extreme-value theory to model weather extremes and to quantify probabilities associated with the occurrence, intensity, and spatial extent of these events. Our approach employs new loss functions adapted to extreme values, enabling our model to prioritize the tail rather than the bulk of the data distribution. Applied to a case study of Western European summertime heat extremes, we use daily 500-hPa geopotential height fields and local soil moisture as predictors to capture the complex interplay between local and remote physical processes. Our generative model reveals the importance of individual circulation features in determining different facets of heat extremes, thereby enriching our process understanding from a data-driven perspective. Heat extremes are sensitive to the relative position of upper-level ridges and troughs that are part of a large-scale wave pattern. Our approach can extrapolate beyond the range of the data to make risk-related probabilistic statements. It applies more generally to other weather extremes and offers an alternative to traditional physical and ML-based techniques that focus less on the extremal aspects of weather data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12195v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Koh, Daniel Steinfeld, Olivia Martius</dc:creator>
    </item>
    <item>
      <title>Granger Causality in High-Dimensional Networks of Time Series</title>
      <link>https://arxiv.org/abs/2406.02360</link>
      <description>arXiv:2406.02360v2 Announce Type: replace 
Abstract: A novel approach is developed for discovering directed connectivity between specified pairs of nodes in a high-dimensional network (HDN) of brain signals. To accurately identify causal connectivity for such specified objectives, it is necessary to properly address the influence of all other nodes within the network. The proposed procedure herein starts with the estimation of a low-dimensional representation of the other nodes in the network utilizing (frequency-domain-based) spectral dynamic principal component analysis (sDPCA). The resulting scores can then be removed from the nodes of interest, thus eliminating the confounding effect of other nodes within the HDN. Accordingly, causal interactions can be dissected between nodes that are isolated from the effects of the network. Extensive simulations have demonstrated the effectiveness of this approach as a tool for causality analysis in complex time series networks. The proposed methodology has also been shown to be applicable to multichannel EEG networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02360v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Surrogate model for Bayesian optimal experimental design in chromatography</title>
      <link>https://arxiv.org/abs/2406.19835</link>
      <description>arXiv:2406.19835v2 Announce Type: replace 
Abstract: We applied Bayesian Optimal Experimental Design (OED) in the estimation of parameters involved in the Equilibrium Dispersive Model for chromatography with two components with the Langmuir adsorption isotherm. The coefficients estimated were Henry's coefficients, the total absorption capacity and the number of theoretical plates, while the design variables were the injection time and the initial concentration. The Bayesian OED algorithm is based on nested Monte Carlo estimation, which becomes computationally challenging due to the simulation time of the PDE involved in the dispersive model. This complication was relaxed by introducing a surrogate model based on Piecewise Sparse Linear Interpolation. Using the surrogate model instead the original reduces significantly the simulation time and it approximates the solution of the PDE with high degree of accuracy. The estimation of the parameters over strategical design points provided by OED reduces the uncertainty in the estimation of parameters. Additionally, the Bayesian OED methodology indicates no improvements when increasing the number of measurements in temporal nodes above a threshold value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19835v2</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose Rodrigo Rojo-Garcia, Heikki Haario, Tapio Helin, Tuomo Sainio</dc:creator>
    </item>
    <item>
      <title>Multiple merger coalescent inference of effective population size</title>
      <link>https://arxiv.org/abs/2407.14976</link>
      <description>arXiv:2407.14976v2 Announce Type: replace 
Abstract: Variation in a sample of molecular sequence data informs about the past evolutionary history of the sample's population. Traditionally, Bayesian modeling coupled with the standard coalescent, is used to infer the sample's bifurcating genealogy and demographic and evolutionary parameters such as effective population size, and mutation rates. However, there are many situations where binary coalescent models do not accurately reflect the true underlying ancestral processes. Here, we propose a Bayesian nonparametric method for inferring effective population size trajectories from a multifurcating genealogy under the $\Lambda-$coalescent. In particular, we jointly estimate the effective population size and model parameters for the Beta-coalescent model, a special type of $\Lambda-$coalescent. Finally, we test our methods on simulations and apply them to study various viral dynamics as well as Japanese sardine population size changes over time. The code and vignettes can be found in the phylodyn package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14976v2</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julie Zhang, Julia A. Palacios</dc:creator>
    </item>
    <item>
      <title>Algorithmic Forecasting of Extreme Heat Waves</title>
      <link>https://arxiv.org/abs/2409.18305</link>
      <description>arXiv:2409.18305v2 Announce Type: replace 
Abstract: This paper provides forecasts of rare and extreme heat waves coupled with estimates of uncertainty. Both rest in part on gaining a better understanding of the similarities and differences between hot days under normal circumstances and rare, extreme heat waves. We analyze AIRS data from the American Pacific Northwest and AIRS data from the Phoenix, Arizona region. Forecasting accuracy is excellent for the Pacific Northwest and is replicated for Phoenix. Uncertainty is estimated with nested conformal prediction sets, provably valid even for finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18305v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard A. Berk, Amy Braverman</dc:creator>
    </item>
    <item>
      <title>CST-YOLO: A Novel Method for Blood Cell Detection Based on Improved YOLOv7 and CNN-Swin Transformer</title>
      <link>https://arxiv.org/abs/2306.14590</link>
      <description>arXiv:2306.14590v2 Announce Type: replace-cross 
Abstract: Blood cell detection is a typical small-scale object detection problem in computer vision. In this paper, we propose a CST-YOLO model for blood cell detection based on YOLOv7 architecture and enhance it with the CNN-Swin Transformer (CST), which is a new attempt at CNN-Transformer fusion. We also introduce three other useful modules: Weighted Efficient Layer Aggregation Networks (W-ELAN), Multiscale Channel Split (MCS), and Concatenate Convolutional Layers (CatConv) in our CST-YOLO to improve small-scale object detection precision. Experimental results show that the proposed CST-YOLO achieves 92.7%, 95.6%, and 91.1% mAP@0.5, respectively, on three blood cell datasets, outperforming state-of-the-art object detectors, e.g., RT-DETR, YOLOv5, and YOLOv7. Our code is available at https://github.com/mkang315/CST-YOLO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14590v2</guid>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICIP51287.2024.10647618</arxiv:DOI>
      <arxiv:journal_reference>In ICIP (2024) 3024--3029</arxiv:journal_reference>
      <dc:creator>Ming Kang, Chee-Ming Ting, Fung Fung Ting, Rapha\"el Phan</dc:creator>
    </item>
    <item>
      <title>An engine to simulate insurance fraud network data</title>
      <link>https://arxiv.org/abs/2308.11659</link>
      <description>arXiv:2308.11659v2 Announce Type: replace-cross 
Abstract: Traditionally, the detection of fraudulent insurance claims relies on business rules and expert judgement which makes it a time-consuming and expensive process (\'Oskarsd\'ottir et al., 2022). Consequently, researchers have been examining ways to develop efficient and accurate analytic strategies to flag suspicious claims. Feeding learning methods with features engineered from the social network of parties involved in a claim is a particularly promising strategy (see for example Van Vlasselaer et al. (2016); Tumminello et al. (2023)). When developing a fraud detection model, however, we are confronted with several challenges. The uncommon nature of fraud, for example, creates a high class imbalance which complicates the development of well performing analytic classification models. In addition, only a small number of claims are investigated and get a label, which results in a large corpus of unlabeled data. Yet another challenge is the lack of publicly available data. This hinders not only the development of new methods, but also the validation of existing techniques. We therefore design a simulation machine that is engineered to create synthetic data with a network structure and available covariates similar to the real life insurance fraud data set analyzed in \'Oskarsd\'ottir et al. (2022). Further, the user has control over several data-generating mechanisms. We can specify the total number of policyholders and parties, the desired level of imbalance and the (effect size of the) features in the fraud generating model. As such, the simulation engine enables researchers and practitioners to examine several methodological challenges as well as to test their (development strategy of) insurance fraud detection models in a range of different settings. Moreover, large synthetic data sets can be generated to evaluate the predictive performance of (advanced) machine learning techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11659v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bavo D. C. Campo, Katrien Antonio</dc:creator>
    </item>
    <item>
      <title>CAFCT-Net: A CNN-Transformer Hybrid Network with Contextual and Attentional Feature Fusion for Liver Tumor Segmentation</title>
      <link>https://arxiv.org/abs/2401.16886</link>
      <description>arXiv:2401.16886v2 Announce Type: replace-cross 
Abstract: Medical image semantic segmentation techniques can help identify tumors automatically from computed tomography (CT) scans. In this paper, we propose a Contextual and Attentional feature Fusions enhanced Convolutional Neural Network (CNN) and Transformer hybrid network (CAFCT-Net) for liver tumor segmentation. We incorporate three novel modules in the CAFCT-Net architecture: Attentional Feature Fusion (AFF), Atrous Spatial Pyramid Pooling (ASPP) of DeepLabv3, and Attention Gates (AGs) to improve contextual information related to tumor boundaries for accurate segmentation. Experimental results show that the proposed model achieves a mean Intersection over Union (IoU) of 76.54% and Dice coefficient of 84.29%, respectively, on the Liver Tumor Segmentation Benchmark (LiTS) dataset, outperforming pure CNN or Transformer methods, e.g., Attention U-Net and PVTFormer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16886v2</guid>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICIP51287.2024.10647768</arxiv:DOI>
      <arxiv:journal_reference>In ICIP (2024) 2970--2974</arxiv:journal_reference>
      <dc:creator>Ming Kang, Chee-Ming Ting, Fung Fung Ting, Rapha\"el Phan</dc:creator>
    </item>
    <item>
      <title>Postprocessing of point predictions for probabilistic forecasting of day-ahead electricity prices: The benefits of using isotonic distributional regression</title>
      <link>https://arxiv.org/abs/2404.02270</link>
      <description>arXiv:2404.02270v2 Announce Type: replace-cross 
Abstract: Operational decisions relying on predictive distributions of electricity prices can result in significantly higher profits compared to those based solely on point forecasts. However, the majority of models developed in both academic and industrial settings provide only point predictions. To address this, we examine three postprocessing methods for converting point forecasts of day-ahead electricity prices into probabilistic ones: Quantile Regression Averaging, Conformal Prediction, and the recently introduced Isotonic Distributional Regression. We find that while the latter demonstrates the most varied behavior, it contributes the most to the ensemble of the three predictive distributions, as measured by Shapley values. Remarkably, the performance of the combination is superior to that of state-of-the-art Distributional Deep Neural Networks over two 4.5-year test periods from the German and Spanish power markets, spanning the COVID pandemic and the war in Ukraine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02270v2</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.eneco.2024.107934</arxiv:DOI>
      <arxiv:journal_reference>Energy Economics (2024) 107934</arxiv:journal_reference>
      <dc:creator>Arkadiusz Lipiecki, Bartosz Uniejewski, Rafa{\l} Weron</dc:creator>
    </item>
    <item>
      <title>Adjusting for Selection Bias Due to Missing Eligibility Criteria in Emulated Target Trials</title>
      <link>https://arxiv.org/abs/2406.16830</link>
      <description>arXiv:2406.16830v2 Announce Type: replace-cross 
Abstract: Target trial emulation (TTE) is a popular framework for observational studies based on electronic health records (EHR). A key component of this framework is determining the patient population eligible for inclusion in both a target trial of interest and its observational emulation. Missingness in variables that define eligibility criteria, however, presents a major challenge towards determining the eligible population when emulating a target trial with an observational study. In practice, patients with incomplete data are almost always excluded from analysis despite the possibility of selection bias, which can arise when subjects with observed eligibility data are fundamentally different than excluded subjects. Despite this, to the best of our knowledge, very little work has been done to mitigate this concern. In this paper, we propose a novel conceptual framework to address selection bias in TTE studies, tailored towards time-to-event endpoints, and describe estimation and inferential procedures via inverse probability weighting (IPW). Under an EHR-based simulation infrastructure, developed to reflect the complexity of EHR data, we characterize common settings under which missing eligibility data poses the threat of selection bias and investigate the ability of the proposed methods to address it. Finally, using EHR databases from Kaiser Permanente, we demonstrate the use of our method to evaluate the effect of bariatric surgery on microvascular outcomes among a cohort of severely obese patients with Type II diabetes mellitus (T2DM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16830v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Benz, Rajarshi Mukherjee, Rui Wang, David Arterburn, Catherine Lee, Heidi Fischer, Susan Shortreed, Sebastien Haneuse</dc:creator>
    </item>
    <item>
      <title>A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios</title>
      <link>https://arxiv.org/abs/2408.01963</link>
      <description>arXiv:2408.01963v2 Announce Type: replace-cross 
Abstract: We evaluate the robustness of several large language models on multiple datasets. Robustness here refers to the relative insensitivity of the model's answers to meaning-preserving variants of their input. Benchmark datasets are constructed by introducing naturally-occurring, non-malicious perturbations, or by generating semantically equivalent paraphrases of input questions or statements. We further propose a novel metric for assessing a model robustness, and demonstrate its benefits in the non-adversarial scenario by empirical evaluation of several models on the created datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01963v2</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Ackerman, Ella Rabinovich, Eitan Farchi, Ateret Anaby-Tavor</dc:creator>
    </item>
  </channel>
</rss>
