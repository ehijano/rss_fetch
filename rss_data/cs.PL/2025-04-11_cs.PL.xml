<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Apr 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Program Skeletons for Automated Program Translation</title>
      <link>https://arxiv.org/abs/2504.07483</link>
      <description>arXiv:2504.07483v1 Announce Type: new 
Abstract: Translating software between programming languages is a challenging task, for which automated techniques have been elusive and hard to scale up to larger programs. A key difficulty in cross-language translation is that one has to re-express the intended behavior of the source program into idiomatic constructs of a different target language. This task needs abstracting away from the source language-specific details, while keeping the overall functionality the same. In this work, we propose a novel and systematic approach for making such translation amenable to automation based on a framework we call program skeletons. A program skeleton retains the high-level structure of the source program by abstracting away and effectively summarizing lower-level concrete code fragments, which can be mechanically translated to the target programming language. A skeleton, by design, permits many different ways of filling in the concrete implementation for fragments, which can work in conjunction with existing data-driven code synthesizers. Most importantly, skeletons can conceptually enable sound decomposition, i.e., if each individual fragment is correctly translated, taken together with the mechanically translated skeleton, the final translated program is deemed to be correct as a whole. We present a prototype system called Skel embodying the idea of skeleton-based translation from Python to JavaScript. Our results show promising scalability compared to prior works. For 9 real-world Python programs, some with more than about 1k lines of code, 95% of their code fragments can be automatically translated, while about 5% require manual effort. All the final translations are correct with respect to whole-program test suites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07483v1</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3729287</arxiv:DOI>
      <dc:creator>Bo Wang, Tianyu Li, Ruishi Li, Umang Mathur, Prateek Saxena</dc:creator>
    </item>
    <item>
      <title>Efficient Formal Verification of Quantum Error Correcting Programs</title>
      <link>https://arxiv.org/abs/2504.07732</link>
      <description>arXiv:2504.07732v1 Announce Type: new 
Abstract: Quantum error correction (QEC) is fundamental for suppressing noise in quantum hardware and enabling fault-tolerant quantum computation. In this paper, we propose an efficient verification framework for QEC programs. We define an assertion logic and a program logic specifically crafted for QEC programs and establish a sound proof system. We then develop an efficient method for handling verification conditions (VCs) of QEC programs: for Pauli errors, the VCs are reduced to classical assertions that can be solved by SMT solvers, and for non-Pauli errors, we provide a heuristic algorithm. We formalize the proposed program logic in Coq proof assistant, making it a verified QEC verifier. Additionally, we implement an automated QEC verifier, Veri-QEC, for verifying various fault-tolerant scenarios. We demonstrate the efficiency and broad functionality of the framework by performing different verification tasks across various scenarios. Finally, we present a benchmark of 14 verified stabilizer codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07732v1</guid>
      <category>cs.PL</category>
      <category>quant-ph</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3729293</arxiv:DOI>
      <dc:creator>Qifan Huang, Li Zhou, Wang Fang, Mengyu Zhao, Mingsheng Ying</dc:creator>
    </item>
    <item>
      <title>A Hybrid Approach to Semi-automated Rust Verification</title>
      <link>https://arxiv.org/abs/2403.15122</link>
      <description>arXiv:2403.15122v3 Announce Type: replace 
Abstract: We propose a hybrid approach to end-to-end Rust verification where the proof effort is split into powerful automated verification of safe Rust and targeted semi-automated verification of unsafe Rust. To this end, we present Gillian-Rust, a proof-of-concept semi-automated verification tool built on top of the Gillian platform that can reason about type safety and functional correctness of unsafe code. Gillian-Rust automates a rich separation logic for real-world Rust, embedding the lifetime logic of RustBelt and the parametric prophecies of RustHornBelt, and is able to verify real-world Rust standard library code with only minor annotations and with verification times orders of magnitude faster than those of comparable tools. We link Gillian-Rust with Creusot, a state-of-the-art verifier for safe Rust, by providing a systematic encoding of unsafe code specifications that Creusot can use but cannot verify, demonstrating the feasibility of our hybrid approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15122v3</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sacha-\'Elie Ayoun, Xavier Denis, Petar Maksimovi\'c, Philippa Gardner</dc:creator>
    </item>
    <item>
      <title>Scaling Optimization Over Uncertainty via Compilation</title>
      <link>https://arxiv.org/abs/2502.18728</link>
      <description>arXiv:2502.18728v2 Announce Type: replace 
Abstract: Probabilistic inference is fundamentally hard, yet many tasks require optimization on top of inference, which is even harder. We present a new optimization-via-compilation strategy to scalably solve a certain class of such problems. In particular, we introduce a new intermediate representation (IR), binary decision diagrams weighted by a novel notion of branch-and-bound semiring, that enables a scalable branch-and-bound based optimization procedure. This IR automatically factorizes problems through program structure and prunes suboptimal values via a straightforward branch-and-bound style algorithm to find optima. Additionally, the IR is naturally amenable to staged compilation, allowing the programmer to query for optima mid-compilation to inform further executions of the program. We showcase the effectiveness and flexibility of the IR by implementing two performant languages that both compile to it: dappl and pineappl. dappl is a functional language that solves maximum expected utility problems with first-class support for rewards, decision making, and conditioning. pineappl is an imperative language that performs exact probabilistic inference with support for nested marginal maximum a posteriori (MMAP) optimization via staging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18728v2</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3720500</arxiv:DOI>
      <dc:creator>Minsung Cho, John Gouwar, Steven Holtzen</dc:creator>
    </item>
    <item>
      <title>Totality for Mixed Inductive and Coinductive Types</title>
      <link>https://arxiv.org/abs/1901.07820</link>
      <description>arXiv:1901.07820v5 Announce Type: replace-cross 
Abstract: This paper introduces an ML / Haskell like programming language with nested inductive and coinductive algebraic datatypes called \chariot. Functions are defined by arbitrary recursive definitions and can thus lead to non-termination and other ``bad'' behavior. \chariot comes with a totality checker that tags possibly ill-behaved definitions. Such a totality checker is mandatory in the context of proof assistants based on type theory like Agda. Proving correctness of this checker is far from trivial and relies on - an interpretation of types as parity games, - an interpretation of correct values as winning strategies for those games, - the Lee, Jones and Ben Amram's size-change principle, used to check that the strategies induced by recursive definitions are winning. This paper develops the first two points, the last step being the subject of an upcoming paper. A prototype has been implemented and can be used to experiment with the resulting totality checker, giving a practical argument in favor of this principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:1901.07820v5</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Hyvernat (LAMA)</dc:creator>
    </item>
    <item>
      <title>COGNAC: Circuit Optimization via Gradients and Noise-Aware Compilation</title>
      <link>https://arxiv.org/abs/2311.02769</link>
      <description>arXiv:2311.02769v3 Announce Type: replace-cross 
Abstract: We present COGNAC, a novel strategy for compiling quantum circuits based on numerical optimization algorithms from scientific computing. Observing that shorter-duration "partially entangling" gates tend to be less noisy than the typical "maximally entangling" gates, we use a simple and versatile noise model to construct a differentiable cost function. Standard gradient-based optimization algorithms running on a GPU can then quickly converge to a local optimum that closely approximates the target unitary. By reducing rotation angles to zero, COGNAC removes gates from a circuit, producing smaller quantum circuits. We have implemented this technique as a general-purpose Qiskit compiler plugin and compared performance with state-of-the-art optimizers on a variety of standard benchmarks. Testing our compiled circuits on superconducting quantum hardware, we find that COGNAC's optimizations produce circuits that are substantially less noisy than those produced by existing optimizers. These runtime performance gains come without a major compile-time cost, as COGNAC's parallelism allows it to retain a competitive optimization speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02769v3</guid>
      <category>quant-ph</category>
      <category>cs.PL</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Finn Voichick, Leonidas Lampropoulos, Robert Rand</dc:creator>
    </item>
  </channel>
</rss>
