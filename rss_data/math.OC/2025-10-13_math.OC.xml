<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Oct 2025 04:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Information Asymmetry in Queues with Strategic Customers</title>
      <link>https://arxiv.org/abs/2510.09899</link>
      <description>arXiv:2510.09899v1 Announce Type: new 
Abstract: This paper studies information asymmetry in an unobservable single-server queueing system. While system managers have knowledge of the true arrival rate, customers may lack this information and instead form arbitrary beliefs. We propose a three-tier hierarchy of information asymmetry with increasing levels of information disclosure:customers keep private beliefs, customers are aware of the beliefs of others, and customers know the true arrival rate. Within this framework, the effects of the belief distribution, which is assumed to be general with minimal restrictions, are analyzed in terms of equilibrium joining probabilities, revenue, and social welfare. Furthermore,strategies for information disclosure are proposed for system managers to regulate the queue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09899v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunan Zheng, John Hasenbein</dc:creator>
    </item>
    <item>
      <title>Characterizing nonconvex boundaries via scalarization</title>
      <link>https://arxiv.org/abs/2510.09918</link>
      <description>arXiv:2510.09918v1 Announce Type: new 
Abstract: We present a unified approach for characterizing the boundary of a possibly nonconvex domain. Motivated by the well-known Pascoletti--Serafini method of scalarization, we recast the boundary characterization as a multi-criteria optimization problem with respect to a local partial order induced by a spherical cone with varying orient. Such an approach enables us to trace the whole boundary and can be considered a general dual representation for arbitrary (nonconvex) sets satisfying an exterior cone condition. We prove the equivalence between the geometrical boundary and the scalarization-implied boundary, particularly in the case of Euclidean spaces and two infinite-dimensional spaces for practical interest. By reformulating each scalarized problem as a parameterized constrained optimization problem, we shall develop a corresponding numerical scheme for the proposed approach. Some related applications are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09918v1</guid>
      <category>math.OC</category>
      <category>math.MG</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Ma, Weixuan Xia, Jianfeng Zhang</dc:creator>
    </item>
    <item>
      <title>Well-posedness of McKean-Vlasov generalized multivalued BSDEs</title>
      <link>https://arxiv.org/abs/2510.10026</link>
      <description>arXiv:2510.10026v1 Announce Type: new 
Abstract: This paper investigates McKean-Vlasov backward stochastic variational inequalities (BSVIs) whose generator depends on the joint law of the solution. We first establish the existence and uniqueness of the solution under globally Lipschitz and linear growth conditions. The analysis is then extended to the more general case of locally Lipschitz and non-linear growth conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10026v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Liu</dc:creator>
    </item>
    <item>
      <title>Quantum Alternating Direction Method of Multipliers for Semidefinite Programming</title>
      <link>https://arxiv.org/abs/2510.10056</link>
      <description>arXiv:2510.10056v1 Announce Type: new 
Abstract: Semidefinite programming (SDP) is a fundamental convex optimization problem with wide-ranging applications. However, solving large-scale instances remains computationally challenging due to the high cost of solving linear systems and performing eigenvalue decompositions. In this paper, we present a quantum alternating direction method of multipliers (QADMM) for SDPs, building on recent advances in quantum computing. An inexact ADMM framework is developed, which tolerates errors in the iterates arising from block-encoding approximation and quantum measurement. Within this robust scheme, we design a polynomial proximal operator to address the semidefinite conic constraints and apply the quantum singular value transformation to accelerate the most costly projection updates. We prove that the scheme converges to an $\epsilon$-optimal solution of the SDP problem under the strong duality assumption. A detailed complexity analysis shows that the QADMM algorithm achieves favorable scaling with respect to dimension compared to the classical ADMM algorithm and quantum interior point methods, highlighting its potential for solving large-scale SDPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10056v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hantao Nie, Dong An, Zaiwen Wen</dc:creator>
    </item>
    <item>
      <title>Mathematical Modeling of Networks in Strength of Materials</title>
      <link>https://arxiv.org/abs/2510.10132</link>
      <description>arXiv:2510.10132v1 Announce Type: new 
Abstract: We study a material modeled as a network of nodes connected by edges. Using a discrete approach, we build a nonlinear algebraic system that connects applied forces to internal forces and node positions. The model can describe elasticity, plasticity, and possibly cracking. The goal is to solve this system and understand how the material responds. Students are asked to start with a simple triangle example and then apply the method to larger structures. The final aim is to solve the full system and justify the results, leading to a possible publication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10132v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ioannis Dassios</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Control with End-to-End Statistically Guaranteed Metric Learning</title>
      <link>https://arxiv.org/abs/2510.10214</link>
      <description>arXiv:2510.10214v1 Announce Type: new 
Abstract: Wasserstein distributionally robust control (DRC) recently emerges as a principled paradigm for handling uncertainty in stochastic dynamical systems. However, it constructs data-driven ambiguity sets via uniform distribution shifts before sequentially incorporating them into downstream control synthesis. This segregation between ambiguity set construction and control objectives inherently introduces a structural misalignment, which undesirably leads to conservative control policies with sub-optimal performance. To address this limitation, we propose a novel end-to-end finite-horizon Wasserstein DRC framework that integrates the learning of anisotropic Wasserstein metrics with downstream control tasks in a closed-loop manner, thus enabling ambiguity sets to be systematically adjusted along performance-critical directions and yielding more effective control policies. This framework is formulated as a bilevel program: the inner level characterizes dynamical system evolution under DRC, while the outer level refines the anisotropic metric leveraging control-performance feedback across a range of initial conditions. To solve this program efficiently, we develop a stochastic augmented Lagrangian algorithm tailored to the bilevel structure. Theoretically, we prove that the learned ambiguity sets preserve statistical finite-sample guarantees under a novel radius adjustment mechanism, and we establish the well-posedness of the bilevel formulation by demonstrating its continuity with respect to the learnable metric. Furthermore, we show that the algorithm converges to stationary points of the outer level problem, which are statistically consistent with the optimal metric at a non-asymptotic convergence rate. Experiments on both numerical and inventory control tasks verify that the proposed framework achieves superior closed-loop performance and robustness compared against state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10214v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyi Wu, Chao Ning, Yang Shi</dc:creator>
    </item>
    <item>
      <title>Average Kernel Sizes -- Computable Sharp Accuracy Bounds for Inverse Problems</title>
      <link>https://arxiv.org/abs/2510.10229</link>
      <description>arXiv:2510.10229v1 Announce Type: new 
Abstract: The reconstruction of an unknown quantity from noisy measurements is a mathematical problem relevant in most applied sciences, for example, in medical imaging, radar inverse scattering, or astronomy. This underlying mathematical problem is often an ill-posed (non-linear) reconstruction problem, referred to as an ill-posed inverse problem. To tackle such problems, there exist a myriad of methods to design approximate inverse maps, ranging from optimization-based approaches, such as compressed sensing, over Bayesian approaches, to data-driven techniques such as deep learning. For all stable approximate inverse maps, there are accuracy limits that are strictly larger than zero for ill-posed inverse problems, due to the accuracy-stability tradeoff [Gottschling et al., SIAM Review, 67.1 (2025)] and [Colbrook et al., Proceedings of the National Academy of Sciences, 119.12 (2022)]. The variety of methods that aim to solve such problems begs for a unifying approach to help scientists choose the approximate inverse map that obtains this theoretical optimum. Up to now there do not exist computable accuracy bounds to this optimum that are applicable to all inverse problems. We provide computable sharp accuracy bounds to the reconstruction error of solution methods to inverse problems. The bounds are method-independent and purely depend on the dataset of signals, the forward model of the inverse problem, and the noise model. To facilitate the use in scientific applications, we provide an algorithmic framework and an accompanying software library to compute these accuracy bounds. We demonstrate the validity of the algorithms on two inverse problems from different domains: fluorescence localization microscopy and super-resolution of multi-spectral satellite data. Computing the accuracy bounds for a problem before solving it, enables a fundamental shift towards optimizing datasets and forward models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10229v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nina M. Gottschling, David Iagaru, Jakob Gawlikowski, Ioannis Sgouralis</dc:creator>
    </item>
    <item>
      <title>Robust Exploratory Stopping under Ambiguity in Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2510.10260</link>
      <description>arXiv:2510.10260v1 Announce Type: new 
Abstract: We propose and analyze a continuous-time robust reinforcement learning framework for optimal stopping problems under ambiguity. In this framework, an agent chooses a stopping rule motivated by two objectives: robust decision-making under ambiguity and learning about the unknown environment. Here, ambiguity refers to considering multiple probability measures dominated by a reference measure, reflecting the agent's awareness that the reference measure representing her learned belief about the environment would be erroneous. Using the $g$-expectation framework, we reformulate an optimal stopping problem under ambiguity as an entropy-regularized optimal control problem under ambiguity, with Bernoulli distributed controls to incorporate exploration into the stopping rules. We then derive the optimal Bernoulli distributed control characterized by backward stochastic differential equations. Moreover, we establish a policy iteration theorem and implement it as a reinforcement learning algorithm. Numerical experiments demonstrate the convergence and robustness of the proposed algorithm across different levels of ambiguity and exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10260v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junyan Ye, Hoi Ying Wong, Kyunghyun Park</dc:creator>
    </item>
    <item>
      <title>Grid Restoration Under Uncertainty Considering Coupled Transportation-Power Networks</title>
      <link>https://arxiv.org/abs/2510.10399</link>
      <description>arXiv:2510.10399v1 Announce Type: new 
Abstract: A stochastic mixed-integer programming model is developed to address the power distribution system repair and restoration following failures caused by extreme events such as natural disasters. This model addresses the complex challenge of efficiently assigning and dispatching repair crews to minimize both downtime and the extent of outages, following the crew sequence constraints. By incorporating a realistic transportation network, our model accounts for uncertainties in repair times, repair demand at damaged nodes, and potential transportation network failures. This ensures uncertainty awareness and mitigation, providing coordinated schedules for the repair crews, assigned based on the severity of failures at each node. Our approach prioritizes components in the distribution network based on their potential for power restoration, while also considering the constraints imposed by the actual transportation network. A case study using an 8500-bus system, coupled with a real-world transportation network at the Dallas-Fort Worth area, demonstrating the effectiveness of this approach, is discussed. The proposed methodology emphasizes rapid power restoration and efficient repair crew routing over an integrated transportation and power network, demonstrating results that match or outperform conventional metaheuristic solvers for classical vehicle routing problems in a deterministic instance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10399v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harshal D. Kaushik, Roshni Anna Jacob, Souma Chowdhury, Jie Zhang</dc:creator>
    </item>
    <item>
      <title>Linear Algebra Problems Solved by Using Damped Dynamical Systems on the Stiefel Manifold</title>
      <link>https://arxiv.org/abs/2510.10535</link>
      <description>arXiv:2510.10535v1 Announce Type: new 
Abstract: We develop a new method for solving minimization problems on the Stiefel Manifold using damped dynamical systems. The constraints are satisfied in the limit by an additional damped dynamical system. The method is illustrated by numerical experiments and compared to a state-of-the-art conjugate gradient method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10535v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M Gulliksson, A Oleynik, M Ogren, R Bakhshandeh-Chamazkoti</dc:creator>
    </item>
    <item>
      <title>Optimal transport paths with capacity induced cost function</title>
      <link>https://arxiv.org/abs/2510.10557</link>
      <description>arXiv:2510.10557v1 Announce Type: new 
Abstract: This article generalizes the study of ramified optimal transport with capacity constraint in transport multi-paths by generalizing the $\mathbf{M}_{\alpha}$ cost to $\mathbf{M}_{\alpha,c}$, which incorporates capacity constraints into the cost function. Equipped with $\mathbf{M}_{\alpha,c}$ cost, we prove the existence of optimal transport path, $\mathbf{M}_{\alpha,c}$ related inequalities, decomposition of any general transport paths, and occurrence of direct line segments in an optimal transport path.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10557v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qinglan Xia, Haotian Sun</dc:creator>
    </item>
    <item>
      <title>Hybrid Ridgelet Deep Neural Networks for Data-Driven Arbitrage Strategies</title>
      <link>https://arxiv.org/abs/2510.10599</link>
      <description>arXiv:2510.10599v1 Announce Type: new 
Abstract: In this study, we propose a novel model framework that integrates deep neural networks with the Ridgelet Transform. The Ridgelet Transform on Borel measurable functions is used for arbitrage detection on high-dimensional sparse structures. This transform also enhances the expressive power of neural networks, enabling them to capture complex and high-dimensional market structures. Theoretically, we determine profitable trading strategies by optimizing hybrid ridgelet deep neural networks. Further, we emphasize the role of activation functions in ensuring stability and adaptability under uncertainty. We use a high-performance computing cluster for the detection of arbitrage across multiple assets, ensuring scalability, and processing large-scale financial data. Empirical results demonstrate strong profitability across diverse scenarios involving up to 50 assets, with particularly robust performance during periods of market volatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10599v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bahadur Yadav, Sanjay Kumar Mohanty</dc:creator>
    </item>
    <item>
      <title>Homogenization-based optimization of wall thickness distribution for TPMS two-fluid heat exchangers</title>
      <link>https://arxiv.org/abs/2510.10622</link>
      <description>arXiv:2510.10622v1 Announce Type: new 
Abstract: Triply Periodic Minimal Surface (TPMS) structures are attracting growing attention as promising geometries for next-generation high-performance heat exchangers (HXs), due to their continuous flow paths and high surface-area-to-volume ratio that enhance heat transfer performance. Among these, graded TPMS structures with spatially varying thickness have emerged as a potential means to further improve performance. This study proposes an optimization method of wall thickness distribution for TPMS HXs based on an effective porous media model, which allows accurate performance prediction while significantly reducing computational cost. The proposed method is applied to a gyroid two-fluid HX aiming to improve the thermal-hydraulic performance. Furthermore, full-scale numerical simulations of the optimized-thickness design show a 12.2% improvement in the performance evaluation criterion (PEC) compared to the uniform-thickness design. The improvement is primarily attributed to the optimized non-uniform wall thickness, which directs more flow toward the core ends and enhances velocity uniformity. As a result, heat transfer is enhanced at the core ends, leading to more effective use of the entire HX core and improved overall thermal performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10622v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaito Ohtani, Hiroki Kawabe, Kentaro Yaji, Kikuo Fujita, Vikrant Aute</dc:creator>
    </item>
    <item>
      <title>Second-order Optimization under Heavy-Tailed Noise: Hessian Clipping and Sample Complexity Limits</title>
      <link>https://arxiv.org/abs/2510.10690</link>
      <description>arXiv:2510.10690v1 Announce Type: new 
Abstract: Heavy-tailed noise is pervasive in modern machine learning applications, arising from data heterogeneity, outliers, and non-stationary stochastic environments. While second-order methods can significantly accelerate convergence in light-tailed or bounded-noise settings, such algorithms are often brittle and lack guarantees under heavy-tailed noise -- precisely the regimes where robustness is most critical. In this work, we take a first step toward a theoretical understanding of second-order optimization under heavy-tailed noise. We consider a setting where stochastic gradients and Hessians have only bounded $p$-th moments, for some $p\in (1,2]$, and establish tight lower bounds on the sample complexity of any second-order method. We then develop a variant of normalized stochastic gradient descent that leverages second-order information and provably matches these lower bounds. To address the instability caused by large deviations, we introduce a novel algorithm based on gradient and Hessian clipping, and prove high-probability upper bounds that nearly match the fundamental limits. Our results provide the first comprehensive sample complexity characterization for second-order optimization under heavy-tailed noise. This positions Hessian clipping as a robust and theoretically sound strategy for second-order algorithm design in heavy-tailed regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10690v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdurakhmon Sadiev, Peter Richt\'arik, Ilyas Fatkhullin</dc:creator>
    </item>
    <item>
      <title>Mean-square and linear convergence of a stochastic proximal point algorithm in metric spaces of nonpositive curvature</title>
      <link>https://arxiv.org/abs/2510.10697</link>
      <description>arXiv:2510.10697v1 Announce Type: new 
Abstract: We define a stochastic variant of the proximal point algorithm in the general setting of nonlinear (separable) Hadamard spaces for approximating zeros of the mean of a stochastically perturbed monotone vector field and prove its convergence under a suitable strong monotonicity assumption, together with a probabilistic independence assumption and a separability assumption on the tangent spaces. As a particular case, our results transfer previous work by P. Bianchi on that method in Hilbert spaces for the first time to Hadamard manifolds. Moreover, our convergence proof is fully effective and allows for the construction of explicit rates of convergence for the iteration towards the (unique) solution both in mean and almost surely. These rates are moreover highly uniform, being independent of most data surrounding the iteration, space or distribution. In that generality, these rates are novel already in the context of Hilbert spaces. Linear nonasymptotic guarantees under additional second-moment conditions on the Yosida approximates and special cases of stochastic convex minimization are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10697v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas Pischke</dc:creator>
    </item>
    <item>
      <title>Dynamic Line Ratings in AC Optimal Power Flow: Transient Temperature, Decomposition, and Large-scale Evaluation</title>
      <link>https://arxiv.org/abs/2510.10832</link>
      <description>arXiv:2510.10832v1 Announce Type: new 
Abstract: As power grids experience increasing renewable penetration and rapid load growth from AI data centers and electrification, alleviating line congestion becomes critical to unlocking additional grid capacity. This work investigates Dynamic Line Rating (DLR), a congestion mitigation method that adjusts power line current limits in response to meteorological conditions. Unlike traditional approaches that impose predefined time-varying limits, we propose a novel optimization framework that embeds the transient-state heat equation governing conductor temperature dynamics, enabling direct constraints on conductor temperature rather than simplified steady-state approximations. We derive a closed-form solution to the heat equation, enabling a finite-dimensional reformulation of the dynamics. We then leverage a distributed decomposition method, a bi-level Alternating Direction Method of Multipliers (ADMM) algorithm with provable convergence, aided by regularity properties of the heat equation solution. These modeling and algorithmic innovations allow us to conduct the first large-scale evaluation of DLR using multi-period AC optimal power flow. Numerical experiments on the 2000-bus Texas grid demonstrate that DLR allows significant reduction in generation cost in congested systems over Static Line Rating (SLR) and Ambient Adjusted Ratings (AAR). The transient temperature formulation provides additional grid flexibility and headroom benefits with minimal computational overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10832v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Baptiste Rabecq, Thomas Lee, Andy Sun</dc:creator>
    </item>
    <item>
      <title>Storage Participation in Electricity Markets: Arbitrage and Ancillary Services</title>
      <link>https://arxiv.org/abs/2510.10856</link>
      <description>arXiv:2510.10856v1 Announce Type: new 
Abstract: Electricity storage is used for intertemporal price arbitrage and for ancillary services that balance unforeseen supply and demand fluctuations via frequency regulation. We present an optimization model that computes bids for both arbitrage and frequency regulation and ensures that storage operators can honor their market commitments at all times for all fluctuation signals in an uncertainty set inspired by market rules. This requirement, initially expressed by an infinite number of nonconvex functional constraints, is shown to be equivalent to a finite number of deterministic constraints. The resulting formulation is a mixed-integer bilinear program that admits mixed-integer linear relaxations and restrictions. Empirical tests on European electricity markets show a negligible optimality gap between the relaxation and the restriction. The model can account for intraday trading and, with a solution time of under 5 seconds, may serve as a building block for more complex trading strategies. Such strategies become necessary as battery capacity exceeds the demand for ancillary services. In a backtest from 1 July 2020 through 30 June 2024 joint market participation more than doubles profits and almost halves energy storage output compared to arbitrage alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10856v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dirk Lauinger, Luc Cot\'e, Andy Sun</dc:creator>
    </item>
    <item>
      <title>Martingale Optimal Transport and Martingale Schr\"odinger Bridges for Calibration of Stochastic Volatility Models</title>
      <link>https://arxiv.org/abs/2510.10860</link>
      <description>arXiv:2510.10860v1 Announce Type: new 
Abstract: Motivated by recent developments in the calibration of stochastic volatility models (SVMs for short), we study continuous-time formulations of martingale optimal transport and martingale Schr\"odinger bridge problems. We establish duality formulas and also provide alternative proofs, via different techniques, of duality results previously established in the mathematical finance literature. Applications include calibration of SVMs to SPX options, as well as joint calibration to both SPX and VIX options.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10860v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonios Zitridis</dc:creator>
    </item>
    <item>
      <title>Successive Fixing for Large-Scale SCUC Using First-Order Methods</title>
      <link>https://arxiv.org/abs/2510.10891</link>
      <description>arXiv:2510.10891v1 Announce Type: new 
Abstract: Security-Constrained Unit Commitment is a fundamental optimization problem in power systems operations. The primary computational bottleneck arises from the need to solve large-scale Linear Programming (LP) relaxations within branch-and-cut. Conventional simplex and barrier methods become computationally prohibitive at this scale due to their reliance on expensive matrix factorizations. While matrix-free first-order methods present a promising alternative, their tendency to converge to non-vertex solutions renders them incompatible with standard branch-and-cut procedures. To bridge this gap, we propose a successive fixing framework that leverages a customized GPU-accelerated first-order LP solver to guide a logic-driven variable-fixing strategy. Each iteration produces a reduced Mixed-Integer Linear Programming (MILP) problem, which is subsequently tightened via presolving. This iterative cycle of relaxation, fixing, and presolving progressively reduces problem complexity, producing a highly tractable final MILP model. When evaluated on public benchmarks exceeding 13,000 buses, our approach achieves a tenfold speedup over state-of-the-art methods without compromising solution quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10891v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinxin Xiong, Yanting Huang, Yingxiao Wang, Linxin Yang, Jianghua Wu, Shunbo Lei, Akang Wang</dc:creator>
    </item>
    <item>
      <title>A Unified Zeroth-Order Optimization Framework via Oblivious Randomized Sketching</title>
      <link>https://arxiv.org/abs/2510.10945</link>
      <description>arXiv:2510.10945v1 Announce Type: new 
Abstract: We propose a new framework for analyzing zeroth-order optimization (ZOO) from the perspective of \emph{oblivious randomized sketching}.In this framework, commonly used gradient estimators in ZOO-such as finite difference (FD) and random finite difference (RFD)-are unified through a general sketch-based formulation. By introducing the concept of oblivious randomized sketching, we show that properly chosen sketch matrices can significantly reduce the high variance of RFD estimates and enable \emph{high-probability} convergence guarantees of ZOO, which are rarely available in existing RFD analyses.
  \noindent We instantiate the framework on convex quadratic objectives and derive a query complexity of $\tilde{\mathcal{O}}(\mathrm{tr}(A)/L \cdot L/\mu\log\frac{1}{\epsilon})$ to achieve a $\epsilon$-suboptimal solution, where $A$ is the Hessian, $L$ is the largest eigenvalue of $A$, and $\mu$ denotes the strong convexity parameter. This complexity can be substantially smaller than the standard query complexity of ${\cO}(d\cdot L/\mu \log\frac{1}{\epsilon})$ that is linearly dependent on problem dimensionality, especially when $A$ has rapidly decaying eigenvalues. These advantages naturally extend to more general settings, including strongly convex and Hessian-aware optimization.
  \noindent Overall, this work offers a novel sketch-based perspective on ZOO that explains why and when RFD-type methods can achieve \emph{weakly dimension-independent} convergence in general smooth problems, providing both theoretical foundations and practical implications for ZOO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10945v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haishan Ye, Xiangyu Chang, Xi Chen</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Optimization for Chemotherapy Scheduling under Asymmetric and Multi-Modal Uncertainty</title>
      <link>https://arxiv.org/abs/2510.10953</link>
      <description>arXiv:2510.10953v1 Announce Type: new 
Abstract: We consider a real-world chemotherapy scheduling template design problem, where we cluster patient types into groups and find a representative time-slot duration for each group to accommodate all patient types assigned to that group, aiming to minimize the total expected idle time and overtime. From Mayo Clinic's real data, most patients' treatment durations are asymmetric (e.g., shorter/longer durations tend to have a longer right/left tail). Motivated by this observation, we consider a distributionally robust optimization (DRO) model under an asymmetric and multi-modal ambiguity set, where the distribution of the random treatment duration is modeled as a mixture of distributions from different patient types. The ambiguity set captures uncertainty in both the mode probabilities, modeled via a variation-distance-based set, and the distributions within each mode, characterized by moment information such as the empirical mean, variance, and semivariance. We reformulate the DRO model as a semi-infinite program, which cannot be solved by off-the-shelf solvers. To overcome this, we derive a closed-form expression for the worst-case expected cost and establish lower and upper bounds that are positively related to the variability of patient types assigned to each group, based on which we develop exact algorithms and highly efficient clustering-based heuristics. The lower and upper bounds on the worst-case cost imply that the optimal cost tends to decrease if we group patient types with similar treatment times. Through numerical experiments based on both synthetic datasets and Mayo Clinic's real data, we illustrate the effectiveness and efficiency of the proposed exact algorithms and heuristics and showcase the benefits of incorporating asymmetric information into the DRO formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10953v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qing Zhu, Xian Yu, Yu-Li Huang</dc:creator>
    </item>
    <item>
      <title>Geoffrion's theorem beyond finiteness and rationality</title>
      <link>https://arxiv.org/abs/2510.10966</link>
      <description>arXiv:2510.10966v1 Announce Type: new 
Abstract: Geoffrion's theorem is a fundamental result from mathematical programming assessing the quality of Lagrangian relaxation, a standard technique to get bounds for integer programs. An often implicit condition is that the set of feasible solutions is finite or described by rational linear constraints. However, we show through concrete examples that the conclusion of Geoffrion's theorem does not necessarily hold when this condition is dropped. We then provide sufficient conditions ensuring the validity of the result even when the feasible set is not finite and cannot be described using finitely-many linear constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10966v1</guid>
      <category>math.OC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santanu S. Dey, Fr\'ed\'eric Meunier, Diego Moran Ramirez</dc:creator>
    </item>
    <item>
      <title>Increasing Value of Information Implies Separable Utility</title>
      <link>https://arxiv.org/abs/2510.11102</link>
      <description>arXiv:2510.11102v1 Announce Type: new 
Abstract: We consider decision-making under incomplete information about an unknown state of nature. Utility acts (that is, utility vectors indexed by states of nature) and beliefs (probability distributions over the states of nature) are naturally paired by bilinear duality, giving the expected utility. With this pairing, an expected utility maximizer (DM) is characterized by a continuous closed convex comprehensive set of utility acts (c-utility act set). We show that DM M values information more than DM L if and only if the c-utility act set of DM M is obtained by Minkowski addition from the cutility act set of DM L. In the classic setting of decision theory, this is interpreted as the equivalence between more valuable information, on the one hand, and multiplying decisions and adding utility, on the other hand (additively separable utility). We also introduce the algebraic structure of dioid to describe two operations between DMs: union (adding options) and fusion (multiplying options and adding utilities). We say that DM M is more exible by union (resp. by fusion) than DM L if DM M is obtained by union (resp. by fusion) from DM L. Our main result is that DM M values information more than DM L if and only if DM M is more exible by fusion than DM L.  We also study when exibility by union can lead to more valuable information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11102v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michel de Lara (CERMICS)</dc:creator>
    </item>
    <item>
      <title>Optimal Policy Characterization for a Class of Multi-Dimensional Ergodic Singular Stochastic Control Problems</title>
      <link>https://arxiv.org/abs/2510.11158</link>
      <description>arXiv:2510.11158v1 Announce Type: new 
Abstract: In ergodic singular stochastic control problems, a decision-maker can instantaneously adjust the evolution of a state variable using a control of bounded variation, with the goal of minimizing a long-term average cost functional. The cost of control is proportional to the magnitude of adjustments. This paper characterizes the optimal policy and the value in a class of multi-dimensional ergodic singular stochastic control problems. These problems involve a linearly controlled one-dimensional stochastic differential equation, whose coefficients, along with the cost functional to be optimized, depend on a multi-dimensional uncontrolled process Y. We first provide general verification theorems providing an optimal control in terms of a Skorokhod reflection at Y-dependent free boundaries, which emerge from the analysis of an auxiliary Dynkin game. We then fully solve two two-dimensional optimal inventory management problems. To the best of our knowledge, this is the first paper to establish a connection between multi-dimensional ergodic singular stochastic control and optimal stopping, and to exploit this connection to achieve a complete solution in a genuinely two-dimensional setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11158v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro Calvia, Federico Cannerozzi, Giorgio Ferrari</dc:creator>
    </item>
    <item>
      <title>Nonlinearly Preconditioned Gradient Methods: Momentum and Stochastic Analysis</title>
      <link>https://arxiv.org/abs/2510.11312</link>
      <description>arXiv:2510.11312v1 Announce Type: new 
Abstract: We study nonlinearly preconditioned gradient methods for smooth nonconvex optimization problems, focusing on sigmoid preconditioners that inherently perform a form of gradient clipping akin to the widely used gradient clipping technique. Building upon this idea, we introduce a novel heavy ball-type algorithm and provide convergence guarantees under a generalized smoothness condition that is less restrictive than traditional Lipschitz smoothness, thus covering a broader class of functions. Additionally, we develop a stochastic variant of the base method and study its convergence properties under different noise assumptions. We compare the proposed algorithms with baseline methods on diverse tasks from machine learning including neural network training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11312v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Konstantinos Oikonomidis, Jan Quan, Panagiotis Patrinos</dc:creator>
    </item>
    <item>
      <title>A model reduction method based on nonlinear optimization for multiscale stochastic optimal control problems</title>
      <link>https://arxiv.org/abs/2510.11325</link>
      <description>arXiv:2510.11325v1 Announce Type: new 
Abstract: This paper presents a nonlinear optimization-based model reduction method for multiscale stochastic optimal control problems governed by stochastic partial differential equations. The proposed approach constructs a non-intrusive, data-driven reduced-order model by employing a parameter-separable structure to handle stochastic dependencies and directly minimizing the L2 norm of the output error via gradient-based optimization. Compared to existing methods, this framework offers three significant advantages: it is entirely data-driven, relying solely on output measurements without requiring access to internal system matrices; it guarantees approximation accuracy for control outputs, aligning directly with the optimization objective; and its computational complexity is independent of the original PDE dimension, ensuring feasibility for real-time control applications. Numerical experiments on stochastic diffusion and advection-diffusion equations demonstrate the method's effectiveness and efficiency, providing a systematic solution for the real-time control of complex uncertain systems and bridging the gap between model reduction theory and practical engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11325v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyi Zhang</dc:creator>
    </item>
    <item>
      <title>Exponential convergence of multiagent systems with lack of connection</title>
      <link>https://arxiv.org/abs/2510.11334</link>
      <description>arXiv:2510.11334v1 Announce Type: new 
Abstract: Finding conditions ensuring consensus, i.e. convergence to a common value, for a networked system is of crucial interest, both for theoretical reasons and applications. This goal is harder to achieve when connections between agents are temporarily lost.
  Here, we prove that known conditions (introduced by Moreau) ensure an exponential convergence to consensus, with explicit rate of convergence. The key result is related to the length of the graph (i.e. the number of connections to reach a common agent): if this is large, then convergence is slow.
  This general result also provides conditions for convergence of second-order cooperative systems with lack of connections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11334v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabio Ancona, Mohamed Bentaibi, Francesco Rossi</dc:creator>
    </item>
    <item>
      <title>Optimal Control of a Bioeconomic Crop-Energy System with Energy Reinvestment</title>
      <link>https://arxiv.org/abs/2510.11381</link>
      <description>arXiv:2510.11381v1 Announce Type: new 
Abstract: We develop an optimal control model for allocating agricultural crop residues between bioenergy production and soil fertility restoration. The system captures a novel circular feedback: a fraction of cumulative energy output is reinvested into soil productivity, linking energy use with ecological regeneration. The dynamics are governed by a nonlinear three-state system describing soil fertility, residue biomass, and accumulated energy, with a single control representing the proportion of biomass diverted to energy. The objective is to maximize a discounted net benefit that accounts for energy revenue, soil value, and operational costs. We apply the Pontryagin Maximum Principle in current-value form to derive necessary optimality conditions and characterize the structure of optimal controls. Numerical simulations based on direct optimization reveal interior and switching regimes, and show how planning horizon and reinvestment efficiency influence optimal strategies. The results highlight the strategic role of energy reinvestment in achieving sustainable residue management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11381v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Othman Cherkaoui Dekkaki</dc:creator>
    </item>
    <item>
      <title>Hamilton-Jacobi Reachability for Viability Analysis of Constrained Waste-to-Energy Systems under Adversarial Uncertainty</title>
      <link>https://arxiv.org/abs/2510.11396</link>
      <description>arXiv:2510.11396v1 Announce Type: new 
Abstract: This paper investigates the problem of maintaining the safe operation of Waste-to-Energy (WtE) systems under operational constraints and uncertain waste inflows. We model this as a robust viability problem, formulated as a zero-sum differential game between a control policy and an adversarial disturbance. Within a Hamilton-Jacobi framework, the viability kernel is characterized as the zero sublevel set of a value function satisfying a constrained Hamilton-Jacobi-Bellman (HJB) equation in the viscosity sense. This formulation provides formal guarantees for ensuring that system trajectories remain within prescribed operational limits under worst-case scenarios. Compared to existing viability studies, this work introduces a rigorous HJB-based characterization explicitly incorporating uncertainty, tailored to nonlinear WtE dynamics. A numerical scheme based on the Local Lax-Friedrichs method is employed to approximate the viability kernel. Numerical experiments illustrate how increasing inflow uncertainty significantly reduces the viability domain, shrinking the safe operating envelope. The proposed method is computationally tractable for systems of moderate dimension and offers a basis for synthesizing robust control policies, contributing to the design of resilient and sustainable WtE infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11396v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Achraf Bouhmady, Othman Cherkaoui Dekkaki</dc:creator>
    </item>
    <item>
      <title>Variational Analysis in Spectral Decomposition Systems</title>
      <link>https://arxiv.org/abs/2510.11433</link>
      <description>arXiv:2510.11433v1 Announce Type: new 
Abstract: This work is concerned with variational analysis of so-called spectral functions and spectral sets of matrices that only depend on eigenvalues of the matrix. Based on our previous work [H. T. B\`ui, M. N. B\`ui, and C. Clason, Convex analysis in spectral decomposition systems, arXiv 2503.14981] on convex analysis of such functions, we consider the question in the abstract framework of spectral decomposition systems, which covers a wide range of previously studied settings, including eigenvalue decomposition of Hermitian matrices and singular value decomposition of rectangular matrices, and allows deriving new results in more general settings such as normal decomposition systems and signed singular value decompositions. The main results characterize Fr\'echet and limiting normal cones to spectral sets as well as Fr\'echet, limiting, and Clarke subdifferentials of spectral functions in terms of the reduced functions. For the latter, we also characterize Fr\'echet differentiability. Finally, we obtain a generalization of Lidski\u{\i}'s theorem on the spectrum of additive perturbations of Hermitian matrices to arbitrary spectral decomposition systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11433v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H\`oa T. B\`ui, Minh N. B\`ui, Christian Clason</dc:creator>
    </item>
    <item>
      <title>Adaptive Conditional Gradient Descent</title>
      <link>https://arxiv.org/abs/2510.11440</link>
      <description>arXiv:2510.11440v1 Announce Type: new 
Abstract: Selecting an effective step-size is a fundamental challenge in first-order optimization, especially for problems with non-Euclidean geometries. This paper presents a novel adaptive step-size strategy for optimization algorithms that rely on linear minimization oracles, as used in the Conditional Gradient or non-Euclidean Normalized Steepest Descent algorithms. Using a simple heuristic to estimate a local Lipschitz constant for the gradient, we can determine step-sizes that guarantee sufficient decrease at each iteration. More precisely, we establish convergence guarantees for our proposed Adaptive Conditional Gradient Descent algorithm, which covers as special cases both the classical Conditional Gradient algorithm and non-Euclidean Normalized Steepest Descent algorithms with adaptive step-sizes. Our analysis covers optimization of continuously differentiable functions in non-convex, quasar-convex, and strongly convex settings, achieving convergence rates that match state-of-the-art theoretical bounds. Comprehensive numerical experiments validate our theoretical findings and illustrate the practical effectiveness of Adaptive Conditional Gradient Descent. The results exhibit competitive performance, underscoring the potential of the adaptive step-size for applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11440v1</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abbas Khademi, Antonio Silveti-Falls</dc:creator>
    </item>
    <item>
      <title>The Branch-and-Bound Tree Closure</title>
      <link>https://arxiv.org/abs/2510.11497</link>
      <description>arXiv:2510.11497v1 Announce Type: new 
Abstract: This paper investigates the a-posteriori analysis of Branch-and-Bound~(BB) trees to extract structural information about the feasible region of mixed-binary linear programs. We introduce three novel outer approximations of the feasible region, systematically constructed from a BB tree. These are: a tight formulation based on disjunctive programming, a branching-based formulation derived from the tree's branching logic, and a mixing-set formulation derived from the on-off properties inside the tree. We establish an inclusion hierarchy, which ranks the approximations by their theoretical strength \wrt to the original feasible region. The analysis is extended to the generation of valid inequalities, revealing a separation-time hierarchy that mirrors the inclusion hierarchy in reverse. This highlights a trade-off between the tightness of an approximation and the computational cost of generating cuts from it. Motivated by the computational expense of the stronger approximations, we introduce a new family of valid inequalities called star tree inequalities. Although their closure forms the weakest of the proposed approximations, their practical appeal lies in an efficient, polynomial-time combinatorial separation algorithm. A computational study on multi-dimensional knapsack and set-covering problems empirically validates the theoretical findings. Moreover, these experiments confirm that computationally useful valid inequalities can be generated from BB trees obtained by solving optimization problems considered in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11497v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Roland, Nagisa Sugishita, Alexandre Forel, Youssouf Emine, Ricardo Fukasawa, Thibaut Vidal</dc:creator>
    </item>
    <item>
      <title>Robust Least Squares Problems with Binary Uncertain Data</title>
      <link>https://arxiv.org/abs/2510.11519</link>
      <description>arXiv:2510.11519v1 Announce Type: new 
Abstract: We propose a Binary Robust Least Squares (BRLS) model that encompasses key robust least squares formulations, such as those involving uncertain binary labels and adversarial noise constrained within a hypercube. We show that the geometric structure of the noise propagation matrix, particularly whether its columns form acute or obtuse angles, implies the supermodularity or submodularity of the inner maximization problem. This structural property enables us to integrate powerful combinatorial optimization tools into a gradient-based minimax algorithmic framework. For the robust linear least squares problem with the supermodularity, we establish the relationship between the minimax points of BRLS and saddle points of its continuous relaxation, and propose a projected gradient algorithm computing $\epsilon$-global minimax points in $O(\epsilon^{-2})$ iterations. For the robust nonlinear least squares problem with supermodularity, we develop a revised framework that finds $\epsilon$-stationary points in the sense of expectation within $O(\epsilon^{-4})$ iterations. For the robust linear least squares problem with the submodularity, we employ a double greedy algorithm as a subsolver, guaranteeing a $(\frac{1}{3}, \epsilon)$-approximate minimax point in $O(\epsilon^{-2})$ iterations. Numerical experiments on health status prediction and phase retrieval demonstrate that BRLS achieves superior robustness against structured noise compared to classical least squares problems and LASSO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11519v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Zhou, Xiaojun Chen</dc:creator>
    </item>
    <item>
      <title>An Efficient Solution Method for Solving Convex Separable Quadratic Optimization Problems</title>
      <link>https://arxiv.org/abs/2510.11554</link>
      <description>arXiv:2510.11554v1 Announce Type: new 
Abstract: Convex separable quadratic optimization problems occur in many practical applications. In this paper, based on an iterative resolution scheme of the KKT system, we develop an efficient method for solving a quadratic programming problem with a convex separable objective function subject to multiple convex separable constraints. We show that the proposed approach leads to a dual coordinate ascent algorithm and provide a convergence proof. Numerical experiments support the superior performance of the proposed method to that of the Gurobi solver, especially for solving large-scale convex separate quadratic programming problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11554v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shaoze Li, Junhao Wu, Cheng Lu, Zhibin Deng, Shu-Cherng Fang</dc:creator>
    </item>
    <item>
      <title>Robust Online Sampling from Possibly Moving Target Distributions</title>
      <link>https://arxiv.org/abs/2510.11571</link>
      <description>arXiv:2510.11571v1 Announce Type: new 
Abstract: We suppose we are given a list of points $x_1, \dots, x_n \in \mathbb{R}$, a target probability measure $\mu$ and are asked to add additional points $x_{n+1}, \dots, x_{n+m}$ so that $x_1, \dots, x_{n+m}$ is as close as possible to the distribution of $\mu$; additionally, we want this to be true uniformly for all $m$. We propose a simple method that achieves this goal. It selects new points in regions where the existing set is lacking points and avoids regions that are already overly crowded. If we replace $\mu$ by another measure $\mu_2$ in the middle of the computation, the method dynamically adjusts and allows us to keep the original sampling points. $x_{n+1}$ can be computed in $\mathcal{O}(n)$ steps and we obtain state-of-the-art results. It appears to be an interesting dynamical system in its own right; we analyze a continuous mean-field version that reflects much of the same behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11571v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Cl\'ement, Stefan Steinerberger</dc:creator>
    </item>
    <item>
      <title>Accelerated stochastic first-order method for convex optimization under heavy-tailed noise</title>
      <link>https://arxiv.org/abs/2510.11676</link>
      <description>arXiv:2510.11676v1 Announce Type: new 
Abstract: We study convex composite optimization problems, where the objective function is given by the sum of a prox-friendly function and a convex function whose subgradients are estimated under heavy-tailed noise. Existing work often employs gradient clipping or normalization techniques in stochastic first-order methods to address heavy-tailed noise. In this paper, we demonstrate that a vanilla stochastic algorithm -- without additional modifications such as clipping or normalization -- can achieve optimal complexity for these problems. In particular, we establish that an accelerated stochastic proximal subgradient method achieves a first-order oracle complexity that is universally optimal for smooth, weakly smooth, and nonsmooth convex optimization, as well as for stochastic convex optimization under heavy-tailed noise. Numerical experiments are further provided to validate our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11676v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chuan He, Zhaosong Lu</dc:creator>
    </item>
    <item>
      <title>Functional Donoho-Elad-Gribonval-Nielsen-Fuchs Sparsity Theorem</title>
      <link>https://arxiv.org/abs/2510.09609</link>
      <description>arXiv:2510.09609v1 Announce Type: cross 
Abstract: Celebrated breakthrough sparsity theorem obtained independently by Donoho and Elad \textit{[Proc. Natl. Acad. Sci. USA, 2003]} and Gribonval and Nielsen \textit{[IEEE Trans. Inform. Theory, 2003]} and Fuchs \textit{[IEEE Trans. Inform. Theory, 2004]} says that unique sparse solution to NP-Hard $\ell_0$-minimization problem can be obtained using unique solution to P-Type $\ell_1$-minimization problem. In this paper, we extend their result to abstract Banach spaces using 1-approximate Schauder frames. We notice that the `normalized' condition for Hilbert spaces can be generalized to a larger extent when we consider Banach spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09609v1</guid>
      <category>math.FA</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>K. Mahesh Krishna</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Lifted Training and Inversion Approaches</title>
      <link>https://arxiv.org/abs/2510.09796</link>
      <description>arXiv:2510.09796v1 Announce Type: cross 
Abstract: The training of deep neural networks predominantly relies on a combination of gradient-based optimisation and back-propagation for the computation of the gradient. While incredibly successful, this approach faces challenges such as vanishing or exploding gradients, difficulties with non-smooth activations, and an inherently sequential structure that limits parallelisation. Lifted training methods offer an alternative by reformulating the nested optimisation problem into a higher-dimensional, constrained optimisation problem where the constraints are no longer enforced directly but penalised with penalty terms. This chapter introduces a unified framework that encapsulates various lifted training strategies, including the Method of Auxiliary Coordinates, Fenchel Lifted Networks, and Lifted Bregman Training, and demonstrates how diverse architectures, such as Multi-Layer Perceptrons, Residual Neural Networks, and Proximal Neural Networks fit within this structure. By leveraging tools from convex optimisation, particularly Bregman distances, the framework facilitates distributed optimisation, accommodates non-differentiable proximal activations, and can improve the conditioning of the training landscape. We discuss the implementation of these methods using block-coordinate descent strategies, including deterministic implementations enhanced by accelerated and adaptive optimisation techniques, as well as implicit stochastic gradient methods. Furthermore, we explore the application of this framework to inverse problems, detailing methodologies for both the training of specialised networks (e.g., unrolled architectures) and the stable inversion of pre-trained networks. Numerical results on standard imaging tasks validate the effectiveness and stability of the lifted Bregman approach compared to conventional training, particularly for architectures employing proximal activations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09796v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Wang, Alexandra Valavanis, Azhir Mahmood, Andreas Mang, Martin Benning, Audrey Repetti</dc:creator>
    </item>
    <item>
      <title>A mathematical theory for understanding when abstract representations emerge in neural networks</title>
      <link>https://arxiv.org/abs/2510.09816</link>
      <description>arXiv:2510.09816v1 Announce Type: cross 
Abstract: Recent experiments reveal that task-relevant variables are often encoded in approximately orthogonal subspaces of the neural activity space. These disentangled low-dimensional representations are observed in multiple brain areas and across different species, and are typically the result of a process of abstraction that supports simple forms of out-of-distribution generalization. The mechanisms by which such geometries emerge remain poorly understood, and the mechanisms that have been investigated are typically unsupervised (e.g., based on variational auto-encoders). Here, we show mathematically that abstract representations of latent variables are guaranteed to appear in the last hidden layer of feedforward nonlinear networks when they are trained on tasks that depend directly on these latent variables. These abstract representations reflect the structure of the desired outputs or the semantics of the input stimuli. To investigate the neural representations that emerge in these networks, we develop an analytical framework that maps the optimization over the network weights into a mean-field problem over the distribution of neural preactivations. Applying this framework to a finite-width ReLU network, we find that its hidden layer exhibits an abstract representation at all global minima of the task objective. We further extend these analyses to two broad families of activation functions and deep feedforward architectures, demonstrating that abstract representations naturally arise in all these scenarios. Together, these results provide an explanation for the widely observed abstract representations in both the brain and artificial neural networks, as well as a mathematically tractable toolkit for understanding the emergence of different kinds of representations in task-optimized, feature-learning network models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09816v1</guid>
      <category>q-bio.NC</category>
      <category>math.OC</category>
      <category>physics.bio-ph</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bin Wang, W. Jeffrey Johnston, Stefano Fusi</dc:creator>
    </item>
    <item>
      <title>Stability of Transformers under Layer Normalization</title>
      <link>https://arxiv.org/abs/2510.09904</link>
      <description>arXiv:2510.09904v1 Announce Type: cross 
Abstract: Despite their widespread use, training deep Transformers can be unstable. Layer normalization, a standard component, improves training stability, but its placement has often been ad-hoc. In this paper, we conduct a principled study on the forward (hidden states) and backward (gradient) stability of Transformers under different layer normalization placements. Our theory provides key insights into the training dynamics: whether training drives Transformers toward regular solutions or pathological behaviors. For forward stability, we derive explicit bounds on the growth of hidden states in trained Transformers. For backward stability, we analyze how layer normalization affects the backpropagation of gradients, thereby explaining the training dynamics of each layer normalization placement. Our analysis also guides the scaling of residual steps in Transformer blocks, where appropriate choices can further improve stability and performance. Our numerical results corroborate our theoretical findings. Beyond these results, our framework provides a principled way to sanity-check the stability of Transformers under new architectural modifications, offering guidance for future designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09904v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelvin Kan, Xingjian Li, Benjamin J. Zhang, Tuhin Sahai, Stanley Osher, Krishna Kumar, Markos A. Katsoulakis</dc:creator>
    </item>
    <item>
      <title>AutoGD: Automatic Learning Rate Selection for Gradient Descent</title>
      <link>https://arxiv.org/abs/2510.09923</link>
      <description>arXiv:2510.09923v1 Announce Type: cross 
Abstract: The performance of gradient-based optimization methods, such as standard gradient descent (GD), greatly depends on the choice of learning rate. However, it can require a non-trivial amount of user tuning effort to select an appropriate learning rate schedule. When such methods appear as inner loops of other algorithms, expecting the user to tune the learning rates may be impractical. To address this, we introduce AutoGD: a gradient descent method that automatically determines whether to increase or decrease the learning rate at a given iteration. We establish the convergence of AutoGD, and show that we can recover the optimal rate of GD (up to a constant) for a broad class of functions without knowledge of smoothness constants. Experiments on a variety of traditional problems and variational inference optimization tasks demonstrate strong performance of the method, along with its extensions to AutoBFGS and AutoLBFGS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09923v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikola Surjanovic, Alexandre Bouchard-C\^ot\'e, Trevor Campbell</dc:creator>
    </item>
    <item>
      <title>Structured Cooperative Multi-Agent Reinforcement Learning: a Bayesian Network Perspective</title>
      <link>https://arxiv.org/abs/2510.09937</link>
      <description>arXiv:2510.09937v1 Announce Type: cross 
Abstract: The empirical success of multi-agent reinforcement learning (MARL) has motivated the search for more efficient and scalable algorithms for large scale multi-agent systems. However, existing state-of-the-art algorithms do not fully exploit inter-agent coupling information to develop MARL algorithms. In this paper, we propose a systematic approach to leverage structures in the inter-agent couplings for efficient model-free reinforcement learning. We model the cooperative MARL problem via a Bayesian network and characterize the subset of agents, termed as the value dependency set, whose information is required by each agent to estimate its local action value function exactly. Moreover, we propose a partially decentralized training decentralized execution (P-DTDE) paradigm based on the value dependency set. We theoretically establish that the total variance of our P-DTDE policy gradient estimator is less than the centralized training decentralized execution (CTDE) policy gradient estimator. We derive a multi-agent policy gradient theorem based on the P-DTDE scheme and develop a scalable actor-critic algorithm. We demonstrate the efficiency and scalability of the proposed algorithm on multi-warehouse resource allocation and multi-zone temperature control examples. For dense value dependency sets, we propose an approximation scheme based on truncation of the Bayesian network and empirically show that it achieves a faster convergence than the exact value dependence set for applications with a large number of agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09937v1</guid>
      <category>cs.MA</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shahbaz P Qadri Syed, He Bai</dc:creator>
    </item>
    <item>
      <title>Tight Robustness Certificates and Wasserstein Distributional Attacks for Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2510.10000</link>
      <description>arXiv:2510.10000v1 Announce Type: cross 
Abstract: Wasserstein distributionally robust optimization (WDRO) provides a framework for adversarial robustness, yet existing methods based on global Lipschitz continuity or strong duality often yield loose upper bounds or require prohibitive computation. In this work, we address these limitations by introducing a primal approach and adopting a notion of exact Lipschitz certificate to tighten this upper bound of WDRO. In addition, we propose a novel Wasserstein distributional attack (WDA) that directly constructs a candidate for the worst-case distribution. Compared to existing point-wise attack and its variants, our WDA offers greater flexibility in the number and location of attack points. In particular, by leveraging the piecewise-affine structure of ReLU networks on their activation cells, our approach results in an exact tractable characterization of the corresponding WDRO problem. Extensive evaluations demonstrate that our method achieves competitive robust accuracy against state-of-the-art baselines while offering tighter certificates than existing methods. Our code is available at https://github.com/OLab-Repo/WDA</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10000v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bach C. Le, Tung V. Dao, Binh T. Nguyen, Hong T. M. Chu</dc:creator>
    </item>
    <item>
      <title>Performance Index Shaping for Closed-loop Optimal Control</title>
      <link>https://arxiv.org/abs/2510.10202</link>
      <description>arXiv:2510.10202v1 Announce Type: cross 
Abstract: The design of the performance index, also referred to as cost or reward shaping, is central to both optimal control and reinforcement learning, as it directly determines the behaviors, trade-offs, and objectives that the resulting control laws seek to achieve. A commonly used approach for this inference task in recent years is differentiable trajectory optimization, which allows gradients to be computed with respect to cost parameters by differentiating through an optimal control solver. However, this method often requires repeated solving of the underlying optimal control problem at every iteration, making the method computationally expensive. In this work, assuming known dynamics, we propose a novel framework that analytically links the performance index to the resulting closed-loop optimal control law, thereby transforming a typically bi-level inverse problem into a tractable single-level formulation. Our approach is motivated by the question: given a closed-loop control law that solves an infinite-horizon optimal control problem, how does this law change when the performance index is modified with additional terms? This formulation yields closed-form characterizations for broad classes of systems and performance indices, which not only facilitate interpretation and stability analysis, but also provide insight into the robust stability and input-to-state stable behavior of the resulting nonlinear closed-loop system. Moreover, this analytical perspective enables the generalization of our approach to diverse design objectives, yielding a unifying framework for performance index shaping. Given specific design objectives, we propose a systematic methodology to guide the shaping of the performance index and thereby design the resulting optimal control law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10202v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayush Rai, Shaoshuai Mou, Brian D. O. Anderson</dc:creator>
    </item>
    <item>
      <title>Optimal annuitization with labor income under age-dependent force of mortality</title>
      <link>https://arxiv.org/abs/2510.10371</link>
      <description>arXiv:2510.10371v1 Announce Type: cross 
Abstract: We consider the problem of optimal annuitization with labour income, where an agent aims to maximize utility from consumption and labour income under age-dependent force of mortality. Using a dynamic programming approach, we derive closed-form solutions for the value function and the optimal consumption, portfolio, and labor supply strategies. Our results show that before retirement, investment behavior increases with wealth until a threshold set by labor supply. After retirement, agents tend to consume a larger portion of their wealth. Two main factors influence optimal annuitization decisions as people get older. First, the agent's perspective (demand side); the agent's personal discount rate rises with age, reducing their desire to annuitize. Second, the insurer's perspective (supply side); insurers offer higher payout rates (mortality credits). Our model demonstrates that beyond a certain age, sharply declining survival probabilities make annuitization substantially optimal, as the powerful incentive of mortality credits outweighs the agent's high personal discount rate. Finally, post-retirement labor income serves as a direct substitute for annuitization by providing an alternative stable income source. It enhances the financial security of retirees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10371v1</guid>
      <category>q-fin.PM</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Criscent Birungi, Cody Hyndman</dc:creator>
    </item>
    <item>
      <title>Encoder Decoder Generative Adversarial Network Model for Stock Market Prediction</title>
      <link>https://arxiv.org/abs/2510.10617</link>
      <description>arXiv:2510.10617v1 Announce Type: cross 
Abstract: Forecasting stock prices remains challenging due to the volatile and non-linear nature of financial markets. Despite the promise of deep learning, issues such as mode collapse, unstable training, and difficulty in capturing temporal and feature level correlations have limited the applications of GANs in this domain. We propose a GRU-based Encoder-Decoder GAN (EDGAN) model that strikes a balance between expressive power and simplicity. The model introduces key innovations such as a temporal decoder with residual connections for precise reconstruction, conditioning on static and dynamic covariates for contextual learning, and a windowing mechanism to capture temporal dynamics. Here, the generator uses a dense encoder-decoder framework with residual GRU blocks. Extensive experiments on diverse stock datasets demonstrate that EDGAN achieves superior forecasting accuracy and training stability, even in volatile markets. It consistently outperforms traditional GAN variants in forecasting accuracy and convergence stability under market conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10617v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bahadur Yadav, Sanjay Kumar Mohanty</dc:creator>
    </item>
    <item>
      <title>I2E2S2R Rumor Spreading Model in Homogeneous Network with Hesitating and Forgetting Mechanisms</title>
      <link>https://arxiv.org/abs/2510.10672</link>
      <description>arXiv:2510.10672v1 Announce Type: cross 
Abstract: The spreading and controlling of rumors have great impacts on our society. The transmission of infectious diseases and the spreading of rumors have some common scenarios. Like cross-infection propagation of diseases, two or many kinds of rumors or information may spread at the same time. In this paper, we propose a novel I2E2S2R rumor-spreading model in a homogeneous network. The rumor-free equilibrium, as well as the basic reproduction number, have been calculated from the mean-field equations of the model. Lyapunov function and the LaSalle invariance principle are used to establish the global stability of the rumor-free equilibrium. In numerical simulations, it is perceived that a higher degree of network helps to spread rumors quickly. We have also found that making people aware can help to disappear rumors faster from the network. In addition, making people divert from the rumor to exact information can lessen the spreading of the rumor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10672v1</guid>
      <category>physics.soc-ph</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md. Nahid Hasan, Sujana Azmi Polin, Saiful Islam, Chandra Nath Podder</dc:creator>
    </item>
    <item>
      <title>Understanding Sampler Stochasticity in Training Diffusion Models for RLHF</title>
      <link>https://arxiv.org/abs/2510.10767</link>
      <description>arXiv:2510.10767v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is increasingly used to fine-tune diffusion models, but a key challenge arises from the mismatch between stochastic samplers used during training and deterministic samplers used during inference. In practice, models are fine-tuned using stochastic SDE samplers to encourage exploration, while inference typically relies on deterministic ODE samplers for efficiency and stability. This discrepancy induces a reward gap, raising concerns about whether high-quality outputs can be expected during inference. In this paper, we theoretically characterize this reward gap and provide non-vacuous bounds for general diffusion models, along with sharper convergence rates for Variance Exploding (VE) and Variance Preserving (VP) Gaussian models. Methodologically, we adopt the generalized denoising diffusion implicit models (gDDIM) framework to support arbitrarily high levels of stochasticity, preserving data marginals throughout. Empirically, our findings through large-scale experiments on text-to-image models using denoising diffusion policy optimization (DDPO) and mixed group relative policy optimization (MixGRPO) validate that reward gaps consistently narrow over training, and ODE sampling quality improves when models are updated using higher-stochasticity SDE training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10767v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayuan Sheng, Hanyang Zhao, Haoxian Chen, David D. Yao, Wenpin Tang</dc:creator>
    </item>
    <item>
      <title>Preconditioned Norms: A Unified Framework for Steepest Descent, Quasi-Newton and Adaptive Methods</title>
      <link>https://arxiv.org/abs/2510.10777</link>
      <description>arXiv:2510.10777v1 Announce Type: cross 
Abstract: Optimization lies at the core of modern deep learning, yet existing methods often face a fundamental trade-off between adapting to problem geometry and leveraging curvature utilization. Steepest descent algorithms adapt to different geometries through norm choices but remain strictly first-order, whereas quasi-Newton and adaptive optimizers incorporate curvature information but are restricted to Frobenius geometry, limiting their applicability across diverse architectures. In this work, we propose a unified framework generalizing steepest descent, quasi-Newton methods, and adaptive methods through the novel notion of preconditioned matrix norms. This abstraction reveals that widely used optimizers such as SGD and Adam, as well as more advanced approaches like Muon and KL-Shampoo, and recent hybrids including SOAP and SPlus, all emerge as special cases of the same principle. Within this framework, we provide the first systematic treatment of affine and scale invariance in the matrix-parameterized setting, establishing necessary and sufficient conditions under generalized norms. Building on this foundation, we introduce two new methods, $\texttt{MuAdam}$ and $\texttt{MuAdam-SANIA}$, which combine the spectral geometry of Muon with Adam-style preconditioning. Our experiments demonstrate that these optimizers are competitive with, and in some cases outperform, existing state-of-the-art methods. Our code is available at https://github.com/brain-lab-research/LIB/tree/quasi_descent</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10777v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrey Veprikov, Arman Bolatov, Samuel Horv\'ath, Aleksandr Beznosikov, Martin Tak\'a\v{c}, Slavomir Hanzely</dc:creator>
    </item>
    <item>
      <title>Hadamard-L\'{e}vy theorems for maps taking values in a finite dimensional space</title>
      <link>https://arxiv.org/abs/2510.11206</link>
      <description>arXiv:2510.11206v1 Announce Type: cross 
Abstract: We propose global surjectivity theorems of differentiable maps based on second order conditions. Using the homotopy continuation method, we demonstrate that, for a $C^2$ differentiable map from a Hilbert space to a finite-dimensional Euclidean space, when its second-order differential has uniform upper and lower bounds, it has a global path-lifting property in the presence of singularities. This is then applied to the nonlinear motion planning problem, establishing in some cases the well-posedness of the continuation method despite critical values of the endpoint maps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11206v1</guid>
      <category>math.CA</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yacine Chitour, Zhengping Ji, Emmanuel Tr\'elat</dc:creator>
    </item>
    <item>
      <title>Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization</title>
      <link>https://arxiv.org/abs/2510.11539</link>
      <description>arXiv:2510.11539v1 Announce Type: cross 
Abstract: Accurate state estimation is critical for legged and aerial robots operating in dynamic, uncertain environments. A key challenge lies in specifying process and measurement noise covariances, which are typically unknown or manually tuned. In this work, we introduce a bi-level optimization framework that jointly calibrates covariance matrices and kinematic parameters in an estimator-in-the-loop manner. The upper level treats noise covariances and model parameters as optimization variables, while the lower level executes a full-information estimator. Differentiating through the estimator allows direct optimization of trajectory-level objectives, resulting in accurate and consistent state estimates. We validate our approach on quadrupedal and humanoid robots, demonstrating significantly improved estimation accuracy and uncertainty calibration compared to hand-tuned baselines. Our method unifies state estimation, sensor, and kinematics calibration into a principled, data-driven framework applicable across diverse robotic platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11539v1</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Denglin Cheng, Jiarong Kang, Xiaobin Xiong</dc:creator>
    </item>
    <item>
      <title>Efficient Group Lasso Regularized Rank Regression with Data-Driven Parameter Determination</title>
      <link>https://arxiv.org/abs/2510.11546</link>
      <description>arXiv:2510.11546v1 Announce Type: cross 
Abstract: High-dimensional regression often suffers from heavy-tailed noise and outliers, which can severely undermine the reliability of least-squares based methods. To improve robustness, we adopt a non-smooth Wilcoxon score based rank objective and incorporate structured group sparsity regularization, a natural generalization of the lasso, yielding a group lasso regularized rank regression method. By extending the tuning-free parameter selection scheme originally developed for the lasso, we introduce a data-driven, simulation-based tuning rule and further establish a finite-sample error bound for the resulting estimator. On the computational side, we develop a proximal augmented Lagrangian method for solving the associated optimization problem, which eliminates the singularity issues encountered in existing methods, thereby enabling efficient semismooth Newton updates for the subproblems. Extensive numerical experiments demonstrate the robustness and effectiveness of our proposed estimator against alternatives, and showcase the scalability of the algorithm across both simulated and real-data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11546v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meixia Lin, Meijiao Shi, Yunhai Xiao, Qian Zhang</dc:creator>
    </item>
    <item>
      <title>Axiomatic and Probabilistic Foundations for the Hodge-Theoretic Shapley Value</title>
      <link>https://arxiv.org/abs/2106.15094</link>
      <description>arXiv:2106.15094v4 Announce Type: replace 
Abstract: This paper establishes a complete theoretical foundation for the Hodge-theoretic extension of the Shapley value introduced by Stern and Tettenhorst (2019). We show that a set of five axioms--efficiency, linearity, symmetry, a modified null-player condition, and an independency principle--uniquely characterize this value across all coalitions, not just the grand coalition. In parallel, we derive a probabilistic representation interpreting each player's value as the expected cumulative marginal contribution along a random walk on the coalition graph. These dual axiomatic and probabilistic results unify fairness and stochastic interpretation, positioning the Hodge-theoretic value as a canonical generalization of Shapley's framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.15094v4</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tongseok Lim</dc:creator>
    </item>
    <item>
      <title>Iterative Implicit Gradients for Nonconvex Optimization with Variational Inequality Constraints</title>
      <link>https://arxiv.org/abs/2203.12653</link>
      <description>arXiv:2203.12653v2 Announce Type: replace 
Abstract: We propose an optimization proxy in terms of iterative implicit gradient methods for solving constrained optimization problems with nonconvex loss functions. This framework can be applied to a broad range of machine learning settings, including meta-learning, hyperparameter optimization, large-scale complicated constrained optimization, and reinforcement learning. The proposed algorithm builds upon the iterative differentiation (ITD) approach. We extend existing convergence and rate analyses from the bilevel optimization literature to a constrained bilevel setting, motivated by learning under explicit constraints. Since solving bilevel problems using first-order methods requires evaluating the gradient of the inner-level optimal solution with respect to the outer variable (the implicit gradient), we develop an efficient computation strategy suitable for large-scale structures. Furthermore, we establish error bounds relative to the true gradients and provide non-asymptotic convergence rate guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.12653v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harshal D. Kaushik, Ming Jin</dc:creator>
    </item>
    <item>
      <title>Bilevel Optimization and Heuristic Algorithms for Integrating Latent Demand into the Design of Large-Scale Transit Systems</title>
      <link>https://arxiv.org/abs/2212.03460</link>
      <description>arXiv:2212.03460v2 Announce Type: replace 
Abstract: Capturing latent demand has a pivotal role in designing transit services as omitting these riders can lead to poor quality of service and/or additional costs. This paper explores this topic in the design of transit networks by considering the perspectives of both the transit agencies and riders. The paper presents a generic bilevel optimization model, namely the Transit Networks Design with Adoptions (TN-DA), that considers the network design decisions in the leader problem, and routing of the riders in the follower problem under the given network design, while allowing a black-box choice function for representing the adoption behavior of latent demand. The paper then identifies structural properties of the optimal solution of the TN-DA problem, which are desirable for transit agencies for capturing adoption behavior of the riders. The paper further provides guideline metrics for the transit agencies based on these desired adoption properties. Due to the computational complexity of this bilevel problem, the paper proposes five efficient heuristic algorithms to solve large-scale instances, which leverage an iterative procedure by solving a simpler version of the TN-DA problem and integrating the evaluation of rider choices. These algorithms either satisfy the desired properties of the optimal solution or provide fast approximations. The paper presents extensive large-scale case studies on two different transit systems by utilizing real datasets: (i) On-demand Multimodal Transit Systems (ODMTS) and (ii) Scooters-Connected Transit Systems (SCTS). The results demonstrate that the heuristic algorithms can find high-quality solutions much faster than exact approaches over various instances, while satisfying key adoption properties of the optimal solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.03460v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongzhao Guan, Beste Basciftci, Pascal Van Hentenryck</dc:creator>
    </item>
    <item>
      <title>Complexity of trust-region methods with unbounded Hessian approximations for smooth and nonsmooth optimization</title>
      <link>https://arxiv.org/abs/2312.15151</link>
      <description>arXiv:2312.15151v4 Announce Type: replace 
Abstract: We develop a worst-case evaluation complexity bound for trust-region methods in the presence of unbounded Hessian approximations. We use the algorithm of arXiv:2103.15993v3 as a model, which is designed for nonsmooth regularized problems, but applies to unconstrained smooth problems as a special case. Our analysis assumes that the growth of the Hessian approximation is controlled by the number of successful iterations. We show that the best known complexity bound of $\epsilon^{-2}$ deteriorates to $\epsilon^{-2/(1-p)}$, where $0 \le p &lt; 1$ is a parameter that controls the growth of the Hessian approximation. The faster the Hessian approximation grows, the more the bound deteriorates. We construct an objective that satisfies all of our assumptions and for which our complexity bound is attained, which establishes that our bound is sharp. To the best of our knowledge, our complexity result is the first to consider potentially unbounded Hessians and is a first step towards addressing a conjecture of Powell [38] that trust-region methods may require an exponential number of iterations in such a case. Numerical experiments conducted in double precision arithmetic are consistent with the analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15151v4</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.22451.40486</arxiv:DOI>
      <dc:creator>Geoffroy Leconte, Dominique Orban</dc:creator>
    </item>
    <item>
      <title>New global Carleman estimates and null controllability for forward/backward semi-linear parabolic SPDEs</title>
      <link>https://arxiv.org/abs/2401.13455</link>
      <description>arXiv:2401.13455v5 Announce Type: replace 
Abstract: In this paper, we study the null controllability for parabolic SPDEs involving both the state and the gradient of the state. To start with, an improved global Carleman estimate for linear forward (resp. backward) parabolic SPDEs with general random coefficients and square-integrable source terms is derived. Based on this, we further develop a new global Carleman estimate for linear forward (resp. backward) parabolic SPDEs with source terms in the Sobolev space of negative order, which enables us to deal with the global null controllability for linear backward (resp. forward) parabolic SPDEs with gradient terms. As a byproduct, a special weighted energy-type estimate for the controlled system that explicitly depends on the parameters $\lambda,\mu$ and the weighted function $\theta$ is obtained, which makes it possible to extend the previous linear null controllability to semi-linear backward (resp. forward) parabolic SPDEs by applying the fixed-point argument in an appropriate Banach space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13455v5</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Zhang, Fan Xu, Bin Liu</dc:creator>
    </item>
    <item>
      <title>Block cubic Newton with greedy selection</title>
      <link>https://arxiv.org/abs/2407.18150</link>
      <description>arXiv:2407.18150v4 Announce Type: replace 
Abstract: A second-order block coordinate descent method is proposed for the unconstrained minimization of an objective function with a Lipschitz continuous Hessian. At each iteration, a block of variables is selected by means of a greedy (Gauss-Southwell) rule which considers the amount of first-order stationarity violation, then an approximate minimizer of a cubic model is computed for the block update. In the proposed scheme, blocks are not required to have a predetermined structure and their size may change during the iterations. For non-convex objective functions, global convergence to stationary points is proved and a worst-case iteration complexity analysis is provided. In particular, given a tolerance $\epsilon$, we show that at most ${\cal O(\epsilon^{-3/2})}$ iterations are needed to drive the stationarity violation with respect to a selected block of variables below $\epsilon$, while at most ${\cal O(\epsilon^{-2})}$ iterations are needed to drive the stationarity violation with respect to all variables below $\epsilon$. Numerical results are finally given, comparing the proposed approach with other second-order methods and block selection rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18150v4</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Cristofari</dc:creator>
    </item>
    <item>
      <title>Enforcing Mesh Quality Constraints in Shape Optimization with a Gradient Projection Method</title>
      <link>https://arxiv.org/abs/2412.00006</link>
      <description>arXiv:2412.00006v4 Announce Type: replace 
Abstract: For the numerical solution of shape optimization problems, particularly those constrained by partial differential equations (PDEs), the quality of the underlying mesh is of utmost importance. Particularly when investigating complex geometries, the mesh quality tends to deteriorate over the course of a shape optimization so that either the optimization comes to a halt or an expensive remeshing operation must be performed before the optimization can be continued. In this paper, we present a novel, semi-discrete approach for enforcing a minimum mesh quality in shape optimization. Our approach is based on Rosen's gradient projection method, which incorporates mesh quality constraints into the shape optimization problem. The proposed constraints bound the angles of triangular and solid angles of tetrahedral mesh cells and, thus, also bound the quality of these mesh cells. The method treats these constraints by projecting the search direction to the linear subspace of the currently active constraints. Additionally, only slight modifications to the usual line search procedure are required to ensure the feasibility of the method. We present our method for two- and three-dimensional simplicial meshes. We investigate the proposed approach numerically for the drag minimization of an obstacle in a two-dimensional Stokes flow, the optimization of the flow in a pipe governed by the Navier-Stokes equations, and for the large-scale, three-dimensional optimization of a structured packing used in a distillation column. Our results show that the proposed method is indeed capable of guaranteeing a minimum mesh quality for both academic examples and challenging industrial applications. Particularly, our approach allows the shape optimization of complex structures while ensuring that the mesh quality does not deteriorate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00006v4</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cma.2025.118451</arxiv:DOI>
      <arxiv:journal_reference>Comput. Methods Appl. Mech. Engrg. 448, 2026</arxiv:journal_reference>
      <dc:creator>Sebastian Blauth, Christian Leith\"auser</dc:creator>
    </item>
    <item>
      <title>KLAP: KYP lemma based low-rank approximation for $\mathcal{H}_2$-optimal passivation</title>
      <link>https://arxiv.org/abs/2501.05178</link>
      <description>arXiv:2501.05178v3 Announce Type: replace 
Abstract: We present a novel passivity enforcement (passivation) method, called KLAP, for linear time-invariant systems based on the Kalman-Yakubovich-Popov (KYP) lemma and the closely related Lur'e equations. The passivation problem in our framework corresponds to finding a perturbation to a given non-passive system that renders the system passive while minimizing the $\mathcal{H}_2$ or frequency-weighted $\mathcal{H}_2$ distance between the original non-passive and the resulting passive system. We show that this problem can be formulated as an unconstrained optimization problem whose objective function can be differentiated efficiently even in large-scale settings. We show that any minimizer of the unconstrained problem yields the same passive system. Furthermore, we prove that, in the absence of a feedthrough term, every local minimizer is also a global minimizer. For cases involving a non-trivial feedthrough term, we analyze global minimizers in relation to the extremal solutions of the Lur'e equations, which can serve as tools for identifying local minima. To solve the resulting numerical optimization problem efficiently, we propose an initialization strategy based on modifying the feedthrough term and a restart strategy when it is likely that the optimization has converged to a non-global local minimum. Numerical examples illustrate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05178v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Nicodemus, Matthias Voigt, Serkan Gugercin, Benjamin Unger</dc:creator>
    </item>
    <item>
      <title>Efficient Single-Loop Stochastic Algorithms for Nonconvex-Concave Minimax Optimization</title>
      <link>https://arxiv.org/abs/2501.05677</link>
      <description>arXiv:2501.05677v2 Announce Type: replace 
Abstract: Nonconvex-concave (NC-C) finite-sum minimax problems have wide applications in signal processing and machine learning tasks. Conventional stochastic gradient algorithms, which rely on uniform sampling for gradient estimation, often suffer from slow convergence rates and require bounded variance assumptions. While variance reduction techniques can significantly improve the convergence of stochastic algorithms, the inherent nonsmooth nature of NC-C problems makes it challenging to design effective variance reduction techniques. To address this challenge, we develop a novel probabilistic variance reduction scheme and propose a single-loop stochastic gradient algorithm called the probabilistic variance-reduced smoothed gradient descent-ascent (PVR-SGDA) algorithm. The proposed PVR-SGDA algorithm achieves an iteration complexity of $\mathcal{O}(\epsilon^{-4})$, surpassing the best-known rates of stochastic algorithms for NC-C minimax problems and matching the performance of state-of-the-art deterministic algorithms. Furthermore, to completely eliminate the need for full gradient computation and reduce the gradient complexity, we explore another variance reduction technique with auxiliary gradient trackers and propose a smoothed gradient descent-ascent algorithm without full gradient calculation, called ZeroSARAH-SGDA, for NC-C problems. The ZeroSARAH-SGDA algorithm achieves a comparable iteration complexity to PVR-SGDA, while reducing the gradient oracle calls at each iteration. Finally, we demonstrate the effectiveness of the proposed two algorithms through numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05677v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xia Jiang, Linglingzhi Zhu, Taoli Zheng, Anthony Man-Cho So</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Real-Time Personalization with Simple Transformers</title>
      <link>https://arxiv.org/abs/2503.00608</link>
      <description>arXiv:2503.00608v2 Announce Type: replace 
Abstract: Real-time personalization has advanced significantly in recent years, with platforms utilizing machine learning models to predict user preferences based on rich behavioral data on each individual user. Traditional approaches usually rely on embedding-based machine learning models to capture user preferences, and then reduce the final optimization task to nearest-neighbors, which can be performed extremely fast. However, these models struggle to capture complex user behaviors, which are essential for making accurate recommendations. Transformer-based models, on the other hand, are known for their practical ability to model sequential behaviors, and hence have been intensively used in personalization recently to overcome these limitations. However, optimizing recommendations under transformer-based models is challenging due to their complicated architectures. In this paper, we address this challenge by considering a specific class of transformers, showing its ability to represent complex user preferences, and developing efficient algorithms for real-time personalization.
  We focus on a particular set of transformers, called simple transformers, which contain a single self-attention layer. We show that simple transformers are capable of capturing complex user preferences. We then develop an algorithm that enables fast optimization of recommendation tasks based on simple transformers. Our algorithm achieves near-optimal performance in sub-linear time. Finally, we demonstrate the effectiveness of our approach through an empirical study on datasets from Spotify and Trivago. Our experiment results show that (1) simple transformers can model/predict user preferences substantially more accurately than non-transformer models and nearly as accurately as more complex transformers, and (2) our algorithm completes simple-transformer-based recommendation tasks quickly and effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00608v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin An, Andrew A. Li, Vaisnavi Nemala, Gabriel Visotsky</dc:creator>
    </item>
    <item>
      <title>Intraday Battery Dispatch for Hybrid Renewable Energy Assets</title>
      <link>https://arxiv.org/abs/2503.12305</link>
      <description>arXiv:2503.12305v2 Announce Type: replace 
Abstract: We develop a mathematical model for intraday dispatch of co-located wind-battery energy assets. Focusing on the primary objective of firming grid-side actual production vis-a-vis the preset day-ahead hourly generation targets, we conduct a comprehensive study of the resulting stochastic control problem across different firming formulations and wind generation dynamics. Among others, we provide a closed-form solution in the special case of a quadratic objective and linear dynamics, as well as design a novel adaptation of a Gaussian Process-based Regression Monte Carlo algorithm for our setting. Extensions studied include an asymmetric loss function for peak shaving, capturing the cost of battery cycling, and the role of battery duration. In the applied portion of our work, we calibrate our model to a collection of 140+ wind-battery assets in Texas, benchmarking the economic benefits of firming based on outputs of a realistic unit commitment and economic dispatch solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12305v2</guid>
      <category>math.OC</category>
      <category>q-fin.CP</category>
      <category>q-fin.MF</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thiha Aung, Mike Ludkovski</dc:creator>
    </item>
    <item>
      <title>"Over-optimizing" for Normality: Budget-constrained Uncertainty Quantification for Contextual Decision-making</title>
      <link>https://arxiv.org/abs/2503.12747</link>
      <description>arXiv:2503.12747v3 Announce Type: replace 
Abstract: We study uncertainty quantification for contextual stochastic optimization, focusing on weighted sample average approximation (wSAA), which uses machine-learned relevance weights based on covariates. Although wSAA is widely used for contextual decisions, its uncertainty quantification remains limited. In addition, computational budgets tie sample size to optimization accuracy, creating a coupling that standard analyses often ignore. We establish central limit theorems for wSAA and construct asymptotic-normality-based confidence intervals for optimal conditional expected costs. We analyze the statistical--computational tradeoff under a computational budget, characterizing how to allocate resources between sample size and optimization iterations to balance statistical and optimization errors. These allocation rules depend on structural parameters of the objective; misspecifying them can break the asymptotic optimality of the wSAA estimator. We show that ``over-optimizing'' (running more iterations than the nominal rule) mitigates this misspecification and preserves asymptotic normality, at the expense of a slight slowdown in the convergence rate of the budget-constrained estimator. The common intuition that ``more data is better'' can fail under computational constraints: increasing the sample size may worsen statistical inference by forcing fewer algorithm iterations and larger optimization error. Our framework provides a principled way to quantify uncertainty for contextual decision-making under computational constraints. It offers practical guidance on allocating limited resources between data acquisition and optimization effort, clarifying when to prioritize additional optimization iterations over more data to ensure valid confidence intervals for conditional performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12747v3</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yanyuan Wang, Xiaowei Zhang</dc:creator>
    </item>
    <item>
      <title>Data-Driven Structured Controller Design Using the Matrix S-Procedure</title>
      <link>https://arxiv.org/abs/2503.14949</link>
      <description>arXiv:2503.14949v2 Announce Type: replace 
Abstract: This paper focuses on the data-driven optimal structured controller design for discrete-time linear time-invariant (LTI) systems, considering both the $H_2$ performance and the $H_\infty$ performance. Specifically, we consider three scenarios: (i) the model-based structured control, (ii) the data-driven unstructured control, and (iii) the data-driven structured control. For the $H_2$ performance, we primarily investigate cases (ii) and (iii), since case (i) has been extensively studied in the literature. For the $H_\infty$ performance, all three scenarios are considered. For the structured control, we introduce a linearization technique that transforms the original nonconvex problem into a semidefinite programming (SDP) problem. Based on this transformation, we develop an iterative linear matrix inequality (ILMI) algorithm. For the data-driven control, we describe the set of all possible system matrices that can generate the sequence of collected data. Additionally, we propose a sufficient condition to handle all possible system matrices using the matrix S-procedure. The data-driven structured control is followed by combining the previous two cases. We compare our methods with those in the existing literature and demonstrate our superiority via several numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14949v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaohua Yang, Yuxing Zhong, Nachuan Yang, Xiaoxu Lyu, Ling Shi</dc:creator>
    </item>
    <item>
      <title>Approximate stationarity in disjunctive optimization: concepts, qualification conditions, and application to MPCCs</title>
      <link>https://arxiv.org/abs/2503.22551</link>
      <description>arXiv:2503.22551v2 Announce Type: replace 
Abstract: In this paper, we are concerned with stationarity conditions and qualification conditions for optimization problems with disjunctive constraints. This class covers, among others, optimization problems with complementarity, vanishing, or switching constraints, which are notoriously challenging due to their highly combinatorial structure. The focus of our study is twofold. First, we investigate approximate stationarity conditions and the associated strict constraint qualifications which can be used to infer stationarity of local minimizers. While such concepts are already known in the context of so-called Mordukhovich-stationarity, we introduce suitable extensions associated with strong stationarity. Second, a qualification condition is established which, based on an approximately Mordukhovich- or strongly stationary point, can be used to infer its Mordukhovich- or strong stationarity, respectively. In contrast to the aforementioned strict constraint qualifications, this condition depends on the involved sequences justifying approximate stationarity and, thus, is not a constraint qualification in the narrower sense. However, it is much easier to verify as it merely requires to check the (positive) linear independence of a certain family of gradients. In order to illustrate the obtained findings, they are applied to optimization problems with complementarity constraints, where they can be naturally extended to the well-known concepts of weak and Clarke-stationarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22551v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Isabella K\"aming, Patrick Mehlitz</dc:creator>
    </item>
    <item>
      <title>An Adaptive Algorithm for Bilevel Optimization on Riemannian Manifolds</title>
      <link>https://arxiv.org/abs/2504.06042</link>
      <description>arXiv:2504.06042v3 Announce Type: replace 
Abstract: Existing methods for solving Riemannian bilevel optimization (RBO) problems require prior knowledge of the problem's first- and second-order information and curvature parameter of the Riemannian manifold to determine step sizes, which poses practical limitations when these parameters are unknown or computationally infeasible to obtain. In this paper, we introduce the Adaptive Riemannian Hypergradient Descent (AdaRHD) algorithm for solving RBO problems. To our knowledge, AdaRHD is the first method to incorporate a fully adaptive step size strategy that eliminates the need for problem-specific parameters in RBO. We prove that AdaRHD achieves an $\mathcal{O}(1/\epsilon)$ iteration complexity for finding an $\epsilon$-stationary point, thus matching the complexity of existing non-adaptive methods. Furthermore, we demonstrate that substituting exponential mappings with retraction mappings maintains the same complexity bound. Experiments demonstrate that AdaRHD achieves comparable performance to existing non-adaptive approaches while exhibiting greater robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06042v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xu Shi, Rufeng Xiao, Rujun Jiang</dc:creator>
    </item>
    <item>
      <title>On the natural domain of Bregman operators</title>
      <link>https://arxiv.org/abs/2506.00465</link>
      <description>arXiv:2506.00465v2 Announce Type: replace 
Abstract: The Bregman proximal mapping and Bregman-Moreau envelope are traditionally studied for functions defined on the entire space $\mathbb{R}^n$, even though these constructions depend only on the values of the function within (the interior of) the domain of the distance-generating function (dgf). While this convention is largely harmless in the convex setting, it leads to substantial limitations in the nonconvex case, as it fails to embrace important classes of functions such as relatively weakly convex ones. In this work, we revisit foundational aspects of Bregman analysis by adopting a domain-aware perspective: we define functions on the natural domain induced by the dgf and impose properties only relative to this set. This framework not only generalizes existing results but also rectifies and simplifies their statements and proofs. Several examples illustrate both the necessity of our assumptions and the advantages of this refined approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00465v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Themelis, Ziyuan Wang</dc:creator>
    </item>
    <item>
      <title>Optimal-PhiBE: A PDE-based Model-free framework for Continuous-time Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2506.05208</link>
      <description>arXiv:2506.05208v2 Announce Type: replace 
Abstract: This paper addresses continuous-time reinforcement learning (CTRL) where the system dynamics are governed by an unknown stochastic differential equation, and only discrete-time observations are available. Existing approaches face limitations: model-based PDE methods suffer from non-identifiability, while model-free methods based on the discrete-time optimal Bellman equation (Optimal-BE) suffer from large discretization errors that are highly sensitive to both the system dynamics and the reward structure. To overcome these challenges, we introduce Optimal-PhiBE, a formulation that integrates discrete-time information into a continuous-time PDE, combining the strength of both existing frameworks while mitigating their limitations. Optimal-PhiBE exhibits smaller discretization errors when the uncontrolled system evolves slowly, and demonstrates reduced sensitivity to oscillatory reward structures, and enables model-free algorithms that bypass explicit dynamics estimation. In the linear-quadratic regulator (LQR) setting, sharp error bounds are established for both Optimal-PhiBE and Optimal-BE. The results show that Optimal-PhiBE exactly recovers the optimal policy in the undiscounted case and substantially outperforms Optimal-BE when the problem is weakly discounted or control-dominant. Furthermore, we extend Optimal-PhiBE to higher orders, providing increasingly accurate approximations. A model-free policy iteration algorithm is proposed to solve the Optimal-PhiBE directly from trajectory data. Numerical experiments are conducted to verify the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05208v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhua Zhu, Yuming Zhang, Haoyu Zhang</dc:creator>
    </item>
    <item>
      <title>Optimal Voltage Control Using Online Exponential Barrier Method</title>
      <link>https://arxiv.org/abs/2506.10247</link>
      <description>arXiv:2506.10247v2 Announce Type: replace 
Abstract: This paper address the optimal voltage control problem of distribution systems with high penetration of inverter-based renewable energy resources, under inaccurate model information. We propose the online exponential barrier method that explicitly leverages the online feedback from grids to enhance the robustness to model inaccuracy and incorporates the voltage constraints to maintain the safety requirements. We provide analytical results on the optimal barrier parameter selection and sufficient conditions for the safety guarantee of converged voltages. We also establish theoretical results on the exponential convergence rate with proper step-size. The effectiveness of the proposed framework is validated on a 56-bus radial network, where we significantly improve the robustness against model inaccuracy compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10247v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Zhang, Baosen Zhang</dc:creator>
    </item>
    <item>
      <title>Equilibrium Correction Iteration for A Class of Mean-Field Game Inverse Problem</title>
      <link>https://arxiv.org/abs/2506.23018</link>
      <description>arXiv:2506.23018v2 Announce Type: replace 
Abstract: This work investigates the ambient potential identification problem in inverse Mean-Field Games (MFGs), where the goal is to recover the unknown potential from the value function at equilibrium. We propose a simple yet effective iterative strategy, Equilibrium Correction Iteration (ECI), that leverages the structure of MFGs rather than relying on generic optimization formulations. ECI uncovers hidden information from equilibrium measurements, offering a new perspective on inverse MFGs. To improve computational efficiency, two acceleration variants are introduced: Best Response Iteration (BRI), which uses inexact forward solvers, and Hierarchical ECI (HECI), which incorporates multilevel grids. While BRI performs efficiently in general settings, HECI proves particularly effective in recovering low-frequency potentials. We also highlight a connection between the potential identification problem in inverse MFGs and inverse linear parabolic equations, suggesting promising directions for future theoretical analysis. Finally, comprehensive numerical experiments demonstrate how viscosity, terminal time, and interaction costs can influence the well-posedness of the inverse problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23018v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiajia Yu, Jian-Guo Liu, Hongkai Zhao</dc:creator>
    </item>
    <item>
      <title>A generalized Hurwitz stability criterion via rectangular block Hankel matrices for nonmonic matrix polynomials</title>
      <link>https://arxiv.org/abs/2508.14376</link>
      <description>arXiv:2508.14376v5 Announce Type: replace 
Abstract: We develop a Hurwitz stability criterion for nonmonic matrix polynomials via column reduction, generalizing existing approaches constrained by the monic assumption and thus serving as a more natural extension of Gantmacher's classical stability criterion via Markov parameters. Starting from redefining the associated Markov parameters through a column-wise adaptive splitting method, our framework constructs two structured matrices whose rectangular Hankel blocks are obtained via the extraction of these parameters. We establish an explicit interrelation between the inertias of column reduced matrix polynomials and the derived structured matrices. Furthermore, we demonstrate that the Hurwitz stability of column reduced matrix polynomials can be determined by the Hermitian positive definiteness of these rectangular block Hankel matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14376v5</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuzhou Zhan, Zixiang Ni</dc:creator>
    </item>
    <item>
      <title>The Aubin Property for Generalized Equations over $C^2$-cone Reducible Sets</title>
      <link>https://arxiv.org/abs/2509.14194</link>
      <description>arXiv:2509.14194v4 Announce Type: replace 
Abstract: This paper establishes the equivalence of the Aubin property and the strong regularity for generalized equations over $C^2$-cone reducible sets. This result resolves a long-standing question in variational analysis and extends the well-known equivalence theorem for polyhedral sets to a significantly broader class of non-polyhedral cases. Our proof strategy departs from traditional variational techniques, integrating insights from convex geometry with powerful tools from algebraic topology. A cornerstone of our analysis is a new fundamental lemma concerning the local structure of the normal cone map for arbitrary closed convex sets, which reveals how the dimension of normal cones varies in the neighborhood of a boundary point. This geometric insight is the key to applying degree theory, allowing us to prove that a crucial function associated with the problem has a topological index of $\pm1$. This, via a homological version of the inverse mapping theorem, implies that the function is a local homeomorphism, which in turn yields the strong regularity of the original solution map. This result unifies and extends several existing stability results for problems such as conventional nonlinear programming, nonlinear second-order cone programming, and nonlinear semidefinite programming under a single general framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14194v4</guid>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaming Ma, Defeng Sun</dc:creator>
    </item>
    <item>
      <title>A dynamical formulation of multi-marginal optimal transport</title>
      <link>https://arxiv.org/abs/2509.22494</link>
      <description>arXiv:2509.22494v2 Announce Type: replace 
Abstract: We present a primal-dual dynamical formulation of the multi-marginal optimal transport problem for (semi-)convex cost functions. Even in the two-marginal setting, this formulation applies to cost functions not covered by the classical dynamical approach of Benamou-Brenier. Our dynamical formulation yields a convex optimization problem, enabling the use of convex optimization tools to find quasi-Monge solutions of the static multi-marginal problem for translation-invariant costs. We illustrate our results numerically with proximal splitting methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22494v2</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brendan Pass, Yair Shenfeld</dc:creator>
    </item>
    <item>
      <title>Overlapping Schwarz Scheme for Linear-Quadratic Programs in Continuous Time</title>
      <link>https://arxiv.org/abs/2510.04478</link>
      <description>arXiv:2510.04478v2 Announce Type: replace 
Abstract: We present an optimize-then-discretize framework for solving linear-quadratic optimal control problems (OCP) governed by time-inhomogeneous ordinary differential equations (ODEs). Our method employs a modified overlapping Schwarz decomposition based on the Pontryagin Minimum Principle, partitioning the temporal domain into overlapping intervals and independently solving Hamiltonian systems in continuous time. We demonstrate that the convergence is ensured by appropriately updating the boundary conditions of the individual Hamiltonian dynamics. The cornerstone of our analysis is to prove that the exponential decay of sensitivity (EDS) exhibited in discrete-time OCPs carries over to the continuous-time setting. Unlike the discretize-then-optimize approach, our method can flexibly incorporate different numerical integration methods for solving the resulting Hamiltonian two-point boundary-value subproblems, including adaptive-time integrators. A numerical experiment on a linear-quadratic OCP illustrates the practicality of our approach in broad scientific applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04478v2</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongli Zhao, Mihai Anitescu, Sen Na</dc:creator>
    </item>
    <item>
      <title>Frictional martingale optimal transport and robust hedging</title>
      <link>https://arxiv.org/abs/2510.08182</link>
      <description>arXiv:2510.08182v2 Announce Type: replace 
Abstract: We study the martingale optimal transport problem with state-dependent trading frictions and develop a geometric and duality framework extending from the one time-step to the multi-marginal setting. Building on the left-monotone structure of frictionless MOT (Beiglb\"ock and Juillet, Ann. Probab., 2016; Henry-Labord\`ere and Touzi, Finance Stoch., 2016; Beiglb\"ock et al., Ann. Probab., 2017), we introduce a convex frictional cost combining proportional bid-ask spreads and quadratic liquidity impacts. The framework extends the martingale Spence-Mirrlees condition to nonlinear frictions and establishes a frictional monotonicity principle.
  At each time step, the joint distribution between consecutive asset prices exhibits a bi-atomic, monotone geometry: conditional on the current price, the next price lies on one of two monotone branches representing upward and downward rebalancing. A no-transaction region, or trade band, arises where maintaining the position is optimal, while outside the band, transitions follow two monotone graphs whose endpoints satisfy an equal-slope condition balancing continuation value and marginal trading cost.
  The framework extends dynamically via a recursive identity, ensuring stability and convergence to the frictionless left-curtain limit, and applies to model-independent pricing and robust hedging of path-dependent derivatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08182v2</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratik Rai</dc:creator>
    </item>
    <item>
      <title>On the minimal algebraic complexity of the rank-one approximation problem for general inner products</title>
      <link>https://arxiv.org/abs/2309.15105</link>
      <description>arXiv:2309.15105v3 Announce Type: replace-cross 
Abstract: We study the algebraic complexity of Euclidean distance minimization from a generic tensor to a variety of rank-one tensors. The Euclidean Distance (ED) degree of the Segre-Veronese variety counts the number of complex critical points of this optimization problem. We regard this invariant as a function of inner products. We prove that Frobenius inner product is a local minimum of the ED degree, and conjecture that it is a global minimum. We prove our conjecture in the case of matrices and symmetric binary and $3\times 3\times 3$ tensors. We discuss the above optimization problem for other algebraic varieties, classifying all possible values of the ED degree. Our approach combines tools from Singularity Theory, Morse Theory, and Algebraic Geometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15105v3</guid>
      <category>math.AG</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khazhgali Kozhasov, Alan Muniz, Yang Qi, Luca Sodomaco</dc:creator>
    </item>
    <item>
      <title>Generalization Bounds of Surrogate Policies for Combinatorial Optimization Problems</title>
      <link>https://arxiv.org/abs/2407.17200</link>
      <description>arXiv:2407.17200v2 Announce Type: replace-cross 
Abstract: A recent line of structured learning methods has advanced the practical state-of-the-art for combinatorial optimization problems with complex, application-specific objectives. These approaches learn policies that couple a statistical model with a tractable surrogate combinatorial optimization oracle, so as to exploit the distribution of problem instances instead of solving each instance independently. A core obstacle is that the empirical risk is then piecewise constant in the model parameters. This hinders gradient-based optimization and only few theoretical guarantees have been provided so far. We address this issue by analyzing smoothed (perturbed) policies: adding controlled random perturbations to the direction used by the linear oracle yields a differentiable surrogate risk and improves generalization. Our main contribution is a generalization bound that decomposes the excess risk into perturbation bias, statistical estimation error, and optimization error. The analysis hinges on a new Uniform Weak (UW) property capturing the geometric interaction between the statistical model and the normal fan of the feasible polytope; we show it holds under mild assumptions, and automatically when a minimal baseline perturbation is present. The framework covers, in particular, contextual stochastic optimization. We illustrate the scope of the results on applications such as stochastic vehicle scheduling, highlighting how smoothing enables both tractable training and controlled generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17200v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Pierre-Cyril Aubin-Frankowski, Yohann De Castro, Axel Parmentier, Alessandro Rudi</dc:creator>
    </item>
    <item>
      <title>Optimal Assignment and Motion Control in Two-Class Continuum Swarms</title>
      <link>https://arxiv.org/abs/2407.18159</link>
      <description>arXiv:2407.18159v2 Announce Type: replace-cross 
Abstract: We consider optimal swarm control problems where two different classes of agents are present. Continuum idealizations of large-scale swarms are used where the dynamics describe the evolution of the spatially-distributed densities of each agent class. The problem formulation we adopt is motivated by applications where agents of one class are assigned to agents of the other class, which we refer to as demand and resource agents respectively. Assignments have costs related to the distances between mutually assigned agents, and the overall cost of an assignment is quantified by a Wasserstein distance between the densities of the two agent classes. When agents can move, the assignment cost can decrease at the expense of a physical motion cost, and this tradeoff sets up a nonlinear infinite-dimensional optimal control problem. We show that in one spatial dimension, this problem can be converted to an infinite-dimensional, but decoupled, linear-quadratic (LQ) tracking problem when expressed in terms of the quantile functions of the respective agent densities. Solutions are given in the general one-dimensional case, as well as in the special cases of constant and periodically time-varying demands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18159v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Emerick, Stacy Patterson, Bassam Bamieh</dc:creator>
    </item>
    <item>
      <title>DUST: A Framework for Data-Driven Density Steering</title>
      <link>https://arxiv.org/abs/2408.02777</link>
      <description>arXiv:2408.02777v2 Announce Type: replace-cross 
Abstract: We consider the problem of data-driven stochastic optimal control of an unknown LTI dynamical system. Assuming the process noise is normally distributed, we pose the problem of steering the state's mean and covariance to a target normal distribution, under noisy data collected from the underlying system, a problem commonly referred to as covariance steering (CS). A novel framework for Data-driven Uncertainty quantification and density STeering (DUST) is presented that simultaneously characterizes the noise affecting the measured data and designs an optimal affine-feedback controller to steer the density of the state to a prescribed terminal value. We use both indirect and direct data-driven design approaches based on the notions of persistency of excitation and subspace identification to exactly represent the mean and covariance dynamics of the state in terms of the data and noise realizations. Since both the mean and the covariance steering sub-problems are plagued with stochastic uncertainty arising from noisy data collection, we first estimate the noise realization from this dataset and subsequently compute tractable upper bounds on the estimation errors. The first and second moment steering problems are then solved to optimality using techniques from robust control and robust optimization. Lastly, we present an alternative control design approach based on the certainty equivalence principle and interpret the problem as one of CS under multiplicative uncertainty. We analyze the performance and efficacy of each of these data-driven approaches using a case study and compare them with their model-based counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02777v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Pilipovsky, Panagiotis Tsiotras</dc:creator>
    </item>
    <item>
      <title>H-DES: a Quantum-Classical Hybrid Differential Equation Solver</title>
      <link>https://arxiv.org/abs/2410.01130</link>
      <description>arXiv:2410.01130v2 Announce Type: replace-cross 
Abstract: In this article, we introduce an original hybrid quantum-classical algorithm based on a variational quantum algorithm for solving systems of differential equations. The algorithm relies on a spectral decomposition of the trial functions that are encoded directly in the quantum states generated by different parametrized circuits, and transforms the task of solving the differential equations into an optimization problem. We first describe the principle of the algorithm from a theoretical point of view. We provide a detailed pseudo-code of the algorithm, on which we elaborate preliminary elements for a complexity analysis to highlight some of its scaling properties. We apply our algorithm to a set of examples, running on emulators and real hardware showcasing its applicability across diverse sets of differential equations. We discuss the advantages of our method and potential avenues for further exploration and refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01130v2</guid>
      <category>quant-ph</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hamza Jaffali, Jonas Bastos de Araujo, Nadia Milazzo, Marta Reina, Henri de Boutray, Karla Baumann, Fr\'ed\'eric Holweck, Youcef Mohdeb, Roland Katz</dc:creator>
    </item>
    <item>
      <title>Modeling AdaGrad, RMSProp, and Adam with Integro-Differential Equations</title>
      <link>https://arxiv.org/abs/2411.09734</link>
      <description>arXiv:2411.09734v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a continuous-time formulation for the AdaGrad, RMSProp, and Adam optimization algorithms by modeling them as first-order integro-differential equations. We perform numerical simulations of these equations, along with stability and convergence analyses, to demonstrate their validity as accurate approximations of the original algorithms. Our results indicate a strong agreement between the behavior of the continuous-time models and the discrete implementations, thus providing a new perspective on the theoretical understanding of adaptive optimization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09734v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Heredia</dc:creator>
    </item>
    <item>
      <title>Mixing Times and Privacy Analysis for the Projected Langevin Algorithm under a Modulus of Continuity</title>
      <link>https://arxiv.org/abs/2501.04134</link>
      <description>arXiv:2501.04134v2 Announce Type: replace-cross 
Abstract: We study the mixing time of the projected Langevin algorithm (LA) and the privacy curve of noisy Stochastic Gradient Descent (SGD), beyond nonexpansive iterations. Specifically, we derive new mixing time bounds for the projected LA which are, in some important cases, dimension-free and poly-logarithmic on the accuracy, closely matching the existing results in the smooth convex case. Additionally, we establish new upper bounds for the privacy curve of the subsampled noisy SGD algorithm. These bounds show a crucial dependency on the regularity of gradients, and are useful for a wide range of convex losses beyond the smooth case. Our analysis relies on a suitable extension of the Privacy Amplification by Iteration (PABI) framework (Feldman et al., 2018; Altschuler and Talwar, 2022, 2023) to noisy iterations whose gradient map is not necessarily nonexpansive. This extension is achieved by designing an optimization problem which accounts for the best possible R\'enyi divergence bound obtained by an application of PABI, where the tractability of the problem is crucially related to the modulus of continuity of the associated gradient mapping. We show that, in several interesting cases -- namely the nonsmooth convex, weakly smooth and (strongly) dissipative -- such optimization problem can be solved exactly and explicitly, yielding the tightest possible PABI-based bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04134v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Bravo, Juan P. Flores-Mella, Crist\'obal Guzm\'an</dc:creator>
    </item>
    <item>
      <title>$k$-SVD with Gradient Descent</title>
      <link>https://arxiv.org/abs/2502.00320</link>
      <description>arXiv:2502.00320v2 Announce Type: replace-cross 
Abstract: The emergence of modern compute infrastructure for iterative optimization has led to great interest in developing optimization-based approaches for a scalable computation of $k$-SVD, i.e., the $k\geq 1$ largest singular values and corresponding vectors of a matrix of rank $d \geq 1$. Despite lots of exciting recent works, all prior works fall short in this pursuit. Specifically, the existing results are either for the exact-parameterized (i.e., $k = d$) and over-parameterized (i.e., $k &gt; d$) settings; or only establish local convergence guarantees; or use a step-size that requires problem-instance-specific oracle-provided information. In this work, we complete this pursuit by providing a gradient-descent method with a simple, universal rule for step-size selection (akin to pre-conditioning), that provably finds $k$-SVD for a matrix of any rank $d \geq 1$. We establish that the gradient method with random initialization enjoys global linear convergence for any $k, d \geq 1$. Our convergence analysis reveals that the gradient method has an attractive region, and within this attractive region, the method behaves like Heron's method (a.k.a. the Babylonian method). Our analytic results about the said attractive region imply that the gradient method can be enhanced by means of Nesterov's momentum-based acceleration technique. The resulting improved convergence rates match those of rather complicated methods typically relying on Lanczos iterations or variants thereof. We provide an empirical study to validate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00320v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yassir Jedra, Devavrat Shah</dc:creator>
    </item>
    <item>
      <title>A Method for Establishing Asymptotically Accurate Bounds for Extremal Roots of Eulerian Polynomials Using Polynomial Stability Preservers</title>
      <link>https://arxiv.org/abs/2503.04628</link>
      <description>arXiv:2503.04628v2 Announce Type: replace-cross 
Abstract: We develop the tools to bound extreme roots of multivariate real zero polynomials globally. This is done through the use of a relaxation that approximates their rigidly convex sets. This relaxation can easily be constructed using the degree $3$ truncation of the polynomial and it produces in this way a spectrahedron whose computation is relatively easy and whose size is relatively small and depending solely on the number of variables of the polynomial. As we know that, in order to be able to produce in general spectrahedral representations of rigidly convex sets it is necessary to build matrices of very big size, we try, analyze and experiment with several constructions that could increase the size of these matrices. These constructions are based principally in two main approaches: adding information about higher degree monomials or non-trivially increasing the number of variables of the original polynomial. We explore these two construction first in a general setting and see that it is necessary to particularize to certain families of polynomials in order to make them work. In particular, we are able to prove that increasing the number of variables improves the behavior of the relaxation along the diagonal in the case of Eulerian polynomials. We see that applying the relaxation to multivariate Eulerian polynomials and then looking at the univariate polynomials injected in their diagonals produces an exponential asymptotic improvement in the bounds provided. We compare these bounds with other bounds that have appeared previously in the literature and refine these previous bounds in order to study how close do the bounds provided by the relaxation are to the actual roots of the univariate Eulerian polynomials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04628v2</guid>
      <category>math.CO</category>
      <category>cs.NA</category>
      <category>math.AG</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Gonz\'alez Nevado</dc:creator>
    </item>
    <item>
      <title>Connecting the Equinoctial Elements and Rodrigues Parameters: A New Set of Elements</title>
      <link>https://arxiv.org/abs/2505.12812</link>
      <description>arXiv:2505.12812v2 Announce Type: replace-cross 
Abstract: A geometric interpretation of the equinoctial elements is given with a connection to orthogonal rotations and attitude dynamics in Euclidean 3-space. An identification is made between the equinoctial elements and classic Rodrigues parameters. A new set of equinoctial elements are developed using the modified Rodrigues parameters, thereby removing the coordinate singularity for retrograde equatorial orbits present in previous versions of these elements. A low-thrust trajectory optimization problem is set up using the new elements to numerically verify convergence for the two-point boundary problem, as compared to their predecessors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12812v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <category>physics.class-ph</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2514/1.G007347</arxiv:DOI>
      <arxiv:journal_reference>Journal of Guidance, Control, and Dynamics, 46(9), 1726-1744 (2023)</arxiv:journal_reference>
      <dc:creator>Joseph T. A. Peterson, Vishala Arya, John L. Junkins</dc:creator>
    </item>
    <item>
      <title>Rolling Ball Optimizer: Learning by ironing out loss landscape wrinkles</title>
      <link>https://arxiv.org/abs/2505.19527</link>
      <description>arXiv:2505.19527v2 Announce Type: replace-cross 
Abstract: Training large neural networks (NNs) requires optimizing high-dimensional data-dependent loss functions. The optimization landscape of these functions is often highly complex and textured, even fractal-like, with many spurious local minima, ill-conditioned valleys, degenerate points, and saddle points. Complicating things further is the fact that these landscape characteristics are a function of the data, meaning that noise in the training data can propagate forward and give rise to unrepresentative small-scale geometry. This poses a difficulty for gradient-based optimization methods, which rely on local geometry to compute updates and are, therefore, vulnerable to being derailed by noisy data. In practice,this translates to a strong dependence of the optimization dynamics on the noise in the data, i.e., poor generalization performance. To remediate this problem, we propose a new optimization procedure: Rolling Ball Optimizer (RBO), that breaks this spatial locality by incorporating information from a larger region of the loss landscape in its updates. We achieve this by simulating the motion of a rigid sphere of finite radius rolling on the loss landscape, a straightforward generalization of Gradient Descent (GD) that simplifies into it in the infinitesimal limit. The radius serves as a hyperparameter that determines the scale at which RBO sees the loss landscape, allowing control over the granularity of its interaction therewith. We are motivated by the intuition that the large-scale geometry of the loss landscape is less data-specific than its fine-grained structure, and that it is easier to optimize. We support this intuition by proving that our algorithm has a smoothing effect on the loss function. Evaluation against SGD, SAM, and Entropy-SGD, on MNIST and CIFAR-10/100 demonstrates promising results in terms of convergence speed, training accuracy, and generalization performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19527v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed D. Belgoumri, Mohamed Reda Bouadjenek, Hakim Hacid, Imran Razzak, Sunil Aryal</dc:creator>
    </item>
    <item>
      <title>MLorc: Momentum Low-rank Compression for Memory Efficient Large Language Model Adaptation</title>
      <link>https://arxiv.org/abs/2506.01897</link>
      <description>arXiv:2506.01897v3 Announce Type: replace-cross 
Abstract: With increasing size of large language models (LLMs), full-parameter fine-tuning imposes substantial memory demands. To alleviate this, we propose a novel memory-efficient training paradigm called Momentum Low-rank compression (MLorc). The key idea of MLorc is to compress and reconstruct the momentum of matrix parameters during training to reduce memory consumption. Compared to LoRA, MLorc avoids enforcing a fixed-rank constraint on weight update matrices and thus enables full-parameter learning. Compared to GaLore, MLorc directly compress the momentum rather than gradients, thereby better preserving the training dynamics of full-parameter fine-tuning. We provide a theoretical guarantee for its convergence under mild assumptions. Empirically, MLorc consistently outperforms other memory-efficient training methods, matches or even exceeds the performance of full fine-tuning at small ranks (e.g., $r=4$), and generalizes well across different optimizers -- all while not compromising time or memory efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01897v3</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Shen, Zhang Yaxiang, Minhui Huang, Mengfan Xu, Jiawei Zhang, Cong Shen</dc:creator>
    </item>
    <item>
      <title>Monotone and Conservative Policy Iteration Beyond the Tabular Case</title>
      <link>https://arxiv.org/abs/2506.07134</link>
      <description>arXiv:2506.07134v2 Announce Type: replace-cross 
Abstract: We introduce Reliable Policy Iteration (RPI) and Conservative RPI (CRPI), variants of Policy Iteration (PI) and Conservative PI (CPI), that retain tabular guarantees under function approximation. RPI uses a novel Bellman-constrained optimization for policy evaluation. We show that RPI restores the textbook \textit{monotonicity} of value estimates and that these estimates provably \textit{lower-bound} the true return; moreover, their limit partially satisfies the \textit{unprojected} Bellman equation. CRPI shares RPI's evaluation, but updates policies conservatively by maximizing a new performance-difference \textit{lower bound} that explicitly accounts for function-approximation-induced errors. CRPI inherits RPI's guarantees and, crucially, admits per-step improvement bounds. In initial simulations, RPI and CRPI outperform PI and its variants. Our work addresses a foundational gap in RL: popular algorithms such as TRPO and PPO derive from tabular CPI yet are deployed with function approximation, where CPI's guarantees often fail-leading to divergence, oscillations, or convergence to suboptimal policies. By restoring PI/CPI-style guarantees for \textit{arbitrary} function classes, RPI and CRPI provide a principled basis for next-generation RL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07134v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>S. R. Eshwar, Gugan Thoppe, Ananyabrata Barua, Aditya Gopalan, Gal Dalal</dc:creator>
    </item>
    <item>
      <title>A Note on Carlier Inequality</title>
      <link>https://arxiv.org/abs/2507.02285</link>
      <description>arXiv:2507.02285v3 Announce Type: replace-cross 
Abstract: Recently, Carlier established in [3] a quantitave version of the Fitzpatrick inequality in a Hilbert space. We extend this result by Carlier to the framework of reflexive Banach spaces. In the Hilbert space setting, we obtain an improved version of the strong Fitzpatrick inequality due to Voisei and Z\u{a}linescu.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02285v3</guid>
      <category>math.FA</category>
      <category>math.OC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Regina S. Burachik, J. E. Mart\'inez-Legaz</dc:creator>
    </item>
  </channel>
</rss>
