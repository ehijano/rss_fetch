<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 May 2025 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System for Real-Time Rendering of Composed Scenes</title>
      <link>https://arxiv.org/abs/2505.06523</link>
      <description>arXiv:2505.06523v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) enables the reconstruction of intricate digital 3D assets from multi-view images by leveraging a set of 3D Gaussian primitives for rendering. Its explicit and discrete representation facilitates the seamless composition of complex digital worlds, offering significant advantages over previous neural implicit methods. However, when applied to large-scale compositions, such as crowd-level scenes, it can encompass numerous 3D Gaussians, posing substantial challenges for real-time rendering. To address this, inspired by Unreal Engine 5's Nanite system, we propose Virtualized 3D Gaussians (V3DG), a cluster-based LOD solution that constructs hierarchical 3D Gaussian clusters and dynamically selects only the necessary ones to accelerate rendering speed. Our approach consists of two stages: (1) Offline Build, where hierarchical clusters are generated using a local splatting method to minimize visual differences across granularities, and (2) Online Selection, where footprint evaluation determines perceptible clusters for efficient rasterization during rendering. We curate a dataset of synthetic and real-world scenes, including objects, trees, people, and buildings, each requiring 0.1 billion 3D Gaussians to capture fine details. Experiments show that our solution balances rendering efficiency and visual quality across user-defined tolerances, facilitating downstream interactive applications that compose extensive 3DGS assets for consistent rendering performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06523v1</guid>
      <category>cs.GR</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3721238.3730602</arxiv:DOI>
      <dc:creator>Xijie Yang, Linning Xu, Lihan Jiang, Dahua Lin, Bo Dai</dc:creator>
    </item>
    <item>
      <title>Gaussian Wave Splatting for Computer-Generated Holography</title>
      <link>https://arxiv.org/abs/2505.06582</link>
      <description>arXiv:2505.06582v1 Announce Type: new 
Abstract: State-of-the-art neural rendering methods optimize Gaussian scene representations from a few photographs for novel-view synthesis. Building on these representations, we develop an efficient algorithm, dubbed Gaussian Wave Splatting, to turn these Gaussians into holograms. Unlike existing computer-generated holography (CGH) algorithms, Gaussian Wave Splatting supports accurate occlusions and view-dependent effects for photorealistic scenes by leveraging recent advances in neural rendering. Specifically, we derive a closed-form solution for a 2D Gaussian-to-hologram transform that supports occlusions and alpha blending. Inspired by classic computer graphics techniques, we also derive an efficient approximation of the aforementioned process in the Fourier domain that is easily parallelizable and implement it using custom CUDA kernels. By integrating emerging neural rendering pipelines with holographic display technology, our Gaussian-based CGH framework paves the way for next-generation holographic displays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06582v1</guid>
      <category>cs.GR</category>
      <category>physics.comp-ph</category>
      <category>physics.optics</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suyeon Choi, Brian Chao, Jacqueline Yang, Manu Gopakumar, Gordon Wetzstein</dc:creator>
    </item>
    <item>
      <title>A Gpu-based solution for large-scale skeletal animation simulation</title>
      <link>https://arxiv.org/abs/2505.06703</link>
      <description>arXiv:2505.06703v1 Announce Type: new 
Abstract: Skeletal animations of large-scale characters are widely used in video games. However, with a large number of characters are involved, relying on the CPU to calculate skeletal animations leads to significant performance problems. There are two main types of traditional GPU- based solutions. One is referred to as pre-baked animation texture technology. The problem with this solution is that it can only play animations from the pre-baked animation. It is impossible to perform interpolation, blending and other calculations on the animation, which affects the quality of the animations. The other solution is referred to as dedicated processing with a simple skeleton hierarchy (the number of skeleton levels &lt; 64). This option does not need to simulate and bake animation data in advance. However, performance is dramatically impaired when processing complex skeletons with too many skeleton levels (such as fluttering clothing, soft plants, dragon-like creatures, etc.). In order to solve these issues, we developed a parallel prefix tree update solution to optimize the animation update process of complex skeletons with too many levels, and combined traditional solutions to implement a GPU-based skeletal animation solution. This solution does not need to simulate and bake animation results. In addition, the performance is superior to traditional solutions for complex skeletons with too many levels. Our work can provide a new option for optimizing the performance of large-scale skeletal animation simulations, providing GPU-based skeletal animations a wider range of application scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06703v1</guid>
      <category>cs.GR</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xi Pan</dc:creator>
    </item>
    <item>
      <title>HoLa: B-Rep Generation using a Holistic Latent Representation</title>
      <link>https://arxiv.org/abs/2504.14257</link>
      <description>arXiv:2504.14257v3 Announce Type: replace 
Abstract: We introduce a novel representation for learning and generating Computer-Aided Design (CAD) models in the form of $\textit{boundary representations}$ (B-Reps). Our representation unifies the continuous geometric properties of B-Rep primitives in different orders (e.g., surfaces and curves) and their discrete topological relations in a $\textit{holistic latent}$ (HoLa) space. This is based on the simple observation that the topological connection between two surfaces is intrinsically tied to the geometry of their intersecting curve. Such a prior allows us to reformulate topology learning in B-Reps as a geometric reconstruction problem in Euclidean space. Specifically, we eliminate the presence of curves, vertices, and all the topological connections in the latent space by learning to distinguish and derive curve geometries from a pair of surface primitives via a neural intersection network. To this end, our holistic latent space is only defined on surfaces but encodes a full B-Rep model, including the geometry of surfaces, curves, vertices, and their topological relations. Our compact and holistic latent space facilitates the design of a first diffusion-based generator to take on a large variety of inputs including point clouds, single/multi-view images, 2D sketches, and text prompts. Our method significantly reduces ambiguities, redundancies, and incoherences among the generated B-Rep primitives, as well as training complexities inherent in prior multi-step B-Rep learning pipelines, while achieving greatly improved validity rate over current state of the art: 82% vs. $\approx$50%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14257v3</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yilin Liu, Duoteng Xu, Xingyao Yu, Xiang Xu, Daniel Cohen-Or, Hao Zhang, Hui Huang</dc:creator>
    </item>
    <item>
      <title>OT-Talk: Animating 3D Talking Head with Optimal Transportation</title>
      <link>https://arxiv.org/abs/2505.01932</link>
      <description>arXiv:2505.01932v2 Announce Type: replace 
Abstract: Animating 3D head meshes using audio inputs has significant applications in AR/VR, gaming, and entertainment through 3D avatars. However, bridging the modality gap between speech signals and facial dynamics remains a challenge, often resulting in incorrect lip syncing and unnatural facial movements. To address this, we propose OT-Talk, the first approach to leverage optimal transportation to optimize the learning model in talking head animation. Building on existing learning frameworks, we utilize a pre-trained Hubert model to extract audio features and a transformer model to process temporal sequences. Unlike previous methods that focus solely on vertex coordinates or displacements, we introduce Chebyshev Graph Convolution to extract geometric features from triangulated meshes. To measure mesh dissimilarities, we go beyond traditional mesh reconstruction errors and velocity differences between adjacent frames. Instead, we represent meshes as probability measures and approximate their surfaces. This allows us to leverage the sliced Wasserstein distance for modeling mesh variations. This approach facilitates the learning of smooth and accurate facial motions, resulting in coherent and natural facial animations. Our experiments on two public audio-mesh datasets demonstrate that our method outperforms state-of-the-art techniques both quantitatively and qualitatively in terms of mesh reconstruction accuracy and temporal alignment. In addition, we conducted a user perception study with 20 volunteers to further assess the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01932v2</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Xinmu Wang, Xiang Gao, Xiyun Song, Heather Yu, Zongfang Lin, Liang Peng, Xianfeng Gu</dc:creator>
    </item>
    <item>
      <title>Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation</title>
      <link>https://arxiv.org/abs/2505.02350</link>
      <description>arXiv:2505.02350v2 Announce Type: replace 
Abstract: Point cloud surface representation is a fundamental problem in computer graphics and vision. This paper presents a machine learning approach for approximating the signed distance function (SDF) of a point cloud using a sparse ellipsoidal radial basis function network, enabling a compact and accurate surface representation. Given the SDF values defined on the grid points constructed from the point cloud, our method approximates the SDF accurately with as few ellipsoidal radial basis functions (ERBFs) as possible, i.e., represents the SDF of a point cloud by sparse ERBFs. To balance sparsity and approximation precision, a dynamic multi-objective optimization strategy is introduced, which adaptively adds the regularization terms and jointly optimizes the weights, centers, shapes, and orientations of ERBFs. To improve computational efficiency, a nearest-neighbor-based data structure is employed, restricting function calculations to points near each Gaussian kernel center. The computations for each kernel are further parallelized on CUDA, which significantly improves the optimization speed. Additionally, a hierarchical octree-based refinement strategy is designed for training. Specifically, the initialization and optimization of network parameters are conducted using coarse grid points in the octree lattice structure. Subsequently, fine lattice points are progressively incorporated to accelerate model convergence and enhance training efficiency. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms previous sparse representation approaches in terms of accuracy, robustness, and computational efficiency. The corresponding executable program is publicly available at https://github.com/lianbobo/SE-RBFNet.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02350v2</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bobo Lian, Dandan Wang, Chenjian Wu, Minxin Chen</dc:creator>
    </item>
    <item>
      <title>An Active Contour Model for Silhouette Vectorization using B\'ezier Curves</title>
      <link>https://arxiv.org/abs/2505.05132</link>
      <description>arXiv:2505.05132v2 Announce Type: replace 
Abstract: In this paper, we propose an active contour model for silhouette vectorization using cubic B\'ezier curves. Among the end points of the B\'ezier curves, we distinguish between corner and regular points where the orientation of the tangent vector is prescribed. By minimizing the distance of the B\'ezier curves to the silhouette boundary, the active contour model optimizes the location of the B\'ezier curves end points, the orientation of the tangent vectors in the regular points, and the estimation of the B\'ezier curve parameters. This active contour model can use the silhouette vectorization obtained by any method as an initial guess. The proposed method significantly reduces the average distance between the silhouette boundary and its vectorization obtained by the world-class graphic software Inkscape, Adobe Illustrator, and a curvature-based vectorization method, which we introduce for comparison. Our method also allows us to impose additional regularity on the B\'ezier curves by reducing their lengths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05132v2</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <category>math.FA</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luis Alvarez, Jean-Michel Morel</dc:creator>
    </item>
    <item>
      <title>MARV: Multiview Augmented Reality Visualisation for Exploring Rich Material Data</title>
      <link>https://arxiv.org/abs/2404.14814</link>
      <description>arXiv:2404.14814v2 Announce Type: replace-cross 
Abstract: Rich material data is complex, large and heterogeneous, integrating primary and secondary non-destructive testing data for spatial, spatio-temporal, as well as high-dimensional data analyses. Currently, materials experts mainly rely on conventional desktop-based systems using 2D visualisation techniques, which render respective analyses a time-consuming and mentally demanding challenge. MARV is a novel immersive visual analytics system, which makes analyses of such data more effective and engaging in an augmented reality setting. For this purpose, MARV includes three newly designed visualisation techniques: MDD Glyphs with a Skewness Kurtosis Mapper, Temporal Evolution Tracker, and Chrono Bins, facilitating interactive exploration and comparison of multidimensional distributions of attribute data from multiple time steps. A qualitative evaluation conducted with materials experts in a real-world case study demonstrates the benefits of the proposed visualisation techniques. This evaluation revealed that combining spatial and abstract data in an immersive environment improves their analytical capabilities and facilitates the identification of patterns, anomalies, as well as changes over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14814v2</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1111/cgf.70150</arxiv:DOI>
      <dc:creator>Alexander Gall, Anja Heim, Eduard Gr\"oller, Christoph Heinzl</dc:creator>
    </item>
    <item>
      <title>One-Shot Real-to-Sim via End-to-End Differentiable Simulation and Rendering</title>
      <link>https://arxiv.org/abs/2412.00259</link>
      <description>arXiv:2412.00259v4 Announce Type: replace-cross 
Abstract: Identifying predictive world models for robots in novel environments from sparse online observations is essential for robot task planning and execution in novel environments. However, existing methods that leverage differentiable programming to identify world models are incapable of jointly optimizing the geometry, appearance, and physical properties of the scene. In this work, we introduce a novel rigid object representation that allows the joint identification of these properties. Our method employs a novel differentiable point-based geometry representation coupled with a grid-based appearance field, which allows differentiable object collision detection and rendering. Combined with a differentiable physical simulator, we achieve end-to-end optimization of world models, given the sparse visual and tactile observations of a physical motion sequence. Through a series of world model identification tasks in simulated and real environments, we show that our method can learn both simulation- and rendering-ready world models from only one robot action sequence. The code and additional videos are available at our project website: https://tianyi20.github.io/rigid-world-model.github.io/</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00259v4</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Zhu, Tianyi Xiang, Aaron Dollar, Zherong Pan</dc:creator>
    </item>
    <item>
      <title>Aerial Path Online Planning for Urban Scene Updation</title>
      <link>https://arxiv.org/abs/2505.01486</link>
      <description>arXiv:2505.01486v2 Announce Type: replace-cross 
Abstract: We present the first scene-update aerial path planning algorithm specifically designed for detecting and updating change areas in urban environments. While existing methods for large-scale 3D urban scene reconstruction focus on achieving high accuracy and completeness, they are inefficient for scenarios requiring periodic updates, as they often re-explore and reconstruct entire scenes, wasting significant time and resources on unchanged areas. To address this limitation, our method leverages prior reconstructions and change probability statistics to guide UAVs in detecting and focusing on areas likely to have changed. Our approach introduces a novel changeability heuristic to evaluate the likelihood of changes, driving the planning of two flight paths: a prior path informed by static priors and a dynamic real-time path that adapts to newly detected changes. The framework integrates surface sampling and candidate view generation strategies, ensuring efficient coverage of change areas with minimal redundancy. Extensive experiments on real-world urban datasets demonstrate that our method significantly reduces flight time and computational overhead, while maintaining high-quality updates comparable to full-scene re-exploration and reconstruction. These contributions pave the way for efficient, scalable, and adaptive UAV-based scene updates in complex urban environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01486v2</guid>
      <category>cs.RO</category>
      <category>cs.GR</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingfeng Tang, Ningna Wang, Ziyuan Xie, Jianwei Hu, Ke Xie, Xiaohu Guo, Hui Huang</dc:creator>
    </item>
  </channel>
</rss>
