<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Apr 2025 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A general sample size framework for developing or updating a clinical prediction model</title>
      <link>https://arxiv.org/abs/2504.18730</link>
      <description>arXiv:2504.18730v1 Announce Type: new 
Abstract: Aims: To propose a general sample size framework for developing or updating a clinical prediction model using any statistical or machine learning method, based on drawing samples from anticipated posterior distributions and targeting assurance in predictive performance.
  Methods: Users provide a reference model (eg, matching outcome incidence, predictor weights and c-statistic of previous models), and a (synthetic) dataset reflecting the joint distribution of candidate predictors in the target population. Then a fully simulation-based approach allows the impact of a chosen development sample size and modelling strategy to be examined. This generates thousands of models and, by applying each to the target population, leads to posterior distributions of individual predictions and model performance (degradation) metrics, to inform required sample size. To improve computation speed for penalised regression, we also propose a one-sample Bayesian analysis combining shrinkage priors with a likelihood decomposed into sample size and Fisher's information.
  Results: The framework is illustrated when developing pre-eclampsia prediction models using logistic regression (unpenalised, uniform shrinkage, lasso or ridge) and random forests. We show it encompasses existing sample size calculation criteria whilst providing model assurance probabilities, instability metrics and degradation statistics about calibration, discrimination, clinical utility, prediction error and fairness. Crucially, the required sample size depends on the users' key estimands and planned model development or updating approach.
  Conclusions: The framework generalises existing sample size proposals for model development by utilising anticipated posterior distributions conditional on a chosen sample size and development strategy. This informs the sample size required to target appropriate model performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18730v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard D Riley, Rebecca Whittle, Mohsen Sadatsafavi, Glen P. Martin, Alexander Pate, Gary S. Collins, Joie Ensor</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Properties of Generalized Ridge Estimators for Nonlinear Models</title>
      <link>https://arxiv.org/abs/2504.19018</link>
      <description>arXiv:2504.19018v1 Announce Type: new 
Abstract: Parameter estimation can result in substantial mean squared error (MSE), even when consistent estimators are used and the sample size is large. This paper addresses the longstanding statistical challenge of analyzing the bias and MSE of ridge-type estimators in nonlinear models, including duration, Poisson, and multinomial choice models, where theoretical results have been scarce. Employing a finite-sample approximation technique developed in the econometrics literature, this study derives new theoretical results showing that the generalized ridge maximum likelihood estimator (MLE) achieves lower finite-sample MSE than the conventional MLE across a broad class of nonlinear models. Importantly, the analysis extends beyond parameter estimation to model-based prediction, demonstrating that the generalized ridge estimator improves predictive accuracy relative to the generic MLE for sufficiently small penalty terms, regardless of the validity of the incorporated hypotheses. Extensive simulation studies and an empirical application involving the estimation of marginal mean and quantile treatment effects further support the superior performance and practical applicability of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19018v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masamune Iwasawa</dc:creator>
    </item>
    <item>
      <title>Selecting Optimal Candidate Profiles in Adversarial Environments Using Conjoint Analysis and Machine Learning</title>
      <link>https://arxiv.org/abs/2504.19043</link>
      <description>arXiv:2504.19043v1 Announce Type: new 
Abstract: Conjoint analysis, an application of factorial experimental design, is a popular tool in social science research for studying multidimensional preferences. In such experiments in the political analysis context, respondents are asked to choose between two hypothetical political candidates with randomly selected features, which can include partisanship, policy positions, gender and race. We consider the problem of identifying optimal candidate profiles. Because the number of unique feature combinations far exceeds the total number of observations in a typical conjoint experiment, it is impossible to determine the optimal profile exactly. To address this identification challenge, we derive an optimal stochastic intervention that represents a probability distribution of various attributes aimed at achieving the most favorable average outcome. We first consider an environment where one political party optimizes their candidate selection. We then move to the more realistic case where two political parties optimize their own candidate selection simultaneously and in opposition to each other. We apply the proposed methodology to an existing candidate choice conjoint experiment concerning vote choice for US president. We find that, in contrast to the non-adversarial approach, expected outcomes in the adversarial regime fall within range of historical electoral outcomes, with optimal strategies suggested by the method more likely to match the actual observed candidates compared to strategies derived from a non-adversarial approach. These findings indicate that incorporating adversarial dynamics into conjoint analysis may yield unique insight into social science data from experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19043v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Connor T. Jerzak, Priyanshi Chandra, Rishi Hazra</dc:creator>
    </item>
    <item>
      <title>Nonparametric Regression and Error Covariance Function Estimation -- Beyond Short-Range Dependence</title>
      <link>https://arxiv.org/abs/2504.19071</link>
      <description>arXiv:2504.19071v1 Announce Type: new 
Abstract: In nonparametric regression analysis, errors are possibly correlated in practice, and neglecting error correlation can undermine most bandwidth selection methods. When no prior knowledge or parametric form of the correlation structure is available in the random design setting, this issue has primarily been studied in the context of short-range dependent errors. When the data exhibits correlations that decay much more slowly, we introduce a special class of kernel functions and propose a procedure for selecting bandwidth in kernel-based nonparametric regression, using local linear regression as an example. Additionally, we provide a nonparametric estimate of the error covariance function, supported by theoretical results. Our simulations demonstrate significant improvements in estimating the nonparametric regression and error covariance functions, particularly in scenarios beyond short-range dependence. The practical application of our procedure is illustrated through the analysis of three datasets: cardiovascular disease mortality, life expectancy, and colon and rectum cancer mortality in the Southeastern United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19071v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sisheng Liu, Xiaoli Kong</dc:creator>
    </item>
    <item>
      <title>A new look at fiducial inference</title>
      <link>https://arxiv.org/abs/2504.19172</link>
      <description>arXiv:2504.19172v1 Announce Type: new 
Abstract: Since the idea of fiducial inference was put forward by Fisher, researchers have been attempting to place it within a rigorous and well motivated framework. It is fair to say that a general definition has remained elusive. In this paper we start with a representation of Bayesian posterior distributions provided by Doob that relies on martingales. This is explicit in defining how a true parameter value should depend on a random sample and hence an approach to "inverse probability" (Fisher, 1930). Taking this as our cue, we introduce a definition of fiducial inference that extends existing ones due to Hannig.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19172v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pier Giovanni Bissiri, Chris Holmes, Stephen Walker</dc:creator>
    </item>
    <item>
      <title>Measuring Feature-Label Dependence Using Projection Correlation Statistic</title>
      <link>https://arxiv.org/abs/2504.19180</link>
      <description>arXiv:2504.19180v1 Announce Type: new 
Abstract: Detecting dependence between variables is a crucial issue in statistical science. In this paper, we propose a novel metric called label projection correlation to measure the dependence between numerical and categorical variables. The proposed correlation does not require any conditions on numerical variables, and it is equal to zero if and only if the two variables are independent. When the numerical variable is one-dimensional, we demonstrate that the computational cost of the correlation estimation can be reduced to O(nlogn), where n is the sample size. Additionally, if the one-dimensional variable is continuous, the correlation can be simplified to a concise rank-based expression. The asymptotic theorems of the estimation are also established. Two simulated experiments are presented to demonstrate the effectiveness of the proposed correlation in feature selection. Furthermore, the metric is applied to feature selection in drivers' facial images and cancer mass-spectrometric data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19180v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixiao Liu, Pengjian Shang</dc:creator>
    </item>
    <item>
      <title>Spatial-Sign based High dimensional Change Point Inference</title>
      <link>https://arxiv.org/abs/2504.19306</link>
      <description>arXiv:2504.19306v1 Announce Type: new 
Abstract: High-dimensional changepoint inference, adaptable to diverse alternative scenarios, has attracted significant attention in recent years. In this paper, we propose an adaptive and robust approach to changepoint testing. Specifically, by generalizing the classical mean-based cumulative sum (CUSUM) statistic, we construct CUSUM statistics based on spatial medians and spatial signs. We introduce test statistics that consider the maximum and summation of the CUSUM statistics across different dimensions, respectively, and take the maximum across all potential changepoint locations. The asymptotic distributions of test statistics under the null hypothesis are derived. Furthermore, the test statistics exhibit asymptotic independence under mild conditions. Building on these results, we propose an adaptive testing procedure that combines the max-$L_\infty$-type and max-$L_2$-type statistics to achieve high power under both sparse and dense alternatives. Through numerical experiments and theoretical analysis, the proposed method demonstrates strong performance and exhibits robustness across a wide range of signal sparsity levels and heavy-tailed distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19306v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jixuan Liu, Long Feng, Liuhua Peng, Zhaojun Wang</dc:creator>
    </item>
    <item>
      <title>Generalizing the Generalized Likelihood Ratio Method Through a Push-Out Leibniz Integration Approach</title>
      <link>https://arxiv.org/abs/2504.19366</link>
      <description>arXiv:2504.19366v1 Announce Type: new 
Abstract: We generalize the generalized likelihood ratio (GLR) method through a novel push-out Leibniz integration approach. Extending the conventional push-out likelihood ratio (LR) method, our approach allows the sample space to be parameter-dependent after the change of variables. Specifically, leveraging the Leibniz integral rule enables differentiation of the parameter-dependent sample space, resulting in a surface integral in addition to the usual LR estimator, which may necessitate additional simulation. Furthermore, our approach extends to cases where the change of variables only locally exists. Notably, the derived estimator includes existing GLR estimators as special cases and is applicable to a broader class of discontinuous sample performances. Moreover, the derivation is streamlined and more straightforward, and the requisite regularity conditions are easier to understand and verify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19366v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5555/3712729.3712771</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the Winter Simulation Conference 2025</arxiv:journal_reference>
      <dc:creator>Xingyu Ren, Michael C. Fu</dc:creator>
    </item>
    <item>
      <title>Selective randomization inference for subgroup effects with continuous biomarkers</title>
      <link>https://arxiv.org/abs/2504.19380</link>
      <description>arXiv:2504.19380v1 Announce Type: new 
Abstract: Randomization tests are a popular method for testing causal effects in clinical trials with finite-sample validity. In the presence of heterogeneous treatment effects, it is often of interest to select a subgroup that benefits from the treatment, frequently by choosing a cutoff for a continuous biomarker. However, selecting the cutoff and testing the effect on the same data may fail to control the type I error. To address this, we propose using "self-contained" methods for selecting biomarker-based subgroups (cutoffs) and applying conditioning to construct valid randomization tests for the subgroup effect. Compared to sample-splitting-based randomization tests, our proposal is fully deterministic, uses the entire selected subgroup for inference, and is thus more powerful. Moreover, we demonstrate scenarios where our procedure achieves power comparable to a randomization test with oracle knowledge of the benefiting subgroup. In addition, our procedure is as computationally efficient as standard randomization tests. Empirically, we illustrate the effectiveness of our method on simulated datasets and the German Breast Cancer Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19380v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zijun Gao</dc:creator>
    </item>
    <item>
      <title>The $m$th Gini index estimator: Unbiasedness for gamma populations</title>
      <link>https://arxiv.org/abs/2504.19381</link>
      <description>arXiv:2504.19381v1 Announce Type: new 
Abstract: This paper establishes the theoretical result that the sample $m$th Gini index is an unbiased estimator of the population $m$th Gini index, introduced by Gavilan-Ruiz (2024), for gamma-distributed populations. An illustrative Monte Carlo simulation study confirms the unbiasedness of the sample $m$th Gini index estimator in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19381v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>Ridge partial correlation screening for ultrahigh-dimensional data</title>
      <link>https://arxiv.org/abs/2504.19393</link>
      <description>arXiv:2504.19393v1 Announce Type: new 
Abstract: Variable selection in ultrahigh-dimensional linear regression is
  challenging due to its high computational cost. Therefore, a
  screening step is usually conducted before variable selection to
  significantly reduce the dimension. Here we propose a novel and
  simple screening method based on ordering the absolute sample ridge
  partial correlations. The proposed method takes into account not
  only the ridge regularized estimates of the regression coefficients
  but also the ridge regularized partial variances of the predictor
  variables providing sure screening property without strong
  assumptions on the marginal correlations. Simulation study and a
  real data analysis show that the proposed method has a competitive
  performance compared with the existing screening procedures. A
  publicly available software implementing the proposed screening
  accompanies the article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19393v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Run Wang, An Nguyen, Somak Dutta, Vivekananda Roy</dc:creator>
    </item>
    <item>
      <title>Two-parameter superposable S-curves</title>
      <link>https://arxiv.org/abs/2504.19488</link>
      <description>arXiv:2504.19488v1 Announce Type: new 
Abstract: Straight line equation $y=mx$ with slope $m$, when singularly perturbed as $ay^3+y=mx$ with a positive parameter $a$, results in S-shaped curves or S-curves on a real plane. As $a\rightarrow 0$, we get back $y=mx$ which is a cumulative distribution function of a continuous uniform distribution that describes the occurrence of every event in an interval to be equally probable. As $a\rightarrow\infty$, the derivative of $y$ has finite support only at $y=0$ resembling a degenerate distribution. Based on these arguments, in this work, we propose that these S-curves can represent maximum entropy uniform distribution to a zero entropy single value. We also argue that these S-curves are superposable as they are only parametrically nonlinear but fundamentally linear. So far, the superposed forms have been used to capture the patterns of natural systems such as nonlinear dynamics of biological growth and kinetics of enzyme reactions. Here, we attempt to use the S-curve and its superposed form as a statistical model. We fit the models on a classical dataset containing flower measurements of iris plants and analyze their usefulness in pattern recognition. Based on these models, we claim that any non-uniform pattern can be represented as a singular perturbation to uniform distribution. However, our parametric estimation procedure have some limitations such as sensitivity to initial conditions depending on the data at hand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19488v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vijay Prakash S</dc:creator>
    </item>
    <item>
      <title>Advances in Approximate Bayesian Inference for Models in Epidemiology</title>
      <link>https://arxiv.org/abs/2504.19698</link>
      <description>arXiv:2504.19698v1 Announce Type: new 
Abstract: Bayesian inference methods are useful in infectious diseases modeling due to their capability to propagate uncertainty, manage sparse data, incorporate latent structures, and address high-dimensional parameter spaces. However, parameter inference through assimilation of observational data in these models remains challenging. While asymptotically exact Bayesian methods offer theoretical guarantees for accurate inference, they can be computationally demanding and impractical for real-time outbreak analysis. This review synthesizes recent advances in approximate Bayesian inference methods that aim to balance inferential accuracy with scalability. We focus on four prominent families: Approximate Bayesian Computation, Bayesian Synthetic Likelihood, Integrated Nested Laplace Approximation, and Variational Inference. For each method, we evaluate its relevance to epidemiological applications, emphasizing innovations that improve both computational efficiency and inference accuracy. We also offer practical guidance on method selection across a range of modeling scenarios. Finally, we identify hybrid exact approximate inference as a promising frontier that combines methodological rigor with the scalability needed for the response to outbreaks. This review provides epidemiologists with a conceptual framework to navigate the trade-off between statistical accuracy and computational feasibility in contemporary disease modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19698v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiahui Li, Fergus Chadwick, Ben Swallow</dc:creator>
    </item>
    <item>
      <title>Discrimination performance in illness-death models with interval-censored disease data</title>
      <link>https://arxiv.org/abs/2504.19726</link>
      <description>arXiv:2504.19726v1 Announce Type: new 
Abstract: In clinical studies, the illness-death model is often used to describe disease progression. A subject starts disease-free, may develop the disease and then die, or die directly. In clinical practice, disease can only be diagnosed at pre-specified follow-up visits, so the exact time of disease onset is often unknown, resulting in interval-censored data. This study examines the impact of ignoring this interval-censored nature of disease data on the discrimination performance of illness-death models, focusing on the time-specific Area Under the receiver operating characteristic Curve (AUC) in both incident/dynamic and cumulative/dynamic definitions. A simulation study with data simulated from Weibull transition hazards and disease state censored at regular intervals is conducted. Estimates are derived using different methods: the Cox model with a time-dependent binary disease marker, which ignores interval-censoring, and the illness-death model for interval-censored data estimated with three implementations - the piecewise-constant model from the msm package, the Weibull and M-spline models from the SmoothHazard package. These methods are also applied to a dataset of 2232 patients with high-grade soft tissue sarcoma, where the interval-censored disease state is the post-operative development of distant metastases. The results suggest that, in the presence of interval-censored disease times, it is important to account for interval-censoring not only when estimating the parameters of the model but also when evaluating the discrimination performance of the disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19726v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anja J. Rueten-Budde, Marta Spreafico, Hein Putter, Marta Fiocco</dc:creator>
    </item>
    <item>
      <title>Test-Negative Designs with Multiple Testing Sources</title>
      <link>https://arxiv.org/abs/2504.19778</link>
      <description>arXiv:2504.19778v1 Announce Type: new 
Abstract: Test-negative designs (TNDs), a form of case-cohort study, are widely used to evaluate infectious disease interventions, notably for influenza and, more recently, COVID-19 vaccines. TNDs rely on recruiting individuals who are tested for the disease of interest and comparing test-positive and test-negative individuals by exposure status (e.g., vaccination). Traditionally, TND studies focused on symptomatic individuals to minimize confounding from healthcare-seeking behavior. However, during outbreaks such as COVID-19 and Ebola, testing also occurred for asymptomatic individuals (e.g., through contact tracing), introducing potential bias when combining symptomatic and asymptomatic cases. Motivated by a trial evaluating an Ebola virus disease (EVD) vaccine, we study a specific version of this ``multiple reasons for testing" problem. In this setting, symptomatic individuals were tested under the standard TND approach, while asymptomatic close contacts of test-positive cases were also tested. We propose a simple method to estimate the common vaccine efficacy across these groups and assess whether efficacy differs by recruitment pathway. Although the EVD trial ended early due to the cessation of the outbreak, the proposed methodology remains relevant for future vaccine trials with similar designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19778v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengxin Yu, Nicholas P. Jewell</dc:creator>
    </item>
    <item>
      <title>Optimal real-time dynamic treatment regimes with application to oxytocin use in preventing postpartum hemorrhage</title>
      <link>https://arxiv.org/abs/2504.19831</link>
      <description>arXiv:2504.19831v1 Announce Type: new 
Abstract: Real-time dynamic treatment regimes (real-time DTRs) refers to decision rules that personalize patient treatment in real-time based on treatment and covariate histories. These rules are crucial for real-time clinical decision support systems and automated drug delivery systems for chronic diseases. Although considerable statistical and machine learning DTR methods have been developed, they are designed for a small number of fixed decision points, and thus can not adapt to real-time cases. This paper proposes a new semiparametric Bayesian method for estimating an optimal treatment regime in real-time, which allows for the existence of latent individual level variables. Specifically, random real-time DTRs are defined through interventional parameters, the optimal values of which are estimated by maximizing the posterior predictive utility. The proposed approach is compared with alternative methods using simulated datasets, and applied to estimate the optimal real-time oxytocin administration regime for preventing postpartum hemorrhage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19831v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiyan Zhu, Yingchun Zhou</dc:creator>
    </item>
    <item>
      <title>Interpretable additive model for analyzing high-dimensional functional time series</title>
      <link>https://arxiv.org/abs/2504.19904</link>
      <description>arXiv:2504.19904v1 Announce Type: new 
Abstract: High-dimensional functional time series offers a powerful framework for extending functional time series analysis to settings with multiple simultaneous dimensions, capturing both temporal dynamics and cross-sectional dependencies. We propose a novel, interpretable additive model tailored for such data, designed to deliver both high predictive accuracy and clear interpretability. The model features bivariate coefficient surfaces to represent relationships across panel dimensions, with sparsity introduced via penalized smoothing and group bridge regression. This enables simultaneous estimation of the surfaces and identification of significant inter-dimensional effects. Through Monte Carlo simulations and an empirical application to Japanese subnational age-specific mortality rates, we demonstrate the proposed model's superior forecasting performance and interpretability compared to existing functional time series approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19904v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haixu Wang, Tianyu Guan, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Demographic Parity-aware Individualized Treatment Rules</title>
      <link>https://arxiv.org/abs/2504.19914</link>
      <description>arXiv:2504.19914v1 Announce Type: new 
Abstract: There has been growing interest in developing optimal individualized treatment rules (ITRs) in various fields, such as precision medicine, business decision-making, and social welfare distribution. The application of ITRs within a societal context raises substantial concerns regarding potential discrimination over sensitive attributes such as age, gender, or race. To address this concern directly, we introduce the concept of demographic parity in ITRs. However, estimating an optimal ITR that satisfies the demographic parity requires solving a non-convex constrained optimization problem. To overcome these computational challenges, we employ tailored fairness proxies inspired by demographic parity and transform it into a convex quadratic programming problem. Additionally, we establish the consistency of the proposed estimator and the risk bound. The performance of the proposed method is demonstrated through extensive simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19914v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wenhai Cui (Department of Applied Mathematics, The Hong Kong Polytechnic University), Wen Su (Department of Biostatistics, City University of Hong Kong), Xiaodong Yan (Department of Mathematics and Statistics, Xi'an Jiaotong University), Donglin Zeng (Department of Biostatistics, University of Michigan), Xingqiu Zhao (Department of Applied Mathematics, The Hong Kong Polytechnic University)</dc:creator>
    </item>
    <item>
      <title>Distributed Reconstruction from Compressive Measurements: Nonconvexity and Heterogeneity</title>
      <link>https://arxiv.org/abs/2504.19919</link>
      <description>arXiv:2504.19919v1 Announce Type: new 
Abstract: The compressive sensing (CS) and 1-bit CS demonstrate superior efficiency in signal acquisition and resource conservation, while 1-bit CS achieves maximum resource efficiency through sign-only measurements. With the emergence of massive data, the distributed signal aggregation under CS and 1-bit CS measurements introduces many challenges, including nonconvexity and heterogeneity. The nonconvexity originates from the unidentifiability of signal magnitude under finite-precision measurements. The heterogeneity arises from the signal and noisy measurement on each node. To address these challenges, we propose a framework with a squared cosine similarity penalty. We address nonconvexity by an novel invex relaxation formulation to ensure the uniqueness of the global optimality. For heterogeneous signals and noisy measurements, the proposed estimate adaptively debiases through correction guided by similarity and signal-to-noise ratio (SNR) information. Our method achieves a high probability minimax-optimal convergence rate under sufficient node counts and similarity conditions, improving from $O\{(p\log{p}/n_j)^{1/2}\}$ to $O\{(p\log{p}/N)^{1/2}+p^{1/2}/n_j\}$, with signal dimension $p$, local and total sizes $n_j$ and $N$. Extensive simulations validate the method's effectiveness and performance gains in reconstructing heterogeneous signals from 1-bit CS measurements. The proposed framework maintains applicability to CS measurements while reducing communication overhead in distributed setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19919v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erbo Li Qi Qin, Yifan Sun, Liping Zhu</dc:creator>
    </item>
    <item>
      <title>Collaborative Inference for Sparse High-Dimensional Models with Non-Shared Data</title>
      <link>https://arxiv.org/abs/2504.19924</link>
      <description>arXiv:2504.19924v1 Announce Type: new 
Abstract: In modern data analysis, statistical efficiency improvement is expected via effective collaboration among multiple data holders with non-shared data. In this article, we propose a collaborative score-type test (CST) for testing linear hypotheses, which accommodates potentially high-dimensional nuisance parameters and a diverging number of constraints and target parameters. Through a careful decomposition of the Kiefer-Bahadur representation for the traditional score statistic, we identify and approximate the key components using aggregated local gradient information from each data source. In addition, we employ a two-stage partial penalization strategy to shrink the approximation error and mitigate the bias from the high-dimensional nuisance parameters. {Unlike existing methods, the CST procedure involves constrained optimization under non-shared and high-dimensional data settings, which requires novel theoretical developments.} We derive the limiting distributions for the CST statistic under the null hypothesis and the local alternatives. Besides, the CST exhibits an oracle property and achieves the global statistical efficiency. Moreover, it relaxes the stringent restrictions on the number of data sources required in the current literature. Extensive numerical studies and a real example demonstrate the effectiveness and validity of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19924v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Gu, Hanfang Yang, Songshan Yang, Hui Zou</dc:creator>
    </item>
    <item>
      <title>Semi-parametric bulk and tail regression using spline-based neural networks</title>
      <link>https://arxiv.org/abs/2504.19994</link>
      <description>arXiv:2504.19994v1 Announce Type: new 
Abstract: Semi-parametric quantile regression (SPQR) is a flexible approach to density regression that learns a spline-based representation of conditional density functions using neural networks. As it makes no parametric assumptions about the underlying density, SPQR performs well for in-sample testing and interpolation. However, it can perform poorly when modelling heavy-tailed data or when asked to extrapolate beyond the range of observations, as it fails to satisfy any of the asymptotic guarantees provided by extreme value theory (EVT). To build semi-parametric density regression models that can be used for reliable tail extrapolation, we create the blended generalised Pareto (GP) distribution, which i) provides a model for the entire range of data and, via a smooth and continuous transition, ii) benefits from exact GP upper-tails without the need for intermediate threshold selection. We combine SPQR with our blended GP to create extremal semi-parametric quantile regression (xSPQR), which provides a flexible semi-parametric approach to density regression that is compliant with traditional EVT. We handle interpretability of xSPQR through the use of model-agnostic variable importance scores, which provide the relative importance of a covariate for separately determining the bulk and tail of the conditional density. The efficacy of xSPQR is illustrated on simulated data, and an application to U.S. wildfire burnt areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19994v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reetam Majumder, Jordan Richards</dc:creator>
    </item>
    <item>
      <title>ReLU integral probability metric and its applications</title>
      <link>https://arxiv.org/abs/2504.18897</link>
      <description>arXiv:2504.18897v1 Announce Type: cross 
Abstract: We propose a parametric integral probability metric (IPM) to measure the discrepancy between two probability measures. The proposed IPM leverages a specific parametric family of discriminators, such as single-node neural networks with ReLU activation, to effectively distinguish between distributions, making it applicable in high-dimensional settings. By optimizing over the parameters of the chosen discriminator class, the proposed IPM demonstrates that its estimators have good convergence rates and can serve as a surrogate for other IPMs that use smooth nonparametric discriminator classes. We present an efficient algorithm for practical computation, offering a simple implementation and requiring fewer hyperparameters. Furthermore, we explore its applications in various tasks, such as covariate balancing for causal inference and fair representation learning. Across such diverse applications, we demonstrate that the proposed IPM provides strong theoretical guarantees, and empirical experiments show that it achieves comparable or even superior performance to other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18897v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuha Park, Kunwoong Kim, Insung Kong, Yongdai Kim</dc:creator>
    </item>
    <item>
      <title>Frequency Domain Resampling for Gridded Spatial Data</title>
      <link>https://arxiv.org/abs/2504.19337</link>
      <description>arXiv:2504.19337v1 Announce Type: cross 
Abstract: In frequency domain analysis for spatial data, spectral averages based on the periodogram often play an important role in understanding spatial covariance structure, but also have complicated sampling distributions due to complex variances from aggregated periodograms. In order to nonparametrically approximate these sampling distributions for purposes of inference, resampling can be useful, but previous developments in spatial bootstrap have faced challenges in the scope of their validity, specifically due to issues in capturing the complex variances of spatial spectral averages. As a consequence, existing frequency domain bootstraps for spatial data are highly restricted in application to only special processes (e.g. Gaussian) or certain spatial statistics. To address this limitation and to approximate a wide range of spatial spectral averages, we propose a practical hybrid-resampling approach that combines two different resampling techniques in the forms of spatial subsampling and spatial bootstrap. Subsampling helps to capture the variance of spectral averages while bootstrap captures the distributional shape. The hybrid resampling procedure can then accurately quantify uncertainty in spectral inference under mild spatial assumptions. Moreover, compared to the more studied time series setting, this work fills a gap in the theory of subsampling/bootstrap for spatial data regarding spectral average statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19337v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Souvick Bera, Daniel J. Nordman, Soutir Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Contextual Online Uncertainty-Aware Preference Learning for Human Feedback</title>
      <link>https://arxiv.org/abs/2504.19342</link>
      <description>arXiv:2504.19342v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a pivotal paradigm in artificial intelligence to align large models with human preferences. In this paper, we propose a novel statistical framework to simultaneously conduct the online decision-making and statistical inference on the optimal model using human preference data based on dynamic contextual information. Our approach introduces an efficient decision strategy that achieves both the optimal regret bound and the asymptotic distribution of the estimators. A key challenge in RLHF is handling the dependent online human preference outcomes with dynamic contexts. To address this, in the methodological aspect, we propose a two-stage algorithm starting with $\epsilon$-greedy followed by exploitations; in the theoretical aspect, we tailor anti-concentration inequalities and matrix martingale concentration techniques to derive the uniform estimation rate and asymptotic normality of the estimators using dependent samples from both stages. Extensive simulation results demonstrate that our method outperforms state-of-the-art strategies. We apply the proposed framework to analyze the human preference data for ranking large language models on the Massive Multitask Language Understanding dataset, yielding insightful results on the performance of different large language models for medical anatomy knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19342v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nan Lu, Ethan X. Fang, Junwei Lu</dc:creator>
    </item>
    <item>
      <title>SILENT: A New Lens on Statistics in Software Timing Side Channels</title>
      <link>https://arxiv.org/abs/2504.19821</link>
      <description>arXiv:2504.19821v1 Announce Type: cross 
Abstract: Cryptographic research takes software timing side channels seriously. Approaches to mitigate them include constant-time coding and techniques to enforce such practices. However, recent attacks like Meltdown [42], Spectre [37], and Hertzbleed [70] have challenged our understanding of what it means for code to execute in constant time on modern CPUs. To ensure that assumptions on the underlying hardware are correct and to create a complete feedback loop, developers should also perform \emph{timing measurements} as a final validation step to ensure the absence of exploitable side channels. Unfortunately, as highlighted by a recent study by Jancar et al. [30], developers often avoid measurements due to the perceived unreliability of the statistical analysis and its guarantees.
  In this work, we combat the view that statistical techniques only provide weak guarantees by introducing a new algorithm for the analysis of timing measurements with strong, formal statistical guarantees, giving developers a reliable analysis tool. Specifically, our algorithm (1) is non-parametric, making minimal assumptions about the underlying distribution and thus overcoming limitations of classical tests like the t-test, (2) handles unknown data dependencies in measurements, (3) can estimate in advance how many samples are needed to detect a leak of a given size, and (4) allows the definition of a negligible leak threshold $\Delta$, ensuring that acceptable non-exploitable leaks do not trigger false positives, without compromising statistical soundness. We demonstrate the necessity, effectiveness, and benefits of our approach on both synthetic benchmarks and real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19821v1</guid>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Dunsche, Patrick Bastian, Marcel Maehren, Nurullah Erinola, Robert Merget, Nicolai Bissantz, Holger Dette, J\"org Schwenk</dc:creator>
    </item>
    <item>
      <title>Stochastic Subspace via Probabilistic Principal Component Analysis for Characterizing Model Error</title>
      <link>https://arxiv.org/abs/2504.19963</link>
      <description>arXiv:2504.19963v1 Announce Type: cross 
Abstract: This paper proposes a probabilistic model of subspaces based on the probabilistic principal component analysis (PCA). Given a sample of vectors in the embedding space -- commonly known as a snapshot matrix -- this method uses quantities derived from the probabilistic PCA to construct distributions of the sample matrix, as well as the principal subspaces. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition and related model reduction methods. The stochastic subspace thus constructed can be used, for example, to characterize model-form uncertainty in computational mechanics. The proposed method has multiple desirable properties: (1) it is naturally justified by the probabilistic PCA and has analytic forms for the induced random matrix models; (2) it satisfies linear constraints, such as boundary conditions of all kinds, by default; (3) it has only one hyperparameter, which significantly simplifies training; and (4) its algorithm is very easy to implement. We compare the proposed method with existing approaches in a low-dimensional visualization example and a parametric static problem, and demonstrate its performance in a dynamics model of a space structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19963v1</guid>
      <category>cs.CE</category>
      <category>math.ST</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akash Yadav, Ruda Zhang</dc:creator>
    </item>
    <item>
      <title>Transfer Learning Under High-Dimensional Network Convolutional Regression Model</title>
      <link>https://arxiv.org/abs/2504.19979</link>
      <description>arXiv:2504.19979v1 Announce Type: cross 
Abstract: Transfer learning enhances model performance by utilizing knowledge from related domains, particularly when labeled data is scarce. While existing research addresses transfer learning under various distribution shifts in independent settings, handling dependencies in networked data remains challenging. To address this challenge, we propose a high-dimensional transfer learning framework based on network convolutional regression (NCR), inspired by the success of graph convolutional networks (GCNs). The NCR model incorporates random network structure by allowing each node's response to depend on its features and the aggregated features of its neighbors, capturing local dependencies effectively. Our methodology includes a two-step transfer learning algorithm that addresses domain shift between source and target networks, along with a source detection mechanism to identify informative domains. Theoretically, we analyze the lasso estimator in the context of a random graph based on the Erdos-Renyi model assumption, demonstrating that transfer learning improves convergence rates when informative sources are present. Empirical evaluations, including simulations and a real-world application using Sina Weibo data, demonstrate substantial improvements in prediction accuracy, particularly when labeled data in the target domain is limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19979v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liyuan Wang, Jiachen Chen, Kathryn L. Lunetta, Danyang Huang, Huimin Cheng, Debarghya Mukherjee</dc:creator>
    </item>
    <item>
      <title>Unifying Summary Statistic Selection for Approximate Bayesian Computation</title>
      <link>https://arxiv.org/abs/2206.02340</link>
      <description>arXiv:2206.02340v3 Announce Type: replace 
Abstract: Extracting low-dimensional summary statistics from large datasets is essential for efficient (likelihood-free) inference. We characterize different classes of summaries and demonstrate their importance for correctly analysing dimensionality reduction algorithms. We demonstrate that minimizing the expected posterior entropy (EPE) under the prior predictive distribution of the model subsumes many existing methods. They are equivalent to or are special or limiting cases of minimizing the EPE. We offer a unifying framework for obtaining informative summaries, provide concrete recommendations for practitioners, and propose a practical method to obtain high-fidelity summaries whose utility we demonstrate for both benchmark and practical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.02340v3</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Till Hoffmann, Jukka-Pekka Onnela</dc:creator>
    </item>
    <item>
      <title>Sliced Elastic Distance for Evaluating Amplitude and Phase Differences in Precipitation Models</title>
      <link>https://arxiv.org/abs/2307.08685</link>
      <description>arXiv:2307.08685v3 Announce Type: replace 
Abstract: Climate model evaluation plays a crucial role in ensuring the accuracy of climatological predictions. However, existing statistical evaluation methods often overlook time misalignment of events in a system's evolution, which can lead to a failure in identifying specific model deficiencies. This issue is particularly relevant for climate variables that involve time-sensitive events such as the monsoon season. To more comprehensively evaluate climate fields, we introduce a new vector-valued metric, the sliced elastic distance, through kernel convolution-derived slices. This metric simultaneously and separately accounts for spatial and temporal variability by decomposing the total distance between model simulations and observational data into three components: amplitude differences, timing variability, and bias (translation). We use the sliced elastic distance to assess CMIP6 precipitation simulations against observational data, evaluating amplitude and phase distances at both global and regional scales. In addition, we conduct a detailed phase analysis of the Indian Summer Monsoon to quantify timing biases in the onset and retreat of the monsoon season across the CMIP6 models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.08685v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert C. Garrett, Trevor Harris, Zhuo Wang, Bo Li</dc:creator>
    </item>
    <item>
      <title>Test-negative designs with various reasons for testing: statistical bias and solution</title>
      <link>https://arxiv.org/abs/2312.03967</link>
      <description>arXiv:2312.03967v4 Announce Type: replace 
Abstract: Test-negative designs are widely used for post-market evaluation of vaccine effectiveness, particularly in cases when randomized trials are not feasible. Differing from classical test-negative designs where only healthcare-seekers with symptoms are included, recent test-negative designs have involved individuals with various reasons for testing, especially in an outbreak setting. While including these data can increase sample size and hence improve precision, concerns have been raised about whether they introduce bias into the current framework of test-negative designs, thereby demanding a formal statistical examination of this modified design. In this article, using statistical derivations, causal graphs, and numerical demonstrations, we show that the standard odds ratio estimator may be biased if various reasons for testing are not accounted for. To eliminate this bias, we identify three categories of reasons for testing, including symptoms, mandatory screening, and case contact tracing, and characterize associated statistical properties and estimands. Based on our characterization, we show how to consistently estimate each estimand via stratification. Furthermore, we describe when these estimands correspond to the same vaccine effectiveness parameter, and, when appropriate, propose a stratified estimator that can incorporate multiple reasons for testing and improve precision. The performance of our proposed method is demonstrated through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03967v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengxin Yu, Tom Hongyi Liu, Kendrick Qijun Li, Nicholas Jewell, Eric Tchetgen Tchetgen, Dylan Small, Xu Shi, Bingkai Wang</dc:creator>
    </item>
    <item>
      <title>Testing composite null hypotheses with high-dimensional dependent data: a computationally scalable FDR-controlling procedure</title>
      <link>https://arxiv.org/abs/2404.05808</link>
      <description>arXiv:2404.05808v2 Announce Type: replace 
Abstract: Testing composite null hypotheses arises in various applications, such as mediation and replicability analyses. The problem becomes more challenging in high-throughput experiments where tens of thousands of features are examined simultaneously. Existing large-scale inference methods for composite null hypothesis testing often fail to explicitly incorporate the dependence structure, producing overly conservative or overly liberal results. In this work, we first develop a four-state hidden Markov model (HMM) to model a bivariate $p$-value sequence from replicability analysis with two studies, accounting for local feature dependence and study heterogeneity. Building on the HMM, we propose a multiple testing procedure that controls the false discovery rate (FDR). Extending the HMM to model the $p$-values from $n$ studies requires a computational cost of exponential order of $n$. To address this challenge, we introduce a novel e-value framework that reduces the computational cost to quadratic growth in the number of studies while maintaining FDR control. We show that the proposed method asymptotically controls the FDR and exhibits higher power numerically than competing methods at the same FDR level. In a real data application to genome-wide association studies (GWAS), our method reveals new biological insights that are overlooked by existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05808v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengfei Lyu, Xianyang Zhang, Hongyuan Cao</dc:creator>
    </item>
    <item>
      <title>Influence analyses of "designs" for evaluating inconsistency in network meta-analysis</title>
      <link>https://arxiv.org/abs/2406.16485</link>
      <description>arXiv:2406.16485v2 Announce Type: replace 
Abstract: Network meta-analysis is an evidence synthesis method for comparing the effectiveness of multiple available treatments. To justify evidence synthesis, consistency is an important assumption; however, existing methods founded on statistical testing can be substantially limited in statistical power or have several drawbacks when handling multi-arm studies. Moreover, inconsistency can be theoretically explained as design-by-treatment interactions, and the primary purpose of such analyses is to prioritize the further investigation of specific "designs" to explore sources of bias and other issues that might influence the overall results. In this article, we propose an alternative framework for evaluating inconsistency using influence diagnostics methods, which enable the influence of individual designs on the overall results to be quantitatively evaluated. We provide four new methods, the averaged studentized residual, MDFFITS, {\Phi}_d, and {\Xi}_d, to quantify the influence of individual designs through a "leave-one-design-out" analysis framework. We also propose a simple summary measure, the O-value, for prioritizing designs and interpreting these influential analyses in a straightforward manner. Furthermore, we propose another testing approach based on the leave-one-design-out analysis framework. By applying the new methods to a network meta-analysis of antihypertensive drugs and performing simulation studies, we demonstrate that the new methods accurately located potential sources of inconsistency. The proposed methods provide new insights into alternatives to existing test-based methods, especially the quantification of the influence of individual designs on the overall network meta-analysis results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16485v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kotaro Sasaki, Hisashi Noma</dc:creator>
    </item>
    <item>
      <title>Debiased Estimating Equation Method for Robust and Efficient Mendelian Randomization Using a Large Number of Correlated Weak and Invalid Instruments</title>
      <link>https://arxiv.org/abs/2408.05386</link>
      <description>arXiv:2408.05386v2 Announce Type: replace 
Abstract: Mendelian randomization (MR) is a widely used tool for causal inference in the presence of unmeasured confounders, which uses single nucleotide polymorphisms (SNPs) as instrumental variables to estimate causal effects. However, SNPs often have weak effects on complex traits, leading to bias in existing MR analysis when weak instruments are included. In addition, existing MR methods often restrict analysis to independent SNPs via linkage disequilibrium clumping and result in a loss of efficiency in estimating the causal effect due to discarding correlated SNPs. To address these issues, we propose the Debiased Estimating Equation Method (DEEM), a summary statistics-based MR approach that can incorporate a large number of correlated, weak-effect, and invalid SNPs. DEEM effectively eliminates the weak instrument bias and improves the statistical efficiency of the causal effect estimation by leveraging information from a large number of correlated SNPs. DEEM also allows for pleiotropic effects, adjusts for the winner's curse, and applies to both two-sample and one-sample MR analyses. Asymptotic analyses of the DEEM estimator demonstrate its attractive theoretical properties. Through extensive simulations and two real data examples, we demonstrate that DEEM significantly improves the efficiency and robustness of MR analysis compared with existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05386v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoyu Wang, Haoyu Zhang, Xihong Lin</dc:creator>
    </item>
    <item>
      <title>Early and Late Buzzards: Comparing Different Approaches for Quantile-based Multiple Testing in Heavy-Tailed Wildlife Research Data</title>
      <link>https://arxiv.org/abs/2409.14926</link>
      <description>arXiv:2409.14926v3 Announce Type: replace 
Abstract: In medical, ecological and psychological research, there is a need for methods to handle multiple testing, for example to consider group comparisons with more than two groups. Typical approaches that deal with multiple testing are mean or variance based which can be less effective in the context of heavy-tailed and skewed data. Here, the median is the preferred measure of location and the interquartile range (IQR) is an adequate alternative to the variance. Therefore, it may be fruitful to formulate research questions of interest in terms of the median or the IQR. For this reason, we compare different inference approaches for two-sided and non-inferiority hypotheses formulated in terms of medians or IQRs in an extensive simulation study. We consider multiple contrast testing procedures combined with a bootstrap method as well as testing procedures with Bonferroni correction. As an example of a multiple testing problem based on heavy-tailed data we analyse an ecological trait variation in early and late breeding in a medium-sized bird of prey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14926v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marl\'ene Baumeister, Merle Munko, Kai-Philipp Gladow, Marc Ditzhaus, Nayden Chakarov, Markus Pauly</dc:creator>
    </item>
    <item>
      <title>Handling Missingness, Failures, and Non-Convergence in Simulation Studies: A Review of Current Practices and Recommendations</title>
      <link>https://arxiv.org/abs/2409.18527</link>
      <description>arXiv:2409.18527v2 Announce Type: replace 
Abstract: Simulation studies are commonly used in methodological research for the empirical evaluation of data analysis methods. They generate artificial data sets under specified mechanisms and compare the performance of methods across conditions. However, simulation repetitions do not always produce valid outputs, e.g., due to non-convergence or other algorithmic failures. This phenomenon complicates the interpretation of results, especially when its occurrence differs between methods and conditions. Despite the potentially serious consequences of such "missingness", quantitative data on its prevalence and specific guidance on how to deal with it are currently limited. To this end, we reviewed 482 simulation studies published in various methodological journals and systematically assessed the prevalence and handling of missingness. We found that only 23% (111/482) of the reviewed simulation studies mention missingness, with even fewer reporting frequency (92/482 = 19%) or how it was handled (67/482 = 14%). We propose a classification of missingness and possible solutions. We give various recommendations, most notably to always quantify and report missingness, even if none was observed, to align missingness handling with study goals, and to share code and data for reproduction and reanalysis. Using a case study on publication bias adjustment methods, we illustrate common pitfalls and solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18527v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Pawel, Franti\v{s}ek Barto\v{s}, Bj\"orn S. Siepe, Anna Lohmann</dc:creator>
    </item>
    <item>
      <title>Improved computational efficiency and stability when imputing censored covariates: Analytic and numerical approaches</title>
      <link>https://arxiv.org/abs/2410.10723</link>
      <description>arXiv:2410.10723v2 Announce Type: replace 
Abstract: Imputation is a popular approach to handling censored, missing, and error-prone covariates -- all coarsened data types for which the true values are unknown. However, there are nuances to imputing these different data types based on the mechanism dominating the unobserved values and other available information. For example, in prospective studies, the time to a disease diagnosis will be incompletely observed if only some patients are diagnosed by the end of the follow-up. Some will be randomly right-censored, and patients' disease-free follow-up times must be incorporated into their imputed values. Assuming noninformative censoring, censored values are replaced with their conditional means, which are calculated by estimating the conditional survival function of the censored covariate and then integrating over it. Semiparametric approaches are common, which estimate the survival with a Cox model and then the integral with the trapezoidal rule. While these approaches offer robustness, they come at the cost of computational efficiency and stability in numerically approximating an improper integral. After modeling the survival function parametrically, we derive analytic solutions for conditional mean imputed values under many common distributions. We define stabilized calculations for other distributions. Parametric imputation using various distributions and calculations is implemented in the R package, speedyCMI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10723v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Ethan M. Alt</dc:creator>
    </item>
    <item>
      <title>Emergence of Collective Accuracy in Socially Connected Networks</title>
      <link>https://arxiv.org/abs/2411.08625</link>
      <description>arXiv:2411.08625v2 Announce Type: replace 
Abstract: We analyze the accuracy of collective decision-making in socially connected populations, where agents update binary choices through local interactions on a network. Each agent receives a private signal that is biased -- even marginally -- toward the correct alternative, and social influence mediates the aggregation of these signals. We show analytically that, in the large-population limit, the probability of a correct majority converges to a nontrivial expression involving the regularized incomplete beta function. Remarkably, this collective accuracy surpasses that of any individual agent whenever private signals are better than random, revealing that network-mediated influence can enhance, rather than impair, group performance. Our findings may inform the design of resilient decision-making systems in social, biological, and engineered networks, where accuracy must emerge from interdependent and noisy agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08625v2</guid>
      <category>stat.ME</category>
      <category>cs.GT</category>
      <category>cs.SI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Braha, Marcus A. M. de Aguiar</dc:creator>
    </item>
    <item>
      <title>Median Based Unit Weibull Distribution (MBUW): Does the Higher Order Probability Weighted Moments (PWM) Add More Information over the Lower Order PWM in Parameter Estimation</title>
      <link>https://arxiv.org/abs/2412.07404</link>
      <description>arXiv:2412.07404v2 Announce Type: replace 
Abstract: In the present paper, Probability weighted moments (PWMs) method for parameter estimation of the median based unit weibull (MBUW) distribution is discussed. The most widely used first order PWMs is compared with the higher order PWMs for parameter estimation of (MBUW) distribution. Asymptotic distribution of this PWM estimator is derived. This comparison is illustrated using real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07404v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohammed Attia</dc:creator>
    </item>
    <item>
      <title>Bayesianize Fuzziness in the Statistical Analysis of Fuzzy Data</title>
      <link>https://arxiv.org/abs/2501.18974</link>
      <description>arXiv:2501.18974v2 Announce Type: replace 
Abstract: Fuzzy data, prevalent in social sciences and other fields, capture uncertainties arising from subjective evaluations and measurement imprecision. Despite significant advancements in fuzzy statistics, a unified inferential regression-based framework remains undeveloped. Hence, we propose a novel approach for analyzing bounded fuzzy variables within a regression framework. Building on the premise that fuzzy data result from a process analogous to statistical coarsening, we introduce a conditional probabilistic approach that links observed fuzzy statistics (e.g., mode, spread) to the underlying, unobserved statistical model, which depends on external covariates. The inferential problem is addressed using Approximate Bayesian methods, mainly through a Gibbs sampler incorporating a quadratic approximation of the posterior distribution. Simulation studies and applications involving external validations are employed to evaluate the effectiveness of the proposed approach for fuzzy data analysis. By reintegrating fuzzy data analysis into a more traditional statistical framework, this work provides a significant step toward enhancing the interpretability and applicability of fuzzy statistical methods in many applicative contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18974v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio Calcagn\`i, Przemys{\l}aw Grzegorzewski, Maciej Romaniuk</dc:creator>
    </item>
    <item>
      <title>Identification and estimation of causal peer effects using instrumental variables</title>
      <link>https://arxiv.org/abs/2504.05658</link>
      <description>arXiv:2504.05658v2 Announce Type: replace 
Abstract: In social science researches, causal inference regarding peer effects often faces significant challenges due to homophily bias and contextual confounding. For example, unmeasured health conditions (e.g., influenza) and psychological states (e.g., happiness, loneliness) can spread among closely connected individuals, such as couples or siblings. To address these issues, we define four effect estimands for dyadic data to characterize direct effects and spillover effects. We employ dual instrumental variables to achieve nonparametric identification of these causal estimands in the presence of unobserved confounding. We then derive the efficient influence functions for these estimands under the nonparametric model. Additionally, we develop a triply robust and locally efficient estimator that remains consistent even under partial misspecification of the observed data model. The proposed robust estimators can be easily adapted to flexible approaches such as machine learning estimation methods, provided that certain rate conditions are satisfied. Finally, we illustrate our approach through simulations and an empirical application evaluating the peer effects of retirement on fluid cognitive perception among couples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05658v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shanshan Luo, Kang Shuai, Yechi Zhang, Wei Li, Yangbo He</dc:creator>
    </item>
    <item>
      <title>Active Learning of Computer Experiment with both Quantitative and Qualitative Inputs</title>
      <link>https://arxiv.org/abs/2504.13441</link>
      <description>arXiv:2504.13441v2 Announce Type: replace 
Abstract: Computer experiments refer to the study of real systems using complex simulation models. They have been widely used as alternatives to physical experiments. Design and analysis of computer experiments have attracted great attention in past three decades. The bulk of the work, however, often focus on experiments with only quantitative inputs. In recent years, research on design and analysis for computer experiments have gain momentum. Statistical methodology for design, modeling and inference of such experiments have been developed. In this chapter, we review some of those key developments, and propose active learning approaches for modeling, optimization, contour estimation of computer experiments with both types of inputs. Numerical studies are conducted to evaluate the performance of the proposed methods in comparison with other existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13441v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anita Shahrokhian, Xinwei Deng, C. Devon Lin</dc:creator>
    </item>
    <item>
      <title>Adaptation using spatially distributed Gaussian Processes</title>
      <link>https://arxiv.org/abs/2312.14130</link>
      <description>arXiv:2312.14130v2 Announce Type: replace-cross 
Abstract: We consider the accuracy of an approximate posterior distribution in nonparametric regression problems by combining posterior distributions computed on subsets of the data defined by the locations of the independent variables. We show that this approximate posterior retains the rate of recovery of the full data posterior distribution, where the rate of recovery adapts to the smoothness of the true regression function. As particular examples we consider Gaussian process priors based on integrated Brownian motion and the Mat\'ern kernel augmented with a prior on the length scale. Besides theoretical guarantees we present a numerical study of the methods both on synthetic and real world data. We also propose a new aggregation technique, which numerically outperforms previous approaches. Finally, we demonstrate empirically that spatially distributed methods can adapt to local regularities, potentially outperforming the original Gaussian process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14130v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Botond Szabo, Amine Hadji, Aad van der Vaart</dc:creator>
    </item>
    <item>
      <title>Negative Control Falsification Tests for Instrumental Variable Designs</title>
      <link>https://arxiv.org/abs/2312.15624</link>
      <description>arXiv:2312.15624v3 Announce Type: replace-cross 
Abstract: The validity of instrumental variable (IV) designs is typically tested using two types of falsification tests. We characterize these tests as conditional independence tests between negative control variables -- proxies for unobserved variables posing a threat to the identification -- and the IV or the outcome. We describe the conditions that variables must satisfy in order to serve as negative controls. We show that these falsification tests examine not only independence and the exclusion restriction, but also functional form assumptions. Our analysis reveals that conventional applications of these tests may flag problems even in valid IV designs. We offer implementation guidance to address these issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15624v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oren Danieli, Daniel Nevo, Itai Walk, Bar Weinstein, Dan Zeltzer</dc:creator>
    </item>
    <item>
      <title>Antithetic Multilevel Methods for Elliptic and Hypo-Elliptic Diffusions with Applications</title>
      <link>https://arxiv.org/abs/2403.13489</link>
      <description>arXiv:2403.13489v2 Announce Type: replace-cross 
Abstract: We present a new antithetic multilevel Monte Carlo (MLMC) method for the estimation of expectations with respect to laws of diffusion processes that can be elliptic or hypo-elliptic. In particular, we consider the case where one has to resort to time discretization of the diffusion and numerical simulation of such schemes. Inspired by recent works, we introduce a new MLMC estimator of expectations, which does not require any L\'evy area simulation and has a strong error of order 2 and a weak error of order 2. We then show how this approach can be used in the context of the filtering problem associated to partially observed diffusions with discrete time observations. We illustrate that in numerical simulations our new approaches provide efficiency gains for several problems, particularly when the diffusion process is hypo-elliptic, relative to some existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13489v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuga Iguchi, Ajay Jasra, Mohamed Maama, Alexandros Beskos</dc:creator>
    </item>
    <item>
      <title>Variational Bayesian Optimal Experimental Design with Normalizing Flows</title>
      <link>https://arxiv.org/abs/2404.13056</link>
      <description>arXiv:2404.13056v2 Announce Type: replace-cross 
Abstract: Bayesian optimal experimental design (OED) seeks experiments that maximize the expected information gain (EIG) in model parameters. Directly estimating the EIG using nested Monte Carlo is computationally expensive and requires an explicit likelihood. Variational OED (vOED), in contrast, estimates a lower bound of the EIG without likelihood evaluations by approximating the posterior distributions with variational forms, and then tightens the bound by optimizing its variational parameters. We introduce the use of normalizing flows (NFs) for representing variational distributions in vOED; we call this approach vOED-NFs. Specifically, we adopt NFs with a conditional invertible neural network architecture built from compositions of coupling layers, and enhanced with a summary network for data dimension reduction. We present Monte Carlo estimators to the lower bound along with gradient expressions to enable a gradient-based simultaneous optimization of the variational parameters and the design variables. The vOED-NFs algorithm is then validated in two benchmark problems, and demonstrated on a partial differential equation-governed application of cathodic electrophoretic deposition and an implicit likelihood case with stochastic modeling of aphid population. The findings suggest that a composition of 4--5 coupling layers is able to achieve lower EIG estimation bias, under a fixed budget of forward model runs, compared to previous approaches. The resulting NFs produce approximate posteriors that agree well with the true posteriors, able to capture non-Gaussian and multi-modal features effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13056v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cma.2024.117457</arxiv:DOI>
      <arxiv:journal_reference>Computer Methods in Applied Mechanics and Engineering 433 (2025) 117457</arxiv:journal_reference>
      <dc:creator>Jiayuan Dong, Christian Jacobsen, Mehdi Khalloufi, Maryam Akram, Wanjiao Liu, Karthik Duraisamy, Xun Huan</dc:creator>
    </item>
    <item>
      <title>Sequential Conditional Transport on Probabilistic Graphs for Interpretable Counterfactual Fairness</title>
      <link>https://arxiv.org/abs/2408.03425</link>
      <description>arXiv:2408.03425v3 Announce Type: replace-cross 
Abstract: In this paper, we link two existing approaches to derive counterfactuals: adaptations based on a causal graph, and optimal transport. We extend "Knothe's rearrangement" and "triangular transport" to probabilistic graphical models, and use this counterfactual approach, referred to as sequential transport, to discuss fairness at the individual level. After establishing the theoretical foundations of the proposed method, we demonstrate its application through numerical experiments on both synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03425v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agathe Fernandes Machado, Arthur Charpentier, Ewen Gallic</dc:creator>
    </item>
    <item>
      <title>On the choice of the non-trainable internal weights in random feature maps</title>
      <link>https://arxiv.org/abs/2408.03626</link>
      <description>arXiv:2408.03626v2 Announce Type: replace-cross 
Abstract: The computationally cheap machine learning architecture of random feature maps can be viewed as a single-layer feedforward network in which the weights of the hidden layer are random but fixed and only the outer weights are learned via linear regression. The internal weights are typically chosen from a prescribed distribution. The choice of the internal weights significantly impacts the accuracy of random feature maps. We address here the task of how to best select the internal weights. In particular, we consider the forecasting problem whereby random feature maps are used to learn a one-step propagator map for a dynamical system. We provide a computationally cheap hit-and-run algorithm to select good internal weights which lead to good forecasting skill. We show that the number of good features is the main factor controlling the forecasting skill of random feature maps and acts as an effective feature dimension. Lastly, we compare random feature maps with single-layer feedforward neural networks in which the internal weights are now learned using gradient descent. We find that random feature maps have superior forecasting capabilities whilst having several orders of magnitude lower computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03626v2</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pinak Mandal, Georg A. Gottwald, Nicholas Cranch</dc:creator>
    </item>
    <item>
      <title>Automatic Double Reinforcement Learning in Semiparametric Markov Decision Processes with Applications to Long-Term Causal Inference</title>
      <link>https://arxiv.org/abs/2501.06926</link>
      <description>arXiv:2501.06926v2 Announce Type: replace-cross 
Abstract: Estimating long-term causal effects from short-term data is essential for decision-making in healthcare, economics, and industry, where long-term follow-up is often infeasible. Markov Decision Processes (MDPs) offer a principled framework for modeling outcomes as sequences of states, actions, and rewards over time. We introduce a semiparametric extension of Double Reinforcement Learning (DRL) for statistically efficient, model-robust inference on linear functionals of the Q-function, such as policy values, in infinite-horizon, time-homogeneous MDPs. By imposing semiparametric structure on the Q-function, our method relaxes the strong state overlap assumptions required by fully nonparametric approaches, improving efficiency and stability. To address computational and robustness challenges of minimax nuisance estimation, we develop a novel debiased plug-in estimator based on isotonic Bellman calibration, which integrates fitted Q-iteration with an isotonic regression step. This procedure leverages the Q-function as a data-driven dimension reduction, debiases all linear functionals of interest simultaneously, and enables nonparametric inference without explicit nuisance function estimation. Bellman calibration generalizes isotonic calibration to MDPs and may be of independent interest for prediction in reinforcement learning. Finally, we show that model selection for the Q-function incurs only second-order bias and extend the adaptive debiased machine learning (ADML) framework to MDPs for data-driven learning of semiparametric structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06926v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, David Hubbard, Allen Tran, Nathan Kallus, Aur\'elien Bibaut</dc:creator>
    </item>
    <item>
      <title>Low-Rank Thinning</title>
      <link>https://arxiv.org/abs/2502.12063</link>
      <description>arXiv:2502.12063v5 Announce Type: replace-cross 
Abstract: The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12063v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annabelle Michael Carrell, Albert Gong, Abhishek Shetty, Raaz Dwivedi, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>Sanity Checking Causal Representation Learning on a Simple Real-World System</title>
      <link>https://arxiv.org/abs/2502.20099</link>
      <description>arXiv:2502.20099v2 Announce Type: replace-cross 
Abstract: We evaluate methods for causal representation learning (CRL) on a simple, real-world system where these methods are expected to work. The system consists of a controlled optical experiment specifically built for this purpose, which satisfies the core assumptions of CRL and where the underlying causal factors (the inputs to the experiment) are known, providing a ground truth. We select methods representative of different approaches to CRL and find that they all fail to recover the underlying causal factors. To understand the failure modes of the evaluated algorithms, we perform an ablation on the data by substituting the real data-generating process with a simpler synthetic equivalent. The results reveal a reproducibility problem, as most methods already fail on this synthetic ablation despite its simple data-generating process. Additionally, we observe that common assumptions on the mixing function are crucial for the performance of some of the methods but do not hold in the real data. Our efforts highlight the contrast between the theoretical promise of the state of the art and the challenges in its application. We hope the benchmark serves as a simple, real-world sanity check to further develop and validate methodology, bridging the gap towards CRL methods that work in practice. We make all code and datasets publicly available at github.com/simonbing/CRLSanityCheck</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20099v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan L. Gamella, Simon Bing, Jakob Runge</dc:creator>
    </item>
  </channel>
</rss>
