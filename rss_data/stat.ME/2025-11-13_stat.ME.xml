<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Nov 2025 14:30:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Practical considerations when designing an online learning algorithm for an app-based mHealth intervention</title>
      <link>https://arxiv.org/abs/2511.08719</link>
      <description>arXiv:2511.08719v1 Announce Type: new 
Abstract: The ubiquitous nature of mobile health (mHealth) technology has expanded opportunities for the integration of reinforcement learning into traditional clinical trial designs, allowing researchers to learn individualized treatment policies during the study. LowSalt4Life 2 (LS4L2) is a recent trial aimed at reducing sodium intake among hypertensive individuals through an app-based intervention. A reinforcement learning algorithm, which was deployed in one of the trial arms, was designed to send reminder notifications to promote app engagement in contexts where the notification would be effective, i.e., when a participant is likely to open the app in the next 30-minute and not when prior data suggested reduced effectiveness. Such an algorithm can improve app-based mHealth interventions by reducing participant burden and more effectively promoting behavior change. We encountered various challenges during the implementation of the learning algorithm, which we present as a template to solving challenges in future trials that deploy reinforcement learning algorithms. We provide template solutions based on LS4L2 for solving the key challenges of (i) defining a relevant reward, (ii) determining a meaningful timescale for optimization, (iii) specifying a robust statistical model that allows for automation, (iv) balancing model flexibility with computational cost, and (v) addressing missing values in gradually collected data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08719v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rachel T Gonzalez, Madeline R Abbott, Brahmajee Nallamothu, Scott Hummel, Michael Dorsch, Walter Dempsey</dc:creator>
    </item>
    <item>
      <title>Deep neural expected shortfall regression with tail-robustness</title>
      <link>https://arxiv.org/abs/2511.08772</link>
      <description>arXiv:2511.08772v1 Announce Type: new 
Abstract: Expected shortfall (ES), also known as conditional value-at-risk, is a widely recognized risk measure that complements value-at-risk by capturing tail-related risks more effectively. Compared with quantile regression, which has been extensively developed and applied across disciplines, ES regression remains in its early stage, partly because the traditional empirical risk minimization framework is not directly applicable. In this paper, we develop a nonparametric framework for expected shortfall regression based on a two-step approach that treats the conditional quantile function as a nuisance parameter. Leveraging the representational power of deep neural networks, we construct a two-step ES estimator using feedforward ReLU networks, which can alleviate the curse of dimensionality when the underlying functions possess hierarchical composition structures. However, ES estimation is inherently sensitive to heavy-tailed response or error distributions. To address this challenge, we integrate a properly tuned Huber loss into the neural network training, yielding a robust deep ES estimator that is provably resistant to heavy-tailedness in a non-asymptotic sense and first-order insensitive to quantile estimation errors in the first stage. Comprehensive simulation studies and an empirical analysis of the effect of El Ni\~no on extreme precipitation illustrate the accuracy and robustness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08772v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Myeonghun Yu, Kean Ming Tan, Huixia Judy Wang, Wen-Xin Zhou</dc:creator>
    </item>
    <item>
      <title>A new approach to reliability assessment based on Exploratory factor analysis</title>
      <link>https://arxiv.org/abs/2511.08952</link>
      <description>arXiv:2511.08952v1 Announce Type: new 
Abstract: We need to collect data in any science and reliability is a fundamental problem for measurement in all of science. Reliability means calculation the variance ratio. Reliability was defined as the fraction of an observed score variance that was not error. here are a lot of methods to estimated reliability. All of these indicators of dependability and stability are in contradiction to the long held belief that a problem with test-retest reliability is that it introduces memory effects of learning and practice. As a result, Kuder and Richardson developed a method named KR20 before advances in computational speed made it trivial to find the factor structure of tests, and were based upon test and item variances. These procedures were essentially short cuts for estimating reliability. Exploratory factor analysis is also a Traditional method to calculate the reliability. It focus on only one variable in the liner model, a statistical method that can be used to collect an important type of validity evidence. but in reality, we need to focus on many more variables. So we will introduce a novel method following.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08952v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shibo Diao</dc:creator>
    </item>
    <item>
      <title>rfBLT: Random Feature Bayesian Lasso Takens Model for time series forecasting</title>
      <link>https://arxiv.org/abs/2511.08957</link>
      <description>arXiv:2511.08957v1 Announce Type: new 
Abstract: Time series prediction is challenging due to our limited understanding of the underlying dynamics. Conventional models such as ARIMA and Holt's linear trend model experience difficulty in identifying nonlinear patterns in time series. In contrast, machine learning models excel at learning complex patterns and handling high-dimensional data; however, they are unable to quantify the uncertainty associated with their predictions, as statistical models do. To overcome these drawbacks, we propose Random Feature Bayesian Lasso Takens (rfBLT) for forecasting time series data. This non-parametric model captures the underlying system via the Takens' theorem and measures the degree of uncertainty with credible intervals. This is achieved by projecting delay embeddings into a higher-dimensional space via random features and applying regularization within the Bayesian framework to identify relevant terms. Our results demonstrate that the rfBLT method is comparable to traditional statistical models on simulated data, while significantly outperforming both conventional and machine learning models when evaluated on real-world data. The proposed algorithm is implemented in an R package, rfBLT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08957v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thu Nguyen, Lam Si Tung Ho</dc:creator>
    </item>
    <item>
      <title>Enhanced Rank-Based Correlation Estimation Using Smoothed Wilcoxon Rank Scores</title>
      <link>https://arxiv.org/abs/2511.08979</link>
      <description>arXiv:2511.08979v1 Announce Type: new 
Abstract: This article proposes an improved version of the Spearman rank correlation based on using Wilcoxon rank score function. A smoothed empirical cumulative distribution function (ecdf)computes the smoothed ranks and replaces the regular ranks in the Wilcoxon rank score function. The smoothed Wilcoxon rank scores are then used for estimation of the Spearman's correlation. The proposed approach is similar to the Spearman's rho estimator which uses ranks of the random samples of X and Y but the proposed method improves Spearman's approach such as handling ties and gaining higher efficiency under monotone associations. A Wald type hypothesis test has been proposed for the new estimator and the asymptotic properties are shown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08979v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.58830/ozgur.pub862.c3491</arxiv:DOI>
      <dc:creator>Feridun Tasdan, Rukiye Dagalp</dc:creator>
    </item>
    <item>
      <title>Instrumental variables system identification with $L^p$ consistency</title>
      <link>https://arxiv.org/abs/2511.09024</link>
      <description>arXiv:2511.09024v1 Announce Type: new 
Abstract: Instrumental variables (eliminate the bias that afflicts least-squares identification of dynamical systems through noisy data, yet traditionally relies on external instruments that are seldom available for nonlinear time series data. We propose an IV estimator that synthesizes instruments from the data. We establish finite-sample $L^{p}$ consistency for all $p \ge 1$ in both discrete- and continuous-time models, recovering a nonparametric $\sqrt{n}$-convergence rate. On a forced Lorenz system our estimator reduces parameter bias by 200x (continuous-time) and 500x (discrete-time) relative to least squares and reduces RMSE by up to tenfold. Because the method only assumes that the model is linear in the unknown parameters, it is broadly applicable to modern sparsity-promoting dynamics learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09024v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Kuang, Xinfan Lin</dc:creator>
    </item>
    <item>
      <title>Principled analysis of crossover designs: causal effects, efficient estimation, and robust inference</title>
      <link>https://arxiv.org/abs/2511.09215</link>
      <description>arXiv:2511.09215v1 Announce Type: new 
Abstract: Crossover designs randomly assign each unit to receive a sequence of treatments. By comparing outcomes within the same unit, these designs can effectively eliminate between-unit variation and facilitate the identification of both instantaneous effects of current treatments and carryover effects from past treatments. They are widely used in traditional biomedical studies and are increasingly adopted in modern digital platforms. However, standard analyses of crossover designs often rely on strong parametric models, making inference vulnerable to model misspecification. This paper adopts a design-based framework to analyze general crossover designs. We make two main contributions. First, we use potential outcomes to formally define the causal estimands and assumptions on the data-generating process. For any given type of crossover design and assumptions on potential outcomes, we outline a procedure for identification and estimation, emphasizing the central role of the treatment assignment mechanism in design-based inference. Second, we unify the analysis of crossover designs using least squares, with restrictions on the coefficients and weights on the units. Based on the theory, we recommend the specification of the regression function, weighting scheme, and coefficient restrictions to assess identifiability, construct efficient estimators, and estimate variances in a unified fashion. Crucially, the least squares procedure is simple to implement, and yields not only consistent and efficient point estimates but also valid variance estimates even when the working regression model is misspecified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09215v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhichao Jiang, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Valid and efficient possibilistic structure learning in Gaussian linear regression</title>
      <link>https://arxiv.org/abs/2511.09305</link>
      <description>arXiv:2511.09305v1 Announce Type: new 
Abstract: A crucial step in fitting a regression model to data is determining the model's structure, i.e., the subset of explanatory variables to be included. However, the uncertainty in this step is often overlooked due to a lack of satisfactory methods. Frequentists have no broadly applicable confidence set constructions for a model's structure, and Bayesian posterior credible sets do not achieve the desired finite-sample coverage. In this paper, we propose an extension of the possibility-theoretic inferential model (IM) framework that offers reliable, data-driven uncertainty quantification about the unknown model structure. This particular extension allows for the inclusion of incomplete prior information about the unknown structure that facilitates regularization. We prove that this new, regularized, possibilistic IM's uncertainty quantification is suitably calibrated relative to the set of joint distributions compatible with the data-generating process and assumed partial prior knowledge about the structure. This implies, among other things, that the derived confidence sets for the unknown model structure attain the nominal coverage probability in finite samples. We provide background and guidance on quantifying prior knowledge in this new context and analyze two benchmark data sets, comparing our results to those obtained by existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09305v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin, Naomi Singer, Jonathan Williams</dc:creator>
    </item>
    <item>
      <title>Nonparametric intensity estimation of spatial point processes by random forests</title>
      <link>https://arxiv.org/abs/2511.09307</link>
      <description>arXiv:2511.09307v1 Announce Type: new 
Abstract: We propose a random forest estimator for the intensity of spatial point processes, applicable with or without covariates. It retains the well-known advantages of a random forest approach, including the ability to handle a large number of covariates, out-of-bag cross-validation, and variable importance assessment. Importantly, even in the absence of covariates, it requires no border correction and adapts naturally to irregularly shaped domains and manifolds. Consistency and convergence rates are established under various asymptotic regimes, revealing the benefit of using covariates when available. Numerical experiments illustrate the methodology and demonstrate that it performs competitively with state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09307v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christophe Biscio (AAU), Fr\'ed\'eric Lavancier (CREST)</dc:creator>
    </item>
    <item>
      <title>Designing Efficient Hybrid and Single-Arm Trials: External Control Borrowing and Sample Size Calculation</title>
      <link>https://arxiv.org/abs/2511.09353</link>
      <description>arXiv:2511.09353v1 Announce Type: new 
Abstract: External controls (ECs) from historical clinical trials or real-world data have gained increasing attention as a way to augment hybrid and single-arm trials, especially when balanced randomization is infeasible. While most existing work has focused on post-trial inference using ECs, their role in prospective trial design remains less explored. We propose a unified experimental design framework that encompasses standard randomized controlled trials (RCTs), hybrid trials, and single-arm trials, focusing on sample size determination and power analysis. Building on estimators derived from the efficient influence function, we develop hybrid and single-arm design strategies that leverage comparable EC data to reduce the required sample size of the current study. We derive asymptotic variance expressions for these estimators in terms of interpretable, population-level quantities and introduce a pre-experimental variance estimation procedure to guide sample size calculation, ensuring prespecified type I error and power for the relevant hypothesis test. Simulation studies demonstrate that the proposed hybrid and single-arm designs maintain valid type I error and achieve target power across diverse scenarios while requiring substantially fewer subjects in the current study than RCT designs. A real data application further illustrates the practical utility and advantages of the proposed hybrid and single-arm designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09353v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujing Gao, Xiang Zhang, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Density ratio model for multiple types of survival data with empirical likelihood</title>
      <link>https://arxiv.org/abs/2511.09398</link>
      <description>arXiv:2511.09398v1 Announce Type: new 
Abstract: The density ratio model (DRM) is a semiparametric model that relates the distributions from multiple samples to a nonparametrically defined reference distribution via exponential tilting, with finite-dimensional parameters governing their differences in shape. When multiple types of partially observed (censored/truncated) failure time data are collected in an observational study, the DRM can be utilized to conduct a single unified analysis of the combined data. In this paper, we extend the methodology for censored length-biased/truncated data to the DRM framework and formulate the inference using empirical likelihood. We develop an EM algorithm to compute the DRM-based maximum empirical likelihood estimators of the model parameters and survival function, and assess its performance through extensive simulations under correct model specification, overspecification, and misspecification, across a range of failure-time distributions and censoring proportions. We also illustrate the efficacy of our method by analyzing the duration of time spent from admission to discharge in a Montreal-area hospital in Canada. The R code that implements our method is available on GitHub at \href{https://github.com/gozhang/DRM-combined-survival}{DRM-combined-survival}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09398v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Hugh McVittie, Archer Gong Zhang</dc:creator>
    </item>
    <item>
      <title>Family-wise error rate control in clinical trials with overlapping populations</title>
      <link>https://arxiv.org/abs/2511.09449</link>
      <description>arXiv:2511.09449v1 Announce Type: new 
Abstract: We consider clinical trials with multiple, overlapping patient populations, that test multiple treatment policies specifically tailored to these populations. Such designs may lead to multiplicity issues, as false statements will affect several populations. For type I error control, often the family-wise error rate (FWER) is controlled, which is the probability to reject at least one true null hypothesis. If the joint distribution of the test statistics is known, the FWER level can be exhausted by determining critical values or adjusted $\alpha$-levels. The adjustment is typically done under the common ANOVA assumptions. However, the performed tests are then only valid under the rather strong assumption of homogeneous null effects, i.e., when the null hypothesis applies to all subpopulations and their intersections. We show that under cancelling null effects, when heterogeneous effects cancel out in some or all subpopulations, this procedure does not provide FWER control. We also suggest different alternatives and compare them in terms of FWER control and their power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09449v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Remi Luschei, Werner Brannath</dc:creator>
    </item>
    <item>
      <title>Local Interaction Autoregressive Model for High Dimension Time Series Data</title>
      <link>https://arxiv.org/abs/2511.09542</link>
      <description>arXiv:2511.09542v1 Announce Type: new 
Abstract: High-dimensional matrix and tensor time series often exhibit local dependency, where each entry interacts mainly with a small neighborhood. Accounting for local interactions in a prediction model can greatly reduce the dimensionality of the parameter space, leading to more efficient inference and more accurate predictions. We propose a Local Interaction Autoregressive (LIAR) framework and study Separable LIAR, a variant with shared row and column components, for high-dimensional matrix/tensor time series forecasting problems. We derive a scalable parameter estimation algorithm via parallel least squares with a BIC-type neighborhood selector. Theoretically, we show consistency of neighborhood selection and derive error bounds for kernel and auto-covariance estimation. Numerical simulations show that the BIC selector recovers the true neighborhood with high success rates, the LIAR achieves small estimation errors, and the forecasts outperform matrix time-series baselines. In real data applications, a Total Electron Content (TEC) case study shows the model can identify localized spatio-temporal propagation and improved prediction as compared with non-local time series prediction models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09542v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyang Li, Yang Chen</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effects and Network Fragility: Theory and Evidence from the 2008 Financial Crisis</title>
      <link>https://arxiv.org/abs/2511.08602</link>
      <description>arXiv:2511.08602v1 Announce Type: cross 
Abstract: The 2008 financial crisis exposed fundamental vulnerabilities in interconnected banking systems, yet existing frameworks fail to integrate spatial propagation with network contagion mechanisms. This paper develops a unified spatial-network framework to analyze systemic risk dynamics, revealing three critical findings that challenge conventional wisdom. First, banking consolidation paradoxically increased systemic fragility: while bank numbers declined 47.3% from 2007 to 2023, network fragility measured by algebraic connectivity rose 315.8%, demonstrating that interconnectedness intensity dominates institutional count. Second, financial contagion propagates globally with negligible spatial decay (boundary d* = 47,474 km), contrasting sharply with localized technology diffusion (d* = 69 km)--a scale difference of 688 times. Third, traditional difference-in-differences methods overestimate crisis impacts by 73.2% when ignoring network structure, producing severely biased policy assessments. Using bilateral exposure data from 156 institutions across 28 countries (2007-2023) and employing spectral analysis of network Laplacian operators combined with spatial difference-in-differences identification, we document that crisis effects amplified over time rather than dissipating, increasing fragility 68.4% above pre-crisis levels with persistent effects through 2023. The consolidation paradox exhibits near-perfect correlation (R = 0.97) between coupling strength and systemic vulnerability, validating theoretical predictions from continuous spatial dynamics. Policy simulations demonstrate network-targeted capital requirements achieve 11.3x amplification effects versus uniform regulations. These findings establish that accurate systemic risk assessment and macroprudential policy design require explicit incorporation of both spatial propagation and network topology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08602v1</guid>
      <category>q-fin.RM</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Data reuse enables cost-efficient randomized trials of medical AI models</title>
      <link>https://arxiv.org/abs/2511.08986</link>
      <description>arXiv:2511.08986v1 Announce Type: cross 
Abstract: Randomized controlled trials (RCTs) are indispensable for establishing the clinical value of medical artificial-intelligence (AI) tools, yet their high cost and long timelines hinder timely validation as new models emerge rapidly. Here, we propose BRIDGE, a data-reuse RCT design for AI-based risk models. AI risk models support a broad range of interventions, including screening, treatment selection, and clinical alerts. BRIDGE trials recycle participant-level data from completed trials of AI models when legacy and updated models make concordant predictions, thereby reducing the enrollment requirement for subsequent trials. We provide a practical checklist for investigators to assess whether reusing data from previous trials allows for valid causal inference and preserves type I error. Using real-world datasets across breast cancer, cardiovascular disease, and sepsis, we demonstrate concordance between successive AI models, with up to 64.8% overlap in top 5% high-risk cohorts. We then simulate a series of breast cancer screening studies, where our design reduced required enrollment by 46.6%--saving over US$2.8 million--while maintaining 80% power. By transforming trials into adaptive, modular studies, our proposed design makes Level I evidence generation feasible for every model iteration, thereby accelerating cost-effective translation of AI into routine care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08986v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Nercessian, Wenxin Zhang, Alexander Schubert, Daphne Yang, Maggie Chung, Ahmed Alaa, Adam Yala</dc:creator>
    </item>
    <item>
      <title>The trade-off between model flexibility and accuracy of the Expected Threat model in football</title>
      <link>https://arxiv.org/abs/2511.09457</link>
      <description>arXiv:2511.09457v1 Announce Type: cross 
Abstract: With an average football (soccer) match recording over 3,000 on-ball events, effective use of this event data is essential for practitioners at football clubs to obtain meaningful insights. Models can extract more information from this data, and explainable methods can make them more accessible to practitioners. The Expected Threat model has been praised for its explainability and offers an accessible option. However, selecting the grid size is a challenging key design choice that has to be made when applying the Expected Threat model. Using a finer grid leads to a more flexible model that can better distinguish between different situations, but the accuracy of the estimates deteriorates with a more flexible model. Consequently, practitioners face challenges in balancing the trade-off between model flexibility and model accuracy. In this study, the Expected Threat model is analyzed from a theoretical perspective and simulations are performed based on the Markov chain of the model to examine its behavior in practice. Our theoretical results establish an upper bound on the error of the Expected Threat model for different flexibilities. Based on the simulations, a more accurate characterization of the model's error is provided, improving over the theoretical bound. Finally, these insights are converted into a practical rule of thumb to help practitioners choose the right balance between the model flexibility and the desired accuracy of the Expected Threat model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09457v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Koen W. van Arem, Jakob S\"ohl, Mirjam Bruinsma, Geurt Jongbloed</dc:creator>
    </item>
    <item>
      <title>A general framework for adaptive nonparametric dimensionality reduction</title>
      <link>https://arxiv.org/abs/2511.09486</link>
      <description>arXiv:2511.09486v1 Announce Type: cross 
Abstract: Dimensionality reduction is a fundamental task in modern data science. Several projection methods specifically tailored to take into account the non-linearity of the data via local embeddings have been proposed. Such methods are often based on local neighbourhood structures and require tuning the number of neighbours that define this local structure, and the dimensionality of the lower-dimensional space onto which the data are projected. Such choices critically influence the quality of the resulting embedding. In this paper, we exploit a recently proposed intrinsic dimension estimator which also returns the optimal locally adaptive neighbourhood sizes according to some desirable criteria. In principle, this adaptive framework can be employed to perform an optimal hyper-parameter tuning of any dimensionality reduction algorithm that relies on local neighbourhood structures. Numerical experiments on both real-world and simulated datasets show that the proposed method can be used to significantly improve well-known projection methods when employed for various learning tasks, with improvements measurable through both quantitative metrics and the quality of low-dimensional visualizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09486v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio Di Noia, Federico Ravenda, Antonietta Mira</dc:creator>
    </item>
    <item>
      <title>Asymptotic Distribution-free Change-point Detection for Modern Data Based on a New Ranking Scheme</title>
      <link>https://arxiv.org/abs/2206.03038</link>
      <description>arXiv:2206.03038v4 Announce Type: replace 
Abstract: Change-point detection (CPD) involves identifying distributional changes in a sequence of independent observations. Among nonparametric methods, rank-based methods are attractive due to their robustness and effectiveness and have been extensively studied for univariate data. However, they are not well explored for high-dimensional or non-Euclidean data. This paper proposes a new method, Rank INduced by Graph Change-Point Detection (RING-CPD), which utilizes graph-induced ranks to handle high-dimensional and non-Euclidean data. The new method is asymptotically distribution-free under the null hypothesis, and an analytic $p$-value approximation is provided for easy type-I error control. Simulation studies show that RING-CPD effectively detects change points across a wide range of alternatives and is also robust to heavy-tailed distribution and outliers. The new method is illustrated by the detection of seizures in a functional connectivity network dataset, changes of digit images, and travel pattern changes in the New York City Taxi dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.03038v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2025.3575858</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Information Theory, vol. 71, no. 8, pp. 6183-6197, 2025</arxiv:journal_reference>
      <dc:creator>Doudou Zhou, Hao Chen</dc:creator>
    </item>
    <item>
      <title>Arc travel time and path choice model estimation subsumed</title>
      <link>https://arxiv.org/abs/2210.14351</link>
      <description>arXiv:2210.14351v2 Announce Type: replace 
Abstract: We address the problem of simultaneously estimating arc travel times in a network \emph{and} parameters of route choice models for strategic and tactical network planning purposes. Hitherto, these interdependent tasks have been approached separately in the literature on road traffic networks. We illustrate that ignoring this interdependence can lead to erroneous route choice model parameter estimates. We propose a method for maximum likelihood estimation to solve the simultaneous estimation problem that is applicable to any differentiable route choice model. Moreover, our approach allows to naturally mix observations at varying levels of granularity, including noisy or partial path data. Numerical results based on real taxi data from New York City show strong performance of our method, even in comparison to a benchmark method focused solely on arc travel time estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.14351v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sobhan Mohammadpour, Emma Frejinger</dc:creator>
    </item>
    <item>
      <title>Using Subject-Level Variability to Predict Time-Varying Outcomes: Investigating the Association between Hormone Variability and BMD Trajectories over the Menopausal Transition</title>
      <link>https://arxiv.org/abs/2309.08000</link>
      <description>arXiv:2309.08000v2 Announce Type: replace 
Abstract: Women are at increased risk of bone loss during the menopausal transition; in fact, nearly 50\% of women's lifetime bone loss occurs during this time. The longitudinal relationships between estradiol (E2) and follicle-stimulating hormone (FSH), two hormones that change have characteristic changes during the menopausal transition, and bone health outcomes are complex. However, in addition to level and rate of change in E2 and FSH, variability in these hormones across the menopausal transition may be an important predictor of bone health, but this question has yet to be well explored. We introduce a joint model that characterizes individual mean estradiol (E2) trajectories and the individual residual variances and links these variances to bone health trajectories. In our application, we found that higher FSH variability was associated with declines in bone mineral density (BMD) before menopause, but this association was moderated over time after the menopausal transition. Additionally, higher mean E2, but not E2 variability, was associated with slower decreases in during the menopausal transition. We also include a simulation study that shows that naive two-stage methods often fail to propagate uncertainty in the individual-level variance estimates, resulting in estimation bias and invalid interval coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08000v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Irena Chen, Zhenke Wu, Sioban D. Harlow, Michelle M. Hood, Carrie A. Karvonen-Gutierrez, Michael R. Elliott</dc:creator>
    </item>
    <item>
      <title>Efficient Multiple-Robust Estimation for Nonresponse Data Under Informative Sampling</title>
      <link>https://arxiv.org/abs/2311.06719</link>
      <description>arXiv:2311.06719v2 Announce Type: replace 
Abstract: Nonresponse after probability sampling is a universal challenge in survey sampling, often necessitating adjustments to mitigate sampling and selection bias simultaneously. This study explored the removal of bias and effective utilization of available information, not just in nonresponse but also in the scenario of data integration, where summary statistics from other data sources are accessible. We reformulate these settings within a two-step monotone missing data framework, where the first step of missingness arises from sampling and the second originates from nonresponse. Subsequently, we derive the semiparametric efficiency bound for the target parameter. We also propose adaptive estimators utilizing methods of moments and empirical likelihood approaches to attain the lower bound. The proposed estimator exhibits both efficiency and double robustness. However, attaining efficiency with an adaptive estimator requires the correct specification of certain working models. To reinforce robustness against the misspecification of working models, we extend the property of double robustness to multiple robustness by proposing a two-step empirical likelihood method that effectively leverages empirical weights. A numerical study is undertaken to investigate the finite-sample performance of the proposed methods. We further applied our methods to a dataset from the National Health and Nutrition Examination Survey data by efficiently incorporating summary statistics from the National Health Interview Survey data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06719v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosuke Morikawa, Kenji Beppu, Wataru Aida</dc:creator>
    </item>
    <item>
      <title>Measuring Dependence between Events</title>
      <link>https://arxiv.org/abs/2403.17580</link>
      <description>arXiv:2403.17580v3 Announce Type: replace 
Abstract: Measuring dependence between two events, or equivalently between two binary random variables, amounts to expressing the dependence structure inherent in a $2\times 2$ contingency table in a real number between $-1$ and $1$. Countless such dependence measures exist, but there is little theoretical guidance on how they compare and on their advantages and shortcomings. Thus, practitioners might be overwhelmed by the problem of choosing a suitable measure. We provide a set of natural desirable properties that a proper dependence measure should fulfill. We show that Yule's Q and the little-known Cole coefficient are proper, while the most widely-used measures, the phi coefficient and all contingency coefficients, are improper. They have a severe attainability problem, that is, even under perfect dependence they can be very far away from $-1$ and $1$, and often differ substantially from the proper measures in that they understate strength of dependence. The structural reason is that these are measures for equality of events rather than of dependence. We derive the (in some instances non-standard) limiting distributions of the measures and illustrate how asymptotically valid confidence intervals can be constructed. In a case study on drug consumption we demonstrate how misleading conclusions may arise from the use of improper dependence measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17580v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc-Oliver Pohle, Timo Dimitriadis, Jan-Lukas Wermuth</dc:creator>
    </item>
    <item>
      <title>Objective Bayesian FDR</title>
      <link>https://arxiv.org/abs/2404.00256</link>
      <description>arXiv:2404.00256v3 Announce Type: replace 
Abstract: Control of false discovery rate (FDR) is important for differential gene expression experiments in typical two-color DNA microarrays. However, control can be lost with the misspecification of FDR. In this study, we developed a Bayesian procedure for analyzing large-scale datasets that objectively provides the optimal posterior FDR. We obtained the estimated null number based on the Storey's $q$-value method, and propose setting the true null number so as to match the posterior null number with the estimated null number. By using the objective Bayesian FDR, we achieved a similar posterior probability to the real FDR, indicating effective control of the FDR level. Moreover, in the estimation process, we adapt a heavy-tailed distribution so that our method can be robust against outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00256v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiko Hayashi</dc:creator>
    </item>
    <item>
      <title>Convergence Rate of Efficient MCMC with Ancillarity-Sufficiency Interweaving Strategy for Panel Data Models</title>
      <link>https://arxiv.org/abs/2507.18404</link>
      <description>arXiv:2507.18404v2 Announce Type: replace 
Abstract: Improving Markov chain Monte Carlo algorithm efficiency is essential for enhancing computational speed and inferential accuracy in Bayesian analysis. These improvements can be effectively achieved using the ancillarity-sufficiency interweaving strategy (ASIS), an effective means of achieving such gains. Herein, we provide the first rigorous theoretical justification for applying ASIS in Bayesian hierarchical panel data models. Asymptotic analysis demonstrated that when the product of prior variance of unobserved heterogeneity and cross-sectional sample size N is sufficiently large, the latent individual effects can be sampled almost independently of their global mean. This near-independence accounts for ASIS's rapid mixing behavior and highlights its suitability for modern "tall" panel datasets. We derived simple inequalities to predict which conventional data augmentation scheme-sufficient augmentation (SA) or ancillary augmentation (AA)-yields faster convergence. By interweaving SA and AA, ASIS achieves optimal geometric rate of convergence and renders the Markov chain for the global mean parameter asymptotically independent and identically distributed. Monte Carlo experiment confirm that this theoretical efficiency ordering holds even for small panels (e.g., N=10). These findings confirm the empirical success of ASIS application across finance, marketing, and sports, laying the groundwork for its extension to models with more complex covariate structures and nonGaussian specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18404v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Makoto Nakakita, Tomoki Toyabe, Teruo Nakatsuma, Takahiro Hoshino</dc:creator>
    </item>
    <item>
      <title>Discrete Chi-Square Method beats Discrete Fourier Transform and other similar parametric time series analysis methods</title>
      <link>https://arxiv.org/abs/2509.01540</link>
      <description>arXiv:2509.01540v3 Announce Type: replace 
Abstract: We compare two time series analysis methods, the Discrete Fourier Transform (DFT) and our Discrete Chi-square Method (DCM). DCM is designed for detecting signal(-s) superimposed on an unknown trend. The analytical solution for the non-linear DCM model is an ill-posed problem. We present a computational statistical well-posed solution for this problem. The backbone of DCM is the Gauss-Markov theorem that the least squares fit is the best unbiased estimator for linear regression models. DCM can not fail because this simple time series analysis method computes a massive number of linear least squares fits. Hence, the data spacing, even or uneven, is irrelevant. We use the Fisher-test to identify the best DCM model from all alternative tested DCM models. This best model must also pass our Predictivity-test. Our analyses of simulated complex data sets expose the weaknesses of DFT and the efficiency of DCM. The DFT and DCM are frequency-domain parametric methods. There are many other similar parametric methods. Just like DFT, those other methods suffer from their own particular application limitations. The list of those limitations is long. DCM suffers from none of those limitations. The performance of DCM depends only on the data set size and accuracy. DCM is an ideal forecasting method because the data set time span $(\Delta T)$ is irrelevant. It does not matter how long $(\Delta T)$ and/or complex the data set is because DCM will inevitably detect the signal(-s) and the trend when the data set size $(n)$ and/or accuracy $(\sigma)$ become adequate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01540v3</guid>
      <category>stat.ME</category>
      <category>astro-ph.IM</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lauri Jetsu</dc:creator>
    </item>
    <item>
      <title>Asynchronous Distributed ECME Algorithm for Matrix Variate Non-Gaussian Responses</title>
      <link>https://arxiv.org/abs/2510.20147</link>
      <description>arXiv:2510.20147v2 Announce Type: replace 
Abstract: We propose a regression model with matrix-variate skew-t response (REGMVST) for analyzing irregular longitudinal data with skewness, symmetry, or heavy tails. REGMVST models matrix-variate responses and predictors, with rows indexing longitudinal measurements per subject. It uses the matrix-variate skew-t (MVST) distribution to handle skewness and heavy tails, a damped exponential correlation (DEC) structure for row-wise dependencies across irregular time profiles, and leaves the column covariance unstructured. For estimation, we initially develop an ECME algorithm for parameter estimation and further mitigate its computational bottleneck via an asynchronous and distributed ECME (ADECME) extension. ADECME accelerates the E-step through parallelization, and retains the simplicity of the conditional M-step, enabling scalable inference. Simulations using synthetic data and a case study exploring matrix-variate periodontal disease endpoints derived from electronic health records demonstrate ADECME's superiority in efficiency and convergence, over the alternatives. We also provide theoretical support for our empirical observations and identify regularity assumptions for ADECME's optimal performance. An accompanying R package is available at https://github.com/rh8liuqy/STMATREG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20147v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyang Liu, Sanvesh Srivastava, Dipankar Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Optimal and computationally tractable lower bounds for logistic log-likelihoods</title>
      <link>https://arxiv.org/abs/2410.10309</link>
      <description>arXiv:2410.10309v2 Announce Type: replace-cross 
Abstract: The logit transform is arguably the most widely-employed link function beyond linear settings. This transformation routinely appears in regression models for binary data and provides a central building-block in popular methods for both classification and regression. Its widespread use, combined with the lack of analytical solutions for the optimization of objective functions involving the logit transform, still motivates active research in computational statistics. Among the directions explored, a central one has focused on the design of tangent lower bounds for logistic log-likelihoods that can be tractably optimized, while providing a tight approximation of these log-likelihoods. This has led to the development of effective minorize-maximize (MM) algorithms for point estimation, and variational schemes for approximate Bayesian inference under several logit models. However, the overarching focus has been on tangent quadratic minorizers. In fact, it is still unclear whether tangent lower bounds sharper than quadratic ones can be derived without undermining the tractability of the resulting minorizer. This article addresses such a question through the design and study of a novel piece-wise quadratic lower bound that uniformly improves any tangent quadratic minorizer, including the sharpest ones, while admitting a direct interpretation in terms of the classical generalized lasso problem. As illustrated in realistic empirical studies, such a sharper bound not only improves the speed of convergence of common MM schemes for penalized maximum likelihood estimation, but also yields tractable variational Bayes (VB) approximations with higher accuracy relative to those obtained under popular quadratic bounds employed in VB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10309v2</guid>
      <category>stat.ML</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niccol\`o Anceschi, Cristian Castiglione, Tommaso Rigon, Giacomo Zanella, Daniele Durante</dc:creator>
    </item>
    <item>
      <title>Testing and Improving the Robustness of Amortized Bayesian Inference for Cognitive Models</title>
      <link>https://arxiv.org/abs/2412.20586</link>
      <description>arXiv:2412.20586v2 Announce Type: replace-cross 
Abstract: Contaminant observations and outliers often cause problems when estimating the parameters of cognitive models, which are statistical models representing cognitive processes. In this study, we test and improve the robustness of parameter estimation using amortized Bayesian inference (ABI) with neural networks. To this end, we conduct systematic analyses on a toy example and analyze both synthetic and real data using a popular cognitive model, the Drift Diffusion Models (DDM). First, we study the sensitivity of ABI to contaminants with tools from robust statistics: the empirical influence function and the breakdown point. Next, we propose a data augmentation or noise injection approach that incorporates a contamination distribution into the data-generating process during training. We examine several candidate distributions and evaluate their performance and cost in terms of accuracy and efficiency loss relative to a standard estimator. Introducing contaminants from a Cauchy distribution during training considerably increases the robustness of the neural density estimator as measured by bounded influence functions and a much higher breakdown point. Overall, the proposed method is straightforward and practical to implement and has a broad applicability in fields where outlier detection or removal is challenging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20586v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufei Wu, Stefan T. Radev, Francis Tuerlinckx</dc:creator>
    </item>
    <item>
      <title>Federated Variational Inference for Bayesian Mixture Models</title>
      <link>https://arxiv.org/abs/2502.12684</link>
      <description>arXiv:2502.12684v2 Announce Type: replace-cross 
Abstract: We present a federated learning approach for Bayesian model-based clustering of large-scale binary and categorical datasets. We introduce a principled 'divide and conquer' inference procedure using variational inference with local merge and delete moves within batches of the data in parallel, followed by 'global' merge moves across batches to find global clustering structures. We show that these merge moves require only summaries of the data in each batch, enabling federated learning across local nodes without requiring the full dataset to be shared. Empirical results on simulated and benchmark datasets demonstrate that our method performs well in comparison to existing clustering algorithms. We validate the practical utility of the method by applying it to large scale electronic health record (EHR) data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12684v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jackie Rao, Francesca L. Crowe, Tom Marshall, Sylvia Richardson, Paul D. W. Kirk</dc:creator>
    </item>
    <item>
      <title>Possibilistic inferential models: a review</title>
      <link>https://arxiv.org/abs/2507.09007</link>
      <description>arXiv:2507.09007v2 Announce Type: replace-cross 
Abstract: An inferential model (IM) is a model describing the construction of provably reliable, data-driven uncertainty quantification and inference about relevant unknowns. IMs and Fisher's fiducial argument have similar objectives, but a fundamental distinction between the two is that the former doesn't require that uncertainty quantification be probabilistic, offering greater flexibility and allowing for a proof of its reliability. Important recent developments have been made thanks in part to newfound connections with the imprecise probability literature, in particular, possibility theory. The brand of possibilistic IMs studied here are straightforward to construct, have very strong frequentist-like reliability properties, and offer fully conditional, Bayesian-like (imprecise) probabilistic reasoning. This paper reviews these key recent developments, describing the new theory, methods, and computational tools. A generalization of the basic possibilistic IM is also presented, making new and unexpected connections with ideas in modern statistics and machine learning, e.g., bootstrap and conformal prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09007v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
  </channel>
</rss>
