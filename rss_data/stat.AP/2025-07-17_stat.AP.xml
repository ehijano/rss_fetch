<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Jul 2025 04:01:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Bayesian Spatio-Temporal Model of Temperature- and Humidity-Related Mortality Using High-Resolution Climate Data</title>
      <link>https://arxiv.org/abs/2507.12643</link>
      <description>arXiv:2507.12643v1 Announce Type: new 
Abstract: In this study, we introduce a novel and comprehensive extension of a Bayesian spatio-temporal disease mapping model that explicitly accounts for gender-specific effects of meteorological exposures. Leveraging fine-scale weekly mortality and high-resolution climate data from Austria (2002 to 2019), we assess how interactions between temperature, humidity, age, and gender influence mortality patterns. Our approach goes beyond conventional modelling by capturing complex dependencies through structured interactions across space-time, space-age, and age-time dimensions, allowing us to capture complex demographic and environmental dependencies. The analysis identifies district-level mortality patterns and quantifies climate-related risks on a weekly basis, offering new insights for public health surveillance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12643v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Corinna Perchtold, Julia Eisenberg, Philipp Otto</dc:creator>
    </item>
    <item>
      <title>Short-term CO2 emissions forecasting: insight from the Italian electricity market</title>
      <link>https://arxiv.org/abs/2507.12992</link>
      <description>arXiv:2507.12992v1 Announce Type: new 
Abstract: This study investigates the short-term forecasting of carbon emissions from electricity generation in the Italian power market. Using hourly data from 2021 to 2023, several statistical models and forecast combination methods are evaluated and compared at the national and zonal levels. Four main model classes are considered: (i) linear parametric models, such as seasonal autoregressive integrated moving average and its exogenousvariable extension; (ii) functional parametric models, including seasonal functional autoregressive models, with and without exogenous variables; (iii) (semi) non-parametric and possibly non-linear models, notably the generalised additive model (GAM) and TBATS (trigonometric seasonality, Box-Cox transformation, ARMA errors, trend, and seasonality); and (iv) a semi-functional approach based on the K-nearest neighbours. Forecast combinations are also considered including simple averaging, the optimal Bates and Granger weighting scheme, and a selection-based strategy that chooses the best model for each hour. The overall findings indicate that GAM reports the most accurate forecasts during the daytime hours, while functional parametric models perform best during the early morning period. GAM can also be considered the best individual model according to the hourly average root mean square error and the Diebold-Mariano (DM) test. Among the combination methods, the selection-based approach consistently outperforms all individual models and forecast combinations, resulting in a substantial reduction in the root mean square error compared to single models and a primary choice for the DM test. These findings underline the value of hybrid forecasting frameworks in improving the accuracy and reliability of short-term carbon emissions predictions in power systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12992v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pierdomenico Duttilo, Francesco Lisi</dc:creator>
    </item>
    <item>
      <title>Differentially Private Conformal Prediction via Quantile Binary Search</title>
      <link>https://arxiv.org/abs/2507.12497</link>
      <description>arXiv:2507.12497v1 Announce Type: cross 
Abstract: Most Differentially Private (DP) approaches focus on limiting privacy leakage from learners based on the data that they are trained on, there are fewer approaches that consider leakage when procedures involve a calibration dataset which is common in uncertainty quantification methods such as Conformal Prediction (CP). Since there is a limited amount of approaches in this direction, in this work we deliver a general DP approach for CP that we call Private Conformity via Quantile Search (P-COQS). The proposed approach adapts an existing randomized binary search algorithm for computing DP quantiles in the calibration phase of CP thereby guaranteeing privacy of the consequent prediction sets. This however comes at a price of slightly under-covering with respect to the desired $(1 - \alpha)$-level when using finite-sample calibration sets (although broad empirical results show that the P-COQS generally targets the required level in the considered cases). Confirming properties of the adapted algorithm and quantifying the approximate coverage guarantees of the consequent CP, we conduct extensive experiments to examine the effects of privacy noise, sample size and significance level on the performance of our approach compared to existing alternatives. In addition, we empirically evaluate our approach on several benchmark datasets, including CIFAR-10, ImageNet and CoronaHack. Our results suggest that the proposed method is robust to privacy noise and performs favorably with respect to the current DP alternative in terms of empirical coverage, efficiency, and informativeness. Specifically, the results indicate that P-COQS produces smaller conformal prediction sets while simultaneously targeting the desired coverage and privacy guarantees in all these experimental settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12497v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ogonnaya M. Romanus, Roberto Molinari</dc:creator>
    </item>
    <item>
      <title>Ranking Vectors Clustering: Theory and Applications</title>
      <link>https://arxiv.org/abs/2507.12583</link>
      <description>arXiv:2507.12583v1 Announce Type: cross 
Abstract: We study the problem of clustering ranking vectors, where each vector represents preferences as an ordered list of distinct integers. Specifically, we focus on the k-centroids ranking vectors clustering problem (KRC), which aims to partition a set of ranking vectors into k clusters and identify the centroid of each cluster. Unlike classical k-means clustering (KMC), KRC constrains both the observations and centroids to be ranking vectors. We establish the NP-hardness of KRC and characterize its feasible set. For the single-cluster case, we derive a closed-form analytical solution for the optimal centroid, which can be computed in linear time. To address the computational challenges of KRC, we develop an efficient approximation algorithm, KRCA, which iteratively refines initial solutions from KMC, referred to as the baseline solution. Additionally, we introduce a branch-and-bound (BnB) algorithm for efficient cluster reconstruction within KRCA, leveraging a decision tree framework to reduce computational time while incorporating a controlling parameter to balance solution quality and efficiency. We establish theoretical error bounds for KRCA and BnB. Through extensive numerical experiments on synthetic and real-world datasets, we demonstrate that KRCA consistently outperforms baseline solutions, delivering significant improvements in solution quality with fast computational times. This work highlights the practical significance of KRC for personalization and large-scale decision making, offering methodological advancements and insights that can be built upon in future studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12583v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Fattahi, Ali Eshragh, Babak Aslani, Meysam Rabiee</dc:creator>
    </item>
    <item>
      <title>mNARX+: A surrogate model for complex dynamical systems using manifold-NARX and automatic feature selection</title>
      <link>https://arxiv.org/abs/2507.13301</link>
      <description>arXiv:2507.13301v1 Announce Type: cross 
Abstract: We propose an automatic approach for manifold nonlinear autoregressive with exogenous inputs (mNARX) modeling that leverages the feature-based structure of functional-NARX (F-NARX) modeling. This novel approach, termed mNARX+, preserves the key strength of the mNARX framework, which is its expressivity allowing it to model complex dynamical systems, while simultaneously addressing a key limitation: the heavy reliance on domain expertise to identify relevant auxiliary quantities and their causal ordering. Our method employs a data-driven, recursive algorithm that automates the construction of the mNARX model sequence. It operates by sequentially selecting temporal features based on their correlation with the model prediction residuals, thereby automatically identifying the most critical auxiliary quantities and the order in which they should be modeled. This procedure significantly reduces the need for prior system knowledge. We demonstrate the effectiveness of the mNARX+ algorithm on two case studies: a Bouc-Wen oscillator with strong hysteresis and a complex aero-servo-elastic wind turbine simulator. The results show that the algorithm provides a systematic, data-driven method for creating accurate and stable surrogate models for complex dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13301v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Sch\"ar, S. Marelli, B. Sudret</dc:creator>
    </item>
    <item>
      <title>A Bayesian Basket Trial Design Using Local Power Prior</title>
      <link>https://arxiv.org/abs/2312.15352</link>
      <description>arXiv:2312.15352v3 Announce Type: replace 
Abstract: In recent years, basket trials, which allow the evaluation of an experimental therapy across multiple tumor types within a single protocol, have gained prominence in early-phase oncology development. Unlike traditional trials, which evaluate each tumor type separately and often face challenges with limited sample sizes, basket trials offer the advantage of borrowing information across various tumor types to enhance statistical power. However, a key challenge in designing basket trials is determining the appropriate extent of information borrowing while maintaining an acceptable type I error rate control. In this paper, we propose a novel 3-component local power prior (local-PP) framework that introduces a dynamic and flexible approach to information borrowing. The framework consists of three components: global borrowing control, pairwise similarity assessments, and a borrowing threshold, allowing for tailored and interpretable borrowing across heterogeneous tumor types. Unlike many existing Bayesian methods that rely on computationally intensive Markov chain Monte Carlo (MCMC) sampling, the proposed approach provides a closed-form solution, significantly reducing computation time in large-scale simulations for evaluating operating characteristics. Extensive simulations demonstrate that the proposed local-PP framework performs comparably to more complex methods while significantly shortening computation time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15352v3</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiming Zhou, Rex Shen, Sutan Wu, Philip He</dc:creator>
    </item>
    <item>
      <title>Prediction intervals for overdispersed binomial endpoints and their application to toxicological historical control data</title>
      <link>https://arxiv.org/abs/2407.13296</link>
      <description>arXiv:2407.13296v4 Announce Type: replace 
Abstract: For toxicology studies, the validation of the concurrent control group by historical control data (HCD) has become requirements. This validation is usually done by historical control limits (HCL), which should cover the observations of the concurrent control with a predefined level of confidence. In many applications, HCL are applied to dichotomous data, e.g. the number of rats with a tumor vs. the number of rats without a tumor (carcinogenicity studies) or the number of cells with a micronucleus out of a total number of cells. Dichotomous HCD may be overdispersed and can be heavily right- (or left-) skewed, which is usually not taken into account in the practical applications of HCL. To overcome this problem, four different prediction intervals (two frequentist, two Bayesian), that can be applied to such data, are proposed. Based on comprehensive Monte-Carlo simulations, the coverage probabilities of the proposed prediction intervals were compared to heuristical HCL typically used in daily toxicological routine (historical range, limits of the np-chart, mean plus minus 2 SD). Our simulations reveal, that frequentist bootstrap calibrated prediction intervals control the type-1-error best, but, also prediction intervals calculated based on Bayesian generalized linear mixed models appear to be practically applicable. Contrary, all heuristics fail to control the type-1-error. The application of HCL is demonstrated based on a real life data set containing historical controls from long-term carcinogenicity studies run on behalf of the U.S. National Toxicology Program. The proposed frequentist prediction intervals are publicly available from the R package predint, whereas R code for the computation of the two Bayesian prediction intervals is provided via GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13296v4</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Max Menssen, Jonathan Rathjens</dc:creator>
    </item>
    <item>
      <title>Modeling Firm-Level ESG-Sentiment Interactions in Stock Returns: Evidence from 16 Companies Using Retrofitted Word Embeddings</title>
      <link>https://arxiv.org/abs/2507.11485</link>
      <description>arXiv:2507.11485v3 Announce Type: replace 
Abstract: This study investigates how emotion-specific sentiment embedded in financial news headlines interacts with firm-level Environmental, Social, and Governance (ESG) ratings to influence stock return behavior. Addressing key methodological gaps in existing literature, the analysis leverages Retrofitted Word Embeddings to encode discrete emotional cues tailored to ESG-relevant narratives. Unlike prior studies that rely on lexicon-based or transformer-based models, this approach explicitly incorporates domain-specific emotional semantics while accounting for firm-level heterogeneity and temporal sentiment fluctuations. Using a dataset of 16 multinational firms and sentiment data extracted from Seeking Alpha headlines, the study tests three hypotheses: (1) emotion-specific sentiment independently predicts stock returns; (2) the moderating effect of sentiment varies across ESG dimensions; and (3) positive (negative) sentiment amplifies (dampens) ESG performance effects. The analysis implements a dual sentiment aggregation strategy and introduces a triple-significance filtering criterion to identify robust interactions. Results support Hypotheses 1 and 2, with emotions such as anticipation and trust showing consistent associations with return variation across firms and ESG categories. However, findings for Hypothesis 3 are mixed: while some sentiment-ESG combinations align with theoretical expectations, many contradictory interactions exhibit stronger effects. In this study, retrofitted embeddings outperform the NRC Emotion Lexicon in explaining stock return variation within ESG-sentiment interaction models, underscoring the value of emotional nuance in ESG-finance modeling. These results underscore the importance of emotion-sensitive sentiment modeling in understanding investor behavior and ESG-related stock price movements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11485v3</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sangdeok Lee</dc:creator>
    </item>
    <item>
      <title>Demographic Distribution Matching between real world and virtual phantom population</title>
      <link>https://arxiv.org/abs/2507.11511</link>
      <description>arXiv:2507.11511v3 Announce Type: replace 
Abstract: Virtual imaging trials (VITs) offer scalable and cost-effective tools for evaluating imaging systems and protocols. However, their translational impact depends on rigorous comparability between virtual and real-world populations. This study introduces DISTINCT (Distributional Subsampling for Covariate-Targeted Alignment), a statistical framework for selecting demographically aligned subsamples from large clinical datasets to support robust comparisons with virtual cohorts. We applied DISTINCT to the National Lung Screening Trial (NLST) and a companion virtual trial dataset (VLST). The algorithm jointly aligned typical continuous (age, BMI) and categorical (sex, race, ethnicity) variables by constructing multidimensional bins based on discretized covariates. For a given target size, DISTINCT samples individuals to match the joint demographic distribution of the reference population. We evaluated the demographic similarity between VLST and progressively larger NLST subsamples using Wasserstein and Kolmogorov-Smirnov (K-S) distances to identify the maximal subsample size with acceptable alignment. The algorithm identified a maximal aligned NLST subsample of 9,974 participants, preserving demographic similarity to the VLST population. Receiver operating characteristic (ROC) analysis using risk scores for lung cancer detection showed that area under the curve (AUC) estimates stabilized beyond 6,000 participants, confirming the sufficiency of aligned subsamples for virtual imaging trial evaluation. Stratified AUC analysis revealed substantial performance variation across demographic subgroups, reinforcing the importance of covariate alignment in comparative studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11511v3</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, Fakrul Islam Tushar, Lavsen Dahal, Liesbeth Vancoillie, Kyle J. Lafata, Ehsan Samei, Joseph Y. Lo, Sheng Luo</dc:creator>
    </item>
    <item>
      <title>Rank-adaptive covariance testing with applications to genomics and neuroimaging</title>
      <link>https://arxiv.org/abs/2309.10284</link>
      <description>arXiv:2309.10284v3 Announce Type: replace-cross 
Abstract: In biomedical studies, testing for differences in covariance offers scientific insights beyond mean differences, especially when differences are driven by complex joint behavior between features. However, when differences in joint behavior are weakly dispersed across many dimensions and arise from differences in low-rank structures within the data, as is often the case in genomics and neuroimaging, existing two-sample covariance testing methods may suffer from power loss. The Ky-Fan(k) norm, defined by the sum of the top Ky-Fan(k) singular values, is a simple and intuitive matrix norm able to capture signals caused by differences in low-rank structures between matrices, but its statistical properties in hypothesis testing have not been studied well. In this paper, we investigate the behavior of the Ky-Fan(k) norm in two-sample covariance testing. Ultimately, we propose a novel methodology, Rank-Adaptive Covariance Testing (RACT), which is able to leverage differences in low-rank structures found in the covariance matrices of two groups in order to maximize power. RACT uses permutation for statistical inference, ensuring an exact Type I error control. We validate RACT in simulation studies and evaluate its performance when testing for differences in gene expression networks between two types of lung cancer, as well as testing for covariance heterogeneity in diffusion tensor imaging (DTI) data taken on two different scanner types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10284v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Veitch, Yinqiu He, Jun Young Park</dc:creator>
    </item>
    <item>
      <title>A Bayesian Joint Modelling for Misclassified Interval-censoring and Competing Risks</title>
      <link>https://arxiv.org/abs/2404.09362</link>
      <description>arXiv:2404.09362v2 Announce Type: replace-cross 
Abstract: In active surveillance of prostate cancer, cancer progression is interval-censored and the examination to detect progression is subject to misclassification, usually false negatives. Meanwhile, patients may initiate early treatment before progression detection, constituting a competing risk. We developed the Misclassification-Corrected Interval-censored Cause-specific Joint Model (MCICJM) to estimate the association between longitudinal biomarkers and cancer progression in this setting. The sensitivity of the examination is considered in the likelihood of this model via a parameter that may be set to a specific value if the sensitivity is known, or for which a prior distribution can be specified if the sensitivity is unknown. Our simulation results show that misspecification of the sensitivity parameter or ignoring it entirely impacts the model parameters, especially the parameter uncertainty and the baseline hazards. Moreover, specification of a prior distribution for the sensitivity parameter may reduce the risk of misspecification in settings where the exact sensitivity is unknown, but may cause identifiability issues. Thus, imposing restrictions on the baseline hazards is recommended. A trade-off between modelling with a sensitivity constant at the risk of misspecification and a sensitivity prior at the cost of flexibility needs to be decided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09362v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhenwei Yang, Dimitris Rizopoulos, Eveline A. M. Heijnsdijk, Lisa F. Newcomb, Nicole S. Erler</dc:creator>
    </item>
    <item>
      <title>Nonparametric IPSS: Fast, flexible feature selection with false discovery control</title>
      <link>https://arxiv.org/abs/2410.02208</link>
      <description>arXiv:2410.02208v3 Announce Type: replace-cross 
Abstract: Feature selection is a critical task in machine learning and statistics. However, existing feature selection methods either (i) rely on parametric methods such as linear or generalized linear models, (ii) lack theoretical false discovery control, or (iii) identify few true positives. Here, we introduce a general feature selection method with finite-sample false discovery control based on applying integrated path stability selection (IPSS) to arbitrary feature importance scores. The method is nonparametric whenever the importance scores are nonparametric, and it estimates q-values, which are better suited to high-dimensional data than p-values. We focus on two special cases using importance scores from gradient boosting (IPSSGB) and random forests (IPSSRF). Extensive nonlinear simulations with RNA sequencing data show that both methods accurately control the false discovery rate and detect more true positives than existing methods. Both methods are also efficient, running in under 20 seconds when there are 500 samples and 5000 features. We apply IPSSGB and IPSSRF to detect microRNAs and genes related to cancer, finding that they yield better predictions with fewer features than existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02208v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/bioinformatics/btaf299</arxiv:DOI>
      <arxiv:journal_reference>Bioinformatics (2025)</arxiv:journal_reference>
      <dc:creator>Omar Melikechi, David B. Dunson, Jeffrey W. Miller</dc:creator>
    </item>
  </channel>
</rss>
