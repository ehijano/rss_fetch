<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Nov 2025 05:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>An improved approximation algorithm for k-Median</title>
      <link>https://arxiv.org/abs/2511.12230</link>
      <description>arXiv:2511.12230v1 Announce Type: new 
Abstract: We give a polynomial-time approximation algorithm for the (not necessarily metric) $k$-Median problem. The algorithm is an $\alpha$-size-approximation algorithm for $\alpha &lt; 1 + 2 \ln(n/k)$. That is, it guarantees a solution having size at most $\alpha\times k$, and cost at most the cost of any size-$k$ solution. This is the first polynomial-time approximation algorithm to match the well-known bounds of $H_\Delta$ and $1 + \ln(n/k)$ for unweighted Set Cover (a special case) within a constant factor. It matches these bounds within a factor of 2. The algorithm runs in time $O(k m \log(n/k) \log m)$, where $n$ is the number of customers and $m$ is the instance size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12230v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neal E. Young</dc:creator>
    </item>
    <item>
      <title>Shortcutting for Negative-Weight Shortest Path</title>
      <link>https://arxiv.org/abs/2511.12714</link>
      <description>arXiv:2511.12714v1 Announce Type: new 
Abstract: Consider the single-source shortest paths problem on a directed graph with real-valued edge weights. We solve this problem in $O(n^{2.5}\log^{4.5}n)$ time, improving on prior work of Fineman (STOC 2024) and Huang-Jin-Quanrud (SODA 2025, 2026) on dense graphs. Our main technique is an shortcutting procedure that iteratively reduces the number of negative-weight edges along shortest paths by a constant factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12714v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Z. Li, Jason Li, Satish Rao, Junkai Zhang</dc:creator>
    </item>
    <item>
      <title>Indirect Coflow Scheduling</title>
      <link>https://arxiv.org/abs/2511.12854</link>
      <description>arXiv:2511.12854v1 Announce Type: new 
Abstract: We consider routing in reconfigurable networks, which is also known as coflow scheduling in the literature. The algorithmic literature generally (perhaps implicitly) assumes that the amount of data to be transferred is large. Thus the standard way to model a collection of requested data transfers is by an integer demand matrix $D$, where the entry in row $i$ and column $j$ of $D$ is an integer representing the amount of information that the application wants to send from machine/node $i$ to machine/node $j$. A feasible coflow schedule is then a sequence of matchings, which represent the sequence of data transfers that covers $D$. In this work, we investigate coflow scheduling when the size of some of the requested data transfers may be small relative to the amount of data that can be transferred in one round. fractional matchings and/or that employ indirect routing, and compare the relative utility of these options. We design algorithms that perform much better for small demands than the algorithms in the literature that were designed for large data transfers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12854v1</guid>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Lindermayr, Kirk Pruhs, Andr\'ea W. Richa, Tegan Wilson</dc:creator>
    </item>
    <item>
      <title>Maximal Palindromes in MPC: Simple and Optimal</title>
      <link>https://arxiv.org/abs/2511.13014</link>
      <description>arXiv:2511.13014v1 Announce Type: new 
Abstract: In the classical longest palindromic substring (LPS) problem, we are given a string $S$ of length $n$, and the task is to output a longest palindromic substring in $S$. Gilbert, Hajiaghayi, Saleh, and Seddighin [SPAA 2023] showed how to solve the LPS problem in the Massively Parallel Computation (MPC) model in $\mathcal{O}(1)$ rounds using $\mathcal{\widetilde{O}}(n)$ total memory, with $\mathcal{\widetilde{O}}(n^{1-\epsilon})$ memory per machine, for any $\epsilon \in (0,0.5]$.
  We present a simple and optimal algorithm to solve the LPS problem in the MPC model in $\mathcal{O}(1)$ rounds. The total time and memory are $\mathcal{O}(n)$, with $\mathcal{O}(n^{1-\epsilon})$ memory per machine, for any $\epsilon \in (0,0.5]$. A key attribute of our algorithm is its ability to compute all maximal palindromes in the same complexities. Furthermore, our new insights allow us to bypass the constraint $\epsilon \in (0,0.5]$ in the Adaptive MPC model. Our algorithms and the one proposed by Gilbert et al. for the LPS problem are randomized and succeed with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13014v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Solon P. Pissis</dc:creator>
    </item>
    <item>
      <title>Greedy matroid base packings with applications to dynamic graph density and orientations</title>
      <link>https://arxiv.org/abs/2511.13205</link>
      <description>arXiv:2511.13205v1 Announce Type: new 
Abstract: Greedy minimum weight spanning tree packings have proven to be useful in connectivity-related problems. We study the process of greedy minimum weight base packings in general matroids and explore its algorithmic applications.
  When specialized to bicircular matroids, our results yield an algorithm for the approximate fully-dynamic densest subgraph density $\rho$. We maintain a $(1+\varepsilon)$-approximation of the density with a worst-case update time $O((\rho\varepsilon^{-2}+\varepsilon^{-4})\rho\log^3 m)$. It improves the dependency on $\varepsilon$ from the current state-of-the-art worst-case update time complexity $O(\varepsilon^{-6}\log^3 n\log\rho)$ [Chekuri, Christiansen, Holm, van der Hoog, Quanrud, Rotenberg, Schwiegelshohn, SODA'24]. We also can maintain an implicit fractional out-orientation with a guarantee that all out-degrees are at most $(1+\varepsilon)\rho$.
  Our algorithms above work by greedily packing pseudoforests, and require maintenance of a minimum-weight pseudoforest in a dynamically changing graph. We show that this problem can be solved in $O(\log n)$ worst-case time per edge insertion or deletion.
  For general matroids, we observe two characterizations of the limit of the base packings (``the vector of ideal loads''), which imply the characterizations from [Cen, Fleischmann, Li, Li, Panigrahi, FOCS'25], namely, their entropy-minimization theorem and their bottom-up cut hierarchy.
  Finally, we give combinatorial results on the greedy tree packings. We show that a tree packing of $O(\lambda^5\log m)$ trees contains a tree crossing some min-cut once, which improves the bound $O(\lambda^7\log^3 m)$ from [Thorup, Combinatorica'07]. We also strengthen the lower bound on the edge load convergence rate from [de Vos, Christiansen, SODA'25], showing that Thorup's upper bound is tight up to a logarithmic factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13205v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavel Arkhipov, Vladimir Kolmogorov</dc:creator>
    </item>
    <item>
      <title>A Complexity Analysis of the c-Closed Vertex Deletion Problem</title>
      <link>https://arxiv.org/abs/2511.13301</link>
      <description>arXiv:2511.13301v1 Announce Type: new 
Abstract: A graph is $c$-closed when every pair of nonadjacent vertices has at most $c-1$ common neighbors. In $c$-Closed Vertex Deletion, the input is a graph $G$ and an integer $k$ and we ask whether $G$ can be transformed into a $c$-closed graph by deleting at most $k$ vertices. We study the classic and parameterized complexity of $c$-Closed Vertex Deletion. We obtain, for example, NP-hardness for the case that $G$ is bipartite with bounded maximum degree. We also show upper and lower bounds on the size of problem kernels for the parameter $k$ and introduce a new parameter, the number $x$ of vertices in bad pairs, for which we show a problem kernel of size $\mathcal{O}(x^3 + x^2\cdot c))$. Here, a pair of nonadjacent vertices is bad if they have at least $c$ common neighbors. Finally, we show that $c$-Closed Vertex Deletion can be solved in polynomial time on unit interval graphs with depth at most $c+1$ and that it is fixed-parameter tractable with respect to the neighborhood diversity of $G$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13301v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lisa Lehner, Christian Komusiewicz, Luca Pascal Staus</dc:creator>
    </item>
    <item>
      <title>Dimension-Free Correlated Sampling for the Hypersimplex</title>
      <link>https://arxiv.org/abs/2511.13573</link>
      <description>arXiv:2511.13573v1 Announce Type: new 
Abstract: Sampling from multiple distributions so as to maximize overlap has been studied by statisticians since the 1950s. Since the 2000s, such correlated sampling from the probability simplex has been a powerful building block in disparate areas of theoretical computer science. We study a generalization of this problem to sampling sets from given vectors in the hypersimplex, i.e., outputting sets of size (at most) some $k$ in $[n]$, while maximizing the sampled sets' overlap. Specifically, the expected difference between two output sets should be at most $\alpha$ times their input vectors' $\ell_1$ distance. A value of $\alpha=O(\log n)$ is known to be achievable, due to Chen et al.~(ICALP'17). We improve this factor to $O(\log k)$, independent of the ambient dimension~$n$. Our algorithm satisfies other desirable properties, including (up to a $\log^* n$ factor) input-sparsity sampling time, logarithmic parallel depth and dynamic update time, as well as preservation of submodular objectives. Anticipating broader use of correlated sampling algorithms for the hypersimplex, we present applications of our algorithm to online paging, offline approximation of metric multi-labeling and swift multi-scenario submodular welfare approximating reallocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13573v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Joseph (Seffi),  Naor, Nitya Raju, Abhishek Shetty, Aravind Srinivasan, Renata Valieva, David Wajc</dc:creator>
    </item>
    <item>
      <title>The Merkle Mountain Belt</title>
      <link>https://arxiv.org/abs/2511.13582</link>
      <description>arXiv:2511.13582v1 Announce Type: new 
Abstract: Merkle structures are widely used as commitment schemes: they allow a prover to publish a compact commitment to an ordered list $X$ of items, and then efficiently prove to a verifier that $x_i\in X$ is the $i$-th item in it. We compare different Merkle structures and their corresponding properties as commitment schemes in the context of blockchain applications. Our primary goal is to speed up light client protocols so that, e.g., a user can verify a transaction efficiently from their smartphone.
  For instance, the Merkle Mountain Range (MMR) yields a succinct scheme: a light client synchronizing for the first time can do so with a complexity sublinear in $|X|$. On the other hand, the Merkle chain, traditionally used to commit to block headers, is not succinct, but it is incremental - a light client resynchronizing frequently can do so with constant complexity - and optimally additive - the structure can be updated in constant time when a new item is appended to list $X$.
  We introduce new Merkle structures, most notably the Merkle Mountain Belt (MMB), the first to be simultaneously succinct, incremental and optimally additive. A variant called UMMB is also asynchronous: a light client may continue to interact with the network even when out of sync with the public commitment. Our Merkle structures are slightly unbalanced, so that items recently appended to $X$ receive shorter membership proofs than older items. This feature reduces a light client's expected costs, in applications where queries are biased towards recently generated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13582v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alfonso Cevallos, Robert Hambrock, Alistair Stewart</dc:creator>
    </item>
    <item>
      <title>Chasing Submodular Objectives, and Submodular Maximization via Cutting Planes</title>
      <link>https://arxiv.org/abs/2511.13605</link>
      <description>arXiv:2511.13605v1 Announce Type: new 
Abstract: We introduce the \emph{submodular objectives chasing problem}, which generalizes many natural and previously-studied problems: a sequence of constrained submodular maximization problems is revealed over time, with both the objective and available ground set changing at each step. The goal is to maintain solutions of high approximation and low total \emph{recourse} (number of changes), compared with exact offline algorithms for the same input sequence. For the central cardinality constraint and partition matroid constraints we provide polynomial-time algorithms achieving both optimal $(1-1/e-\epsilon)$-approximation and optimal competitive recourse for \emph{any} constant-approximation.
  Key to our algorithm's polynomial time, and of possible independent interest, is a new meta-algorithm for $(1-1/e-\epsilon)$-approximately maximizing the multilinear extension under general constraints, which we call {\em approximate-or-separate}. Our algorithm relies on an improvement of the round-and-separate method [Gupta-Levin SODA'20], inspired by an earlier proof by [Vondr\'ak, PhD~Thesis'07]. The algorithm, whose guarantees are similar to the influential {\em continuous greedy} algorithm [Calinescu-Chekuri-P\'al-Vondr\'ak SICOMP'11], can use any cutting plane method and separation oracle for the constraints. This allows us to introduce cutting plane methods, used for exact unconstrained submodular minimization since the '80s [Gr\"otschel/Lov\'asz/Schrijver Combinatorica'81], as a useful method for (optimal approximate) constrained submodular maximization. We show further applications of this approach to static algorithms with curvature-sensitive approximation, and to communication complexity protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13605v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niv Buchbinder (Seffi),  Joseph (Seffi),  Naor, David Wajc</dc:creator>
    </item>
    <item>
      <title>Optimal and Efficient Partite Decompositions of Hypergraphs</title>
      <link>https://arxiv.org/abs/2511.11855</link>
      <description>arXiv:2511.11855v1 Announce Type: cross 
Abstract: We study the problem of partitioning the edges of a $d$-uniform hypergraph $H$ into a family $F$ of complete $d$-partite hypergraphs ($d$-cliques). We show that there is a partition $F$ in which every vertex $v \in V(H)$ belongs to at most $(\frac{1}{d!} + o_d(1))n^{d-1}/\lg n$ members of $F$. This settles the central question of a line of research initiated by Erd\H{o}s and Pyber (1997) for graphs, and more recently by Csirmaz, Ligeti, and Tardos (2014) for hypergraphs. The $d=2$ case of this theorem answers a 40-year-old question of Chung, Erd\H{o}s, and Spencer (1983). An immediate corollary of our result is an improved upper bound for the maximum share size for binary secret sharing schemes on uniform hypergraphs.
  Building on results of Nechiporuk (1969), we prove that every graph with fixed edge density $\gamma \in (0,1)$ has a biclique partition of total weight at most $(\tfrac{1}{2}+o(1))\cdot h_2(\gamma) \frac{n^2}{\lg n}$, where $h_2$ is the binary entropy function. Our construction implies that such biclique partitions can be constructed in time $O(m)$, which answers a question of Feder and Motwani (1995) and also improves upon results of Mubayi and Tur\'an (2010) as well as Chavan, Rabinia, Grosu, and Brocanelli (2025). Using similar techniques, we also give an $n^{1+o(1)}$ algorithm for finding a subgraph $K_{t,t}$ with $t = (1-o(1)) \frac{\gamma}{h_2(\gamma)} \lg n$.
  Our results show that biclique partitions are information-theoretically optimal representations for graphs at every fixed density. We show that with this succinct representation one can answer independent set queries and cut queries in time $O(n^2/ \lg n)$, and if we increase the space usage by a constant factor, we can compute a $2\alpha$-approximation for the densest subgraph problem in time $O(n^2/\lg \alpha)$ for any $\alpha &gt; 1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11855v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Krapivin, Benjamin Przybocki, Nicol\'as Sanhueza-Matamala, Bernardo Subercaseaux</dc:creator>
    </item>
    <item>
      <title>Graded Projection Recursion (GPR): A Framework for Controlling Bit-Complexity of Algebraic Packing</title>
      <link>https://arxiv.org/abs/2511.11988</link>
      <description>arXiv:2511.11988v1 Announce Type: cross 
Abstract: Recursive algorithms that use algebraic packing may appear to achieve reduced computational complexity that does not reflect the dominating bit-complexity, i.e., the implemented performance would not exhibit the claimed scaling. This paper introduces Graded Projection Recursion (GPR), a formal framework designed to address this problem by defining a rigorous mechanism that provably controls bit growth if specific conditions are satisfied.
  We use GPR to develop a matrix multiplication algorithm that is model-honest and achieves a true near-quadratic bit-complexity. This framework also provides a general GPR Substitution Principle recipe for accelerating a significant class of related algorithms in a provably rigorous way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11988v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Uhlmann</dc:creator>
    </item>
    <item>
      <title>Preserving Extreme Singular Values with One Oblivious Sketch</title>
      <link>https://arxiv.org/abs/2511.12802</link>
      <description>arXiv:2511.12802v1 Announce Type: cross 
Abstract: We study when a single linear sketch can control the largest and smallest nonzero singular values of every rank-$r$ matrix. Classical oblivious embeddings require $s=\Theta(r/\varepsilon^{2})$ for $(1\pm\varepsilon)$ distortion, but this does not yield constant-factor control of extreme singular values or condition numbers. We formalize a conjecture that $s=O(r\log r)$ suffices for such preservation. On the constructive side, we show that combining a sparse oblivious sketch with a deterministic geometric balancing map produces a sketch whose nonzero singular values collapse to a common scale under bounded condition number and coherence. On the negative side, we prove that any oblivious sketch achieving relative $\varepsilon$-accurate singular values for all rank-$r$ matrices must satisfy $s=\Omega((r+\log(1/\delta))/\varepsilon^{2})$. Numerical experiments on structured matrix families confirm that balancing improves conditioning and accelerates iterative solvers, while coherent or nearly rank-deficient inputs manifest the predicted failure modes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12802v1</guid>
      <category>math.NA</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John M. Mango, Ronald Katende</dc:creator>
    </item>
    <item>
      <title>Approximate Message Passing for Quantum State Tomography</title>
      <link>https://arxiv.org/abs/2511.12857</link>
      <description>arXiv:2511.12857v1 Announce Type: cross 
Abstract: Quantum state tomography (QST) is an indispensable tool for characterizing many-body quantum systems. However, due to the exponential scaling cost of the protocol with system size, many approaches have been developed for quantum states with specific structure, such as low-rank states. In this paper, we show how approximate message passing (AMP), a compressed sensing technique, can be used to perform low-rank QST. AMP provides asymptotically optimal performance guarantees for large systems, which suggests its utility for QST. We discuss the design challenges that come with applying AMP to QST, and show that by properly designing the AMP algorithm, we can reduce the reconstruction infidelity by over an order of magnitude compared to existing approaches to low-rank QST. We also performed tomographic experiments on IBM Kingston and considered the effect of device noise on the reliability of the predicted fidelity of state preparation. Our work advances the state of low-rank QST and may be applicable to other quantum tomography protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12857v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noah Siekierski, Kausthubh Chandramouli, Christian K\"ummerle, Bojko N. Bakalov, Dror Baron</dc:creator>
    </item>
    <item>
      <title>An FPTAS for 7/9-Approximation to Maximin Share Allocations</title>
      <link>https://arxiv.org/abs/2511.13056</link>
      <description>arXiv:2511.13056v1 Announce Type: cross 
Abstract: We present a new algorithm that achieves a $\frac{7}{9}$-approximation for the maximin share (MMS) allocation of indivisible goods under additive valuations, improving the current best ratio of $\frac{10}{13}$ (Heidari et al., SODA 2026). Building on a new analytical framework, we further obtain an FPTAS that achieves a $\frac{7}{9}-\varepsilon$ approximation in $\tfrac{1}{\varepsilon} \cdot \mathrm{poly}(n,m)$ time. Compared with prior work (Heidari et al., SODA 2026), our algorithm is substantially simpler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13056v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Huang, Shengwei Zhou</dc:creator>
    </item>
    <item>
      <title>MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity</title>
      <link>https://arxiv.org/abs/2511.13061</link>
      <description>arXiv:2511.13061v1 Announce Type: cross 
Abstract: Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13061v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladim\'ir Macko, Vladim\'ir Bo\v{z}a</dc:creator>
    </item>
    <item>
      <title>Subgraph Isomorphism: Prolog vs. Conventional</title>
      <link>https://arxiv.org/abs/2511.13600</link>
      <description>arXiv:2511.13600v1 Announce Type: cross 
Abstract: Subgraph Isomorphism uses a small graph as a pattern to identify within a larger graph a set of vertices that have matching edges. This paper addresses a logic program written in Prolog for a specific relatively complex graph pattern for which multiple conventional implementations (including parallel) exist. The goal is to understand the complexity differences between programming logically and programming conventionally. Discussion includes the process of converting the graph pattern into logic statements in Prolog, and the resulting characteristics as the size of the graph increased. The analysis shows that using a logic paradigm is an efficient way to attack complex graph problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13600v1</guid>
      <category>cs.LO</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claire Y. Yin, Peter M. Kogge</dc:creator>
    </item>
    <item>
      <title>Efficient Calibration for Decision Making</title>
      <link>https://arxiv.org/abs/2511.13699</link>
      <description>arXiv:2511.13699v1 Announce Type: cross 
Abstract: A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.
  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13699v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parikshit Gopalan, Konstantinos Stavropoulos, Kunal Talwar, Pranay Tankala</dc:creator>
    </item>
    <item>
      <title>Faster Algorithms for Longest Common Substring</title>
      <link>https://arxiv.org/abs/2105.03106</link>
      <description>arXiv:2105.03106v2 Announce Type: replace 
Abstract: In the classic longest common substring (LCS) problem, we are given two strings $S$ and $T$, each of length at most $n$, over an alphabet of size $\sigma$, and we are asked to find a longest string occurring as a fragment of both $S$ and $T$. Weiner, in his seminal paper that introduced the suffix tree, presented an $O(n \log \sigma)$-time algorithm for this problem [SWAT 1973]. For polynomially-bounded integer alphabets, the linear-time construction of suffix trees by Farach yielded an $O(n)$-time algorithm for the LCS problem [FOCS 1997]. However, for small alphabets, this is not necessarily optimal for the LCS problem in the word RAM model of computation, in which the strings can be stored in $O(n \log \sigma/\log n )$ space and read in $O(n \log \sigma/\log n )$ time. We show that, in this model, we can compute an LCS in time $O(n \log \sigma / \sqrt{\log n})$, which is sublinear in $n$ if $\sigma=2^{o(\sqrt{\log n})}$ (in particular, if $\sigma=O(1)$), using optimal space $O(n \log \sigma/\log n)$. In fact, it was recently shown that this result is conditionally optimal [Kempa and Kociumaka, STOC 2025].
  We then lift our ideas to the problem of computing a $k$-mismatch LCS, which has received considerable attention in recent years. In this problem, the aim is to compute a longest substring of $S$ that occurs in $T$ with at most $k$ mismatches. Thankachan et al.~showed how to compute a $k$-mismatch LCS in $O(n \log^k n)$ time for $k=O(1)$ [J. Comput. Biol. 2016]. We show an $O(n \log^{k-1/2} n)$-time algorithm, for any constant $k&gt;0$ and irrespective of the alphabet size, using $O(n)$ space as the previous approaches. We thus notably break through the well-known $n \log^k n$ barrier, which stems from a recursive heavy-path decomposition technique that was first introduced in the seminal paper of Cole et al. [STOC 2004] for string indexing with $k$ errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.03106v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panagiotis Charalampopoulos, Tomasz Kociumaka, Jakub Radoszewski, Solon P. Pissis</dc:creator>
    </item>
    <item>
      <title>A Multivariate Complexity Analysis of the Generalized Noah's Ark Problem</title>
      <link>https://arxiv.org/abs/2307.03518</link>
      <description>arXiv:2307.03518v3 Announce Type: replace 
Abstract: In the Generalized Noah's Ark Problem, one is given a phylogenetic tree on a set of species X and a set of conservation projects for each species. Each project comes with a cost and raises the survival probability of the corresponding species. The aim is to select a conservation project for each species such that the total cost of the selected projects does not exceed some given threshold and the expected phylogenetic diversity is as large as possible. We study the complexity of Generalized Noah's Ark Problem and some of its special cases with respect to several parameters related to the input structure, such as the number of different costs, the number of different survival probabilities, or the number of species, |X|.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03518v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Komusiewicz, Jannik Schestag</dc:creator>
    </item>
    <item>
      <title>Tile Reconfiguration by a Finite Automaton</title>
      <link>https://arxiv.org/abs/2501.08663</link>
      <description>arXiv:2501.08663v2 Announce Type: replace 
Abstract: Shape formation is one of the most thoroughly studied problems in programmable matter and swarm robotics. However, in many models, the class of shapes that can be formed is highly restricted due to the particles' limited memory. In the hybrid model, an active agent with the computational power of a deterministic finite automaton can form shapes by lifting and placing passive tiles on the triangular lattice. We study the shape reconfiguration problem where the agent additionally has the ability to distinguish so-called target nodes from non-target nodes and needs to form a target shape from the initial tile configuration. We present a worst-case optimal $O(mn)$ algorithm for simply connected target shapes, where $m$ is the initial number of unoccupied target nodes and $n$ is the total number of tiles. Furthermore, we show how an agent can reconfigure a large class of target shapes with holes in $O(n^4)$ steps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08663v2</guid>
      <category>cs.DS</category>
      <category>cs.ET</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Friemel, David Liedtke, Christian Scheffer</dc:creator>
    </item>
    <item>
      <title>Bicluster Editing with Overlaps: A Vertex Splitting Approach</title>
      <link>https://arxiv.org/abs/2505.03959</link>
      <description>arXiv:2505.03959v2 Announce Type: replace 
Abstract: The BiCluster Editing problem aims at editing a given bipartite graph into a disjoint union of bicliques via a minimum number of edge deletion or addition operations. As a graph-based model for data clustering, the problem aims at a partition of the input dataset, which cannot always obtain meaningful clusters when some data elements are expected to belong to more than one cluster each. To address this limitation, we introduce the Bicluster Editing with Vertex Splitting problem (BCEVS) which consists of finding a minimum sequence of edge editions and vertex splittings such that the resulting graph is a disjoint union of bicliques. The vertex splitting operation consists of replacing a vertex $v$ with two vertices whose union of neighborhoods is the neighborhood of $v$. We also introduce the problem of Bicluster Editing with One-Sided Vertex Splitting (BCEOVS) where we restrict the splitting operations to the only one set of the two sets forming the bipartition. We prove that the two problems are NP-complete even when restricted to bipartite planar graphs of maximum degree three. Moreover, assuming the {\sc Exponential Time Hypothesis} holds, there is no $2^{o(n)}n^{O(1)}$-time (resp. $2^{o(\sqrt{n})}n^{O(1)}$-time) algorithm for BCEVS and BCEOVS on bipartite (resp. planar) graphs with maximum degree three, where $n$ is the number of vertices of the graph. Furthermore we prove both problems are APX-hard and solvable in polynomial time on trees. On the other hand, we prove that BCEOVS is fixed-parameter tractable with respect to solution size by showing that it admits a polynomial size kernel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03959v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faisal N. Abu-Khzam, Lucas Isenmann, Zeina Merchad</dc:creator>
    </item>
    <item>
      <title>Persiansort: an alternative to mergesort inspired by persian rug</title>
      <link>https://arxiv.org/abs/2505.05775</link>
      <description>arXiv:2505.05775v3 Announce Type: replace 
Abstract: This paper introduces persiansort, new stable sorting algorithm inspired by Persian rug. Persiansort does not have the weaknesses of mergesort under scenarios involving nearly sorted and partially sorted data, also utilizing less auxiliary memory than mergesort and take advantage of runs. Initial experimental showed, this method is flexible, powerful and works better than mergesort in almost all types of data. Persiansort offers several advantages over merge methods make it a potential replacement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05775v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Parviz Afereidoon</dc:creator>
    </item>
    <item>
      <title>Reweighted Spectral Partitioning Works: A Simple Algorithm for Vertex Separators in Special Graph Classes</title>
      <link>https://arxiv.org/abs/2506.01228</link>
      <description>arXiv:2506.01228v4 Announce Type: replace 
Abstract: We establish that a simple polynomial-time algorithm that we call reweighted spectral partitioning obtains small 2/3-balanced vertex-separators for a number of graph classes, including $O(\sqrt{n})$-sized separators for planar graphs, $O(\min\{(\log g)^2,\log\Delta\}\cdot\sqrt{gn})$-sized separators for genus-$g$ graphs of maximum degree $\Delta$, and $O(\min\{\log h,\sqrt{\log\Delta}\}(h\log h\log\log h)\sqrt{n})$-sized separators for $K_h$-minor-free graphs of maximum degree $\Delta$.
  To accomplish this, we first obtain a refined form of a Cheeger-style inequality relating the vertex expansion of a graph and the solution to a semidefinite program defined over the graph. Then, to obtain the guarantees for specific graph classes, we derive direct bounds on the value of the semidefinite program.
  We also obtain several other results of independent interest, including an improved separator theorem for the intersection graphs of $d$-dimensional balls with bounded ply, a new bound on the Fiedler value of genus-$g$ graphs, and a new "spectral" proof of the planar separator theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01228v4</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jack Spalding-Jamieson</dc:creator>
    </item>
    <item>
      <title>Recent Advances in Maximum-Entropy Sampling</title>
      <link>https://arxiv.org/abs/2507.05066</link>
      <description>arXiv:2507.05066v2 Announce Type: replace 
Abstract: In 2022, we published the book Maximum-Entropy Sampling: Algorithms and Application (Springer). Since then, there have been several notable advancements on this topic. In this manuscript, we survey some recent highlights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05066v2</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcia Fampa, Jon Lee</dc:creator>
    </item>
    <item>
      <title>Optimal Subspace Embeddings: Resolving Nelson-Nguyen Conjecture Up to Sub-Polylogarithmic Factors</title>
      <link>https://arxiv.org/abs/2508.14234</link>
      <description>arXiv:2508.14234v2 Announce Type: replace 
Abstract: We give a proof of the conjecture of Nelson and Nguyen [FOCS 2013] on the optimal dimension and sparsity of oblivious subspace embeddings, up to sub-polylogarithmic factors: For any $n\geq d$ and $\epsilon\geq d^{-O(1)}$, there is a random $\tilde O(d/\epsilon^2)\times n$ matrix $\Pi$ with $\tilde O(\log(d)/\epsilon)$ non-zeros per column such that for any $A\in\mathbb{R}^{n\times d}$, with high probability, $(1-\epsilon)\|Ax\|\leq\|\Pi Ax\|\leq(1+\epsilon)\|Ax\|$ for all $x\in\mathbb{R}^d$, where $\tilde O(\cdot)$ hides only sub-polylogarithmic factors in $d$. Our result in particular implies a new fastest sub-current matrix multiplication time reduction of size $\tilde O(d/\epsilon^2)$ for a broad class of $n\times d$ linear regression tasks.
  A key novelty in our analysis is a matrix concentration technique we call iterative decoupling, which we use to fine-tune the higher-order trace moment bounds attainable via existing random matrix universality tools [Brailovskaya and van Handel, GAFA 2024].</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14234v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shabarish Chenakkod, Micha{\l} Derezi\'nski, Xiaoyu Dong</dc:creator>
    </item>
    <item>
      <title>Hardness of Dynamic Tree Edit Distance and Friends</title>
      <link>https://arxiv.org/abs/2511.09842</link>
      <description>arXiv:2511.09842v2 Announce Type: replace 
Abstract: String Edit Distance is a more-than-classical problem whose behavior in the dynamic setting, where the strings are updated over time, is well studied. A single-character substitution, insertion, or deletion can be processed in time $\tilde{\mathcal{O}}(n w)$ when operation costs are positive integers bounded by $w$ [Charalampopoulos, Kociumaka, Mozes, CPM 2020][Gorbachev, Kociumaka, STOC 2025]. If the weights are further uniform (insertions and deletions have equal cost), also an $\tilde{\mathcal{O}}(n \sqrt{n})$-update time algorithm exists [Charalampopoulos, Kociumaka, Mozes, CPM 2020]. This is a substantial improvement over the static $\mathcal{O}(n^2)$ algorithm when $w \ll n$ or when we are dealing with uniform weights.
  In contrast, for inherently related problems such as Tree Edit Distance, Dyck Edit Distance, and RNA Folding, it has remained unknown whether it is possible to devise dynamic algorithms with an advantage over the static algorithm. In this paper, we resolve this question by showing that (weighted) Tree Edit Distance, Dyck Edit Distance, and RNA Folding admit no dynamic speedup: under well-known fine-grained assumptions we show that the best possible algorithm recomputes the solution from scratch after each update. Furthermore, we prove a quadratic per-update lower bound for unweighted Tree Edit Distance under the $k$-Clique Conjecture. This provides the first separation between dynamic unweighted String Edit Distance and unweighted Tree Edit Distance, problems whose relative difficulty in the static setting is still open.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09842v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingbing Hu, Jakob Nogler, Barna Saha</dc:creator>
    </item>
    <item>
      <title>Pandora's Box Problem With Time Constraints</title>
      <link>https://arxiv.org/abs/2407.15261</link>
      <description>arXiv:2407.15261v3 Announce Type: replace-cross 
Abstract: The Pandora's Box problem models the search for the best alternative when evaluation is costly. In the simplest variant, a decision maker is presented with $n$ boxes, each associated with a cost of inspection and a hidden random reward. The decision maker inspects a subset of these boxes one after the other, in a possibly adaptive order, and gains the difference between the largest revealed reward and the sum of the inspection costs. Although this classic version is well understood (Weitzman 1979), there is a flourishing recent literature on variants of the problem. Here we introduce a general framework -- the Pandora's Box Over Time problem -- that captures a wide range of variants where time plays a role, e.g., by constraining the schedules of exploration and influencing costs and rewards. In our framework, boxes have time-dependent rewards and costs, whereas inspection may require a box-specific processing time. Moreover, once a box is inspected, its reward may deteriorate over time. Our main result is an efficient constant-factor approximation to the optimal strategy for the Pandora's Box Over Time problem, which is generally NP-hard to compute. We further obtain improved results for the natural special cases where boxes have no processing time, boxes are available only in specific time slots, or when costs and reward distributions are time-independent (but rewards may still deteriorate after inspection).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15261v3</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.artint.2025.104426</arxiv:DOI>
      <arxiv:journal_reference>Artificial Intelligence, Vol. 349, 104426, 2025</arxiv:journal_reference>
      <dc:creator>Georgios Amanatidis, Ben Berger, Tomer Ezra, Michal Feldman, Federico Fusco, Rebecca Reiffenh\"auser, Artem Tsikiridis</dc:creator>
    </item>
    <item>
      <title>Position Fair Mechanisms Allocating Indivisible Goods</title>
      <link>https://arxiv.org/abs/2409.06423</link>
      <description>arXiv:2409.06423v2 Announce Type: replace-cross 
Abstract: Fair division mechanisms for indivisible goods require agent orderings to deterministically select one allocation when running the algorithm in practice. We introduce position envy-freeness up to one good (PEF1) as a fairness criterion for mechanisms: a mechanism is said to satisfy PEF1 if for any pair of agent orderings, no agent prefers their bundle determined under one ordering to that under another ordering by more than the utility of a single good. First, we propose a scale-invariant, polynomial-time mechanism that satisfies PEF1 and yields an envy-freeness up to one good (EF1) allocation. For the case of two agents, we establish that any mechanism producing a maximum Nash welfare allocation eliminates envy based on positions by removing one good, provided that utilities are positive. Additionally, we present a polynomial-time mechanism based on the adjusted winner procedure, which satisfies PEF1 and produces an EF1 and Pareto optimal allocation for two agents. In contrast, we demonstrate that well-known mechanisms such as round-robin and envy-cycle elimination do not generally satisfy PEF1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06423v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryoga Mahara, Ryuhei Mizutani, Taihei Oki, Tomohiko Yokoyama</dc:creator>
    </item>
    <item>
      <title>Assortment optimization given basket shopping behavior using the Ising model</title>
      <link>https://arxiv.org/abs/2502.16260</link>
      <description>arXiv:2502.16260v2 Announce Type: replace-cross 
Abstract: In markets where customers tend to purchase baskets of products rather than single products, assortment optimization is a major challenge for retailers. Removing a product from a retailer's assortment can result in a severe drop in aggregate demand if this product is a complement to other products. Therefore, accounting for the complementarity effect is essential when making assortment decisions. In this paper, we develop a modeling framework designed to address this problem. We model customers' choices using a Markov random field -- in particular, the Ising model -- which captures pairwise demand dependencies as well as the individual attractiveness of each product. Using the Ising model allows us to leverage existing methodologies for various purposes including parameter estimation and efficient simulation of customer choices. We formulate the assortment optimization problem under this model and show that it is APX-hard. We also provide multiple theoretical insights into the structure of the optimal assortments based on the graphical representation of the Ising model, and propose several heuristic algorithms that can be used to obtain high-quality solutions to the assortment optimization problem. Our numerical analysis demonstrates that the developed simulated annealing procedure leads to an expected profit gain of 15% compared to offering an unoptimized assortment (where all products are included) and around 5% compared to using a revenue-ordered heuristic algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16260v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrey Vasilyev, Sebastian Maier, Ralf W. Seifert</dc:creator>
    </item>
    <item>
      <title>Simultaneous Swap Regret Minimization via KL-Calibration</title>
      <link>https://arxiv.org/abs/2502.16387</link>
      <description>arXiv:2502.16387v2 Announce Type: replace-cross 
Abstract: Calibration is a fundamental concept that aims at ensuring the reliability of probabilistic predictions by aligning them with real-world outcomes. There is a surge of studies on new calibration measures that are easier to optimize compared to the classical $\ell_1$-Calibration while still having strong implications for downstream applications. One recent such example is the work by Fishelson et al. (2025) who show that it is possible to achieve $O(T^{1/3})$ pseudo $\ell_2$-Calibration error via minimizing pseudo swap regret of the squared loss, which in fact implies the same bound for all bounded proper losses with a smooth univariate form. In this work, we significantly generalize their result in the following ways: (a) in addition to smooth univariate forms, our algorithm also simultaneously achieves $O(T^{1/3})$ swap regret for any proper loss with a twice continuously differentiable univariate form (such as Tsallis entropy); (b) our bounds hold not only for pseudo swap regret that measures losses using the forecaster's distributions on predictions, but also hold for the actual swap regret that measures losses using the forecaster's actual realized predictions.
  We achieve so by introducing a new stronger notion of calibration called (pseudo) KL-Calibration, which we show is equivalent to the (pseudo) swap regret for log loss. We prove that there exists an algorithm that achieves $O(T^{1/3})$ KL-Calibration error and provide an explicit algorithm that achieves $O(T^{1/3})$ pseudo KL-Calibration error. Moreover, we show that the same algorithm achieves $O(T^{1/3}(\log T)^{-1/3}\log(T/\delta))$ swap regret w.p. $\ge 1-\delta$ for any proper loss with a smooth univariate form, which implies $O(T^{1/3})$ $\ell_2$-Calibration error. A technical contribution of our work is a new randomized rounding procedure and a non-uniform discretization scheme to minimize the swap regret for log loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16387v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haipeng Luo, Spandan Senapati, Vatsal Sharan</dc:creator>
    </item>
    <item>
      <title>Smallest Suffixient Sets as a Repetitiveness Measure</title>
      <link>https://arxiv.org/abs/2506.05638</link>
      <description>arXiv:2506.05638v3 Announce Type: replace-cross 
Abstract: A suffixient set is a novel combinatorial object that captures the essential information of repetitive strings in a way that, provided with a random access mechanism, supports various forms of pattern matching. In this paper, we study the size $\chi$ of the smallest suffixient set as a repetitiveness measure: we place it between known measures and study its sensitivity to various string operations. As a corollary of our results, we give simple online algorithms to compute smallest suffixient sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05638v3</guid>
      <category>cs.FL</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gonzalo Navarro, Giuseppe Romana, Cristian Urbina</dc:creator>
    </item>
    <item>
      <title>Private Evolution Converges</title>
      <link>https://arxiv.org/abs/2506.08312</link>
      <description>arXiv:2506.08312v2 Announce Type: replace-cross 
Abstract: Private Evolution (PE) is a promising training-free method for differentially private (DP) synthetic data generation. While it achieves strong performance in some domains (e.g., images and text), its behavior in others (e.g., tabular data) is less consistent. To date, the only theoretical analysis of the convergence of PE depends on unrealistic assumptions about both the algorithm's behavior and the structure of the sensitive dataset. In this work, we develop a new theoretical framework to understand PE's practical behavior and identify sufficient conditions for its convergence. For $d$-dimensional sensitive datasets with $n$ data points from a convex and compact domain, we prove that under the right hyperparameter settings and given access to the Gaussian variation API proposed in \cite{PE23}, PE produces an $(\varepsilon, \delta)$-DP synthetic dataset with expected 1-Wasserstein distance $\tilde{O}(d(n\varepsilon)^{-1/d})$ from the original; this establishes worst-case convergence of the algorithm as $n \to \infty$. Our analysis extends to general Banach spaces as well. We also connect PE to the Private Signed Measure Mechanism, a method for DP synthetic data generation that has thus far not seen much practical adoption. We demonstrate the practical relevance of our theoretical findings in experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08312v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom\'as Gonz\'alez, Giulia Fanti, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach</title>
      <link>https://arxiv.org/abs/2508.07015</link>
      <description>arXiv:2508.07015v2 Announce Type: replace-cross 
Abstract: The implicit hitting set (IHS) approach offers a general framework for solving computationally hard combinatorial optimization problems declaratively. IHS iterates between a decision oracle used for extracting sources of inconsistency and an optimizer for computing so-called hitting sets (HSs) over the accumulated sources of inconsistency. While the decision oracle is language-specific, the optimizers is usually instantiated through integer programming.
  We explore alternative algorithmic techniques for hitting set optimization based on different ways of employing pseudo-Boolean (PB) reasoning as well as stochastic local search. We extensively evaluate the practical feasibility of the alternatives in particular in the context of pseudo-Boolean (0-1 IP) optimization as one of the most recent instantiations of IHS. Highlighting a trade-off between efficiency and reliability, while a commercial IP solver turns out to remain the most effective way to instantiate HS computations, it can cause correctness issues due to numerical instability; in fact, we show that exact HS computations instantiated via PB reasoning can be made competitive with a numerically exact IP solver. Furthermore, the use of PB reasoning as a basis for HS computations allows for obtaining certificates for the correctness of IHS computations, generally applicable to any IHS instantiation in which reasoning in the declarative language at hand can be captured in the PB-based proof format we employ.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07015v2</guid>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannes Ihalainen, Dieter Vandesande, Andr\'e Schidler, Jeremias Berg, Bart Bogaerts, Matti J\"arvisalo</dc:creator>
    </item>
    <item>
      <title>Unbounded-width CSPs are Untestable in a Sublinear Number of Queries</title>
      <link>https://arxiv.org/abs/2510.27012</link>
      <description>arXiv:2510.27012v2 Announce Type: replace-cross 
Abstract: The bounded-degree query model, introduced by Goldreich and Ron (\textit{Algorithmica, 2002}), is a standard framework in graph property testing and sublinear-time algorithms. Many properties studied in this model, such as bipartiteness and 3-colorability of graphs, can be expressed as satisfiability of constraint satisfaction problems (CSPs). We prove that for the entire class of \emph{unbounded-width} CSPs, testing satisfiability requires $\Omega(n)$ queries in the bounded-degree model. This result unifies and generalizes several previous lower bounds. In particular, it applies to all CSPs that are known to be $\mathbf{NP}$-hard to solve, including $k$-colorability of $\ell$-uniform hypergraphs for any $k,\ell \ge 2$ with $(k,\ell) \neq (2,2)$.
  Our proof combines the techniques from Bogdanov, Obata, and Trevisan (\textit{FOCS, 2002}), who established the first $\Omega(n)$ query lower bound for CSP testing in the bounded-degree model, with known results from universal algebra.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27012v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yumou Fei</dc:creator>
    </item>
    <item>
      <title>Fair Multi-agent Persuasion with Submodular Constraints</title>
      <link>https://arxiv.org/abs/2511.08538</link>
      <description>arXiv:2511.08538v2 Announce Type: replace-cross 
Abstract: We study the problem of selection in the context of Bayesian persuasion. We are given multiple agents with hidden values (or quality scores), to whom resources must be allocated by a welfare-maximizing decision-maker. An intermediary with knowledge of the agents' values seeks to influence the outcome of the selection by designing informative signals and providing tie-breaking policies, so that when the receiver maximizes welfare over the resulting posteriors, the expected utilities of the agents (where utility is defined as allocation times value) achieve certain fairness properties. The fairness measure we will use is majorization, which simultaneously approximately maximizes all symmetric, monotone, concave functions of the utilities. We consider the general setting where the allocation to the agents needs to respect arbitrary submodular constraints, as given by the corresponding polymatroid.
  We present a signaling policy that, under a mild bounded rationality assumption on the receiver, achieves a logarithmically approximate majorized policy in this setting. The approximation ratio is almost best possible, and that significantly outperforms generic results that only yield linear approximations. A key component of our result is a structural characterization showing that the vector of agent utilities for a given signaling policy defines the base polytope of a different polymatroid, a result that may be of independent interest. In addition, we show that an arbitrarily good additive approximation to this vector can be produced in (weakly) polynomial time via the multiplicative weights update method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08538v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yannan Bai, Kamesh Munagala, Yiheng Shen, Davidson Zhu</dc:creator>
    </item>
    <item>
      <title>Learning Intersections of Two Margin Halfspaces under Factorizable Distributions</title>
      <link>https://arxiv.org/abs/2511.09832</link>
      <description>arXiv:2511.09832v2 Announce Type: replace-cross 
Abstract: Learning intersections of halfspaces is a central problem in Computational Learning Theory. Even for just two halfspaces, it remains a major open question whether learning is possible in polynomial time with respect to the margin $\gamma$ of the data points and their dimensionality $d$. The best-known algorithms run in quasi-polynomial time $d^{O(\log(1/\gamma))}$, and it has been shown that this complexity is unavoidable for any algorithm relying solely on correlational statistical queries (CSQ).
  In this work, we introduce a novel algorithm that provably circumvents the CSQ hardness barrier. Our approach applies to a broad class of distributions satisfying a natural, previously studied, factorizability assumption. Factorizable distributions lie between distribution-specific and distribution-free settings, and significantly extend previously known tractable cases. Under these distributions, we show that CSQ-based methods still require quasipolynomial time even for weakly learning, whereas our algorithm achieves $poly(d,1/\gamma)$ time by leveraging more general statistical queries (SQ), establishing a strong separation between CSQ and SQ for this simple realizable PAC learning problem.
  Our result is grounded in a rigorous analysis utilizing a novel duality framework that characterizes the moment tensor structure induced by the marginal distributions. Building on these structural insights, we propose new, efficient learning algorithms. These algorithms combine a refined variant of Jennrich's Algorithm with PCA over random projections of the moment tensor, along with a gradient-descent-based non-convex optimization framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09832v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Mingchen Ma, Lisheng Ren, Christos Tzamos</dc:creator>
    </item>
    <item>
      <title>Support Recovery in One-bit Compressed Sensing with Near-Optimal Measurements and Sublinear Time</title>
      <link>https://arxiv.org/abs/2511.10777</link>
      <description>arXiv:2511.10777v2 Announce Type: replace-cross 
Abstract: The problem of support recovery in one-bit compressed sensing (1bCS) aim to recover the support of a signal $x\in \mathbb{R}^n$, denoted as supp$(x)$, from the observation $y=\text{sign}(Ax)$, where $A\in \mathbb{R}^{m\times n}$ is a sensing matrix and $|\text{supp}(x)|\leq k, k \ll n$. Under this setting, most preexisting works have a recovery runtime $\Omega(n)$. In this paper, we propose two schemes that have sublinear $o(n)$ runtime. (1): For the universal exact support recovery, a scheme of $m=O(k^2\log(n/k)\log n)$ measurements and runtime $D=O(km)$. For the universal $\epsilon$-approximate support recovery, the same scheme with $m=O(k\epsilon^{-1}\log(n/k)\log n)$ and runtime $D=O(\epsilon^{-1}m)$, improving the runtime significantly with an extra $O(\log n)$ factor in the number of measurements compared to the current optimal (Matsumoto et al., 2023). (2): For the probabilistic exact support recovery in the sublinear regime, a scheme of $m:=O(k\frac{\log k}{\log\log k}\log n)$ measurements and runtime $O(m)$, with vanishing error probability, improving the recent result of Yang et al., 2025.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10777v2</guid>
      <category>cs.IT</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.IT</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaxin Li, Arya Mazumdar</dc:creator>
    </item>
  </channel>
</rss>
