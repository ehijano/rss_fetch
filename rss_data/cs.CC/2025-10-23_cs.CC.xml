<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Oct 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Compression of Voxelized Vector Field Data by Boxes is Hard</title>
      <link>https://arxiv.org/abs/2510.20801</link>
      <description>arXiv:2510.20801v1 Announce Type: new 
Abstract: Voxelized vector field data consists of a vector field over a high dimensional lattice. The lattice consists of integer coordinates called voxels. The voxelized vector field assigns a vector at each voxel. This data type encompasses images, tensors, and voxel data.
  Assume there is a nice energy function on the vector field. We consider the problem of lossy compression of voxelized vector field data in Shannon's rate-distortion framework. This means the data is compressed then decompressed up to a bound on the distortion of the energy at each voxel. We formulate this in terms of compressing a single voxelized vector field by a collection of box summary pairs. We call this problem the $(k,D)$-RectLossyVFCompression} problem.
  We show three main results about this problem. The first is that decompression for this problem is polynomial time tractable. This means that the only obstruction to a tractable solution of the $(k,D)$-RectLossyVFCompression problem lies in the compression stage. This is shown by the two hardness results about the compression stage. We show that the compression stage is NP-Hard to compute exactly and that it is even APX-Hard to approximate for $k,D\geq 2$.
  Assuming $P\neq NP$, this shows that when $k,D \geq 2$ there can be no exact polynomial time algorithm nor can there even be a PTAS approximation algorithm for the $(k,D)$-RectLossyVFCompression problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20801v1</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Zhang</dc:creator>
    </item>
    <item>
      <title>New Hardness Results for the LOCAL Model via a Simple Self-Reduction</title>
      <link>https://arxiv.org/abs/2510.19972</link>
      <description>arXiv:2510.19972v1 Announce Type: cross 
Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta, \log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and $\Delta$ is the maximum degree. This result is shown through a new technique, called round elimination via self-reduction. The lower bound proof is beautiful and presents very nice ideas. However, it spans more than 25 pages of technical details, and hence it is hard to digest and generalize to other problems. Historically, the simplification of proofs and techniques has marked an important turning point in our understanding of the complexity of graph problems. Our paper makes a step forward towards this direction, and provides the following contributions.
  1. We present a short and simplified version of the round elimination via self-reduction technique. The simplification of this technique enables us to obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal $b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching problem is a generalization of the matching problem where each vertex can have up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log \Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le \Delta^{1-\varepsilon}$ and any constant $\varepsilon &gt; 0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19972v1</guid>
      <category>cs.DC</category>
      <category>cs.CC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alkida Balliu, Filippo Casagrande, Francesco d'Amore, Dennis Olivetti</dc:creator>
    </item>
    <item>
      <title>Counterexample to majority optimality in NICD with erasures</title>
      <link>https://arxiv.org/abs/2510.20013</link>
      <description>arXiv:2510.20013v1 Announce Type: cross 
Abstract: We asked GPT-5 Pro to look for counterexamples among a public list of open problems (the Simons ``Real Analysis in Computer Science'' collection). After several numerical experiments, it suggested a counterexample for the Non-Interactive Correlation Distillation (NICD) with erasures question: namely, a Boolean function on 5 bits that achieves a strictly larger value of $\mathbb{E}|f(z)|$ than the 5-bit majority function when the erasure parameter is $p=0.40.$ In this very short note we record the finding, state the problem precisely, give the explicit function, and verify the computation step by step by hand so that it can be checked without a computer. In addition, we show that for each fixed odd $n$ the majority is optimal (among unbiased Boolean functions) in a neighborhood of $p=0$. We view this as a little spark of an AI contribution in Theoretical Computer Science: while modern Large Language Models (LLMs) often assist with literature and numerics, here a concrete finite counterexample emerged.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20013v1</guid>
      <category>math.PR</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paata Ivanisvili, Xinyuan Xie</dc:creator>
    </item>
    <item>
      <title>A Classification of Long-Refinement Graphs for Colour Refinement</title>
      <link>https://arxiv.org/abs/2510.20802</link>
      <description>arXiv:2510.20802v1 Announce Type: cross 
Abstract: The Colour Refinement algorithm is a classical procedure to detect symmetries in graphs, whose most prominent application is in graph-isomorphism tests. The algorithm and its generalisation, the Weisfeiler-Leman algorithm, evaluate local information to compute a colouring for the vertices in an iterative fashion. Different final colours of two vertices certify that no isomorphism can map one onto the other. The number of iterations that the algorithm takes to terminate is its central complexity parameter. For a long time, it was open whether graphs that take the maximum theoretically possible number of Colour Refinement iterations actually exist. Starting from an exhaustive search on graphs of low degrees, Kiefer and McKay proved the existence of infinite families of such long-refinement graphs with degrees 2 and 3, thereby showing that the trivial upper bound on the iteration number of Colour Refinement is tight. In this work, we provide a complete characterisation of the long-refinement graphs with low (or, equivalently, high) degrees. We show that, with one exception, the aforementioned families are the only long-refinement graphs with maximum degree at most 3, and we fully classify the long-refinement graphs with maximum degree 4. To this end, via a reverse-engineering approach, we show that all low-degree long-refinement graphs can be represented as compact strings, and we derive multiple structural insights from this surprising fact. Since long-refinement graphs are closed under taking edge complements, this also yields a classification of long-refinement graphs with high degrees. Kiefer and McKay initiated a search for long-refinement graphs that are only distinguished in the last iteration of Colour Refinement before termination. We conclude it in this submission by showing that such graphs cannot exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20802v1</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <category>math.CO</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandra Kiefer, T. Devini de Mel</dc:creator>
    </item>
    <item>
      <title>The Parameterized Complexity of Computing the VC-Dimension</title>
      <link>https://arxiv.org/abs/2510.17451</link>
      <description>arXiv:2510.17451v2 Announce Type: replace 
Abstract: The VC-dimension is a well-studied and fundamental complexity measure of a set system (or hypergraph) that is central to many areas of machine learning. We establish several new results on the complexity of computing the VC-dimension. In particular, given a hypergraph $\mathcal{H}=(\mathcal{V},\mathcal{E})$, we prove that the naive $2^{\mathcal{O}(|\mathcal{V}|)}$-time algorithm is asymptotically tight under the Exponential Time Hypothesis (ETH). We then prove that the problem admits a $1$-additive fixed-parameter approximation algorithm when parameterized by the maximum degree of $\mathcal{H}$ and a fixed-parameter algorithm when parameterized by its dimension, and that these are essentially the only such exploitable structural parameters. Lastly, we consider a generalization of the problem, formulated using graphs, which captures the VC-dimension of both set systems and graphs. We design a $2^{\mathcal{O}(\rm{tw}\cdot \log \rm{tw})}\cdot |V|$-time algorithm for any graph $G=(V,E)$ of treewidth $\rm{tw}$ (which, for a set system, applies to the treewidth of its incidence graph). This is in contrast with closely related problems that require a double-exponential dependency on the treewidth (assuming the ETH).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17451v2</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florent Foucaud, Harmender Gahlawat, Fionn Mc Inerney, Prafullkumar Tale</dc:creator>
    </item>
  </channel>
</rss>
