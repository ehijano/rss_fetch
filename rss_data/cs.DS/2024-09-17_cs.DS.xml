<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Sep 2024 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multi-Slot Tag Assignment Problem in Billboard Advertisement</title>
      <link>https://arxiv.org/abs/2409.09623</link>
      <description>arXiv:2409.09623v1 Announce Type: new 
Abstract: Nowadays, billboard advertising has emerged as an effective advertising technique due to higher returns on investment. Given a set of selected slots and tags, how to effectively assign the tags to the slots remains an important question. In this paper, we study the problem of assigning tags to the slots such that the number of tags for which influence demand of each zone is satisfied gets maximized. Formally, we call this problem the Multi-Slot Tag Assignment Problem. The input to the problem is a geographical region partitioned into several zones, a set of selected tags and slots, a trajectory, a billboard database, and the influence demand for every tag for each zone. The task here is to find out the assignment of tags to the slots, such the number of tags for which the zonal influence demand is satisfied is maximized. We show that the problem is NP-hard, and we propose an efficient approximation algorithm to solve this problem. A time and space complexity analysis of the proposed methodology has been done. The proposed methodology has been implemented with real-life datasets, and a number of experiments have been carried out to show the effectiveness and efficiency of the proposed approach. The obtained results have been compared with the baseline methods, and we observe that the proposed approach leads to a number of tags whose zonal influence demand is satisfied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09623v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dildar Ali, Suman Banerjee, Yamuna Prasad</dc:creator>
    </item>
    <item>
      <title>Entrywise Approximate Laplacian Solving</title>
      <link>https://arxiv.org/abs/2409.10022</link>
      <description>arXiv:2409.10022v1 Announce Type: new 
Abstract: We study the escape probability problem in random walks over graphs. Given vertices, $s,t,$ and $p$, the problem asks for the probability that a random walk starting at $s$ will hit $t$ before hitting $p$. Such probabilities can be exponentially small even for unweighted undirected graphs with polynomial mixing time. Therefore current approaches, which are mostly based on fixed-point arithmetic, require $n$ bits of precision in the worst case. We present algorithms and analyses for weighted directed graphs under floating-point arithmetic and improve the previous best running times in terms of the number of bit operations. We believe our techniques and analysis could have a broader impact on the computation of random walks on graphs both in theory and in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10022v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingbang Chen, Mehrdad Ghadiri, Hoai-An Nguyen, Richard Peng, Junzhao Yang</dc:creator>
    </item>
    <item>
      <title>Efficient approximation schemes for scheduling on a stochastic number of machines</title>
      <link>https://arxiv.org/abs/2409.10155</link>
      <description>arXiv:2409.10155v1 Announce Type: new 
Abstract: We study three two-stage optimization problems with a similar structure and different objectives. In the first stage of each problem, the goal is to assign input jobs of positive sizes to unsplittable bags. After this assignment is decided, the realization of the number of identical machines that will be available is revealed. Then, in the second stage, the bags are assigned to machines. The probability vector of the number of machines in the second stage is known to the algorithm as part of the input before making the decisions of the first stage. Thus, the vector of machine completion times is a random variable. The goal of the first problem is to minimize the expected value of the makespan of the second stage schedule, while the goal of the second problem is to maximize the expected value of the minimum completion time of the machines in the second stage solution. The goal of the third problem is to minimize the \ell_p norm for a fixed p&gt;1, where the norm is applied on machines' completion times vectors. Each one of the first two problems admits a PTAS as Buchem et al. showed recently. Here we significantly improve all their results by designing an EPTAS for each one of these problems. We also design an EPTAS for \ell_p norm minimization for any p&gt;1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10155v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leah Epstein, Asaf Levin</dc:creator>
    </item>
    <item>
      <title>Pareto Sums of Pareto Sets: Lower Bounds and Algorithms</title>
      <link>https://arxiv.org/abs/2409.10232</link>
      <description>arXiv:2409.10232v1 Announce Type: new 
Abstract: In bi-criteria optimization problems, the goal is typically to compute the set of Pareto-optimal solutions. Many algorithms for these types of problems rely on efficient merging or combining of partial solutions and filtering of dominated solutions in the resulting sets. In this article, we consider the task of computing the Pareto sum of two given Pareto sets $A, B$ of size $n$. The Pareto sum $C$ contains all non-dominated points of the Minkowski sum $M = \{a+b|a \in A, b\in B\}$. Since the Minkowski sum has a size of $n^2$, but the Pareto sum $C$ can be much smaller, the goal is to compute $C$ without having to compute and store all of $M$. We present several new algorithms for efficient Pareto sum computation, including an output-sensitive successive algorithm with a running time of $O(n \log n + nk)$ and a space consumption of $O(n+k)$ for $k=|C|$. If the elements of $C$ are streamed, the space consumption reduces to $O(n)$. For output sizes $k \geq 2n$, we prove a conditional lower bound for Pareto sum computation, which excludes running times in $O(n^{2-\delta})$ for $\delta &gt; 0$ unless the (min,+)-convolution hardness conjecture fails. The successive algorithm matches this lower bound for $k \in \Theta(n)$. However, for $k \in \Theta(n^2)$, the successive algorithm exhibits a cubic running time. But we also present an algorithm with an output-sensitive space consumption and a running time of $O(n^2 \log n)$, which matches the lower bound up to a logarithmic factor even for large $k$. Furthermore, we describe suitable engineering techniques to improve the practical running times of our algorithms. Finally, we provide an extensive comparative experimental study on generated and real-world data. As a showcase application, we consider preprocessing-based bi-criteria route planning in road networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10232v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Funke, Demian Hespe, Peter Sanders, Sabine Storandt, Carina Truschel</dc:creator>
    </item>
    <item>
      <title>Practical and Asymptotically Optimal Quantization of High-Dimensional Vectors in Euclidean Space for Approximate Nearest Neighbor Search</title>
      <link>https://arxiv.org/abs/2409.09913</link>
      <description>arXiv:2409.09913v1 Announce Type: cross 
Abstract: Approximate nearest neighbor (ANN) query in high-dimensional Euclidean space is a key operator in database systems. For this query, quantization is a popular family of methods developed for compressing vectors and reducing memory consumption. Recently, a method called RaBitQ achieves the state-of-the-art performance among these methods. It produces better empirical performance in both accuracy and efficiency when using the same compression rate and provides rigorous theoretical guarantees. However, the method is only designed for compressing vectors at high compression rates (32x) and lacks support for achieving higher accuracy by using more space. In this paper, we introduce a new quantization method to address this limitation by extending RaBitQ. The new method inherits the theoretical guarantees of RaBitQ and achieves the asymptotic optimality in terms of the trade-off between space and error bounds as to be proven in this study. Additionally, we present efficient implementations of the method, enabling its application to ANN queries to reduce both space and time consumption. Extensive experiments on real-world datasets confirm that our method consistently outperforms the state-of-the-art baselines in both accuracy and efficiency when using the same amount of memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09913v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jianyang Gao, Yutong Gou, Yuexuan Xu, Yongyi Yang, Cheng Long, Raymond Chi-Wing Wong</dc:creator>
    </item>
    <item>
      <title>High-level quantum algorithm programming using Silq</title>
      <link>https://arxiv.org/abs/2409.10231</link>
      <description>arXiv:2409.10231v1 Announce Type: cross 
Abstract: Quantum computing, with its vast potential, is fundamentally shaped by the intricacies of quantum mechanics, which both empower and constrain its capabilities. The development of a universal, robust quantum programming language has emerged as a key research focus in this rapidly evolving field. This paper explores Silq, a recent high-level quantum programming language, highlighting its strengths and unique features. We aim to share our insights on designing and implementing high-level quantum algorithms using Silq, demonstrating its practical applications and advantages for quantum programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10231v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.PL</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Viktorija Bezganovic, Marco Lewis, Sadegh Soudjani, Paolo Zuliani</dc:creator>
    </item>
    <item>
      <title>Reducing Leximin Fairness to Utilitarian Optimization</title>
      <link>https://arxiv.org/abs/2409.10395</link>
      <description>arXiv:2409.10395v1 Announce Type: cross 
Abstract: Two prominent objectives in social choice are utilitarian - maximizing the sum of agents' utilities, and leximin - maximizing the smallest agent's utility, then the second-smallest, etc. Utilitarianism is typically computationally easier to attain but is generally viewed as less fair. This paper presents a general reduction scheme that, given a utilitarian solver, produces a distribution over outcomes that is leximin in expectation. Importantly, the scheme is robust in the sense that, given an approximate utilitarian solver, it produces an outcome that is approximately-leximin (in expectation) - with the same approximation factor. We apply our scheme to several social choice problems: stochastic allocations of indivisible goods, giveaway lotteries, and fair lotteries for participatory budgeting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10395v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>cs.MA</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eden Hartman, Yonatan Aumann, Avinatan Hassidim, Erel Segal-Halevi</dc:creator>
    </item>
    <item>
      <title>From Width-Based Model Checking to Width-Based Automated Theorem Proving</title>
      <link>https://arxiv.org/abs/2205.10995</link>
      <description>arXiv:2205.10995v3 Announce Type: replace 
Abstract: In the field of parameterized complexity theory, the study of graph width measures has been intimately connected with the development of width-based model checking algorithms for combinatorial properties on graphs. In this work, we introduce a general framework to convert a large class of width-based model-checking algorithms into algorithms that can be used to test the validity of graph-theoretic conjectures on classes of graphs of bounded width. Our framework is modular and can be applied with respect to several well-studied width measures for graphs, including treewidth and cliquewidth.
  As a quantitative application of our framework, we prove analytically that for several long-standing graph-theoretic conjectures, there exists an algorithm that takes a number $k$ as input and correctly determines in time double-exponential in $k^{O(1)}$ whether the conjecture is valid on all graphs of treewidth at most $k$. These upper bounds, which may be regarded as upper-bounds on the size of proofs/disproofs for these conjectures on the class of graphs of treewidth at most $k$, improve significantly on theoretical upper bounds obtained using previously available techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.10995v3</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.DM</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mateus de Oliveira Oliveira, Farhad Vadiee</dc:creator>
    </item>
    <item>
      <title>Parameterized Approximation for Robust Clustering in Discrete Geometric Spaces</title>
      <link>https://arxiv.org/abs/2305.07316</link>
      <description>arXiv:2305.07316v2 Announce Type: replace 
Abstract: We consider the well-studied Robust $(k, z)$-Clustering problem, which generalizes the classic $k$-Median, $k$-Means, and $k$-Center problems. Given a constant $z\ge 1$, the input to Robust $(k, z)$-Clustering is a set $P$ of $n$ weighted points in a metric space $(M,\delta)$ and a positive integer $k$. Further, each point belongs to one (or more) of the $m$ many different groups $S_1,S_2,\ldots,S_m$. Our goal is to find a set $X$ of $k$ centers such that $\max_{i \in [m]} \sum_{p \in S_i} w(p) \delta(p,X)^z$ is minimized.
  This problem arises in the domains of robust optimization [Anthony, Goyal, Gupta, Nagarajan, Math. Oper. Res. 2010] and in algorithmic fairness. For polynomial time computation, an approximation factor of $O(\log m/\log\log m)$ is known [Makarychev, Vakilian, COLT $2021$], which is tight under a plausible complexity assumption even in the line metrics. For FPT time, there is a $(3^z+\epsilon)$-approximation algorithm, which is tight under GAP-ETH [Goyal, Jaiswal, Inf. Proc. Letters, 2023].
  Motivated by the tight lower bounds for general discrete metrics, we focus on \emph{geometric} spaces such as the (discrete) high-dimensional Euclidean setting and metrics of low doubling dimension, which play an important role in data analysis applications. First, for a universal constant $\eta_0 &gt;0.0006$, we devise a $3^z(1-\eta_{0})$-factor FPT approximation algorithm for discrete high-dimensional Euclidean spaces thereby bypassing the lower bound for general metrics. We complement this result by showing that even the special case of $k$-Center in dimension $\Theta(\log n)$ is $(\sqrt{3/2}- o(1))$-hard to approximate for FPT algorithms. Finally, we complete the FPT approximation landscape by designing an FPT $(1+\epsilon)$-approximation scheme (EPAS) for the metric of sub-logarithmic doubling dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.07316v2</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.ICALP.2024.6</arxiv:DOI>
      <arxiv:journal_reference>51st International Colloquium on Automata, Languages, and Programming (ICALP 2024), Leibniz International Proceedings in Informatics (LIPIcs), vol. 297, pp. 6:1--6:19, Schloss Dagstuhl -- Leibniz-Zentrum f\"ur Informatik, 2024</arxiv:journal_reference>
      <dc:creator>Fateme Abbasi, Sandip Banerjee, Jaros{\l}aw Byrka, Parinya Chalermsook, Ameet Gadekar, Kamyar Khodamoradi, D\'aniel Marx, Roohani Sharma, Joachim Spoerhase</dc:creator>
    </item>
    <item>
      <title>Are there graphs whose shortest path structure requires large edge weights?</title>
      <link>https://arxiv.org/abs/2308.13054</link>
      <description>arXiv:2308.13054v3 Announce Type: replace 
Abstract: The aspect ratio of a (positively) weighted graph $G$ is the ratio of its maximum edge weight to its minimum edge weight. Aspect ratio commonly arises as a complexity measure in graph algorithms, especially related to the computation of shortest paths. Popular paradigms are to interpolate between the settings of weighted and unweighted input graphs by incurring a dependence on aspect ratio, or by simply restricting attention to input graphs of low aspect ratio.
  This paper studies the effects of these paradigms, investigating whether graphs of low aspect ratio have more structured shortest paths than graphs in general. In particular, we raise the question of whether one can generally take a graph of large aspect ratio and reweight its edges, to obtain a graph with bounded aspect ratio while preserving the structure of its shortest paths. Our findings are:
  - Every weighted DAG on $n$ nodes has a shortest-paths preserving graph of aspect ratio $O(n)$. A simple lower bound shows that this is tight.
  - The previous result does not extend to general directed or undirected graphs; in fact, the answer turns out to be exponential in these settings. In particular, we construct directed and undirected $n$-node graphs for which any shortest-paths preserving graph has aspect ratio $2^{\Omega(n)}$.
  We also consider the approximate version of this problem, where the goal is for shortest paths in $H$ to correspond to approximate shortest paths in $G$. We show that our exponential lower bounds extend even to this setting. We also show that in a closely related model, where approximate shortest paths in $H$ must also correspond to approximate shortest paths in $G$, even DAGs require exponential aspect ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13054v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaron Bernstein, Greg Bodwin, Nicole Wein</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Minimizing Energy Consumption of Partitioning DAG Tasks</title>
      <link>https://arxiv.org/abs/2404.01022</link>
      <description>arXiv:2404.01022v3 Announce Type: replace 
Abstract: We study a graph partition problem where we are given a directed acyclic graph (DAG) whose vertices and arcs can be respectively regarded as tasks and dependencies among tasks. The objective of the problem is to minimize the total energy consumed for completing these tasks by assigning the tasks to k heterogeneous machines. We first show that the problem is NP-hard. Then, we present polynomial-time algorithms for two special cases where there are only two machines and where the input DAG is a directed path. Finally, we study a natural variant where there are only two machines with one of them being capable of executing a limited number of tasks. We show that this special case remains computationally hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01022v3</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Liu, Jian-Jia Chen, Yongjie Yang</dc:creator>
    </item>
    <item>
      <title>Beyond Uniform Reverse Sampling: A Hybrid Sampling Technique for Misinformation Prevention</title>
      <link>https://arxiv.org/abs/1901.05149</link>
      <description>arXiv:1901.05149v3 Announce Type: replace-cross 
Abstract: Online misinformation has been considered as one of the top global risks as it may cause serious consequences such as economic damages and public panic. The misinformation prevention problem aims at generating a positive cascade with appropriate seed nodes in order to compete against the misinformation. In this paper, we study the misinformation prevention problem under the prominent independent cascade model. Due to the #P-hardness in computing influence, the core problem is to design effective sampling methods to estimate the function value. The main contribution of this paper is a novel sampling method. Different from the classic reverse sampling technique which treats all nodes equally and samples the node uniformly, the proposed method proceeds with a hybrid sampling process which is able to attach high weights to the users who are prone to be affected by the misinformation. Consequently, the new sampling method is more powerful in generating effective samples used for computing seed nodes for the positive cascade. Based on the new hybrid sample technique, we design an algorithm offering a $(1-1/e-\epsilon)$-approximation. We experimentally evaluate the proposed method on extensive datasets and show that it significantly outperforms the state-of-the-art solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:1901.05149v3</guid>
      <category>cs.SI</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gunagmo Tong, Ding-Zhu Du</dc:creator>
    </item>
  </channel>
</rss>
