<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Feb 2026 05:00:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Inference From Random Restarts</title>
      <link>https://arxiv.org/abs/2602.13450</link>
      <description>arXiv:2602.13450v1 Announce Type: new 
Abstract: Algorithms for computing equilibria, optima, and fixed points in nonconvex problems often depend sensitively on practitioner-chosen initial conditions. When uniqueness of a solution is of interest, a common heuristic is to run such algorithms from many randomly selected initial conditions and to interpret repeated convergence to the same output as evidence of a unique solution or a dominant basin of attraction. Despite its widespread use, this practice lacks a formal inferential foundation.
  We provide a simple probabilistic framework for interpreting such numerical evidence. First, we give sufficient conditions under which an algorithm's terminal output is a measurable function of its initial condition, allowing probabilistic reasoning over outcomes. Second, we provide sufficient conditions ensuring that an algorithm admits only finitely many possible terminal outcomes. While these conditions may be difficult to verify on a case-by-case basis, we give simple sufficient conditions for broad classes of problems under which almost all instances admit only finitely many outcomes (in the sense of prevalence). Standard algorithms such as gradient descent and damped fixed-point iteration applied to sufficiently smooth functions satisfy these conditions.
  Within this framework, repeated solver runs correspond to independent samples from the induced distribution over outcomes. We adopt a Bayesian approach to infer basin sizes and the probability of solution uniqueness from repeated identical outputs, and we establish convergence rates for the resulting posterior beliefs. Finally, we apply our framework to settings in the existing industrial organization literature, where random-restart heuristics are used. Our results formalize and qualify these arguments, clarifying when repeated convergence provides meaningful evidence for uniqueness and when it does not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13450v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Moeen Nehzati, Diego Cussen</dc:creator>
    </item>
    <item>
      <title>Post-Matching Two-Way Fixed Effects Estimation</title>
      <link>https://arxiv.org/abs/2602.13453</link>
      <description>arXiv:2602.13453v1 Announce Type: new 
Abstract: When estimating treatment effects with two-way fixed effects (2WFE) models, researchers often use matching as a pre-processing step when the parallel trends assumption is thought to hold conditionally on covariates. Specifically, in a first step, each treated unit is matched to one or more untreated units based on observed time-invariant covariates. In the second step, treatment effects are estimated with a 2WFE regression in the matched sample, reweighting the untreated units by the number of times they are matched. We formally analyze this common practice and highlight two problems. First, when different treatment cohorts enter treatment in different time periods, the post-matching 2WFE estimator that pools all treated cohorts has an asymptotic bias, even when the treatment effect is constant across units and over time. Second, failing to account for the variability introduced by the matching procedure yields invalid standard error estimators, which can be biased upwards or downwards depending on the data generating process. We propose simple post-matching difference-in-differences estimators that compare each treated cohort to the never-treated separately, instead of pooling all treated cohorts. We provide conditions under which these estimators are consistent for well-defined causal parameters, and derive valid standard errors that account for the matching step. We illustrate our results with simulations and with an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13453v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Liu, Gonzalo Vazquez-Bare</dc:creator>
    </item>
    <item>
      <title>Cluster-Robust Inference for Quadratic Forms</title>
      <link>https://arxiv.org/abs/2602.13537</link>
      <description>arXiv:2602.13537v1 Announce Type: new 
Abstract: This paper studies inference for quadratic forms of linear regression coefficients with clustered data and many covariates. Our framework covers three important special cases: instrumental variables regression with many instruments and controls, inference on variance components, and testing multiple restrictions in a linear regression. Na\"{\i}ve plug-in estimators are known to be biased. We study a leave-one-cluster-out estimator that is unbiased, and provide sufficient conditions for its asymptotic normality. For inference, we establish the consistency of a leave-three-cluster-out variance estimator under primitive conditions. In addition, we develop a novel leave-two-cluster-out variance estimator that is computationally simpler and guaranteed to be conservative under weaker conditions. Our analysis allows cluster sizes to diverge with the sample size, accommodates strong within-cluster dependence, and permits the dimension of the covariates to diverge with the sample size, potentially at the same rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13537v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michal Koles\'ar, Pengjin Min, Wenjie Wang, Yichong Zhang</dc:creator>
    </item>
    <item>
      <title>The Accuracy Smoothness Dilemma in Prediction: a Novel Multivariate M-SSA Forecast Approach</title>
      <link>https://arxiv.org/abs/2602.13722</link>
      <description>arXiv:2602.13722v1 Announce Type: new 
Abstract: Forecasting presents a complex estimation challenge, as it involves balancing multiple, often conflicting, priorities and objectives. Conventional forecast optimization methods typically emphasize a single metric--such as minimizing the mean squared error (MSE)--which may neglect other crucial aspects of predictive performance. To address this limitation, the recently developed Smooth Sign Accuracy (SSA) framework extends the traditional MSE approach by simultaneously accounting for sign accuracy, MSE, and the frequency of sign changes in the predictor. This addresses a fundamental trade-off--the so-called accuracy-smoothness (AS) dilemma--in prediction. We extend this approach to the multivariate M-SSA, leveraging the original criterion to incorporate cross-sectional information across multiple time series. As a result, the M-SSA criterion enables the integration of various design objectives related to AS forecasting performance, effectively generalizing conventional MSE-based metrics. To demonstrate its practical applicability and versatility, we explore the application of the M-SSA in three primary domains: forecasting, real-time signal extraction (nowcasting), and smoothing. These case studies illustrate the framework's capacity to adapt to different contexts while effectively managing inherent trade-offs in predictive modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13722v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Wildi</dc:creator>
    </item>
    <item>
      <title>Dual-Channel Closed Loop Supply Chain Competition: A Stackelberg--Nash Approach</title>
      <link>https://arxiv.org/abs/2602.14288</link>
      <description>arXiv:2602.14288v1 Announce Type: new 
Abstract: In many consumer electronics and appliance markets, manufacturers sell products through competing retailers while simultaneously relying on take-back programs to recover used items for remanufacturing. Designing such programs is challenging when firms compete on prices and consumers differ in their willingness to return products. Motivated by these settings, this paper develops a game theoretic framework to analyze pricing and take-back decisions in a dual-channel closed loop supply chain (CLSC) with two competing manufacturers and two competing retailers. Manufacturers act as Stackelberg leaders, simultaneously determining wholesale prices and consumer take-back bonuses, while retailers engage in Nash competition over retail prices. The model integrates three key elements: (i) segmented linear demand with cross-price effects, (ii) deterministic product returns, and (iii) an inertia responsiveness allocation mechanism governing the distribution of returned products between manufacturers. Closed form Nash equilibria are derived for the retailer subgame, along with symmetric Stackelberg equilibria for manufacturers. We derive a feasibility threshold for take-back incentives, identifying conditions under which firms optimally offer positive bonuses to consumers. The results further demonstrate that higher remanufacturing value or return rates lead the manufacturers to lower wholesale prices in order to expand sales and capture additional return volumes, while high consumer inertia weakens incentives for active collection. Numerical experiments illustrate and reinforce the analytical results, highlighting how consumer behavior, market structure and product substitutability influence prices, bonuses, and return volumes. Overall, the study provides managerial insights for designing effective take-back programs and coordinating pricing decisions in competitive circular supply chains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14288v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gurkirat Wadhwa</dc:creator>
    </item>
    <item>
      <title>The Role of Measured Covariates in Assessing Sensitivity to Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2602.14414</link>
      <description>arXiv:2602.14414v1 Announce Type: cross 
Abstract: Sensitivity analysis is widely used to assess the robustness of causal conclusions in observational studies, yet its interaction with the structure of measured covariates is often overlooked. When latent confounders cannot be directly adjusted for and are instead controlled using proxy variables, strong associations between exposure and measured proxies can amplify sensitivity to residual confounding. We formalize this phenomenon in linear regression settings by showing that a simple ratio involving the exposure model coefficient and residual exposure variance provides an observable measure of this increased sensitivity. Applying our framework to smoking and lung cancer, we document how growing socioeconomic stratification in smoking behavior over time leads to heightened sensitivity to unmeasured confounding in more recent data. These results highlight the importance of multicollinearity when interpreting sensitivity analyses based on proxy adjustment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14414v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhinandan Dalal, Iris Horng, Yang Feng, Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>Statistical Inference of Optimal Allocations I: Regularities and their Implications</title>
      <link>https://arxiv.org/abs/2403.18248</link>
      <description>arXiv:2403.18248v4 Announce Type: replace 
Abstract: In this paper, we develop a functional differentiability approach for solving statistical optimal allocation problems. We derive Hadamard differentiability of the value functions through analyzing the properties of the sorting operator using tools from geometric measure theory. Building on our Hadamard differentiability results, we apply the functional delta method to obtain the asymptotic properties of the value function process for the binary constrained optimal allocation problem and the plug-in ROC curve estimator. Moreover, the convexity of the optimal allocation value functions facilitates demonstrating the degeneracy of first order derivatives with respect to the policy. We then present a double / debiased estimator for the value functions. Importantly, the conditions that validate Hadamard differentiability justify the margin assumption from the statistical classification literature for the fast convergence rate of plug-in methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18248v4</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Feng, Han Hong, Denis Nekipelov</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences with Sample Selection</title>
      <link>https://arxiv.org/abs/2411.09221</link>
      <description>arXiv:2411.09221v3 Announce Type: replace 
Abstract: We consider the identification of average treatment effects on the treated (ATT) in difference-in-differences (DiD) settings in the presence of endogenous sample selection. We first establish that the conventional DiD estimand generally fails to recover causally meaningful treatment effects, even if selection and treatment assignment are independent. We then partially identify the ATT for individuals whose outcomes would be observed post-treatment under either counterfactual treatment state, and derive sharp bounds on this parameter under different sets of assumptions on the relationship between sample selection and treatment assignment. These identification results are extended to allow for covariates, repeated cross-section data, and two-by-two comparisons in staggered adoption designs. Furthermore, we present identification results for the ATT of three additional empirically relevant latent groups by imposing outcome mean dominance assumptions that have intuitive appeal in applications. Finally, two empirical illustrations demonstrate the approach's usefulness by revisiting (i) the effect of a job training program on earnings and (ii) the effect of a working-from-home policy on employee performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09221v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gayani Rathnayake, Akanksha Negi, Otavio Bartalotti, Xueyan Zhao</dc:creator>
    </item>
    <item>
      <title>Joint Inference for the Regression Discontinuity Effect and Its External Validity</title>
      <link>https://arxiv.org/abs/2509.26380</link>
      <description>arXiv:2509.26380v2 Announce Type: replace 
Abstract: The external validity of regression discontinuity designs is crucial for informing policy but is rarely examined in applied work. To advance empirical practice, we propose a joint inference procedure for the treatment effect and its local external validity, captured by the treatment effect derivative (TED), within a robust bias correction framework. We further introduce a locally linear treatment effects assumption, which extends the scope of the TED and enables identification and the construction of a uniform confidence band for extrapolated effects. These methods apply to most empirical studies. Empirical illustrations demonstrate their practical usefulness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26380v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuta Okamoto</dc:creator>
    </item>
    <item>
      <title>Do Test Scores Help Teachers Give Better Track Advice to Students? A Principal Stratification Analysis</title>
      <link>https://arxiv.org/abs/2511.05128</link>
      <description>arXiv:2511.05128v3 Announce Type: replace 
Abstract: Every year, over one million EU students choose a secondary school track based on teacher recommendations, yet little evidence shows this yields optimal assignments. Using Dutch data, we examine whether access to standardized test scores improves recommendation quality. We develop a Principal-Stratification metric in a quasi-randomized setting, conduct a welfare analysis that flexibly weights short- and long-term losses, and assess principal fairness by examining whether test-score access affects equity across protected attributes. Results are robust to replacing the Exclusion Restriction assumption underlying our main identification strategy with alternative assumptions. Allowing recommendation upgrades when test scores exceed expectations increases successful placement in more demanding tracks by at least 6%, while misplacing 7% of weaker students. Only unrealistically high weights on short-term losses would justify banning such upgrades. Test-score access also yields fairer recommendations for immigrant and low-SES students. Our methodology and findings contribute to the literature on algorithm-assisted human decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05128v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Ichino, Fabrizia Mealli, Javier Viviens</dc:creator>
    </item>
    <item>
      <title>A Quadratic Link between Out-of-Sample $R^2$ and Directional Accuracy</title>
      <link>https://arxiv.org/abs/2602.07841</link>
      <description>arXiv:2602.07841v2 Announce Type: replace 
Abstract: This study provides a novel perspective on the metric disconnect phenomenon in financial time series forecasting through an analytical link that reconciles the out-of-sample $R^2$ ($R^2_{\text{OOS}}$) and directional accuracy (DA). In particular, using the random walk model as a baseline and assuming that sign correctness is independent of the realized magnitude, we show that these two metrics exhibit a quadratic relationship for MSE-optimal point forecasts. For point forecasts with modest DAs, the theoretical value of $R^2_{\text{OOS}}$ is intrinsically negligible. Thus, a negative empirical $R^2_{\text{OOS}}$ is expected if the model is suboptimal or affected by finite sample noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07841v2</guid>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cheng Zhang</dc:creator>
    </item>
    <item>
      <title>VCDF: A Validated Consensus-Driven Framework for Time Series Causal Discovery</title>
      <link>https://arxiv.org/abs/2410.19412</link>
      <description>arXiv:2410.19412v2 Announce Type: replace-cross 
Abstract: Time series causal discovery is essential for understanding dynamic systems, yet many existing methods remain sensitive to noise, non-stationarity, and sampling variability. We propose the Validated Consensus-Driven Framework (VCDF), a simple and method-agnostic layer that improves robustness by evaluating the stability of causal relations across blocked temporal subsets. VCDF requires no modification to base algorithms and can be applied to methods such as VAR-LiNGAM and PCMCI. Experiments on synthetic datasets show that VCDF improves VAR-LiNGAM by approximately 0.08-0.12 in both window and summary F1 scores across diverse data characteristics, with gains most pronounced for moderate-to-long sequences. The framework also benefits from longer sequences, yielding up to 0.18 absolute improvement on time series of length 1000 and above. Evaluations on simulated fMRI data and IT-monitoring scenarios further demonstrate enhanced stability and structural accuracy under realistic noise conditions. VCDF provides an effective reliability layer for time series causal discovery without altering underlying modeling assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19412v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gene Yu, Ce Guo, Wayne Luk</dc:creator>
    </item>
    <item>
      <title>A Powerful Bootstrap Test of Independence in High Dimensions</title>
      <link>https://arxiv.org/abs/2503.21715</link>
      <description>arXiv:2503.21715v3 Announce Type: replace-cross 
Abstract: This paper proposes a nonparametric test of pairwise independence of one random variable from a large pool of other random variables. The test statistic is the maximum of several Chatterjee's rank correlations and critical values are computed via a block multiplier bootstrap. We show in simulations that other popular tests based on distance covariances do not necessarily control size under this null. Our test, on the other hand, is shown to asymptotically control size uniformly over a large class of data-generating processes, even when the number of variables is much larger than sample size. The test is consistent against any fixed alternative. It can be combined with a stepwise procedure for selecting those variables from the pool that violate independence, while controlling the family-wise error rate. All formal results leave the dependence among variables in the pool completely unrestricted. In simulations, we find that our test is typically more powerful than competing methods (in settings where they are valid), particularly in high-dimensional scenarios or when there is dependence among variables in the pool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21715v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauricio Olivares, Tomasz Olma, Daniel Wilhelm</dc:creator>
    </item>
    <item>
      <title>Differentially Private Two-Stage Gradient Descent for Instrumental Variable Regression</title>
      <link>https://arxiv.org/abs/2509.22794</link>
      <description>arXiv:2509.22794v3 Announce Type: replace-cross 
Abstract: We study instrumental variable regression (IVaR) under differential privacy constraints. Classical IVaR methods (like two-stage least squares regression) rely on solving moment equations that directly use sensitive covariates and instruments, creating significant risks of privacy leakage and posing challenges in designing algorithms that are both statistically efficient and differentially private. We propose a noisy two-stage gradient descent algorithm that ensures $\rho$-zero-concentrated differential privacy by injecting carefully calibrated noise into the gradient updates. Our analysis establishes finite-sample convergence rates for the proposed method, showing that the algorithm achieves consistency while preserving privacy. In particular, we derive precise bounds quantifying the trade-off among optimization, privacy, and sampling error. To the best of our knowledge, this is the first work to provide both privacy guarantees and provable convergence rates for instrumental variable regression in linear models. We further validate our theoretical findings with experiments on both synthetic and real datasets, demonstrating that our method offers practical accuracy-privacy trade-offs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22794v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haodong Liang, Yanhao Jin, Krishnakumar Balasubramanian, Lifeng Lai</dc:creator>
    </item>
  </channel>
</rss>
