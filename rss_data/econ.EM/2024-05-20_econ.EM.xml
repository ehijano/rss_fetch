<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 May 2024 04:07:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 20 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Optimal Text-Based Time-Series Indices</title>
      <link>https://arxiv.org/abs/2405.10449</link>
      <description>arXiv:2405.10449v1 Announce Type: new 
Abstract: We propose an approach to construct text-based time-series indices in an optimal way--typically, indices that maximize the contemporaneous relation or the predictive performance with respect to a target variable, such as inflation. We illustrate our methodology with a corpus of news articles from the Wall Street Journal by optimizing text-based indices focusing on tracking the VIX index and inflation expectations. Our results highlight the superior performance of our approach compared to existing indices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10449v1</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <category>q-fin.CP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Ardia, Keven Bluteau</dc:creator>
    </item>
    <item>
      <title>Overcoming Medical Overuse with AI Assistance: An Experimental Investigation</title>
      <link>https://arxiv.org/abs/2405.10539</link>
      <description>arXiv:2405.10539v1 Announce Type: new 
Abstract: This study evaluates the effectiveness of Artificial Intelligence (AI) in mitigating medical overtreatment, a significant issue characterized by unnecessary interventions that inflate healthcare costs and pose risks to patients. We conducted a lab-in-the-field experiment at a medical school, utilizing a novel medical prescription task, manipulating monetary incentives and the availability of AI assistance among medical students using a three-by-two factorial design. We tested three incentive schemes: Flat (constant pay regardless of treatment quantity), Progressive (pay increases with the number of treatments), and Regressive (penalties for overtreatment) to assess their influence on the adoption and effectiveness of AI assistance. Our findings demonstrate that AI significantly reduced overtreatment rates by up to 62% in the Regressive incentive conditions where (prospective) physician and patient interests were most aligned. Diagnostic accuracy improved by 17% to 37%, depending on the incentive scheme. Adoption of AI advice was high, with approximately half of the participants modifying their decisions based on AI input across all settings. For policy implications, we quantified the monetary (57%) and non-monetary (43%) incentives of overtreatment and highlighted AI's potential to mitigate non-monetary incentives and enhance social welfare. Our results provide valuable insights for healthcare administrators considering AI integration into healthcare systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10539v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyi Wang, Lijia Wei, Lian Xue</dc:creator>
    </item>
    <item>
      <title>Macroeconomic Factors, Industrial Indexes and Bank Spread in Brazil</title>
      <link>https://arxiv.org/abs/2405.10655</link>
      <description>arXiv:2405.10655v1 Announce Type: new 
Abstract: The main objective of this paper is to Identify which macroe conomic factors and industrial indexes influenced the total Brazilian banking spread between March 2011 and March 2015. This paper considers subclassification of industrial activities in Brazil. Monthly time series data were used in multivariate linear regression models using Eviews (7.0). Eighteen variables were considered as candidates to be determinants. Variables which positively influenced bank spread are; Default, IPIs (Industrial Production Indexes) for capital goods, intermediate goods, du rable consumer goods, semi-durable and non-durable goods, the Selic, GDP, unemployment rate and EMBI +. Variables which influence negatively are; Consumer and general consumer goods IPIs, IPCA, the balance of the loan portfolio and the retail sales index. A p-value of 05% was considered. The main conclusion of this work is that the progress of industry, job creation and consumption can reduce bank spread. Keywords: Credit. Bank spread. Macroeconomics. Industrial Production Indexes. Finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10655v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlos Alberto Durigan Junior, Andr\'e Taue Saito, Daniel Reed Bergmann, Nuno Manoel Martins Dias Fouto</dc:creator>
    </item>
    <item>
      <title>Simulation-Based Benchmarking of Reinforcement Learning Agents for Personalized Retail Promotions</title>
      <link>https://arxiv.org/abs/2405.10469</link>
      <description>arXiv:2405.10469v1 Announce Type: cross 
Abstract: The development of open benchmarking platforms could greatly accelerate the adoption of AI agents in retail. This paper presents comprehensive simulations of customer shopping behaviors for the purpose of benchmarking reinforcement learning (RL) agents that optimize coupon targeting. The difficulty of this learning problem is largely driven by the sparsity of customer purchase events. We trained agents using offline batch data comprising summarized customer purchase histories to help mitigate this effect. Our experiments revealed that contextual bandit and deep RL methods that are less prone to over-fitting the sparse reward distributions significantly outperform static policies. This study offers a practical framework for simulating AI agents that optimize the entire retail customer journey. It aims to inspire the further development of simulation tools for retail AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10469v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Xia, Sriram Narayanamoorthy, Zhengyuan Zhou, Joshua Mabry</dc:creator>
    </item>
    <item>
      <title>Lassoed Boosting and Linear Prediction in the Equities Market</title>
      <link>https://arxiv.org/abs/2112.08934</link>
      <description>arXiv:2112.08934v3 Announce Type: replace 
Abstract: We consider a two-stage estimation method for linear regression. First, it uses the lasso in Tibshirani (1996) to screen variables and, second, re-estimates the coefficients using the least-squares boosting method in Friedman (2001) on every set of selected variables. Based on the large-scale simulation experiment in Hastie et al. (2020), lassoed boosting performs as well as the relaxed lasso in Meinshausen (2007) and, under certain scenarios, can yield a sparser model. Applied to predicting equity returns, lassoed boosting gives the smallest mean-squared prediction error compared to several other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.08934v3</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiao Huang</dc:creator>
    </item>
    <item>
      <title>Asymptotic equivalence of Principal Components and Quasi Maximum Likelihood estimators in Large Approximate Factor Models</title>
      <link>https://arxiv.org/abs/2307.09864</link>
      <description>arXiv:2307.09864v4 Announce Type: replace 
Abstract: We provide an alternative derivation of the asymptotic results for the Principal Components estimator of a large approximate factor model. Results are derived under a minimal set of assumptions and, in particular, we require only the existence of 4th order moments. A special focus is given to the time series setting, a case considered in almost all recent econometric applications of factor models. Hence, estimation is based on the classical $n\times n$ sample covariance matrix and not on a $T\times T$ covariance matrix often considered in the literature. Indeed, despite the two approaches being asymptotically equivalent, the former is more coherent with a time series setting and it immediately allows us to write more intuitive asymptotic expansions for the Principal Component estimators showing that they are equivalent to OLS as long as $\sqrt n/T\to 0$ and $\sqrt T/n\to 0$, that is the loadings are estimated in a time series regression as if the factors were known, while the factors are estimated in a cross-sectional regression as if the loadings were known. Finally, we give some alternative sets of primitive sufficient conditions for mean-squared consistency of the sample covariance matrix of the factors, of the idiosyncratic components, and of the observed time series, which is the starting point for Principal Component Analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.09864v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi</dc:creator>
    </item>
    <item>
      <title>Imputation of Counterfactual Outcomes when the Errors are Predictable</title>
      <link>https://arxiv.org/abs/2403.08130</link>
      <description>arXiv:2403.08130v2 Announce Type: replace 
Abstract: A crucial input into causal inference is the imputed counterfactual outcome. Imputation error can arise because of sampling uncertainty from estimating the prediction model using the untreated observations, or from out-of-sample information not captured by the model. While the literature has focused on sampling uncertainty, it vanishes with the sample size. Often overlooked is the possibility that the out-of-sample error can be informative about the missing counterfactual outcome if it is mutually or serially correlated. Motivated by the best linear unbiased predictor (\blup) of \citet{goldberger:62} in a time series setting, we propose an improved predictor of potential outcome when the errors are correlated. The proposed \pup\; is practical as it is not restricted to linear models, can be used with consistent estimators already developed, and improves mean-squared error for a large class of strong mixing error processes. Ignoring predictability in the errors can distort conditional inference. However, the precise impact will depend on the choice of estimator as well as the realized values of the residuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08130v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Silvia Goncalves, Serena Ng</dc:creator>
    </item>
    <item>
      <title>Data-driven fixed-point tuning for truncated realized variations</title>
      <link>https://arxiv.org/abs/2311.00905</link>
      <description>arXiv:2311.00905v2 Announce Type: replace-cross 
Abstract: Many methods for estimating integrated volatility and related functionals of semimartingales in the presence of jumps require specification of tuning parameters for their use in practice. In much of the available theory, tuning parameters are assumed to be deterministic and their values are specified only up to asymptotic constraints. However, in empirical work and in simulation studies, they are typically chosen to be random and data-dependent, with explicit choices often relying entirely on heuristics. In this paper, we consider novel data-driven tuning procedures for the truncated realized variations of a semimartingale with jumps based on a type of random fixed-point iteration. Being effectively automated, our approach alleviates the need for delicate decision-making regarding tuning parameters in practice and can be implemented using information regarding sampling frequency alone. We show our methods can lead to asymptotically efficient estimation of integrated volatility and exhibit superior finite-sample performance compared to popular alternatives in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00905v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>B. Cooper Boniece, Jos\'e E. Figueroa-L\'opez, Yuchen Han</dc:creator>
    </item>
  </channel>
</rss>
