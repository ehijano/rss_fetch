<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Mar 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Asymmetric Cross-Correlation in Multivariate Spatial Stochastic Processes: A Primer</title>
      <link>https://arxiv.org/abs/2503.02903</link>
      <description>arXiv:2503.02903v1 Announce Type: new 
Abstract: Multivariate spatial phenomena are ubiquitous, spanning domains such as climate, pandemics, air quality, and social economy. Cross-correlation between different quantities of interest at different locations is asymmetric in general. This paper provides the visualization, structure, and properties of asymmetric cross-correlation as well as symmetric auto-correlation. It reviews mainstream multivariate spatial models and analyzes their capability to accommodate asymmetric cross-correlation. It also illustrates the difference in model accuracy with and without asymmetric accommodation using a 1D simulated example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02903v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaoqing Chen</dc:creator>
    </item>
    <item>
      <title>Determine the Order of Functional Data</title>
      <link>https://arxiv.org/abs/2503.03000</link>
      <description>arXiv:2503.03000v1 Announce Type: new 
Abstract: Dimension reduction is often necessary in functional data analysis, with functional principal component analysis being one of the most widely used techniques. A key challenge in applying these methods is determining the number of eigen-pairs to retain, a problem known as order determination. When a covariance function admits a finite representation, the challenge becomes estimating the rank of the associated covariance operator. While this problem is straightforward when the full trajectories of functional data are available, in practice, functional data are typically collected discretely and are subject to measurement error contamination. This contamination introduces a ridge to the empirical covariance function, which obscures the true rank of the covariance operator. We propose a novel procedure to identify the true rank of the covariance operator by leveraging the information of eigenvalues and eigenfunctions. By incorporating the nonparametric nature of functional data through smoothing techniques, the method is applicable to functional data collected at random, subject-specific points. Extensive simulation studies demonstrate the excellent performance of our approach across a wide range of settings, outperforming commonly used information-criterion-based methods and maintaining effectiveness even in high-noise scenarios. We further illustrate our method with two real-world data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03000v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi Zhang, Peijun Sang, Yingli Qin</dc:creator>
    </item>
    <item>
      <title>Estimating treatment effects with competing intercurrent events in randomized controlled trials</title>
      <link>https://arxiv.org/abs/2503.03049</link>
      <description>arXiv:2503.03049v1 Announce Type: new 
Abstract: The analysis of randomized controlled trials is often complicated by intercurrent events--events that occur after treatment initiation and may impact outcome assessment. These events may lead to patients discontinuing their assigned treatment or dropping out of the trial entirely. In an analysis of data from two recent immunology trials, we categorize intercurrent events into two broad types: those unrelated to treatment (e.g., withdrawal from the study due to external factors like pandemics or relocation) and those related to treatment (e.g., adverse events or lack of efficacy). We adopt distinct strategies to handle each type, aiming to target a clinically more relevant estimand. For treatment-related intercurrent events, they often meaningfully describe the patient's outcome, we employ a composite variable strategy, where we attribute an outcome value that reflects the lack of treatment success. For treatment-unrelated intercurrent events, we adopt a hypothetical strategy that assumes these event times are conditionally independent of the outcome, given treatment and covariates, and envisions a scenario in which the intercurrent events do not occur. We establish the nonparametric identification and semiparametric estimation theory for the causal estimand and introduce doubly robust estimators. We illustrate our methods through the re-analysis of two randomized trials on baricitinib for Systemic Lupus Erythematosus. We classify intercurrent events, apply four estimators, and compare our approach with common ad-hoc methods, highlighting the robustness and practical implications of our framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03049v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sizhu Lu, Yanyao Yi, Yongming Qu, Huayu Karen Liu, Ting Ye, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Meta-analysis of median survival times with inverse-variance weighting</title>
      <link>https://arxiv.org/abs/2503.03065</link>
      <description>arXiv:2503.03065v1 Announce Type: new 
Abstract: We consider the problem of meta-analyzing outcome measures based on median survival times, such as the difference of median survival times between groups. Primary studies with time-to-event outcomes often report estimates of median survival times and corresponding confidence intervals based on the Kaplan-Meier estimator. However, applying conventional inverse-variance weighted methods to meta-analyze outcome measures based on median survival is often challenging because within-study standard error estimates are typically not available in this setting. In this article, we consider an inverse-variance weighted approach to meta-analyze median survival times that estimates the within-study standard errors from the reported confidence intervals. We conduct a series of simulation studies evaluating the performance of this approach at the study level (i.e., for estimating the standard error of median survival) and the meta-analytic level (i.e., for estimating the pooled median, difference of medians, and ratio of medians). We find that this approach often performs comparable to a benchmark approach that uses the true within-study standard errors for meta-analyzing median-based outcome measures. We then illustrate an application of this approach in a meta-analysis evaluating survival benefits of being assigned to experimental arms versus comparator arms in randomized trials for non-small cell lung cancer therapies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03065v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sean McGrath, Jonathan Kimmelman, Omer Ozturk, Russell Steele, Andrea Benedetti</dc:creator>
    </item>
    <item>
      <title>Two-Round Distributed Principal Component Analysis: Closing the Statistical Efficiency Gap</title>
      <link>https://arxiv.org/abs/2503.03123</link>
      <description>arXiv:2503.03123v1 Announce Type: new 
Abstract: We enhance Fan et al.'s (2019) one-round distributed principal component analysis algorithm by adding a second fixed-point iteration round. Random matrix theory reveals the one-round estimator exhibits higher asymptotic error than the pooling estimator under moderate local signal-to-noise ratios. Remarkably, our second iteration round eliminates this efficiency gap. It follows from a careful analysis of the first-order perturbation of eigenspaces. Empirical experiments on synthetic and benchmark datasets consistently demonstrate the two-round method's statistical advantage over the one-round approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03123v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>ZeYu Li, Xinsheng Zhang, Wang Zhou</dc:creator>
    </item>
    <item>
      <title>Causal drivers of dynamic networks</title>
      <link>https://arxiv.org/abs/2503.03333</link>
      <description>arXiv:2503.03333v1 Announce Type: new 
Abstract: Dynamic networks models describe temporal interactions between social actors, and as such have been used to describe financial fraudulent transactions, dispersion of destructive invasive species across the globe, and the spread of fake news. An important question in all of these examples is what are the causal drivers underlying these processes. Current network models are exclusively descriptive and based on correlative structures.
  In this paper we propose a causal extension of dynamic network modelling. In particular, we prove that the causal model satisfies a set of population conditions that uniquely identifies the causal drivers. The empirical analogue of these conditions provide a consistent causal discovery algorithm, which distinguishes it from other inferential approaches. Crucially, data from a single environment is sufficient. We apply the method in an analysis of bike sharing data in Washington D.C. in July 2023.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03333v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Melania Lembo, Ester Riccardi, Veronica Vinciotti, Ernst C. Wit</dc:creator>
    </item>
    <item>
      <title>Functional regression with randomized signatures: An application to age-specific mortality rates</title>
      <link>https://arxiv.org/abs/2503.03513</link>
      <description>arXiv:2503.03513v1 Announce Type: new 
Abstract: We propose a novel extension of the Hyndman-Ullah (HU) model to forecast mortality rates by integrating randomized signatures, referred to as the HU model with randomized signatures (HUrs). Unlike truncated signatures, which grow exponentially with order, randomized signatures, based on the Johnson-Lindenstrauss lemma, are able to approximate higher-order interactions in a computationally feasible way. Using mortality data from four countries, we evaluate the performance of the novel HUrs model compared to two alternative HU model versions. Our empirical results show that the proposed HUrs model performs well, particularly for Bulgarian and Japanese data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03513v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhong Jing Yap, Dharini Pathmanathan, Sophie Dabo-Niang</dc:creator>
    </item>
    <item>
      <title>Inference for Heterogeneous Treatment Effects with Efficient Instruments and Machine Learning</title>
      <link>https://arxiv.org/abs/2503.03530</link>
      <description>arXiv:2503.03530v1 Announce Type: new 
Abstract: We introduce a new instrumental variable (IV) estimator for heterogeneous treatment effects in the presence of endogeneity. Our estimator is based on double/debiased machine learning (DML) and uses efficient machine learning instruments (MLIV) and kernel smoothing. We prove consistency and asymptotic normality of our estimator and also construct confidence sets that are more robust towards weak IV. Along the way, we also provide an accessible discussion of the corresponding estimator for the homogeneous treatment effect with efficient machine learning instruments. The methods are evaluated on synthetic and real datasets and an implementation is made available in the R package IVDML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03530v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyrill Scheidegger, Zijian Guo, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>Semiparametric Growth-Curve Modeling in Hierarchical, Longitudinal Studies</title>
      <link>https://arxiv.org/abs/2503.03550</link>
      <description>arXiv:2503.03550v1 Announce Type: new 
Abstract: Modeling of growth (or decay) curves arises in many fields such as microbiology, epidemiology, marketing, and econometrics. Parametric forms like Logistic and Gompertz are often used for modeling such monotonic patterns. While useful for compact description, the real-life growth curves rarely follow these parametric forms perfectly. Therefore, the curve estimation methods that strike a balance between prior information in the parametric form and fidelity with the observed data are preferred. In hierarchical, longitudinal studies the interest lies in comparing the growth curves of different groups while accounting for the differences between the within-group subjects. This article describes a flexible state space modeling framework that enables semiparametric growth curve modeling for the data generated from hierarchical, longitudinal studies. The methodology, a type of functional mixed effects modeling, is illustrated with a real-life example of bacterial growth in different settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03550v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajesh Selukar</dc:creator>
    </item>
    <item>
      <title>Robust Sparse Precision Matrix Estimation and its Application</title>
      <link>https://arxiv.org/abs/2503.03575</link>
      <description>arXiv:2503.03575v1 Announce Type: new 
Abstract: We address the problem of robust sparse estimation of the precision matrix for heavy-tailed distributions in high-dimensional settings. In such high-dimensional contexts, we observe that the covariance matrix can be approximated by a spatial-sign covariance matrix, scaled by a constant. Based on this insight, we introduce two new procedures, the Spatial-Sign Constrained $l_1$ Inverse Matrix Estimation (SCLIME) and the Spatial-sign Graphic LASSO Estimation (SGLASSO), to estimate the precision matrix. Under mild regularity conditions, we establish that the consistency rate of these estimators matches that of existing estimators from the literature. To demonstrate its practical utility, we apply the proposed estimator to two classical problems: the elliptical graphical model and linear discriminant analysis. Through extensive simulation studies and real data applications, we show that our estimators outperforms existing methods, particularly in the presence of heavy-tailed distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03575v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengke Lu, Long Feng</dc:creator>
    </item>
    <item>
      <title>Empowering Multi-class Classification for Complex Functional Data with Simultaneous Feature Selection</title>
      <link>https://arxiv.org/abs/2503.03679</link>
      <description>arXiv:2503.03679v1 Announce Type: new 
Abstract: The opportunity to utilize complex functional data types for conducting classification tasks is emerging with the growing availability of imaging data. However, the tools capable of effectively managing imaging data are limited, let alone those that can further leverage other one-dimensional functional data. Inspired by the extensive data provided by the Alzheimer's Disease Neuroimaging Initiative (ADNI), we introduce a novel classifier tailored for complex functional data. Each observation in this framework may be associated with numerous functional processes, varying in dimensions, such as curves and images. Each predictor is a random element in an infinite dimensional function space, and the number of functional predictors p can potentially be much greater than the sample size n. In this paper, we introduce a novel and scalable classifier termed functional BIC deep neural network. By adopting a sparse deep Rectified Linear Unit network architecture and incorporating the LassoNet algorithm, the proposed unified model performs feature selection and classification simultaneously, which is contrast to the existing functional data classifiers. The challenge arises from the complex inter-correlation structures among multiple functional processes, and at meanwhile without any assumptions on the distribution of these processes. Simulation study and real data application are carried out to demonstrate its favorable performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03679v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuoyang Wang, Guanqun Cao, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>Causal language jumps in clinical practice guidelines for diabetes management</title>
      <link>https://arxiv.org/abs/2503.03557</link>
      <description>arXiv:2503.03557v1 Announce Type: cross 
Abstract: Clinical practice guidelines are designed to guide clinical practice and involve causal language. Sometimes guidelines make or require stronger causal claims than those in the references they rely on, a phenomenon we refer to as 'causal language jump'. We evaluated the strength of expressed causation in diabetes guidelines and the evidence they reference to assess the pattern of jumps. We randomly sampled 300 guideline statements from four diabetes guidelines. We rated the causation strength in the statements and the dependence on causation in recommendations supported by these statements using existing scales. Among the causal statements, the cited original studies were similarly assessed. We also assessed how well they report target trial emulation (TTE) components as a proxy for reliability. Of the sampled statements, 114 (38.0%) were causal, and 76 (66.7%) expressed strong causation. 27.2% (31/114) of causal guideline statements demonstrated a "causal language jump", and 34.9% (29/83) of guideline recommendations cannot be effectively supported. Of the 53 eligible studies for TTE rating, most did not report treatment assignment and causal contrast in detail. Our findings suggest causal language jumps were common among diabetes guidelines. While these jumps are sometimes inevitable, they should always be supported by good causal inference practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03557v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Keling Wang, Chang Wei, Jeremy A. Labrecque</dc:creator>
    </item>
    <item>
      <title>Visual tests using several safe confidence intervals</title>
      <link>https://arxiv.org/abs/2503.03567</link>
      <description>arXiv:2503.03567v1 Announce Type: cross 
Abstract: We propose a new statistical hypothesis testing framework which decides visually, using confidence intervals, whether the means of two samples are equal or if one is larger than the other. With our method, the user can at the same time visualize the confidence region of the means and do a test to decide if the means of the two populations are significantly different or not by looking whether the two confidence intervals overlap. To design this test we use confidence intervals constructed using e-variables, which provide a measure of evidence in hypothesis testing. We propose both a sequential test and a non-sequential test based on the overlap of confidence intervals and for each of these tests we give finite-time error bounds on the probabilities of error. We also illustrate the practicality of our method by applying it to the comparison of sequential learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03567v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timoth\'ee Mathieu</dc:creator>
    </item>
    <item>
      <title>Feature Matching Intervention: Leveraging Observational Data for Causal Representation Learning</title>
      <link>https://arxiv.org/abs/2503.03634</link>
      <description>arXiv:2503.03634v1 Announce Type: cross 
Abstract: A major challenge in causal discovery from observational data is the absence of perfect interventions, making it difficult to distinguish causal features from spurious ones. We propose an innovative approach, Feature Matching Intervention (FMI), which uses a matching procedure to mimic perfect interventions. We define causal latent graphs, extending structural causal models to latent feature space, providing a framework that connects FMI with causal graph learning. Our feature matching procedure emulates perfect interventions within these causal latent graphs. Theoretical results demonstrate that FMI exhibits strong out-of-distribution (OOD) generalizability. Experiments further highlight FMI's superior performance in effectively identifying causal features solely from observational data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03634v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoze Li, Jun Xie</dc:creator>
    </item>
    <item>
      <title>Evaluation of binary classifiers for asymptotically dependent and independent extremes</title>
      <link>https://arxiv.org/abs/2112.13738</link>
      <description>arXiv:2112.13738v4 Announce Type: replace 
Abstract: Machine learning classification methods usually assume that all possible classes are sufficiently present within the training set. Due to their inherent rarities, extreme events are always under-represented and classifiers tailored for predicting extremes need to be carefully designed to handle this under-representation. In this paper, we address the question of how to assess and compare classifiers with respect to their capacity to capture extreme occurrences. This is also related to the topic of scoring rules used in forecasting literature. In this context, we propose and study a risk function adapted to extremal classifiers. The inferential properties of our empirical risk estimator are derived under the framework of multivariate regular variation and hidden regular variation. A simulation study compares different classifiers and indicates their performance with respect to our risk function. To conclude, we apply our framework to the analysis of extreme river discharges in the Danube river basin. The application compares different predictive algorithms and test their capacity at forecasting river discharges from other river stations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.13738v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juliette Legrand, Philippe Naveau, Marco Oesting</dc:creator>
    </item>
    <item>
      <title>Correcting for bias due to mismeasured exposure in mediation analysis with a survival outcome</title>
      <link>https://arxiv.org/abs/2304.04868</link>
      <description>arXiv:2304.04868v4 Announce Type: replace 
Abstract: Mediation analysis is widely used in health science research to evaluate the extent to which an intermediate variable explains an observed exposure-outcome relationship. However, the validity of analysis can be compromised when the exposure is measured with error. Motivated by the Health Professionals Follow-up Study (HPFS), we investigate the impact of exposure measurement error on assessing mediation with a survival outcome, based on the Cox proportional hazards outcome model. When the outcome is rare and there is no exposure-mediator interaction, we show that the uncorrected estimators of the natural indirect and direct effects can be biased into either direction, but the uncorrected estimator of the mediation proportion is approximately unbiased as long as the measurement error is not large or the mediator-exposure association is not strong. We develop ordinary regression calibration and risk set regression calibration approaches to correct the exposure measurement error-induced bias when estimating mediation effects and allowing for an exposure-mediator interaction in the Cox outcome model. The proposed approaches require a validation study to characterize the measurement error process. We apply the proposed approaches to the HPFS (1986-2016) to evaluate extent to which reduced body mass index mediates the protective effect of vigorous physical activity on the risk of cardiovascular diseases, and compare the finite-sample properties of the proposed estimators via simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.04868v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Cheng, Donna Spiegelman, Fan Li</dc:creator>
    </item>
    <item>
      <title>Single Proxy Synthetic Control</title>
      <link>https://arxiv.org/abs/2307.16353</link>
      <description>arXiv:2307.16353v4 Announce Type: replace 
Abstract: Synthetic control methods are widely used to estimate the treatment effect on a single treated unit in time-series settings. A common approach to estimate synthetic control weights is to regress the treated unit's pre-treatment outcome and covariates' time series measurements on those of untreated units via ordinary least squares. However, this approach can perform poorly if the pre-treatment fit is not near perfect, whether the weights are normalized or not. In this paper, we introduce a single proxy synthetic control approach, which views the outcomes of untreated units as proxies of the treatment-free potential outcome of the treated unit, a perspective we leverage to construct a valid synthetic control. Under this framework, we establish an alternative identification strategy and corresponding estimation methods for synthetic controls and the treatment effect on the treated unit. Notably, unlike existing proximal synthetic control methods, which require two types of proxies for identification, ours relies on a single type of proxy, thus facilitating its practical relevance. Additionally, we adapt a conformal inference approach to perform inference about the treatment effect, obviating the need for a large number of post-treatment observations. Lastly, our framework can accommodate time-varying covariates and nonlinear models. We demonstrate the proposed approach in a simulation study and a real-world application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16353v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chan Park, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Nonparametric efficient causal estimation of the intervention-specific expected number of recurrent events with continuous-time targeted maximum likelihood and highly adaptive lasso estimation</title>
      <link>https://arxiv.org/abs/2404.01736</link>
      <description>arXiv:2404.01736v2 Announce Type: replace 
Abstract: Longitudinal settings involving outcome, competing risks and censoring events occurring and recurring in continuous time are common in medical research, but are often analyzed with methods that do not allow for taking post-baseline information into account. In this work, we define statistical and causal target parameters via the g-computation formula by carrying out interventions directly on the product integral representing the observed data distribution in a continuous-time counting process model framework. In recurrent events settings our target parameter identifies the expected number of recurrent events also in settings where the censoring mechanism or post-baseline treatment decisions depend on past information of post-baseline covariates such as the recurrent event process. We propose a flexible estimation procedure based on targeted maximum likelihood estimation coupled with highly adaptive lasso estimation to provide a novel approach for double robust and nonparametric inference for the considered target parameter. We illustrate the methods in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01736v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Helene C. W. Rytgaard, Mark J. van der Laan</dc:creator>
    </item>
    <item>
      <title>Parameter identifiability, parameter estimation and model prediction for differential equation models</title>
      <link>https://arxiv.org/abs/2405.08177</link>
      <description>arXiv:2405.08177v5 Announce Type: replace 
Abstract: Interpreting data with mathematical models is an important aspect of real-world industrial and applied mathematical modeling. Often we are interested to understand the extent to which a particular set of data informs and constrains model parameters. This question is closely related to the concept of parameter identifiability, and in this article we present a series of computational exercises to introduce tools that can be used to assess parameter identifiability, estimate parameters and generate model predictions. Taking a likelihood-based approach, we show that very similar ideas and algorithms can be used to deal with a range of different mathematical modeling frameworks. The exercises and results presented in this article are supported by a suite of open access codes that can be accessed on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08177v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew J Simpson, Ruth E Baker</dc:creator>
    </item>
    <item>
      <title>Improving Wald's (approximate) sequential probability ratio test by avoiding overshoot</title>
      <link>https://arxiv.org/abs/2410.16076</link>
      <description>arXiv:2410.16076v3 Announce Type: replace 
Abstract: Wald's sequential probability ratio test (SPRT) is a cornerstone of sequential analysis. Based on desired type-I, II error levels $\alpha, \beta$, it stops when the likelihood ratio crosses certain thresholds, guaranteeing optimality of the expected sample size. However, these thresholds are not closed form and the test is often applied with approximate thresholds $(1-\beta)/\alpha$ and $\beta/(1-\alpha)$ (approximate SPRT). When $\beta &gt; 0$, this neither guarantees error control at $\alpha,\beta$ nor optimality. When $\beta=0$ (power-one SPRT), this method is conservative and not optimal. The looseness in both cases is caused by overshoot: the test statistic overshoots the thresholds at the stopping time. Numerically calculating thresholds may be infeasible, and most software packages do not do this. We improve the approximate SPRT by modifying the test statistic to avoid overshoot. Our `sequential boosting' technique uniformly improves power-one SPRTs $(\beta=0)$ for simple nulls and alternatives, or for one-sided nulls and alternatives in exponential families. When $\beta &gt; 0$, our techniques provide guaranteed error control at $\alpha,\beta$, while needing less samples than the approximate SPRT in our simulations. We also provide several nontrivial extensions: confidence sequences, sampling without replacement and conformal martingales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16076v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>PCM Selector: Penalized Covariate-Mediator Selection Operator for Evaluating Linear Causal Effects</title>
      <link>https://arxiv.org/abs/2412.18180</link>
      <description>arXiv:2412.18180v3 Announce Type: replace 
Abstract: For a data-generating process for random variables that can be described with a linear structural equation model, we consider a situation in which (i) a set of covariates satisfying the back-door criterion cannot be observed or (ii) such a set can be observed, but standard statistical estimation methods cannot be applied to estimate causal effects because of multicollinearity/high-dimensional data problems. We propose a novel two-stage penalized regression approach, the penalized covariate-mediator selection operator (PCM Selector), to estimate the causal effects in such scenarios. Unlike existing penalized regression analyses, when a set of intermediate variables is available, PCM Selector provides a consistent or less biased estimator of the causal effect. In addition, PCM Selector provides a variable selection procedure for intermediate variables to obtain better estimation accuracy of the causal effects than does the back-door criterion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18180v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hisayoshi Nanmo, Manabu Kuroki</dc:creator>
    </item>
    <item>
      <title>Cheap Subsampling bootstrap confidence intervals for fast and robust inference</title>
      <link>https://arxiv.org/abs/2501.10289</link>
      <description>arXiv:2501.10289v3 Announce Type: replace 
Abstract: Bootstrapping is often applied to get confidence limits for semiparametric inference of a target parameter in the presence of nuisance parameters. Bootstrapping with replacement can be computationally expensive and problematic when cross-validation is used in the estimation algorithm due to duplicate observations in the bootstrap samples. We provide a valid, fast, easy-to-implement subsampling bootstrap method for constructing confidence intervals for asymptotically linear estimators and discuss its application to semiparametric causal inference. Our method, inspired by the Cheap Bootstrap (Lam, 2022), leverages the quantiles of a t-distribution and has the desired coverage with few bootstrap replications. We show that the method is asymptotically valid if the subsample size is chosen appropriately as a function of the sample size. We illustrate our method with data from the LEADER trial (Marso et al., 2016), obtaining confidence intervals for a longitudinal targeted minimum loss-based estimator (van der Laan and Gruber, 2012). Through a series of empirical experiments, we also explore the impact of subsample size, sample size, and the number of bootstrap repetitions on the performance of the confidence interval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10289v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johan Sebastian Ohlendorff, Anders Munch, Kathrine Kold S{\o}rensen, Thomas Alexander Gerds</dc:creator>
    </item>
    <item>
      <title>Cohering Disaggregation and Uncertainty Quantification for Spatially Misaligned Data</title>
      <link>https://arxiv.org/abs/2502.10584</link>
      <description>arXiv:2502.10584v2 Announce Type: replace 
Abstract: Spatial misalignment problems arise from both data aggregation and attempts to align misaligned data, leading to information loss. We propose a Bayesian disaggregation framework that links misaligned data to a continuous domain model using an iteratively linearised integration method via integrated nested Laplace approximation (INLA). The framework supports point pattern and aggregated count models under four covariate field scenarios: \textit{Raster at Full Resolution (RastFull), Raster Aggregation (RastAgg), Polygon Aggregation (PolyAgg), and Point Values (PointVal)}. The first three involve aggregation, while the latter two have incomplete fields. For PolyAgg and PointVal, we estimate the full covariate field using \textit{Value Plugin, Joint Uncertainty, and Uncertainty Plugin} methods, with the latter two accounting for uncertainty propagation. These methods demonstrate superior performance, and remain more robust even under model misspecification (i.e.\ modelling a nonlinear field as linear).
  In landslide studies, landslide occurrences are often aggregated into counts based on slope units, reducing spatial detail. The results indicate that point pattern observations and full-resolution covariate fields should be prioritized. For incomplete fields, methods incorporating uncertainty propagation are preferred. This framework supports landslide susceptibility and other spatial mapping, integrating seamlessly with INLA-extension packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10584v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Man Ho Suen, Mark Naylor, Finn Lindgren</dc:creator>
    </item>
    <item>
      <title>A Framework to Analyze Multiscale Sampling MCMC Methods</title>
      <link>https://arxiv.org/abs/2503.00251</link>
      <description>arXiv:2503.00251v2 Announce Type: replace 
Abstract: We consider the theoretical analysis of Multiscale Sampling Methods, which are a new class of gradient-free Markov chain Monte Carlo (MCMC) methods for high dimensional inverse differential equation problems. A detailed presentation of those methods is given, including a review of each MCMC technique that they employ. Then, we propose a two-part framework to study and compare those methods. The first part identifies the new corresponding state space for the chain of random fields, and the second assesses convergence conditions on the instrumental and target distributions. Three Multiscale Sampling Methods are then analyzed using this new framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00251v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lucas Seiffert, Felipe Pereira</dc:creator>
    </item>
    <item>
      <title>HiGrad: Uncertainty Quantification for Online Learning and Stochastic Approximation</title>
      <link>https://arxiv.org/abs/1802.04876</link>
      <description>arXiv:1802.04876v3 Announce Type: replace-cross 
Abstract: Stochastic gradient descent (SGD) is an immensely popular approach for online learning in settings where data arrives in a stream or data sizes are very large. However, despite an ever-increasing volume of work on SGD, much less is known about the statistical inferential properties of SGD-based predictions. Taking a fully inferential viewpoint, this paper introduces a novel procedure termed HiGrad to conduct statistical inference for online learning, without incurring additional computational cost compared with SGD. The HiGrad procedure begins by performing SGD updates for a while and then splits the single thread into several threads, and this procedure hierarchically operates in this fashion along each thread. With predictions provided by multiple threads in place, a $t$-based confidence interval is constructed by decorrelating predictions using covariance structures given by a Donsker-style extension of the Ruppert--Polyak averaging scheme, which is a technical contribution of independent interest. Under certain regularity conditions, the HiGrad confidence interval is shown to attain asymptotically exact coverage probability. Finally, the performance of HiGrad is evaluated through extensive simulation studies and a real data example. An R package \texttt{higrad} has been developed to implement the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:1802.04876v3</guid>
      <category>stat.ML</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weijie J. Su, Yuancheng Zhu</dc:creator>
    </item>
    <item>
      <title>The Fragility of Sparsity</title>
      <link>https://arxiv.org/abs/2311.02299</link>
      <description>arXiv:2311.02299v4 Announce Type: replace-cross 
Abstract: We show, using three empirical applications, that linear regression estimates which rely on the assumption of sparsity are fragile in two ways. First, we document that different choices of the regressor matrix that do not impact ordinary least squares (OLS) estimates, such as the choice of baseline category with categorical controls, can move sparsity-based estimates by two standard errors or more. Second, we develop two tests of the sparsity assumption based on comparing sparsity-based estimators with OLS. The tests tend to reject the sparsity assumption in all three applications. Unless the number of regressors is comparable to or exceeds the sample size, OLS yields more robust inference at little efficiency cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02299v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michal Koles\'ar, Ulrich K. M\"uller, Sebastian T. Roelsgaard</dc:creator>
    </item>
    <item>
      <title>A Meta-Learning Approach to Bayesian Causal Discovery</title>
      <link>https://arxiv.org/abs/2412.16577</link>
      <description>arXiv:2412.16577v3 Announce Type: replace-cross 
Abstract: Discovering a unique causal structure is difficult due to both inherent identifiability issues, and the consequences of finite data. As such, uncertainty over causal structures, such as those obtained from a Bayesian posterior, are often necessary for downstream tasks. Finding an accurate approximation to this posterior is challenging, due to the large number of possible causal graphs, as well as the difficulty in the subproblem of finding posteriors over the functional relationships of the causal edges. Recent works have used meta-learning to view the problem of estimating the maximum a-posteriori causal graph as supervised learning. Yet, these methods are limited when estimating the full posterior as they fail to encode key properties of the posterior, such as correlation between edges and permutation equivariance with respect to nodes. Further, these methods also cannot reliably sample from the posterior over causal structures. To address these limitations, we propose a Bayesian meta learning model that allows for sampling causal structures from the posterior and encodes these key properties. We compare our meta-Bayesian causal discovery against existing Bayesian causal discovery methods, demonstrating the advantages of directly learning a posterior over causal structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16577v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anish Dhir, Matthew Ashman, James Requeima, Mark van der Wilk</dc:creator>
    </item>
    <item>
      <title>Generative Adversarial Networks for High-Dimensional Item Factor Analysis: A Deep Adversarial Learning Algorithm</title>
      <link>https://arxiv.org/abs/2502.10650</link>
      <description>arXiv:2502.10650v2 Announce Type: replace-cross 
Abstract: Advances in deep learning and representation learning have transformed item factor analysis (IFA) in the item response theory (IRT) literature by enabling more efficient and accurate parameter estimation. Variational Autoencoders (VAEs) have been one of the most impactful techniques in modeling high-dimensional latent variables in this context. However, the limited expressiveness of the inference model based on traditional VAEs can still hinder the estimation performance. We introduce Adversarial Variational Bayes (AVB) algorithms as an improvement to VAEs for IFA with improved flexibility and accuracy. By bridging the strengths of VAEs and Generative Adversarial Networks (GANs), AVB incorporates an auxiliary discriminator network to reframe the estimation process as a two-player adversarial game and removes the restrictive assumption of standard normal distributions in the inference model. Theoretically, AVB can achieve similar or higher likelihood compared to VAEs. A further enhanced algorithm, Importance-weighted Adversarial Variational Bayes (IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE). In an exploratory analysis of empirical data, IWAVB demonstrated superior expressiveness by achieving a higher likelihood compared to IWAE. In confirmatory analysis with simulated data, IWAVB achieved similar mean-square error results to IWAE while consistently achieving higher likelihoods. When latent variables followed a multimodal distribution, IWAVB outperformed IWAE. With its innovative use of GANs, IWAVB is shown to have the potential to extend IFA to handle large-scale data, facilitating the potential integration of psychometrics and multimodal data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10650v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nanyu Luo, Feng Ji</dc:creator>
    </item>
  </channel>
</rss>
