<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Nov 2025 02:36:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Particle method for stationary transport equations</title>
      <link>https://arxiv.org/abs/2511.08774</link>
      <description>arXiv:2511.08774v1 Announce Type: new 
Abstract: We present and study a Particle method for the stationary solutions of a class of transport equations. This method is inspired by non-stationary Particle methods, the time variable being replaced by one spatial variable. Particles trajectories are computed using the ``time-dependent'' equations, and then the approximation is based on a quadrature method using the particle locations as quadrature points. We prove the convergence of the scheme under suitable regularity assumptions on the data and the solution, together with a ``characteristic completeness'' assumption (the characteristic curves fullfill the whole computational domain). We also provide an error estimate. The scheme is tested numerically on a two dimensional linear equation and we present a numerical study of convergence. Finally, we use this method to carry out numerical simulations of a landscape evolution model, where an erodible topography evolves under the effects of water erosion and sedimentation. The scheme is then useful to deal with wet/dry areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08774v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rafael Bailo, Julie Binard, Pierre Degond, Pascal Noble</dc:creator>
    </item>
    <item>
      <title>A Neural-Operator Preconditioned Newton Method for Accelerated Nonlinear Solvers</title>
      <link>https://arxiv.org/abs/2511.08811</link>
      <description>arXiv:2511.08811v1 Announce Type: new 
Abstract: We propose a novel neural preconditioned Newton (NP-Newton) method for solving parametric nonlinear systems of equations. To overcome the stagnation or instability of Newton iterations caused by unbalanced nonlinearities, we introduce a fixed-point neural operator (FPNO) that learns the direct mapping from the current iterate to the solution by emulating fixed-point iterations. Unlike traditional line-search or trust-region algorithms, the proposed FPNO adaptively employs negative step sizes to effectively mitigate the effects of unbalanced nonlinearities. Through numerical experiments we demonstrate the computational efficiency and robustness of the proposed NP-Newton method across multiple real-world applications, especially for very strong nonlinearities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08811v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Youngkyu Lee, Shanqing Liu, Jerome Darbon, George Em Karniadakis</dc:creator>
    </item>
    <item>
      <title>New perturbation bounds for low rank approximation of matrices via contour analysis</title>
      <link>https://arxiv.org/abs/2511.08875</link>
      <description>arXiv:2511.08875v1 Announce Type: new 
Abstract: Let $A$ be an $m \times n$ matrix with rank $r$ and spectral decomposition $A = \sum _{i=1}^r \sigma_i u_i v_i^\top, $ where $\sigma_i$ are its singular values, ordered decreasingly, and $u_i, v_i$ are the corresponding left and right singular vectors. For a parameter $1 \le p \le r$, $A_p := \sum_{i=1}^p \sigma_i u_i v_i^\top$ is the best rank $p$ approximation of $A$. In practice, one often chooses $p$ to be small, leading the commonly used phrase "low-rank approximation". Low-rank approximation plays a central role in data science because it can substantially reduce the dimensionality of the original data, the matrix $A$. For a large data matrix $A$, one typically computes a rank-$p$ approximation $A_p$ for a suitably chosen small $p$, stores $A_p$, and uses it as input for further computations. The reduced dimension of $A_p$ enables faster computations and significant data compression. In practice, noise is inevitable. We often have access only to noisy data $\tilde A = A + E$, where $E$ represents the noise. Consequently, the low-rank approximation used as input in many downstream tasks is $\tilde A_p$, the best rank $p$ approximation of $\tilde A$, rather than $A_p$. Therefore, it is natural and important to estimate the error $ \| \tilde A_p - A_p \|$. In this paper, we develop a new method (based on contour analysis) to bound $\| \tilde A_p - A_p \|$. We introduce new parameters that measure the skewness between the noise matrix $E$ and the singular vectors of $A$, and exploit these to obtain notable improvements in many popular settings. This method is of independent interest and has many further applications. We focus on the case where $A$ itself has relatively low rank. This assumption is frequently met in practice, and we think that this case deserves a separate, accessible treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08875v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>math.SP</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Phuc Tran, Van Vu</dc:creator>
    </item>
    <item>
      <title>Generalized Singular Value Decompositions of Dual Quaternion Matrix Triplets</title>
      <link>https://arxiv.org/abs/2511.08885</link>
      <description>arXiv:2511.08885v1 Announce Type: new 
Abstract: In signal processing and identification, generalized singular value decomposition (GSVD), related to a sequence of matrices in product/quotient form are essential numerical linear algebra tools. On behalf of the growing demand for efficient processing of coupled rotation-translation signals in modern engineering, we introduce the restricted SVD of a dual quaternion matrix triplet $(\boldsymbol{A},\boldsymbol{B},\boldsymbol{C})$ with $\boldsymbol{A}\in {\bf \mathbb{DQ}}^{m \times n}$, $\boldsymbol{B} \in {\bf \mathbb{DQ}}^{m \times p}$, $\boldsymbol{C} \in {\bf \mathbb{DQ}}^{q\times n}$, and the product-product SVD of a dual quaternion matrix triplet $(\boldsymbol{A},\boldsymbol{B},\boldsymbol{C})$ with $\boldsymbol{A}\in {\bf \mathbb{DQ}}^{m \times n}$, $\boldsymbol{B} \in {\bf \mathbb{DQ}}^{n \times p}$, $\boldsymbol{C} \in {\bf \mathbb{DQ}}^{p\times q}$. The two types of GSVDs represent a sophisticated matrix factorization that accounts for a given dual quaternion matrix in conjunction with two additional dual quaternion matrices. The decompositions can be conceptualized as an adaptation of the standard SVD, where the distinctive feature lies in the application of distinct inner products to the row and column spaces. Two examples are outlined to illustrate the feasibility of the decompositions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08885v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sitao Ling, Wenxuan Ma, Musheng Wei</dc:creator>
    </item>
    <item>
      <title>A polynomially accelerated fixed-point iteration for vector problems</title>
      <link>https://arxiv.org/abs/2511.09012</link>
      <description>arXiv:2511.09012v1 Announce Type: new 
Abstract: Fixed-point procedures frequently slow down because an error mode decays much more slowly than the others, leaving the base iteration with a persistent residual plateau. To counter this obstruction we formulate a three-point polynomial accelerator (TPA) that fits inside existing fixed-point algorithms with negligible modification and computational cost. TPA first infers the dominant contraction factor directly from the residual dynamics and then assembles a regularised three-point update from the last three iterates. We show that across a suite of tests: a linear system with clustered eigenvalues, a nonlinear tanh mapping, and a discretised Poisson equation, TPA attains a prescribed tolerance in markedly fewer map evaluations than Picard iteration, weighted Jacobi/SOR, and shallow Anderson schemes while preserving a minimal memory and arithmetic footprint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09012v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Alemanno</dc:creator>
    </item>
    <item>
      <title>Recursive algorithms for computing Birkhoff interpolation polynomials</title>
      <link>https://arxiv.org/abs/2511.09014</link>
      <description>arXiv:2511.09014v1 Announce Type: new 
Abstract: As a generalization of Hermite interpolation problem, Birkhoff interpolation is an important subject in numerical approximation. This paper generalizes the existing Generalized Recursive Polynomial Interpolation Algorithm (GRPIA) that is used to compute the Hermite interpolation polynomial. Based on the theory of the Schur complement and the Sylvester identity, the proposed recursive algorithms are applicable to a broader class of Birkhoff interpolation problems, where each interpolation condition is given by the composition of an evaluation functional and a differential polynomial. The approach incorporates a judgment condition to ensure the problem's well-posedness and computes a lower-degree Newton-type interpolation basis (which is also a strongly proper interpolation basis) along with the corresponding interpolation polynomial. Compared with existing algorithms that rely on Gaussian elimination to compute the interpolation basis, our recursive approach significantly reduces the computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09014v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xue Jiang, Yuanhe Li, Zhe Li</dc:creator>
    </item>
    <item>
      <title>A lattice algorithm with multiple shifts for function approximation in Korobov spaces</title>
      <link>https://arxiv.org/abs/2511.09071</link>
      <description>arXiv:2511.09071v1 Announce Type: new 
Abstract: In this paper, we propose a novel algorithm for function approximation in a weighted Korobov space based on shifted rank-1 lattice rules. To mitigate aliasing errors inherent in lattice-based Fourier coefficient estimation, we employ $\mathcal{O}((\log N)^{d} )$ good shifts and recover each Fourier coefficient via a least-squares procedure. We show that the resulting approximation achieves the optimal convergence rate for the $L_{\infty}$-approximation error in the worst-case setting, namely $\mathcal{O}(N^{-\alpha+1/2+\varepsilon})$ for arbitrarily small $\varepsilon&gt;0$. Moreover, by incorporating random shifts, the algorithm attains the optimal rate for the $L_{2}$-approximation error in the randomized setting, which is $\mathcal{O}(N^{-\alpha+\varepsilon})$. Numerical experiments are presented to support the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09071v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mou Cai, Josef Dick, Takashi Goda</dc:creator>
    </item>
    <item>
      <title>Finite Volume Analysis of the Poisson Problem via a Reduced Discontinuous Galerkin Space</title>
      <link>https://arxiv.org/abs/2511.09099</link>
      <description>arXiv:2511.09099v1 Announce Type: new 
Abstract: In this paper, we propose and analyze a high-order finite volume method for the Poisson problem based on the reduced discontinuous Galerkin (RDG) space. The main idea is to employ the RDG space as the trial space and the piecewise constant space as the test space, thereby formulating the scheme in a Petrov-Galerkin framework. This approach inherits the local conservation property of finite volume methods while benefiting from the approximation capabilities of discontinuous Galerkin spaces with significantly fewer degrees of freedom. We establish a rigorous error analysis of the proposed scheme: in particular, we prove optimal-order convergence in the DG energy norm and suboptimal-order convergence in \(L^2\) norm. The theoretical analysis is supported by a set of one- and two-dimensional numerical experiments with Dirichlet and periodic boundary conditions, which confirm both the accuracy and efficiency of the method. The significance of this work lies in bridging finite volume and discontinuous Galerkin methodologies through the RDG space, thus enabling finite volume schemes with a mathematically rigorous convergence theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09099v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wenbo Hu, Yinhua Xia</dc:creator>
    </item>
    <item>
      <title>Optimal convergence rates of an adaptive finite element method for unbounded domains</title>
      <link>https://arxiv.org/abs/2511.09145</link>
      <description>arXiv:2511.09145v1 Announce Type: new 
Abstract: We consider linear reaction-diffusion equations posed on unbounded domains, and discretized by adaptive Lagrange finite elements. To obtain finite-dimensional spaces, it is necessary to introduce a truncation boundary, whereby only a bounded computational subdomain is meshed, leading to an approximation of the solution by zero in the remainder of the domain. We propose a residual-based error estimator that accounts for both the standard discretization error as well as the effect of the truncation boundary. This estimator is shown to be reliable and efficient under appropriate assumptions on the triangulation. Based on this estimator, we devise an adaptive algorithm that automatically refines the mesh and pushes the truncation boundary towards infinity. We prove that this algorithm converges and even achieves optimal rates in terms of the number of degrees of freedom. We finally provide numerical examples illustrating our key theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09145v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Th\'eophile Chaumont-Frelet, Gregor Gantner</dc:creator>
    </item>
    <item>
      <title>A surrogate-based approach to accelerate the design and build phases of reinforced concrete bridges</title>
      <link>https://arxiv.org/abs/2511.09273</link>
      <description>arXiv:2511.09273v1 Announce Type: new 
Abstract: Integrating uncertainties in the design process of reinforced concrete rail bridges, in a fully probabilistic framework, makes their design more complex and challenging. To propagate these uncertainties and convey their influence on the performance of the engineering system, a high-dimensional design space is supposed to be explored. A great challenge to be considered here lies in the computational burden as conducting such an exploration campaign requires substantial calls to computationally expensive finite element simulations. To address this challenge, a surrogate model mapping the design space to the reinforced concrete bridge performance functions is developed in the context of an active learning algorithm. The importance of this model lies in its ability to explore as many design scenarios as possible with minimal computational resources and classify the design scenarios into failure and safe scenarios. This work considers a 4-span reinforced concrete bridge deck. A multi-fiber finite element model of this beam is developed in Cast3m to generate the required design of experiments for the surrogate model. A performance comparison is undertaken to evaluate the Kriging surrogate model effectiveness with and without active learning while the reliability of Kriging predictions is also assessed in comparison to PC-Kriging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09273v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.class-ph</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>RUGC 2025, Jun 2025, Marseille, France</arxiv:journal_reference>
      <dc:creator>Mouhammed Achhab (LMPS), Pierre Jehel (LMPS), Fabrice Gatuingt (LMPS)</dc:creator>
    </item>
    <item>
      <title>Dual Weighted Residual-driven adaptive mesh refinement to enhance biomechanical simulations</title>
      <link>https://arxiv.org/abs/2511.09333</link>
      <description>arXiv:2511.09333v1 Announce Type: new 
Abstract: This chapter describes how a posteriori error estimates targeting a user-defined quantity of interest, using the Dual Weighted Residual (DWR) technique, can be easily applied for biomechanical simulations in current engineering practice. The proposed method considers a very general setting that encompasses complex geometries, model non-linearities (hyperelasticity, fluid-structure interaction) and multi-goal oriented techniques. The developments are substantiated with some numerical tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09333v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roland Becker, Franz Chouly, Michel Duprez, Thomas Richter, Pierre-Yves Rohan, Thomas Wick</dc:creator>
    </item>
    <item>
      <title>A coupled finite element-virtual element method for thermomechanical analysis of electronic packaging structures</title>
      <link>https://arxiv.org/abs/2511.09348</link>
      <description>arXiv:2511.09348v1 Announce Type: new 
Abstract: This study presents a finite element and virtual element (FE-VE) coupled method for thermomechanical analysis in electronic packaging structures. The approach partitions computational domains strategically, employing FEM for regular geometries to maximize computational efficiency and VEM for complex shapes to enhance geometric flexibility. Interface compatibility is maintained through coincident nodal correspondence, ensuring solution continuity across domain boundaries while reducing meshing complexity and computational overhead. Validation through electronic packaging applications demonstrates reasonable agreement with reference solutions and acceptable convergence characteristics across varying mesh densities. The method effectively captures thermal distributions and stress concentrations in multi-material systems, establishing a practical computational framework for electronic packaging analysis involving complex geometries. Source codes are available at https://github.com/yanpeng-gong/FeVeCoupled-ElectronicPackaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09348v1</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanpeng Gong, Sishuai Li, Fei Qin, Yue Mei, Xiaoying Zhuang, Timon Rabczuk</dc:creator>
    </item>
    <item>
      <title>A Semi-Convergent Stage-Wise Framework with Provable Global Convergence for Adaptive Total Variation Regularization</title>
      <link>https://arxiv.org/abs/2511.09357</link>
      <description>arXiv:2511.09357v1 Announce Type: new 
Abstract: Image restoration requires a careful balance between noise suppression and structure preservation. While first-order total variation (TV) regularization effectively preserves edges, it often introduces staircase artifacts, whereas higher-order TV removes such artifacts but oversmooths fine details. To reconcile these competing effects, we propose a semi-convergent stage-wise framework that sequentially integrates first- and higher-order TV regularizers within an iterative restoration process implemented via ADMM. Each stage exhibits semi-convergence behavior, i.e., the iterates initially approach the ground truth before being degraded by over-regularization. By monitoring this evolution, the algorithm adaptively selects the locally optimal iterate (e.g., with the highest PSNR) and propagates it as the initial point for the next stage. This select-and-propagate mechanism effectively transfers local semi-convergence into a globally convergent iterative process. We establish theoretical guarantees showing that the sequence of stage-wise iterates is bounded, the objective values decrease monotonically. Extensive numerical experiments on denoising and deblurring benchmarks confirm that the proposed method achieves superior quantitative and perceptual performance compared with conventional first-, higher-order, hybrid TV methods, and learning based methods, while maintaining theoretical interpretability and algorithmic simplicity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09357v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liang Luo, Lei Zhang</dc:creator>
    </item>
    <item>
      <title>Numerical analysis and efficient implementation of fast collocation methods for fractional Laplacian model on nonuniform grids</title>
      <link>https://arxiv.org/abs/2511.09367</link>
      <description>arXiv:2511.09367v1 Announce Type: new 
Abstract: We propose a fast collocation method based on Krylov subspace iterative solver on general nonuniform grids for the fractional Laplacian problem, in which the fractional operator is presented in a singular integral formulation. The method is proved to be uniquely solvable on general nonuniform grids for $\alpha\in(0,1)$, provided that the sum-of-exponentials (SOE) approximation is sufficiently accurate. In addition, a modified scheme is developed and proved to be uniquely solvable on uniform grids for $\alpha\in(0,2)$. Efficient implementation of the proposed fast collocation schemes based on fast matrix-vector multiplication is carefully discussed, in terms of computational complexity and memory requirement. To further improve computational efficiency, a banded preconditioner is incorporated into the Krylov subspace iterative solver. A rigorous maximum-norm error analysis for $\alpha\in(0,1)$ is presented on specific graded grids, which shows that the convergence order depends on the grading parameter. Numerical experiments validate the predicted convergence and demonstrate the efficiency of the fast collocation schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09367v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meijie Kong, Hongfei Fu</dc:creator>
    </item>
    <item>
      <title>Efficient Numerical Evaluation of Triple Integral Using the Euler's Method and Richardson's Extrapolation</title>
      <link>https://arxiv.org/abs/2511.09408</link>
      <description>arXiv:2511.09408v1 Announce Type: new 
Abstract: In this study, we employ Euler's method and Richardson's extrapolation to solve a triple integral, which is then transformed into a third-order initial value problem. Our objective is to resolve the computational challenges associated with triple integration by transforming it into an initial value problem. Euler's method is the fundamental numerical technique for approximating the solution, thereby establishing a baseline for accuracy. The precision of our computations is subsequently improved by employing Richardson's extrapolation to reduce errors systematically. This approach not only illustrates the adaptability of numerical methods in solving intricate mathematical problems, but it also emphasizes the significance of strategic error reduction techniques in enhancing computational outcomes. We present the efficacy of this method in solving triple integrals in an efficient manner through experimentation and analysis, thereby making a significant contribution to the fields of numerical computation and mathematical modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09408v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shubhangini Gupta, Prashant Sharma, Tamal Pramanick</dc:creator>
    </item>
    <item>
      <title>Physics-Informed Machine Learning for Characterizing System Stability</title>
      <link>https://arxiv.org/abs/2511.08831</link>
      <description>arXiv:2511.08831v1 Announce Type: cross 
Abstract: In the design and operation of complex dynamical systems, it is essential to ensure that all state trajectories of the dynamical system converge to a desired equilibrium within a guaranteed stability region. Yet, for many practical systems -- especially in aerospace -- this region cannot be determined a priori and is often challenging to compute. One of the most common methods for computing the stability region is to identify a Lyapunov function. A Lyapunov function is a positive function whose time derivative along system trajectories is non-positive, which provides a sufficient condition for stability and characterizes an estimated stability region. However, existing methods of characterizing a stability region via a Lyapunov function often rely on explicit knowledge of the system governing equations. In this work, we present a new physics-informed machine learning method of characterizing an estimated stability region by inferring a Lyapunov function from system trajectory data that treats the dynamical system as a black box and does not require explicit knowledge of the system governing equations. In our presented Lyapunov function Inference method (LyapInf), we propose a quadratic form for the unknown Lyapunov function and fit the unknown quadratic operator to system trajectory data by minimizing the average residual of the Zubov equation, a first-order partial differential equation whose solution yields a Lyapunov function. The inferred quadratic Lyapunov function can then characterize an ellipsoidal estimate of the stability region. Numerical results on benchmark examples demonstrate that our physics-informed stability analysis method successfully characterizes a near-maximal ellipsoid of the system stability region associated with the inferred Lyapunov function without requiring knowledge of the system governing equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08831v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomoki Koike, Elizabeth Qian</dc:creator>
    </item>
    <item>
      <title>When is a System Discoverable from Data? Discovery Requires Chaos</title>
      <link>https://arxiv.org/abs/2511.08860</link>
      <description>arXiv:2511.08860v1 Announce Type: cross 
Abstract: The deep learning revolution has spurred a rise in advances of using AI in sciences. Within physical sciences the main focus has been on discovery of dynamical systems from observational data. Yet the reliability of learned surrogates and symbolic models is often undermined by the fundamental problem of non-uniqueness. The resulting models may fit the available data perfectly, but lack genuine predictive power. This raises the question: under what conditions can the systems governing equations be uniquely identified from a finite set of observations? We show, counter-intuitively, that chaos, typically associated with unpredictability, is crucial for ensuring a system is discoverable in the space of continuous or analytic functions. The prevalence of chaotic systems in benchmark datasets may have inadvertently obscured this fundamental limitation.
  More concretely, we show that systems chaotic on their entire domain are discoverable from a single trajectory within the space of continuous functions, and systems chaotic on a strange attractor are analytically discoverable under a geometric condition on the attractor. As a consequence, we demonstrate for the first time that the classical Lorenz system is analytically discoverable. Moreover, we establish that analytic discoverability is impossible in the presence of first integrals, common in real-world systems. These findings help explain the success of data-driven methods in inherently chaotic domains like weather forecasting, while revealing a significant challenge for engineering applications like digital twins, where stable, predictable behavior is desired. For these non-chaotic systems, we find that while trajectory data alone is insufficient, certain prior physical knowledge can help ensure discoverability. These findings warrant a critical re-evaluation of the fundamental assumptions underpinning purely data-driven discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08860v1</guid>
      <category>math.DS</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>nlin.CD</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zakhar Shumaylov, Peter Zaika, Philipp Scholl, Gitta Kutyniok, Lior Horesh, Carola-Bibiane Sch\"onlieb</dc:creator>
    </item>
    <item>
      <title>A 5D concept for space-time optimal control problems with application to simplified Carreau flow</title>
      <link>https://arxiv.org/abs/2511.09086</link>
      <description>arXiv:2511.09086v1 Announce Type: cross 
Abstract: This work presents a 5D concept to optimizing non-Newtonian fluid flows through a simplified Carreau flow model. We solve the optimization problem by approximating the solution of the KKT System with fully space-time finite element methods instead of the more traditional time-stepping technique combined with spatial finite element discretization. Therein, the finite element method is formulated in 3D in space, 1D in time, and 1D in the optimization loop, yielding a 5D overall framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09086v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S. Beuchler, B. Endtmayer, U. Langer, A. Schafelner, T. Wick</dc:creator>
    </item>
    <item>
      <title>Classical Optimization Strategies for Variational Quantum Algorithms: A Systematic Study of Noise Effects and Parameter Efficiency</title>
      <link>https://arxiv.org/abs/2511.09314</link>
      <description>arXiv:2511.09314v1 Announce Type: cross 
Abstract: This study systematically benchmarks classical optimization strategies for the Quantum Approximate Optimization Algorithm when applied to Generalized Mean-Variance Problems under near-term Noisy Intermediate-Scale Quantum conditions. We evaluate Dual Annealing, Constrained Optimization by Linear Approximation, and the Powell Method across noiseless, sampling noise, and two thermal noise models. Our Cost Function Landscape Analysis revealed that the Quantum Approximate Optimization Algorithm angle parameters $\gamma$ were largely inactive in the noiseless regime. This insight motivated a parameter-filtered optimization approach, in which we focused the search space exclusively on the active $\beta$ parameters. This filtering substantially improved parameter efficiency for fast optimizers like Constrained Optimization by Linear Approximation (reducing evaluations from 21 to 12 in the noiseless case) and enhanced robustness, demonstrating that leveraging structural insights is an effective architecture-aware noise mitigation strategy for Variational Quantum Algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09314v1</guid>
      <category>quant-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom\'a\v{s} Bezd\v{e}k, Haomu Yuan, Vojt\v{e}ch Nov\'ak, Silvie Ill\'esov\'a, Martin Beseda</dc:creator>
    </item>
    <item>
      <title>Convergence rate of Euler-Maruyama scheme for McKean-Vlasov SDEs with density-dependent drift</title>
      <link>https://arxiv.org/abs/2412.19121</link>
      <description>arXiv:2412.19121v2 Announce Type: replace 
Abstract: In this paper, we study well-posedness of McKean-Vlasov stochastic differential equations (SDE) whose drift depends pointwisely on marginal density and satisfies a local integrability condition in time-space variables. The drift and noise coefficients are assumed to be Lipschitz continuous in distribution variable with respect to Wasserstein metric $W_p$. Our approach is by approximation with mollifiers. We prove strong existence of a solution. Weak and strong uniqueness are obtained when $p=1$, the drift coefficient is bounded, and the diffusion coefficient is distribution free.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19121v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anh-Dung Le (TSE-R), St\'ephane Villeneuve (TSE-R)</dc:creator>
    </item>
    <item>
      <title>A Fast Direct Solver for Boundary Integral Equations Using Quadrature By Expansion</title>
      <link>https://arxiv.org/abs/2504.13809</link>
      <description>arXiv:2504.13809v5 Announce Type: replace 
Abstract: We construct and analyze a hierarchical direct solver for linear systems arising from the discretization of boundary integral equations using the Quadrature by Expansion (QBX) method. Our scheme builds on the existing theory of Hierarchical Semi-Separable (HSS) matrix operators that contain low-rank off-diagonal submatrices. We use proxy-based approximations of the far-field interactions and the Interpolative Decomposition (ID) to construct compressed HSS operators that are used as fast direct solvers for the original system. We describe a number of modifications to the standard HSS framework that enable compatibility with the QBX family of discretization methods. We establish an error model for the direct solver that is based on a multipole expansion of the QBX-mediated proxy interactions and standard estimates for the ID. Based on these theoretical results, we develop an automatic approach for setting scheme parameters based on user-provided error tolerances. The resulting solver seamlessly generalizes across two- and tree-dimensional problems and achieves state-of-the-art asymptotic scaling. We conclude with numerical experiments that support the theoretical expectations for the error and computational cost of the direct solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13809v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandru Fikl, Andreas Kl\"ockner</dc:creator>
    </item>
    <item>
      <title>On recovering the Radon-Nikodym derivative under the big data assumption</title>
      <link>https://arxiv.org/abs/2506.03891</link>
      <description>arXiv:2506.03891v2 Announce Type: replace 
Abstract: The present paper is focused on the problem of recovering the Radon-Nikodym derivative under the big data assumption. To address the above problem, we design an algorithm that is a combination of the Nystr\"om subsampling and the standard Tikhonov regularization. The convergence rate of the corresponding algorithm is established both in the case when the Radon-Nikodym derivative belongs to RKHS and in the case when it does not. We prove that the proposed approach not only ensures the order of accuracy as algorithms based on the whole sample size, but also allows to achieve subquadratic computational costs in the number of observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03891v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanna Myleiko, Sergei Solodky</dc:creator>
    </item>
    <item>
      <title>Lengthscale-informed sparse grids for kernel methods in high dimensions</title>
      <link>https://arxiv.org/abs/2506.07797</link>
      <description>arXiv:2506.07797v2 Announce Type: replace 
Abstract: Kernel interpolation, especially in the context of Gaussian process emulation, is a widely used technique in surrogate modelling, where the goal is to cheaply approximate an input-output map using a limited number of function evaluations. However, in high-dimensional settings, such methods typically suffer from the curse of dimensionality; the number of required evaluations to achieve a fixed approximation error grows exponentially with the input dimension. To overcome this, a common technique used in high-dimensional approximation methods, such as quasi-Monte Carlo and sparse grids, is to exploit functional anisotropy: the idea that some input dimensions are more 'sensitive' than others. In doing so, such methods can significantly reduce the dimension dependence in the error. In this work, we propose a generalisation of sparse grid methods that incorporates a form of anisotropy encoded by the lengthscale parameter in Mat\'ern kernels. We derive error bounds and perform numerical experiments that show that our approach enables effective emulation over arbitrarily high dimensions for functions exhibiting sufficient anisotropy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07797v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elliot J. Addy, Jonas Latz, Aretha L. Teckentrup</dc:creator>
    </item>
    <item>
      <title>Fully discrete error analysis of finite element discretizations of time-dependent Stokes equations in a stream-function formulation</title>
      <link>https://arxiv.org/abs/2508.06235</link>
      <description>arXiv:2508.06235v3 Announce Type: replace 
Abstract: In this paper we establish best approximation type error estimates for the fully discrete Galerkin solutions of the time-dependent Stokes problem using the stream-function formulation. For the time discretization we use the discontinuous Galerkin method of arbitrary degree, whereas we present the space discretization in a general framework. This makes our result applicable for a wide variety of space discretization methods, provided some Galerkin orthogonality conditions are satisfied. As an example, conformal $C^1$ and $C^0$ interior penalty methods are covered by our analysis. The results do not require any additional regularity assumptions beyond the natural regularity given by the domain and data and can be used for optimal control problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06235v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dmitriy Leykekhman, Boris Vexler, Jakob Wagner</dc:creator>
    </item>
    <item>
      <title>Auto-Adaptive PINNs with Applications to Phase Transitions</title>
      <link>https://arxiv.org/abs/2510.23999</link>
      <description>arXiv:2510.23999v3 Announce Type: replace 
Abstract: We propose an adaptive sampling method for the training of Physics Informed Neural Networks (PINNs) which allows for sampling based on an arbitrary problem-specific heuristic which may depend on the network and its gradients. In particular we focus our analysis on the Allen-Cahn equations, attempting to accurately resolve the characteristic interfacial regions using a PINN without any post-hoc resampling. In experiments, we show the effectiveness of these methods over residual-adaptive frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23999v3</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Buck, Woojeong Kim</dc:creator>
    </item>
    <item>
      <title>Accelerated decomposition of bistochastic kernel matrices by low rank approximation</title>
      <link>https://arxiv.org/abs/2510.26574</link>
      <description>arXiv:2510.26574v2 Announce Type: replace 
Abstract: We develop an accelerated algorithm for computing an approximate eigenvalue decomposition of bistochastic normalized kernel matrices. Our approach constructs a low rank approximation of the original kernel matrix by the pivoted partial Cholesky algorithm and uses it to compute an approximate decomposition of its bistochastic normalization without requiring the formation of the full kernel matrix. The cost of the proposed algorithm depends linearly on the size of the employed training dataset and quadratically on the rank of the low rank approximation, offering a significant cost reduction compared to the naive approach. We apply the proposed algorithm to the kernel based extraction of spatiotemporal patterns from chaotic dynamics, demonstrating its accuracy while also comparing it with an alternative algorithm consisting of subsampling and Nystroem extension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26574v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris Vales, Dimitrios Giannakis</dc:creator>
    </item>
    <item>
      <title>Hyperellipsoid Density Sampling: Exploitative Sequences to Accelerate High-Dimensional Optimization</title>
      <link>https://arxiv.org/abs/2511.07836</link>
      <description>arXiv:2511.07836v2 Announce Type: replace 
Abstract: The curse of dimensionality presents a pervasive challenge in optimization problems, with exponential expansion of the search space rapidly causing traditional algorithms to become inefficient or infeasible. An adaptive sampling strategy is presented to accelerate optimization in this domain as an alternative to uniform quasi-Monte Carlo (QMC) methods.
  This method, referred to as Hyperellipsoid Density Sampling (HDS), generates its sequences by defining multiple hyperellipsoids throughout the search space. HDS uses three types of unsupervised learning algorithms to circumvent high-dimensional geometric calculations, producing an intelligent, non-uniform sample sequence that exploits statistically promising regions of the parameter space and improves final solution quality in high-dimensional optimization problems.
  A key feature of the method is optional Gaussian weights, which may be provided to influence the sample distribution towards known locations of interest. This capability makes HDS versatile for applications beyond optimization, providing a focused, denser sample distribution where models need to concentrate their efforts on specific, non-uniform regions of the parameter space.
  The method was evaluated against Sobol, a standard QMC method, using differential evolution (DE) on the 29 CEC2017 benchmark test functions. The results show statistically significant improvements in solution geometric mean error (p &lt; 0.05), with average performance gains ranging from 3% in 30D to 37% in 10D. This paper demonstrates the efficacy of HDS as a robust alternative to QMC sampling for high-dimensional optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07836v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>cs.NE</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian Soltes</dc:creator>
    </item>
    <item>
      <title>Fast integral methods for the Neumann Green's function: applications to capture and signaling problems in two dimensions</title>
      <link>https://arxiv.org/abs/2511.08458</link>
      <description>arXiv:2511.08458v2 Announce Type: replace 
Abstract: We present a high order numerical method for the solution of the Neumann Green's function in two dimensions. For a general closed planar curve, our computational method resolves both the interior and exterior Green's functions with the source placed either in the bulk or on the surface -- yielding four distinct functions. Our method exactly represents the singular nature of the Green's function by decomposing the singular and regular components. In the case of the interior function, we exactly prescribe an integral constraint which is necessary to obtain a unique solution given the arbitrary constant solution associated with Neumann boundary conditions. Our implementation is based on a fast integral method for the regular part of the Green's function which allows for a rapid and high order discretization for general domains. We demonstrate the accuracy of our method for simple geometries such as disks and ellipses where closed form solutions are available. To exhibit the usefulness of these new routines, we demonstrate several applications to open problems in the capture of Brownian particles, specifically, how the small traps or boundary windows should be configured to maximize the capture rate of Brownian particles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08458v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanchita Chakraborty, Jeremy Hoskins, Alan E. Lindsay</dc:creator>
    </item>
    <item>
      <title>Advancing Lunar Communication through Inter-domain Space Networks and Dynamic Orchestration</title>
      <link>https://arxiv.org/abs/2507.15483</link>
      <description>arXiv:2507.15483v2 Announce Type: replace-cross 
Abstract: The resurgent era of lunar exploration is defined by a strategic shift from temporary visits to a sustained international and commercial presence, resulting in an unprecedented demand for a robust and continuously available communication infrastructure. The conventional direct-to-Earth communication architecture relies on limited and oversubscribed deep space networks, which are further challenged by the radiative environment and insufficient visibility in certain areas of the cislunar domain. We address these issues by proposing a foundational move toward inter-domain space network cooperation by introducing architectures based on near space networks. They can directly service lunar surface users or, via cislunar relays, by forming a resilient and multi-layered communication backbone. First, we establish a unified link analysis framework incorporating frequently disregarded environmental factors, such as the Moon's variable illumination, to provide a high-fidelity performance evaluation. Second, we assess architectures' reliability based on the outage risk, essential for quantifying the operational robustness of communication links. Finally, to manage the inherent dynamism of architectures, we propose an inter-domain space digital twin: a dynamic decision-making engine that performs real-time analysis to autonomously select the best communication path, ensuring high and stable reliability while simultaneously optimizing power consumption. Overall, our paper provides a holistic architectural and conceptual management framework, emphasizing the necessity of lunar communications to support a permanent human and economic foothold on the Moon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15483v2</guid>
      <category>cs.ET</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Selen Gecgel Cetin, Baris Donmez, Gunes Karabulut Kurt</dc:creator>
    </item>
    <item>
      <title>Minimal Denominators Lying in Subsets of the Ring of Polynomials over a Finite Field</title>
      <link>https://arxiv.org/abs/2510.07787</link>
      <description>arXiv:2510.07787v2 Announce Type: replace-cross 
Abstract: Given a subset $\mathcal{S}\subseteq \mathbb{F}_q[x]$ and fixed $n,m\in \mathbb{N}$, one can study the distribution of the value of the smallest denominator $Q\in \mathcal{S}$, for which there exists $\mathbf{P}\in \mathbb{F}_q[x]^m$ such that $\frac{P}{Q}\in B(\boldsymbol{\alpha},q^{-n})$, where $Q\in \mathcal{S}$. On the other hand, one can study the discrete analogue, when $N\in \mathbb{F}_q[x]$ is a polynomial with $deg(N)=n$ and $\boldsymbol{\alpha}\in \frac{1}{N}\mathbb{F}_q[x]^m$ as a discrete probability distribution function. We prove that for any infinite subset $\mathcal{S}\subseteq \mathbb{F}_q[x]$, for any $n\in \mathbb{N}$, and for any dimension $m$, the probability distribution functions of both these random variables are equal to one another. This is significantly stronger than the real setting, where Balazard and Martin proved that these functions have asymptotically close averages, when there are no restrictions on the denominators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07787v2</guid>
      <category>math.NT</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Noy Soffer Aranov</dc:creator>
    </item>
    <item>
      <title>Stochastic representation of solutions for the parabolic Cauchy problem with variable exponent coefficients</title>
      <link>https://arxiv.org/abs/2511.00773</link>
      <description>arXiv:2511.00773v3 Announce Type: replace-cross 
Abstract: In this work, we prove existence and uniqueness of a bounded viscosity solution for the Cauchy problem of degenerate parabolic equations with variable exponent coefficients. We construct the solution directly using the stochastic representation, then verify it satisfies the Cauchy problem. The corresponding SDE, on the other hand, allows the drift and diffusion coefficients to respond nonlinearly to the current state through the state-dependent variable exponents, and thus, extends the expressive power of classical SDEs to better capture complex dynamics. To validate our theoretical framework, we conduct comprehensive numerical experiments comparing finite difference solutions (Crank-Nicolson on logarithmic grids) with Monte Carlo simulations of the SDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00773v3</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mustafa Avci</dc:creator>
    </item>
  </channel>
</rss>
