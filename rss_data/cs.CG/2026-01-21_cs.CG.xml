<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CG updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CG</link>
    <description>cs.CG updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CG" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Jan 2026 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>How to Get Close to the Median Shape</title>
      <link>https://arxiv.org/abs/2601.12529</link>
      <description>arXiv:2601.12529v1 Announce Type: new 
Abstract: $\renewcommand{\Re}{\mathbb{R}}\newcommand{\eps}{{\varepsilon}}\newcommand{\poly}{\mathrm{poly}} $In this paper, we study the problem of $L_1$-fitting a shape to a set of $n$ points in $\Re^d$ (where $d$ is a fixed constant), where the target is to minimize the sum of distances of the points to the shape, or the sum of squared distances. We present a general technique for computing a $(1 + \eps ) $-approximation for such a problem, with running time $O(n + \poly( \log n, 1/\eps))$, where $\poly(\log n, 1/\eps)$ is a polynomial of constant degree of $\log n$ and $1/\eps$ (the power of the polynomial is a function of $d$). The new algorithm runs in linear time for a fixed $\eps&gt;0$, and is the first subquadratic algorithm for this problem.
  Applications of the algorithm include best fitting either a circle, a sphere, or a cylinder to a set of points when minimizing the sum of distances (or squared distances) to the respective shape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12529v1</guid>
      <category>cs.CG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sariel Har-Peled</dc:creator>
    </item>
    <item>
      <title>Optimistic Imprecise Shortest Watchtower in 1.5D and 2.5D</title>
      <link>https://arxiv.org/abs/2601.13165</link>
      <description>arXiv:2601.13165v1 Announce Type: new 
Abstract: A 1.5D imprecise terrain is an $x$-monotone polyline with fixed $x$-coordinates, the $y$-coordinate of each vertex is not fixed but is constrained to be in a given vertical interval. A 2.5D imprecise terrain is a triangulation with fixed $x$ and $y$-coordinates, but the $z$-coordinate of each vertex is constrained to a given vertical interval. Given an imprecise terrain with $n$ intervals, the optimistic shortest watchtower problem asks for a terrain $T$ realized by a precise point in each vertical interval such that the height of the shortest vertical line segment whose lower endpoint lies on $T$ and upper endpoint sees the entire terrain is minimized. In this paper, we present a linear time algorithm to solve the 1.5D optimistic shortest watchtower problem exactly. For the discrete version of the 2.5D case (where the watchtower must be placed on a vertex of $T$), and we give an additive approximation scheme running in $O(\frac{{OPT}}{\varepsilon}n^3)$ time, achieving a solution within an additive error of $\varepsilon$ from the optimal solution value ${OPT}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13165v1</guid>
      <category>cs.CG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bradley McCoy, Binhai Zhu</dc:creator>
    </item>
    <item>
      <title>Classifiers in High Dimensional Hilbert Metrics</title>
      <link>https://arxiv.org/abs/2601.13410</link>
      <description>arXiv:2601.13410v1 Announce Type: new 
Abstract: Classifying points in high dimensional spaces is a fundamental geometric problem in machine learning. In this paper, we address classifying points in the $d$-dimensional Hilbert polygonal metric. The Hilbert metric is a generalization of the Cayley-Klein hyperbolic distance to arbitrary convex bodies and has a diverse range of applications in machine learning and convex geometry. We first present an efficient LP-based algorithm in the metric for the large-margin SVM problem. Our algorithm runs in time polynomial to the number of points, bounding facets, and dimension. This is a significant improvement on previous works, which either provide no theoretical guarantees on running time, or suffer from exponential runtime. We also consider the closely related Funk metric. We also present efficient algorithms for the soft-margin SVM problem and for nearest neighbor-based classification in the Hilbert metric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13410v1</guid>
      <category>cs.CG</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Acharya, Auguste H. Gezalyan, David M. Mount</dc:creator>
    </item>
    <item>
      <title>DCCVT: Differentiable Clipped Centroidal Voronoi Tessellation</title>
      <link>https://arxiv.org/abs/2601.13603</link>
      <description>arXiv:2601.13603v1 Announce Type: new 
Abstract: While Marching Cubes (MC) and Marching Tetrahedra (MTet) are widely adopted in 3D reconstruction pipelines due to their simplicity and efficiency, their differentiable variants remain suboptimal for mesh extraction. This often limits the quality of 3D meshes reconstructed from point clouds or images in learning-based frameworks. In contrast, clipped CVTs offer stronger theoretical guarantees and yield higher-quality meshes. However, the lack of a differentiable formulation has prevented their integration into modern machine learning pipelines. To bridge this gap, we propose DCCVT, a differentiable algorithm that extracts high-quality 3D meshes from noisy signed distance fields (SDFs) using clipped CVTs. We derive a fully differentiable formulation for computing clipped CVTs and demonstrate its integration with deep learning-based SDF estimation to reconstruct accurate 3D meshes from input point clouds. Our experiments with synthetic data demonstrate the superior ability of DCCVT against state-of-the-art methods in mesh quality and reconstruction fidelity. https://wylliamcantincharawi.dev/DCCVT.github.io/</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13603v1</guid>
      <category>cs.CG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wylliam Cantin Charawi, Adrien Gruson, Jane Wu, Christian Desrosiers, Diego Thomas</dc:creator>
    </item>
    <item>
      <title>On the stability, complexity, and distribution of similarity classes of the longest edge bisection process for triangles</title>
      <link>https://arxiv.org/abs/2601.13663</link>
      <description>arXiv:2601.13663v1 Announce Type: new 
Abstract: The Longest Edge Bisection (LEB) of a triangle is performed by joining the midpoint of its longest edge to the opposite vertex. Applying this procedure iteratively produces an infinite family of triangles. Surprisingly, a classical result of Adler (1983) shows that for any initial triangle, this infinite family falls into finitely many similarity classes.
  While the set of classes is finite, we show that a far smaller, stable subset of ``fat'' triangles, called {\bf terminal quadruples}, effectively dominates the final mesh structure. We prove the following asymptotic area distribution result: for every initial triangle, the portion of area occupied by terminal quadruples tends to one, with the convergence occurring at an exponential rate. In fact, we provide the precise distribution of triangles in every step. We introduce the {\bf bisection graph} and use spectral methods to establish this result.
  Given this dominance, we provide a complete characterization of triangles possessing a single terminal quadruple, while conversely exhibiting a sequence of triangles with an unbounded number of terminal quadruples. Furthermore, we reveal several fundamental geometric properties of the points of a terminal quadruple, laying the groundwork for studying the geometric distribution of the entire orbit. Our analysis leverages the hyperbolic geometry framework of Perdomo and Plaza (2014) and refines their techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13663v1</guid>
      <category>cs.CG</category>
      <category>math.CO</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Kalmanovich, Yaar Solomon</dc:creator>
    </item>
    <item>
      <title>Area-universality in Outerplanar Graphs</title>
      <link>https://arxiv.org/abs/2601.13781</link>
      <description>arXiv:2601.13781v1 Announce Type: new 
Abstract: A rectangular floorplan is a partition of a rectangle into smaller rectangles such that no four rectangles meet at a single point. Rectangular floorplans arise naturally in a variety of applications, including VLSI design, architectural layout, and cartography, where efficient and flexible spatial subdivisions are required. A central concept in this domain is that of area-universality: a floorplan (or more generally, a rectangular layout) is area-universal if, for any assignment of target areas to its constituent rectangles, there exists a combinatorially equivalent layout that realizes these areas.
  In this paper, we investigate the structural conditions under which an outerplanar graph admits an area-universal rectangular layout. We establish a necessary and sufficient condition for area-universality in this setting, thereby providing a complete characterization of admissible outerplanar graphs. Furthermore, we present an algorithmic construction that guarantees that the resulting layout is always area-universal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13781v1</guid>
      <category>cs.CG</category>
      <category>math.CO</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ravi Suthar,  Raveena, Krishnendra Shekhawat</dc:creator>
    </item>
    <item>
      <title>Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy</title>
      <link>https://arxiv.org/abs/2601.12257</link>
      <description>arXiv:2601.12257v1 Announce Type: cross 
Abstract: Conventional imaging requires a line of sight to create accurate visual representations of a scene. In certain circumstances, however, obtaining a suitable line of sight may be impractical, dangerous, or even impossible. Non-line-of-sight (NLOS) imaging addresses this challenge by reconstructing the scene from indirect measurements. Recently, passive NLOS methods that use an ordinary photograph of the subtle shadow cast onto a visible wall by the hidden scene have gained interest. These methods are currently limited to 1D or low-resolution 2D color imaging or to localizing a hidden object whose shape is approximately known. Here, we generalize this class of methods and demonstrate a 3D reconstruction of a hidden scene from an ordinary NLOS photograph. To achieve this, we propose a novel reformulation of the light transport model that conveniently decomposes the hidden scene into \textit{light-occluding} and \textit{non-light-occluding} components to yield a separable non-linear least squares (SNLLS) inverse problem. We develop two solutions: A gradient-based optimization method and a physics-inspired neural network approach, which we call Soft Shadow diffusion (SSD). Despite the challenging ill-conditioned inverse problem encountered here, our approaches are effective on numerous 3D scenes in real experimental scenarios. Moreover, SSD is trained in simulation but generalizes well to unseen classes in simulation and real-world NLOS scenes. SSD also shows surprising robustness to noise and ambient illumination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12257v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CG</category>
      <category>cs.GR</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>European Conference on Computer Vision (ECCV 2024)</arxiv:journal_reference>
      <dc:creator>Fadlullah Raji, John Murray-Bruce</dc:creator>
    </item>
    <item>
      <title>Fusing in 3D: Free-Viewpoint Fusion Rendering with a 3D Infrared-Visible Scene Representation</title>
      <link>https://arxiv.org/abs/2601.12697</link>
      <description>arXiv:2601.12697v1 Announce Type: cross 
Abstract: Infrared-visible image fusion aims to integrate infrared and visible information into a single fused image. Existing 2D fusion methods focus on fusing images from fixed camera viewpoints, neglecting a comprehensive understanding of complex scenarios, which results in the loss of critical information about the scene. To address this limitation, we propose a novel Infrared-Visible Gaussian Fusion (IVGF) framework, which reconstructs scene geometry from multimodal 2D inputs and enables direct rendering of fused images. Specifically, we propose a cross-modal adjustment (CMA) module that modulates the opacity of Gaussians to solve the problem of cross-modal conflicts. Moreover, to preserve the distinctive features from both modalities, we introduce a fusion loss that guides the optimization of CMA, thus ensuring that the fused image retains the critical characteristics of each modality. Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12697v1</guid>
      <category>cs.CV</category>
      <category>cs.CG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Yang, Deshui Miao, Chao Tian, Guoqing Zhu, Yameng Gu, Zhenyu He</dc:creator>
    </item>
    <item>
      <title>RPT*: Global Planning with Probabilistic Terminals for Target Search in Complex Environments</title>
      <link>https://arxiv.org/abs/2601.12701</link>
      <description>arXiv:2601.12701v1 Announce Type: cross 
Abstract: Routing problems such as Hamiltonian Path Problem (HPP), seeks a path to visit all the vertices in a graph while minimizing the path cost. This paper studies a variant, HPP with Probabilistic Terminals (HPP-PT), where each vertex has a probability representing the likelihood that the robot's path terminates there, and the objective is to minimize the expected path cost. HPP-PT arises in target object search, where a mobile robot must visit all candidate locations to find an object, and prior knowledge of the object's location is expressed as vertex probabilities. While routing problems have been studied for decades, few of them consider uncertainty as required in this work. The challenge lies not only in optimally ordering the vertices, as in standard HPP, but also in handling history dependency: the expected path cost depends on the order in which vertices were previously visited. This makes many existing methods inefficient or inapplicable. To address the challenge, we propose a search-based approach RPT* with solution optimality guarantees, which leverages dynamic programming in a new state space to bypass the history dependency and novel heuristics to speed up the computation. Building on RPT*, we design a Hierarchical Autonomous Target Search (HATS) system that combines RPT* with either Bayesian filtering for lifelong target search with noisy sensors, or autonomous exploration to find targets in unknown environments. Experiments in both simulation and real robot show that our approach can naturally balance between exploitation and exploration, thereby finding targets more quickly on average than baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12701v1</guid>
      <category>cs.RO</category>
      <category>cs.CG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunpeng Lyu, Chao Cao, Ji Zhang, Howie Choset, Zhongqiang Ren</dc:creator>
    </item>
    <item>
      <title>Freeze-Tag is NP-hard in 2D with $L_1$ distance</title>
      <link>https://arxiv.org/abs/2509.14357</link>
      <description>arXiv:2509.14357v2 Announce Type: replace 
Abstract: The Freeze-Tag Problem (FTP) is a scheduling problem with application in robot swarm activation and was introduced by Arkin et al. in 2002. This problem seeks an efficient way of activating a robot swarm, starting with a single active robot. Activations occur through direct contact, and once a robot becomes active, it can move and help activate other robots. Although the problem has been shown to be NP-hard in the Euclidean plane $\mathbb{R}^2$ under the $L_2$ distance, and in three-dimensional Euclidean space $\mathbb{R}^3$ under any $L_p$ distance with $p \ge 1$, its complexity under the $L_1$ (Manhattan) distance in $\mathbb{R}^2$ has remained an open question. In this paper, we settle this question by proving that FTP is strongly NP-hard in the Euclidean plane with $L_1$ distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14357v2</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lucas de Oliveira Silva, Lehilton Lelis Chaves Pedrosa</dc:creator>
    </item>
    <item>
      <title>Topology-Preserving Line Densification for Creating Contiguous Cartograms</title>
      <link>https://arxiv.org/abs/2511.08121</link>
      <description>arXiv:2511.08121v2 Announce Type: replace 
Abstract: Cartograms depict geographic regions with areas proportional to quantitative data. However, when created using density-equalizing map projections, cartograms may exhibit invalid topologies if boundary polygons are drawn using only a finite set of vertices connected by straight lines. Here we introduce a method for topology-preserving line densification that guarantees that cartogram regions remain connected and non-overlapping when using density-equalizing map projections. By combining our densification technique with a flow-based cartogram generator, we present a robust framework for strictly topology-preserving cartogram construction. Quantitative evaluations demonstrate that the proposed algorithm produces cartograms with greater accuracy and speed than alternative methods while maintaining comparable shape fidelity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08121v2</guid>
      <category>cs.CG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nihal Z. Miaji, Adi Singhania, Matthias E. Goh, Callista Le, Atima Tharatipyakul, Michael T. Gastner</dc:creator>
    </item>
    <item>
      <title>A Unified Architecture for N-Dimensional Visualization and Simulation: 4D Implementation and Evaluation including Boolean Operations</title>
      <link>https://arxiv.org/abs/2512.01501</link>
      <description>arXiv:2512.01501v3 Announce Type: replace 
Abstract: This paper proposes a unified software architecture for visualization and simulation based on a design targeting an $N$-dimensional space. The contribution of this work lies in presenting an architectural configuration that integrates multiple processes into a single software architecture: Quickhull-based convex hull mesh generation, Boolean operations, coordinate transformations for high-dimensional exploration (including orientation and view transformations), and hyperplane slicing for visualization. The proposed approach adopts an approximate implementation that tolerates numerical errors and prioritizes implementation transparency over guarantees of numerical rigor. The experimental results and evaluations presented in this paper are limited to a 4D implementation; no evaluation is conducted for $N&gt;4$, and the discussion is restricted to stating that the architecture itself has a dimension-independent structure. This paper also proposes an interaction design for high-dimensional exploration based on FPS navigation. As an input example involving shape changes over time, a non-rigid body simulation based on XPBD (Extended Position Based Dynamics) is integrated into the 4D implementation. Experimental results confirm that the 4D implementation runs on a single PC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01501v3</guid>
      <category>cs.CG</category>
      <category>cs.GR</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hirohito Arai</dc:creator>
    </item>
  </channel>
</rss>
