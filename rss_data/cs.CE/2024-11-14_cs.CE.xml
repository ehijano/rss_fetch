<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Nov 2024 05:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Design optimization of semiconductor manufacturing equipment using a novel multi-fidelity surrogate modeling approach</title>
      <link>https://arxiv.org/abs/2411.08149</link>
      <description>arXiv:2411.08149v1 Announce Type: new 
Abstract: Careful design of semiconductor manufacturing equipment is crucial for ensuring the performance, yield, and reliability of semiconductor devices. Despite this, numerical optimization methods are seldom applied to optimize the design of such equipment due to the difficulty of obtaining accurate simulation models. In this paper, we address a practical and industrially relevant electrostatic chuck (ESC) design optimization problem by proposing a novel multi-fidelity surrogate modeling approach. The optimization aims to improve the temperature uniformity of the wafer during the etching process by adjusting seven parameters associated with the coolant path and embossing. Our approach combines low-fidelity (LF) and high-fidelity (HF) simulation data to efficiently predict spatial-field quantities, even with a limited number of data points. We use proper orthogonal decomposition (POD) to project the spatially interpolated HF and LF field data onto a shared latent space, followed by the construction of a multi-fidelity kriging model to predict the latent variables of the HF output field. In the ESC design problem, with hundreds or fewer data, our approach achieves a more than 10% reduction in prediction error compared to using kriging models with only HF or LF data. Additionally, in the ESC optimization problem, our proposed method yields better solutions with improvements in all of the quantities of interest, while requiring 20% less data generation cost compared to the HF surrogate modeling approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08149v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bingran Wang, Min Sung Kim, Taewoong Yoon, Dasom Lee, Byeong-Sang Kim, Dougyong Sung, John T. Hwang</dc:creator>
    </item>
    <item>
      <title>An Ising Machine Formulation for Design Updates in Topology Optimization of Flow Channels</title>
      <link>https://arxiv.org/abs/2411.08405</link>
      <description>arXiv:2411.08405v1 Announce Type: new 
Abstract: Topology optimization is an essential tool in computational engineering, for example, to improve the design and efficiency of flow channels. At the same time, Ising machines, including digital or quantum annealers, have been used as efficient solvers for combinatorial optimization problems. Beyond combinatorial optimization, recent works have demonstrated applicability to other engineering tasks by tailoring corresponding problem formulations. In this study, we present a novel Ising machine formulation for computing design updates during topology optimization with the goal of minimizing dissipation energy in flow channels. We explore the potential of this approach to improve the efficiency and performance of the optimization process. To this end, we conduct experiments to study the impact of various factors within the novel formulation. Additionally, we compare it to a classical method using the number of optimization steps and the final values of the objective function as indicators of the time intensity of the optimization and the performance of the resulting designs, respectively. Our findings show that the proposed update strategy can accelerate the topology optimization process while producing comparable designs. However, it tends to be less exploratory, which may lead to lower performance of the designs. These results highlight the potential of incorporating Ising formulations for optimization tasks but also show their limitations when used to compute design updates in an iterative optimization process. In conclusion, this work provides an efficient alternative for design updates in topology optimization and enhances the understanding of integrating Ising machine formulations in engineering optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08405v1</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yudai Suzuki, Shiori Aoki, Fabian Key, Katsuhiro Endo, Yoshiki Matsuda, Shu Tanaka, Marek Behr, Mayu Muramatsu</dc:creator>
    </item>
    <item>
      <title>A probabilistic reduced-order modeling framework for patient-specific cardio-mechanical analysis</title>
      <link>https://arxiv.org/abs/2411.08822</link>
      <description>arXiv:2411.08822v1 Announce Type: new 
Abstract: Cardio-mechanical models can be used to support clinical decision-making. Unfortunately, the substantial computational effort involved in many cardiac models hinders their application in the clinic, despite the fact that they may provide valuable information. In this work, we present a probabilistic reduced-order modeling (ROM) framework to dramatically reduce the computational effort of such models while providing a credibility interval. In the online stage, a fast-to-evaluate generalized one-fiber model is considered. This generalized one-fiber model incorporates correction factors to emulate patient-specific attributes, such as local geometry variations. In the offline stage, Bayesian inference is used to calibrate these correction factors on training data generated using a full-order isogeometric cardiac model (FOM). A Gaussian process is used in the online stage to predict the correction factors for geometries that are not in the training data. The proposed framework is demonstrated using two examples. The first example considers idealized left-ventricle geometries, for which the behavior of the ROM framework can be studied in detail. In the second example, the ROM framework is applied to scan-based geometries, based on which the application of the ROM framework in the clinical setting is discussed. The results for the two examples convey that the ROM framework can provide accurate online predictions, provided that adequate FOM training data is available. The uncertainty bands provided by the ROM framework give insight into the trustworthiness of its results. Large uncertainty bands can be considered as an indicator for the further population of the training data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08822v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robin Willems, Peter F\"orster, Sebastian Sch\"ops, Olaf van der Sluis, Clemens V. Verhoosel</dc:creator>
    </item>
    <item>
      <title>GPTree: Towards Explainable Decision-Making via LLM-powered Decision Trees</title>
      <link>https://arxiv.org/abs/2411.08257</link>
      <description>arXiv:2411.08257v1 Announce Type: cross 
Abstract: Traditional decision tree algorithms are explainable but struggle with non-linear, high-dimensional data, limiting its applicability in complex decision-making. Neural networks excel at capturing complex patterns but sacrifice explainability in the process. In this work, we present GPTree, a novel framework combining explainability of decision trees with the advanced reasoning capabilities of LLMs. GPTree eliminates the need for feature engineering and prompt chaining, requiring only a task-specific prompt and leveraging a tree-based structure to dynamically split samples. We also introduce an expert-in-the-loop feedback mechanism to further enhance performance by enabling human intervention to refine and rebuild decision paths, emphasizing the harmony between human expertise and machine intelligence. Our decision tree achieved a 7.8% precision rate for identifying "unicorn" startups at the inception stage of a startup, surpassing gpt-4o with few-shot learning as well as the best human decision-makers (3.1% to 5.6%).</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08257v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sichao Xiong, Yigit Ihlamur, Fuat Alican, Aaron Ontoyin Yin</dc:creator>
    </item>
    <item>
      <title>Graph Neural Networks in Supply Chain Analytics and Optimization: Concepts, Perspectives, Dataset and Benchmarks</title>
      <link>https://arxiv.org/abs/2411.08550</link>
      <description>arXiv:2411.08550v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have recently gained traction in transportation, bioinformatics, language and image processing, but research on their application to supply chain management remains limited. Supply chains are inherently graph-like, making them ideal for GNN methodologies, which can optimize and solve complex problems. The barriers include a lack of proper conceptual foundations, familiarity with graph applications in SCM, and real-world benchmark datasets for GNN-based supply chain research. To address this, we discuss and connect supply chains with graph structures for effective GNN application, providing detailed formulations, examples, mathematical definitions, and task guidelines. Additionally, we present a multi-perspective real-world benchmark dataset from a leading FMCG company in Bangladesh, focusing on supply chain planning. We discuss various supply chain tasks using GNNs and benchmark several state-of-the-art models on homogeneous and heterogeneous graphs across six supply chain analytics tasks. Our analysis shows that GNN-based models consistently outperform statistical Machine Learning and other Deep Learning models by around 10-30% in regression, 10-30% in classification and detection tasks, and 15-40% in anomaly detection tasks on designated metrics. With this work, we lay the groundwork for solving supply chain problems using GNNs, supported by conceptual discussions, methodological insights, and a comprehensive dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08550v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>stat.ML</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Azmine Toushik Wasi, MD Shafikul Islam, Adipto Raihan Akib, Mahathir Mohammad Bappy</dc:creator>
    </item>
    <item>
      <title>The Galactica database: an open, generic and versatile tool for the dissemination of simulation data in astrophysics</title>
      <link>https://arxiv.org/abs/2411.08647</link>
      <description>arXiv:2411.08647v1 Announce Type: cross 
Abstract: The Galactica simulation database is a platform designed to assist computational astrophysicists with their open science approach based on FAIR (Findable, Accessible, Interoperable, Reusable) principles. It offers the means to publish their numerical simulation projects, whatever their field of application or research theme and provides access to reduced datasets and object catalogs online. The application implements the Simulation Datamodel IVOA standard. To provide the scientific community indirect access to raw simulation data, Galactica can generate, on an "on-demand" basis, custom high-level data products to meet specific user requirements. These data products, accessible through online WebServices, are produced remotely from the raw simulation datasets. To that end, the Galactica central web application communicates with a high-scalability ecosystem of data-processing servers called Terminus by means of an industry-proven asynchronous task management system. Each Terminus node, hosted in a research institute, a regional or national supercomputing facility, contributes to the ecosystem by providing both the storage and the computational resources required to store the massive simulation datasets and post-process them to create the data products requested on Galactica, hence guaranteeing fine-grained sovereignty over data and resources. This distributed architecture is very versatile, it can be interfaced with any kind of data-processing software, written in any language, handling raw data produced by every type of simulation code used in the field of computational astrophysics. Its generality and versatility, together with its excellent scalability makes it a powerful tool for the scientific community to disseminate numerical models in astrophysics in the exascale era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08647v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.CE</category>
      <category>cs.DB</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Damien Chapon, Patrick Hennebelle</dc:creator>
    </item>
    <item>
      <title>Analytical assessment of workers' safety concerning direct and indirect ways of getting infected by dangerous pathogen</title>
      <link>https://arxiv.org/abs/2409.16809</link>
      <description>arXiv:2409.16809v2 Announce Type: replace 
Abstract: Developing safety policies to protect large groups of individuals working in indoor environments from disease spread is an important and challenging task. To address this issue, we investigate the scenario of workers becoming infected by a dangerous airborne pathogen in a near-real-life industrial environment. We present a simple analytical model based on observations made during the recent COVID-19 pandemic and business expectations concerning worker protection. The model can be adapted to address other epidemic or non-epidemic threats, including hazardous vapors from industrial processes. In the presented model, we consider both direct and indirect modes of infection. Direct infection occurs through direct contact with an infected individual, while indirect infection results from contact with a contaminated environment, including airborne pathogens in enclosed spaces or contaminated surfaces. Our analysis utilizes a simplified droplet/aerosol diffusion model, validated by droplet spread simulations. This model can be easily applied to new scenarios and has modest computational requirements compared to full simulations. Thus, it can be implemented within an automated protection ecosystem in an industrial setting, where rapid assessment of potential danger is required, and calculations must be performed almost in real-time. We validate general research findings on disease spread using a simple agent-based model. Based on our results, we outline a set of countermeasures for infection prevention, which could serve as the foundation for a prevention policy suited to industrial scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16809v2</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Krzysztof Domino, Arkadiusz Sochan, Jaros{\l}aw Adam Miszczak</dc:creator>
    </item>
    <item>
      <title>Private Order Flows and Builder Bidding Dynamics: The Road to Monopoly in Ethereum's Block Building Market</title>
      <link>https://arxiv.org/abs/2410.12352</link>
      <description>arXiv:2410.12352v3 Announce Type: replace 
Abstract: Ethereum, as a representative of Web3, adopts a novel framework called Proposer Builder Separation (PBS) to prevent the centralization of block profits in the hands of institutional Ethereum stakers. Introducing builders to generate blocks based on public transactions, PBS aims to ensure that block profits are distributed among all stakers. Through the auction among builders, only one will win the block in each slot. Ideally, the equilibrium strategy of builders under public information would lead them to bid all block profits. However, builders are now capable of extracting profits from private order flows. In this paper, we explore the effect of PBS with private order flows. Specifically, we propose the asymmetry auction model of MEV-Boost auction. Moreover, we conduct empirical study on Ethereum blocks from January 2023 to May 2024. Our analysis indicates that private order flows contribute to 54.59% of the block value, indicating that different builders will build blocks with different valuations. Interestingly, we find that builders with more private order flows (i.e., higher block valuations) are more likely to win the block, while retain larger proportion of profits. In return, such builders will further attract more private order flows, resulting in a monopolistic market gradually. Our findings reveal that PBS in current stage is unable to balance the profit distribution, which just transits the centralization of block profits from institutional stakers to the monopolistic builder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12352v3</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuzheng Wang, Yue Huang, Wenqin Zhang, Yuming Huang, Xuechao Wang, Jing Tang</dc:creator>
    </item>
    <item>
      <title>Application of an ANN and LSTM-based Ensemble Model for Stock Market Prediction</title>
      <link>https://arxiv.org/abs/2410.20253</link>
      <description>arXiv:2410.20253v3 Announce Type: replace 
Abstract: Stock trading has always been a key economic indicator in modern society and a primary source of profit for financial giants such as investment banks, quantitative trading firms, and hedge funds. Discovering the underlying patterns within the seemingly volatile yet intrinsically structured economic activities has become a central focus of research for many companies. Our study leverages widely-used modern financial forecasting algorithms, including LSTM, ANN, CNN, and BiLSTM. We begin by comparing the predictive performance of these well-known algorithms on our stock market data, utilizing metrics such as R2, MAE, MSE, RMSE for detailed evaluation. Based on the performance of these models, we then aim to combine their strengths while mitigating their weaknesses, striving to construct a powerful hybrid model that overcomes the performance limitations of individual models.Through rigorous experimentation and exploration, we ultimately developed an LSTM+ANN model that breaks through prior performance bottlenecks, achieving promising and exciting results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20253v3</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fang Liu, Shaobo Guo, Qianwen Xing, Xinye Sha, Ying Chen, Yuhui Jin, Qi Zheng, Chang Yu</dc:creator>
    </item>
    <item>
      <title>Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming</title>
      <link>https://arxiv.org/abs/2402.13224</link>
      <description>arXiv:2402.13224v4 Announce Type: replace-cross 
Abstract: This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach demonstrates robustness against early disconnections by considering a wider range of uncertainty scenarios for optimization. The algorithm prioritizing user satisfaction over electricity cost achieves a 20% and 36% improvement in two user satisfaction metrics compared to an industry-standard baseline. Additionally, the algorithm striking the best balance between cost and user satisfaction exhibits a mere 3% relative cost increase compared to the theoretically optimal baseline - for which the nonanticipativity constraint is relaxed - while attaining 94% and 84% of the user satisfaction performance in the two used satisfaction metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13224v4</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alban Puech, Tristan Rigaut, William Templier, Maud Tournoud</dc:creator>
    </item>
    <item>
      <title>Neural Persistence Dynamics</title>
      <link>https://arxiv.org/abs/2405.15732</link>
      <description>arXiv:2405.15732v2 Announce Type: replace-cross 
Abstract: We consider the problem of learning the dynamics in the topology of time-evolving point clouds, the prevalent spatiotemporal model for systems exhibiting collective behavior, such as swarms of insects and birds or particles in physics. In such systems, patterns emerge from (local) interactions among self-propelled entities. While several well-understood governing equations for motion and interaction exist, they are notoriously difficult to fit to data, as most prior work requires knowledge about individual motion trajectories, i.e., a requirement that is challenging to satisfy with an increasing number of entities. To evade such confounding factors, we investigate collective behavior from a $\textit{topological perspective}$, but instead of summarizing entire observation sequences (as done previously), we propose learning a latent dynamical model from topological features $\textit{per time point}$. The latter is then used to formulate a downstream regression task to predict the parametrization of some a priori specified governing equation. We implement this idea based on a latent ODE learned from vectorized (static) persistence diagrams and show that a combination of recent stability results for persistent homology justifies this modeling choice. Various (ablation) experiments not only demonstrate the relevance of each model component but provide compelling empirical evidence that our proposed model - $\textit{Neural Persistence Dynamics}$ - substantially outperforms the state-of-the-art across a diverse set of parameter regression tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15732v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Zeng, Florian Graf, Martin Uray, Stefan Huber, Roland Kwitt</dc:creator>
    </item>
    <item>
      <title>On Long-Term Species Coexistence in Five-Species Evolutionary Spatial Cyclic Games with Ablated and Non-Ablated Dominance Networks</title>
      <link>https://arxiv.org/abs/2410.03807</link>
      <description>arXiv:2410.03807v2 Announce Type: replace-cross 
Abstract: I present a replication and, to some extent, a refutation of key results published by Zhong, Zhang, Li, Dai, &amp; Yang in their 2022 paper "Species coexistence in spatial cyclic game of five species" (Chaos, Solitons and Fractals, 156: 111806), where ecosystem species coexistence was explored via simulation studies of the evolutionary spatial cyclic game (ESCG) Rock-Paper-Scissors-Lizard-Spock (RPSLS) with certain predator-prey relationships removed from the game's "interaction structure", i.e. with specific arcs ablated in the ESCG's dominance network, and with the ESCG run for 100,000 Monte Carlo Steps (MCS) to identify its asymptotic behaviors. I replicate the results presented by Zhong et al. for interaction structures with one, two, three, and four arcs ablated from the dominance network. I then empirically demonstrate that the dynamics of the RPSLS ESCG have sufficiently long time constants that the true asymptotic outcomes can often only be identified after running the ablated ESCG for 10,000,000MCS or longer, and that the true long-term outcomes can be markedly less diverse than those reported by Zhong et al. as asymptotic. Finally I demonstrate that, when run for sufficiently many MCS, the original unablated RPSLS system exhibits essentially the same asymptotic outcomes as the ablated RPSLS systems, and in this sense the only causal effect of the ablations is to alter the time required for the system to converge to the long-term asymptotic states that the unablated system eventually settles to anyhow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03807v2</guid>
      <category>q-bio.PE</category>
      <category>cs.CE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dave Cliff</dc:creator>
    </item>
  </channel>
</rss>
