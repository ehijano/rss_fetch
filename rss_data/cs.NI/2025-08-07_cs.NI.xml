<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Aug 2025 01:29:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>CASH: Context-Aware Smart Handover for Reliable UAV Connectivity on Aerial Corridors</title>
      <link>https://arxiv.org/abs/2508.03862</link>
      <description>arXiv:2508.03862v1 Announce Type: new 
Abstract: Urban Air Mobility (UAM) envisions aerial corridors for Unmanned Aerial Vehicles (UAVs) to reduce ground traffic congestion by supporting 3D mobility, such as air taxis. A key challenge in these high-mobility aerial corridors is ensuring reliable connectivity, where frequent handovers can degrade network performance. To resolve this, we present a Context-Aware Smart Handover (CASH) protocol that uses a forward-looking scoring mechanism based on UAV trajectory to make proactive handover decisions. We evaluate the performance of the proposed CASH against existing handover protocols in a custom-built simulator. Results show that CASH reduces handover frequency by up to 78% while maintaining low outage probability. We then investigate the impact of base station density and safety margin on handover performance, where their optimal setups are empirically obtained to ensure reliable UAM communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03862v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul Saboor, Zhuangzhuang Cui, Achiel Colpaert, Evgenii Vinogradov, Sofie Pollin</dc:creator>
    </item>
    <item>
      <title>Confidence Driven Classification of Application Types in the Presence of Background Network</title>
      <link>https://arxiv.org/abs/2508.03891</link>
      <description>arXiv:2508.03891v1 Announce Type: new 
Abstract: Accurately classifying the application types of network traffic using deep learning models has recently gained popularity. However, we find that these classifiers do not perform well on real-world traffic data due to the presence of non-application-specific generic background traffic originating from advertisements, analytics, shared APIs, and trackers. Unfortunately, state-of-the-art application classifiers overlook such traffic in curated datasets and only classify relevant application traffic. To address this issue, when we label and train using an additional class for background traffic, it leads to additional confusion between application and background traffic, as the latter is heterogeneous and encompasses all traffic that is not relevant to the application sessions. To avoid falsely classifying background traffic as one of the relevant application types, a reliable confidence measure is warranted, such that we can refrain from classifying uncertain samples. Therefore, we design a Gaussian Mixture Model-based classification framework that improves the indication of the deep learning classifier's confidence to allow more reliable classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03891v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eun Hun Choi, Jasleen Kaur, Vladas Pipiras, Nelson Gomes Rodrigues Antunes, Brendan Massey</dc:creator>
    </item>
    <item>
      <title>Enabling Site-Specific Cellular Network Simulation Through Ray-Tracing-Driven ns-3</title>
      <link>https://arxiv.org/abs/2508.04004</link>
      <description>arXiv:2508.04004v1 Announce Type: new 
Abstract: Evaluating cellular systems, from 5G New Radio (NR) and 5G-Advanced to 6G, is challenging because the performance emerges from the tight coupling of propagation, beam management, scheduling, and higher-layer interactions. System-level simulation is therefore indispensable, yet the vast majority of studies rely on the statistical 3GPP channel models. These are well suited to capture average behavior across many statistical realizations, but cannot reproduce site-specific phenomena such as corner diffraction, street-canyon blockage, or deterministic line-of-sight conditions and angle-of-departure/arrival relationships that drive directional links. This paper extends 5G-LENA, an NR module for the system-level Network Simulator 3 (ns-3), with a trace-based channel model that processes the Multipath Components (MPCs) obtained from external ray-tracers (e.g., Sionna Ray Tracer (RT)) or measurement campaigns. Our module constructs frequency-domain channel matrices and feeds them to the existing Physical (PHY)/Medium Access Control (MAC) stack without any further modifications. The result is a geometry-based channel model that remains fully compatible with the standard 3GPP implementation in 5G-LENA, while delivering site-specific geometric fidelity. This new module provides a key building block toward Digital Twin (DT) capabilities by offering realistic site-specific channel modeling, unlocking studies that require site awareness, including beam management, blockage mitigation, and environment-aware sensing. We demonstrate its capabilities for precise beam-steering validation and end-to-end metric analysis. In both cases, the trace-driven engine exposes performance inflections that the statistical model does not exhibit, confirming its value for high-fidelity system-level cellular networks research and as a step toward DT applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04004v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanguy Ropitault, Matteo Bordin, Paolo Testolina, Michele Polese, Pedram Johari, Nada Golmie, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>A Novel Hierarchical Co-Optimization Framework for Coordinated Task Scheduling and Power Dispatch in Computing Power Networks</title>
      <link>https://arxiv.org/abs/2508.04015</link>
      <description>arXiv:2508.04015v1 Announce Type: new 
Abstract: The proliferation of large-scale artificial intelligence and data-intensive applications has spurred the development of Computing Power Networks (CPNs), which promise to deliver ubiquitous and on-demand computational resources. However, the immense energy consumption of these networks poses a significant sustainability challenge. Simultaneously, power grids are grappling with the instability introduced by the high penetration of intermittent renewable energy sources (RES). This paper addresses these dual challenges through a novel Two-Stage Co-Optimization (TSCO) framework that synergistically manages power system dispatch and CPN task scheduling to achieve low-carbon operations. The framework decomposes the complex, large-scale problem into a day-ahead stochastic unit commitment (SUC) stage and a real-time operational stage. The former is solved using Benders decomposition for computational tractability, while in the latter, economic dispatch of generation assets is coupled with an adaptive CPN task scheduling managed by a Deep Reinforcement Learning (DRL) agent. This agent makes intelligent, carbon-aware decisions by responding to dynamic grid conditions, including real-time electricity prices and marginal carbon intensity. Through extensive simulations on an IEEE 30-bus system integrated with a CPN, the TSCO framework is shown to significantly outperform baseline approaches. Results demonstrate that the proposed framework reduces total carbon emissions and operational costs, while simultaneously decreasing RES curtailment by more than 60% and maintaining stringent Quality of Service (QoS) for computational tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04015v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Haoxiang Luo, Kun Yang, Qi Huang, Schahram Dustdar</dc:creator>
    </item>
    <item>
      <title>Metaverse Framework for Wireless Systems Management</title>
      <link>https://arxiv.org/abs/2508.04150</link>
      <description>arXiv:2508.04150v1 Announce Type: new 
Abstract: This article introduces a comprehensive metaverse framework, which is designed for the simulation, emulation, and interaction with wireless systems. The proposed framework integrates core metaverse technologies such as extended reality (XR), digital twins (DTs), artificial intelligence (AI), internet of things (IoT), blockchain, and advanced 6G networking solutions to create a dynamic, immersive platform for both system development and management. By leveraging XR, users can visualize and engage with complex systems, while DTs enable real-time monitoring and optimization. AI generates the three-dimensional (3D) content, enhances decision-making and system performance, whereas IoT devices provide real-time sensor data for boosting the simulation accuracy. Additionally, blockchain ensures secure, decentralized interactions, and 5G/6G networks offer the necessary infrastructure for seamless, low-latency communication. This framework serves as a robust tool for exploring, developing, and optimizing wireless systems, aiming to provide valuable insights into the future of networked environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04150v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Chrysovergis, Alexandros-Apostolos A. Boulogeorgos, Theodoros A. Tsiftsis, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>DSNS: The Deep Space Network Simulator</title>
      <link>https://arxiv.org/abs/2508.04317</link>
      <description>arXiv:2508.04317v1 Announce Type: new 
Abstract: Simulation tools are commonly used in the development and testing of new protocols or new networks. However, as satellite networks start to grow to encompass thousands of nodes, and as companies and space agencies begin to realize the interplanetary internet, existing satellite and network simulation tools have become impractical for use in this context.
  We therefore present the Deep Space Network Simulator (DSNS): a new network simulator with a focus on large-scale satellite networks. We demonstrate its improved capabilities compared to existing offerings, showcase its flexibility and extensibility through an implementation of existing protocols and the DTN simulation reference scenarios recommended by CCSDS, and evaluate its scalability, showing that it exceeds existing tools while providing better fidelity.
  DSNS provides concrete usefulness to both standards bodies and satellite operators, enabling fast iteration on protocol development and testing of parameters under highly realistic conditions. By removing roadblocks to research and innovation, we can accelerate the development of upcoming satellite networks and ensure that their communication is both fast and secure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04317v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Smailes, Filip Futera, Sebastian K\"ohler, Simon Birnbach, Martin Strohmeier, Ivan Martinovic</dc:creator>
    </item>
    <item>
      <title>Empowering Nanoscale Connectivity through Molecular Communication: A Case Study of Virus Infection</title>
      <link>https://arxiv.org/abs/2508.04415</link>
      <description>arXiv:2508.04415v1 Announce Type: new 
Abstract: The Internet of Bio-Nano Things (IoBNT), envisioned as a revolutionary healthcare paradigm, shows promise for epidemic control. This paper explores the potential of using molecular communication (MC) to address the challenges in constructing IoBNT for epidemic prevention, specifically focusing on modeling viral transmission, detecting the virus/infected individuals, and identifying virus mutations. First, the MC channels in macroscale and microscale scenarios are discussed to match viral transmission in both scales separately. Besides, the detection methods for these two scales are also studied, along with the localization mechanism designed for the virus/infected individuals. Moreover, an identification strategy is proposed to determine potential virus mutations, which is validated through simulation using the ORF3a protein as a benchmark. Finally, open research issues are discussed. In summary, this paper aims to analyze viral transmission through MC and combat viral spread using signal processing techniques within MC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04415v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan Chen, Yu Huang, Miaowen Wen, Shahid Mumtaz, Fatih Gulec, Anwer Al-Dulaimi, Andrew W. Eckford</dc:creator>
    </item>
    <item>
      <title>Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions</title>
      <link>https://arxiv.org/abs/2508.04526</link>
      <description>arXiv:2508.04526v1 Announce Type: new 
Abstract: Traditional security architectures are becoming more vulnerable to distributed attacks due to significant dependence on trust. This will further escalate when implementing agentic AI within the systems, as more components must be secured over a similar distributed space. These scenarios can be observed in consumer technologies, such as the dense Internet of things (IoT). Here, zero-trust architecture (ZTA) can be seen as a potential solution, which relies on a key principle of not giving users explicit trust, instead always verifying their privileges whenever a request is made. However, the overall security in ZTA is managed through its policies, and unverified policies can lead to unauthorized access. Thus, this paper explores challenges and solutions for ZTA policy design in the context of distributed networks, which is referred to as zero-trust distributed networks (ZTDN). This is followed by a case-study on formal verification of policies using UPPAAL. Subsequently, the importance of accountability and responsibility in the system's security is discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04526v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fannya R. Sandjaja, Ayesha A. Majeed, Abdullah Abdullah, Gyan Wickremasinghe, Karen Rafferty, Vishal Sharma</dc:creator>
    </item>
    <item>
      <title>CONVERGE: A Multi-Agent Vision-Radio Architecture for xApps</title>
      <link>https://arxiv.org/abs/2508.04556</link>
      <description>arXiv:2508.04556v1 Announce Type: new 
Abstract: Telecommunications and computer vision have evolved independently. With the emergence of high-frequency wireless links operating mostly in line-of-sight, visual data can help predict the channel dynamics by detecting obstacles and help overcoming them through beamforming or handover techniques.
  This paper proposes a novel architecture for delivering real-time radio and video sensing information to O-RAN xApps through a multi-agent approach, and introduces a new video function capable of generating blockage information for xApps, enabling Integrated Sensing and Communications. Experimental results show that the delay of sensing information remains under 1\,ms and that an xApp can successfully use radio and video sensing information to control the 5G/6G RAN in real-time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04556v1</guid>
      <category>cs.NI</category>
      <category>cs.CV</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Filipe B. Teixeira, Carolina Sim\~oes, Paulo Fidalgo, Wagner Pedrosa, Andr\'e Coelho, Manuel Ricardo, Luis M. Pessoa</dc:creator>
    </item>
    <item>
      <title>Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning</title>
      <link>https://arxiv.org/abs/2508.03863</link>
      <description>arXiv:2508.03863v1 Announce Type: cross 
Abstract: Accurate spectrum demand prediction is crucial for informed spectrum allocation, effective regulatory planning, and fostering sustainable growth in modern wireless communication networks. It supports governmental efforts, particularly those led by the international telecommunication union (ITU), to establish fair spectrum allocation policies, improve auction mechanisms, and meet the requirements of emerging technologies such as advanced 5G, forthcoming 6G, and the internet of things (IoT). This paper presents an effective spatio-temporal prediction framework that leverages crowdsourced user-side key performance indicators (KPIs) and regulatory datasets to model and forecast spectrum demand. The proposed methodology achieves superior prediction accuracy and cross-regional generalizability by incorporating advanced feature engineering, comprehensive correlation analysis, and transfer learning techniques. Unlike traditional ITU models, which are often constrained by arbitrary inputs and unrealistic assumptions, this approach exploits granular, data-driven insights to account for spatial and temporal variations in spectrum utilization. Comparative evaluations against ITU estimates, as the benchmark, underscore our framework's capability to deliver more realistic and actionable predictions. Experimental results validate the efficacy of our methodology, highlighting its potential as a robust approach for policymakers and regulatory bodies to enhance spectrum management and planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03863v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amin Farajzadeh, Hongzhao Zheng, Sarah Dumoulin, Trevor Ha, Halim Yanikomeroglu, Amir Ghasemi</dc:creator>
    </item>
    <item>
      <title>Entanglement distribution in quantum networks via swapping of partially entangled states</title>
      <link>https://arxiv.org/abs/2508.04536</link>
      <description>arXiv:2508.04536v1 Announce Type: cross 
Abstract: The entanglement swapping protocol (ESP) is a fundamental primitive for distributing quantum correlations across distant nodes in a quantum network. Recent studies have demonstrated that even when the involved qubit pairs are only partially entangled, it is still possible to concentrate and transmit entanglement via Bell-basis measurements. In this work, we extend these ideas to quantum networks with various topologies - including linear, star, and hybrid configurations - by analyzing the application of the ESP to initially partially entangled states. We investigate how entanglement evolves under such protocols by examining the transformations of the initial states and evaluating the success probabilities for generating maximally entangled states at the output. Our results offer new insights into the dynamics of the entanglement distribution in quantum networks and provide practical guidelines for designing robust quantum communication strategies under realistic conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04536v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henrique Guerra, Tailan S. Sarubi, Rafael Chaves, Jonas Maziero</dc:creator>
    </item>
    <item>
      <title>Edge-assisted Parallel Uncertain Skyline Processing for Low-latency IoE Analysis</title>
      <link>https://arxiv.org/abs/2508.04596</link>
      <description>arXiv:2508.04596v1 Announce Type: cross 
Abstract: Due to the Internet of Everything (IoE), data generated in our life become larger. As a result, we need more effort to analyze the data and extract valuable information. In the cloud computing environment, all data analysis is done in the cloud, and the client only needs less computing power to handle some simple tasks. However, with the rapid increase in data volume, sending all data to the cloud via the Internet has become more expensive. The required cloud computing resources have also become larger. To solve this problem, edge computing is proposed. Edge is granted with more computation power to process data before sending it to the cloud. Therefore, the data transmitted over the Internet and the computing resources required by the cloud can be effectively reduced. In this work, we proposed an Edge-assisted Parallel Uncertain Skyline (EPUS) algorithm for emerging low-latency IoE analytic applications. We use the concept of skyline candidate set to prune data that are less likely to become the skyline data on the parallel edge computing nodes. With the candidate skyline set, each edge computing node only sends the information required to the server for updating the global skyline, which reduces the amount of data that transfer over the internet. According to the simulation results, the proposed method is better than two comparative methods, which reduces the latency of processing two-dimensional data by more than 50%. For high-dimensional data, the proposed EPUS method also outperforms the other existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04596v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuan-Chi Lai, Yan-Lin Chen, Bo-Xin Liu, Chuan-Ming Liu</dc:creator>
    </item>
    <item>
      <title>OpenOptics: An Open Research Framework for Optical Data Center Networks</title>
      <link>https://arxiv.org/abs/2411.18319</link>
      <description>arXiv:2411.18319v2 Announce Type: replace 
Abstract: Optical data center networks (DCNs) are emerging as a promising design for cloud infrastructure. However, existing optical DCN architectures operate as closed ecosystems, tying software solutions to specific optical hardware. We introduce OpenOptics, an open research framework that decouples software from hardware, allowing them to evolve independently. OpenOptics features: (1) a time-flow table abstraction as a common interface between optical hardware and software, (2) a unified workflow and user-friendly API for implementing various optical DCNs with simple Python scripts, and (3) a backend system that re-architects queue management to support the time-flow tables and provides rich infrastructure services for diverse applications. Built on programmable switches, OpenOptics achieves a record-breaking minimum optical circuit duration of 2 $\mu$s using commodity devices. We validate OpenOptics' generality by implementing six optical architectures and seven routing schemes on an optical testbed and conducting benchmarks on a 108-ToR setup, showcasing its efficiency. Additionally, case studies highlight novel research opportunities enabled by OpenOptics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18319v2</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Lei, Federico De Marchi, Jialong Li, Raj Joshi, Balakrishnan Chandrasekaran, Yiting Xia</dc:creator>
    </item>
    <item>
      <title>A Study on 5G Network Slice Isolation Based on Native Cloud and Edge Computing Tools</title>
      <link>https://arxiv.org/abs/2502.02842</link>
      <description>arXiv:2502.02842v2 Announce Type: replace 
Abstract: 5G networks support various advanced applications through network slicing, network function virtualization (NFV), and edge computing, ensuring low latency and service isolation. However, private 5G networks relying on open-source tools still face challenges in maturity and integration with edge/cloud platforms, compromising proper slice isolation. This study investigates resource allocation mechanisms to address this issue, conducting experiments in a hospital scenario with medical video conferencing. The results show that CPU limitations improve the performance of prioritized slices, while memory restrictions have minimal impact. The generated data and scripts have been made publicly available for future research and machine learning applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02842v2</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10922-025-09958-5</arxiv:DOI>
      <arxiv:journal_reference>Journal of Network and Systems Management, volume 33, 90 (2025)</arxiv:journal_reference>
      <dc:creator>Maiko Andrade, Juliano Araujo Wickboldt</dc:creator>
    </item>
    <item>
      <title>Optimal Packetization Towards Low Latency in Random Access Networks (extended version)</title>
      <link>https://arxiv.org/abs/2507.23286</link>
      <description>arXiv:2507.23286v2 Announce Type: replace 
Abstract: As the demand for low-latency services grows, ensuring the delay performance of random access (RA) networks has become a priority. Existing studies on the queueing delay performance of the Aloha model universally treat packets as atomic transmission units, focusing primarily on delay measured in time slots. However, the impact of packetization on queueing delay has been consistently overlooked, particularly for the mean queueing delay measured in seconds, which serves as a more precise and practically relevant performance metric than its slot-based counterpart. Here, packetization refers to the process of determining the number of bits assembled into a packet. To optimize queueing delay from the perspective of packetization, this paper establishes the mathematical relationship between packetization and mean queueing delay in seconds for both connection-free and connection-based Aloha schemes, and explores the optimal packetization strategy to minimize this delay. We identify the optimal mean queueing delay and its corresponding packet size via numerical methods, and further analyze the influence of various network parameters. We further use simulations to investigate the similar impact of packetization on jitter of queueing delay. We then apply our analysis to re-evaluate the complex trade-off between the connection-free and connection-based schemes through the new perspective of packetization. Furthermore, recognizing that an analysis of the queueing delay performance for RA-SDT in NTN scenarios, especially from a packetization perspective, also remains an unexplored area, we apply the analysis to this scenario as a case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23286v2</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihong Li, Anshan Yuan, Xinghua Sun</dc:creator>
    </item>
    <item>
      <title>Non-Terrestrial Network Models Using Stochastic Geometry: Planar or Spherical?</title>
      <link>https://arxiv.org/abs/2508.00010</link>
      <description>arXiv:2508.00010v2 Announce Type: replace 
Abstract: With the explosive deployment of non-terrestrial networks (NTNs), the computational complexity of network performance analysis is rapidly escalating. As one of the most suitable mathematical tools for analyzing large-scale network topologies, stochastic geometry (SG) enables the representation of network performance metrics as functions of network parameters, thus offering low-complexity performance analysis solutions. However, choosing between planar and spherical models remains challenging. Planar models neglect Earth's curvature, causing deviations in high-altitude NTN analysis, yet are still often used for simplicity. This paper introduces relative error to quantify the gap between planar and spherical models, helping determine when planar modeling is sufficient. To calculate the relative error, we first propose a point process (PP) generation algorithm that simultaneously generates a pair of homogeneous and asymptotically similar planar and spherical PPs. We then introduce several typical similarity metrics, including topology-related and network-level metrics, and further develop a relative error estimation algorithm based on these metrics. In addition, we derive an analytical expression for the optimal planar altitude, which reduces computational complexity and provides theoretical support for planar approximation. Finally, numerical results investigate how deployment altitude and region affect NTN modeling, with case studies on HAP and LEO satellite constellations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00010v2</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ruibo Wang, Baha Eddine Youcef Belmekki, Howard H. Yang, Mohamed Slim Alouini</dc:creator>
    </item>
    <item>
      <title>Pull-Based Query Scheduling for Goal-Oriented Semantic Communication</title>
      <link>https://arxiv.org/abs/2503.06725</link>
      <description>arXiv:2503.06725v2 Announce Type: replace-cross 
Abstract: This paper addresses query scheduling for goal-oriented semantic communication in pull-based status update systems. We consider a system where multiple sensing agents (SAs) observe a source characterized by various attributes and provide updates to multiple actuation agents (AAs), which act upon the received information to fulfill their heterogeneous goals at the endpoint. A hub serves as an intermediary, querying the SAs for updates on observed attributes and maintaining a knowledge base, which is then broadcast to the AAs. The AAs leverage the knowledge to perform their actions effectively. To quantify the semantic value of updates, we introduce a grade of effectiveness (GoE) metric. Furthermore, we integrate cumulative perspective theory (CPT) into the long-term effectiveness analysis to account for risk awareness and loss aversion in the system. Leveraging this framework, we compute effect-aware scheduling policies aimed at maximizing the expected discounted sum of CPT-based total GoE provided by the transmitted updates while complying with a given query cost constraint. To achieve this, we propose a model-based solution based on dynamic programming and model-free solutions employing state-of-the-art deep reinforcement learning (DRL) algorithms. Our findings demonstrate that effect-aware scheduling significantly enhances the effectiveness of communicated updates compared to benchmark scheduling methods, particularly in settings with stringent cost constraints where optimal query scheduling is vital for system performance and overall effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06725v2</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pouya Agheli, Nikolaos Pappas, Marios Kountouris</dc:creator>
    </item>
    <item>
      <title>Heterogeneity-Oblivious Robust Federated Learning</title>
      <link>https://arxiv.org/abs/2508.03579</link>
      <description>arXiv:2508.03579v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) remains highly vulnerable to poisoning attacks, especially under real-world hyper-heterogeneity, where clients differ significantly in data distributions, communication capabilities, and model architectures. Such heterogeneity not only undermines the effectiveness of aggregation strategies but also makes attacks more difficult to detect. Furthermore, high-dimensional models expand the attack surface. To address these challenges, we propose Horus, a heterogeneity-oblivious robust FL framework centered on low-rank adaptations (LoRAs). Rather than aggregating full model parameters, Horus inserts LoRAs into empirically stable layers and aggregates only LoRAs to reduce the attack uncover a key empirical observation that the input projection (LoRA-A) is markedly more stable than the output projection (LoRA-B) under heterogeneity and poisoning. Leveraging this, we design a Heterogeneity-Oblivious Poisoning Score using the features from LoRA-A to filter poisoned clients. For the remaining benign clients, we propose projection-aware aggregation mechanism to preserve collaborative signals while suppressing drifts, which reweights client updates by consistency with the global directions. Extensive experiments across diverse datasets, model architectures, and attacks demonstrate that Horus consistently outperforms state-of-the-art baselines in both robustness and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03579v2</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weiyao Zhang, Jinyang Li, Qi Song, Miao Wang, Chungang Lin, Haitong Luo, Xuying Meng, Yujun Zhang</dc:creator>
    </item>
  </channel>
</rss>
