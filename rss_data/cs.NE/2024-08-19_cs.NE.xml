<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Aug 2024 02:32:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 19 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Hardware-Algorithm Re-engineering of Retinal Circuit for Intelligent Object Motion Segmentation</title>
      <link>https://arxiv.org/abs/2408.08320</link>
      <description>arXiv:2408.08320v1 Announce Type: new 
Abstract: Recent advances in retinal neuroscience have fueled various hardware and algorithmic efforts to develop retina-inspired solutions for computer vision tasks. In this work, we focus on a fundamental visual feature within the mammalian retina, Object Motion Sensitivity (OMS). Using DVS data from EV-IMO dataset, we analyze the performance of an algorithmic implementation of OMS circuitry for motion segmentation in presence of ego-motion. This holistic analysis considers the underlying constraints arising from the hardware circuit implementation. We present novel CMOS circuits that implement OMS functionality inside image sensors, while providing run-time re-configurability for key algorithmic parameters. In-sensor technologies for dynamical environment adaptation are crucial for ensuring high system performance. Finally, we verify the functionality and re-configurability of the proposed CMOS circuit designs through Cadence simulations in 180nm technology. In summary, the presented work lays foundation for hardware-algorithm re-engineering of known biological circuits to suit application needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08320v1</guid>
      <category>cs.NE</category>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Sinaga, Victoria Clerico, Md Abdullah-Al Kaiser, Shay Snyder, Arya Lohia, Gregory Schwartz, Maryam Parsa, Akhilesh Jaiswal</dc:creator>
    </item>
    <item>
      <title>Bee-yond the Plateau: Training QNNs with Swarm Algorithms</title>
      <link>https://arxiv.org/abs/2408.08836</link>
      <description>arXiv:2408.08836v1 Announce Type: cross 
Abstract: In the quest to harness the power of quantum computing, training quantum neural networks (QNNs) presents a formidable challenge. This study introduces an innovative approach, integrating the Bees Optimization Algorithm (BOA) to overcome one of the most significant hurdles -- barren plateaus. Our experiments across varying qubit counts and circuit depths demonstrate the BOA's superior performance compared to the Adam algorithm. Notably, BOA achieves faster convergence, higher accuracy, and greater computational efficiency. This study confirms BOA's potential in enhancing the applicability of QNNs in complex quantum computations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08836v1</guid>
      <category>quant-ph</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rub\'en Dar\'io Guerrero</dc:creator>
    </item>
    <item>
      <title>Deep Convolutional Autoencoder for Assessment of Anomalies in Multi-stream Sensor Data</title>
      <link>https://arxiv.org/abs/2202.07592</link>
      <description>arXiv:2202.07592v2 Announce Type: replace-cross 
Abstract: This work investigates a practical and novel method for automated unsupervised fault detection in vehicles using a fully convolutional autoencoder. The results demonstrate the algorithm we developed can detect anomalies which correspond to powertrain faults by learning patterns in the multivariate time-series data of hybrid-electric vehicle powertrain sensors. Data was collected by engineers at Ford Motor Company from numerous sensors over several drive cycle variations. This study provides evidence of the anomaly detecting capability of our trained autoencoder and investigates the suitability of our autoencoder relative to other unsupervised methods for automatic fault detection in this data set. Preliminary results of testing the autoencoder on the powertrain sensor data indicate the data reconstruction approach availed by the autoencoder is a robust technique for identifying the abnormal sequences in the multivariate series. These results support that irregularities in hybrid-electric vehicles' powertrains are conveyed via sensor signals in the embedded electronic communication system, and therefore can be identified mechanistically with a trained algorithm. Additional unsupervised methods are tested and show the autoencoder performs better at fault detection than outlier detectors and other novel deep learning techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.07592v2</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/SSCI51031.2022.10022233</arxiv:DOI>
      <arxiv:journal_reference>2022 IEEE Symposium Series on Computational Intelligence (SSCI), Singapore, Singapore, 2022</arxiv:journal_reference>
      <dc:creator>Anthony Geglio, Eisa Hedayati, Mark Tascillo, Dyche Anderson, Jonathan Barker, Timothy C. Havens</dc:creator>
    </item>
    <item>
      <title>Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent</title>
      <link>https://arxiv.org/abs/2311.01205</link>
      <description>arXiv:2311.01205v2 Announce Type: replace-cross 
Abstract: Prior attacks on graph neural networks have mostly focused on graph poisoning and evasion, neglecting the network's weights and biases. Traditional weight-based fault injection attacks, such as bit flip attacks used for convolutional neural networks, do not consider the unique properties of graph neural networks. We propose the Injectivity Bit Flip Attack, the first bit flip attack designed specifically for graph neural networks. Our attack targets the learnable neighborhood aggregation functions in quantized message passing neural networks, degrading their ability to distinguish graph structures and losing the expressivity of the Weisfeiler-Lehman test. Our findings suggest that exploiting mathematical properties specific to certain graph neural network architectures can significantly increase their vulnerability to bit flip attacks. Injectivity Bit Flip Attacks can degrade the maximal expressive Graph Isomorphism Networks trained on various graph property prediction datasets to random output by flipping only a small fraction of the network's bits, demonstrating its higher destructive power compared to a bit flip attack transferred from convolutional neural networks. Our attack is transparent and motivated by theoretical insights which are confirmed by extensive empirical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01205v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3637528.3671890</arxiv:DOI>
      <dc:creator>Lorenz Kummer, Samir Moustafa, Nils N. Kriege, Wilfried N. Gansterer</dc:creator>
    </item>
    <item>
      <title>Dataset-learning duality and emergent criticality</title>
      <link>https://arxiv.org/abs/2405.17391</link>
      <description>arXiv:2405.17391v2 Announce Type: replace-cross 
Abstract: In artificial neural networks, the activation dynamics of non-trainable variables is strongly coupled to the learning dynamics of trainable variables. During the activation pass, the boundary neurons (e.g., input neurons) are mapped to the bulk neurons (e.g., hidden neurons), and during the learning pass, both bulk and boundary neurons are mapped to changes in trainable variables (e.g., weights and biases). For example, in feed-forward neural networks, forward propagation is the activation pass and backward propagation is the learning pass. We show that a composition of the two maps establishes a duality map between a subspace of non-trainable boundary variables (e.g., dataset) and a tangent subspace of trainable variables (i.e., learning). In general, the dataset-learning duality is a complex non-linear map between high-dimensional spaces, but in a learning equilibrium, the problem can be linearized and reduced to many weakly coupled one-dimensional problems. We use the duality to study the emergence of criticality, or the power-law distributions of fluctuations of the trainable variables. In particular, we show that criticality can emerge in the learning system even from the dataset in a non-critical state, and that the power-law distribution can be modified by changing either the activation function or the loss function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17391v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ekaterina Kukleva, Vitaly Vanchurin</dc:creator>
    </item>
    <item>
      <title>Activations Through Extensions: A Framework To Boost Performance Of Neural Networks</title>
      <link>https://arxiv.org/abs/2408.03599</link>
      <description>arXiv:2408.03599v2 Announce Type: replace-cross 
Abstract: Activation functions are non-linearities in neural networks that allow them to learn complex mapping between inputs and outputs. Typical choices for activation functions are ReLU, Tanh, Sigmoid etc., where the choice generally depends on the application domain. In this work, we propose a framework/strategy that unifies several works on activation functions and theoretically explains the performance benefits of these works. We also propose novel techniques that originate from the framework and allow us to obtain ``extensions'' (i.e. special generalizations of a given neural network) of neural networks through operations on activation functions. We theoretically and empirically show that ``extensions'' of neural networks have performance benefits compared to vanilla neural networks with insignificant space and time complexity costs on standard test functions. We also show the benefits of neural network ``extensions'' in the time-series domain on real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03599v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>cs.NE</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chandramouli Kamanchi, Sumanta Mukherjee, Kameshwaran Sampath, Pankaj Dayama, Arindam Jati, Vijay Ekambaram, Dzung Phan</dc:creator>
    </item>
  </channel>
</rss>
