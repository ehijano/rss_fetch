<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Oct 2024 04:01:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Hazard and Beyond: Exploring Five Distributional Representations of Accelerometry Data for Disability Discrimination in Multiple Sclerosis</title>
      <link>https://arxiv.org/abs/2410.20620</link>
      <description>arXiv:2410.20620v1 Announce Type: new 
Abstract: Research on modeling the distributional aspects in sensor-based digital health (sDHT) data has grown significantly in recent years. Most existing approaches focus on using individual-specific density or quantile functions. However, there has been limited exploration to assess the practical utility of alternative distributional representations in clinical contexts collecting sDHT data. This study is motivated by accelerometry data collected on 246 individuals with multiple sclerosis (MS) representing a wide range of disability (Expanded Disability Status Scale, EDSS: 0-7). We consider five different individual-level distributional representations of minute-level activity counts: density, survival, hazard, quantile, and total time on test functions. For each of the five distributional representations, scalar-on-function regression fits linear discriminators for binary and continuously measured MS disability, and cross-validated discriminatory performance of these linear discriminators is compared across. The results show that individual-level hazard functions provide the highest discriminatory accuracy, more than double the accuracy compared to density functions. Individual-level quantile functions provided the second-highest discriminatory accuracy. These findings highlight the importance of focusing on distributional representations that capture the tail behavior of distributions when analyzing digital health data, especially in clinical contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20620v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratim Guha Niyogi, Muraleetharan Sanjayan, Kathryn C. Fitzgerald, Ellen M. Mowry, Vadim Zipunnikov</dc:creator>
    </item>
    <item>
      <title>Analysis of Diurnal Air Temperature Trends and Pattern Similarities in Highland and Lowland Stations of Italy and UK</title>
      <link>https://arxiv.org/abs/2410.20726</link>
      <description>arXiv:2410.20726v1 Announce Type: new 
Abstract: In this paper, an analysis of hourly air temperatures in four groups of 32 stations of the UK highland (five stations), UK lowland (four stations), Italian highland (eleven stations), and Italian lowland (twelve stations) at various altitudes was conducted over the period from 2002 to 2021. The study aimed to examine the trends of each hour of the day in that period, over different averaging time windows (-10 day, -30 day, and -60 day). The trends were computed using the Mann-Kendall trend test and Sen's slope estimator. The similarity of trends within and across the groups of stations was assessed using the hierarchical clustering with dynamic time warping technique. An additional analysis was conducted to show the correlation of trends among the group of stations using the correlation distance matrix. Hierarchical clustering and distance correlation analysis show trend similarities and correlations, also indicating dissimilarities among different groups. Using 30 day averages, significant warming trends in specific months at the Italian stations are evident, especially in February, July, August, and December. The UK highland stations did not show statistically significant trends, but clear pattern similarities were found within the groups, especially in certain months. The ultimate goal of this paper is to provide insights into temperature dynamics and climate change characteristics on regional and diurnal scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20726v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/joc.8643</arxiv:DOI>
      <arxiv:journal_reference>International journal of Climatology 2024</arxiv:journal_reference>
      <dc:creator>Chalachew Muluken Liyew. Rosa Meo, Stefano Ferraris, Elvira Di Nardo</dc:creator>
    </item>
    <item>
      <title>Spatial Risk Patterns-ANOVA: Multivariate Analysis Of Suicide-Related Emergency Calls</title>
      <link>https://arxiv.org/abs/2410.21227</link>
      <description>arXiv:2410.21227v1 Announce Type: new 
Abstract: Multivariate spatial disease mapping has become a pivotal part of everyday practice in social epidemiology. Despite the existence of several specifications for the relation between different outcomes, there is still a need for a new strategy that focuses on comparing the spatial risk patterns of different subgroups of the population. This paper introduces a new approach for detecting differences in spatial risk patterns between different populations at risk, using suicide-related emergency calls to study suicide risks in the Valencian Community (Spain).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21227v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>P. Escobar-Hern\'andez, A. L\'opez-Qu\'ilez, F. Palm\'i-Perales, M. Marco</dc:creator>
    </item>
    <item>
      <title>Establishing Nationwide Power System Vulnerability Index across US Counties Using Interpretable Machine Learning</title>
      <link>https://arxiv.org/abs/2410.19754</link>
      <description>arXiv:2410.19754v1 Announce Type: cross 
Abstract: Power outages have become increasingly frequent, intense, and prolonged in the US due to climate change, aging electrical grids, and rising energy demand. However, largely due to the absence of granular spatiotemporal outage data, we lack data-driven evidence and analytics-based metrics to quantify power system vulnerability. This limitation has hindered the ability to effectively evaluate and address vulnerability to power outages in US communities. Here, we collected ~179 million power outage records at 15-minute intervals across 3022 US contiguous counties (96.15% of the area) from 2014 to 2023. We developed a power system vulnerability assessment framework based on three dimensions (intensity, frequency, and duration) and applied interpretable machine learning models (XGBoost and SHAP) to compute Power System Vulnerability Index (PSVI) at the county level. Our analysis reveals a consistent increase in power system vulnerability over the past decade. We identified 318 counties across 45 states as hotspots for high power system vulnerability, particularly in the West Coast (California and Washington), the East Coast (Florida and the Northeast area), the Great Lakes megalopolis (Chicago-Detroit metropolitan areas), and the Gulf of Mexico (Texas). Heterogeneity analysis indicates that urban counties, counties with interconnected grids, and states with high solar generation exhibit significantly higher vulnerability. Our results highlight the significance of the proposed PSVI for evaluating the vulnerability of communities to power outages. The findings underscore the widespread and pervasive impact of power outages across the country and offer crucial insights to support infrastructure operators, policymakers, and emergency managers in formulating policies and programs aimed at enhancing the resilience of the US power infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19754v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junwei Ma, Bo Li, Olufemi A. Omitaomu, Ali Mostafavi</dc:creator>
    </item>
    <item>
      <title>Predicting potato plant vigor from the seed tuber properties</title>
      <link>https://arxiv.org/abs/2410.19875</link>
      <description>arXiv:2410.19875v1 Announce Type: cross 
Abstract: The vigor of potato plants, defined as the canopy area at the end of the exponential growth stage, depends on the origin and physiological state of the seed tuber. Experiments carried out with six potato varieties in three test fields over three years show that there is a 73%-90% correlation in the vigor of the plants from the same seedlot grown in different test fields. However, these correlations are not always observed on the level of individual varieties and vanish or become negative when the seed tubers and young plants experience environmental stress. A comprehensive study of the association between the vigor and the seed tuber biochemistry has revealed that, while 50%-70% of the variation in the plant vigor is explained by the tuber data, the vigor is dominated by the potato genotype. Analysis of individual predictors, such as the abundance of a particular metabolite, indicates that the vigor enhancing properties of the seed tubers differ between genotypes. Variety-specific models show that, for some varieties, up to 30% of the vigor variation within the variety is explained by and can be predicted from the tuber biochemistry, whereas, for other varieties, the association between the tuber composition and the vigor is much weaker.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19875v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Elisa Atza, Rob Klooster, Falko Hofstra, Frank van der Werff, Hans van Doorn, Neil Budko</dc:creator>
    </item>
    <item>
      <title>Sample Efficient Bayesian Learning of Causal Graphs from Interventions</title>
      <link>https://arxiv.org/abs/2410.20089</link>
      <description>arXiv:2410.20089v1 Announce Type: cross 
Abstract: Causal discovery is a fundamental problem with applications spanning various areas in science and engineering. It is well understood that solely using observational data, one can only orient the causal graph up to its Markov equivalence class, necessitating interventional data to learn the complete causal graph. Most works in the literature design causal discovery policies with perfect interventions, i.e., they have access to infinite interventional samples. This study considers a Bayesian approach for learning causal graphs with limited interventional samples, mirroring real-world scenarios where such samples are usually costly to obtain. By leveraging the recent result of Wien\"obst et al. (2023) on uniform DAG sampling in polynomial time, we can efficiently enumerate all the cut configurations and their corresponding interventional distributions of a target set, and further track their posteriors. Given any number of interventional samples, our proposed algorithm randomly intervenes on a set of target vertices that cut all the edges in the graph and returns a causal graph according to the posterior of each target set. When the number of interventional samples is large enough, we show theoretically that our proposed algorithm will return the true causal graph with high probability. We compare our algorithm against various baseline methods on simulated datasets, demonstrating its superior accuracy measured by the structural Hamming distance between the learned DAG and the ground truth. Additionally, we present a case study showing how this algorithm could be modified to answer more general causal questions without learning the whole graph. As an example, we illustrate that our method can be used to estimate the causal effect of a variable that cannot be intervened.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20089v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihan Zhou, Muhammad Qasim Elahi, Murat Kocaoglu</dc:creator>
    </item>
    <item>
      <title>Functional Mixture Regression Control Chart</title>
      <link>https://arxiv.org/abs/2410.20138</link>
      <description>arXiv:2410.20138v1 Announce Type: cross 
Abstract: Industrial applications often exhibit multiple in-control patterns due to varying operating conditions, which makes a single functional linear model (FLM) inadequate to capture the complexity of the true relationship between a functional quality characteristic and covariates, which gives rise to the multimode profile monitoring problem. This issue is clearly illustrated in the resistance spot welding (RSW) process in the automotive industry, where different operating conditions lead to multiple in-control states. In these states, factors such as electrode tip wear and dressing may influence the functional quality characteristic differently, resulting in distinct FLMs across subpopulations. To address this problem, this article introduces the functional mixture regression control chart (FMRCC) to monitor functional quality characteristics with multiple in-control patterns and covariate information, modeled using a mixture of FLMs. A monitoring strategy based on the likelihood ratio test is proposed to monitor any deviation from the estimated in-control heterogeneous population. An extensive Monte Carlo simulation study is performed to compare the FMRCC with competing monitoring schemes that have already appeared in the literature, and a case study in the monitoring of an RSW process in the automotive industry, which motivated this research, illustrates its practical applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20138v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Capezza, Fabio Centofanti, Davide Forcina, Antonio Lepore, Biagio Palumbo</dc:creator>
    </item>
    <item>
      <title>DeepMIDE: A Multivariate Spatio-Temporal Method for Ultra-Scale Offshore Wind Energy Forecasting</title>
      <link>https://arxiv.org/abs/2410.20166</link>
      <description>arXiv:2410.20166v1 Announce Type: cross 
Abstract: To unlock access to stronger winds, the offshore wind industry is advancing with significantly larger and taller wind turbines. This massive upscaling motivates a departure from univariate wind forecasting methods that traditionally focused on a single representative height. To fill this gap, we propose DeepMIDE--a statistical deep learning method which jointly models the offshore wind speeds across space, time, and height. DeepMIDE is formulated as a multi-output integro-difference equation model with a multivariate, nonstationary, and state-dependent kernel characterized by a set of advection vectors that encode the physics of wind field formation and propagation. Embedded within DeepMIDE, an advanced deep learning architecture learns these advection vectors from high dimensional streams of exogenous weather information, which, along with other parameters, are plugged back into the statistical model for probabilistic multi-height space-time forecasting. Tested on real-world data from future offshore wind energy sites in the Northeastern United States, the wind speed and power forecasts from DeepMIDE are shown to outperform those from prevalent time series, spatio-temporal, and deep learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20166v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Feng Ye, Xinxi Zhang, Michael Stein, Ahmed Aziz Ezzat</dc:creator>
    </item>
    <item>
      <title>CDsampling: an R Package for Constrained D-optimal Sampling in Paid Research Study</title>
      <link>https://arxiv.org/abs/2410.20606</link>
      <description>arXiv:2410.20606v1 Announce Type: cross 
Abstract: In the context of paid research studies and clinical trials, budget considerations and patient sampling from available populations are subject to inherent constraints. We introduce the R package CDsampling, which is the first to our knowledge to integrate optimal design theories within the framework of constrained sampling. This package offers the possibility to find both D-optimal approximate and exact allocations for samplings with or without constraints. Additionally, it provides functions to find constrained uniform sampling as a robust sampling strategy when the model information is limited. To demonstrate its efficacy, we provide simulated examples and a real-data example with datasets embedded in the package and compare them with classical sampling methods. Furthermore, it revisits the theoretical results of the Fisher information matrix for generalized linear models (including regular linear regression model) and multinomial logistic models, offering functions for its computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20606v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Huang, Liping Tong, Jie Yang</dc:creator>
    </item>
    <item>
      <title>The Curious Problem of the Normal Inverse Mean</title>
      <link>https://arxiv.org/abs/2410.20641</link>
      <description>arXiv:2410.20641v1 Announce Type: cross 
Abstract: In astronomical observations, the estimation of distances from parallaxes is a challenging task due to the inherent measurement errors and the non-linear relationship between the parallax and the distance. This study leverages ideas from robust Bayesian inference to tackle these challenges, investigating a broad class of prior densities for estimating distances with a reduced bias and variance. Through theoretical analysis, simulation experiments, and the application to data from the Gaia Data Release 1 (GDR1), we demonstrate that heavy-tailed priors provide more reliable distance estimates, particularly in the presence of large fractional parallax errors. Theoretical results highlight the "curse of a single observation," where the likelihood dominates the posterior, limiting the impact of the prior. Nevertheless, heavy-tailed priors can delay the explosion of posterior risk, offering a more robust framework for distance estimation. The findings suggest that reciprocal invariant priors, with polynomial decay in their tails, such as the Half-Cauchy and Product Half-Cauchy, are particularly well-suited for this task, providing a balance between bias reduction and variance control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20641v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soham Ghosh, Uttaran Chatterjee, Jyotishka Datta</dc:creator>
    </item>
    <item>
      <title>Valid Bootstraps for Networks with Applications to Network Visualisation</title>
      <link>https://arxiv.org/abs/2410.20895</link>
      <description>arXiv:2410.20895v1 Announce Type: cross 
Abstract: Quantifying uncertainty in networks is an important step in modelling relationships and interactions between entities. We consider the challenge of bootstrapping an inhomogeneous random graph when only a single observation of the network is made and the underlying data generating function is unknown. We utilise an exchangeable network test that can empirically validate bootstrap samples generated by any method, by testing if the observed and bootstrapped networks are statistically distinguishable. We find that existing methods fail this test. To address this, we propose a principled, novel, distribution-free network bootstrap using k-nearest neighbour smoothing, that can regularly pass this exchangeable network test in both synthetic and real-data scenarios. We demonstrate the utility of this work in combination with the popular data visualisation method t-SNE, where uncertainty estimates from bootstrapping are used to explain whether visible structures represent real statistically sound structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20895v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emerald Dilworth, Ed Davis, Daniel J. Lawson</dc:creator>
    </item>
    <item>
      <title>BSD: a Bayesian framework for parametric models of neural spectra</title>
      <link>https://arxiv.org/abs/2410.20896</link>
      <description>arXiv:2410.20896v1 Announce Type: cross 
Abstract: The analysis of neural power spectra plays a crucial role in understanding brain function and dysfunction. While recent efforts have led to the development of methods for decomposing spectral data, challenges remain in performing statistical analysis and group-level comparisons. Here, we introduce Bayesian Spectral Decomposition (BSD), a Bayesian framework for analysing neural spectral power. BSD allows for the specification, inversion, comparison, and analysis of parametric models of neural spectra, addressing limitations of existing methods. We first establish the face validity of BSD on simulated data and show how it outperforms an established method (\fooof{}) for peak detection on artificial spectral data. We then demonstrate the efficacy of BSD on a group-level study of EEG spectra in 204 healthy subjects from the LEMON dataset. Our results not only highlight the effectiveness of BSD in model selection and parameter estimation, but also illustrate how BSD enables straightforward group-level regression of the effect of continuous covariates such as age. By using Bayesian inference techniques, BSD provides a robust framework for studying neural spectral data and their relationship to brain function and dysfunction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20896v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johan Medrano, Nicholas A. Alexander, Robert A. Seymour, Peter Zeidman</dc:creator>
    </item>
    <item>
      <title>On Spatio-Temporal Stochastic Frontier Models</title>
      <link>https://arxiv.org/abs/2410.20915</link>
      <description>arXiv:2410.20915v1 Announce Type: cross 
Abstract: In the literature on stochastic frontier models until the early 2000s, the joint consideration of spatial and temporal dimensions was often inadequately addressed, if not completely neglected. However, from an evolutionary economics perspective, the production process of the decision-making units constantly changes over both dimensions: it is not stable over time due to managerial enhancements and/or internal or external shocks, and is influenced by the nearest territorial neighbours. This paper proposes an extension of the Fusco and Vidoli [2013] SEM-like approach, which globally accounts for spatial and temporal effects in the term of inefficiency. In particular, coherently with the stochastic panel frontier literature, two different versions of the model are proposed: the time-invariant and the time-varying spatial stochastic frontier models. In order to evaluate the inferential properties of the proposed estimators, we first run Monte Carlo experiments and we then present the results of an application to a set of commonly referenced data, demonstrating robustness and stability of estimates across all scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20915v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Elisa Fusco, Giuseppe Arbia, Francesco Vidoli, Vincenzo Nardelli</dc:creator>
    </item>
    <item>
      <title>Almost goodness-of-fit tests</title>
      <link>https://arxiv.org/abs/2410.20918</link>
      <description>arXiv:2410.20918v1 Announce Type: cross 
Abstract: We introduce the almost goodness-of-fit test, a procedure to decide if a (parametric) model provides a good representation of the probability distribution generating the observed sample. We consider the approximate model determined by an M-estimator of the parameters as the best representative of the unknown distribution within the parametric class. The objective is the approximate validation of a distribution or an entire parametric family up to a pre-specified threshold value, the margin of error. The methodology also allows quantifying the percentage improvement of the proposed model compared to a non-informative (constant) one. The test statistic is the $\mathrm{L}^p$-distance between the empirical distribution function and the corresponding one of the estimated (parametric) model. The value of the parameter $p$ allows modulating the impact of the tails of the distribution in the validation of the model. By deriving the asymptotic distribution of the test statistic, as well as proving the consistency of its bootstrap approximation, we present an easy-to-implement and flexible method. The performance of the proposal is illustrated with a simulation study and the analysis of a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20918v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amparo Ba\'illo, Javier C\'arcamo</dc:creator>
    </item>
    <item>
      <title>BanditCAT and AutoIRT: Machine Learning Approaches to Computerized Adaptive Testing and Item Calibration</title>
      <link>https://arxiv.org/abs/2410.21033</link>
      <description>arXiv:2410.21033v1 Announce Type: cross 
Abstract: In this paper, we present a complete framework for quickly calibrating and administering a robust large-scale computerized adaptive test (CAT) with a small number of responses. Calibration - learning item parameters in a test - is done using AutoIRT, a new method that uses automated machine learning (AutoML) in combination with item response theory (IRT), originally proposed in [Sharpnack et al., 2024]. AutoIRT trains a non-parametric AutoML grading model using item features, followed by an item-specific parametric model, which results in an explanatory IRT model. In our work, we use tabular AutoML tools (AutoGluon.tabular, [Erickson et al., 2020]) along with BERT embeddings and linguistically motivated NLP features. In this framework, we use Bayesian updating to obtain test taker ability posterior distributions for administration and scoring.
  For administration of our adaptive test, we propose the BanditCAT framework, a methodology motivated by casting the problem in the contextual bandit framework and utilizing item response theory (IRT). The key insight lies in defining the bandit reward as the Fisher information for the selected item, given the latent test taker ability from IRT assumptions. We use Thompson sampling to balance between exploring items with different psychometric characteristics and selecting highly discriminative items that give more precise information about ability. To control item exposure, we inject noise through an additional randomization step before computing the Fisher information. This framework was used to initially launch two new item types on the DET practice test using limited training data. We outline some reliability and exposure metrics for the 5 practice test experiments that utilized this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21033v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Sharpnack, Kevin Hao, Phoebe Mulcaire, Klinton Bicknell, Geoff LaFlair, Kevin Yancey, Alina A. von Davier</dc:creator>
    </item>
    <item>
      <title>A model and method for analyzing the precision of binary measurement methods based on beta-binomial distributions, and related statistical tests</title>
      <link>https://arxiv.org/abs/2008.13619</link>
      <description>arXiv:2008.13619v3 Announce Type: replace 
Abstract: This study developed a new statistical model and method for analyzing the precision of binary measurement methods from collaborative studies. The model is based on beta-binomial distributions. In other words, it assumes that the sensitivity of each laboratory obeys a beta distribution, and the binary measured values under a given sensitivity follow a binomial distribution. We propose the key precision measures of repeatability and reproducibility for the model, and provide their unbiased estimates. Further, through consideration of a number of statistical test methods for homogeneity of proportions, we propose appropriate methods for determining laboratory effects in the new model. Finally, we apply the results to real-world examples in the fields of food safety and chemical risk assessment and management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.13619v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun-ichi Takeshita, Tomomichi Suzuki</dc:creator>
    </item>
    <item>
      <title>Seeding with Differentially Private Network Information</title>
      <link>https://arxiv.org/abs/2305.16590</link>
      <description>arXiv:2305.16590v3 Announce Type: replace-cross 
Abstract: In public health interventions such as distributing pre-exposure prophylaxis (PrEP) for HIV prevention, decision makers rely on seeding algorithms to identify key individuals who can amplify the impact of their interventions. In such cases, constructing a complete sexual activity network is often infeasible due to privacy concerns. Instead, contact tracing can provide influence samples, i.e., sequences of sexual contacts without requiring complete network information. This brings two challenges: protecting individual privacy in the contact data and adapting seeding algorithms to work effectively with incomplete network information. To solve these two problems, we study privacy guarantees for influence maximization algorithms when the social network is unknown and the inputs are samples of prior influence cascades that are collected at random and need privacy protection. Building on recent results that address seeding with costly network information, our privacy-preserving algorithms introduce randomization in the collected data or the algorithm output, and can bound the privacy loss of each node (or group of nodes) in deciding to include their data in the algorithm input. We provide theoretical guarantees of seeding performance with a limited sample size subject to differential privacy budgets in both central and local privacy regimes. Simulations on synthetic random graphs and empirically grounded sexual contacts of men who have sex with men reveal the diminishing value of network information with decreasing privacy budget in both regimes and graceful decrease in performance with decreasing privacy budget in the central regime. Achieving good performance with local privacy guarantees requires relatively higher privacy budgets that confirm our theoretical expectations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.16590v3</guid>
      <category>cs.SI</category>
      <category>cs.CC</category>
      <category>cs.MA</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Amin Rahimian, Fang-Yi Yu, Yuxin Liu, Carlos Hurtado</dc:creator>
    </item>
    <item>
      <title>Statistical Response of ENSO Complexity to Initial Condition and Model Parameter Perturbations</title>
      <link>https://arxiv.org/abs/2401.03281</link>
      <description>arXiv:2401.03281v5 Announce Type: replace-cross 
Abstract: Studying the response of a climate system to perturbations has practical significance. Standard methods in computing the trajectory-wise deviation caused by perturbations may suffer from the chaotic nature that makes the model error dominate the true response after a short lead time. Statistical response, which computes the return described by the statistics, provides a systematic way of reaching robust outcomes with an appropriate quantification of the uncertainty and extreme events. In this paper, information theory is applied to compute the statistical response and find the most sensitive perturbation direction of different El Ni\~no-Southern Oscillation (ENSO) events to initial value and model parameter perturbations. Depending on the initial phase and the time horizon, different state variables contribute to the most sensitive perturbation direction. While initial perturbations in sea surface temperature (SST) and thermocline depth usually lead to the most significant response of SST at short- and long-range, respectively, initial adjustment of the zonal advection can be crucial to trigger strong statistical responses at medium-range around 5 to 7 months, especially at the transient phases between El Ni\~no and La Ni\~na. It is also shown that the response in the variance triggered by external random forcing perturbations, such as the wind bursts, often dominates the mean response, making the resulting most sensitive direction very different from the trajectory-wise methods. Finally, despite the strong non-Gaussian climatology distributions, using Gaussian approximations in the information theory is efficient and accurate for computing the statistical response, allowing the method to be applied to sophisticated operational systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03281v5</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1175/JCLI-D-24-0017.1</arxiv:DOI>
      <dc:creator>Marios Andreou, Nan Chen</dc:creator>
    </item>
    <item>
      <title>An Adaptive Multivariate Functional EWMA Control Chart</title>
      <link>https://arxiv.org/abs/2403.03837</link>
      <description>arXiv:2403.03837v2 Announce Type: replace-cross 
Abstract: In many modern industrial scenarios, the measurements of the quality characteristics of interest are often required to be represented as functional data or profiles. This motivates the growing interest in extending traditional univariate statistical process monitoring (SPM) schemes to the functional data setting. This article proposes a new SPM scheme, which is referred to as adaptive multivariate functional EWMA (AMFEWMA), to extend the well-known exponentially weighted moving average (EWMA) control chart from the univariate scalar to the multivariate functional setting. The favorable performance of the AMFEWMA control chart over existing methods is assessed via an extensive Monte Carlo simulation. Its practical applicability is demonstrated through a case study in the monitoring of the quality of a resistance spot welding process in the automotive industry through the online observations of dynamic resistance curves, which are associated with multiple spot welds on the same car body and recognized as the full technological signature of the process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03837v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Capezza, Giovanna Capizzi, Fabio Centofanti, Antonio Lepore, Biagio Palumbo</dc:creator>
    </item>
    <item>
      <title>Pruning neural network models for gene regulatory dynamics using data and domain knowledge</title>
      <link>https://arxiv.org/abs/2403.04805</link>
      <description>arXiv:2403.04805v2 Announce Type: replace-cross 
Abstract: The practical utility of machine learning models in the sciences often hinges on their interpretability. It is common to assess a model's merit for scientific discovery, and thus novel insights, by how well it aligns with already available domain knowledge--a dimension that is currently largely disregarded in the comparison of neural network models. While pruning can simplify deep neural network architectures and excels in identifying sparse models, as we show in the context of gene regulatory network inference, state-of-the-art techniques struggle with biologically meaningful structure learning. To address this issue, we propose DASH, a generalizable framework that guides network pruning by using domain-specific structural information in model fitting and leads to sparser, better interpretable models that are more robust to noise. Using both synthetic data with ground truth information, as well as real-world gene expression data, we show that DASH, using knowledge about gene interaction partners within the putative regulatory network, outperforms general pruning methods by a large margin and yields deeper insights into the biological systems being studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04805v2</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Intekhab Hossain, Jonas Fischer, Rebekka Burkholz, John Quackenbush</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Genomic Data with Multiple Heterogeneous Outcomes</title>
      <link>https://arxiv.org/abs/2404.09119</link>
      <description>arXiv:2404.09119v3 Announce Type: replace-cross 
Abstract: With the evolution of single-cell RNA sequencing techniques into a standard approach in genomics, it has become possible to conduct cohort-level causal inferences based on single-cell-level measurements. However, the individual gene expression levels of interest are not directly observable; instead, only repeated proxy measurements from each individual's cells are available, providing a derived outcome to estimate the underlying outcome for each of many genes. In this paper, we propose a generic semiparametric inference framework for doubly robust estimation with multiple derived outcomes, which also encompasses the usual setting of multiple outcomes when the response of each unit is available. To reliably quantify the causal effects of heterogeneous outcomes, we specialize the analysis to standardized average treatment effects and quantile treatment effects. Through this, we demonstrate the use of the semiparametric inferential results for doubly robust estimators derived from both Von Mises expansions and estimating equations. A multiple testing procedure based on Gaussian multiplier bootstrap is tailored for doubly robust estimators to control the false discovery exceedance rate. Applications in single-cell CRISPR perturbation analysis and individual-level differential expression analysis demonstrate the utility of the proposed methods and offer insights into the usage of different estimands for causal inference in genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09119v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Zhenghao Zeng, Edward H. Kennedy, Larry Wasserman, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Distribution of lowest eigenvalue in $k$-body bosonic random matrix ensembles</title>
      <link>https://arxiv.org/abs/2405.00190</link>
      <description>arXiv:2405.00190v2 Announce Type: replace-cross 
Abstract: We numerically study the distribution of the lowest eigenvalue of finite many-boson systems with $k$-body interactions modeled by Bosonic Embedded Gaussian Orthogonal [BEGOE($k$)] and Unitary [BEGUE($k$)] random matrix Ensembles. Following the recently published result that the $q$-normal describes the smooth form of the eigenvalue density of the $k$-body embedded ensembles, the first four moments of the distribution of lowest eigenvalues have been analyzed as a function of the $q$ parameter, with $q \sim 1$ for $k = 1$ and $q = 0$ for $k = m$; $m$ being the number of bosons. Analytics are difficult as we are dealing with highly correlated variables, however we provide ansatzs for centroids and variances of these distributions. These match very well with the numerical results obtained. Our results show the distribution exhibits a smooth transition from Gaussian like for $q$ close to 1 to a modified Gumbel like for intermediate values of $q$ to the well-known Tracy-Widom distribution for $q=0$. It should be emphasized that this is a new result which numerically demonstrates that the distribution of the lowest eigenvalue of finite many-boson systems with $k$-body interactions exhibits a smooth transition from Gaussian like (for $q$ close to 1) to a modified Gumbel like (for intermediate values of $q$) to the well-known Tracy-Widom distribution (for $q=0$). In addition, we have also studied the distribution of normalized spacing between the lowest and next lowest eigenvalues and it is seen that this distribution exhibits a transition from Wigner's surmise (for $k=1$) to Poisson (for intermediate $k$ values with $k \le m/2$) to Wigner's surmise (starting from $k = m/2$ to $k = m$) with decreasing $q$ value. Thus, the spacings at the spectrum edge behave differently from the spacings inside the spectrum bulk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00190v2</guid>
      <category>quant-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>N. D. Chavda, Priyanka Rao, V. K. B. Kota, Manan Vyas</dc:creator>
    </item>
    <item>
      <title>Variational Inference for Acceleration of SN Ia Photometric Distance Estimation with BayeSN</title>
      <link>https://arxiv.org/abs/2405.06013</link>
      <description>arXiv:2405.06013v2 Announce Type: replace-cross 
Abstract: Type Ia supernovae (SNe Ia) are standarizable candles whose observed light curves can be used to infer their distances, which can in turn be used in cosmological analyses. As the quantity of observed SNe Ia grows with current and upcoming surveys, increasingly scalable analyses are necessary to take full advantage of these new datasets for precise estimation of cosmological parameters. Bayesian inference methods enable fitting SN Ia light curves with robust uncertainty quantification, but traditional posterior sampling using Markov Chain Monte Carlo (MCMC) is computationally expensive. We present an implementation of variational inference (VI) to accelerate the fitting of SN Ia light curves using the BayeSN hierarchical Bayesian model for time-varying SN Ia spectral energy distributions (SEDs). We demonstrate and evaluate its performance on both simulated light curves and data from the Foundation Supernova Survey with two different forms of surrogate posterior -- a multivariate normal and a custom multivariate zero-lower-truncated normal distribution -- and compare them with the Laplace Approximation and full MCMC analysis. To validate of our variational approximation, we calculate the pareto-smoothed importance sampling (PSIS) diagnostic, and perform variational simulation-based calibration (VSBC). The VI approximation achieves similar results to MCMC but with an order-of-magnitude speedup for the inference of the photometric distance moduli. Overall, we show that VI is a promising method for scalable parameter inference that enables analysis of larger datasets for precision cosmology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06013v2</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana Sof\'ia M. Uzsoy, Stephen Thorp, Matthew Grayling, Kaisey S. Mandel</dc:creator>
    </item>
    <item>
      <title>Sparse Causal Effect Estimation using Two-Sample Summary Statistics in the Presence of Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2410.12300</link>
      <description>arXiv:2410.12300v3 Announce Type: replace-cross 
Abstract: Observational genome-wide association studies are now widely used for causal inference in genetic epidemiology. To maintain privacy, such data is often only publicly available as summary statistics, and often studies for the endogenous covariates and the outcome are available separately. This has necessitated methods tailored to two-sample summary statistics. Current state-of-the-art methods modify linear instrumental variable (IV) regression -- with genetic variants as instruments -- to account for unmeasured confounding. However, since the endogenous covariates can be high dimensional, standard IV assumptions are generally insufficient to identify all causal effects simultaneously. We ensure identifiability by assuming the causal effects are sparse and propose a sparse causal effect two-sample IV estimator, spaceTSIV, adapting the spaceIV estimator by Pfister and Peters (2022) for two-sample summary statistics. We provide two methods, based on L0- and L1-penalization, respectively. We prove identifiability of the sparse causal effects in the two-sample setting and consistency of spaceTSIV. The performance of spaceTSIV is compared with existing two-sample IV methods in simulations. Finally, we showcase our methods using real proteomic and gene-expression data for drug-target discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12300v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shimeng Huang, Niklas Pfister, Jack Bowden</dc:creator>
    </item>
  </channel>
</rss>
