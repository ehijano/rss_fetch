<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Jul 2024 01:51:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A pseudo-outcome-based framework to analyze treatment heterogeneity in survival data using electronic health records</title>
      <link>https://arxiv.org/abs/2407.01565</link>
      <description>arXiv:2407.01565v1 Announce Type: new 
Abstract: An important aspect of precision medicine focuses on characterizing diverse responses to treatment due to unique patient characteristics, also known as heterogeneous treatment effects (HTE), and identifying beneficial subgroups with enhanced treatment effects. Estimating HTE with right-censored data in observational studies remains challenging. In this paper, we propose a pseudo-outcome-based framework for analyzing HTE in survival data, which includes a list of meta-learners for estimating HTE, a variable importance metric for identifying predictive variables to HTE, and a data-adaptive procedure to select subgroups with enhanced treatment effects. We evaluate the finite sample performance of the framework under various settings of observational studies. Furthermore, we applied the proposed methods to analyze the treatment heterogeneity of a Written Asthma Action Plan (WAAP) on time-to-ED (Emergency Department) return due to asthma exacerbation using a large asthma electronic health records dataset with visit records expanded from pre- to post-COVID-19 pandemic. We identified vulnerable subgroups of patients with poorer asthma outcomes but enhanced benefits from WAAP and characterized patient profiles. Our research provides valuable insights for healthcare providers on the strategic distribution of WAAP, particularly during disruptive public health crises, ultimately improving the management and control of pediatric asthma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01565v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Na Bo, Jong-Hyeon Jeong, Erick Forno, Ying Ding</dc:creator>
    </item>
    <item>
      <title>Model Identifiability for Bivariate Failure Time Data with Competing Risks: Parametric Cause-specific Hazards and Non-parametric Frailty</title>
      <link>https://arxiv.org/abs/2407.01631</link>
      <description>arXiv:2407.01631v1 Announce Type: new 
Abstract: One of the commonly used approaches to capture dependence in multivariate survival data is through the frailty variables. The identifiability issues should be carefully investigated while modeling multivariate survival with or without competing risks. The use of non-parametric frailty distribution(s) is sometimes preferred for its robustness and flexibility properties. In this paper, we consider modeling of bivariate survival data with competing risks through four different kinds of non-parametric frailty and parametric baseline cause-specific hazard functions to investigate the corresponding model identifiability. We make the common assumption of the frailty mean being equal to unity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01631v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Biswadeep Ghosh, Anup Dewanji, Sudipta Das</dc:creator>
    </item>
    <item>
      <title>A Cepstral Model for Efficient Spectral Analysis of Covariate-dependent Time Series</title>
      <link>https://arxiv.org/abs/2407.01763</link>
      <description>arXiv:2407.01763v1 Announce Type: new 
Abstract: This article introduces a novel and computationally fast model to study the association between covariates and power spectra of replicated time series. A random covariate-dependent Cram\'{e}r spectral representation and a semiparametric log-spectral model are used to quantify the association between the log-spectra and covariates. Each replicate-specific log-spectrum is represented by the cepstrum, inducing a cepstral-based multivariate linear model with the cepstral coefficients as the responses. By using only a small number of cepstral coefficients, the model parsimoniously captures frequency patterns of time series and saves a significant amount of computational time compared to existing methods. A two-stage estimation procedure is proposed. In the first stage, a Whittle likelihood-based approach is used to estimate the truncated replicate-specific cepstral coefficients. In the second stage, parameters of the cepstral-based multivariate linear model, and consequently the effect functions of covariates, are estimated. The model is flexible in the sense that it can accommodate various estimation methods for the multivariate linear model, depending on the application, domain knowledge, or characteristics of the covariates. Numerical studies confirm that the proposed method outperforms some existing methods despite its simplicity and shorter computational time. Supplementary materials for this article are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01763v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zeda Li, Yuexiao Dong</dc:creator>
    </item>
    <item>
      <title>A General Framework for Design-Based Treatment Effect Estimation in Paired Cluster-Randomized Experiments</title>
      <link>https://arxiv.org/abs/2407.01765</link>
      <description>arXiv:2407.01765v1 Announce Type: new 
Abstract: Paired cluster-randomized experiments (pCRTs) are common across many disciplines because there is often natural clustering of individuals, and paired randomization can help balance baseline covariates to improve experimental precision. Although pCRTs are common, there is surprisingly no obvious way to analyze this randomization design if an individual-level (rather than cluster-level) treatment effect is of interest. Variance estimation is also complicated due to the dependency created through pairing clusters. Therefore, we aim to provide an intuitive and practical comparison between different estimation strategies in pCRTs in order to inform practitioners' choice of strategy. To this end, we present a general framework for design-based estimation in pCRTs for average individual effects. This framework offers a novel and intuitive view on the bias-variance trade-off between estimators and emphasizes the benefits of covariate adjustment for estimation with pCRTs. In addition to providing a general framework for estimation in pCRTs, the point and variance estimators we present support fixed-sample unbiased estimation with similar precision to a common regression model and consistently conservative variance estimation. Through simulation studies, we compare the performance of the point and variance estimators reviewed. Finally, we compare the performance of estimators with simulations using real data from an educational efficacy trial. Our analysis and simulation studies inform the choice of point and variance estimators for analyzing pCRTs in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01765v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charlotte Z. Mann, Adam C. Sales, Johann A. Gagnon-Bartsch</dc:creator>
    </item>
    <item>
      <title>Exploring causal effects of hormone- and radio-treatments in an observational study of breast cancer using copula-based semi-competing risks models</title>
      <link>https://arxiv.org/abs/2407.01770</link>
      <description>arXiv:2407.01770v1 Announce Type: new 
Abstract: Breast cancer patients may experience relapse or death after surgery during the follow-up period, leading to dependent censoring of relapse. This phenomenon, known as semi-competing risk, imposes challenges in analyzing treatment effects on breast cancer and necessitates advanced statistical tools for unbiased analysis. Despite progress in estimation and inference within semi-competing risks regression, its application to causal inference is still in its early stages. This article aims to propose a frequentist and semi-parametric framework based on copula models that can facilitate valid causal inference, net quantity estimation and interpretation, and sensitivity analysis for unmeasured factors under right-censored semi-competing risks data. We also propose novel procedures to enhance parameter estimation and its applicability in real practice. After that, we apply the proposed framework to a breast cancer study and detect the time-varying causal effects of hormone- and radio-treatments on patients' relapse-free survival and overall survival. Moreover, extensive numerical evaluations demonstrate the method's feasibility, highlighting minimal estimation bias and reliable statistical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01770v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tonghui Yu, Mengjiao Peng, Yifan Cui, Elynn Chen, Chixiang Chen</dc:creator>
    </item>
    <item>
      <title>Forecast Linear Augmented Projection (FLAP): A free lunch to reduce forecast error variance</title>
      <link>https://arxiv.org/abs/2407.01868</link>
      <description>arXiv:2407.01868v1 Announce Type: new 
Abstract: A novel forecast linear augmented projection (FLAP) method is introduced, which reduces the forecast error variance of any unbiased multivariate forecast without introducing bias. The method first constructs new component series which are linear combinations of the original series. Forecasts are then generated for both the original and component series. Finally, the full vector of forecasts is projected onto a linear subspace where the constraints implied by the combination weights hold. It is proven that the trace of the forecast error variance is non-increasing with the number of components, and mild conditions are established for which it is strictly decreasing. It is also shown that the proposed method achieves maximum forecast error variance reduction among linear projection methods. The theoretical results are validated through simulations and two empirical applications based on Australian tourism and FRED-MD data. Notably, using FLAP with Principal Component Analysis (PCA) to construct the new series leads to substantial forecast error variance reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01868v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yangzhuoran Fin Yang, George Athanasopoulos, Rob J. Hyndman, Anastasios Panagiotelis</dc:creator>
    </item>
    <item>
      <title>Robust Linear Mixed Models using Hierarchical Gamma-Divergence</title>
      <link>https://arxiv.org/abs/2407.01883</link>
      <description>arXiv:2407.01883v1 Announce Type: new 
Abstract: Linear mixed models (LMMs), which typically assume normality for both the random effects and error terms, are a popular class of methods for analyzing longitudinal and clustered data. However, such models can be sensitive to outliers, and this can lead to poor statistical results (e.g., biased inference on model parameters and inaccurate prediction of random effects) if the data are contaminated. We propose a new approach to robust estimation and inference for LMMs using a hierarchical gamma divergence, which offers an automated, data-driven approach to downweight the effects of outliers occurring in both the error, and the random effects, using normalized powered density weights. For estimation and inference, we develop a computationally scalable minorization-maximization algorithm for the resulting objective function, along with a clustered bootstrap method for uncertainty quantification and a Hyvarinen score criterion for selecting a tuning parameter controlling the degree of robustness. When the genuine and contamination mixed effects distributions are sufficiently separated, then under suitable regularity conditions assuming the number of clusters tends to infinity, we show the resulting robust estimates can be asymptotically controlled even under a heavy level of (covariate-dependent) contamination. Simulation studies demonstrate hierarchical gamma divergence consistently outperforms several currently available methods for robustifying LMMs, under a wide range of scenarios of outlier generation at both the response and random effects levels. We illustrate the proposed method using data from a multi-center AIDS cohort study, where the use of a robust LMMs using hierarchical gamma divergence approach produces noticeably different results compared to methods that do not adequately adjust for potential outlier contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01883v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shonosuke Sugasawa, Francis K. C. Hui, Alan H. Welsh</dc:creator>
    </item>
    <item>
      <title>Regularized estimation of Monge-Kantorovich quantiles for spherical data</title>
      <link>https://arxiv.org/abs/2407.02085</link>
      <description>arXiv:2407.02085v1 Announce Type: new 
Abstract: Tools from optimal transport (OT) theory have recently been used to define a notion of quantile function for directional data. In practice, regularization is mandatory for applications that require out-of-sample estimates. To this end, we introduce a regularized estimator built from entropic optimal transport, by extending the definition of the entropic map to the spherical setting. We propose a stochastic algorithm to directly solve a continuous OT problem between the uniform distribution and a target distribution, by expanding Kantorovich potentials in the basis of spherical harmonics. In addition, we define the directional Monge-Kantorovich depth, a companion concept for OT-based quantiles. We show that it benefits from desirable properties related to Liu-Zuo-Serfling axioms for the statistical analysis of directional data. Building on our regularized estimators, we illustrate the benefits of our methodology for data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02085v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernard Bercu, J\'er\'emie Bigot, Gauthier Thurin</dc:creator>
    </item>
    <item>
      <title>Reverse time-to-death as time-scale in time-to-event analysis for studies of advanced illness and palliative care</title>
      <link>https://arxiv.org/abs/2407.02178</link>
      <description>arXiv:2407.02178v1 Announce Type: new 
Abstract: Background: Incidence of adverse outcome events rises as patients with advanced illness approach end-of-life. Exposures that tend to occur near end-of-life, e.g., use of wheelchair, oxygen therapy and palliative care, may therefore be found associated with the incidence of the adverse outcomes. We propose a strategy for time-to-event analysis to mitigate the time-varying confounding. Methods: We propose a concept of reverse time-to-death (rTTD) and its use for the time-scale in time-to-event analysis. We used data on community-based palliative care uptake (exposure) and emergency department visits (outcome) among patients with advanced cancer in Singapore to illustrate. We compare the results against that of the common practice of using time-on-study (TOS) as time-scale. Results: Graphical analysis demonstrated that cancer patients receiving palliative care had higher rate of emergency department visits than non-recipients mainly because they were closer to end-of-life, and that rTTD analysis made comparison between patients at the same time-to-death. Analysis of emergency department visits in relation to palliative care using TOS time-scale showed significant increase in hazard ratio estimate when observed time-varying covariates were omitted from statistical adjustment (change-in-estimate=0.38; 95% CI 0.15 to 0.60). There was no such change in otherwise the same analysis using rTTD (change-in-estimate=0.04; 95% CI -0.02 to 0.11), demonstrating the ability of rTTD time-scale to mitigate confounding that intensifies in relation to time-to-death. Conclusion: Use of rTTD as time-scale in time-to-event analysis provides a simple and robust approach to control time-varying confounding in studies of advanced illness, even if the confounders are unmeasured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02178v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yin Bun Cheung, Xiangmei Ma, Isha Chaudhry, Nan Liu, Qingyuan Zhuang, Grace Meijuan Yang, Chetna Malhotra, Eric Andrew Finkelstein</dc:creator>
    </item>
    <item>
      <title>Rediscovering Bottom-Up: Effective Forecasting in Temporal Hierarchies</title>
      <link>https://arxiv.org/abs/2407.02367</link>
      <description>arXiv:2407.02367v1 Announce Type: new 
Abstract: Forecast reconciliation has become a prominent topic in recent forecasting literature, with a primary distinction made between cross-sectional and temporal hierarchies. This work focuses on temporal hierarchies, such as aggregating monthly time series data to annual data. We explore the impact of various forecast reconciliation methods on temporally aggregated ARIMA models, thereby bridging the fields of hierarchical forecast reconciliation and temporal aggregation both theoretically and experimentally. Our paper is the first to theoretically examine the effects of temporal hierarchical forecast reconciliation, demonstrating that the optimal method aligns with a bottom-up aggregation approach. To assess the practical implications and performance of the reconciled forecasts, we conduct a series of simulation studies, confirming that the findings extend to more complex models. This result helps explain the strong performance of the bottom-up approach observed in many prior studies. Finally, we apply our methods to real data examples, where we observe similar results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02367v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Neubauer, Peter Filzmoser</dc:creator>
    </item>
    <item>
      <title>Fuzzy Social Network Analysis: Theory and Application in a University Department's Collaboration Network</title>
      <link>https://arxiv.org/abs/2407.02401</link>
      <description>arXiv:2407.02401v1 Announce Type: new 
Abstract: Social network analysis (SNA) helps us understand the relationships and interactions between individuals, groups, organisations, or other social entities. In SNA, ties are generally binary or weighted based on their strength. Nonetheless, when actors are individuals, the relationships between actors are often imprecise and identifying them with simple scalars leads to information loss. Social relationships are often vague in real life. Despite many classical social network techniques contemplate the use of weighted links, these approaches do not align with the original philosophy of fuzzy logic, which instead aims to preserve the vagueness inherent in human language and real life. Dealing with imprecise ties and introducing fuzziness in the definition of relationships requires an extension of social network analysis to fuzzy numbers instead of crisp values. The mathematical formalisation for this generalisation needs to extend classical centrality indices and operations to fuzzy numbers. For this reason, this paper proposes a generalisation of the so-called Fuzzy Social Network Analysis (FSNA) to the context of imprecise relationships among actors. The article shows the theory and application of real data collected through a fascinating mouse tracking technique to study the fuzzy relationships in a collaboration network among the members of a University department.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02401v1</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annamaria Porreca, Fabrizio Maturo, Viviana Ventre</dc:creator>
    </item>
    <item>
      <title>Deciphering interventional dynamical causality from non-intervention systems</title>
      <link>https://arxiv.org/abs/2407.01621</link>
      <description>arXiv:2407.01621v1 Announce Type: cross 
Abstract: Detecting and quantifying causality is a focal topic in the fields of science, engineering, and interdisciplinary studies. However, causal studies on non-intervention systems attract much attention but remain extremely challenging. To address this challenge, we propose a framework named Interventional Dynamical Causality (IntDC) for such non-intervention systems, along with its computational criterion, Interventional Embedding Entropy (IEE), to quantify causality. The IEE criterion theoretically and numerically enables the deciphering of IntDC solely from observational (non-interventional) time-series data, without requiring any knowledge of dynamical models or real interventions in the considered system. Demonstrations of performance showed the accuracy and robustness of IEE on benchmark simulated systems as well as real-world systems, including the neural connectomes of C. elegans, COVID-19 transmission networks in Japan, and regulatory networks surrounding key circadian genes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01621v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jifan Shi, Yang Li, Juan Zhao, Siyang Leng, Kazuyuki Aihara, Luonan Chen, Wei Lin</dc:creator>
    </item>
    <item>
      <title>Uncertainty estimation in satellite precipitation spatial prediction by combining distributional regression algorithms</title>
      <link>https://arxiv.org/abs/2407.01623</link>
      <description>arXiv:2407.01623v1 Announce Type: cross 
Abstract: To facilitate effective decision-making, gridded satellite precipitation products should include uncertainty estimates. Machine learning has been proposed for issuing such estimates. However, most existing algorithms for this purpose rely on quantile regression. Distributional regression offers distinct advantages over quantile regression, including the ability to model intermittency as well as a stronger ability to extrapolate beyond the training data, which is critical for predicting extreme precipitation. In this work, we introduce the concept of distributional regression for the engineering task of creating precipitation datasets through data merging. Building upon this concept, we propose new ensemble learning methods that can be valuable not only for spatial prediction but also for prediction problems in general. These methods exploit conditional zero-adjusted probability distributions estimated with generalized additive models for location, scale, and shape (GAMLSS), spline-based GAMLSS and distributional regression forests as well as their ensembles (stacking based on quantile regression, and equal-weight averaging). To identify the most effective methods for our specific problem, we compared them to benchmarks using a large, multi-source precipitation dataset. Stacking emerged as the most successful strategy. Three specific stacking methods achieved the best performance based on the quantile scoring rule, although the ranking of these methods varied across quantile levels. This suggests that a task-specific combination of multiple algorithms could yield significant benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01623v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgia Papacharalampous, Hristos Tyralis, Nikolaos Doulamis, Anastasios Doulamis</dc:creator>
    </item>
    <item>
      <title>Asymptotic tests for monotonicity and convexity of a probability mass function</title>
      <link>https://arxiv.org/abs/2407.01751</link>
      <description>arXiv:2407.01751v1 Announce Type: cross 
Abstract: In shape-constrained nonparametric inference, it is often necessary to perform preliminary tests to verify whether a probability mass function (p.m.f.) satisfies qualitative constraints such as monotonicity, convexity or in general $k$-monotonicity. In this paper, we are interested in testing $k$-monotonicity of a compactly supported p.m.f. and we put our main focus on monotonicity and convexity; i.e., $k \in \{1,2\}$. We consider new testing procedures that are directly derived from the definition of $k$-monotonicity and rely exclusively on the empirical measure, as well as tests that are based on the projection of the empirical measure on the class of $k$-monotone p.m.f.s. The asymptotic behaviour of the introduced test statistics is derived and a simulation study is performed to assess the finite sample performance of all the proposed tests. Applications to real datasets are presented to illustrate the theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01751v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fadoua Balabdaoui, Antonio Di Noia</dc:creator>
    </item>
    <item>
      <title>Conditionally valid Probabilistic Conformal Prediction</title>
      <link>https://arxiv.org/abs/2407.01794</link>
      <description>arXiv:2407.01794v1 Announce Type: cross 
Abstract: We develop a new method for creating prediction sets that combines the flexibility of conformal methods with an estimate of the conditional distribution $P_{Y \mid X}$. Most existing methods, such as conformalized quantile regression and probabilistic conformal prediction, only offer marginal coverage guarantees. Our approach extends these methods to achieve conditional coverage, which is essential for many practical applications. While exact conditional guarantees are impossible without assumptions on the data distribution, we provide non-asymptotic bounds that explicitly depend on the quality of the available estimate of the conditional distribution. Our confidence sets are highly adaptive to the local structure of the data, making them particularly useful in high heteroskedasticity situations. We demonstrate the effectiveness of our approach through extensive simulations, showing that it outperforms existing methods in terms of conditional coverage and improves the reliability of statistical inference in a wide range of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01794v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Plassier, Alexander Fishkov, Maxim Panov, Eric Moulines</dc:creator>
    </item>
    <item>
      <title>Simultaneous semiparametric inference for single-index models</title>
      <link>https://arxiv.org/abs/2407.01874</link>
      <description>arXiv:2407.01874v1 Announce Type: cross 
Abstract: In the common partially linear single-index model we establish a Bahadur representation for a smoothing spline estimator of all model parameters and use this result to prove the joint weak convergence of the estimator of the index link function at a given point, together with the estimators of the parametric regression coefficients. We obtain the surprising result that, despite of the nature of single-index models where the link function is evaluated at a linear combination of the index-coefficients, the estimator of the link function and the estimator of the index-coefficients are asymptotically independent. Our approach leverages a delicate analysis based on reproducing kernel Hilbert space and empirical process theory.
  We show that the smoothing spline estimator achieves the minimax optimal rate with respect to the $L^2$-risk and consider several statistical applications where joint inference on all model parameters is of interest. In particular, we develop a simultaneous confidence band for the link function and propose inference tools to investigate if the maximum absolute deviation between the (unknown) link function and a given function exceeds a given threshold. We also construct tests for joint hypotheses regarding model parameters which involve both the nonparametric and parametric components and propose novel multiplier bootstrap procedures to avoid the estimation of unknown asymptotic quantities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01874v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiajun Tang, Holger Dette</dc:creator>
    </item>
    <item>
      <title>On minimum contrast method for multivariate spatial point processes</title>
      <link>https://arxiv.org/abs/2208.07044</link>
      <description>arXiv:2208.07044v3 Announce Type: replace 
Abstract: Compared to widely used likelihood-based approaches, the minimum contrast (MC) method offers a computationally efficient method for estimation and inference of spatial point processes. These relative gains in computing time become more pronounced when analyzing complicated multivariate point process models. Despite this, there has been little exploration of the MC method for multivariate spatial point processes. Therefore, this article introduces a new MC method for parametric multivariate spatial point processes. A contrast function is computed based on the trace of the power of the difference between the conjectured $K$-function matrix and its nonparametric unbiased edge-corrected estimator. Under standard assumptions, we derive the asymptotic normality of our MC estimator. The performance of the proposed method is demonstrated through simulation studies of bivariate log-Gaussian Cox processes and five-variate product-shot-noise Cox processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.07044v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Zhu, Junho Yang, Mikyoung Jun, Scott Cook</dc:creator>
    </item>
    <item>
      <title>Causal Meta-Analysis by Integrating Multiple Observational Studies with Multivariate Outcomes</title>
      <link>https://arxiv.org/abs/2306.16715</link>
      <description>arXiv:2306.16715v5 Announce Type: replace 
Abstract: Integrating multiple observational studies to make unconfounded causal or descriptive comparisons of group potential outcomes in a large natural population is challenging. Moreover, retrospective cohorts, being convenience samples, are usually unrepresentative of the natural population of interest and have groups with unbalanced covariates. We propose a general covariate-balancing framework based on pseudo-populations that extends established weighting methods to the meta-analysis of multiple retrospective cohorts with multiple groups. Additionally, by maximizing the effective sample sizes of the cohorts, we propose a FLEXible, Optimized, and Realistic (FLEXOR) weighting method appropriate for integrative analyses. We develop new weighted estimators for unconfounded inferences on wide-ranging population-level features and estimands relevant to group comparisons of quantitative, categorical, or multivariate outcomes. Asymptotic properties of these estimators are examined. Through simulation studies and meta-analyses of TCGA datasets, we demonstrate the versatility and reliability of the proposed weighting strategy, especially for the FLEXOR pseudo-population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16715v5</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subharup Guha, Yi Li</dc:creator>
    </item>
    <item>
      <title>CausalMetaR: An R package for performing causally interpretable meta-analyses</title>
      <link>https://arxiv.org/abs/2402.04341</link>
      <description>arXiv:2402.04341v2 Announce Type: replace 
Abstract: Researchers would often like to leverage data from a collection of sources (e.g., primary studies in a meta-analysis) to estimate causal effects in a target population of interest. However, traditional meta-analytic methods do not produce causally interpretable estimates for a well-defined target population. In this paper, we present the CausalMetaR R package, which implements efficient and robust methods to estimate causal effects in a given internal or external target population using multi-source data. The package includes estimators of average and subgroup treatment effects for the entire target population. To produce efficient and robust estimates of causal effects, the package implements doubly robust and non-parametric efficient estimators and supports using flexible data-adaptive (e.g., machine learning techniques) methods and cross-fitting techniques to estimate the nuisance models (e.g., the treatment model, the outcome model). We describe the key features of the package and demonstrate how to use the package through an example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04341v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Guanbo Wang, Sean McGrath, Yi Lian</dc:creator>
    </item>
    <item>
      <title>Design-based Causal Inference for Balanced Incomplete Block Designs</title>
      <link>https://arxiv.org/abs/2405.19312</link>
      <description>arXiv:2405.19312v2 Announce Type: replace 
Abstract: Researchers often turn to block randomization to increase the precision of their inference or due to practical considerations, such as in multi-site trials. However, if the number of treatments under consideration is large it might not be practical or even feasible to assign all treatments within each block. We develop novel inference results under the finite-population design-based framework for a natural alternative to the complete block design that does not require reducing the number of treatment arms, the balanced incomplete block design (BIBD). This includes deriving the properties of two estimators for BIBDs and proposing conservative variance estimators. To assist practitioners in understanding the trade-offs of using BIBDs over other designs, the precisions of resulting estimators are compared to standard estimators for the complete block, cluster-randomized, and completely randomized designs. Simulations and a data illustration demonstrate the trade-offs of using BIBDs. This work highlights BIBDs as practical and currently underutilized designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19312v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taehyeon Koo, Nicole E. Pashley</dc:creator>
    </item>
    <item>
      <title>Zero Inflation as a Missing Data Problem: a Proxy-based Approach</title>
      <link>https://arxiv.org/abs/2406.00549</link>
      <description>arXiv:2406.00549v2 Announce Type: replace 
Abstract: A common type of zero-inflated data has certain true values incorrectly replaced by zeros due to data recording conventions (rare outcomes assumed to be absent) or details of data recording equipment (e.g. artificial zeros in gene expression data).
  Existing methods for zero-inflated data either fit the observed data likelihood via parametric mixture models that explicitly represent excess zeros, or aim to replace excess zeros by imputed values. If the goal of the analysis relies on knowing true data realizations, a particular challenge with zero-inflated data is identifiability, since it is difficult to correctly determine which observed zeros are real and which are inflated.
  This paper views zero-inflated data as a general type of missing data problem, where the observability indicator for a potentially censored variable is itself unobserved whenever a zero is recorded. We show that, without additional assumptions, target parameters involving a zero-inflated variable are not identified. However, if a proxy of the missingness indicator is observed, a modification of the effect restoration approach of Kuroki and Pearl allows identification and estimation, given the proxy-indicator relationship is known.
  If this relationship is unknown, our approach yields a partial identification strategy for sensitivity analysis. Specifically, we show that only certain proxy-indicator relationships are compatible with the observed data distribution. We give an analytic bound for this relationship in cases with a categorical outcome, which is sharp in certain models. For more complex cases, sharp numerical bounds may be computed using methods in Duarte et al.[2023].
  We illustrate our method via simulation studies and a data application on central line-associated bloodstream infections (CLABSIs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00549v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Trung Phung, Jaron J. R. Lee, Opeyemi Oladapo-Shittu, Eili Y. Klein, Ayse Pinar Gurses, Susan M. Hannum, Kimberly Weems, Jill A. Marsteller, Sara E. Cosgrove, Sara C. Keller, Ilya Shpitser</dc:creator>
    </item>
    <item>
      <title>Approximate Gibbs Sampler for Efficient Inference of Hierarchical Bayesian Models for Grouped Count Data</title>
      <link>https://arxiv.org/abs/2211.15771</link>
      <description>arXiv:2211.15771v2 Announce Type: replace-cross 
Abstract: Hierarchical Bayesian Poisson regression models (HBPRMs) provide a flexible modeling approach of the relationship between predictors and count response variables. The applications of HBPRMs to large-scale datasets require efficient inference algorithms due to the high computational cost of inferring many model parameters based on random sampling. Although Markov Chain Monte Carlo (MCMC) algorithms have been widely used for Bayesian inference, sampling using this class of algorithms is time-consuming for applications with large-scale data and time-sensitive decision-making, partially due to the non-conjugacy of many models. To overcome this limitation, this research develops an approximate Gibbs sampler (AGS) to efficiently learn the HBPRMs while maintaining the inference accuracy. In the proposed sampler, the data likelihood is approximated with Gaussian distribution such that the conditional posterior of the coefficients has a closed-form solution. Numerical experiments using real and synthetic datasets with small and large counts demonstrate the superior performance of AGS in comparison to the state-of-the-art sampling algorithm, especially for large datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.15771v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/00949655.2024.2364843</arxiv:DOI>
      <dc:creator>Jin-Zhu Yu, Hiba Baroud</dc:creator>
    </item>
    <item>
      <title>Sparse Variational Contaminated Noise Gaussian Process Regression with Applications in Geomagnetic Perturbations Forecasting</title>
      <link>https://arxiv.org/abs/2402.17570</link>
      <description>arXiv:2402.17570v3 Announce Type: replace-cross 
Abstract: Gaussian Processes (GP) have become popular machine-learning methods for kernel-based learning on datasets with complicated covariance structures. In this paper, we present a novel extension to the GP framework using a contaminated normal likelihood function to better account for heteroscedastic variance and outlier noise. We propose a scalable inference algorithm based on the Sparse Variational Gaussian Process (SVGP) method for fitting sparse Gaussian process regression models with contaminated normal noise on large datasets. We examine an application to geomagnetic ground perturbations, where the state-of-the-art prediction model is based on neural networks. We show that our approach yields shorter prediction intervals for similar coverage and accuracy when compared to an artificial dense neural network baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17570v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Iong, Matthew McAnear, Yuezhou Qu, Shasha Zou, Gabor Toth, Yang Chen</dc:creator>
    </item>
  </channel>
</rss>
