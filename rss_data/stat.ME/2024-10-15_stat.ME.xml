<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Oct 2024 02:04:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Flexibly Modeling Shocks to Demographic and Health Indicators with Bayesian Shrinkage Priors</title>
      <link>https://arxiv.org/abs/2410.09217</link>
      <description>arXiv:2410.09217v1 Announce Type: new 
Abstract: Demographic and health indicators may exhibit short or large short-term shocks; for example, armed conflicts, epidemics, or famines may cause shocks in period measures of life expectancy. Statistical models for estimating historical trends and generating future projections of these indicators for a large number of populations may be biased or not well probabilistically calibrated if they do not account for the presence of shocks. We propose a flexible method for modeling shocks when producing estimates and projections for multiple populations. The proposed approach makes no assumptions about the shape or duration of a shock, and requires no prior knowledge of when shocks may have occurred. Our approach is based on the modeling of shocks in level of the indicator of interest. We use Bayesian shrinkage priors such that shock terms are shrunk to zero unless the data suggest otherwise. The method is demonstrated in a model for male period life expectancy at birth. We use as a starting point an existing projection model and expand it by including the shock terms, modeled by the Bayesian shrinkage priors. Out-of-sample validation exercises find that including shocks in the model results in sharper uncertainty intervals without sacrificing empirical coverage or prediction error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09217v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Herbert Susmann, Leontine Alkema</dc:creator>
    </item>
    <item>
      <title>Experimentation on Endogenous Graphs</title>
      <link>https://arxiv.org/abs/2410.09267</link>
      <description>arXiv:2410.09267v1 Announce Type: new 
Abstract: We study experimentation under endogenous network interference. Interference patterns are mediated by an endogenous graph, where edges can be formed or eliminated as a result of treatment. We show that conventional estimators are biased in these circumstances, and present a class of unbiased, consistent and asymptotically normal estimators of total treatment effects in the presence of such interference. Our results apply both to bipartite experimentation, in which the units of analysis and measurement differ, and the standard network experimentation case, in which they are the same.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09267v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenshuo Wang, Edvard Bakhitov, Dominic Coey</dc:creator>
    </item>
    <item>
      <title>Hierarchical Latent Class Models for Mortality Surveillance Using Partially Verified Verbal Autopsies</title>
      <link>https://arxiv.org/abs/2410.09274</link>
      <description>arXiv:2410.09274v1 Announce Type: new 
Abstract: Monitoring data on causes of death is an important part of understanding the burden of diseases and effects of public health interventions. Verbal autopsy (VA) is a well-established method for gathering information about deaths outside of hospitals by conducting an interview to family members or caregivers of a deceased person. Existing cause-of-death assignment algorithms using VA data require either domain knowledge about the symptom-cause relationship, or large training datasets. When a new disease emerges, however, only limited information on symptom-cause relationship exists and training data are usually lacking, making it challenging to evaluate the impact of the disease. In this paper, we propose a novel Bayesian framework to estimate the fraction of deaths due to an emerging disease using VAs collected with partially verified cause of death. We use a latent class model to capture the distribution of symptoms and their dependence in a parsimonious way. We discuss potential sources of bias that may occur due to the cause-of-death verification process and adapt our framework to account for the verification mechanism. We also develop structured priors to improve prevalence estimation for sub-populations. We demonstrate the performance of our model using a mortality surveillance dataset that includes suspected COVID-19 related deaths in Brazil in 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09274v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Zhu, Zehang Richard Li</dc:creator>
    </item>
    <item>
      <title>Measurement Error Correction for Spatially Defined Environmental Exposures in Survival Analysis</title>
      <link>https://arxiv.org/abs/2410.09278</link>
      <description>arXiv:2410.09278v1 Announce Type: new 
Abstract: Environmental exposures are often defined using buffer zones around geocoded home addresses, but these static boundaries can miss dynamic daily activity patterns, leading to biased results. This paper presents a novel measurement error correction method for spatially defined environmental exposures within a survival analysis framework using the Cox proportional hazards model. The method corrects high-dimensional surrogate exposures from geocoded residential data at multiple buffer radii by applying principal component analysis for dimension reduction and leveraging external GPS-tracked validation datasets containing true exposure measurements. It also derives the asymptotic properties and variances of the proposed estimators. Extensive simulations are conducted to evaluate the performance of the proposed estimators, demonstrating its ability to improve accuracy in estimated exposure effects. An illustrative application assesses the impact of greenness exposure on depression incidence in the Nurses' Health Study (NHS). The results demonstrate that correcting for measurement error significantly enhances the accuracy of exposure estimates. This method offers a critical advancement for accurately assessing the health impacts of environmental exposures, outperforming traditional static buffer approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09278v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Ge, Ce Yang, David Zucker, Jiaxuan Li, Donna Spiegelman, Molin Wang</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Continuous-Time Confidence Processes for Inhomogeneous Poisson Processes</title>
      <link>https://arxiv.org/abs/2410.09282</link>
      <description>arXiv:2410.09282v1 Announce Type: new 
Abstract: Motivated by monitoring the arrival of incoming adverse events such as customer support calls or crash reports from users exposed to an experimental product change, we consider sequential hypothesis testing of continuous-time inhomogeneous Poisson point processes. Specifically, we provide an interval-valued confidence process $C^\alpha(t)$ over continuous time $t$ for the cumulative arrival rate $\Lambda(t) = \int_0^t \lambda(s) \mathrm{d}s$ with a continuous-time anytime-valid coverage guarantee $\mathbb{P}[\Lambda(t) \in C^\alpha(t) \, \forall t &gt;0] \geq 1-\alpha$. We extend our results to compare two independent arrival processes by constructing multivariate confidence processes and a closed-form $e$-process for testing the equality of rates with a time-uniform Type-I error guarantee at a nominal $\alpha$. We characterize the asymptotic growth rate of the proposed $e$-process under the alternative and show that it has power 1 when the average rates of the two Poisson process differ in the limit. We also observe a complementary relationship between our multivariate confidence process and the universal inference $e$-process for testing composite null hypotheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09282v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lindon, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Bayesian Transfer Learning for Artificially Intelligent Geospatial Systems: A Predictive Stacking Approach</title>
      <link>https://arxiv.org/abs/2410.09504</link>
      <description>arXiv:2410.09504v1 Announce Type: new 
Abstract: Building artificially intelligent geospatial systems require rapid delivery of spatial data analysis at massive scales with minimal human intervention. Depending upon their intended use, data analysis may also entail model assessment and uncertainty quantification. This article devises transfer learning frameworks for deployment in artificially intelligent systems, where a massive data set is split into smaller data sets that stream into the analytical framework to propagate learning and assimilate inference for the entire data set. Specifically, we introduce Bayesian predictive stacking for multivariate spatial data and demonstrate its effectiveness in rapidly analyzing massive data sets. Furthermore, we make inference feasible in a reasonable amount of time, and without excessively demanding hardware settings. We illustrate the effectiveness of this approach in extensive simulation experiments and subsequently analyze massive data sets in climate science on sea surface temperatures and on vegetation index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09504v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Presicce, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Distribution-Aware Mean Estimation under User-level Local Differential Privacy</title>
      <link>https://arxiv.org/abs/2410.09506</link>
      <description>arXiv:2410.09506v1 Announce Type: new 
Abstract: We consider the problem of mean estimation under user-level local differential privacy, where $n$ users are contributing through their local pool of data samples. Previous work assume that the number of data samples is the same across users. In contrast, we consider a more general and realistic scenario where each user $u \in [n]$ owns $m_u$ data samples drawn from some generative distribution $\mu$; $m_u$ being unknown to the statistician but drawn from a known distribution $M$ over $\mathbb{N}^\star$. Based on a distribution-aware mean estimation algorithm, we establish an $M$-dependent upper bounds on the worst-case risk over $\mu$ for the task of mean estimation. We then derive a lower bound. The two bounds are asymptotically matching up to logarithmic factors and reduce to known bounds when $m_u = m$ for any user $u$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09506v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Corentin Pla, Hugo Richard, Maxime Vono</dc:creator>
    </item>
    <item>
      <title>Model-based clustering of time-dependent observations with common structural changes</title>
      <link>https://arxiv.org/abs/2410.09552</link>
      <description>arXiv:2410.09552v1 Announce Type: new 
Abstract: We propose a novel model-based clustering approach for samples of time series. We assume as a unique commonality that two observations belong to the same group if structural changes in their behaviours happen at the same time. We resort to a latent representation of structural changes in each time series based on random orders to induce ties among different observations. Such an approach results in a general modeling strategy and can be combined with many time-dependent models known in the literature. Our studies have been motivated by an epidemiological problem, where we want to provide clusters of different countries of the European Union, where two countries belong to the same cluster if the spreading processes of the COVID-19 virus had structural changes at the same time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09552v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Corradin, Luca Danese, Wasiur R. KhudaBukhsh, Andrea Ongaro</dc:creator>
    </item>
    <item>
      <title>ipd: An R Package for Conducting Inference on Predicted Data</title>
      <link>https://arxiv.org/abs/2410.09665</link>
      <description>arXiv:2410.09665v1 Announce Type: new 
Abstract: Summary: ipd is an open-source R software package for the downstream modeling of an outcome and its associated features where a potentially sizable portion of the outcome data has been imputed by an artificial intelligence or machine learning (AI/ML) prediction algorithm. The package implements several recent proposed methods for inference on predicted data (IPD) with a single, user-friendly wrapper function, ipd. The package also provides custom print, summary, tidy, glance, and augment methods to facilitate easy model inspection. This document introduces the ipd software package and provides a demonstration of its basic usage. Availability: ipd is freely available on CRAN or as a developer version at our GitHub page: github.com/ipd-tools/ipd. Full documentation, including detailed instructions and a usage `vignette' are available at github.com/ipd-tools/ipd. Contact: jtleek@fredhutch.org and tylermc@uw.edu</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09665v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Salerno, Jiacheng Miao, Awan Afiaz, Kentaro Hoffman, Anna Neufeld, Qiongshi Lu, Tyler H. McCormick, Jeffrey T. Leek</dc:creator>
    </item>
    <item>
      <title>Random effects model-based sufficient dimension reduction for independent clustered data</title>
      <link>https://arxiv.org/abs/2410.09712</link>
      <description>arXiv:2410.09712v1 Announce Type: new 
Abstract: Sufficient dimension reduction (SDR) is a popular class of regression methods which aim to find a small number of linear combinations of covariates that capture all the information of the responses i.e., a central subspace. The majority of current methods for SDR focus on the setting of independent observations, while the few techniques that have been developed for clustered data assume the linear transformation is identical across clusters. In this article, we introduce random effects SDR, where cluster-specific random effect central subspaces are assumed to follow a distribution on the Grassmann manifold, and the random effects distribution is characterized by a covariance matrix that captures the heterogeneity between clusters in the SDR process itself. We incorporate random effect SDR within a model-based inverse regression framework. Specifically, we propose a random effects principal fitted components model, where a two-stage algorithm is used to estimate the overall fixed effect central subspace, and predict the cluster-specific random effect central subspaces. We demonstrate the consistency of the proposed estimators, while simulation studies demonstrate the superior performance of the proposed approach compared to global and cluster-specific SDR approaches. We also present extensions of the above model to handle mixed predictors, demonstrating how random effects SDR can be achieved in the case of mixed continuous and binary covariates. Applying the proposed methods to study the longitudinal association between the life expectancy of women and socioeconomic variables across 117 countries, we find log income per capita, infant mortality, and income inequality are the main drivers of a two-dimensional fixed effect central subspace, although there is considerable heterogeneity in how the country-specific central subspaces are driven by the predictors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09712v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linh H. Nghiem, F. K. C. Hui</dc:creator>
    </item>
    <item>
      <title>Optimal item calibration in the context of the Swedish Scholastic Aptitude Test</title>
      <link>https://arxiv.org/abs/2410.09808</link>
      <description>arXiv:2410.09808v1 Announce Type: new 
Abstract: Large scale achievement tests require the existence of item banks with items for use in future tests. Before an item is included into the bank, its characteristics need to be estimated. The process of estimating the item characteristics is called item calibration. For the quality of the future achievement tests, it is important to perform this calibration well and it is desirable to estimate the item characteristics as efficiently as possible. Methods of optimal design have been developed to allocate calibration items to examinees with the most suited ability. Theoretical evidence shows advantages with using ability-dependent allocation of calibration items. However, it is not clear whether these theoretical results hold also in a real testing situation. In this paper, we investigate the performance of an optimal ability-dependent allocation in the context of the Swedish Scholastic Aptitude Test (SweSAT) and quantify the gain from using the optimal allocation. On average over all items, we see an improved precision of calibration. While this average improvement is moderate, we are able to identify for what kind of items the method works well. This enables targeting specific item types for optimal calibration. We also discuss possibilities for improvements of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09808v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Bjermo, Ellinor Fackle Fornius, Frank Miller</dc:creator>
    </item>
    <item>
      <title>Doubly unfolded adjacency spectral embedding of dynamic multiplex graphs</title>
      <link>https://arxiv.org/abs/2410.09810</link>
      <description>arXiv:2410.09810v1 Announce Type: new 
Abstract: Many real-world networks evolve dynamically over time and present different types of connections between nodes, often called layers. In this work, we propose a latent position model for these objects, called the dynamic multiplex random dot product graph (DMPRDPG), which uses an inner product between layer-specific and time-specific latent representations of the nodes to obtain edge probabilities. We further introduce a computationally efficient spectral embedding method for estimation of DMPRDPG parameters, called doubly unfolded adjacency spectral embedding (DUASE). The DUASE estimates are proved to be consistent and asymptotically normally distributed, demonstrating the optimality properties of the proposed estimator. A key strength of our method is the encoding of time-specific node representations and layer-specific effects in separate latent spaces, which allows the model to capture complex behaviours while maintaining relatively low dimensionality. The embedding method we propose can also be efficiently used for subsequent inference tasks. In particular, we highlight the use of the ISOMAP algorithm in conjunction with DUASE as a way to efficiently capture trends and global changepoints within a network, and the use of DUASE for graph clustering. Applications on real-world networks describing geopolitical interactions between countries and financial news reporting demonstrate practical uses of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09810v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maximilian Baum, Francesco Sanna Passino, Axel Gandy</dc:creator>
    </item>
    <item>
      <title>Detecting Structural Shifts and Estimating Change-Points in Interval-Based Time Series</title>
      <link>https://arxiv.org/abs/2410.09884</link>
      <description>arXiv:2410.09884v1 Announce Type: new 
Abstract: This paper addresses the open problem of conducting change-point analysis for interval-valued time series data using the maximum likelihood estimation (MLE) framework. Motivated by financial time series, we analyze data that includes daily opening (O), up (U), low (L), and closing (C) values, rather than just a closing value as traditionally used. To tackle this, we propose a fundamental model based on stochastic differential equations, which also serves as a transformation of other widely used models, such as the log-transformed geometric Brownian motion model. We derive the joint distribution for these interval-valued observations using the reflection principle and Girsanov's theorem. The MLE is obtained by optimizing the log-likelihood function through first and second-order derivative calculations, utilizing the Newton-Raphson algorithm. We further propose a novel parametric bootstrap method to compute confidence intervals, addressing challenges related to temporal dependency and interval-based data relationships. The performance of the model is evaluated through extensive simulations and real data analysis using S&amp;P500 returns during the 2022 Russo-Ukrainian War. The results demonstrate that the proposed OULC model consistently outperforms the traditional OC model, offering more accurate and reliable change-point detection and parameter estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09884v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li-Hsien Sun, Zong-Yuan Huang, Chi-Yang Chiu, Ning Ning</dc:creator>
    </item>
    <item>
      <title>A Bayesian promotion time cure model with current status data</title>
      <link>https://arxiv.org/abs/2410.09892</link>
      <description>arXiv:2410.09892v1 Announce Type: new 
Abstract: Analysis of lifetime data from epidemiological studies or destructive testing often involves current status censoring, wherein individuals are examined only once and their event status is recorded only at that specific time point. In practice, some of these individuals may never experience the event of interest, leading to current status data with a cured fraction. Cure models are used to estimate the proportion of non-susceptible individuals, the distribution of susceptible ones, and covariate effects. Motivated from a biological interpretation of cancer metastasis, promotion time cure model is a popular alternative to the mixture cure rate model for analysing such data. The current study is the first to put forth a Bayesian inference procedure for analysing current status data with a cure fraction, resorting to a promotion time cure model. An adaptive Metropolis-Hastings algorithm is utilised for posterior computation. Simulation studies prove our approach's efficiency, while analyses of lung tumor and breast cancer data illustrate its practical utility. This approach has the potential to improve clinical cure rates through the incorporation of prior knowledge regarding the disease dynamics and therapeutic options.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09892v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavithra Hariharan, P. G. Sankaran</dc:creator>
    </item>
    <item>
      <title>A Bayesian Joint Modelling of Current Status and Current Count Data</title>
      <link>https://arxiv.org/abs/2410.09898</link>
      <description>arXiv:2410.09898v1 Announce Type: new 
Abstract: Current status censoring or case I interval censoring takes place when subjects in a study are observed just once to check if a particular event has occurred. If the event is recurring, the data are classified as current count data; if non-recurring, they are classified as current status data. Several instances of dependence of these recurring and non-recurring events are observable in epidemiology and pathology. Estimation of the degree of this dependence and identification of major risk factors for the events are the major objectives of such studies. The current study proposes a Bayesian method for the joint modelling of such related events, employing a shared frailty-based semiparametric regression model. Computational implementation makes use of an adaptive Metropolis-Hastings algorithm. Simulation studies are put into use to show the effectiveness of the method proposed and fracture-osteoporosis data are worked through to highlight its application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09898v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavithra Hariharan, P. G. Sankaran</dc:creator>
    </item>
    <item>
      <title>Sparse Multivariate Linear Regression with Strongly Associated Response Variables</title>
      <link>https://arxiv.org/abs/2410.10025</link>
      <description>arXiv:2410.10025v2 Announce Type: new 
Abstract: We propose new methods for multivariate linear regression when the regression coefficient matrix is sparse and the error covariance matrix is dense. We assume that the error covariance matrix has equicorrelation across the response variables. Two procedures are proposed: one is based on constant marginal response variance (compound symmetry), and the other is based on general varying marginal response variance. Two approximate procedures are also developed for high dimensions. We propose an approximation to the Gaussian validation likelihood for tuning parameter selection. Extensive numerical experiments illustrate when our procedures outperform relevant competitors as well as their robustness to model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10025v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daeyoung Ham, Bradley S. Price, Adam J. Rothman</dc:creator>
    </item>
    <item>
      <title>Effective Positive Cauchy Combination Test</title>
      <link>https://arxiv.org/abs/2410.10345</link>
      <description>arXiv:2410.10345v1 Announce Type: new 
Abstract: In the field of multiple hypothesis testing, combining p-values represents a fundamental statistical method. The Cauchy combination test (CCT) (Liu and Xie, 2020) excels among numerous methods for combining p-values with powerful and computationally efficient performance. However, large p-values may diminish the significance of testing, even extremely small p-values exist. We propose a novel approach named the positive Cauchy combination test (PCCT) to surmount this flaw. Building on the relationship between the PCCT and CCT methods, we obtain critical values by applying the Cauchy distribution to the PCCT statistic. We find, however, that the PCCT tends to be effective only when the significance level is substantially small or the test statistics are strongly correlated. Otherwise, it becomes challenging to control type I errors, a problem that also pertains to the CCT. Thanks to the theories of stable distributions and the generalized central limit theorem, we have demonstrated critical values under weak dependence, which effectively controls type I errors for any given significance level. For more general scenarios, we correct the test statistic using the generalized mean method, which can control the size under any dependence structure and cannot be further optimized.Our method exhibits excellent performance, as demonstrated through comprehensive simulation studies. We further validate the effectiveness of our proposed method by applying it to a genetic dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10345v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yanyan Ouyang, Xingwei Liu, Lixing Zhu, Wangli Xu</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric modeling of heterogeneous populations of networks</title>
      <link>https://arxiv.org/abs/2410.10354</link>
      <description>arXiv:2410.10354v1 Announce Type: new 
Abstract: The increasing availability of multiple network data has highlighted the need for statistical models for heterogeneous populations of networks. A convenient framework makes use of metrics to measure similarity between networks. In this context, we propose a novel Bayesian nonparametric model that identifies clusters of networks characterized by similar connectivity patterns. Our approach relies on a location-scale Dirichlet process mixture of centered Erd\H{o}s--R\'enyi kernels, with components parametrized by a unique network representative, or mode, and a univariate measure of dispersion around the mode. We demonstrate that this model has full support in the Kullback--Leibler sense and is strongly consistent. An efficient Markov chain Monte Carlo scheme is proposed for posterior inference and clustering of multiple network data. The performance of the model is validated through extensive simulation studies, showing improvements over state-of-the-art methods. Additionally, we present an effective strategy to extend the application of the proposed model to datasets with a large number of nodes. We illustrate our approach with the analysis of human brain network data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10354v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Barile, Sim\'on Lunag\'omez, Bernardo Nipoti</dc:creator>
    </item>
    <item>
      <title>Regression Model for Speckled Data with Extremely Variability</title>
      <link>https://arxiv.org/abs/2410.10482</link>
      <description>arXiv:2410.10482v1 Announce Type: new 
Abstract: Synthetic aperture radar (SAR) is an efficient and widely used remote sensing tool. However, data extracted from SAR images are contaminated with speckle, which precludes the application of techniques based on the assumption of additive and normally distributed noise. One of the most successful approaches to describing such data is the multiplicative model, where intensities can follow a variety of distributions with positive support. The $\mathcal{G}^0_I$ model is among the most successful ones. Although several estimation methods for the $\mathcal{G}^0_I$ parameters have been proposed, there is no work exploring a regression structure for this model. Such a structure could allow us to infer unobserved values from available ones. In this work, we propose a $\mathcal{G}^0_I$ regression model and use it to describe the influence of intensities from other polarimetric channels. We derive some theoretical properties for the new model: Fisher information matrix, residual measures, and influential tools. Maximum likelihood point and interval estimation methods are proposed and evaluated by Monte Carlo experiments. Results from simulated and actual data show that the new model can be helpful for SAR image analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10482v1</guid>
      <category>stat.ME</category>
      <category>eess.IV</category>
      <category>physics.data-an</category>
      <category>physics.ins-det</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.isprsjprs.2024.05.009</arxiv:DOI>
      <arxiv:journal_reference>Elsevier ISPRS Journal of Photogrammetry and Remote Sensing, Volume 213, July 2024, Pages 1-13</arxiv:journal_reference>
      <dc:creator>A. D. C. Nascimento, J. M. Vasconcelos, R. J. Cintra, A. C. Frery</dc:creator>
    </item>
    <item>
      <title>An Approximate Identity Link Function for Bayesian Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2410.10618</link>
      <description>arXiv:2410.10618v1 Announce Type: new 
Abstract: In this note, we consider using a link function that has heavier tails than the usual exponential link function. We construct efficient Gibbs algorithms for Poisson and Multinomial models based on this link function by introducing gamma and inverse Gaussian latent variables and show that the algorithms generate geometrically ergodic Markov chains in simple settings. Our algorithms can be used for more complicated models with many parameters. We fit our simple Poisson model to a real dataset and confirm that the posterior distribution has similar implications to those under the usual Poisson regression model based on the exponential link function. Although less interpretable, our models are potentially more tractable or flexible from a computational point of view in some cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10618v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Hamura</dc:creator>
    </item>
    <item>
      <title>Partially exchangeable stochastic block models for multilayer networks</title>
      <link>https://arxiv.org/abs/2410.10619</link>
      <description>arXiv:2410.10619v1 Announce Type: new 
Abstract: Multilayer networks generalize single-layered connectivity data in several directions. These generalizations include, among others, settings where multiple types of edges are observed among the same set of nodes (edge-colored networks) or where a single notion of connectivity is measured between nodes belonging to different pre-specified layers (node-colored networks). While progress has been made in statistical modeling of edge-colored networks, principled approaches that flexibly account for both within and across layer block-connectivity structures while incorporating layer information through a rigorous probabilistic construction are still lacking for node-colored multilayer networks. We fill this gap by introducing a novel class of partially exchangeable stochastic block models specified in terms of a hierarchical random partition prior for the allocation of nodes to groups, whose number is learned by the model. This goal is achieved without jeopardizing probabilistic coherence, uncertainty quantification and derivation of closed-form predictive within- and across-layer co-clustering probabilities. Our approach facilitates prior elicitation, the understanding of theoretical properties and the development of yet-unexplored predictive strategies for both the connections and the allocations of future incoming nodes. Posterior inference proceeds via a tractable collapsed Gibbs sampler, while performance is illustrated in simulations and in a real-world criminal network application. The notable gains achieved over competitors clarify the importance of developing general stochastic block models based on suitable node-exchangeability structures coherent with the type of multilayer network being analyzed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10619v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniele Durante, Francesco Gaffi, Antonio Lijoi, Igor Pr\"unster</dc:creator>
    </item>
    <item>
      <title>Missing data imputation using a truncated infinite factor model with application to metabolomics data</title>
      <link>https://arxiv.org/abs/2410.10633</link>
      <description>arXiv:2410.10633v1 Announce Type: new 
Abstract: In metabolomics, the study of small molecules in biological samples, data are often acquired through mass spectrometry. The resulting data contain highly correlated variables, typically with a larger number of variables than observations. Missing data are prevalent, and imputation is critical as data acquisition can be difficult and expensive, and many analysis methods necessitate complete data. In such data, missing at random (MAR) missingness occurs due to acquisition or processing error, while missing not at random (MNAR) missingness occurs when true values lie below the threshold for detection. Existing imputation methods generally assume one missingness type, or impute values outside the physical constraints of the data, which lack utility. A truncated factor analysis model with an infinite number of factors (tIFA) is proposed to facilitate imputation in metabolomics data, in a statistically and physically principled manner. Truncated distributional assumptions underpin tIFA, ensuring cognisance of the data's physical constraints when imputing. Further, tIFA allows for both MAR and MNAR missingness, and a Bayesian inferential approach provides uncertainty quantification for imputed values and missingness types. The infinite factor model parsimoniously models the high-dimensional, multicollinear data, with nonparametric shrinkage priors obviating the need for model selection tools to infer the number of latent factors. A simulation study is performed to assess the performance of tIFA and an application to a urinary metabolomics dataset results in a full dataset with practically useful imputed values, and associated uncertainty, ready for use in metabolomics analyses. Open-source R code accompanies tIFA, facilitating its widespread use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10633v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kate Finucane, Lorraine Brennan, Isobel Claire Gormley</dc:creator>
    </item>
    <item>
      <title>Statistically and computationally efficient conditional mean imputation for censored covariates</title>
      <link>https://arxiv.org/abs/2410.10723</link>
      <description>arXiv:2410.10723v1 Announce Type: new 
Abstract: Censored, missing, and error-prone covariates are all coarsened data types for which the true values are unknown. Many methods to handle the unobserved values, including imputation, are shared between these data types, with nuances based on the mechanism dominating the unobserved values and any other available information. For example, in prospective studies, the time to a specific disease diagnosis will be incompletely observed if only some patients are diagnosed by the end of the follow-up. Specifically, some times will be randomly right-censored, and patients' disease-free follow-up times must be incorporated into their imputed values. Assuming noninformative censoring, these censored values are replaced with their conditional means, the calculations of which require (i) estimating the conditional distribution of the censored covariate and (ii) integrating over the corresponding survival function. Semiparametric approaches are common, which estimate the distribution with a Cox proportional hazards model and then the integral with the trapezoidal rule. While these approaches offer robustness, they come at the cost of statistical and computational efficiency. We propose a general framework for parametric conditional mean imputation of censored covariates that offers better statistical precision and requires less computational strain by modeling the survival function parametrically, where conditional means often have an analytic solution. The framework is implemented in the open-source R package, speedyCMI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10723v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Ethan M. Alt</dc:creator>
    </item>
    <item>
      <title>Peer effects in the linear-in-means model may be inestimable even when identified</title>
      <link>https://arxiv.org/abs/2410.10772</link>
      <description>arXiv:2410.10772v1 Announce Type: new 
Abstract: Linear-in-means models are widely used to investigate peer effects. Identifying peer effects in these models is challenging, but conditions for identification are well-known. However, even when peer effects are identified, they may not be estimable, due to an asymptotic colinearity issue: as sample size increases, peer effects become more and more linearly dependent. We show that asymptotic colinearity occurs whenever nodal covariates are independent of the network and the minimum degree of the network is growing. Asymptotic colinearity can cause estimators to be inconsistent or to converge at slower than expected rates. We also demonstrate that dependence between nodal covariates and network structure can alleviate colinearity issues in random dot product graphs. These results suggest that linear-in-means models are less reliable for studying peer influence than previously believed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10772v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Hayes, Keith Levin</dc:creator>
    </item>
    <item>
      <title>Fast Data-independent KLT Approximations Based on Integer Functions</title>
      <link>https://arxiv.org/abs/2410.09227</link>
      <description>arXiv:2410.09227v1 Announce Type: cross 
Abstract: The Karhunen-Lo\`eve transform (KLT) stands as a well-established discrete transform, demonstrating optimal characteristics in data decorrelation and dimensionality reduction. Its ability to condense energy compression into a select few main components has rendered it instrumental in various applications within image compression frameworks. However, computing the KLT depends on the covariance matrix of the input data, which makes it difficult to develop fast algorithms for its implementation. Approximations for the KLT, utilizing specific rounding functions, have been introduced to reduce its computational complexity. Therefore, our paper introduces a category of low-complexity, data-independent KLT approximations, employing a range of round-off functions. The design methodology of the approximate transform is defined for any block-length $N$, but emphasis is given to transforms of $N = 8$ due to its wide use in image and video compression. The proposed transforms perform well when compared to the exact KLT and approximations considering classical performance measures. For particular scenarios, our proposed transforms demonstrated superior performance when compared to KLT approximations documented in the literature. We also developed fast algorithms for the proposed transforms, further reducing the arithmetic cost associated with their implementation. Evaluation of field programmable gate array (FPGA) hardware implementation metrics was conducted. Practical applications in image encoding showed the relevance of the proposed transforms. In fact, we showed that one of the proposed transforms outperformed the exact KLT given certain compression ratios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09227v1</guid>
      <category>eess.SP</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>eess.IV</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11042-024-18159-2</arxiv:DOI>
      <arxiv:journal_reference>Multimedia Tools and Applications, 83(26):67303--67325, January 2024</arxiv:journal_reference>
      <dc:creator>A. P. Rad\"unz, D. F. G. Coelho, F. M. Bayer, R. J. Cintra, A. Madanayake</dc:creator>
    </item>
    <item>
      <title>Knockoffs for exchangeable categorical covariates</title>
      <link>https://arxiv.org/abs/2410.09835</link>
      <description>arXiv:2410.09835v1 Announce Type: cross 
Abstract: Let $X=(X_1,\ldots,X_p)$ be a $p$-variate random vector and $F$ a fixed finite set. In a number of applications, mainly in genetics, it turns out that $X_i\in F$ for each $i=1,\ldots,p$. Despite the latter fact, to obtain a knockoff $\widetilde{X}$ (in the sense of \cite{CFJL18}), $X$ is usually modeled as an absolutely continuous random vector. While comprehensible from the point of view of applications, this approximate procedure does not make sense theoretically, since $X$ is supported by the finite set $F^p$. In this paper, explicit formulae for the joint distribution of $(X,\widetilde{X})$ are provided when $P(X\in F^p)=1$ and $X$ is exchangeable or partially exchangeable. In fact, when $X_i\in F$ for all $i$, there seem to be various reasons for assuming $X$ exchangeable or partially exchangeable. The robustness of $\widetilde{X}$, with respect to the de Finetti's measure $\pi$ of $X$, is investigated as well. Let $\mathcal{L}_\pi(\widetilde{X}\mid X=x)$ denote the conditional distribution of $\widetilde{X}$, given $X=x$, when the de Finetti's measure is $\pi$. It is shown that $$\norm{\mathcal{L}_{\pi_1}(\widetilde{X}\mid X=x)-\mathcal{L}_{\pi_2}(\widetilde{X}\mid X=x)}\le c(x)\,\norm{\pi_1-\pi_2}$$ where $\norm{\cdot}$ is total variation distance and $c(x)$ a suitable constant. Finally, a numerical experiment is performed. Overall, the knockoffs of this paper outperform the alternatives (i.e., the knockoffs obtained by giving $X$ an absolutely continuous distribution) as regards the false discovery rate but are slightly weaker in terms of power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09835v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Emanuela Dreassi, Luca Pratelli, Pietro Rigo</dc:creator>
    </item>
    <item>
      <title>Exact MCMC for Intractable Proposals</title>
      <link>https://arxiv.org/abs/2410.10282</link>
      <description>arXiv:2410.10282v1 Announce Type: cross 
Abstract: Accept-reject based Markov chain Monte Carlo (MCMC) methods are the workhorse algorithm for Bayesian inference. These algorithms, like Metropolis-Hastings, require the choice of a proposal distribution which is typically informed by the desired target distribution. Surprisingly, proposal distributions with unknown normalizing constants are not uncommon, even though for such a choice of a proposal, the Metropolis-Hastings acceptance ratio cannot be evaluated exactly. Across the literature, authors resort to approximation methods that yield inexact MCMC or develop specialized algorithms to combat this problem. We show how Bernoulli factory MCMC algorithms, originally proposed for doubly intractable target distributions, can quite naturally be adapted to this situation. We present three diverse and relevant examples demonstrating the usefulness of the Bernoulli factory approach to this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10282v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dwija Kakkad, Dootika Vats</dc:creator>
    </item>
    <item>
      <title>Optimal lower bounds for logistic log-likelihoods</title>
      <link>https://arxiv.org/abs/2410.10309</link>
      <description>arXiv:2410.10309v1 Announce Type: cross 
Abstract: The logit transform is arguably the most widely-employed link function beyond linear settings. This transformation routinely appears in regression models for binary data and provides, either explicitly or implicitly, a core building-block within state-of-the-art methodologies for both classification and regression. Its widespread use, combined with the lack of analytical solutions for the optimization of general losses involving the logit transform, still motivates active research in computational statistics. Among the directions explored, a central one has focused on the design of tangent lower bounds for logistic log-likelihoods that can be tractably optimized, while providing a tight approximation of these log-likelihoods. Although progress along these lines has led to the development of effective minorize-maximize (MM) algorithms for point estimation and coordinate ascent variational inference schemes for approximate Bayesian inference under several logit models, the overarching focus in the literature has been on tangent quadratic minorizers. In fact, it is still unclear whether tangent lower bounds sharper than quadratic ones can be derived without undermining the tractability of the resulting minorizer. This article addresses such a challenging question through the design and study of a novel piece-wise quadratic lower bound that uniformly improves any tangent quadratic minorizer, including the sharpest ones, while admitting a direct interpretation in terms of the classical generalized lasso problem. As illustrated in a ridge logistic regression, this unique connection facilitates more effective implementations than those provided by available piece-wise bounds, while improving the convergence speed of quadratic ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10309v1</guid>
      <category>stat.ML</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niccol\`o Anceschi, Tommaso Rigon, Giacomo Zanella, Daniele Durante</dc:creator>
    </item>
    <item>
      <title>Data-Driven Approaches for Modelling Target Behaviour</title>
      <link>https://arxiv.org/abs/2410.10538</link>
      <description>arXiv:2410.10538v1 Announce Type: cross 
Abstract: The performance of tracking algorithms strongly depends on the chosen model assumptions regarding the target dynamics. If there is a strong mismatch between the chosen model and the true object motion, the track quality may be poor or the track is easily lost. Still, the true dynamics might not be known a priori or it is too complex to be expressed in a tractable mathematical formulation. This paper provides a comparative study between three different methods that use machine learning to describe the underlying object motion based on training data. The first method builds on Gaussian Processes (GPs) for predicting the object motion, the second learns the parameters of an Interacting Multiple Model (IMM) filter and the third uses a Long Short-Term Memory (LSTM) network as a motion model. All methods are compared against an Extended Kalman Filter (EKF) with an analytic motion model as a benchmark and their respective strengths are highlighted in one simulated and two real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10538v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isabel Schlangen, Andr\'e Brandenburger, Mengwei Sun, James R. Hopgood</dc:creator>
    </item>
    <item>
      <title>Echo State Networks for Spatio-Temporal Area-Level Data</title>
      <link>https://arxiv.org/abs/2410.10641</link>
      <description>arXiv:2410.10641v1 Announce Type: cross 
Abstract: Spatio-temporal area-level datasets play a critical role in official statistics, providing valuable insights for policy-making and regional planning. Accurate modeling and forecasting of these datasets can be extremely useful for policymakers to develop informed strategies for future planning. Echo State Networks (ESNs) are efficient methods for capturing nonlinear temporal dynamics and generating forecasts. However, ESNs lack a direct mechanism to account for the neighborhood structure inherent in area-level data. Ignoring these spatial relationships can significantly compromise the accuracy and utility of forecasts. In this paper, we incorporate approximate graph spectral filters at the input stage of the ESN, thereby improving forecast accuracy while preserving the model's computational efficiency during training. We demonstrate the effectiveness of our approach using Eurostat's tourism occupancy dataset and show how it can support more informed decision-making in policy and planning contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10641v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenhua Wang, Scott H. Holan, Christopher K. Wikle</dc:creator>
    </item>
    <item>
      <title>Vecchia Gaussian Processes: Probabilistic Properties, Minimax Rates and Methodological Developments</title>
      <link>https://arxiv.org/abs/2410.10649</link>
      <description>arXiv:2410.10649v1 Announce Type: cross 
Abstract: Gaussian Processes (GPs) are widely used to model dependency in spatial statistics and machine learning, yet the exact computation suffers an intractable time complexity of $O(n^3)$. Vecchia approximation allows scalable Bayesian inference of GPs in $O(n)$ time by introducing sparsity in the spatial dependency structure that is characterized by a directed acyclic graph (DAG). Despite the popularity in practice, it is still unclear how to choose the DAG structure and there are still no theoretical guarantees in nonparametric settings. In this paper, we systematically study the Vecchia GPs as standalone stochastic processes and uncover important probabilistic properties and statistical results in methodology and theory. For probabilistic properties, we prove that the conditional distributions of the Mat\'{e}rn GPs, as well as the Vecchia approximations of the Mat\'{e}rn GPs, can be characterized by polynomials. This allows us to prove a series of results regarding the small ball probabilities and RKHSs of Vecchia GPs. For statistical methodology, we provide a principled guideline to choose parent sets as norming sets with fixed cardinality and provide detailed algorithms following such guidelines. For statistical theory, we prove posterior contraction rates for applying Vecchia GPs to regression problems, where minimax optimality is achieved by optimally tuned GPs via either oracle rescaling or hierarchical Bayesian methods. Our theory and methodology are demonstrated with numerical studies, where we also provide efficient implementation of our methods in C++ with R interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10649v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Botond Szabo, Yichen Zhu</dc:creator>
    </item>
    <item>
      <title>Estimation beyond Missing (Completely) at Random</title>
      <link>https://arxiv.org/abs/2410.10704</link>
      <description>arXiv:2410.10704v1 Announce Type: cross 
Abstract: We study the effects of missingness on the estimation of population parameters. Moving beyond restrictive missing completely at random (MCAR) assumptions, we first formulate a missing data analogue of Huber's arbitrary $\epsilon$-contamination model. For mean estimation with respect to squared Euclidean error loss, we show that the minimax quantiles decompose as a sum of the corresponding minimax quantiles under a heterogeneous, MCAR assumption, and a robust error term, depending on $\epsilon$, that reflects the additional error incurred by departure from MCAR.
  We next introduce natural classes of realisable $\epsilon$-contamination models, where an MCAR version of a base distribution $P$ is contaminated by an arbitrary missing not at random (MNAR) version of $P$. These classes are rich enough to capture various notions of biased sampling and sensitivity conditions, yet we show that they enjoy improved minimax performance relative to our earlier arbitrary contamination classes for both parametric and nonparametric classes of base distributions. For instance, with a univariate Gaussian base distribution, consistent mean estimation over realisable $\epsilon$-contamination classes is possible even when $\epsilon$ and the proportion of missingness converge (slowly) to 1. Finally, we extend our results to the setting of departures from missing at random (MAR) in normal linear regression with a realisable missing response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10704v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Ma, Kabir A. Verchand, Thomas B. Berrett, Tengyao Wang, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>Meta-analysis of Bayesian analyses</title>
      <link>https://arxiv.org/abs/1904.04484</link>
      <description>arXiv:1904.04484v2 Announce Type: replace 
Abstract: Meta-analysis aims to generalize results from multiple related statistical analyses through a combined analysis. While the natural outcome of a Bayesian study is a posterior distribution, traditional Bayesian meta-analyses proceed by combining summary statistics (i.e., point-valued estimates) computed from data. In this paper, we develop a framework for combining posterior distributions from multiple related Bayesian studies into a meta-analysis. Importantly, the method is capable of reusing pre-computed posteriors from computationally costly analyses, without needing the implementation details from each study. Besides providing a consensus across studies, the method enables updating the local posteriors post-hoc and therefore refining them by sharing statistical strength between the studies, without rerunning the original analyses. We illustrate the wide applicability of the framework by combining results from likelihood-free Bayesian analyses, which would be difficult to carry out using standard methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:1904.04484v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/24-BA1465</arxiv:DOI>
      <dc:creator>Paul Blomstedt, Diego Mesquita, Omar Rivasplata, Jarno Lintusaari, Tuomas Sivula, Jukka Corander, Samuel Kaski</dc:creator>
    </item>
    <item>
      <title>Multivariate Tie-breaker Designs</title>
      <link>https://arxiv.org/abs/2202.10030</link>
      <description>arXiv:2202.10030v5 Announce Type: replace 
Abstract: In a tie-breaker design (TBD), subjects with high values of a running variable are given some (usually desirable) treatment, subjects with low values are not, and subjects in the middle are randomized. TBDs are intermediate between regression discontinuity designs (RDDs) and randomized controlled trials (RCTs). TBDs allow a tradeoff between the resource allocation efficiency of an RDD and the statistical efficiency of an RCT. We study a model where the expected response is one multivariate regression for treated subjects and another for control subjects. We propose a prospective D-optimality, analogous to Bayesian optimal design, to understand design tradeoffs without reference to a specific data set. For given covariates, we show how to use convex optimization to choose treatment probabilities that optimize this criterion. We can incorporate a variety of constraints motivated by economic and ethical considerations. In our model, D-optimality for the treatment effect coincides with D-optimality for the whole regression, and, without constraints, an RCT is globally optimal. We show that a monotonicity constraint favoring more deserving subjects induces sparsity in the number of distinct treatment probabilities. We apply the convex optimization solution to a semi-synthetic example involving triage data from the MIMIC-IV-ED database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.10030v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim P. Morrison, Art B. Owen</dc:creator>
    </item>
    <item>
      <title>Parameter Estimation in Nonlinear Multivariate Stochastic Differential Equations Based on Splitting Schemes</title>
      <link>https://arxiv.org/abs/2211.11884</link>
      <description>arXiv:2211.11884v3 Announce Type: replace 
Abstract: The likelihood functions for discretely observed nonlinear continuous-time models based on stochastic differential equations are not available except for a few cases. Various parameter estimation techniques have been proposed, each with advantages, disadvantages, and limitations depending on the application. Most applications still use the Euler-Maruyama discretization, despite many proofs of its bias. More sophisticated methods, such as Kessler's Gaussian approximation, Ozaki's Local Linearization, A\"it-Sahalia's Hermite expansions, or MCMC methods, might be complex to implement, do not scale well with increasing model dimension, or can be numerically unstable. We propose two efficient and easy-to-implement likelihood-based estimators based on the Lie-Trotter (LT) and the Strang (S) splitting schemes. We prove that S has $L^p$ convergence rate of order 1, a property already known for LT. We show that the estimators are consistent and asymptotically efficient under the less restrictive one-sided Lipschitz assumption. A numerical study on the 3-dimensional stochastic Lorenz system complements our theoretical findings. The simulation shows that the S estimator performs the best when measured on precision and computational speed compared to the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11884v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOS2371</arxiv:DOI>
      <arxiv:journal_reference>The Annals of Statistics (2024), Vol. 52, No. 2, 842 - 867</arxiv:journal_reference>
      <dc:creator>Predrag Pilipovic, Adeline Samson, Susanne Ditlevsen</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for partial orders from random linear extensions: power relations from 12th Century Royal Acta</title>
      <link>https://arxiv.org/abs/2212.05524</link>
      <description>arXiv:2212.05524v3 Announce Type: replace 
Abstract: In the eleventh and twelfth centuries in England, Wales and Normandy, Royal Acta were legal documents in which witnesses were listed in order of social status. Any bishops present were listed as a group. For our purposes, each witness-list is an ordered permutation of bishop names with a known date or date-range. Changes over time in the order bishops are listed may reflect changes in their authority. Historians would like to detect and quantify these changes. There is no reason to assume that the underlying social order which constrains bishop-order within lists is a complete order. We therefore model the evolving social order as an evolving partial ordered set or {\it poset}.
  We construct a Hidden Markov Model for these data. The hidden state is an evolving poset (the evolving social hierarchy) and the emitted data are random total orders (dated lists) respecting the poset present at the time the order was observed. This generalises existing models for rank-order data such as Mallows and Plackett-Luce. We account for noise via a random ``queue-jumping'' process. Our latent-variable prior for the random process of posets is marginally consistent. A parameter controls poset depth and actor-covariates inform the position of actors in the hierarchy. We fit the model, estimate posets and find evidence for changes in status over time. We interpret our results in terms of court politics. Simpler models, based on Bucket Orders and vertex-series-parallel orders, are rejected. We compare our results with a time-series extension of the Plackett-Luce model. Our software is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05524v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geoff K. Nicholls, Jeong Eun Lee, Nicholas Karn, David Johnson, Rukuang Huang, Alexis Muir-Watt</dc:creator>
    </item>
    <item>
      <title>Tensor Formulation of the General Linear Model with Einstein Notation</title>
      <link>https://arxiv.org/abs/2301.03799</link>
      <description>arXiv:2301.03799v2 Announce Type: replace 
Abstract: The general linear model is a universally accepted method to conduct and test multiple linear regression models. Using this model one has the ability to simultaneously regress covariates among different groups of data. Moreover, there are hundreds of applications and statistical tests associated with the general linear model. However, the conventional matrix formulation is relatively inelegant which yields multiple difficulties including slow computation speed due to a large number of computations, increased memory usage due to needlessly large data structures, and organizational inconsistency. This is due to the fundamental incongruence between the degrees of freedom of the information the data structures in the conventional formulation of the general linear model are intended to represent and the rank of the data structures themselves. Here, I briefly suggest an elegant reformulation of the general linear model which involves the use of tensors and multidimensional arrays as opposed to exclusively flat structures in the conventional formulation. To demonstrate the efficacy of this approach I translate a few common applications of the general linear model from the conventional formulation to the tensor formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.03799v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.33552/ABBA.2023.05.000617</arxiv:DOI>
      <arxiv:journal_reference>Annal Biostat &amp; Biomed Appli. 5(4): 2023. ABBA.MS.ID.000617</arxiv:journal_reference>
      <dc:creator>Gavin T. Kress</dc:creator>
    </item>
    <item>
      <title>A stochastic network approach to clustering and visualising single-cell genomic count data</title>
      <link>https://arxiv.org/abs/2303.02498</link>
      <description>arXiv:2303.02498v5 Announce Type: replace 
Abstract: Important tasks in the study of genomic data include the identification of groups of similar cells (for example by clustering), and visualisation of data summaries (for example by dimensional reduction). In this paper, we develop a novel approach to these tasks in the context of single-cell genomic data. To do so, we propose to model the observed genomic data count matrix $\mathbf{X}\in\mathbb{Z}_{\geq0}^{p\times n}$, by representing these measurements as a bipartite network with multi-edges. Utilising this first-principles network model of the raw data, we cluster single cells in a suitably identified $d$-dimensional Laplacian Eigenspace (LE) via a Gaussian mixture model (GMM-LE), and employ UMAP to non-linearly project the LE to two dimensions for visualisation (UMAP-LE). This LE representation of the data-points estimates transformed latent positions (of genes and cells), under a latent position statistical model of nodes in a bipartite stochastic network. We demonstrate how transformations of these estimated latent positions can enable fine-grained clustering and visualisation of single-cell genomic data, by application to data from three recent genomics studies in different biological contexts. In each data application, clusters of cells independently learned by our proposed methodology are found to correspond to cells expressing specific marker genes that were independently defined by domain experts. In this validation setting, our proposed clustering methodology outperforms the industry-standard for these data. Furthermore, we validate components of the LE decomposition of the data by contrasting healthy cells from normal and at-risk groups in a machine-learning model, thereby identifying an LE cancer biomarker that significantly predicts long-term patient survival outcome in two independent validation cohorts with data from 1904 and 1091 individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.02498v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Thomas E. Bartlett, Swati Chandna, Sandipan Roy</dc:creator>
    </item>
    <item>
      <title>Instrumental Variable Approach to Estimating Individual Causal Effects in N-of-1 Trials: Application to ISTOP Study</title>
      <link>https://arxiv.org/abs/2306.14019</link>
      <description>arXiv:2306.14019v3 Announce Type: replace 
Abstract: An N-of-1 trial is a multiple crossover trial conducted in a single individual to provide evidence to directly inform personalized treatment decisions. Advancements in wearable devices greatly improved the feasibility of adopting these trials to identify optimal individual treatment plans, particularly when treatments differ among individuals and responses are highly heterogeneous. Our work was motivated by the I-STOP-AFib Study, which examined the impact of different triggers on atrial fibrillation (AF) occurrence. We described a causal framework for 'N-of-1' trial using potential treatment selection paths and potential outcome paths. Two estimands of individual causal effect were defined:(a) the effect of continuous exposure, and (b) the effect of an individual observed behavior. We addressed three challenges: (a) imperfect compliance to the randomized treatment assignment; (b) binary treatments and binary outcomes which led to the 'non-collapsibility' issue of estimating odds ratios; and (c) serial inference in the longitudinal observations. We adopted the Bayesian IV approach where the study randomization was the IV as it impacted the choice of exposure of a subject but not directly the outcome. Estimations were through a system of two parametric Bayesian models to estimate the individual causal effect. Our model got around the non-collapsibility and non-consistency by modeling the confounding mechanism through latent structural models and by inferring with Bayesian posterior of functionals. Autocorrelation present in the repeated measurements was also accounted for. The simulation study showed our method largely reduced bias and greatly improved the coverage of the estimated causal effect, compared to existing methods (ITT, PP, and AT). We applied the method to I-STOP-AFib Study to estimate the individual effect of alcohol on AF occurrence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14019v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kexin Qu, Christopher H. Schmid, Tao Liu</dc:creator>
    </item>
    <item>
      <title>From isotonic to Lipschitz regression: a new interpolative perspective on shape-restricted estimation</title>
      <link>https://arxiv.org/abs/2307.05732</link>
      <description>arXiv:2307.05732v4 Announce Type: replace 
Abstract: This manuscript seeks to bridge two seemingly disjoint paradigms of nonparametric regression: estimation based on smoothness assumptions and shape constraints. The proposed approach is motivated by a conceptually simple observation: Every Lipschitz function is a sum of monotonic and linear functions. This principle is further generalized to the higher-order monotonicity and multivariate covariates. A family of estimators is proposed based on a sample-splitting procedure, which inherits desirable methodological, theoretical, and computational properties of shape-restricted estimators. The theoretical analysis provides convergence guarantees of the estimator under heteroscedastic and heavy-tailed errors, as well as adaptivity properties to the unknown complexity of the true regression function. The generality of the proposed decomposition framework is demonstrated through new approximation results, and extensive numerical studies validate the theoretical properties of the proposed estimation framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05732v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenta Takatsu, Tianyu Zhang, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Simultaneous inference for generalized linear models with unmeasured confounders</title>
      <link>https://arxiv.org/abs/2309.07261</link>
      <description>arXiv:2309.07261v4 Announce Type: replace 
Abstract: Tens of thousands of simultaneous hypothesis tests are routinely performed in genomic studies to identify differentially expressed genes. However, due to unmeasured confounders, many standard statistical approaches may be substantially biased. This paper investigates the large-scale hypothesis testing problem for multivariate generalized linear models in the presence of confounding effects. Under arbitrary confounding mechanisms, we propose a unified statistical estimation and inference framework that harnesses orthogonal structures and integrates linear projections into three key stages. It begins by disentangling marginal and uncorrelated confounding effects to recover the latent coefficients. Subsequently, latent factors and primary effects are jointly estimated through lasso-type optimization. Finally, we incorporate projected and weighted bias-correction steps for hypothesis testing. Theoretically, we establish the identification conditions of various effects and non-asymptotic error bounds. We show effective Type-I error control of asymptotic $z$-tests as sample and response sizes approach infinity. Numerical experiments demonstrate that the proposed method controls the false discovery rate by the Benjamini-Hochberg procedure and is more powerful than alternative methods. By comparing single-cell RNA-seq counts from two groups of samples, we demonstrate the suitability of adjusting confounding effects when significant covariates are absent from the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07261v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Larry Wasserman, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>A Unified Bayesian Framework for Modeling Measurement Error in Multinomial Data</title>
      <link>https://arxiv.org/abs/2310.09345</link>
      <description>arXiv:2310.09345v2 Announce Type: replace 
Abstract: Measurement error in multinomial data is a well-known and well-studied inferential problem that is encountered in many fields, including engineering, biomedical and omics research, ecology, finance, official statistics, and social sciences. Methods developed to accommodate measurement error in multinomial data are typically equipped to handle false negatives or false positives, but not both. We provide a unified framework for accommodating both forms of measurement error using a Bayesian hierarchical approach. We demonstrate the proposed method's performance on simulated data and apply it to acoustic bat monitoring and official crime data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09345v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew D. Koslovsky, Andee Kaplan, Victoria A. Terranova, Mevin B. Hooten</dc:creator>
    </item>
    <item>
      <title>Partial identification and unmeasured confounding with multiple treatments and multiple outcomes</title>
      <link>https://arxiv.org/abs/2311.12252</link>
      <description>arXiv:2311.12252v2 Announce Type: replace 
Abstract: In this work, we develop a framework for partial identification of causal effects in settings with multiple treatments and multiple outcomes. We highlight several advantages of jointly analyzing causal effects across multiple estimands under a "factor confounding assumption" where residual dependence amongst treatments and outcomes is assumed to be driven by unmeasured confounding. In this setting, we show that joint partial identification regions for multiple estimands can be more informative than considering partial identification for individual estimands one at a time. We additionally show how assumptions related to the strength of confounding or the magnitude of plausible effect sizes for any one estimand can reduce the partial identification regions for other estimands. As a special case of this result, we explore how negative control assumptions reduce partial identification regions and discuss conditions under which point identification can be obtained. We develop novel computational approaches to finding partial identification regions under a variety of these assumptions. Lastly, we demonstrate our approach in an analysis of the causal effects of multiple air pollutants on several health outcomes in the United States using claims data from Medicare, where we find that some exposures have effects that are robust to the presence of unmeasured confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12252v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suyeon Kang, Alexander Franks, Michelle Audirac, Danielle Braun, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Bayesian regression discontinuity design with unknown cutoff</title>
      <link>https://arxiv.org/abs/2406.11585</link>
      <description>arXiv:2406.11585v2 Announce Type: replace 
Abstract: The regression discontinuity design (RDD) is a quasi-experimental approach used to estimate the causal effects of an intervention assigned based on a cutoff criterion. RDD exploits the idea that close to the cutoff units below and above are similar; hence, they can be meaningfully compared. Consequently, the causal effect can be estimated only locally at the cutoff point. This makes the cutoff point an essential element of RDD. However, especially in medical applications, the exact cutoff location may not always be disclosed to the researcher, and even when it is, the actual location may deviate from the official one. As we illustrate on the application of RDD to the HIV treatment eligibility data, estimating the causal effect at an incorrect cutoff point leads to meaningless results. The method we present, LoTTA (Local Trimmed Taylor Approximation), can be applied both as an estimation and validation tool in RDD. We use a Bayesian approach to incorporate prior knowledge and uncertainty about the cutoff location in the causal effect estimation. At the same time, LoTTA is fitted globally to the whole data, whereas RDD is a local, boundary point estimation problem. In this work we address a natural question that arises: how to make Bayesian inference more local to render a meaningful and powerful estimate of the treatment effect?</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11585v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Kowalska, Mark van de Wiel, St\'ephanie van der Pas</dc:creator>
    </item>
    <item>
      <title>Locally Adaptive Random Walk Stochastic Volatility</title>
      <link>https://arxiv.org/abs/2408.11315</link>
      <description>arXiv:2408.11315v2 Announce Type: replace 
Abstract: We introduce a novel Bayesian framework for estimating time-varying volatility by extending the Random Walk Stochastic Volatility (RWSV) model with a new Dynamic Shrinkage Process (DSP) in (log) variances. Unlike classical Stochastic Volatility or GARCH-type models with restrictive parametric stationarity assumptions, our proposed Adaptive Stochastic Volatility (ASV) model provides smooth yet dynamically adaptive estimates of evolving volatility and its uncertainty (vol of vol). We derive the theoretical properties of the proposed global-local shrinkage prior. Through simulation studies, we demonstrate that ASV exhibits remarkable misspecification resilience with low prediction error across various data generating scenarios in simulation. Furthermore, ASV's capacity to yield locally smooth and interpretable estimates facilitates a clearer understanding of underlying patterns and trends in volatility. Additionally, we propose and illustrate an extension for Bayesian Trend Filtering simultaneously in both mean and variance. Finally, we show that this attribute makes ASV a robust tool applicable across a wide range of disciplines, including in finance, environmental science, epidemiology, and medicine, among others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11315v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason B. Cho, David S. Matteson</dc:creator>
    </item>
    <item>
      <title>Supervised low-rank approximation of high-dimensional multivariate functional data via tensor decomposition</title>
      <link>https://arxiv.org/abs/2409.13819</link>
      <description>arXiv:2409.13819v2 Announce Type: replace 
Abstract: Motivated by the challenges of analyzing high-dimensional ($p \gg n$) sequencing data from longitudinal microbiome studies, where samples are collected at multiple time points from each subject, we propose supervised functional tensor singular value decomposition (SupFTSVD), a novel dimensionality reduction method that leverages auxiliary information in the dimensionality reduction of high-dimensional functional tensors. Although multivariate functional principal component analysis is a natural choice for dimensionality reduction of multivariate functional data, it becomes computationally burdensome in high-dimensional settings. Low-rank tensor decomposition is a feasible alternative and has gained popularity in recent literature, but existing methods in this realm are often incapable of simultaneously utilizing the temporal structure of the data and subject-level auxiliary information. SupFTSVD overcomes these limitations by generating low-rank representations of high-dimensional functional tensors while incorporating subject-level auxiliary information and accounting for the functional nature of the data. Moreover, SupFTSVD produces low-dimensional representations of subjects, features, and time, as well as subject-specific trajectories, providing valuable insights into the biological significance of variations within the data. In simulation studies, we demonstrate that our method achieves notable improvement in tensor approximation accuracy and loading estimation by utilizing auxiliary information. Finally, we applied SupFTSVD to two longitudinal microbiome studies where biologically meaningful patterns in the data were revealed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13819v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammad Samsul Alam (Department of Biostatistics &amp; Bioinformatics, Duke University), Ana-Maria Staicu (Department of Statistics, North Carolina State University), Pixu Shi (Department of Biostatistics &amp; Bioinformatics, Duke University)</dc:creator>
    </item>
    <item>
      <title>BRcal: An R Package to Boldness-Recalibrate Probability Predictions</title>
      <link>https://arxiv.org/abs/2409.13858</link>
      <description>arXiv:2409.13858v2 Announce Type: replace 
Abstract: When probability predictions are too cautious for decision making, boldness-recalibration enables responsible emboldening while maintaining the probability of calibration required by the user. We introduce BRcal, an R package implementing boldness-recalibration and supporting methodology as recently proposed. The BRcal package provides direct control of the calibration-boldness tradeoff and visualizes how different calibration levels change individual predictions. We describe the implementation details in BRcal related to non-linear optimization of boldness with a non-linear inequality constraint on calibration. Package functionality is demonstrated via a real world case study involving housing foreclosure predictions. The BRcal package is available on the Comprehensive R Archive Network (CRAN) (https://cran.r-project.org/web/packages/BRcal/index.html) and on Github (https://github.com/apguthrie/BRcal).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13858v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adeline P. Guthrie, Christopher T. Franck</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification in Synthetic Controls with Staggered Treatment Adoption</title>
      <link>https://arxiv.org/abs/2210.05026</link>
      <description>arXiv:2210.05026v4 Announce Type: replace-cross 
Abstract: We propose principled prediction intervals to quantify the uncertainty of a large class of synthetic control predictions (or estimators) in settings with staggered treatment adoption, offering precise non-asymptotic coverage probability guarantees. From a methodological perspective, we provide a detailed discussion of different causal quantities to be predicted, which we call causal predictands, allowing for multiple treated units with treatment adoption at possibly different points in time. From a theoretical perspective, our uncertainty quantification methods improve on prior literature by (i) covering a large class of causal predictands in staggered adoption settings, (ii) allowing for synthetic control methods with possibly nonlinear constraints, (iii) proposing scalable robust conic optimization methods and principled data-driven tuning parameter selection, and (iv) offering valid uniform inference across post-treatment periods. We illustrate our methodology with an empirical application studying the effects of economic liberalization on real GDP per capita for Sub-Saharan African countries. Companion general-purpose software packages are provided in Python, R, and Stata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.05026v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Yingjie Feng, Filippo Palomba, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>Deflated HeteroPCA: Overcoming the curse of ill-conditioning in heteroskedastic PCA</title>
      <link>https://arxiv.org/abs/2303.06198</link>
      <description>arXiv:2303.06198v2 Announce Type: replace-cross 
Abstract: This paper is concerned with estimating the column subspace of a low-rank matrix $\boldsymbol{X}^\star \in \mathbb{R}^{n_1\times n_2}$ from contaminated data. How to obtain optimal statistical accuracy while accommodating the widest range of signal-to-noise ratios (SNRs) becomes particularly challenging in the presence of heteroskedastic noise and unbalanced dimensionality (i.e., $n_2\gg n_1$). While the state-of-the-art algorithm $\textsf{HeteroPCA}$ emerges as a powerful solution for solving this problem, it suffers from "the curse of ill-conditioning," namely, its performance degrades as the condition number of $\boldsymbol{X}^\star$ grows. In order to overcome this critical issue without compromising the range of allowable SNRs, we propose a novel algorithm, called $\textsf{Deflated-HeteroPCA}$, that achieves near-optimal and condition-number-free theoretical guarantees in terms of both $\ell_2$ and $\ell_{2,\infty}$ statistical accuracy. The proposed algorithm divides the spectrum of $\boldsymbol{X}^\star$ into well-conditioned and mutually well-separated subblocks, and applies $\textsf{HeteroPCA}$ to conquer each subblock successively. Further, an application of our algorithm and theory to two canonical examples -- the factor model and tensor PCA -- leads to remarkable improvement for each application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.06198v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Zhou, Yuxin Chen</dc:creator>
    </item>
    <item>
      <title>Bridging the gap: Towards an Expanded Toolkit for AI-driven Decision-Making in the Public Sector</title>
      <link>https://arxiv.org/abs/2310.19091</link>
      <description>arXiv:2310.19091v3 Announce Type: replace-cross 
Abstract: AI-driven decision-making systems are becoming instrumental in the public sector, with applications spanning areas like criminal justice, social welfare, financial fraud detection, and public health. While these systems offer great potential benefits to institutional decision-making processes, such as improved efficiency and reliability, these systems face the challenge of aligning machine learning (ML) models with the complex realities of public sector decision-making. In this paper, we examine five key challenges where misalignment can occur, including distribution shifts, label bias, the influence of past decision-making on the data side, as well as competing objectives and human-in-the-loop on the model output side. Our findings suggest that standard ML methods often rely on assumptions that do not fully account for these complexities, potentially leading to unreliable and harmful predictions. To address this, we propose a shift in modeling efforts from focusing solely on predictive accuracy to improving decision-making outcomes. We offer guidance for selecting appropriate modeling frameworks, including counterfactual prediction and policy learning, by considering how the model estimand connects to the decision-maker's utility. Additionally, we outline technical methods that address specific challenges within each modeling approach. Finally, we argue for the importance of external input from domain experts and stakeholders to ensure that model assumptions and design choices align with real-world policy objectives, taking a step towards harmonizing AI and public sector objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19091v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.giq.2024.101976</arxiv:DOI>
      <arxiv:journal_reference>Government Information Quarterly, Volume 41, Issue 4, 2024, 101976</arxiv:journal_reference>
      <dc:creator>Unai Fischer-Abaigar, Christoph Kern, Noam Barda, Frauke Kreuter</dc:creator>
    </item>
    <item>
      <title>Does AI help humans make better decisions? A statistical evaluation framework for experimental and observational studies</title>
      <link>https://arxiv.org/abs/2403.12108</link>
      <description>arXiv:2403.12108v3 Announce Type: replace-cross 
Abstract: The use of Artificial Intelligence (AI), or more generally data-driven algorithms, has become ubiquitous in today's society. Yet, in many cases and especially when stakes are high, humans still make final decisions. The critical question, therefore, is whether AI helps humans make better decisions compared to a human-alone or AI-alone system. We introduce a new methodological framework to empirically answer this question with a minimal set of assumptions. We measure a decision maker's ability to make correct decisions using standard classification metrics based on the baseline potential outcome. We consider a single-blinded and unconfounded treatment assignment, where the provision of AI-generated recommendations is assumed to be randomized across cases with humans making final decisions. Under this study design, we show how to compare the performance of three alternative decision-making systems--human-alone, human-with-AI, and AI-alone. Importantly, the AI-alone system includes any individualized treatment assignment, including those that are not used in the original study. We also show when AI recommendations should be provided to a human-decision maker, and when one should follow such recommendations. We apply the proposed methodology to our own randomized controlled trial evaluating a pretrial risk assessment instrument. We find that the risk assessment recommendations do not improve the classification accuracy of a judge's decision to impose cash bail. Furthermore, we find that replacing a human judge with algorithms--the risk assessment score and a large language model in particular--leads to a worse classification performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12108v3</guid>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eli Ben-Michael, D. James Greiner, Melody Huang, Kosuke Imai, Zhichao Jiang, Sooahn Shin</dc:creator>
    </item>
    <item>
      <title>Differential contributions of machine learning and statistical analysis to language and cognitive sciences</title>
      <link>https://arxiv.org/abs/2404.14052</link>
      <description>arXiv:2404.14052v2 Announce Type: replace-cross 
Abstract: Data-driven approaches have revolutionized scientific research, with machine learning and statistical analysis being commonly used methodologies. Despite their widespread use, these approaches differ significantly in their techniques, objectives and implementations. Few studies have systematically applied both methods to identical datasets to highlight potential differences, particularly in language and cognitive sciences. This study employs the Buckeye Speech Corpus to illustrate how machine learning and statistical analysis are applied in data-driven research to obtain distinct insights on language production. We demonstrate the theoretical differences, implementation steps, and unique objectives of each approach through a comprehensive, tutorial-like comparison. Our analysis reveals that while machine learning excels at pattern recognition and prediction, statistical methods provide deeper insights into relationships between variables. The study highlights how semantic relevance, a novel metric measuring contextual influence on target words, contributes to understanding word duration in speech. We also systematically compare the differences between regression models used in machine learning and statistical analysis, particularly focusing on the training and fitting processes. Additionally, we clarify several common misconceptions that contribute to the confusion between these two approaches. Overall, by elucidating the complementary strengths of machine learning and statistics, this research enhances our understanding of diverse data-driven strategies in language and cognitive sciences, offering researchers valuable guidance on when and how to effectively apply these approaches in different research contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14052v2</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kun Sun, Rong Wang</dc:creator>
    </item>
    <item>
      <title>Forecasting with Hyper-Trees</title>
      <link>https://arxiv.org/abs/2405.07836</link>
      <description>arXiv:2405.07836v3 Announce Type: replace-cross 
Abstract: We introduce the concept of Hyper-Trees and offer a new direction in applying tree-based models to time series data. Unlike conventional applications of decision trees that forecast time series directly, Hyper-Trees are designed to learn the parameters of time series models. Our framework combines the effectiveness of gradient boosted trees on tabular data with the advantages of established time series models, thereby naturally inducing a time series inductive bias to tree models. By relating the parameters of a target time series model to features, Hyper-Trees also address the issue of parameter non-stationarity. To resolve the inherent scaling issue of boosted trees when estimating a large number of target model parameters, we combine decision trees and neural networks within a unified framework. In this novel approach, the trees first generate informative representations from the input features, which a shallow network then maps to the target model parameters. With our research, we aim to explore the effectiveness of Hyper-Trees across various forecasting scenarios and to extend the application of gradient boosted trees outside their conventional use in time series modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07836v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander M\"arz, Kashif Rasul</dc:creator>
    </item>
    <item>
      <title>Synthetic Potential Outcomes and Causal Mixture Identifiability</title>
      <link>https://arxiv.org/abs/2405.19225</link>
      <description>arXiv:2405.19225v2 Announce Type: replace-cross 
Abstract: A mixture model consists of a latent class that exerts a discrete signal on the observed data. Uncovering these latent classes is fundamental to unsupervised learning. In this paper, we consider the problem of recovering latent classes defined with respect to causal responses. We allow overlapping support in the distributions of these classes, meaning individuals cannot be clustered into groups with a similar response. Instead, we build on a setting from proximal causal inference to develop a method of moments approach to synthetically sample potential outcome distributions. This approach is the first known identifiability result for what we call Mixtures of Treatment Effects (MTEs). More broadly, we show how MTEs fit into a hierarchy of causal identifiability that unifies a number of perspectives on latent class confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19225v2</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bijan Mazaheri, Chandler Squires, Caroline Uhler</dc:creator>
    </item>
  </channel>
</rss>
