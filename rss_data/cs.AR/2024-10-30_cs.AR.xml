<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Oct 2024 04:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Systolic Array Data Flows for Efficient Matrix Multiplication in Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2410.22595</link>
      <description>arXiv:2410.22595v1 Announce Type: new 
Abstract: The paper discusses how Systolic Arrays can improve matrix multiplication for deep neural networks (DNNs). With AI models like OpenAI's GPT now containing trillions of parameters, the need for efficient matrix multiplication is more critical than ever. In this paper, the three main systolic array data flows: Weight Stationary (WS), Input Stationary (IS), and Output Stationary (OS) are discussed. Each data flow's energy consumption and efficiency across various matrix sizes are calculated using the SCALE-Sim simulator. The results show that selecting the right data flow for specific matrix configurations can drastically reduce energy consumption. The conclusions provide helpful insights into optimizing hardware for AI and machine learning applications, offering potential improvements in designing energy-efficient DNN accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22595v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tejas Raja</dc:creator>
    </item>
    <item>
      <title>An Event-Based Digital Compute-In-Memory Accelerator with Flexible Operand Resolution and Layer-Wise Weight/Output Stationarity</title>
      <link>https://arxiv.org/abs/2410.23082</link>
      <description>arXiv:2410.23082v1 Announce Type: new 
Abstract: Compute-in-memory (CIM) accelerators for spiking neural networks (SNNs) are promising solutions to enable $\mu$s-level inference latency and ultra-low energy in edge vision applications. Yet, their current lack of flexibility at both the circuit and system levels prevents their deployment in a wide range of real-life scenarios. In this work, we propose a novel digital CIM macro that supports arbitrary operand resolution and shape, with a unified CIM storage for weights and membrane potentials. These circuit-level techniques enable a hybrid weight- and output-stationary dataflow at the system level to maximize operand reuse, thereby minimizing costly on- and off-chip data movements during the SNN execution. Measurement results of a fabricated FlexSpIM prototype in 40-nm CMOS demonstrate a 2$\times$ increase in bit-normalized energy efficiency compared to prior fixed-precision digital CIM-SNNs, while providing resolution reconfiguration with bitwise granularity. Our approach can save up to 90% energy in large-scale systems, while reaching a state-of-the-art classification accuracy of 95.8% on the IBM DVS gesture dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23082v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Chauvaux, Adrian Kneip, Christoph Posch, Kofi Makinwa, Charlotte Frenkel</dc:creator>
    </item>
    <item>
      <title>Educating for Hardware Specialization in the Chiplet Era: A Path for the HPC Community</title>
      <link>https://arxiv.org/abs/2410.23127</link>
      <description>arXiv:2410.23127v1 Announce Type: new 
Abstract: The advent of chiplet technology introduces cutting-edge opportunities for constructing highly heterogeneous platforms with specialized accelerators. However, the HPC community currently lacks expertise in hardware development, a gap that must be bridged to leverage these advancements. Additionally, technologies like chiplet is cutting-edge with limited educational resource available. This paper addresses potential hardware specialization direction in HPC and how to cultivate these skills among students and staff, emphasizing the importance of understanding and developing custom hardware (e.g., rapid prototyping and resource estimation). We have been mentoring graduate-level students and new staff in hardware designs in a hands-on manner, encouraging them to utilize modern open-source hardware tools for their designs, which facilitates the sharing of research ideas. Additionally, we provide a summary of theses tools as part of our approach to prototyping and mentoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23127v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kazutomo Yoshii, Mohamed El-Hadedy</dc:creator>
    </item>
    <item>
      <title>Lost and Found in Speculation: Hybrid Speculative Vulnerability Detection</title>
      <link>https://arxiv.org/abs/2410.22555</link>
      <description>arXiv:2410.22555v1 Announce Type: cross 
Abstract: Microarchitectural attacks represent a challenging and persistent threat to modern processors, exploiting inherent design vulnerabilities in processors to leak sensitive information or compromise systems. Of particular concern is the susceptibility of Speculative Execution, a fundamental part of performance enhancement, to such attacks. We introduce Specure, a novel pre-silicon verification method composing hardware fuzzing with Information Flow Tracking (IFT) to address speculative execution leakages. Integrating IFT enables two significant and non-trivial enhancements over the existing fuzzing approaches: i) automatic detection of microarchitectural information leakages vulnerabilities without golden model and ii) a novel Leakage Path coverage metric for efficient vulnerability detection. Specure identifies previously overlooked speculative execution vulnerabilities on the RISC-V BOOM processor and explores the vulnerability search space 6.45x faster than existing fuzzing techniques. Moreover, Specure detected known vulnerabilities 20x faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22555v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3649329.3658469</arxiv:DOI>
      <dc:creator>Mohamadreza Rostami (JV), Shaza Zeitouni (JV), Rahul Kande (JV), Chen Chen (JV), Pouya Mahmoody (JV),  Jeyavijayan (JV),  Rajendran, Ahmad-Reza Sadeghi</dc:creator>
    </item>
    <item>
      <title>KALAM: toolKit for Automating high-Level synthesis of Analog computing systeMs</title>
      <link>https://arxiv.org/abs/2410.22946</link>
      <description>arXiv:2410.22946v1 Announce Type: cross 
Abstract: Diverse computing paradigms have emerged to meet the growing needs for intelligent energy-efficient systems. The Margin Propagation (MP) framework, being one such initiative in the analog computing domain, stands out due to its scalability across biasing conditions, temperatures, and diminishing process technology nodes. However, the lack of digital-like automation tools for designing analog systems (including that of MP analog) hinders their adoption for designing large systems. The inherent scalability and modularity of MP systems present a unique opportunity in this regard. This paper introduces KALAM (toolKit for Automating high-Level synthesis of Analog computing systeMs), which leverages factor graphs as the foundational paradigm for synthesizing MP-based analog computing systems. Factor graphs are the basis of various signal processing tasks and, when coupled with MP, can be used to design scalable and energy-efficient analog signal processors. Using Python scripting language, the KALAM automation flow translates an input factor graph to its equivalent SPICE-compatible circuit netlist that can be used to validate the intended functionality. KALAM also allows the integration of design optimization strategies such as precision tuning, variable elimination, and mathematical simplification. We demonstrate KALAM's versatility for tasks such as Bayesian inference, Low-Density Parity Check (LDPC) decoding, and Artificial Neural Networks (ANN). Simulation results of the netlists align closely with software implementations, affirming the efficacy of our proposed automation tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22946v1</guid>
      <category>eess.SY</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ankita Nandi, Krishil Gandhi, Mahendra Pratap Singh, Shantanu Chakrabartty, Chetan Singh Thakur</dc:creator>
    </item>
    <item>
      <title>HyperSense: Hyperdimensional Intelligent Sensing for Energy-Efficient Sparse Data Processing</title>
      <link>https://arxiv.org/abs/2401.10267</link>
      <description>arXiv:2401.10267v4 Announce Type: replace 
Abstract: Introducing HyperSense, our co-designed hardware and software system efficiently controls Analog-to-Digital Converter (ADC) modules' data generation rate based on object presence predictions in sensor data. Addressing challenges posed by escalating sensor quantities and data rates, HyperSense reduces redundant digital data using energy-efficient low-precision ADC, diminishing machine learning system costs. Leveraging neurally-inspired HyperDimensional Computing (HDC), HyperSense analyzes real-time raw low-precision sensor data, offering advantages in handling noise, memory-centricity, and real-time learning. Our proposed HyperSense model combines high-performance software for object detection with real-time hardware prediction, introducing the novel concept of Intelligent Sensor Control. Comprehensive software and hardware evaluations demonstrate our solution's superior performance, evidenced by the highest Area Under the Curve (AUC) and sharpest Receiver Operating Characteristic (ROC) curve among lightweight models. Hardware-wise, our FPGA-based domain-specific accelerator tailored for HyperSense achieves a 5.6x speedup compared to YOLOv4 on NVIDIA Jetson Orin while showing up to 92.1% energy saving compared to the conventional system. These results underscore HyperSense's effectiveness and efficiency, positioning it as a promising solution for intelligent sensing and real-time data processing across diverse applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10267v4</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Advanced Intelligent Systems, 2024</arxiv:journal_reference>
      <dc:creator>Sanggeon Yun, Hanning Chen, Ryozo Masukawa, Hamza Errahmouni Barkam, Andrew Ding, Wenjun Huang, Arghavan Rezvani, Shaahin Angizi, Mohsen Imani</dc:creator>
    </item>
    <item>
      <title>DFabric: Scaling Out Data Parallel Applications with CXL-Ethernet Hybrid Interconnects</title>
      <link>https://arxiv.org/abs/2409.05404</link>
      <description>arXiv:2409.05404v2 Announce Type: replace-cross 
Abstract: Emerging interconnects, such as CXL and NVLink, have been integrated into the intra-host topology to scale more accelerators and facilitate efficient communication between them, such as GPUs. To keep pace with the accelerator's growing computing throughput, the interconnect has seen substantial enhancement in link bandwidth, e.g., 256GBps for CXL 3.0 links, which surpasses Ethernet and InfiniBand network links by an order of magnitude or more. Consequently, when data-intensive jobs, such as LLM training, scale across multiple hosts beyond the reach limit of the interconnect, the performance is significantly hindered by the limiting bandwidth of the network infrastructure. We address the problem by proposing DFabric, a two-tier interconnect architecture. We address the problem by proposing DFabric, a two-tier interconnect architecture. First, DFabric disaggregates rack's computing units with an interconnect fabric, i.e., CXL fabric, which scales at rack-level, so that they can enjoy intra-rack efficient interconnecting. Second, DFabric disaggregates NICs from hosts, and consolidates them to form a NIC pool with CXL fabric. By providing sufficient aggregated capacity comparable to interconnect bandwidth, the NIC pool bridges efficient communication across racks or beyond the reach limit of interconnect fabric. However, the local memory accessing becomes the bottleneck when enabling each host to utilize the NIC pool efficiently. To the end, DFabric builds a memory pool with sufficient bandwidth by disaggregating host local memory and adding more memory devices. We have implemented a prototype of DFabric that can run applications transparently. We validated its performance gain by running various microbenchmarks and compute-intensive applications such as DNN and graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05404v2</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xu Zhang, Ke Liu, Yisong Chang, Ke Zhang, Mingyu Chen</dc:creator>
    </item>
  </channel>
</rss>
