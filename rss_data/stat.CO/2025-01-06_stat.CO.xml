<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jan 2025 03:22:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Integrative Learning of Intensity Fluctuations of Quantum Dots under Excitation via a Tailored Mixture Hidden Markov Model</title>
      <link>https://arxiv.org/abs/2501.01292</link>
      <description>arXiv:2501.01292v1 Announce Type: cross 
Abstract: Semiconductor nano-crystals, known as quantum dots (QDs), have garnered significant interest in various scientific fields due to their unique fluorescence properties. One captivating characteristic of QDs is their ability to emit photons under continuous excitation. The intensity of photon emission fluctuates during the excitation, and such a fluctuation pattern can vary across different dots even under the same experimental conditions. What adding to the complication is that the processed intensity series are non-Gaussian and truncated due to necessary thresholding and normalization. As such, conventional approaches in the chemistry literature, typified by single-dot analysis of raw intensity data with Gaussian hidden Markov models (HMM), cannot meet the many analytical challenges and may fail to capture any novel yet rare fluctuation patterns among QDs. Collaborating with scientists in the chemistry field, we have developed an integrative learning approach to simultaneously analyzing intensity series of multiple QDs. Our approach still inherits the HMM as the skeleton to model the intensity fluctuations of each dot, and based on the data structure and the hypothesized collective behaviors of the QDs, our approach asserts that (i) under each hidden state, the normalized intensity follows a 0/1 inflated Beta distribution, (ii) the state distributions are shared across all the QDs, and (iii) the patterns of transitions can vary across QDs. These unique features allow for a precise characterization of the intensity fluctuation patterns and facilitate the clustering of the QDs. With experimental data collected on 128 QDs, our methods reveal several QD clusters characterized by unique transition patterns across three intensity states. The results provide deeper insight into QD behaviors and their design/application potentials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01292v1</guid>
      <category>physics.app-ph</category>
      <category>cond-mat.mes-hall</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Yang, Hawi Nyiera, Yonglei Sun, Jing Zhao, Kun Chen</dc:creator>
    </item>
    <item>
      <title>Parallelized Midpoint Randomization for Langevin Monte Carlo</title>
      <link>https://arxiv.org/abs/2402.14434</link>
      <description>arXiv:2402.14434v3 Announce Type: replace-cross 
Abstract: We study the problem of sampling from a target probability density function in frameworks where parallel evaluations of the log-density gradient are feasible. Focusing on smooth and strongly log-concave densities, we revisit the parallelized randomized midpoint method and investigate its properties using recently developed techniques for analyzing its sequential version. Through these techniques, we derive upper bounds on the Wasserstein distance between sampling and target densities. These bounds quantify the substantial runtime improvements achieved through parallel processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14434v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Yu, Arnak Dalalyan</dc:creator>
    </item>
    <item>
      <title>Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal Transport Metrics for Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2411.14674</link>
      <description>arXiv:2411.14674v4 Announce Type: replace-cross 
Abstract: Existing methods to summarize posterior inference for mixture models focus on identifying a point estimate of the implied random partition for clustering, with density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et al., 2022). We propose a novel approach for summarizing posterior inference in nonparametric Bayesian mixture models, prioritizing density estimation of the mixing measure (or mixture) as an inference target. One of the key features is the model-agnostic nature of the approach, which remains valid under arbitrarily complex dependence structures in the underlying sampling model. Using a decision-theoretic framework, our method identifies a point estimate by minimizing posterior expected loss. A loss function is defined as a discrepancy between mixing measures. Estimating the mixing measure implies inference on the mixture density and the random partition. Exploiting the discrete nature of the mixing measure, we use a version of sliced Wasserstein distance. We introduce two specific variants for Gaussian mixtures. The first, mixed sliced Wasserstein, applies generalized geodesic projections on the product of the Euclidean space and the manifold of symmetric positive definite matrices. The second, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture measures for efficient projection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14674v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Peter Mueller</dc:creator>
    </item>
  </channel>
</rss>
