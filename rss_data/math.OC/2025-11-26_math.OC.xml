<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Nov 2025 02:40:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Extending Douglas-Rachford Splitting for Convex Optimization</title>
      <link>https://arxiv.org/abs/2511.19637</link>
      <description>arXiv:2511.19637v1 Announce Type: new 
Abstract: The Douglas-Rachford splitting method is a classical and widely used algorithm for solving monotone inclusions involving the sum of two maximally monotone operators. It was recently shown to be the unique frugal, no-lifting resolvent-splitting method that is unconditionally convergent in the general two-operator setting. In this work, we show that this uniqueness does not hold in the convex optimization case: when the operators are subdifferentials of proper, closed, convex functions, a strictly larger class of frugal, no-lifting resolvent-splitting methods is unconditionally convergent. We provide a complete characterization of all such methods in the convex optimization setting and prove that this characterization is sharp: unconditional convergence holds exactly on the identified parameter regions. These results immediately yield new families of convergent ADMM-type and Chambolle-Pock-type methods obtained through their Douglas-Rachford reformulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19637v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Nilsson, Anton {\AA}kerman, Pontus Giselsson</dc:creator>
    </item>
    <item>
      <title>Catalyzing System-level Decarbonization: An Analysis of Carbon Matching As An Accounting Framework</title>
      <link>https://arxiv.org/abs/2511.19666</link>
      <description>arXiv:2511.19666v1 Announce Type: new 
Abstract: Carbon matching aims to improve corporate carbon accounting by tracking emissions rather than energy consumption and production. We present a mathematical derivation of carbon matching using marginal emission rates, where the unit of matching is tons of carbon emitted. We present analysis and open source notebooks showing how marginal emissions can be calculated on simulated electric bus networks. Importantly, we prove mathematically that distinct emissions rates can be assigned to all aspects of the electric grid - including transmission, storage, generation, and consumption - completely allocating electric grid emissions. We show that carbon matching is an accurate carbon accounting framework that can inspire ambitious and impactful action. This research fills a gap by blending carbon accounting expertise and power systems modeling to consider the effectiveness of alternative methodologies for allocating electric system emissions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19666v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikky Avila, Hank He, Reza Rastegar, Jamie Tolan, Tobias Tiecke, Brian White</dc:creator>
    </item>
    <item>
      <title>Anytime-Feasible First-Order Optimization via Safe Sequential QCQP</title>
      <link>https://arxiv.org/abs/2511.19675</link>
      <description>arXiv:2511.19675v1 Announce Type: new 
Abstract: This paper presents the Safe Sequential Quadratically Constrained Quadratic Programming (SS-QCQP) algorithm, a first-order method for smooth inequality-constrained nonconvex optimization that guarantees feasibility at every iteration. The method is derived from a continuous-time dynamical system whose vector field is obtained by solving a convex QCQP that enforces monotonic descent of the objective and forward invariance of the feasible set. The resulting continuous-time dynamics achieve an $O(1/t)$ convergence rate to first-order stationary points under standard constraint qualification conditions. We then propose a safeguarded Euler discretization with adaptive step-size selection that preserves this convergence rate while maintaining both descent and feasibility in discrete time. To enhance scalability, we develop an active-set variant (SS-QCQP-AS) that selectively enforces constraints near the boundary, substantially reducing computational cost without compromising theoretical guarantees. Numerical experiments on a multi-agent nonlinear optimal control problem demonstrate that SS-QCQP and SS-QCQP-AS maintain feasibility, exhibit the predicted convergence behavior, and deliver solution quality comparable to second-order solvers such as SQP and IPOPT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19675v1</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiarui Wang, Mahyar Fazlyab</dc:creator>
    </item>
    <item>
      <title>Optimal dividend and capital injection under self-exciting claims</title>
      <link>https://arxiv.org/abs/2511.19701</link>
      <description>arXiv:2511.19701v1 Announce Type: new 
Abstract: In this paper, we study an optimal dividend and capital-injection problem in a Cram\'er--Lundberg model where claim arrivals follow a Hawkes process, capturing clustering effects often observed in insurance portfolios. We establish key analytical properties of the value function and characterise the optimal capital-injection strategy through an explicit threshold. We also show that the value function is the unique viscosity solution of the associated HJB variational inequality. For numerical purposes, we first compute a benchmark solution via a monotone finite-difference scheme with Howard's policy iteration. We then develop a reinforcement learning approach based on policy-gradient and actor-critic methods. The learned strategies closely match the PDE benchmark and remain stable across initial conditions. The results highlight the relevance of policy-gradient techniques for dividend optimisation under self-exciting claim dynamics and point toward scalable methods for higher-dimensional extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19701v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>q-fin.RM</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paulin Aubert, Etienne Chevalier, Vathana Ly Vath</dc:creator>
    </item>
    <item>
      <title>An Accelerated Distributed Algorithm with Equality and Inequality Coupling Constraints</title>
      <link>https://arxiv.org/abs/2511.19708</link>
      <description>arXiv:2511.19708v1 Announce Type: new 
Abstract: This paper studies distributed convex optimization with both affine equality and nonlinear inequality couplings through the duality analysis. We first formulate the dual of the coupling-constraint problem and reformulate it as a consensus optimization problem over a connected network. To efficiently solve this dual problem and hence the primal problem, we design an accelerated linearized algorithm that, at each round, a look-ahead linearization of the separable objective is combined with a quadratic penalty on the Laplacian constraint, a proximal step, and an aggregation of iterations. On the theory side, we prove non-ergodic rates for both the primal optimality error and the feasibility error. On the other hand, numerical experiments show a faster decrease of optimality error and feasibility residual than augmented-Lagrangian tracking and distributed subgradient baselines under the same communication budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19708v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyang Qiu, Yangyang Qian, Zongli Lin, Yacov A. Shamash</dc:creator>
    </item>
    <item>
      <title>Non-Ergodic Convergence Algorithms for Distributed Consensus and Coupling-Constrained Optimization</title>
      <link>https://arxiv.org/abs/2511.19714</link>
      <description>arXiv:2511.19714v1 Announce Type: new 
Abstract: We study distributed convex optimization with two ubiquitous forms of coupling: consensus constraints and global affine equalities. We first design a linearized method of multipliers for the consensus optimization problem. Without smoothness or strong convexity, we establish non-ergodic sublinear rates of order O(1/\sqrt{k}) for both the objective optimality and the consensus violation. Leveraging duality, we then show that the economic dispatch problem admits a dual consensus formulation, and that applying the same algorithm to the dual economic dispatch yields non-ergodic O(1/\sqrt{k}) decay for the error of the summation of the cost over the network and the equality-constraint residual under convexity and Slater's condition. Numerical results on the IEEE 118-bus system demonstrate faster reduction of both objective error and feasibility error relative to the state-of-the-art baselines, while the dual variables reach network-wide consensus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19714v1</guid>
      <category>math.OC</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyang Qiu, Zongli Lin</dc:creator>
    </item>
    <item>
      <title>A Distributed Gradient-based Algorithm for Optimization Problems with Coupled Equality Constraints</title>
      <link>https://arxiv.org/abs/2511.19723</link>
      <description>arXiv:2511.19723v1 Announce Type: new 
Abstract: This paper studies a class of distributed optimization problems with coupled equality constraints in networked systems. Many existing distributed algorithms rely on solving local subproblems via the $\operatorname{argmin}$ operator in each iteration. Such approaches become computationally burdensome or intractable when local cost functions are complex. To address this challenge, we propose a novel distributed gradient-based algorithm that avoids solving a local optimization problem at each iteration by leveraging first-order approximations and projection onto local feasible sets. The algorithm operates in a fully distributed manner, requiring only local communication without exchanging gradients or primal variables. We rigorously establish sublinear convergence for general convex cost functions and linear convergence under strong convexity and smoothness conditions. Numerical simulation on the IEEE 118-bus system demonstrates the superior computational efficiency and scalability of the proposed method compared to several state-of-the-art distributed optimization algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19723v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyang Qiu, Zongli Lin</dc:creator>
    </item>
    <item>
      <title>Convexification of classes of mixed-integer sets with L$^\natural$-convexity</title>
      <link>https://arxiv.org/abs/2511.19754</link>
      <description>arXiv:2511.19754v1 Announce Type: new 
Abstract: L$^\natural$ (natural)-convex functions encompass a large class of nonlinear functions over general integer domains and arise in a wide range of real-world applications. We explore the minimization of L$^\natural$-convex functions, of multiple L$^\natural$-convex functions with common variables, and of a mixed-integer extension of L$^\natural$-convex functions -- functions defined over a mixed-integer domain with properties that resemble L$^\natural$-convexity. For each of these families of minimization problems, we propose valid linear inequalities and provide convex hull descriptions for the corresponding epigraphs. For all classes of proposed inequalities, we discuss their facet conditions, develop exact separation methods, and analyze the complexity of the separation problem. We discover hidden L$^\natural$-convexity in well-known mixed-integer structures in the integer programming literature, namely the (general integer) mixing set and the continuous mixing set. We show that our findings subsume the existing polyhedral results for these sets and establish new results for the multi-capacity variant of the continuous mixing set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19754v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qimeng Yu, Simge K\"u\c{c}\"ukyavuz</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Mean Field Games and Local Well-posedness</title>
      <link>https://arxiv.org/abs/2511.19766</link>
      <description>arXiv:2511.19766v1 Announce Type: new 
Abstract: Motivated by the recent interests in asymmetric mean field games, this paper provides a general framework of Heterogeneous Mean Field Game (HMFG) that subsumes different formulations of graphon mean field games. The key feature of the HMFG is that the players interact with the population through the density ensemble. In this case, the HMFG system becomes an infinite-dimensional Forward-Backward SDE (FBSDE) system. We show that the FBSDE is locally well-posed, thus the HMFG has a unique equilibrium. In addition, we show that the equilibrium of HMFG is a good approximate equilibrium of the corresponding N-Player Game. Lastly, we derive the It\^{o} formula of infinite-dimensional measure flow and use it to obtain the master equation for HMFG as a decoupling field of the infinite-dimensional FBSDE system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19766v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bixing Qiao</dc:creator>
    </item>
    <item>
      <title>On the Fundamental Limit of Stochastic Gradient Identification Algorithm Under Non-Persistent Excitation</title>
      <link>https://arxiv.org/abs/2511.19981</link>
      <description>arXiv:2511.19981v1 Announce Type: new 
Abstract: Stochastic gradient methods are of fundamental importance in system identification and machine learning, enabling online parameter estimation for large-scale and data-streaming processes. The stochastic gradient algorithm stands as a classical identification method that has been extensively studied for decades. Under non-persistent excitation, the best known convergence result requires the condition number of the Fisher information matrix to satisfy $\kappa(\sum_{i=1}^n \varphi_i \varphi_i^\top) = O((\log r_n)^\alpha)$, where $r_n = 1 + \sum_{i=1}^n \|\varphi_i\|^2$, with strong consistency guaranteed for $\alpha \leq 1/3$ but known to fail for $\alpha &gt; 1$. This paper establishes that strong consistency in fact holds for the entire range $0 \leq \alpha &lt; 1$, achieved through a novel algebraic framework that yields substantially sharper matrix norm bounds. Our result nearly resolves the four-decade-old conjecture of Chen and Guo (1986), bridging the theoretical gap from $\alpha \leq 1/3$ to nearly the entire feasible range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19981v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Senhan Yao, Longxu Zhang</dc:creator>
    </item>
    <item>
      <title>A Local Parametrization of the State-Feedback Matrices in the Pole Assignment Problem</title>
      <link>https://arxiv.org/abs/2511.20029</link>
      <description>arXiv:2511.20029v1 Announce Type: new 
Abstract: Given a controllable system $(F,G)$, a local parametrization is obtained for the set of feedback gain matrices $K$ such that the state matrix, $F+GK$, of the closed loop system is in a prescribed similarity class. It is shown that this set can be endowed with the structure of a differentiable manifold whose dimension is also computed. Then a local parametrization and a local system of coordinates is provided using a diffeomorphism between this set of state feedback matrices and the orbit space of a set of truncated observability matrices via de action of a Lie group.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20029v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>I. Baraga\~na (University of the Basque Country. UPV/EHU), F. Puerta (E.T.S. Enginyeria Industrial de Barcelona. UPC), I. Zaballa (University of the Basque Country. UPV/EHU)</dc:creator>
    </item>
    <item>
      <title>On the differentiability of the value function of switched linear systems under arbitrary and controlled switching</title>
      <link>https://arxiv.org/abs/2511.20037</link>
      <description>arXiv:2511.20037v1 Announce Type: new 
Abstract: This paper studies the differentiability of the value function of switched linear systems under arbitrary switching and controlled switching, referred to as worst-case and optimal value functions respectively. First, we show that the value functions are Lipschitz continuous, when the cost function is Lipschitz continuous. Then, as the central contribution of this work, we show with examples that each of these functions can be non-differentiable on dense subsets of the state space, even if the cost function is smooth and Lipschitz continuous. This has implications for optimal control and reinforcement learning since it implies that the exact computation of these value functions requires templates involving functions that are non-differentiable on dense subsets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20037v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume O. Berger</dc:creator>
    </item>
    <item>
      <title>Scaling limits of multi-period distributionally robust optimization problems</title>
      <link>https://arxiv.org/abs/2511.20126</link>
      <description>arXiv:2511.20126v1 Announce Type: new 
Abstract: We examine the scaling limit of multi-period distributionally robust optimization (DRO) problems via a semigroup approach. Each period involves a worst-case maximization over distributions in a Wasserstein ball around the transition probability of a reference process with radius proportional to the length of the period, and the multi-period DRO problem arises through its sequential composition. We show that the scaling limit of the multi-period DRO, as the length of each period tends to zero, is a strongly continuous monotone semigroup on $\mathrm{C_b}$. Furthermore, we show that its infinitesimal generator is equal to the generator associated with the non-robust scaling limit plus an additional perturbation term induced by the Wasserstein uncertainty. As an application, we show that when the reference process follows an It\^o process, the viscosity solution of the associated nonlinear PDE coincides with the value of continuous-time robust optimization problems under parametric uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20126v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Nendel, Ariel Neufeld, Kyunghyun Park, Alessandro Sgarabottolo</dc:creator>
    </item>
    <item>
      <title>Stochastic Sequential Quadratic Programming for Optimization with Functional Constraints</title>
      <link>https://arxiv.org/abs/2511.20178</link>
      <description>arXiv:2511.20178v1 Announce Type: new 
Abstract: Stochastic convex optimization problems with nonlinear functional constraints are ubiquitous in machine learning applications, including multi-task learning, structured prediction, and multi-view learning. The presence of nonlinear functional constraints renders the traditional projected stochastic gradient descent and related projection-based methods inefficient, and motivates the use of first-order methods. However, existing first-order methods, including primal and primal-dual algorithms, typically rely on a bounded (sub-)gradient assumption, which may be too restrictive in many settings.
  We propose a stochastic sequential quadratic programming (SSQP) algorithm that works entirely in the primal domain, avoids projecting onto the feasible region, obviates the need for bounded gradients, and achieves state-of-the-art oracle complexity under standard smoothness and convexity assumptions. A faster version, namely SSQP-Skip, is also proposed where the quadratic subproblems can be skipped in most iterations. Finally, we develop an accelerated variance-reduced version of SSQP (VARAS), whose oracle complexity bounds match those for solving unconstrained finite-sum convex optimization problems. The superior performance of the proposed algorithms is demonstrated via numerical experiments on real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20178v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Panchajanya Sanyal, Srujan Teja Thomdapu, Ketan Rajawat</dc:creator>
    </item>
    <item>
      <title>Adaptive SGD with Line-Search and Polyak Stepsizes: Nonconvex Convergence and Accelerated Rates</title>
      <link>https://arxiv.org/abs/2511.20207</link>
      <description>arXiv:2511.20207v2 Announce Type: new 
Abstract: We extend the convergence analysis of AdaSLS and AdaSPS in [Jiang and Stich, 2024] to the nonconvex setting, presenting a unified convergence analysis of stochastic gradient descent with adaptive Armijo line-search (AdaSLS) and Polyak stepsize (AdaSPS) for nonconvex optimization. Our contributions include: (1) an $\mathcal{O}(1/\sqrt{T})$ convergence rate for general nonconvex smooth functions, (2) an $\mathcal{O}(1/T)$ rate under quasar-convexity and interpolation, and (3) an $\mathcal{O}(1/T)$ rate under the strong growth condition for general nonconvex functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20207v2</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haotian Wu</dc:creator>
    </item>
    <item>
      <title>Scaled relative graphs for pairs of operators beyond classical monotonicity</title>
      <link>https://arxiv.org/abs/2511.20209</link>
      <description>arXiv:2511.20209v1 Announce Type: new 
Abstract: We introduce a generalization of the scaled relative graph (SRG) to pairs of operators, enabling the visualization of their relative incremental properties. This novel SRG framework provides the geometric counterpart for the study of nonlinear resolvents based on paired monotonicity conditions. We demonstrate that these conditions apply to linear operators composed with monotone mappings, a class that notably includes NPN transistors, allowing us to compute the response of multivalued, nonsmooth and highly nonmonotone electrical circuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20209v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jan Quan, Alexander Bodard, Konstantinos Oikonomidis, Panagiotis Patrinos</dc:creator>
    </item>
    <item>
      <title>Nonlinearly preconditioned gradient flows</title>
      <link>https://arxiv.org/abs/2511.20370</link>
      <description>arXiv:2511.20370v1 Announce Type: new 
Abstract: We study a continuous-time dynamical system which arises as the limit of a broad class of nonlinearly preconditioned gradient methods. Under mild assumptions, we establish existence of global solutions and derive Lyapunov-based convergence guarantees. For convex costs, we prove a sublinear decay in a geometry induced by some reference function, and under a generalized gradient-dominance condition we obtain exponential convergence. We further uncover a duality connection with mirror descent, and use it to establish that the flow of interest solves an infinite-horizon optimal-control problem of which the value function is the Bregman divergence generated by the cost. These results clarify the structure and optimization behavior of nonlinearly preconditioned gradient flows and connect them to known continuous-time models in non-Euclidean optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20370v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Konstantinos Oikonomidis, Alexander Bodard, Jan Quan, Panagiotis Patrinos</dc:creator>
    </item>
    <item>
      <title>Self-Identifying Internal Model-Based Online Optimization</title>
      <link>https://arxiv.org/abs/2511.20411</link>
      <description>arXiv:2511.20411v1 Announce Type: new 
Abstract: In this paper, we propose a novel online optimization algorithm built by combining ideas from control theory and system identification. The foundation of our algorithm is a control-based design that makes use of the internal model of the online problem. Since such prior knowledge of this internal model might not be available in practice, we incorporate an identification routine that learns this model on the fly. The algorithm is designed starting from quadratic online problems but can be applied to general problems. For quadratic cases, we characterize the asymptotic convergence to the optimal solution trajectory. We compare the proposed algorithm with existing approaches, and demonstrate how the identification routine ensures its adaptability to changes in the underlying internal model. Numerical results also indicate strong performance beyond the quadratic setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20411v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wouter J. A. van Weerelt, Lantian Zhang, Silun Zhang, Nicola Bastianello</dc:creator>
    </item>
    <item>
      <title>PAC-Bayes Meets Online Contextual Optimization</title>
      <link>https://arxiv.org/abs/2511.20413</link>
      <description>arXiv:2511.20413v1 Announce Type: new 
Abstract: The predict-then-optimize paradigm bridges online learning and contextual optimization in dynamic environments. Previous works have investigated the sequential updating of predictors using feedback from downstream decisions to minimize regret in the full-information settings. However, existing approaches are predominantly frequentist, rely heavily on gradient-based strategies, and employ deterministic predictors that could yield high variance in practice despite their asymptotic guarantees. This work introduces, to the best of our knowledge, the first Bayesian online contextual optimization framework. Grounded in PAC-Bayes theory and general Bayesian updating principles, our framework achieves $\mathcal{O}(\sqrt{T})$ regret for bounded and mixable losses via a Gibbs posterior, eliminates the dependence on gradients through sequential Monte Carlo samplers, and thereby accommodates nondifferentiable problems. Theoretical developments and numerical experiments substantiate our claims.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20413v1</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhuojun Xie, Adam Abdin, Yiping Fang</dc:creator>
    </item>
    <item>
      <title>Family of Riemannian problems on the Heisenberg group</title>
      <link>https://arxiv.org/abs/2511.20491</link>
      <description>arXiv:2511.20491v1 Announce Type: new 
Abstract: We study a family of Riemannian problems on the Heisenberg group that tends to the sub-Riemannian problem on this group.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20491v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu. Sachkov</dc:creator>
    </item>
    <item>
      <title>Optimization of Sums of Bivariate Functions: An Introduction to Relaxation-Based Methods for the Case of Finite Domains</title>
      <link>https://arxiv.org/abs/2511.20607</link>
      <description>arXiv:2511.20607v1 Announce Type: new 
Abstract: We study the optimization of functions with $n&gt;2$ arguments that have a representation as a sum of several functions that have only $2$ of the $n$ arguments each, termed sums of bivariates, on finite domains. The complexity of optimizing sums of bivariates is shown to be NP-equivalent and it is shown that there exists free lunch in the optimization of sums of bivariates. Based on measure-valued extensions of the objective function, so-called relaxations, $\ell^2$-approximation, and entropy-regularization, we derive several tractable problem formulations solvable with linear programming, coordinate ascent as well as with closed-form solutions. The limits of applying tractable versions of such relaxations to sums of bivariates are investigated using general results for reconstructing measures from their bivariate marginals. Experiments in which the derived algorithms are applied to random functions, vertex coloring, and signal reconstruction problems provide insights into qualitatively different function classes that can be modeled as sums of bivariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20607v1</guid>
      <category>math.OC</category>
      <category>cs.CV</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nils M\"uller</dc:creator>
    </item>
    <item>
      <title>Lower Complexity Bounds for Nonconvex-Strongly-Convex Bilevel Optimization with First-Order Oracles</title>
      <link>https://arxiv.org/abs/2511.19656</link>
      <description>arXiv:2511.19656v2 Announce Type: cross 
Abstract: Although upper bound guarantees for bilevel optimization have been widely studied, progress on lower bounds has been limited due to the complexity of the bilevel structure. In this work, we focus on the smooth nonconvex-strongly-convex setting and develop new hard instances that yield nontrivial lower bounds under deterministic and stochastic first-order oracle models. In the deterministic case, we prove that any first-order zero-respecting algorithm requires at least $\Omega(\kappa^{3/2}\epsilon^{-2})$ oracle calls to find an $\epsilon$-accurate stationary point, improving the optimal lower bounds known for single-level nonconvex optimization and for nonconvex-strongly-convex min-max problems. In the stochastic case, we show that at least $\Omega(\kappa^{5/2}\epsilon^{-4})$ stochastic oracle calls are necessary, again strengthening the best known bounds in related settings. Our results expose substantial gaps between current upper and lower bounds for bilevel optimization and suggest that even simplified regimes, such as those with quadratic lower-level objectives, warrant further investigation toward understanding the optimal complexity of bilevel optimization under standard first-order oracles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19656v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiyi Ji</dc:creator>
    </item>
    <item>
      <title>Joint Satellite Power Consumption and Handover Optimization for LEO Constellations</title>
      <link>https://arxiv.org/abs/2511.19745</link>
      <description>arXiv:2511.19745v1 Announce Type: cross 
Abstract: In satellite constellation-based communication systems, continuous user coverage requires frequent handoffs due to the dynamic topology induced by the Low Earth Orbit (LEO) satellites. Each handoff between a satellite and ground users introduces additional signaling and power consumption, which can become a significant burden as the size of the constellation continues to increase. This work focuses on the optimization of the total transmission rate in a LEO-to-user system, by jointly considering the total transmitted power, user-satellite associations, and power consumption, the latter being handled through a penalty on handoff events. We consider a system where LEO satellites serve users located in remote areas with no terrestrial connectivity, and formulate the power allocation problem as a mixed-integer concave linear program (MICP) subject to power and association constraints. Our approach can be solved with off-the-shelf solvers and is benchmarked against a naive baseline where users associate to their closest visible satellite. Extensive Monte Carlo simulations demonstrate the effectiveness of the proposed method in controlling the handoff frequency while maintaining high user throughput. These performance gains highlight the effectiveness of our handover-aware optimization strategy, which ensures that user rates improve significantly, by about 40%, without incurring a disproportionate rise in the handoff frequency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19745v1</guid>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.1109/WiSEE57913.2025.11229869</arxiv:DOI>
      <arxiv:journal_reference>2025 IEEE International Conference on Wireless for Space and Extreme Environments (WiSEE)</arxiv:journal_reference>
      <dc:creator>Yassine Afif, Mohammed Almekhlafi, Antoine Lesage-Landry, Gunes Karabulut Kurt</dc:creator>
    </item>
    <item>
      <title>Adaptivity and Universality: Problem-dependent Universal Regret for Online Convex Optimization</title>
      <link>https://arxiv.org/abs/2511.19937</link>
      <description>arXiv:2511.19937v1 Announce Type: cross 
Abstract: Universal online learning aims to achieve optimal regret guarantees without requiring prior knowledge of the curvature of online functions. Existing methods have established minimax-optimal regret bounds for universal online learning, where a single algorithm can simultaneously attain $\mathcal{O}(\sqrt{T})$ regret for convex functions, $\mathcal{O}(d \log T)$ for exp-concave functions, and $\mathcal{O}(\log T)$ for strongly convex functions, where $T$ is the number of rounds and $d$ is the dimension of the feasible domain. However, these methods still lack problem-dependent adaptivity. In particular, no universal method provides regret bounds that scale with the gradient variation $V_T$, a key quantity that plays a crucial role in applications such as stochastic optimization and fast-rate convergence in games. In this work, we introduce UniGrad, a novel approach that achieves both universality and adaptivity, with two distinct realizations: UniGrad.Correct and UniGrad.Bregman. Both methods achieve universal regret guarantees that adapt to gradient variation, simultaneously attaining $\mathcal{O}(\log V_T)$ regret for strongly convex functions and $\mathcal{O}(d \log V_T)$ regret for exp-concave functions. For convex functions, the regret bounds differ: UniGrad.Correct achieves an $\mathcal{O}(\sqrt{V_T \log V_T})$ bound while preserving the RVU property that is crucial for fast convergence in online games, whereas UniGrad.Bregman achieves the optimal $\mathcal{O}(\sqrt{V_T})$ regret bound through a novel design. Both methods employ a meta algorithm with $\mathcal{O}(\log T)$ base learners, which naturally requires $\mathcal{O}(\log T)$ gradient queries per round. To enhance computational efficiency, we introduce UniGrad++, which retains the regret while reducing the gradient query to just $1$ per round via surrogate optimization. We further provide various implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19937v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Zhao, Yu-Hu Yan, Hang Yu, Zhi-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Energy Efficient Nonlinear Microscopic Dynamical Model for Autonomous and Electric Vehicles</title>
      <link>https://arxiv.org/abs/2511.20054</link>
      <description>arXiv:2511.20054v1 Announce Type: cross 
Abstract: This article proposes a nonlinear microscopic dynamical model for autonomous electric vehicles (A-EVs) that considers battery energy efficiency in the car-following dynamics. The model builds upon the Optimal Velocity Model (OVM), with the control term based on the battery dynamics to enable thermally optimal and energy-efficient driving. We rigorously prove that the proposed model achieves lower energy consumption compared to the Optimal Velocity Follow-the-Leader (OVFL) model. Through numerical simulations, we validate the analytical results on the energy efficiency. We additionally investigate the stability properties of the proposed model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20054v1</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuneil Yeo, Jaewoong Lee, Scott Moura, Maria Laura Delle Monache</dc:creator>
    </item>
    <item>
      <title>Communication-Efficient Learning for Satellite Constellations</title>
      <link>https://arxiv.org/abs/2511.20220</link>
      <description>arXiv:2511.20220v1 Announce Type: cross 
Abstract: Satellite constellations in low-Earth orbit are now widespread, enabling positioning, Earth imaging, and communications. In this paper we address the solution of learning problems using these satellite constellations. In particular, we focus on a federated approach, where satellites collect and locally process data, with the ground station aggregating local models. We focus on designing a novel, communication-efficient algorithm that still yields accurate trained models. To this end, we employ several mechanisms to reduce the number of communications with the ground station (local training) and their size (compression). We then propose an error feedback mechanism that enhances accuracy, which yields, as a byproduct, an algorithm-agnostic error feedback scheme that can be more broadly applied. We analyze the convergence of the resulting algorithm, and compare it with the state of the art through simulations in a realistic space scenario, showcasing superior performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20220v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruxandra-Stefania Tudose, Moritz H. W. Gr\"uss, Grace Ra Kim, Karl H. Johansson, Nicola Bastianello</dc:creator>
    </item>
    <item>
      <title>Same-day or next-day? Transparent time-dependent shipment pricing for e-fulfillment</title>
      <link>https://arxiv.org/abs/2307.10119</link>
      <description>arXiv:2307.10119v2 Announce Type: replace 
Abstract: We develop a parsimonious model of an e-commerce fulfillment center that offers time-dependent shipment options and corresponding fees to utility-maximizing customers arriving according to a Poisson process. For any such policy, we provide an exact steady-state analysis using the underlying periodic Markov chain to characterize system performance. Because shipment fees shape both the volume and timing of same-day demand, direct optimization over the price domain is analytically intractable. To enable structural and computational insights, we introduce a transformation that maps each shipment-fee policy to its induced cumulative demand profile. This reformulation reveals that the optimal policy features a cutoff time and monotonically increasing fees, and it yields a supermodular profit function that can be optimized in polynomial time. We also propose a simple two-level time-dependent fee structure that is intuitive for customers and achieves near-optimal performance. Numerical experiments show that introducing a cutoff time substantially improves profits under static fees, and that using time-dependent fees produces further significant gains. Overall, transparent time-dependent shipment policies help firms align same-day demand with fulfillment capacity while maintaining transparency and fairness for customers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10119v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uta Mohring, Melvin Drent, Ivo Adan, Willem van Jaarsveld</dc:creator>
    </item>
    <item>
      <title>Augmenting Subspace Optimization Methods with Linear Bandits</title>
      <link>https://arxiv.org/abs/2412.14278</link>
      <description>arXiv:2412.14278v4 Announce Type: replace 
Abstract: We consider the framework of methods for unconstrained minimization that are, in each iteration, restricted to a model that is only a valid approximation to the objective function on some affine subspace containing an incumbent point. These methods are of practical interest in computational settings where derivative information is either expensive or impossible to obtain. Recent attention has been paid in the literature to employing randomized matrix sketching for generating the affine subspaces within this framework.
  We consider a relatively straightforward, deterministic augmentation of such a generic subspace optimization method. In particular, we consider a sequential optimization framework where actions consist of one-dimensional linear subspaces and rewards consist of (approximations to) the magnitudes of directional derivatives computed in the direction of the action subspace. Reward maximization in this context is consistent with maximizing lower bounds on descent guaranteed by first-order Taylor models. This sequential optimization problem can be analyzed through the lens of dynamic regret. We modify an existing linear upper confidence bound (UCB) bandit method and prove sublinear dynamic regret in the subspace optimization setting. We demonstrate the efficacy of employing this linear UCB method in a setting where forward-mode algorithmic differentiation can provide directional derivatives in arbitrary directions and in a derivative-free setting. For the derivative-free setting, we propose SS-POUNDers, an extension of the derivative-free optimization method POUNDers that employs the linear UCB mechanism to identify promising subspaces. Our numerical experiments suggest a preference, in either computational setting, for employing a linear UCB mechanism within a subspace optimization method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14278v4</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matt Menickelly</dc:creator>
    </item>
    <item>
      <title>Controlling Klein-Gordon Chains and Lattices</title>
      <link>https://arxiv.org/abs/2502.08852</link>
      <description>arXiv:2502.08852v3 Announce Type: replace 
Abstract: In this work, we initiate the study of controlling nonlinear Klein-Gordon chains and lattices through their emergent collective flocking behavior. By constructing appropriate feedback control mechanisms, we demonstrate that any physically admissible flock state can be achieved in finite time, meaning the chain can be driven from arbitrary initial vibrations toward a coherent traveling-wave motion. Finally, we reveal a deep connection between the flocking problem and a minimal-time control principle formulated within the framework of nonlinear Hamilton-Jacobi equations and optimal control theory, providing a unifying view-point for wave control in discrete nonlinear media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08852v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sarah Strikwerda, Hung Vinh Tran, Minh-Binh Tran</dc:creator>
    </item>
    <item>
      <title>Stochastic exit-time control on the half-line over a finite horizon</title>
      <link>https://arxiv.org/abs/2503.19648</link>
      <description>arXiv:2503.19648v3 Announce Type: replace 
Abstract: We consider a finite-time stochastic drift control problem with the assumption that the control is bounded and the system is controlled until the state process leaves the half-line. Assuming general conditions, it is proved that the resulting parabolic Hamilton-Jacobi-Bellman equation has a classical solution. In fact, we consider an even more general family of semilinear equations, which might be helpful in solving other control or game problems. Not only is the existence result proved, but also a recursive procedure for finding a solution resulting from a fixed-point argument is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19648v3</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dariusz Zawisza</dc:creator>
    </item>
    <item>
      <title>Proximal Iterative Hard Thresholding Algorithm for Sparse Group $\ell_0$-Regularized Optimization with Box Constraint</title>
      <link>https://arxiv.org/abs/2506.00196</link>
      <description>arXiv:2506.00196v2 Announce Type: replace 
Abstract: This paper investigates a general class of problems in which a lower bounded smooth convex function incorporating $\ell_{0}$ and $\ell_{2,0}$ regularization is minimized over a box constraint. Although such problems arise frequently in practical applications, their inherent non-convexity poses significant challenges for solution methods. In particular, we focus on the proximal operator associated with these regularizations, which incorporates both group-sparsity and element-wise sparsity terms. Besides, we introduce the concepts of $\tau$-stationary point and support optimal (SO) point then analyze their relationship with the minimizer of the considered problem. Based on the proximal operator, we propose a novel proximal iterative hard thresholding algorithm to solve the problem. Furthermore, we establish the global convergence and the computational complexity analysis of the proposed method. Finally, extensive experiments demonstrate the effectiveness and efficiency of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00196v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuge Ye, Qingna Li</dc:creator>
    </item>
    <item>
      <title>Modified Block Newton Algorithm for $\ell_0$-Regularized Optimization</title>
      <link>https://arxiv.org/abs/2507.03566</link>
      <description>arXiv:2507.03566v2 Announce Type: replace 
Abstract: In this paper, we propose a globally convergent Newton type method to solve $\ell_0$ regularized sparse optimization problem. In fact, a line search strategy is applied to the Newton method to obtain global convergence. The Jacobian matrix of the original problem is a block upper triangular matrix. To reduce the computational burden, our method only requires the calculation of the block diagonal. We also introduced regularization to overcome matrix singularity. Although we only use the block-diagonal part of the Jacobian matrix, our algorithm still maintains global convergence and achieves a local quadratic convergence rate. Numerical results demonstrate the efficiency of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03566v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/02331934.2025.2574469</arxiv:DOI>
      <arxiv:journal_reference>Optimization, 1-24 (2025)</arxiv:journal_reference>
      <dc:creator>Yuge Ye, Qingna Li</dc:creator>
    </item>
    <item>
      <title>Policy Optimization in the Linear Quadratic Gaussian Problem: A Frequency Domain Perspective</title>
      <link>https://arxiv.org/abs/2508.17252</link>
      <description>arXiv:2508.17252v2 Announce Type: replace 
Abstract: The Linear Quadratic Gaussian (LQG) problem is a classic and widely studied model in optimal control, providing a fundamental framework for designing controllers for linear systems subject to process and observation noises. In recent years, researchers have increasingly focused on directly parameterizing dynamic controllers and optimizing the LQG cost over the resulting parameterized set. However, this parameterization typically gives rise to a highly non-convex optimization landscape for the resulting parameterized LQG problem. To our knowledge, there is currently no general method for certifying the global optimality of candidate controller parameters in this setting. In this work, we address these gaps with the following contributions. First, we derive a necessary and sufficient condition for the global optimality of stationary points in a parameterized LQG problems. This condition reduces the verification of optimality to a test of the controllability and observability for a novel, specially constructed transfer function, yielding a precise and computationally tractable certificate. Furthermore, our condition provides a rigorous explanation for why traditional parameterizations can lead to suboptimal stationary points. Second, we elevate the controller parameter space from conventional finite-dimensional settings to the infinite-dimensional $\mathcal{RH}_\infty$ space and develop a gradient-based algorithm in this setting, for which we provide a theoretical analysis establishing global convergence. Finally, representative numerical experiments validate the theoretical findings and demonstrate the practical viability of the proposed approach. Additionally, the appendix section explores a data-driven extension to the model-free setting, where we outline a parameter estimation scheme and demonstrate its practical viability through numerical simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17252v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Li, Xun Li, Yuan-Hua Ni, Xuebo Zhang</dc:creator>
    </item>
    <item>
      <title>AS-BOX: Additional Sampling Method for Weighted Sum Problems with Box Constraints</title>
      <link>https://arxiv.org/abs/2509.00547</link>
      <description>arXiv:2509.00547v2 Announce Type: replace 
Abstract: A class of optimization problems characterized by a weighted finite-sum objective function subject to box constraints is considered. We propose a novel stochastic optimization method, named AS-BOX (\text{A}ddi\-ti\-onal \text{S}ampling for \text{BOX} constraints), that combines projected gradient directions with adaptive variable sample size strategies and nonmonotone line search. The method dynamically adjusts the batch size based on progress with respect to the additional sampling function and on structural consistency of the projected direction, enabling practical adaptivity of AS-BOX, while ensuring theoretical support. We establish almost sure convergence under standard assumptions and provide complexity bounds. Numerical experiments demonstrate the efficiency and competitiveness of the proposed method compared to state-of-the-art algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00547v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nata\v{s}a Kreji\'c, Nata\v{s}a Krklec Jerinki\'c, Tijana Ostoji\'c, Nemanja Vu\v{c}i\'cevi\'c</dc:creator>
    </item>
    <item>
      <title>Qualification-free convex analysis via the joint supporting subspace</title>
      <link>https://arxiv.org/abs/2510.12244</link>
      <description>arXiv:2510.12244v3 Announce Type: replace 
Abstract: In convex analysis, qualification conditions (also termed constraint qualifications) help avoid pathological behavior at domain boundaries. In this work we remove the need for such conditions by localizing to an affine subspace -- the joint supporting subspace -- that contains the feasible region and ensures qualification conditions hold after localization.
  Our theory generalizes Borwein and Wolkowicz's facial reduction beyond conic programs to convex programs of the form $f(x) + g(Ax)$. Intuitively, the joint supporting subspace corresponds to a bilateral facial reduction between any two convex sets. It enables simple qualification-free generalizations for a host of central results of convex analysis. These include: an exact Fenchel-Rockafellar dual; Karush-Kuhn-Tucker (KKT) optimality conditions; attained infimal convolution for convex conjugates; subdifferential sum and chain rules; and a characterization of the normal cones of the intersection of two convex sets. All generalizations reduce seamlessly to their original formulations when qualification conditions hold.
  We offer a number of characterizations for the joint supporting subspace, one of which is constructive. Our proofs are self-contained, and introduce a novel theoretical framework consisting of nested normals and supporting subspaces, which simultaneously describe both the boundary of convex sets and the lattice of faces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12244v3</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew S. Scott</dc:creator>
    </item>
    <item>
      <title>Exponential Consensus through Z-Control in High-Order Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2511.19252</link>
      <description>arXiv:2511.19252v2 Announce Type: replace 
Abstract: In this work, we introduce a Z-control strategy for multi-agent systems of arbitrary order, aimed at driving the agents toward consensus in the highest-order observable state. The proposed framework supports both direct and indirect control schemes, making it applicable in scenarios where high-order derivatives such as acceleration cannot be directly manipulated. Theoretical analysis ensures exponential convergence while preserving the average dynamics, and a hierarchy of control laws is derived accordingly. Numerical experiments up to third-order models, including opinion dynamics and Cucker-Smale flocking systems, demonstrate the robustness and flexibility of Z-control under varying interaction regimes and control intensities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19252v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angela Monti, Fasma Diele</dc:creator>
    </item>
    <item>
      <title>Quantum Boltzmann machine learning of ground-state energies</title>
      <link>https://arxiv.org/abs/2410.12935</link>
      <description>arXiv:2410.12935v3 Announce Type: replace-cross 
Abstract: Estimating the ground-state energy of Hamiltonians is a fundamental task for which it is believed that quantum computers can be helpful. Several approaches have been proposed toward this goal, including algorithms based on quantum phase estimation and hybrid quantum-classical optimizers involving parameterized quantum circuits, the latter falling under the umbrella of the variational quantum eigensolver. Here, we analyze the performance of quantum Boltzmann machines for this task, which is a less explored ansatz based on parameterized thermal states and which is not known to suffer from the barren-plateau problem. We delineate a hybrid quantum-classical algorithm for this task and rigorously prove that it converges to an $\varepsilon$-approximate stationary point of the energy function optimized over parameter space, while using a number of parameterized-thermal-state samples that is polynomial in $\varepsilon^{-1}$, the number of parameters, and the norm of the Hamiltonian being optimized. Our algorithm estimates the gradient of the energy function efficiently by means of a quantum circuit construction that combines classical random sampling, Hamiltonian simulation, and the Hadamard test. Additionally, supporting our main claims are calculations of the gradient and Hessian of the energy function, as well as an upper bound on the matrix elements of the latter that is used in the convergence analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12935v3</guid>
      <category>quant-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dhrumil Patel, Daniel Koch, Saahil Patel, Mark M. Wilde</dc:creator>
    </item>
    <item>
      <title>Simultaneous recovery of a corroded boundary and admittance using the Kohn-Vogelius method</title>
      <link>https://arxiv.org/abs/2506.17938</link>
      <description>arXiv:2506.17938v3 Announce Type: replace-cross 
Abstract: We address the problem of identifying an unknown portion $\Gamma$ of the boundary of a $d$-dimensional ($d \in \{1, 2\}$) domain $\Omega$ and its associated Robin admittance coefficient, using two sets of boundary Cauchy data $(f, g)$--representing boundary temperature and heat flux--measured on the accessible portion $\Sigma$ of the boundary. Identifiability results \cite{Bacchelli2009,PaganiPierotti2009} indicate that a single measurement on $\Sigma$ is insufficient to uniquely determine both $\Gamma$ and $\alpha$, but two independent inputs yielding distinct solutions ensure the uniqueness of the pair $\Gamma$ and $\alpha$. In this paper, we propose a cost function based on the energy-gap of two auxiliary problems. We derive the variational derivatives of this objective functional with respect to both the Robin boundary $\Gamma$ and the admittance coefficient $\alpha$. These derivatives are utilized to develop a nonlinear gradient-based iterative scheme for the simultaneous numerical reconstruction of $\Gamma$ and $\alpha$. Numerical experiments are presented to demonstrate the effectiveness and practicality of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17938v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moustapha Essahraoui, Elmehdi Cherrat, Lekbir Afraites, Julius Fergy Tiongson Rabago</dc:creator>
    </item>
    <item>
      <title>Adjoint Schr\"odinger Bridge Sampler</title>
      <link>https://arxiv.org/abs/2506.22565</link>
      <description>arXiv:2506.22565v2 Announce Type: replace-cross 
Abstract: Computational methods for learning to sample from the Boltzmann distribution -- where the target distribution is known only up to an unnormalized energy function -- have advanced significantly recently. Due to the lack of explicit target samples, however, prior diffusion-based methods, known as diffusion samplers, often require importance-weighted estimation or complicated learning processes. Both trade off scalability with extensive evaluations of the energy and model, thereby limiting their practical usage. In this work, we propose Adjoint Schr\"odinger Bridge Sampler (ASBS), a new diffusion sampler that employs simple and scalable matching-based objectives yet without the need to estimate target samples during training. ASBS is grounded on a mathematical model -- the Schr\"odinger Bridge -- which enhances sampling efficiency via kinetic-optimal transportation. Through a new lens of stochastic optimal control theory, we demonstrate how SB-based diffusion samplers can be learned at scale via Adjoint Matching and prove convergence to the global solution. Notably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to arbitrary source distributions by relaxing the so-called memoryless condition that largely restricts the design space. Through extensive experiments, we demonstrate the effectiveness of ASBS on sampling from classical energy functions, amortized conformer generation, and molecular Boltzmann distributions. Code available at https://github.com/facebookresearch/adjoint_samplers</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22565v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guan-Horng Liu, Jaemoo Choi, Yongxin Chen, Benjamin Kurt Miller, Ricky T. Q. Chen</dc:creator>
    </item>
    <item>
      <title>Radial and Non-Radial Solution Structures for Quasilinear Hamilton--Jacobi--Bellman Equations in Bounded Settings</title>
      <link>https://arxiv.org/abs/2511.06277</link>
      <description>arXiv:2511.06277v2 Announce Type: replace-cross 
Abstract: We study quasilinear Hamilton--Jacobi--Bellman equations on bounded smooth convex domains. We show that the quasilinear Hamilton--Jacobi--Bellman equations arise naturally from stochastic optimal control problems with exit-time costs. The PDE is obtained via dynamic programming applied to controlled It\^{o} diffusions, providing both a probabilistic interpretation and a rigorous derivation. Our result establishes existence and uniqueness of positive classical solutions under sub-quadratic growth conditions on the source term. The constructive proofs, based on monotone iteration and barrier techniques, also provide a framework for algorithmic implementation with applications in production planning and image restoration. We provide complete detailed proofs with rigorous estimates and establish the connection to stochastic control theory through the dynamic programming principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06277v2</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dragos-Patru Covei</dc:creator>
    </item>
    <item>
      <title>Computable Characterisations of Scaled Relative Graphs of Closed Operators</title>
      <link>https://arxiv.org/abs/2511.08420</link>
      <description>arXiv:2511.08420v2 Announce Type: replace-cross 
Abstract: Scaled Relative Graphs (SRGs) provide a promising tool for stability and robustness analysis of multi-input-multi-output systems. In this paper, we provide tools for exact and computable constructions of the SRG for closed linear operators, based on maximum and minimum gain computations. The results are suitable for bounded and unbounded operators, and we specify how they can be used to draw SRGs for the typical operators that are used to model linear-time-invariant dynamical systems. Furthermore, for the special case of state-space models, we show how the Bounded Real Lemma can be used to construct the SRG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08420v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Talitha Nauta, Richard Pates</dc:creator>
    </item>
    <item>
      <title>Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games</title>
      <link>https://arxiv.org/abs/2511.11602</link>
      <description>arXiv:2511.11602v2 Announce Type: replace-cross 
Abstract: Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11602v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georgios C. Chasparis</dc:creator>
    </item>
  </channel>
</rss>
