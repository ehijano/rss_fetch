<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Oct 2025 01:45:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>High-Quality and Large-Scale Image Downscaling for Modern Display Devices</title>
      <link>https://arxiv.org/abs/2510.24334</link>
      <description>arXiv:2510.24334v1 Announce Type: new 
Abstract: In modern display technology and visualization tools, downscaling images is one of the most important activities. This procedure aims to maintain both visual authenticity and structural integrity while reducing the dimensions of an image at a large scale to fit the dimension of the display devices. In this study, we proposed a new technique for downscaling images that uses co-occurrence learning to maintain structural and perceptual information while reducing resolution. The technique uses the input image to create a data-driven co-occurrence profile that captures the frequency of intensity correlations in nearby neighborhoods. A refined filtering process is guided by this profile, which acts as a content-adaptive range kernel. The contribution of each input pixel is based on how closely it resembles pair-wise intensity values with it's neighbors. We validate our proposed technique on four datasets: DIV2K, BSD100, Urban100, and RealSR to show its effective downscaling capacity. Our technique could obtain up to 39.22 dB PSNR on the DIV2K dataset and PIQE up to 26.35 on the same dataset when downscaling by 8x and 16x, respectively. Numerous experimental findings attest to the ability of the suggested picture downscaling method to outperform more contemporary approaches in terms of both visual quality and performance measures. Unlike most existing methods, which did not focus on the large-scale image resizing scenario, we achieve high-quality downscaled images without texture loss or edge blurring. Our method, LSID (large scale image downscaling), successfully preserves high-frequency structures like edges, textures, and repeating patterns by focusing on statistically consistent pixels while reducing aliasing and blurring artifacts that are typical of traditional downscaling techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24334v1</guid>
      <category>eess.IV</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Suvrojit Mitra, G B Kevin Arjun, Sanjay Ghosh</dc:creator>
    </item>
    <item>
      <title>Fast algorithms enabling optimization and deep learning for photoacoustic tomography in a circular detection geometry</title>
      <link>https://arxiv.org/abs/2510.24687</link>
      <description>arXiv:2510.24687v1 Announce Type: new 
Abstract: The inverse source problem arising in photoacoustic tomography and in several other coupled-physics modalities is frequently solved by iterative algorithms. Such algorithms are based on the minimization of a certain cost functional. In addition, novel deep learning techniques are currently being investigated to further improve such optimization approaches. All such methods require multiple applications of the operator defining the forward problem, and of its adjoint. In this paper, we present new asymptotically fast algorithms for numerical evaluation of the forward and adjoint operators, applicable in the circular acquisition geometry. For an $(n \times n)$ image, our algorithms compute these operators in $\mathcal{O}(n^2 \log n)$ floating point operations. We demonstrate the performance of our algorithms in numerical simulations, where they are used as an integral part of several iterative image reconstruction techniques: classic variational methods, such as non-negative least squares and total variation regularized least squares, as well as deep learning methods, such as learned primal dual. A Python implementation of our algorithms and computational examples is available to the general public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24687v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Hauptmann, Leonid Kunyansky, Jenni Poimala</dc:creator>
    </item>
    <item>
      <title>Dipole-lets: a new multiscale decomposition for MR phase and quantitative susceptibility mapping</title>
      <link>https://arxiv.org/abs/2510.24705</link>
      <description>arXiv:2510.24705v1 Announce Type: new 
Abstract: Identifying and suppressing streaking artifacts is one of the most challenging problems in quantitative susceptibility mapping. The measured phase from tissue magnetization is assumed to be the convolution by the magnetic dipole kernel; direct inversion or standard regularization methods tend to create streaking artifacts in the estimated susceptibility. This is caused by extreme noise and by the presence of non-dipolar phase contributions, which are amplified by the dipole kernel following the streaking pattern. In this work, we introduce a multiscale transform, called Dipole-lets, as an optimal decomposition method for identifying dipole incompatibilities in measured field data by extracting features of different characteristic size and orientation with respect to the dipole kernel's zero-valued double-cone surface (the magic cone). We provide experiments that showcase that non-dipolar content can be extracted by Dipole-lets from phase data through artifact localization. We also present implementations of Dipole-lets as a optimization functional regularizator, through simple Tikhonov and infinity norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24705v1</guid>
      <category>eess.IV</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ignacio Contreras-Z\'u\~niga, Mathias Lambert, Benjam\'in Palacios, Cristian Tejos, Carlos Milovic</dc:creator>
    </item>
    <item>
      <title>Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models</title>
      <link>https://arxiv.org/abs/2510.23633</link>
      <description>arXiv:2510.23633v1 Announce Type: cross 
Abstract: Pretrained diffusion models have demonstrated strong capabilities in zero-shot inverse problem solving by incorporating observation information into the generation process of the diffusion models. However, this presents an inherent dilemma: excessive integration can disrupt the generative process, while insufficient integration fails to emphasize the constraints imposed by the inverse problem. To address this, we propose \emph{Noise Combination Sampling}, a novel method that synthesizes an optimal noise vector from a noise subspace to approximate the measurement score, replacing the noise term in the standard Denoising Diffusion Probabilistic Models process. This enables conditional information to be naturally embedded into the generation process without reliance on step-wise hyperparameter tuning. Our method can be applied to a wide range of inverse problem solvers, including image compression, and, particularly when the number of generation steps $T$ is small, achieves superior performance with negligible computational overhead, significantly improving robustness and stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23633v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xun Su, Hiroyuki Kasai</dc:creator>
    </item>
    <item>
      <title>Gut decisions based on the liver: A radiomics approach to boost colorectal cancer screening</title>
      <link>https://arxiv.org/abs/2510.23687</link>
      <description>arXiv:2510.23687v1 Announce Type: cross 
Abstract: Non-invasive colorectal cancer (CRC) screening represents a key opportunity to improve colonoscopy participation rates and reduce CRC mortality. This study explores the potential of the gut-liver axis for predicting colorectal neoplasia through liver-derived radiomic features extracted from routine CT images as a novel opportunistic screening approach. In this retrospective study, we analyzed data from 1,997 patients who underwent colonoscopy and abdominal CT. Patients either had no colorectal neoplasia (n=1,189) or colorectal neoplasia (n_total=808; adenomas n=423, CRC n=385). Radiomics features were extracted from 3D liver segmentations using the Radiomics Processing ToolKit (RPTK), which performed feature extraction, filtering, and classification. The dataset was split into training (n=1,397) and test (n=600) cohorts. Five machine learning models were trained with 5-fold cross-validation on the 20 most informative features, and the best model ensemble was selected based on the validation AUROC. The best radiomics-based XGBoost model achieved a test AUROC of 0.810, clearly outperforming the best clinical-only model (test AUROC: 0.457). Subclassification between colorectal cancer and adenoma showed lower accuracy (test AUROC: 0.674). Our findings establish proof-of-concept that liver-derived radiomics from routine abdominal CT can predict colorectal neoplasia. Beyond offering a pragmatic, widely accessible adjunct to CRC screening, this approach highlights the gut-liver axis as a novel biomarker source for opportunistic screening and sparks new mechanistic hypotheses for future translational research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23687v1</guid>
      <category>q-bio.QM</category>
      <category>eess.IV</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Hinterberger (DKFZ Hector Cancer Institute at the University Medical Center Mannheim, Germany, Junior Clinical Cooperation Unit Translational Molecular Imaging in Oncologic Therapy Monitoring), Jonas Bohn (Division of Medical Image Computing, German Cancer Research Center, Translational Lung Research Center, Faculty of Biosciences, Heidelberg University, Heidelberg, Germany, National Center for Tumor Diseases), Dasha Trofimova (Division of Medical Image Computing, German Cancer Research Center, Helmholtz Imaging, Heidelberg, Germany), Nicolas Knabe (Department of Radiology and Nuclear Medicine, University Medical Center Mannheim, Heidelberg University, Mannheim, Germany), Julia Dettling (Department of Radiology and Nuclear Medicine, University Medical Center Mannheim, Heidelberg University, Mannheim, Germany), Tobias Norajitra (Division of Medical Image Computing, German Cancer Research Center, Translational Lung Research Center, Pattern Analysis and Learning Group, Heidelberg University Hospital, Heidelberg, Germany), Fabian Isensee (Division of Medical Image Computing, German Cancer Research Center, Helmholtz Imaging, Heidelberg, Germany), Johannes Betge (DKFZ Hector Cancer Institute at the University Medical Center Mannheim, Germany, Department of Medicine II, University Medical Center Mannheim, Medical Faculty Mannheim, Mannheim, Germany, Junior Clinical Cooperation Unit Translational Gastrointestinal Oncology and Preclinical Models, German Cancer Research Center, Heidelberg, Germany, German Cancer Consortium, DKTK, Heidelberg, Germany), Stefan O. Sch\"onberg (Department of Radiology and Nuclear Medicine, University Medical Center Mannheim, Heidelberg University, Mannheim, Germany), Dominik N\"orenberg (Department of Radiology and Nuclear Medicine, University Medical Center Mannheim, Heidelberg University, Mannheim, Germany), Sergio Grosu (Department of Radiology, University Hospital, LMU Munich, Munich, Germany), Sonja Loges (DKFZ Hector Cancer Institute at the University Medical Center Mannheim, Germany, Division of Personalized Medical Oncology, Department of Personalized Oncology, University Hospital Mannheim, Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany), Ralf Floca (Division of Medical Image Computing, German Cancer Research Center, National Center for Tumor Diseases, Pattern Analysis and Learning Group, Heidelberg University Hospital, Heidelberg, Germany), Jakob Nikolas Kather (Else Kroener Fresenius Center for Digital Health, Faculty of Medicine and University Hospital Carl Gustav Carus, TUD Dresden University of Technology, Dresden, Germany, Department of Medicine I, University Hospital Dresden, Dresden, Germany, Medical Oncology, National Center for Tumor Diseases), Klaus Maier-Hein (Division of Medical Image Computing, German Cancer Research Center, Translational Lung Research Center, National Center for Tumor Diseases, Helmholtz Imaging, Heidelberg, Germany, Pattern Analysis and Learning Group, Heidelberg University Hospital, Heidelberg, Germany, Faculty of Medicine, University of Heidelberg, Heidelberg, Germany, Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany), Freba Grawe (DKFZ Hector Cancer Institute at the University Medical Center Mannheim, Germany, Junior Clinical Cooperation Unit Translational Molecular Imaging in Oncologic Therapy Monitoring, Department of Radiology and Nuclear Medicine, University Medical Center Mannheim, Heidelberg University, Mannheim, Germany)</dc:creator>
    </item>
    <item>
      <title>Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices</title>
      <link>https://arxiv.org/abs/2510.23775</link>
      <description>arXiv:2510.23775v1 Announce Type: cross 
Abstract: The increasing realism of AI-generated imagery poses challenges for verifying visual authenticity. We present an explainable image authenticity detection system that combines a lightweight convolutional classifier ("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify, localize, and explain artifacts in 32x32 images. Our model achieves 96.5% accuracy on the extended CiFAKE dataset augmented with adversarial perturbations and maintains an inference time of 175ms on 8-core CPUs, enabling deployment on local or edge devices. Using autoencoder-based reconstruction error maps, we generate artifact localization heatmaps, which enhance interpretability for both humans and the VLM. We further categorize 70 visual artifact types into eight semantic groups and demonstrate explainable text generation for each detected anomaly. This work highlights the feasibility of combining visual and linguistic reasoning for interpretable authenticity detection in low-resolution imagery and outlines potential cross-domain applications in forensics, industrial inspection, and social media moderation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23775v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aryan Mathur, Asaduddin Ahmed, Pushti Amit Vasoya, Simeon Kandan Sonar, Yasir Z, Madesh Kuppusamy</dc:creator>
    </item>
    <item>
      <title>Listening without Looking: Modality Bias in Audio-Visual Captioning</title>
      <link>https://arxiv.org/abs/2510.24024</link>
      <description>arXiv:2510.24024v1 Announce Type: cross 
Abstract: Audio-visual captioning aims to generate holistic scene descriptions by jointly modeling sound and vision. While recent methods have improved performance through sophisticated modality fusion, it remains unclear to what extent the two modalities are complementary in current audio-visual captioning models and how robust these models are when one modality is degraded. We address these questions by conducting systematic modality robustness tests on LAVCap, a state-of-the-art audio-visual captioning model, in which we selectively suppress or corrupt the audio or visual streams to quantify sensitivity and complementarity. The analysis reveals a pronounced bias toward the audio stream in LAVCap. To evaluate how balanced audio-visual captioning models are in their use of both modalities, we augment AudioCaps with textual annotations that jointly describe the audio and visual streams, yielding the AudioVisualCaps dataset. In our experiments, we report LAVCap baseline results on AudioVisualCaps. We also evaluate the model under modality robustness tests on AudioVisualCaps and the results indicate that LAVCap trained on AudioVisualCaps exhibits less modality bias than when trained on AudioCaps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24024v1</guid>
      <category>eess.AS</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchi Ishikawa, Toranosuke Manabe, Tatsuya Komatsu, Yoshimitsu Aoki</dc:creator>
    </item>
    <item>
      <title>Sound Source Localization for Spatial Mapping of Surgical Actions in Dynamic Scenes</title>
      <link>https://arxiv.org/abs/2510.24332</link>
      <description>arXiv:2510.24332v1 Announce Type: cross 
Abstract: Purpose: Surgical scene understanding is key to advancing computer-aided and intelligent surgical systems. Current approaches predominantly rely on visual data or end-to-end learning, which limits fine-grained contextual modeling. This work aims to enhance surgical scene representations by integrating 3D acoustic information, enabling temporally and spatially aware multimodal understanding of surgical environments.
  Methods: We propose a novel framework for generating 4D audio-visual representations of surgical scenes by projecting acoustic localization information from a phased microphone array onto dynamic point clouds from an RGB-D camera. A transformer-based acoustic event detection module identifies relevant temporal segments containing tool-tissue interactions which are spatially localized in the audio-visual scene representation. The system was experimentally evaluated in a realistic operating room setup during simulated surgical procedures performed by experts.
  Results: The proposed method successfully localizes surgical acoustic events in 3D space and associates them with visual scene elements. Experimental evaluation demonstrates accurate spatial sound localization and robust fusion of multimodal data, providing a comprehensive, dynamic representation of surgical activity.
  Conclusion: This work introduces the first approach for spatial sound localization in dynamic surgical scenes, marking a significant advancement toward multimodal surgical scene representations. By integrating acoustic and visual data, the proposed framework enables richer contextual understanding and provides a foundation for future intelligent and autonomous surgical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24332v1</guid>
      <category>cs.SD</category>
      <category>cs.CV</category>
      <category>eess.AS</category>
      <category>eess.IV</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jonas Hein, Lazaros Vlachopoulos, Maurits Geert Laurent Olthof, Bastian Sigrist, Philipp F\"urnstahl, Matthias Seibold</dc:creator>
    </item>
    <item>
      <title>Servo navigation and phase equalization enhanced by run-time stabilization (PEERS) for 3D EPI time series</title>
      <link>https://arxiv.org/abs/2505.03637</link>
      <description>arXiv:2505.03637v2 Announce Type: replace 
Abstract: Purpose: To enhance time-resolved segmented imaging by synergy of run-time stabilization and retrospective, data-driven phase correction. Methods: A segmented 3D EPI sequence for fMRI time series is equipped with servo navigation based on short orbital navigators and a linear perturbation model, enabling run-time correction for rigid-body motion as well as bulk phase and frequency fluctuation. Complementary retrospective phase correction is based on the repetitive structure of the time series and serves to address residual phase and frequency offsets. The combined approach is termed phase equalization enhanced by run-time stabilization (PEERS). Results: The proposed strategy is evaluated in a phantom and in-vivo. Servo navigation is found to diminish motion confound in raw data and maintain k-space consistency over time series. In turn, retrospective phase equalization is found to eliminate shot-wise phase and frequency offsets relative to the navigator, which are attributed to eddy-currents and vibrations from phase encoding. Retrospective phase equalization reduces the precision requirements for run-time frequency control, supporting the use of short navigators. Relative to conventional volume realignment, PEERS achieved tSNR improvements up to $30\%$ for small motion and in the order of $10\%$ when volunteers tried to hold still. Retrospective phase equalization is found to clearly outperform phase correction based solely on navigator-based frequency estimates. Conclusion: Servo navigation achieves high-precision run-time motion correction for 3D EPI fMRI. Coarse frequency tracking based on short navigators is supplemented by precise retrospective frequency and phase correction. Fully automatic and self-calibrated, PEERS offers effective plug-and-play motion and phase correction for 3D fMRI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03637v2</guid>
      <category>eess.IV</category>
      <category>physics.med-ph</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Malte Riedel, Thomas Ulrich, Samuel Bianchi, Klaas P. Pruessmann</dc:creator>
    </item>
    <item>
      <title>TraceTrans: Translation and Spatial Tracing for Surgical Prediction</title>
      <link>https://arxiv.org/abs/2510.22379</link>
      <description>arXiv:2510.22379v2 Announce Type: replace 
Abstract: Image-to-image translation models have achieved notable success in converting images across visual domains and are increasingly used for medical tasks such as predicting post-operative outcomes and modeling disease progression. However, most existing methods primarily aim to match the target distribution and often neglect spatial correspondences between the source and translated images. This limitation can lead to structural inconsistencies and hallucinations, undermining the reliability and interpretability of the predictions. These challenges are accentuated in clinical applications by the stringent requirement for anatomical accuracy. In this work, we present TraceTrans, a novel deformable image translation model designed for post-operative prediction that generates images aligned with the target distribution while explicitly revealing spatial correspondences with the pre-operative input. The framework employs an encoder for feature extraction and dual decoders for predicting spatial deformations and synthesizing the translated image. The predicted deformation field imposes spatial constraints on the generated output, ensuring anatomical consistency with the source. Extensive experiments on medical cosmetology and brain MRI datasets demonstrate that TraceTrans delivers accurate and interpretable post-operative predictions, highlighting its potential for reliable clinical deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22379v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiyu Luo, Haodong Li, Xinxing Cheng, He Zhao, Yang Hu, Xuan Song, Tianyang Zhang</dc:creator>
    </item>
    <item>
      <title>Caption-Driven Explainability: Probing CNNs for Bias via CLIP</title>
      <link>https://arxiv.org/abs/2510.22035</link>
      <description>arXiv:2510.22035v3 Announce Type: replace-cross 
Abstract: Robustness has become one of the most critical problems in machine learning (ML). The science of interpreting ML models to understand their behavior and improve their robustness is referred to as explainable artificial intelligence (XAI). One of the state-of-the-art XAI methods for computer vision problems is to generate saliency maps. A saliency map highlights the pixel space of an image that excites the ML model the most. However, this property could be misleading if spurious and salient features are present in overlapping pixel spaces. In this paper, we propose a caption-based XAI method, which integrates a standalone model to be explained into the contrastive language-image pre-training (CLIP) model using a novel network surgery approach. The resulting caption-based XAI model identifies the dominant concept that contributes the most to the models prediction. This explanation minimizes the risk of the standalone model falling for a covariate shift and contributes significantly towards developing robust ML models. Our code is available at https://github.com/patch0816/caption-driven-xai</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22035v3</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Koller (Northwestern University, Evanston, Illinois, United States), Amil V. Dravid (University of California, Berkeley, California, United States), Guido M. Schuster (Eastern Switzerland University of Applied Sciences, Rapperswil, St. Gallen, Switzerland), Aggelos K. Katsaggelos (Northwestern University, Evanston, Illinois, United States)</dc:creator>
    </item>
  </channel>
</rss>
