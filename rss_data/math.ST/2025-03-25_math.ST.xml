<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Mar 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Entropy and Crossentropy of Generalized Mallows Models</title>
      <link>https://arxiv.org/abs/2503.17521</link>
      <description>arXiv:2503.17521v1 Announce Type: new 
Abstract: The Generalized Mallows Model (GMM) is a well known family of models for ranking data. A GMM is a distribution over $\mathbb{S}_n$, the set of permutations of n objects, characterized by a location parameter $\sigma \in \mathbb{S}_n$, known as central permutation and a set of dispersion parameters $\theta_{1:n-1}\in(0,1]$. The GMM shares many properties, such as having sufficient statistics, with exponential models, thus it can be seen as an exponential family with a discrete parameter $\sigma$. This paper shows that computing entropy, crossentropy and Kullback-Leibler divergence in the the class of GMM is tractable, paving the way for a better understanding of this exponential family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17521v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marina Meil\u{a}</dc:creator>
    </item>
    <item>
      <title>Modeling of stochastic processes in $L_p(T)$ using orthogonal polynomials</title>
      <link>https://arxiv.org/abs/2503.17532</link>
      <description>arXiv:2503.17532v1 Announce Type: new 
Abstract: In this paper, models that approximate stochastic processes from the space $Sub_\varphi(\Omega)$ with given reliability and accuracy in $L_p(T)$ are considered for some specific functions $\varphi(t)$. For processes that are decomposited in series using orthonormal bases, such models are constructed in the case where elements of such decomposition cannot be found explicitly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17532v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.13189/ujam.2014.020304</arxiv:DOI>
      <arxiv:journal_reference>Universal Journal of Applied Mathematics Vol. 2(3), pp. 141 - 147, 2014</arxiv:journal_reference>
      <dc:creator>Oleksandr Mokliachuk</dc:creator>
    </item>
    <item>
      <title>Two-Sample Tests for Optimal Lifts, Manifold Stability and Reverse Labeling Reflection Shap</title>
      <link>https://arxiv.org/abs/2503.17879</link>
      <description>arXiv:2503.17879v1 Announce Type: new 
Abstract: We consider a quotient of a complete Riemannian manifold modulo an isometrically and properly acting Lie group and lifts of the quotient to the manifolds in optimal position to a reference point on the manifold. With respect to the pushed forward Riemannian volume onto the quotient we derive continuity and uniqueness a.e. and smoothness to large extents also with respect to the reference point. In consequence we derive a general manifold stability theorem: the Fr\'echet mean lies in the highest dimensional stratum assumed with positive probability, and a strong law for optimal lifts. This allows to define new two-sample tests utilizing individual optimal lifts which outperform existing two-sample tests on simulated data. They also outperform existing tests on a newly derived reverse labeling reflection shape space, that is used to model filament data of microtubules within cells in a biological application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17879v1</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Do Tran Van, Susovan Pal, Benjamin Eltzner, Stephan F. Huckemann</dc:creator>
    </item>
    <item>
      <title>Asymptotically uniformly most powerful tests for diffusion processes with nonsynchronous observations</title>
      <link>https://arxiv.org/abs/2503.18400</link>
      <description>arXiv:2503.18400v1 Announce Type: new 
Abstract: This paper introduces a quasi-likelihood ratio testing procedure for diffusion processes observed under nonsynchronous sampling schemes. High-frequency data, particularly in financial econometrics, are often recorded at irregular time points, challenging conventional synchronous methods for parameter estimation and hypothesis testing. To address these challenges, we develop a quasi-likelihood framework that accommodates irregular sampling while integrating adaptive estimation techniques for both drift and diffusion coefficients, thereby enhancing optimization stability and reducing computational burden. We rigorously derive the asymptotic properties of the proposed test statistic, showing that it converges to a chi-squared distribution under the null hypothesis and exhibits consistency under alternatives. Moreover, we establish that the resulting tests are asymptotically uniformly most powerful. Extensive numerical experiments corroborate the theoretical findings and demonstrate that our method outperforms existing nonparametric approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18400v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Teppei Ogihara, Futo Ueno</dc:creator>
    </item>
    <item>
      <title>Minimax Rate-Optimal Inference for Individualized Quantile Treatment Effects in High-dimensional Models</title>
      <link>https://arxiv.org/abs/2503.18523</link>
      <description>arXiv:2503.18523v1 Announce Type: new 
Abstract: The quantification of treatment effects plays an important role in a wide range of applications, including policy making and bio-pharmaceutical research. In this article, we study the quantile treatment effect (QTE) while addressing two specific types of heterogeneities: (a) personalized heterogeneity, which captures the varying treatment effects for different individuals, and (b) quantile heterogeneity, which accounts for how the impact of covariates varies across different quantile levels. A well-designed debiased estimator for the individualized quantile treatment effect (IQTE) is proposed to capture such heterogeneities effectively. We show that this estimator converges weakly to a Gaussian process as a function of the quantile levels and propose valid statistical inference methods, including the construction of confidence intervals and the development of hypothesis testing decision rules. In addition, the minimax optimality frameworks for these inference procedures are established. Specifically, we derive the minimax optimal rates for the expected length of confidence intervals and the magnitude of the detection boundary for hypothesis testing procedures, illustrating the superiority of the proposed estimator. The effectiveness of our methods is demonstrated through extensive simulations and an analysis of the National Health and Nutrition Examination Survey (NHANES) datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18523v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiachen Sun, Yin Xia</dc:creator>
    </item>
    <item>
      <title>Differentially Private Joint Independence Test</title>
      <link>https://arxiv.org/abs/2503.18721</link>
      <description>arXiv:2503.18721v1 Announce Type: new 
Abstract: Identification of joint dependence among more than two random vectors plays an important role in many statistical applications, where the data may contain sensitive or confidential information. In this paper, we consider the the d-variable Hilbert-Schmidt independence criterion (dHSIC) in the context of differential privacy. Given the limiting distribution of the empirical estimate of dHSIC is complicated Gaussian chaos, constructing tests in the non-privacy regime is typically based on permutation and bootstrap. To detect joint dependence in privacy, we propose a dHSIC-based testing procedure by employing a differentially private permutation methodology. Our method enjoys privacy guarantee, valid level and pointwise consistency, while the bootstrap counterpart suffers inconsistent power. We further investigate the uniform power of the proposed test in dHSIC metric and $L_2$ metric, indicating that the proposed test attains the minimax optimal power across different privacy regimes. As a byproduct, our results also contain the pointwise and uniform power of the non-private permutation dHSIC, addressing an unsolved question remained in Pfister et al. (2018).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18721v1</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingwei Liu, Yuexin Chen, Wangli Xu</dc:creator>
    </item>
    <item>
      <title>An improved central limit theorem for the empirical sliced Wasserstein distance</title>
      <link>https://arxiv.org/abs/2503.18831</link>
      <description>arXiv:2503.18831v1 Announce Type: new 
Abstract: Optimal transport theory has become a fundamental tool for handling diverse types of data, with growing applications across various fields. However, the Wasserstein distance presents significant computational and statistical challenges in high-dimensional settings. To address these issues, alternative distances such as the sliced Wasserstein distance, which leverages one-dimensional projections, have been introduced. In this work, we establish a novel central limit theorem for the p-sliced Wasserstein distance, for p&gt;1, using the Efron-Stein inequality-a technique that has proven effective in related problems. This approach yields a central limit theorem centered at the expected value of the empirical cost, under mild regularity conditions. Notably, unlike the general Wasserstein distance in arbitrary dimensions, we demonstrate that, under specific assumptions, the centering constants can be replaced by the population cost, which is essential for statistical inference. This generalizes and significantly refines existing results for the one-dimensional case. Consequently, we present the first asymptotically valid inference framework for the sliced Wasserstein distance applicable to measures that are not necessarily compactly supported, for p&gt;1. Finally, we address key practical aspects for inference, including Monte Carlo estimation of the integral and estimation of the asymptotic variance, ensuring applicability in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18831v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David Rodr\'iguez-V\'itores, Eustasio del Barrio, Jean-Michel Loubes</dc:creator>
    </item>
    <item>
      <title>Calibration Bands for Mean Estimates within the Exponential Dispersion Family</title>
      <link>https://arxiv.org/abs/2503.18896</link>
      <description>arXiv:2503.18896v1 Announce Type: new 
Abstract: A statistical model is said to be calibrated if the resulting mean estimates perfectly match the true means of the underlying responses. Aiming for calibration is often not achievable in practice as one has to deal with finite samples of noisy observations. A weaker notion of calibration is auto-calibration. An auto-calibrated model satisfies that the expected value of the responses being given the same mean estimate matches this estimate. Testing for auto-calibration has only been considered recently in the literature and we propose a new approach based on calibration bands. Calibration bands denote a set of lower and upper bounds such that the probability that the true means lie simultaneously inside those bounds exceeds some given confidence level. Such bands were constructed by Yang-Barber (2019) for sub-Gaussian distributions. Dimitriadis et al. (2023) then introduced narrower bands for the Bernoulli distribution and we use the same idea in order to extend the construction to the entire exponential dispersion family that contains for example the binomial, Poisson, negative binomial, gamma and normal distributions. Moreover, we show that the obtained calibration bands allow us to construct various tests for calibration and auto-calibration, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18896v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\L}ukasz Delong, Selim Gatti, Mario V. W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Efficient Knowledge Distillation via Curriculum Extraction</title>
      <link>https://arxiv.org/abs/2503.17494</link>
      <description>arXiv:2503.17494v1 Announce Type: cross 
Abstract: Knowledge distillation is a technique used to train a small student network using the output generated by a large teacher network, and has many empirical advantages~\citep{Hinton2015DistillingTK}. While the standard one-shot approach to distillation only uses the output of the final teacher network, recent work~\citep{panigrahi2024progressive} has shown that using intermediate checkpoints from the teacher's training process as an implicit ``curriculum'' for progressive distillation can significantly speed up training. However, such schemes require storing these checkpoints, and often require careful selection of the intermediate checkpoints to train on, which can be impractical for large-scale training.
  In this paper, we show that a curriculum can be \emph{extracted} from just the fully trained teacher network, and that this extracted curriculum can give similar efficiency benefits to those of progressive distillation. Our extraction scheme is natural; we use a random projection of the hidden representations of the teacher network to progressively train the student network, before training using the output of the full network. We show that our scheme significantly outperforms one-shot distillation and achieves a performance similar to that of progressive distillation for learning sparse parities with two-layer networks, and provide theoretical guarantees for this setting. Additionally, we show that our method outperforms one-shot distillation even when using transformer-based architectures, both for sparse-parity learning, and language modeling tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17494v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shivam Gupta, Sushrut Karmalkar</dc:creator>
    </item>
    <item>
      <title>A Statistical Theory of Contrastive Learning via Approximate Sufficient Statistics</title>
      <link>https://arxiv.org/abs/2503.17538</link>
      <description>arXiv:2503.17538v1 Announce Type: cross 
Abstract: Contrastive learning -- a modern approach to extract useful representations from unlabeled data by training models to distinguish similar samples from dissimilar ones -- has driven significant progress in foundation models. In this work, we develop a new theoretical framework for analyzing data augmentation-based contrastive learning, with a focus on SimCLR as a representative example. Our approach is based on the concept of \emph{approximate sufficient statistics}, which we extend beyond its original definition in \cite{oko2025statistical} for contrastive language-image pretraining (CLIP) using KL-divergence. We generalize it to equivalent forms and general f-divergences, and show that minimizing SimCLR and other contrastive losses yields encoders that are approximately sufficient. Furthermore, we demonstrate that these near-sufficient encoders can be effectively adapted to downstream regression and classification tasks, with performance depending on their sufficiency and the error induced by data augmentation in contrastive learning. Concrete examples in linear regression and topic classification are provided to illustrate the broad applicability of our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17538v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Licong Lin, Song Mei</dc:creator>
    </item>
    <item>
      <title>A new tail bound for the sum of bounded independent random variables</title>
      <link>https://arxiv.org/abs/2503.17594</link>
      <description>arXiv:2503.17594v1 Announce Type: cross 
Abstract: We construct a new tail bound for the sum of independent random variables for situations in which the expected value of the sum is known and each random variable lies within a specified interval, which may be different for each variable. This new bound can be computed by solving a two-dimensional convex optimization problem. Simulations demonstrate that the new bound is often substantially tighter than Hoeffding's inequality for cases in which both bounds are applicable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17594v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jackson Loper, Jeffrey Regier</dc:creator>
    </item>
    <item>
      <title>Poisson-Process Topic Model for Integrating Knowledge from Pre-trained Language Models</title>
      <link>https://arxiv.org/abs/2503.17809</link>
      <description>arXiv:2503.17809v1 Announce Type: cross 
Abstract: Topic modeling is traditionally applied to word counts without accounting for the context in which words appear. Recent advancements in large language models (LLMs) offer contextualized word embeddings, which capture deeper meaning and relationships between words. We aim to leverage such embeddings to improve topic modeling.
  We use a pre-trained LLM to convert each document into a sequence of word embeddings. This sequence is then modeled as a Poisson point process, with its intensity measure expressed as a convex combination of $K$ base measures, each corresponding to a topic. To estimate these topics, we propose a flexible algorithm that integrates traditional topic modeling methods, enhanced by net-rounding applied before and kernel smoothing applied after. One advantage of this framework is that it treats the LLM as a black box, requiring no fine-tuning of its parameters. Another advantage is its ability to seamlessly integrate any traditional topic modeling approach as a plug-in module, without the need for modifications
  Assuming each topic is a $\beta$-H\"{o}lder smooth intensity measure on the embedded space, we establish the rate of convergence of our method. We also provide a minimax lower bound and show that the rate of our method matches with the lower bound when $\beta\leq 1$. Additionally, we apply our method to several datasets, providing evidence that it offers an advantage over traditional topic modeling approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17809v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Morgane Austern, Yuanchuan Guo, Zheng Tracy Ke, Tianle Liu</dc:creator>
    </item>
    <item>
      <title>Non-Bayesian Learning in Misspecified Models</title>
      <link>https://arxiv.org/abs/2503.18024</link>
      <description>arXiv:2503.18024v1 Announce Type: cross 
Abstract: Deviations from Bayesian updating are traditionally categorized as biases, errors, or fallacies, thus implying their inherent ``sub-optimality.'' We offer a more nuanced view. We demonstrate that, in learning problems with misspecified models, non-Bayesian updating can outperform Bayesian updating.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18024v1</guid>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Bervoets, Mathieu Faure, Ludovic Renou</dc:creator>
    </item>
    <item>
      <title>Recovering a (1+1)-dimensional wave equation from a single white noise boundary measurement</title>
      <link>https://arxiv.org/abs/2503.18515</link>
      <description>arXiv:2503.18515v1 Announce Type: cross 
Abstract: We consider the following inverse problem: Suppose a $(1+1)$-dimensional wave equation on $\mathbb R_+$ with zero initial conditions is excited with a Neumann boundary data modelled as a white noise process. Given also the Dirichlet data at the same point, determine the unknown first order coefficient function of the system.
  We first establish that direct problem is well-posed. The inverse problem is then solved by showing that correlations of the boundary data determine the Neumann-to-Dirichlet operator in the sense of distributions, which is known to uniquely identify the coefficient. This approach has applications in acoustic measurements of internal cross-sections of fluid pipes such as pressurised water supply pipes and vocal tract shape determination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18515v1</guid>
      <category>math.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emilia L. K. Bl{\aa}sten, Tapio Helin, Antti Kujanp\"a\"a, Lauri Oksanen, Jesse Railo</dc:creator>
    </item>
    <item>
      <title>AutoBayes: A Compositional Framework for Generalized Variational Inference</title>
      <link>https://arxiv.org/abs/2503.18608</link>
      <description>arXiv:2503.18608v1 Announce Type: cross 
Abstract: We introduce a new compositional framework for generalized variational inference, clarifying the different parts of a model, how they interact, and how they compose. We explain that both exact Bayesian inference and the loss functions typical of variational inference (such as variational free energy and its generalizations) satisfy chain rules akin to that of reverse-mode automatic differentiation, and we advocate for exploiting this to build and optimize models accordingly. To this end, we construct a series of compositional tools: for building models; for constructing their inversions; for attaching local loss functions; and for exposing parameters. Finally, we explain how the resulting parameterized statistical games may be optimized locally, too. We illustrate our framework with a number of classic examples, pointing to new areas of extensibility that are revealed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18608v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Toby St Clere Smithe, Marco Perin</dc:creator>
    </item>
    <item>
      <title>Confidence set for mixture order selection</title>
      <link>https://arxiv.org/abs/2503.18790</link>
      <description>arXiv:2503.18790v1 Announce Type: cross 
Abstract: A fundamental challenge in the application of finite mixture models is selecting the number of mixture components, also known as order. Traditional approaches rely on selecting a single best model using information criteria. However, in the presence of noisy data, and when models with different orders yield similar fits, model selection uncertainty can be substantial, making it challenging to confidently identify the true number of components. In this paper, we introduce the Model Selection Confidence Set (MSCS) for order selection - a set-valued estimator that, with a predefined confidence level, includes the true mixture order across repeated samples. Rather than selecting a single model, our MSCS identifies all plausible orders by determining whether each candidate model is at least as plausible as the best-selected one, using a screening test based on a penalized likelihood ratio statistic. We provide theoretical guarantees for the asymptotic coverage of our confidence set and demonstrate its practical advantages through simulations and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18790v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Casa, Davide Ferrari</dc:creator>
    </item>
    <item>
      <title>Benign landscapes for synchronization on spheres via normalized Laplacian matrices</title>
      <link>https://arxiv.org/abs/2503.18801</link>
      <description>arXiv:2503.18801v1 Announce Type: cross 
Abstract: We study the nonconvex optimization landscapes of synchronization problems on spheres. First, we present new results for the statistical problem of synchronization over the two-element group $\mathbf{Z}_2$. We consider the nonconvex least-squares problem with $\mathbf{Z}_2 = \{\pm 1\}$ relaxed to the unit sphere in $\mathbf{R}^r$ for $r \geq 2$; for several popular models, including graph clustering under the binary stochastic block model, we show that, for any $r \geq 2$, every second-order critical point recovers the ground truth in the asymptotic regimes where exact recovery is information-theoretically possible. Such statistical optimality via spherical relaxations had previously only been shown for (potentially arbitrarily) larger relaxation dimension $r$. Second, we consider the global synchronization of networks of coupled oscillators under the (homogeneous) Kuramoto model. We prove new and optimal asymptotic results for random signed networks on an Erd\H{o}s--R\'enyi graph, and we give new and simple proofs for several existing state-of-the-art results. Our key tool is a deterministic landscape condition that extends a recent result of Rakoto Endor and Waldspurger. This result says that, if a certain problem-dependent Laplacian matrix has small enough condition number, the nonconvex landscape is benign. Our extension allows the condition number to include an arbitrary diagonal preconditioner, which gives tighter results for many problems. We show that, for the synchronization of Kuramoto oscillator networks on nearest-neighbor circulant graphs as studied by Wiley, Strogatz, and Girvan, this condition is optimal. We also prove a natural complex extension that may be of interest for synchronization on the special orthogonal group $\operatorname{SO}(2)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18801v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew D. McRae</dc:creator>
    </item>
    <item>
      <title>Maximum a posteriori testing in statistical inverse problems</title>
      <link>https://arxiv.org/abs/2402.00686</link>
      <description>arXiv:2402.00686v5 Announce Type: replace 
Abstract: This paper is concerned with a Bayesian approach to testing hypotheses in statistical inverse problems. Based on the posterior distribution $\Pi \left(\cdot |Y = y\right)$, we want to infer whether a feature $\langle\varphi, u^\dagger\rangle$ of the unknown quantity of interest $u^\dagger$ is positive. This can be done by the so-called maximum a posteriori test. We provide a frequentistic analysis of this test's properties such as level and power, and prove that it is a regularized test in the sense of Kretschmann et al. (2024). Furthermore we provide lower bounds for its power under classical spectral source conditions in case of Gaussian priors. Numerical simulations illustrate its superior performance both in moderately and severely ill-posed situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00686v5</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Remo Kretschmann, Frank Werner</dc:creator>
    </item>
    <item>
      <title>The Limits of Assumption-free Tests for Algorithm Performance</title>
      <link>https://arxiv.org/abs/2402.07388</link>
      <description>arXiv:2402.07388v3 Announce Type: replace 
Abstract: Algorithm evaluation and comparison are fundamental questions in machine learning and statistics -- how well does an algorithm perform at a given modeling task, and which algorithm performs best? Many methods have been developed to assess algorithm performance, often based around cross-validation type strategies, retraining the algorithm of interest on different subsets of the data and assessing its performance on the held-out data points. Despite the broad use of such procedures, the theoretical properties of these methods are not yet fully understood. In this work, we explore some fundamental limits for answering these questions with limited amounts of data. In particular, we make a distinction between two questions: how good is an algorithm $A$ at the problem of learning from a training set of size $n$, versus, how good is a particular fitted model produced by running $A$ on a particular training data set of size $n$?
  Our main results prove that, for any test that treats the algorithm $A$ as a ``black box'' (i.e., we can only study the behavior of $A$ empirically), there is a fundamental limit on our ability to carry out inference on the performance of $A$, unless the number of available data points $N$ is many times larger than the sample size $n$ of interest. (On the other hand, evaluating the performance of a particular fitted model is easy as long as a holdout data set is available -- that is, as long as $N-n$ is not too small.) We also ask whether an assumption of algorithmic stability might be sufficient to circumvent this hardness result. Surprisingly, we find that this is not the case: the same hardness result still holds for the problem of evaluating the performance of $A$, aside from a high-stability regime where fitted models are essentially nonrandom. Finally, we also establish similar hardness results for the problem of comparing multiple algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07388v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuetian Luo, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>Asymptotic Theory for Linear Functionals of Kernel Ridge Regression</title>
      <link>https://arxiv.org/abs/2403.04248</link>
      <description>arXiv:2403.04248v2 Announce Type: replace 
Abstract: An asymptotic theory is established for linear functionals of the predictive function given by kernel ridge regression, when the reproducing kernel Hilbert space is equivalent to a Sobolev space. The theory covers a wide variety of linear functionals, including point evaluations, evaluation of derivatives, $L_2$ inner products, etc. We establish the upper and lower bounds of the estimates and their asymptotic normality. It is shown that $\lambda\sim n^{-1}$ is the universal optimal order of magnitude for the smoothing parameter to balance the variance and the worst-case bias. The theory also implies that the optimal $L_\infty$ error of kernel ridge regression can be attained under the optimal smoothing parameter $\lambda\sim n^{-1}\log n$. These optimal rates for the smoothing parameter differ from the known optimal rate $\lambda\sim n^{-\frac{2m}{2m+d}}$ that minimizes the $L_2$ error of the kernel ridge regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04248v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Tuo, Lu Zou</dc:creator>
    </item>
    <item>
      <title>The empirical copula process in high dimensions: Stute's representation and applications</title>
      <link>https://arxiv.org/abs/2405.05597</link>
      <description>arXiv:2405.05597v2 Announce Type: replace 
Abstract: The empirical copula process, a fundamental tool for copula inference, is studied in the high dimensional regime where the dimension is allowed to grow to infinity exponentially in the sample size. Under natural, weak smoothness assumptions on the underlying copula, it is shown that Stute's representation is valid in the following sense: all low-dimensional margins of fixed dimension of the empirical copula process can be approximated by a functional of the low-dimensional margins of the standard empirical process, with the almost sure error term being uniform in the margins. The result has numerous potential applications, and is exemplary applied to the problem of testing pairwise stochastic independence in high dimensions, leading to various extensions of recent results in the literature: for certain test statistics based on pairwise association measures, type-I error control is obtained for models beyond mutual independence. Moreover, bootstrap-based critical values are shown to yield strong control of the familywise error rate for a large class of data generating processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05597v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axel B\"ucher, Cambyse Pakzad</dc:creator>
    </item>
    <item>
      <title>Regularized e-processes: anytime valid inference with knowledge-based efficiency gains</title>
      <link>https://arxiv.org/abs/2410.01427</link>
      <description>arXiv:2410.01427v3 Announce Type: replace 
Abstract: Classical statistical methods have theoretical justification when the sample size is predetermined. In applications, however, it's often the case that sample sizes are data-dependent rather than predetermined. The aforementioned methods aren't reliable in this latter case, hence the recent interest in e-processes and methods that are anytime valid, i.e., reliable for any dynamic data-collection plan. But if the investigator has relevant-yet-incomplete prior information about the quantity of interest, then there's an opportunity for efficiency gain. This paper proposes a regularized e-process framework featuring a knowledge-based, imprecise-probabilistic regularization with improved efficiency. A generalized version of Ville's inequality is established, ensuring that inference based on the regularized e-process are anytime valid in a novel, knowledge-dependent sense. Regularized e-processes also facilitate possibility-theoretic uncertainty quantification with strong frequentist-like calibration properties and other Bayesian-like properties: satisfies the likelihood principle, avoids sure-loss, and offers formal decision-making with reliability guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01427v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Mean Estimation in Banach Spaces Under Infinite Variance and Martingale Dependence</title>
      <link>https://arxiv.org/abs/2411.11271</link>
      <description>arXiv:2411.11271v2 Announce Type: replace 
Abstract: We consider estimating the shared mean of a sequence of heavy-tailed random variables taking values in a Banach space. In particular, we revisit and extend a simple truncation-based mean estimator first proposed by Catoni and Giulini. While existing truncation-based approaches require a bound on the raw (non-central) second moment of observations, our results hold under a bound on either the central or non-central $p$th moment for some $p \in (1,2]$. Our analysis thus handles distributions with infinite variance. The main contributions of the paper follow from exploiting connections between truncation-based mean estimation and the concentration of martingales in smooth Banach spaces. We prove two types of time-uniform bounds on the distance between the estimator and unknown mean: line-crossing inequalities, which can be optimized for a fixed sample size $n$, and iterated logarithm inequalities, which match the tightness of line-crossing inequalities at all points in time up to a doubly logarithmic factor in $n$. Our results do not depend on the dimension of the Banach space, hold under martingale dependence, and all constants in the inequalities are known and small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11271v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin Whitehouse, Ben Chugg, Diego Martinez-Taboada, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Precision and Cholesky Factor Estimation for Gaussian Processes</title>
      <link>https://arxiv.org/abs/2412.08820</link>
      <description>arXiv:2412.08820v2 Announce Type: replace 
Abstract: This paper studies the estimation of large precision matrices and Cholesky factors obtained by observing a Gaussian process at many locations. Under general assumptions on the precision and the observations, we show that the sample complexity scales poly-logarithmically with the size of the precision matrix and its Cholesky factor. The key challenge in these estimation tasks is the polynomial growth of the condition number of the target matrices with their size. For precision estimation, our theory hinges on an intuitive local regression technique on the lattice graph which exploits the approximate sparsity implied by the screening effect. For Cholesky factor estimation, we leverage a block-Cholesky decomposition recently used to establish complexity bounds for sparse Cholesky factorization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08820v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaheng Chen, Daniel Sanz-Alonso</dc:creator>
    </item>
    <item>
      <title>Sobol' Matrices For Multi-Output Models With Quantified Uncertainty</title>
      <link>https://arxiv.org/abs/2501.04602</link>
      <description>arXiv:2501.04602v3 Announce Type: replace 
Abstract: Variance based global sensitivity analysis measures the relevance of inputs to a single output using Sobol' indices. This paper extends the definition in a natural way to multiple outputs, directly measuring the relevance of inputs to the linkages between outputs in a correlation-like matrix of indices. The usual Sobol' indices constitute the diagonal of this matrix. Existence, uniqueness and uncertainty quantification are established by developing the indices from a putative multi-output model with quantified uncertainty. Sobol' matrices and their standard errors are related to the moments of the multi-output model, to enable calculation. These are benchmarked numerically against test functions (with added noise) whose Sobol' matrices are calculated analytically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04602v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert A. Milton, Solomon F. Brown</dc:creator>
    </item>
    <item>
      <title>Glivenko-Cantelli for $f$-divergence</title>
      <link>https://arxiv.org/abs/2503.17355</link>
      <description>arXiv:2503.17355v2 Announce Type: replace 
Abstract: We extend the celebrated Glivenko-Cantelli theorem, sometimes called the fundamental theorem of statistics, from its standard setting of total variation distance to all $f$-divergences. A key obstacle in this endeavor is to define $f$-divergence on a subcollection of a $\sigma$-algebra that forms a $\pi$-system but not a $\sigma$-subalgebra. This is a side contribution of our work. We will show that this notion of $f$-divergence on the $\pi$-system of rays preserves nearly all known properties of standard $f$-divergence, yields a novel integral representation of the Kolmogorov-Smirnov distance, and has a Glivenko-Cantelli theorem. We will also discuss the prospects of a Vapnik-Chervonenkis theory for $f$-divergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17355v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoming Wang, Lek-Heng Lim</dc:creator>
    </item>
    <item>
      <title>The Local Approach to Causal Inference under Network Interference</title>
      <link>https://arxiv.org/abs/2105.03810</link>
      <description>arXiv:2105.03810v5 Announce Type: replace-cross 
Abstract: We propose a new nonparametric modeling framework for causal inference when outcomes depend on how agents are linked in a social or economic network. Such network interference describes a large literature on treatment spillovers, social interactions, social learning, information diffusion, disease and financial contagion, social capital formation, and more. Our approach works by first characterizing how an agent is linked in the network using the configuration of other agents and connections nearby as measured by path distance. The impact of a policy or treatment assignment is then learned by pooling outcome data across similarly configured agents. We demonstrate the approach by deriving finite-sample bounds on the mean-squared error of a k-nearest-neighbor estimator for the average treatment response as well as proposing an asymptotically valid test for the hypothesis of policy irrelevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.03810v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Auerbach, Hongchang Guo, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>Confidences in Hypotheses</title>
      <link>https://arxiv.org/abs/2111.10715</link>
      <description>arXiv:2111.10715v5 Announce Type: replace-cross 
Abstract: This article extends the hypotheses assessment method to the case with two competing simple hypotheses. In doing so we further clarify the benefits that hypotheses assessments can bring to classical statistical analyses. Given that confidences in hypotheses are based on conditional probabilities, we address the issue of what to condition on in order to avoid poor conditional properties. This step is essential if the resulting inferences are to be relevant to the data at hand. Admissibility is addressed within a framework of seeking confidences that are relevant to the data at hand and are as powerful as the application allows. Confidence procedures are said to be consistent if they are free of super-relevant betting strategies. For simple hypotheses, the assessment method produces minimum and maximum confidences in each hypothesis. Assessments for both symmetric and asymmetric experiments are included, and the relationship with Bayesian posterior probabilities discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.10715v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Graham N. Bornholt</dc:creator>
    </item>
    <item>
      <title>Embedding Network Autoregression for time series analysis and causal peer effect inference</title>
      <link>https://arxiv.org/abs/2406.05944</link>
      <description>arXiv:2406.05944v2 Announce Type: replace-cross 
Abstract: We propose an Embedding Network Autoregressive Model for multivariate networked longitudinal data. We assume the network is generated from a latent variable model, and these unobserved variables are included in a structural peer effect model or a time series network autoregressive model as additive effects. This approach takes a unified view of two related yet fundamentally different problems: (1) modeling and predicting multivariate networked time series data and (2) causal peer influence estimation in the presence of homophily from finite time longitudinal data. Our estimation strategy comprises estimating latent variables from the observed network followed by least squares estimation of the network autoregressive model. We show that the estimated momentum and peer effect parameters are consistent and asymptotically normally distributed in setups with a growing number of network vertices (N) while considering both a growing number of time points T (for the time series problem) and finite T cases (for the peer effect problem). We allow the number of latent vectors K to grow at appropriate rates, which improves upon existing rates when such results are available for related models. Our theoretical results encompass cases both when the network is modeled with the random dot product graph model (ENAR) and a more general latent space model with both additive and multiplicative effects (AMNAR). We also develop a selection criterion when K is unknown that provably does not under-select and show that the theoretical guarantees hold with the selected number for K as well. Interestingly, even though we propose a unified model, our theoretical results find that different growth rates and restrictions on the latent vectors are needed to induce omitted variable bias in the peer effect problem and to ensure consistent estimation in the time series problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05944v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jae Ho Chang, Subhadeep Paul</dc:creator>
    </item>
    <item>
      <title>How well behaved is finite dimensional Diffusion Maps?</title>
      <link>https://arxiv.org/abs/2412.03992</link>
      <description>arXiv:2412.03992v2 Announce Type: replace-cross 
Abstract: Under a set of assumptions on a family of submanifolds $\subset {\mathbb R}^D$, we derive a series of geometric properties that remain valid after finite-dimensional and almost isometric Diffusion Maps (DM), including almost uniform density, finite polynomial approximation and reach. Leveraging these properties, we establish rigorous bounds on the embedding errors introduced by the DM algorithm is $O\left((\frac{\log n}{n})^{\frac{1}{8d+16}}\right)$. Furthermore, we quantify the error between the estimated tangent spaces and the true tangent spaces over the submanifolds after the DM embedding,
  $\sup_{P\in \mathcal{P}}\mathbb{E}_{P^{\otimes \tilde{n}}} \max_{1\leq j \angle (T_{Y_{\varphi(M),j}}\varphi(M),\hat{T}_j)\leq \tilde{n}} \leq C \left(\frac{\log n }{n}\right)^\frac{k-1}{(8d+16)k}$,
  which providing a precise characterization of the geometric accuracy of the embeddings. These results offer a solid theoretical foundation for understanding the performance and reliability of DM in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03992v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenyu Bo (Department of Statistics University of Washington Seattle, WA), Marina Meil\u{a} (Department of Statistics University of Washington Seattle, WA)</dc:creator>
    </item>
    <item>
      <title>Sequential Monte Carlo with Gaussian Mixture Approximation for Infinite-Dimensional Statistical Inverse Problems</title>
      <link>https://arxiv.org/abs/2503.16028</link>
      <description>arXiv:2503.16028v2 Announce Type: replace-cross 
Abstract: By formulating the inverse problem of partial differential equations (PDEs) as a statistical inference problem, the Bayesian approach provides a general framework for quantifying uncertainties. In the inverse problem of PDEs, parameters are defined on an infinite-dimensional function space, and the PDEs induce a computationally intensive likelihood function. Additionally, sparse data tends to lead to a multi-modal posterior. These features make it difficult to apply existing sequential Monte Carlo (SMC) algorithms. To overcome these difficulties, we propose new conditions for the likelihood functions, construct a Gaussian mixture based preconditioned Crank-Nicolson transition kernel, and demonstrate the universal approximation property of the infinite-dimensional Gaussian mixture probability measure. By combining these three novel tools, we propose a new SMC algorithm, named SMC-GM. For this new algorithm, we obtain a convergence theorem that allows Gaussian priors, illustrating that the sequential particle filter actually reproduces the true posterior distribution. Furthermore, the proposed new algorithm is rigorously defined on the infinite-dimensional function space, naturally exhibiting the discretization-invariant property. Numerical experiments demonstrate that the new approach has a strong ability to probe the multi-modality of the posterior, significantly reduces the computational burden, and numerically exhibits the discretization-invariant property (important for large-scale problems).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16028v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyu Lu, Junxiong Jia, Deyu Meng</dc:creator>
    </item>
  </channel>
</rss>
