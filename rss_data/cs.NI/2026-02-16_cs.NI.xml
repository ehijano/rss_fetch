<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Feb 2026 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On Borrowed Time: Measurement-Informed Understanding of the NTP Pool's Robustness to Monopoly Attacks</title>
      <link>https://arxiv.org/abs/2602.12321</link>
      <description>arXiv:2602.12321v1 Announce Type: new 
Abstract: Internet services and applications depend critically on the availability and acc uracy of network time. The Network Time Protocol (NTP) is one of the oldest core network protocols and remains the de facto mechanism for clock synchronization across the Internet today. While multiple NTP infrastructures exist, one, the "NTP Pool," presents an attractive attack target for two basic reasons, it is: 1) administratively distributed and based on volunteer servers; and 2) heavily utilized, including by IoT and infrastructure devices worldwide. We %develop measurements to gather the first direct, non-inferential, and comprehensive data on the NTP pool, including: longitudinal server and account membership, server configurations, time quality, aliases, and global query traffic load.
  We gather complete and granular data over a nine month period to discover over 15k servers (both active and inactive) and shed new light into the NTP Pool's use, dynamics, and robustness. By analyzing address aliases, accounts, and network connectivity, we find that only 19.7% of the pool's active servers are fully independent. Finally, we show that an adversary informed with our data can better and more precisely mount "monopoly attacks" to capture the preponderance of NTP pool traffic in 90% of all countries with only 10 or fewer malicious NTP servers. Our results suggest multiple avenues by which the robustness of the pool can be improved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12321v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.14722/ndss.2026.240541</arxiv:DOI>
      <dc:creator>Robert Beverly, Erik Rye</dc:creator>
    </item>
    <item>
      <title>Tracking The Trackers: Commercial Surveillance Occurring on U.S. Army Networks</title>
      <link>https://arxiv.org/abs/2602.12388</link>
      <description>arXiv:2602.12388v1 Announce Type: new 
Abstract: Despite current security implementations, Internet activity on DoD networks is susceptible to web trackers and commercial data collection, which have the potential to expose information about service members and unit operations. This report documents the outcomes of a study to characterize web tracking occurring on Army CONUS unclassified networks. We derived a dataset from the Cloud-Based Internet Isolation (CBII) platform, encompassing data measured over a two-month period in 2024. This dataset comprised the 1,000 most frequently accessed Internet resources, determined by the number of connection requests on CONUS DoDIN-A during the study period. We then compared all domains and subdomains in the dataset against Ghostery's WhoTracks.me, an open-source database of commercial tracking entities. We found that over 21% of the domains accessed during the study period were Internet trackers. The ACI recommends that the Army implement changes to its enterprise networks to limit commercial Internet-based tracking, as well as policy changes towards the same end. With relatively minor configuration changes, CBII can serve as a more effective mitigation against risks posed by commercially available information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12388v1</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Master, Jaclyn Fox, Nicolas Starck, Maxwell Love, Benjamin Allison</dc:creator>
    </item>
    <item>
      <title>Generalizing UxV Network Control Optimization with Disruption Tolerant Networking</title>
      <link>https://arxiv.org/abs/2602.12448</link>
      <description>arXiv:2602.12448v1 Announce Type: new 
Abstract: Military and disaster relief operations increasingly rely on unmanned vehicles (UxVs). It is important to develop a network control system (NCS) that can continuously coordinate and optimize the movement of UxVs based on mission objectives. However, prior research on NCS aims to always maintain a connected network topology, which limits the utility of the resulting systems. In this paper, we present an approach to systematically increase the topology flexibility for an NCS by leveraging the well-studied concept of disruption-tolerant networking (DTN). We design a DTN-compatible communication utility model that, while allowing some nodes to temporarily disconnect from others, provides for a fine-grain specification of the minimum communication frequency and the maximum hops permitted for message delivery between each pair of nodes. As such, the model supports what-if analyses before a mission to determine the best communication parameters to use for a given set of UxVs. Furthermore, we incorporate our communication model into an existing NCS and evaluate its performance in a simulated scenario involving the use of five UxVs searching for an enemy ship. The results show that our model not only enables the NCS to find the enemy ship faster but also facilitates new capabilities, such as dividing the UxVs into multiple teams responsible for different search areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12448v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Quyen Dang, Geoffrey Xie</dc:creator>
    </item>
    <item>
      <title>Photonic Rails in ML Datacenters with Opus</title>
      <link>https://arxiv.org/abs/2602.12521</link>
      <description>arXiv:2602.12521v1 Announce Type: new 
Abstract: Rail-optimized network fabrics have become the de facto datacenter scale-out fabric for large-scale ML training. However, the use of high-radix electrical switches to provide all-to-all connectivity in rails imposes massive power and cost. We propose a rethinking of the rail abstraction by retaining its communication semantics, but realizing it using optical circuit switches. The key challenge is that optical switches support one-to-one connectivity at a time, limiting the fan-out of traffic in ML workloads using hybrid parallelisms. We overcome this through \emph{parallelism-driven rail reconfiguration}, which exploits the non-overlapping communication phases of different parallelism dimensions. This time-multiplexes a single set of physical ports across circuit configurations tailored to each phase within a training iteration. We design and implement Opus, a control plane that orchestrates this in-job reconfiguration of photonic rails at parallelism phase boundaries, and evaluate it on a physical OCS testbed, the Perlmutter supercomputer, and in simulation at up to 2,048 GPUs. Our results show that photonic rails can achieve over $23\times$ network power reduction and $4\times$ cost savings while incurring less than $6\%$ training overhead at production-relevant OCS reconfiguration latencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12521v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Ding, Barry Lyu, Bhaskar Kataria, Rachee Singh</dc:creator>
    </item>
    <item>
      <title>Adaptive Meta-Aggregation Federated Learning for Intrusion Detection in Heterogeneous Internet of Things</title>
      <link>https://arxiv.org/abs/2602.12541</link>
      <description>arXiv:2602.12541v1 Announce Type: new 
Abstract: The rapid proliferation of the Internet of Things (IoT) has brought remarkable advancements to industries by enabling interconnected systems and intelligent automation. However, this exponential growth has also introduced significant security vulnerabilities, making IoT networks increasingly targets for sophisticated cyberattacks. The heterogeneity of IoT devices poses critical challenges for traditional intrusion detection systems. To address these challenges, this paper proposes an innovative method called Adaptive Meta-Aggregation Federated Learning (AMAFed), designed to enhance intrusion detection in heterogeneous IoT networks. By employing a dynamic weighting mechanism using meta-learning, AMAFed assigns adaptive importance to local models based on their data quality and contributions, enabling personalized yet collaborative learning across devices. The proposed method was evaluated on three benchmark IoT datasets: ToN-IoT, N-BaIoT, and BoT-IoT, representing diverse real-world scenarios. Experimental results demonstrate that AMAFed achieves detection accuracy up to 99.8% on ToN-IoT, with F1-scores exceeding 98% across all datasets. On the N-BaIoT dataset, it reaches 99.88% accuracy, and on BoT-IoT, it achieves 98.12% accuracy, consistently outperforming state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12541v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saadat Izadi, Mahmood Ahmadi</dc:creator>
    </item>
    <item>
      <title>Lightweight Cluster-Based Federated Learning for Intrusion Detection in Heterogeneous IoT Networks</title>
      <link>https://arxiv.org/abs/2602.12543</link>
      <description>arXiv:2602.12543v1 Announce Type: new 
Abstract: The rise of heterogeneous Internet of Things (IoT) devices has raised security concerns due to their vulnerability to cyberattacks. Intrusion Detection Systems (IDS) are crucial in addressing these threats. Federated Learning (FL) offers a privacy-preserving solution, but IoT heterogeneity and limited computational resources cause increased latency and reduced performance. This paper introduces a novel approach Cluster-based federated intrusion detection with lightweight networks for heterogeneous IoT designed to address these limitations. The proposed framework utilizes a hierarchical IoT architecture that encompasses edge, fog, and cloud layers. Intrusion detection clients operate at the fog layer, leveraging federated learning to enhance data privacy and distributed processing efficiency. To enhance efficiency, the method employs the lightweight MobileNet model alongside a hybrid loss function that integrates Gumbel-SoftMax and SoftMax, optimizing resource consumption while maintaining high detection accuracy. A key feature of this approach is clustering IoT devices based on hardware similarities, enabling more efficient model training and aggregation tailored to each cluster's computational capacity. This strategy not only simplifies the complexity of managing heterogeneous data and devices but also improves scalability and overall system performance. To validate the effectiveness of the proposed method, extensive experiments were conducted using the ToN-IoT and CICDDoS2019 datasets. Results demonstrate that the proposed approach reduces end-to-end training time by 2.47x compared to traditional FL methods, achieves 2.16x lower testing latency, and maintains exceptionally high detection accuracy of 99.22% and 99.02% on the ToN-IoT and CICDDoS2019 datasets, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12543v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saadat Izadi, Mahmood Ahmadi</dc:creator>
    </item>
    <item>
      <title>CF-HFC:Calibrated Federated based Hardware-aware Fuzzy Clustering for Intrusion Detection in Heterogeneous IoTs</title>
      <link>https://arxiv.org/abs/2602.12557</link>
      <description>arXiv:2602.12557v1 Announce Type: new 
Abstract: The rapid expansion of heterogeneous Internet of Things (IoT) environments has heightened security risks, as resource-constrained devices remain vulnerable to diverse cyberattacks. Federated Learning (FL) has emerged as a privacy-preserving paradigm for collaborative intrusion detection; however, device and data heterogeneity introduce major challenges, including straggler delays, unstable convergence, and unbalanced error rates. This paper presents a Calibrated Federated Learning method with Hardware-aware Fuzzy Clustering (CF-HFC) to enhance intrusion detection performance in heterogeneous IoT networks. The proposed three-tier Edge-Fog-Cloud architecture integrates three complementary components: (1) hardware-aware fuzzy clustering, which organizes clients by computational capacity to mitigate straggler effects; (2) Fuzzy-FedProx aggregation, which stabilizes optimization under non-IID data distributions; and (3) Adaptive Conformal Calibration (ACC), which dynamically adjusts decision thresholds to balance false negative and false positive rates. Extensive experiments on ToN-IoT, BoT-IoT, Edge-IIoTset, and CICDDoS2019 datasets demonstrate that CF-HFC outperforms baseline methods such as FedAvg and FedProx, achieving over 99% detection accuracy, faster convergence, and lower communication latency. Overall, the results verify that CF-HFC effectively mitigates both device- and data-level heterogeneity, compared to existing federated learning approaches, providing accurate and efficient intrusion detection across Heterogeneous IoTs environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12557v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saadat Izadi, Mahmood Ahmadi</dc:creator>
    </item>
    <item>
      <title>Artic: AI-oriented Real-time Communication for MLLM Video Assistant</title>
      <link>https://arxiv.org/abs/2602.12641</link>
      <description>arXiv:2602.12641v1 Announce Type: new 
Abstract: AI Video Assistant emerges as a new paradigm for Real-time Communication (RTC), where one peer is a Multimodal Large Language Model (MLLM) deployed in the cloud. This makes interaction between humans and AI more intuitive, akin to chatting with a real person. However, a fundamental mismatch exists between current RTC frameworks and AI Video Assistants, stemming from the drastic shift in Quality of Experience (QoE) and more challenging networks. Measurements on our production prototype also confirm that current RTC fails, causing latency spikes and accuracy drops.
  To address these challenges, we propose Artic, an AI-oriented RTC framework for MLLM Video Assistants, exploring the shift from "humans watching video" to "AI understanding video." Specifically, Artic proposes: (1) Response Capability-aware Adaptive Bitrate, which utilizes MLLM accuracy saturation to proactively cap bitrate, reserving bandwidth headroom to absorb future fluctuations for latency reduction; (2) Zero-overhead Context-aware Streaming, which allocates limited bitrate to regions most important for the response, maintaining accuracy even under ultra-low bitrates; and (3) Degraded Video Understanding Benchmark, the first benchmark evaluating how RTC-induced video degradation affects MLLM accuracy. Prototype experiments using real-world uplink traces show that compared with existing methods, Artic significantly improves accuracy by 15.12% and reduces latency by 135.31 ms. We will release the benchmark and codes at https://github.com/pku-netvideo/DeViBench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12641v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiangkai Wu, Zhiyuan Ren, Junquan Zhong, Liming Liu, Xinggong Zhang</dc:creator>
    </item>
    <item>
      <title>PEMI: Transparent Performance Enhancements for QUIC</title>
      <link>https://arxiv.org/abs/2602.12732</link>
      <description>arXiv:2602.12732v1 Announce Type: new 
Abstract: QUIC, as the transport layer of the next-generation Web stack (HTTP/3), natively provides security and performance improvements over TCP-based stacks. However, since QUIC provides end-to-end encryption for both data and packet headers, in-network assistance like Performance-Enhancing Proxy (PEP) is unavailable for QUIC. To achieve the similar optimization as TCP, some works seek to collaborate endpoints and middleboxes to provide in-network assistance for QUIC. But involving both host and in-network devices increases the difficulty of deployment in the Internet.
  In this paper, by analyzing the QUIC standard, implementations, and the locality of application traffic, we identify opportunities for transparent middleboxes to measure RTT and infer packet loss for QUIC connections, despite the absence of plaintext ACK information. We then propose PEMI as a concrete system that continuously measures RTT and infers lost packets, enabling fast retransmissions for QUIC. PEMI enables performance enhancement for QUIC in a completely transparent manner, without requiring any explicit cooperation from the endpoints. To keep fairness, PEMI employs a delay-based congestion control and utilizes feedback-based methods to enforce CWND. Extensive evaluation results, including Mininet and trace-driven dynamic experiments, show that PEMI can significantly improve the performance of QUIC. For example, in the Mininet experiments, PEMI increases the goodput of file transfers by up to 2.5$\times$, and reduces the 90th percentile jitter of RTC frames by 20-75%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12732v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Zhang, Lei Zhang, Ziyi Wang, Chenxiang Sun, Yuming Hu, Xiaohui Xie, Zeqi Lai, Yong Cui</dc:creator>
    </item>
    <item>
      <title>Chimera: Neuro-Symbolic Attention Primitives for Trustworthy Dataplane Intelligence</title>
      <link>https://arxiv.org/abs/2602.12851</link>
      <description>arXiv:2602.12851v1 Announce Type: new 
Abstract: Deploying expressive learning models directly on programmable dataplanes promises line-rate, low-latency traffic analysis but remains hindered by strict hardware constraints and the need for predictable, auditable behavior. Chimera introduces a principled framework that maps attention-oriented neural computations and symbolic constraints onto dataplane primitives, enabling trustworthy inference within the match-action pipeline. Chimera combines a kernelized, linearized attention approximation with a two-layer key-selection hierarchy and a cascade fusion mechanism that enforces hard symbolic guarantees while preserving neural expressivity. The design includes a hardware-aware mapping protocol and a two-timescale update scheme that together permit stable, line-rate operation under realistic dataplane budgets. The paper presents the Chimera architecture, a hardware mapping strategy, and empirical evidence showing that neuro-symbolic attention primitives can achieve high-fidelity inference within the resource envelope of commodity programmable switches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12851v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rong Fu, Wenxin Zhang, Xiaowen Ma, Kun Liu, Wangyu Wu, Ziyu Kong, Jia Yee Tan, Tailong Luo, Xianda Li, Zeli Su, Youjin Wang, Yongtai Liu, Simon Fong</dc:creator>
    </item>
    <item>
      <title>TENORAN: Automating Fine-grained Energy Efficiency Profiling in Open RAN Systems</title>
      <link>https://arxiv.org/abs/2602.13085</link>
      <description>arXiv:2602.13085v1 Announce Type: new 
Abstract: The transition to disaggregated and interoperable Open Radio Access Network (RAN) architectures and the introduction of RAN Intelligent Controllers (RICs) in O-RAN creates new resource optimization opportunities and fine-grained tuning and configuration of network components to save energy while fulfilling service demand. However, unlocking this potential requires fine-grained and accurate energy measurements across heterogeneous deployments. Three factors make this particularly challenging [...]. To address these challenges, we design the TENORAN framework, an automated measurement scaffold for fine-grained energy efficiency profiling of O-RAN deployments, and prototype it on a heterogeneous OpenShift cluster. TENORAN instruments an end-to-end deployment based on high-level specifications (e.g., gNB software stack and split options, traffic profiles), and collects synchronized performance metrics and power measurements for individual RAN components while the network is under controlled workloads including over-the-air traffic. Our experimental results demonstrate energy profiling of end-to-end experiments with xApps in the loop, energy efficiency differences between two RAN stacks, OpenAirInterface and srsRAN, in uplink and downlink, and core network power consumption trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13085v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ravis Shirkhani, Stefano Maxenti, Leonardo Bonati, Niloofar Mohamadi, Maxime Elkael, Umair Hashmi, Jeebak Mitra, Michele Polese, Tommaso Melodia, Salvatore D'Oro</dc:creator>
    </item>
    <item>
      <title>Resource-Adaptive Teleportation Under Imperfect Entanglement: A Code-Puncturing Framework</title>
      <link>https://arxiv.org/abs/2602.12309</link>
      <description>arXiv:2602.12309v1 Announce Type: cross 
Abstract: Quantum teleportation is a foundational protocol for sending quantum information through entanglement distribution and classical communication. Assuming ideal classical communication, the reliability of quantum teleportation is limited by the fidelity of the shared EPR pairs. This reliability can be improved through two mechanisms: entanglement purification and quantum error correction (QEC). Using both techniques in concert requires flexible QEC rates, since purification alters the structure of errors induced by imperfect-EPR teleportation, and fixed-rate codes cannot be uniformly effective across purification regimes or reliability targets. In this work, we supplement purification with punctured QEC codes, providing a family of code variants that can be adapted to error-channel characteristics and reliability targets. Punctured codes improve teleportation reliability across a broader range of purification regimes, enabling target reliability to be met without hardware-level code switching. This is corroborated by numerical results, showing that different punctured codes achieve the lowest logical error probability in different operating regimes, and that selecting among them reduces logical error relative to fixed-rate encoded teleportation. This reduction relaxes the requirement on the initial EPR fidelity or purification needed to achieve a target reliability. Overall, puncturing enables adaptation to varying entanglement conditions and reliability requirements while reusing a single stabilizer structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12309v1</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahmoud Saad Abouamer, Jaron Skovsted Gundersen, S{\o}ren Pilegaard Rasmussen, Petar Popovski</dc:creator>
    </item>
    <item>
      <title>Arcalis: Accelerating Remote Procedure Calls Using a Lightweight Near-Cache Solution</title>
      <link>https://arxiv.org/abs/2602.12596</link>
      <description>arXiv:2602.12596v1 Announce Type: cross 
Abstract: Modern microservices increasingly depend on high-performance remote procedure calls (RPCs) to coordinate fine-grained, distributed computation. As network bandwidths continue to scale, the CPU overhead associated with RPC processing, particularly serialization, deserialization, and protocol handling, has become a critical bottleneck. This challenge is exacerbated by fast user-space networking stacks such as DPDK, which expose RPC processing as the dominant performance limiter. While prior work has explored software optimizations and FPGA-based offload engines, these approaches remain physically distant from the CPU's memory hierarchy, incurring unnecessary data movement and cache pollution. We present Arcalis, a near-cache RPC accelerator that positions a lightweight hardware engine adjacent to the last-level cache (LLC). Arcalis offloads RPC processing to dedicated microengines on receive and transmit paths that operate with cache-line latency while preserving programmability. By decoupling RPC processing logic, enabling microservice-specific execution, and positioning itself near the LLC to immediately consume data injected by network cards, Arcalis achieves 1.79-4.16$\times$ end-to-end speedup compared to the CPU baseline, while significantly reducing microarchitectural overhead by up to 88%, and achieves up to a 1.62$\times$ higher throughput than prior solutions. These results highlight the potential of near-cache RPC acceleration as a practical solution for high-performance microservice deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12596v1</guid>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johnson Umeike (University of Maryland, College Park), Pongstorn Maidee (AMD Research,Advanced Development), Bahar Asgari (University of Maryland, College Park)</dc:creator>
    </item>
    <item>
      <title>Can Neural Networks Provide Latent Embeddings for Telemetry-Aware Greedy Routing?</title>
      <link>https://arxiv.org/abs/2602.12798</link>
      <description>arXiv:2602.12798v1 Announce Type: cross 
Abstract: Telemetry-Aware routing promises to increase efficacy and responsiveness to traffic surges in computer networks. Recent research leverages Machine Learning to deal with the complex dependency between network state and routing, but sacrifices explainability of routing decisions due to the black-box nature of the proposed neural routing modules. We propose \emph{Placer}, a novel algorithm using Message Passing Networks to transform network states into latent node embeddings. These embeddings facilitate quick greedy next-hop routing without directly solving the all-pairs shortest paths problem, and let us visualize how certain network events shape routing decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12798v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Boltres, Niklas Freymuth, Gerhard Neumann</dc:creator>
    </item>
    <item>
      <title>Reliable Hierarchical Operating System Fingerprinting via Conformal Prediction</title>
      <link>https://arxiv.org/abs/2602.12825</link>
      <description>arXiv:2602.12825v1 Announce Type: cross 
Abstract: Operating System (OS) fingerprinting is critical for network security, but conventional methods do not provide formal uncertainty quantification mechanisms. Conformal Prediction (CP) could be directly wrapped around existing methods to obtain prediction sets with guaranteed coverage. However, a direct application of CP would treat OS identification as a flat classification problem, ignoring the natural taxonomic structure of OSs and providing brittle point predictions. This work addresses these limitations by introducing and evaluating two distinct structured CP strategies: level-wise CP (L-CP), which calibrates each hierarchy level independently, and projection-based CP (P-CP), which ensures structural consistency by projecting leaf-level sets upwards. Our results demonstrate that, while both methods satisfy validity guarantees, they expose a fundamental trade-off between level-wise efficiency and structural consistency. L-CP yields tighter prediction sets suitable for human forensic analysis but suffers from taxonomic inconsistencies. Conversely, P-CP guarantees hierarchically consistent, nested sets ideal for automated policy enforcement, albeit at the cost of reduced efficiency at coarser levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12825v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rub\'en P\'erez-Jove, Osvaldo Simeone, Alejandro Pazos, Jose V\'azquez-Naya</dc:creator>
    </item>
    <item>
      <title>Backdoor Attacks on Contrastive Continual Learning for IoT Systems</title>
      <link>https://arxiv.org/abs/2602.13062</link>
      <description>arXiv:2602.13062v1 Announce Type: cross 
Abstract: The Internet of Things (IoT) systems increasingly depend on continual learning to adapt to non-stationary environments. These environments can include factors such as sensor drift, changing user behavior, device aging, and adversarial dynamics. Contrastive continual learning (CCL) combines contrastive representation learning with incremental adaptation, enabling robust feature reuse across tasks and domains. However, the geometric nature of contrastive objectives, when paired with replay-based rehearsal and stability-preserving regularization, introduces new security vulnerabilities. Notably, backdoor attacks can exploit embedding alignment and replay reinforcement, enabling the implantation of persistent malicious behaviors that endure through updates and deployment cycles. This paper provides a comprehensive analysis of backdoor attacks on CCL within IoT systems. We formalize the objectives of embedding-level attacks, examine persistence mechanisms unique to IoT deployments, and develop a layered taxonomy tailored to IoT. Additionally, we compare vulnerabilities across various learning paradigms and evaluate defense strategies under IoT constraints, including limited memory, edge computing, and federated aggregation. Our findings indicate that while CCL is effective for enhancing adaptive IoT intelligence, it may also elevate long-lived representation-level threats if not adequately secured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13062v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alfous Tim, Kuniyilh Simi D</dc:creator>
    </item>
    <item>
      <title>Load Balancing for AI Training Workloads</title>
      <link>https://arxiv.org/abs/2507.21372</link>
      <description>arXiv:2507.21372v2 Announce Type: replace 
Abstract: The extreme bandwidth demands of AI training has made load-balancing a critical component in AI fabrics, and a variety of load-balancing designs have emerged in recent work from both industry and research. However, there is currently little consensus on which design approach dominates or the conditions under which an approach dominates. We also lack an understanding of how far these approaches are from optimal.
  We provide a technical foundation for answering these questions by systematically evaluating leading load-balancing designs, while decoupling them from specific congestion control and loss recovery stacks. We find that load-balancing based on packet spraying dominates traditional approaches that load balance traffic at flow, flowlet, or subflow granularities. When comparing host- vs switch-based approaches to packet spraying, we find that they perform similarly in failure-free scenarios but that a host-based approach dominates under link failure because of its rapid visibility into end-to-end path conditions. We also identify that no leading approach achieves optimal O(1) queue scaling at maximum utilization. We demonstrate why a destination-based rotation (DR) discipline can reach this optimum and introduce Ofan, a switch-based implementation of DR that we show offers valuable performance gains over other packet spraying approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21372v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah McClure, Evyatar Cohen, Alex Shpiner, Mark Silberstein, Sylvia Ratnasamy, Scott Shenker, Isaac Keslassy</dc:creator>
    </item>
    <item>
      <title>On the Power Saving in High-Speed Ethernet-based Networks for Supercomputers and Data Centers</title>
      <link>https://arxiv.org/abs/2510.19783</link>
      <description>arXiv:2510.19783v3 Announce Type: replace 
Abstract: The increase in computation and storage has led to a significant growth in the scale of systems powering applications and services, raising concerns about sustainability and operational costs. In this paper, we explore power-saving techniques in high-performance computing (HPC) and datacenter networks, and their relation with performance degradation. From this premise, we propose leveraging Energy Efficient Ethernet (EEE) protocol, with the flexibility to extend to conventional Ethernet or upcoming Ethernet-derived interconnect versions of BXI and Omnipath.
  We analyze the PerfBound power-saving mechanism, identifying possible improvements and modeling it into a simulation framework. Through different experiments, we examine its impact on performance and determine the most appropriate interconnect. We also study traffic patterns generated by selected HPC and machine learning applications to evaluate the behavior of power-saving techniques.
  From these experiments, we provide an analysis of how applications affect system and network energy consumption. Based on this, we disclose the weakness of dynamic power-down mechanisms and propose an approach that improves energy reduction with minimal or no performance penalty. To the best of our knowledge, this work presents the first thorough analysis of PerfBound and an enhancement to the technique, while also targeting emerging post-exascale networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19783v3</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Miguel S\'anchez de la Rosa, Francisco J. and\'ujar, Jesus Escudero-Sahuquillo, Jos\'e L. S\'anchez, Francisco J. Alfaro-Cort\'es</dc:creator>
    </item>
    <item>
      <title>Learning-based Radio Link Failure Prediction Based on Measurement Dataset in Railway Environments</title>
      <link>https://arxiv.org/abs/2511.08851</link>
      <description>arXiv:2511.08851v3 Announce Type: replace 
Abstract: This paper presents a measurement-driven case study on early radio link failure (RLF) warning as device-side network sensing and analytics for proactive mobility management in 5G non-standalone (NSA) railway environments. Using 10~Hz metro-train measurement traces with serving- and neighbor-cell indicators, we benchmark six representative learning models, including CNN, LSTM, XGBoost, Anomaly Transformer, PatchTST, and TimesNet, under multiple observation windows and prediction horizons. Rather than proposing a new prediction architecture, this study focuses on quantifying the feasibility of early warning and the trade-offs among observation context, prediction horizon, and alarm reliability under real railway mobility. Experimental results show that learning models can anticipate RLF-related reliability degradation seconds in advance using lightweight features available on commercial devices. The presented benchmark provides practical insights for sensing-assisted communication control, such as proactive redundancy activation and adaptive handover strategies, aligning with the 6G vision of integrating sensing and analytics into mobility control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08851v3</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Po-Heng Chou, Da-Chih Lin, Hung-Yu Wei, Walid Saad, Yu Tsao</dc:creator>
    </item>
    <item>
      <title>Designing Transport-Level Encryption for Datacenter Networks</title>
      <link>https://arxiv.org/abs/2406.15686</link>
      <description>arXiv:2406.15686v3 Announce Type: replace-cross 
Abstract: Cloud applications need network data encryption to isolate from other tenants and protect their data from potential eavesdroppers in the network infrastructure. This paper presents SMT, a protocol design for emerging datacenter transport protocols, such as NDP and Homa, to integrate data encryption. SMT integrates TLS-based encryption with a message-based transport protocol that supports efficient Remote Procedure Calls (RPCs), a common workload in datacenters. This architecture enables the use of per-message record sequence number spaces in a secure session, while ensuring unique message identities to prevent replay attacks. It also enables the use of existing NIC offloads designed for TLS over TCP, while being a native transport protocol alongside TCP and UDP. We implement SMT in the Linux kernel by extending Homa/Linux and improve RPC throughput by up to 41 % and latency by up to 35 % in comparison to TLS/TCP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15686v3</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/SP63933.2026.00080</arxiv:DOI>
      <dc:creator>Tianyi Gao, Xinshu Ma, Suhas Narreddy, Eugenio Luo, Steven W. D. Chien, Michio Honda</dc:creator>
    </item>
    <item>
      <title>DRL-Based Beam Positioning for LEO Satellite Constellations with Weighted Least Squares</title>
      <link>https://arxiv.org/abs/2511.08852</link>
      <description>arXiv:2511.08852v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a reinforcement learning based beam weighting framework that couples a policy network with an augmented weighted least squares (WLS) estimator for accurate and low-complexity positioning in multi-beam LEO constellations. Unlike conventional geometry or CSI-dependent approaches, the policy learns directly from uplink pilot responses and geometry features, enabling robust localization without explicit CSI estimation. An augmented WLS jointly estimates position and receiver clock bias, improving numerical stability under dynamic beam geometry. Across representative scenarios, the proposed method reduces the mean positioning error by 99.3% compared with the geometry-based baseline, achieving 0.395 m RMSE with near real-time inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08852v2</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Po-Heng Chou, Chiapin Wang, Kuan-Hao Chen, Wei-Chen Hsiao</dc:creator>
    </item>
  </channel>
</rss>
