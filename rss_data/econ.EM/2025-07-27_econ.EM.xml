<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Jul 2025 04:05:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Batched Adaptive Network Formation</title>
      <link>https://arxiv.org/abs/2507.18961</link>
      <description>arXiv:2507.18961v1 Announce Type: new 
Abstract: Networks are central to many economic and organizational applications, including workplace team formation, social platform recommendations, and classroom friendship development. In these settings, networks are modeled as graphs, with agents as nodes, agent pairs as edges, and edge weights capturing pairwise production or interaction outcomes. This paper develops an adaptive, or \textit{online}, policy that learns to form increasingly effective networks as data accumulates over time, progressively improving total network output measured by the sum of edge weights.
  Our approach builds on the weighted stochastic block model (WSBM), which captures agents' unobservable heterogeneity through discrete latent types and models their complementarities in a flexible, nonparametric manner. We frame the online network formation problem as a non-standard \textit{batched multi-armed bandit}, where each type pair corresponds to an arm, and pairwise reward depends on type complementarity. This strikes a balance between exploration -- learning latent types and complementarities -- and exploitation -- forming high-weighted networks. We establish two key results: a \textit{batched local asymptotic normality} result for the WSBM and an asymptotic equivalence between maximum likelihood and variational estimates of the intractable likelihood. Together, they provide a theoretical foundation for treating variational estimates as normal signals, enabling principled Bayesian updating across batches. The resulting posteriors are then incorporated into a tailored maximum-weight matching problem to determine the policy for the next batch. Simulations show that our algorithm substantially improves outcomes within a few batches, yields increasingly accurate parameter estimates, and remains effective even in nonstationary settings with evolving agent pools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18961v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Xu, Bo Zhou</dc:creator>
    </item>
    <item>
      <title>Flexible estimation of skill formation models</title>
      <link>https://arxiv.org/abs/2507.18995</link>
      <description>arXiv:2507.18995v1 Announce Type: new 
Abstract: This paper examines estimation of skill formation models, a critical component in understanding human capital development and its effects on individual outcomes. Existing estimators are either based on moment conditions and only applicable in specific settings or rely on distributional approximations that often do not align with the model. Our method employs an iterative likelihood-based procedure, which flexibly estimates latent variable distributions and recursively incorporates model restrictions across time periods. This approach reduces computational complexity while accommodating nonlinear production functions and measurement systems. Inference can be based on a bootstrap procedure that does not require re-estimating the model for bootstrap samples. Monte Carlo simulations and an empirical application demonstrate that our estimator outperforms existing methods, whose estimators can be substantially biased or noisy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18995v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonia Antweiler, Joachim Freyberger</dc:creator>
    </item>
    <item>
      <title>Interactive, Grouped and Non-separable Fixed Effects: A Practitioner's Guide to the New Panel Data Econometrics</title>
      <link>https://arxiv.org/abs/2507.19099</link>
      <description>arXiv:2507.19099v1 Announce Type: new 
Abstract: The past 20 years have brought fundamental advances in modeling unobserved heterogeneity in panel data. Interactive Fixed Effects (IFE) proved to be a foundational framework, generalizing the standard one-way and two-way fixed effects models by allowing the unit-specific unobserved heterogeneity to be interacted with unobserved time-varying common factors, allowing for more general forms of omitted variables. The IFE framework laid the theoretical foundations for other forms of heterogeneity, such as grouped fixed effects (GFE) and non-separable two-way fixed effects (NSTW). The existence of IFE, GFE or NSTW has significant implications for identification, estimation, and inference, leading to the development of many new estimators for panel data models. This paper provides an accessible review of the new estimation methods and their associated diagnostic tests, and offers a guide to empirical practice. In two separate empirical investigations we demonstrate that there is empirical support for the new forms of fixed effects and that the results can differ significantly from those obtained using traditional fixed effects estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19099v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Ditzen, Yiannis Karavias</dc:creator>
    </item>
    <item>
      <title>Asymptotic Theory of Principal Component Analysis for High-Dimensional Time Series Data under a Factor Structure</title>
      <link>https://arxiv.org/abs/2211.01921</link>
      <description>arXiv:2211.01921v4 Announce Type: replace 
Abstract: We review Principal Components (PC) estimation of a large approximate factor model for a panel of $n$ stationary time series and we provide new derivations of the asymptotic properties of the estimators, which are derived under a minimal set of assumptions requiring only the existence of 4th order moments. To this end, we also review various alternative sets of primitive sufficient conditions for mean-squared consistency of the sample covariance matrix. Finally, we discuss in detail the issue of identification of the loadings and factors as well as its implications for inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.01921v4</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences with Unpoolable Data</title>
      <link>https://arxiv.org/abs/2403.15910</link>
      <description>arXiv:2403.15910v3 Announce Type: replace 
Abstract: Difference-in-differences (DID) is commonly used to estimate treatment effects but is infeasible in settings where data are unpoolable due to privacy concerns or legal restrictions on data sharing, particularly across jurisdictions. In this study, we identify and relax the assumption of data poolability in DID estimation. We propose an innovative approach to estimate DID with unpoolable data (UN-DID) which can accommodate covariates, multiple groups, and staggered adoption. Through analytical proofs and Monte Carlo simulations, we show that UN-DID and conventional DID estimates of the average treatment effect and standard errors are equal and unbiased in settings without covariates. With covariates, both methods produce estimates that are unbiased, equivalent, and converge to the true value. The estimates differ slightly but the statistical inference and substantive conclusions remain the same. Two empirical examples with real-world data further underscore UN-DID's utility. The UN-DID method allows the estimation of cross-jurisdictional treatment effects with unpoolable data, enabling better counterfactuals to be used and new research questions to be answered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15910v3</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sunny Karim, Matthew D. Webb, Nichole Austin, Erin Strumpf</dc:creator>
    </item>
    <item>
      <title>Testing Sign Congruence Between Two Parameters</title>
      <link>https://arxiv.org/abs/2405.11759</link>
      <description>arXiv:2405.11759v4 Announce Type: replace 
Abstract: We test the null hypothesis that two parameters $(\mu_1,\mu_2)$ have the same sign, assuming that (asymptotically) normal estimators $(\hat{\mu}_1,\hat{\mu}_2)$ are available. Examples of this problem include the analysis of heterogeneous treatment effects, causal interpretation of reduced-form estimands, meta-studies, and mediation analysis. A number of tests were recently proposed. We recommend a test that is simple and rejects more often than many of these recent proposals. Like all other tests in the literature, it is conservative if the truth is near $(0,0)$ and therefore also biased. To clarify whether these features are avoidable, we also provide a test that is unbiased and has exact size control on the boundary of the null hypothesis, but which has counterintuitive properties and hence we do not recommend. We use the test to improve p-values in Kowalski (2022) from information contained in that paper's main text and to establish statistical significance of some key estimates in Dippel et al. (2021).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11759v4</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Douglas L. Miller, Francesca Molinari, J\"org Stoye</dc:creator>
    </item>
    <item>
      <title>Closed-form estimation and inference for panels with attrition and refreshment samples</title>
      <link>https://arxiv.org/abs/2410.11263</link>
      <description>arXiv:2410.11263v2 Announce Type: replace 
Abstract: It has long been established that, if a panel dataset suffers from attrition, auxiliary (refreshment) sampling restores full identification under additional assumptions that still allow for nontrivial attrition mechanisms. Such identification results rely on implausible assumptions about the attrition process or lead to theoretically and computationally challenging estimation procedures. We propose an alternative identifying assumption that, despite its nonparametric nature, suggests a simple estimation algorithm based on a transformation of the empirical cumulative distribution function of the data. This estimation procedure requires neither tuning parameters nor optimization in the first step, i.e., has a closed form. We prove that our estimator is consistent and asymptotically normal and demonstrate its good performance in simulations. We provide an empirical illustration with income data from the Understanding America Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11263v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigory Franguridi, Lidia Kosenkova</dc:creator>
    </item>
    <item>
      <title>Estimating Discrete Choice Demand Models with Sparse Market-Product Shocks</title>
      <link>https://arxiv.org/abs/2501.02381</link>
      <description>arXiv:2501.02381v2 Announce Type: replace 
Abstract: We propose a new approach to estimating the random coefficient logit demand model for differentiated products when the vector of market-product level shocks is sparse. Assuming sparsity, we establish nonparametric identification of the distribution of random coefficients and demand shocks under mild conditions. Then we develop a Bayesian procedure, which exploits the sparsity structure using shrinkage priors, to conduct inference about the model parameters and counterfactual quantities. Comparing to the standard BLP (Berry, Levinsohn, &amp; Pakes, 1995) method, our approach does not require demand inversion or instrumental variables (IVs), thus provides a compelling alternative when IVs are not available or their validity is questionable. Monte Carlo simulations validate our theoretical findings and demonstrate the effectiveness of our approach, while empirical applications reveal evidence of sparse demand shocks in well-known datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02381v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhentong Lu, Kenichi Shimizu</dc:creator>
    </item>
    <item>
      <title>Generalizability with ignorance in mind: learning what we do (not) know for archetypes discovery</title>
      <link>https://arxiv.org/abs/2501.13355</link>
      <description>arXiv:2501.13355v2 Announce Type: replace 
Abstract: When studying policy interventions, researchers often pursue two goals: i) identifying for whom the program has the largest effects (heterogeneity) and ii) determining whether those patterns of treatment effects have predictive power across environments (generalizability). We develop a framework to learn when and how to partition observations into groups of individual and environmental characterstics within which treatment effects are predictively stable, and when instead extrapolation is unwarranted and further evidence is needed. Our procedure determines in which contexts effects are generalizable and when, instead, researchers should admit ignorance and collect more data. We provide a decision-theoretic foundation, derive finite-sample regret guarantees, and establish asymptotic inference results. We illustrate the benefits of our approach by reanalyzing a multifaceted anti-poverty program across six countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13355v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Emily Breza, Arun G. Chandrasekhar, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Large structural VARs with multiple linear shock and impact inequality restrictions</title>
      <link>https://arxiv.org/abs/2505.19244</link>
      <description>arXiv:2505.19244v2 Announce Type: replace 
Abstract: We propose a high-dimensional structural vector autoregression framework that features a factor structure in the error terms and accommodates a large number of linear inequality restrictions on impact impulse responses, structural shocks, and their element-wise products. In particular, we demonstrate that narrative restrictions can be imposed via constraints on the structural shocks, which can be used to sharpen inference and disentangle structurally interpretable shocks. To estimate the model, we develop a highly efficient sampling algorithm that scales well with both the model dimension and the number of inequality restrictions on impact responses and structural shocks. It remains computationally feasible even in settings where existing algorithms may break down. To illustrate the practical utility of our approach, we identify five structural shocks and examine the dynamic responses of thirty macroeconomic variables, highlighting the model's flexibility and feasibility in complex empirical applications. We provide empirical evidence that financial shocks are the most important driver of business cycle dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19244v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Berend, Jan Pr\"user</dc:creator>
    </item>
    <item>
      <title>Production Function Estimation without Invertibility: Imperfectly Competitive Environments and Demand Shocks</title>
      <link>https://arxiv.org/abs/2506.13520</link>
      <description>arXiv:2506.13520v2 Announce Type: replace 
Abstract: We advance the proxy variable approach to production function estimation. We show that the invertibility assumption at its heart is testable. We characterize what goes wrong if invertibility fails and what can still be done. We show that rethinking how the estimation procedure is implemented either eliminates or mitigates the bias that arises if invertibility fails. In particular, a simple change to the first step of the estimation procedure provides a first-order bias correction for the GMM estimator in the second step. Furthermore, a modification of the moment condition in the second step ensures Neyman orthogonality and enhances efficiency and robustness by rendering the asymptotic distribution of the GMM estimator invariant to estimation noise from the first step.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13520v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ulrich Doraszelski, Lixiong Li</dc:creator>
    </item>
  </channel>
</rss>
