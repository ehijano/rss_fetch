<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Sep 2024 01:46:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Empirical Bernstein in smooth Banach spaces</title>
      <link>https://arxiv.org/abs/2409.06060</link>
      <description>arXiv:2409.06060v1 Announce Type: new 
Abstract: Existing concentration bounds for bounded vector-valued random variables include extensions of the scalar Hoeffding and Bernstein inequalities. While the latter is typically tighter, it requires knowing a bound on the variance of the random variables. We derive a new vector-valued empirical Bernstein inequality, which makes use of an empirical estimator of the variance instead of the true variance. The bound holds in 2-smooth separable Banach spaces, which include finite dimensional Euclidean spaces and separable Hilbert spaces. The resulting confidence sets are instantiated for both the batch setting (where the sample size is fixed) and the sequential setting (where the sample size is a stopping time). The confidence set width asymptotically exactly matches that achieved by Bernstein in the leading term. The method and supermartingale proof technique combine several tools of Pinelis (1994) and Waudby-Smith and Ramdas (2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06060v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diego Martinez-Taboada, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>On Sparsity and Sub-Gaussianity in the Johnson-Lindenstrauss Lemma</title>
      <link>https://arxiv.org/abs/2409.06275</link>
      <description>arXiv:2409.06275v1 Announce Type: new 
Abstract: We provide a simple proof of the Johnson-Lindenstrauss lemma for sub-Gaussian variables. We extend the analysis to identify how sparse projections can be, and what the cost of sparsity is on the target dimension.The Johnson-Lindenstrauss lemma is the theoretical core of the dimensionality reduction methods based on random projections. While its original formulation involves matrices with Gaussian entries, the computational cost of random projections can be drastically reduced by the use of simpler variables, especially if they vanish with a high probability. In this paper, we propose a simple and elementary analysis of random projections under classical assumptions that emphasizes the key role of sub-Gaussianity. Furthermore, we show how to extend it to sparse projections, emphasizing the limits induced by the sparsity of the data itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06275v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aur\'elien Garivier (UMPA-ENSL, MC2), Emmanuel Pilliat (UMPA-ENSL)</dc:creator>
    </item>
    <item>
      <title>Many-sample tests for the equality and the proportionality hypotheses between large covariance matrices</title>
      <link>https://arxiv.org/abs/2409.06296</link>
      <description>arXiv:2409.06296v1 Announce Type: new 
Abstract: This paper proposes procedures for testing the equality hypothesis and the proportionality hypothesis involving a large number of $q$ covariance matrices of dimension $p\times p$. Under a limiting scheme where $p$, $q$ and the sample sizes from the $q$ populations grow to infinity in a proper manner, the proposed test statistics are shown to be asymptotically normal. Simulation results show that finite sample properties of the test procedures are satisfactory under both the null and alternatives. As an application, we derive a test procedure for the Kronecker product covariance specification for transposable data. Empirical analysis of datasets from the Mouse Aging Project and the 1000 Genomes Project (phase 3) is also conducted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06296v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianxing Mei, Chen Wang, Jianfeng Yao</dc:creator>
    </item>
    <item>
      <title>Where does the tail start? Inflection Points and Maximum Curvature as Boundaries</title>
      <link>https://arxiv.org/abs/2409.06308</link>
      <description>arXiv:2409.06308v1 Announce Type: new 
Abstract: Understanding the tail behavior of distributions is crucial in statistical theory. For instance, the tail of a distribution plays a ubiquitous role in extreme value statistics, where it is associated with the likelihood of extreme events. There are several ways to characterize the tail of a distribution based on how the tail function, $\bar{F}(x) = P(X&gt;x)$, behaves when $x\to\infty$. However, for unimodal distributions, where does the core of the distribution end and the tail begin? This paper addresses this unresolved question and explores the usage of delimiting points obtained from the derivatives of the density function of continuous random variables, namely, the inflection point and the point of maximum curvature. These points are used to delimit the bulk of the distribution from its tails. We discuss the estimation of these delimiting points and compare them with other measures associated with the tail of a distribution, such as the kurtosis and extreme quantiles. We derive the proposed delimiting points for several known distributions and show that it can be a reasonable criterion for defining the starting point of the tail of a distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06308v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rafael Cabral, Maria de Iorio, Andrea Cremaschi</dc:creator>
    </item>
    <item>
      <title>Statistical Mechanics of Min-Max Problems</title>
      <link>https://arxiv.org/abs/2409.06053</link>
      <description>arXiv:2409.06053v1 Announce Type: cross 
Abstract: Min-max optimization problems, also known as saddle point problems, have attracted significant attention due to their applications in various fields, such as fair beamforming, generative adversarial networks (GANs), and adversarial learning. However, understanding the properties of these min-max problems has remained a substantial challenge. This study introduces a statistical mechanical formalism for analyzing the equilibrium values of min-max problems in the high-dimensional limit, while appropriately addressing the order of operations for min and max. As a first step, we apply this formalism to bilinear min-max games and simple GANs, deriving the relationship between the amount of training data and generalization error and indicating the optimal ratio of fake to real data for effective learning. This formalism provides a groundwork for a deeper theoretical analysis of the equilibrium properties in various machine learning methods based on min-max problems and encourages the development of new algorithms and architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06053v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuma Ichikawa, Koji Hukushima</dc:creator>
    </item>
    <item>
      <title>Asymptotic properties of the maximum likelihood estimator for Hidden Markov Models indexed by binary trees</title>
      <link>https://arxiv.org/abs/2409.06295</link>
      <description>arXiv:2409.06295v1 Announce Type: cross 
Abstract: We consider hidden Markov models indexed by a binary tree where the hidden state space is a general metric space. We study the maximum likelihood estimator (MLE) of the model parameters based only on the observed variables. In both stationary and non-stationary regimes, we prove strong consistency and asymptotic normality of the MLE under standard assumptions. Those standard assumptions imply uniform exponential memorylessness properties of the initial distribution conditional on the observations. The proofs rely on ergodic theorems for Markov chain indexed by trees with neighborhood-dependent functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06295v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julien Weibel (IDP, CERMICS)</dc:creator>
    </item>
    <item>
      <title>Enzyme kinetic reactions as interacting particle systems: Stochastic averaging and parameter inference</title>
      <link>https://arxiv.org/abs/2409.06565</link>
      <description>arXiv:2409.06565v1 Announce Type: cross 
Abstract: We consider a stochastic model of multistage Michaelis--Menten (MM) type enzyme kinetic reactions describing the conversion of substrate molecules to a product through several intermediate species. The high-dimensional, multiscale nature of these reaction networks presents significant computational challenges, especially in statistical estimation of reaction rates. This difficulty is amplified when direct data on system states are unavailable, and one only has access to a random sample of product formation times. To address this, we proceed in two stages. First, under certain technical assumptions akin to those made in the Quasi-steady-state approximation (QSSA) literature, we prove two asymptotic results: a stochastic averaging principle that yields a lower-dimensional model, and a functional central limit theorem that quantifies the associated fluctuations. Next, for statistical inference of the parameters of the original MM reaction network, we develop a mathematical framework involving an interacting particle system (IPS) and prove a propagation of chaos result that allows us to write a product-form likelihood function. The novelty of the IPS-based inference method is that it does not require information about the state of the system and works with only a random sample of product formation times. We provide numerical examples to illustrate the efficacy of the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06565v1</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnab Ganguly, Wasiur R. KhudaBukhsh</dc:creator>
    </item>
    <item>
      <title>A Discontinuity Adjustment for Subdistribution Function Confidence Bands Applied to Right-Censored Competing Risks Data (with Erratum)</title>
      <link>https://arxiv.org/abs/1702.01081</link>
      <description>arXiv:1702.01081v2 Announce Type: replace 
Abstract: The wild bootstrap is the resampling method of choice in survival analytic applications. Theoretic justifications rely on the assumption of existing intensity functions which is equivalent to an exclusion of ties among the event times. However, such ties are omnipresent in practical studies. It turns out that the wild bootstrap should only be applied in a modified manner that corrects for altered limit variances and emerging dependencies. This again ensures the asymptotic exactness of inferential procedures. An analogous necessity is the use of the Greenwood-type variance estimator for Nelson-Aalen estimators which is particularly preferred in tied data regimes. All theoretic arguments are transferred to bootstrapping Aalen-Johansen estimators for cumulative incidence functions in competing risks. An extensive simulation study as well as an application to real competing risks data of male intensive care unit patients suffering from pneumonia illustrate the practicability of the proposed technique.</description>
      <guid isPermaLink="false">oai:arXiv.org:1702.01081v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dennis Dobler, Merle Munko</dc:creator>
    </item>
    <item>
      <title>Distribution-free tests of multivariate independence based on center-outward quadrant, Spearman, Kendall, and van der Waerden statistics</title>
      <link>https://arxiv.org/abs/2111.15567</link>
      <description>arXiv:2111.15567v5 Announce Type: replace 
Abstract: Due to the lack of a canonical ordering in ${\mathbb R}^d$ for $d&gt;1$, defining multivariate generalizations of the classical univariate ranks has been a long-standing open problem in statistics. Optimal transport has been shown to offer a solution in which multivariate ranks are obtained by transporting data points to a grid that approximates a uniform reference measure (Chernozhukov et al., 2017; Hallin, 2017; Hallin et al., 2021), thereby inducing ranks, signs, and a data-driven ordering of ${\mathbb R}^d$. We take up this new perspective to define and study multivariate analogues of the sign covariance/quadrant statistic, Spearman's rho, Kendall's tau, and van der Waerden covariances. The resulting tests of multivariate independence are fully distribution-free, hence uniformly valid irrespective of the actual (absolutely continuous) distribution of the observations. Our results provide the asymptotic distribution theory for these new test statistics, with asymptotic approximations to critical values to be used for testing independence between random vectors, as well as a power analysis of the resulting tests in an extension of the so-called (bivariate) Konijn model. This power analysis includes a multivariate Chernoff--Savage property guaranteeing that, under elliptical generalized Konijn models, the asymptotic relative efficiency of our van der Waerden tests with respect to Wilks' classical (pseudo-)Gaussian procedure is strictly larger than or equal to one, where equality is achieved under Gaussian distributions only. We similarly provide a lower bound for the asymptotic relative efficiency of our Spearman procedure with respect to Wilks' test, thus extending the classical result by Hodges and Lehmann on the asymptotic relative efficiency, in univariate location models, of Wilcoxon tests with respect to the Student ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.15567v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Shi, Mathias Drton, Marc Hallin, Fang Han</dc:creator>
    </item>
    <item>
      <title>A statistical framework for analyzing shape in a time series of random geometric objects</title>
      <link>https://arxiv.org/abs/2304.01984</link>
      <description>arXiv:2304.01984v3 Announce Type: replace 
Abstract: We introduce a new framework to analyze shape descriptors that capture the geometric features of an ensemble of point clouds. At the core of our approach is the point of view that the data arises as sampled recordings from a metric space-valued stochastic process, possibly of nonstationary nature, thereby integrating geometric data analysis into the realm of functional time series analysis. Our framework allows for natural incorporation of spatial-temporal dynamics, heterogeneous sampling, and the study of convergence rates. Further, we derive complete invariants for classes of metric space-valued stochastic processes in the spirit of Gromov, and relate these invariants to so-called ball volume processes. Under mild dependence conditions, a weak invariance principle in $D([0,1]\times [0,\mathscr{R}])$ is established for sequential empirical versions of the latter, assuming the probabilistic structure possibly changes over time. Finally, we use this result to introduce novel test statistics for topological change, which are distribution-free in the limit under the hypothesis of stationarity. We explore these test statistics on time series of single-cell mRNA expression data, using shape descriptors coming from topological data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01984v3</guid>
      <category>math.ST</category>
      <category>cs.CG</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anne van Delft, Andrew J. Blumberg</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: I. Mean</title>
      <link>https://arxiv.org/abs/2403.12110</link>
      <description>arXiv:2403.12110v5 Announce Type: replace 
Abstract: As the most fundamental problem in statistics, robust location estimation has many prominent solutions, such as the trimmed mean, Winsorized mean, Hodges Lehmann estimator, Huber M estimator, and median of means. Recent studies suggest that their maximum biases concerning the mean can be quite different, but the underlying mechanisms largely remain unclear. This study exploited a semiparametric method to classify distributions by the asymptotic orderliness of quantile combinations with varying breakdown points, showing their interrelations and connections to parametric distributions. Further deductions explain why the Winsorized mean typically has smaller biases compared to the trimmed mean; two sequences of semiparametric robust mean estimators emerge, particularly highlighting the superiority of the median Hodges Lehmann mean. This article sheds light on the understanding of the common nature of probability distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12110v5</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: II. Central Moments</title>
      <link>https://arxiv.org/abs/2403.14570</link>
      <description>arXiv:2403.14570v3 Announce Type: replace 
Abstract: In descriptive statistics, $U$-statistics arise naturally in producing minimum-variance unbiased estimators. In 1984, Serfling considered the distribution formed by evaluating the kernel of the $U$-statistics and proposed generalized $L$-statistics which includes Hodges-Lehamnn estimator and Bickel-Lehmann spread as special cases. However, the structures of the kernel distributions remain unclear. In 1954, Hodges and Lehmann demonstrated that if $X$ and $Y$ are independently sampled from the same unimodal distribution, $X-Y$ will exhibit symmetrical unimodality with its peak centered at zero. Building upon this foundational work, the current study delves into the structure of the kernel distribution. It is shown that the $\mathbf{k}$th central moment kernel distributions ($\mathbf{k}&gt;2$) derived from a unimodal distribution exhibit location invariance and is also nearly unimodal with the mode and median close to zero. This article provides an approach to study the general structure of kernel distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14570v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: III. Invariant Moments</title>
      <link>https://arxiv.org/abs/2403.16039</link>
      <description>arXiv:2403.16039v3 Announce Type: replace 
Abstract: Descriptive statistics for parametric models are currently highly sensative to departures, gross errors, and/or random errors. Here, leveraging the structures of parametric distributions and their central moment kernel distributions, a class of estimators, consistent simultanously for both a semiparametric distribution and a distinct parametric distribution, is proposed. These efficient estimators are robust to both gross errors and departures from parametric assumptions, making them ideal for estimating the mean and central moments of common unimodal distributions. This article opens up the possibility of utilizing the common nature of probability models to construct near-optimal estimators that are suitable for various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16039v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>A simplified directional KeRF algorithm</title>
      <link>https://arxiv.org/abs/2407.04042</link>
      <description>arXiv:2407.04042v2 Announce Type: replace 
Abstract: Random forest methods belong to the class of non-parametric machine learning algorithms. They were first introduced in 2001 by Breiman and they perform with accuracy in high dimensional settings. In this article, we consider, a simplified kernel-based random forest algorithm called simplified directional KeRF (Kernel Random Forest). We establish the asymptotic equivalence between simplified directional KeRF and centered KeRF, with additional numerical experiments supporting our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04042v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iakovidis Isidoros, Nicola Arcozzi</dc:creator>
    </item>
    <item>
      <title>Convergence Rates for the Maximum A Posteriori Estimator in PDE-Regression Models with Random Design</title>
      <link>https://arxiv.org/abs/2409.03417</link>
      <description>arXiv:2409.03417v2 Announce Type: replace 
Abstract: We consider the statistical inverse problem of recovering a parameter $\theta\in H^\alpha$ from data arising from the Gaussian regression problem \begin{equation*}
  Y = \mathscr{G}(\theta)(Z)+\varepsilon \end{equation*} with nonlinear forward map $\mathscr{G}:\mathbb{L}^2\to\mathbb{L}^2$, random design points $Z$ and Gaussian noise $\varepsilon$. The estimation strategy is based on a least squares approach under $\Vert\cdot\Vert_{H^\alpha}$-constraints. We establish the existence of a least squares estimator $\hat{\theta}$ as a maximizer for a given functional under Lipschitz-type assumptions on the forward map $\mathscr{G}$. A general concentration result is shown, which is used to prove consistency and upper bounds for the prediction error. The corresponding rates of convergence reflect not only the smoothness of the parameter of interest but also the ill-posedness of the underlying inverse problem. We apply the general model to the Darcy problem, where the recovery of an unknown coefficient function $f$ of a PDE is of interest. For this example, we also provide corresponding rates of convergence for the prediction and estimation errors. Additionally, we briefly discuss the applicability of the general model to other problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03417v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Siebel</dc:creator>
    </item>
    <item>
      <title>Optimal Regularization for a Data Source</title>
      <link>https://arxiv.org/abs/2212.13597</link>
      <description>arXiv:2212.13597v4 Announce Type: replace-cross 
Abstract: In optimization-based approaches to inverse problems and to statistical estimation, it is common to augment criteria that enforce data fidelity with a regularizer that promotes desired structural properties in the solution. The choice of a suitable regularizer is typically driven by a combination of prior domain information and computational considerations. Convex regularizers are attractive computationally but they are limited in the types of structure they can promote. On the other hand, nonconvex regularizers are more flexible in the forms of structure they can promote and they have showcased strong empirical performance in some applications, but they come with the computational challenge of solving the associated optimization problems. In this paper, we seek a systematic understanding of the power and the limitations of convex regularization by investigating the following questions: Given a distribution, what is the optimal regularizer for data drawn from the distribution? What properties of a data source govern whether the optimal regularizer is convex? We address these questions for the class of regularizers specified by functionals that are continuous, positively homogeneous, and positive away from the origin. We say that a regularizer is optimal for a data distribution if the Gibbs density with energy given by the regularizer maximizes the population likelihood (or equivalently, minimizes cross-entropy loss) over all regularizer-induced Gibbs densities. As the regularizers we consider are in one-to-one correspondence with star bodies, we leverage dual Brunn-Minkowski theory to show that a radial function derived from a data distribution is akin to a ``computational sufficient statistic'' as it is the key quantity for identifying optimal regularizers and for assessing the amenability of a data source to convex regularization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.13597v4</guid>
      <category>math.OC</category>
      <category>math.MG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oscar Leong, Eliza O'Reilly, Yong Sheng Soh, Venkat Chandrasekaran</dc:creator>
    </item>
    <item>
      <title>On the Identifiablility of Nonlocal Interaction Kernels in First-Order Systems of Interacting Particles on Riemannian Manifolds</title>
      <link>https://arxiv.org/abs/2305.12340</link>
      <description>arXiv:2305.12340v2 Announce Type: replace-cross 
Abstract: In this paper, we tackle a critical issue in nonparametric inference for systems of interacting particles on Riemannian manifolds: the identifiability of the interaction functions. Specifically, we define the function spaces on which the interaction kernels can be identified given infinite i.i.d observational derivative data sampled from a distribution. Our methodology involves casting the learning problem as a linear statistical inverse problem using a operator theoretical framework. We prove the well-posedness of inverse problem by establishing the strict positivity of a related integral operator and our analysis allows us to refine the results on specific manifolds such as the sphere and Hyperbolic space. Our findings indicate that a numerically stable procedure exists to recover the interaction kernel from finite (noisy) data, and the estimator will be convergent to the ground truth. This also answers an open question in [MMQZ21] and demonstrate that least square estimators can be statistically optimal in certain scenarios. Finally, our theoretical analysis could be extended to the mean-field case, revealing that the corresponding nonparametric inverse problem is ill-posed in general and necessitates effective regularization techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12340v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.CA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Siam Journal on Applied Math 2024</arxiv:journal_reference>
      <dc:creator>Sui Tang, Malik Tuerkoen, Hanming Zhou</dc:creator>
    </item>
    <item>
      <title>Common Trends and Long-Run Identification in Nonlinear Structural VARs</title>
      <link>https://arxiv.org/abs/2404.05349</link>
      <description>arXiv:2404.05349v2 Announce Type: replace-cross 
Abstract: While it is widely recognised that linear (structural) VARs may fail to capture important aspects of economic time series, the use of nonlinear SVARs has to date been almost entirely confined to the modelling of stationary time series, because of a lack of understanding as to how common stochastic trends may be accommodated within nonlinear models. This has unfortunately circumscribed the range of series to which such models can be applied -- and/or required that these series be first transformed to stationarity, a potential source of misspecification -- and prevented the use of long-run identifying restrictions in these models. To address these problems, we develop a flexible class of additively time-separable nonlinear SVARs, which subsume models with threshold-type endogenous regime switching, both of the piecewise linear and smooth transition varieties. We extend the Granger--Johansen representation theorem to this class of models, obtaining conditions that specialise exactly to the usual ones when the model is linear. We further show that, as a corollary, these models are capable of supporting the same kinds of long-run identifying restrictions as are available in linearly cointegrated SVARs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05349v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James A. Duffy, Sophocles Mavroeidis</dc:creator>
    </item>
    <item>
      <title>Simultaneous Inference for Local Structural Parameters with Random Forests</title>
      <link>https://arxiv.org/abs/2405.07860</link>
      <description>arXiv:2405.07860v3 Announce Type: replace-cross 
Abstract: We construct simultaneous confidence intervals for solutions to conditional moment equations. The intervals are built around a class of nonparametric regression algorithms based on subsampled kernels. This class encompasses various forms of subsampled random forest regression, including Generalized Random Forests (Athey et al., 2019). Although simultaneous validity is often desirable in practice -- for example, for fine-grained characterization of treatment effect heterogeneity -- only confidence intervals that confer pointwise guarantees were previously available. Our work closes this gap. As a by-product, we obtain several new order-explicit results on the concentration and normal approximation of high-dimensional U-statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07860v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David M. Ritzwoller, Vasilis Syrgkanis</dc:creator>
    </item>
  </channel>
</rss>
