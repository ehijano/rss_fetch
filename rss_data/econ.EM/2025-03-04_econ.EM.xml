<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Mar 2025 03:12:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Uniform Limit Theory for Network Data</title>
      <link>https://arxiv.org/abs/2503.00290</link>
      <description>arXiv:2503.00290v1 Announce Type: new 
Abstract: I present a novel uniform law of large numbers (ULLN) for network-dependent data. While Kojevnikov, Marmer, and Song (KMS, 2021) provide a comprehensive suite of limit theorems and a robust variance estimator for network-dependent processes, their analysis focuses on pointwise convergence. On the other hand, uniform convergence is essential for nonlinear estimators such as M and GMM estimators (e.g., Newey and McFadden, 1994, Section 2). Building on KMS, I establish the ULLN under network dependence and demonstrate its utility by proving the consistency of both M and GMM estimators. A byproduct of this work is a novel maximal inequality for network data, which may prove useful for future research beyond the scope of this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00290v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yuya Sasaki</dc:creator>
    </item>
    <item>
      <title>The Uncertainty of Machine Learning Predictions in Asset Pricing</title>
      <link>https://arxiv.org/abs/2503.00549</link>
      <description>arXiv:2503.00549v1 Announce Type: new 
Abstract: Machine learning in asset pricing typically predicts expected returns as point estimates, ignoring uncertainty. We develop new methods to construct forecast confidence intervals for expected returns obtained from neural networks. We show that neural network forecasts of expected returns share the same asymptotic distribution as classic nonparametric methods, enabling a closed-form expression for their standard errors. We also propose a computationally feasible bootstrap to obtain the asymptotic distribution. We incorporate these forecast confidence intervals into an uncertainty-averse investment framework. This provides an economic rationale for shrinkage implementations of portfolio selection. Empirically, our methods improve out-of-sample performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00549v1</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Liao, Xinjie Ma, Andreas Neuhierl, Linda Schilling</dc:creator>
    </item>
    <item>
      <title>Causal Inference on Outcomes Learned from Text</title>
      <link>https://arxiv.org/abs/2503.00725</link>
      <description>arXiv:2503.00725v1 Announce Type: new 
Abstract: We propose a machine-learning tool that yields causal inference on text in randomized trials. Based on a simple econometric framework in which text may capture outcomes of interest, our procedure addresses three questions: First, is the text affected by the treatment? Second, which outcomes is the effect on? And third, how complete is our description of causal effects? To answer all three questions, our approach uses large language models (LLMs) that suggest systematic differences across two groups of text documents and then provides valid inference based on costly validation. Specifically, we highlight the need for sample splitting to allow for statistical validation of LLM outputs, as well as the need for human labeling to validate substantive claims about how documents differ across groups. We illustrate the tool in a proof-of-concept application using abstracts of academic manuscripts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00725v1</guid>
      <category>econ.EM</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iman Modarressi, Jann Spiess, Amar Venugopal</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for dynamic spatial quantile models with interactive effects</title>
      <link>https://arxiv.org/abs/2503.00772</link>
      <description>arXiv:2503.00772v1 Announce Type: new 
Abstract: With the rapid advancement of information technology and data collection systems, large-scale spatial panel data presents new methodological and computational challenges. This paper introduces a dynamic spatial panel quantile model that incorporates unobserved heterogeneity. The proposed model captures the dynamic structure of panel data, high-dimensional cross-sectional dependence, and allows for heterogeneous regression coefficients. To estimate the model, we propose a novel Bayesian Markov Chain Monte Carlo (MCMC) algorithm. Contributions to Bayesian computation include the development of quantile randomization, a new Gibbs sampler for structural parameters, and stabilization of the tail behavior of the inverse Gaussian random generator. We establish Bayesian consistency for the proposed estimation method as both the time and cross-sectional dimensions of the panel approach infinity. Monte Carlo simulations demonstrate the effectiveness of the method. Finally, we illustrate the applicability of the approach through a case study on the quantile co-movement structure of the gasoline market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00772v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tomohiro Ando, Jushan Bai, Kunpeng Li, Yong Song</dc:creator>
    </item>
    <item>
      <title>Dynamic Factor Correlation Model</title>
      <link>https://arxiv.org/abs/2503.01080</link>
      <description>arXiv:2503.01080v1 Announce Type: new 
Abstract: We introduce a new dynamic factor correlation model with a novel variation-free parametrization of factor loadings. The model is applicable to high dimensions and can accommodate time-varying correlations, heterogeneous heavy-tailed distributions, and dependent idiosyncratic shocks, such as those observed in returns on stocks in the same subindustry. We apply the model to a "small universe" with 12 asset returns and to a "large universe" with 323 asset returns. The former facilitates a comprehensive empirical analysis and comparisons and the latter demonstrates the flexibility and scalability of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01080v1</guid>
      <category>econ.EM</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Tong, Peter Reinhard Hansen</dc:creator>
    </item>
    <item>
      <title>Wikipedia Contributions in the Wake of ChatGPT</title>
      <link>https://arxiv.org/abs/2503.00757</link>
      <description>arXiv:2503.00757v1 Announce Type: cross 
Abstract: How has Wikipedia activity changed for articles with content similar to ChatGPT following its introduction? We estimate the impact using differences-in-differences models, with dissimilar Wikipedia articles as a baseline for comparison, to examine how changes in voluntary knowledge contributions and information-seeking behavior differ by article content. Our analysis reveals that newly created, popular articles whose content overlaps with ChatGPT 3.5 saw a greater decline in editing and viewership after the November 2022 launch of ChatGPT than dissimilar articles did. These findings indicate heterogeneous substitution effects, where users selectively engage less with existing platforms when AI provides comparable content. This points to potential uneven impacts on the future of human-driven online knowledge contributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00757v1</guid>
      <category>cs.HC</category>
      <category>econ.EM</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liang Lyu, James Siderius, Hannah Li, Daron Acemoglu, Daniel Huttenlocher, Asuman Ozdaglar</dc:creator>
    </item>
    <item>
      <title>Vector Copula Variational Inference and Dependent Block Posterior Approximations</title>
      <link>https://arxiv.org/abs/2503.01072</link>
      <description>arXiv:2503.01072v1 Announce Type: cross 
Abstract: Variational inference (VI) is a popular method to estimate statistical and econometric models. The key to VI is the selection of a tractable density to approximate the Bayesian posterior. For large and complex models a common choice is to assume independence between multivariate blocks in a partition of the parameter space. While this simplifies the problem it can reduce accuracy. This paper proposes using vector copulas to capture dependence between the blocks parsimoniously. Tailored multivariate marginals are constructed using learnable cyclically monotone transformations. We call the resulting joint distribution a ``dependent block posterior'' approximation. Vector copula models are suggested that make tractable and flexible variational approximations. They allow for differing marginals, numbers of blocks, block sizes and forms of between block dependence. They also allow for solution of the variational optimization using fast and efficient stochastic gradient methods. The efficacy and versatility of the approach is demonstrated using four different statistical models and 16 datasets which have posteriors that are challenging to approximate. In all cases, our method produces more accurate posterior approximations than benchmark VI methods that either assume block independence or factor-based dependence, at limited additional computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01072v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yu Fu, Michael Stanley Smith, Anastasios Panagiotelis</dc:creator>
    </item>
    <item>
      <title>Optimal Decision Rules Under Partial Identification</title>
      <link>https://arxiv.org/abs/2111.04926</link>
      <description>arXiv:2111.04926v4 Announce Type: replace 
Abstract: I consider a class of statistical decision problems in which the policymaker must decide between two policies to maximize social welfare (e.g., the population mean of an outcome) based on a finite sample. The framework introduced in this paper allows for various types of restrictions on the structural parameter (e.g., the smoothness of a conditional mean potential outcome function) and accommodates settings with partial identification of social welfare. As the main theoretical result, I derive a finite-sample optimal decision rule under the minimax regret criterion. This rule has a simple form, yet achieves optimality among all decision rules; no ad hoc restrictions are imposed on the class of decision rules. I apply my results to the problem of whether to change an eligibility cutoff in a regression discontinuity setup, and illustrate them in an empirical application to a school construction program in Burkina Faso.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.04926v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kohei Yata</dc:creator>
    </item>
    <item>
      <title>Decomposing Global Bank Network Connectedness: What is Common, Idiosyncratic and When?</title>
      <link>https://arxiv.org/abs/2402.02482</link>
      <description>arXiv:2402.02482v2 Announce Type: replace 
Abstract: We propose a novel approach to estimate high-dimensional global bank network connectedness in both the time and frequency domains. By employing a factor model with sparse VAR idiosyncratic components, we decompose system-wide connectedness (SWC) into two key drivers: (i) common component shocks and (ii) idiosyncratic shocks. We also provide bootstrap confidence bands for all SWC measures. Furthermore, spectral density estimation allows us to disentangle SWC into short-, medium-, and long-term frequency responses to these shocks. We apply our methodology to two datasets of daily stock price volatilities for over 90 global banks, spanning the periods 2003-2013 and 2014-2023. Our empirical analysis reveals that SWC spikes during global crises, primarily driven by common component shocks and their short term effects. Conversely, in normal times, SWC is largely influenced by idiosyncratic shocks and medium-term dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02482v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Krampe, Luca Margaritella</dc:creator>
    </item>
    <item>
      <title>Estimating Time-Varying Parameters of Various Smoothness in Linear Models via Kernel Regression</title>
      <link>https://arxiv.org/abs/2406.14046</link>
      <description>arXiv:2406.14046v4 Announce Type: replace 
Abstract: We consider estimating nonparametric time-varying parameters in linear models using kernel regression. Our contributions are threefold. First, we consider a broad class of time-varying parameters including deterministic smooth functions, the rescaled random walk, structural breaks, the threshold model and their mixtures. We show that those time-varying parameters can be consistently estimated by kernel regression. Our analysis exploits the smoothness of the time-varying parameter quantified by a single parameter. The second contribution is to reveal that the bandwidth used in kernel regression determines a trade-off between the rate of convergence and the size of the class of time-varying parameters that can be estimated. We demonstrate that an improper choice of the bandwidth yields biased estimation, and argue that the bandwidth should be selected according to the smoothness of the time-varying parameter. Our third contribution is to propose a data-driven procedure for bandwidth selection that is adaptive to the smoothness of the time-varying parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14046v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikihito Nishi</dc:creator>
    </item>
    <item>
      <title>Identification of a Rank-dependent Peer Effect Model</title>
      <link>https://arxiv.org/abs/2410.14317</link>
      <description>arXiv:2410.14317v2 Announce Type: replace 
Abstract: We develop a model that captures peer effect heterogeneity by modeling the endogenous spillover to be linear in ordered peer outcomes. Unlike the canonical linear-in-means model, our approach accounts for the distribution of peer outcomes as well as the size of peer groups. Under a minimal condition, our model admits a unique equilibrium and is therefore tractable and identified. Simulations show our estimator has good finite sample performance. Finally, we apply our model to educational data from Norway, finding that higher-performing friends disproportionately drive GPA spillovers. Our framework provides new insights into the structure of peer effects beyond aggregate measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14317v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eyo I. Herstad, Myungkou Shin</dc:creator>
    </item>
    <item>
      <title>LABOR-LLM: Language-Based Occupational Representations with Large Language Models</title>
      <link>https://arxiv.org/abs/2406.17972</link>
      <description>arXiv:2406.17972v3 Announce Type: replace-cross 
Abstract: Vafa et al. (2024) introduced a transformer-based econometric model, CAREER, that predicts a worker's next job as a function of career history (an "occupation model"). CAREER was initially estimated ("pre-trained") using a large, unrepresentative resume dataset, which served as a "foundation model," and parameter estimation was continued ("fine-tuned") using data from a representative survey. CAREER had better predictive performance than benchmarks. This paper considers an alternative where the resume-based foundation model is replaced by a large language model (LLM). We convert tabular data from the survey into text files that resemble resumes and fine-tune the LLMs using these text files with the objective to predict the next token (word). The resulting fine-tuned LLM is used as an input to an occupation model. Its predictive performance surpasses all prior models. We demonstrate the value of fine-tuning and further show that by adding more career data from a different population, fine-tuning smaller LLMs surpasses the performance of fine-tuning larger models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17972v3</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>econ.EM</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susan Athey, Herman Brunborg, Tianyu Du, Ayush Kanodia, Keyon Vafa</dc:creator>
    </item>
    <item>
      <title>Bayesian penalized empirical likelihood and Markov Chain Monte Carlo sampling</title>
      <link>https://arxiv.org/abs/2412.17354</link>
      <description>arXiv:2412.17354v3 Announce Type: replace-cross 
Abstract: In this study, we introduce a novel methodological framework called Bayesian Penalized Empirical Likelihood (BPEL), designed to address the computational challenges inherent in empirical likelihood (EL) approaches. Our approach has two primary objectives: (i) to enhance the inherent flexibility of EL in accommodating diverse model conditions, and (ii) to facilitate the use of well-established Markov Chain Monte Carlo (MCMC) sampling schemes as a convenient alternative to the complex optimization typically required for statistical inference using EL. To achieve the first objective, we propose a penalized approach that regularizes the Lagrange multipliers, significantly reducing the dimensionality of the problem while accommodating a comprehensive set of model conditions. For the second objective, our study designs and thoroughly investigates two popular sampling schemes within the BPEL context. We demonstrate that the BPEL framework is highly flexible and efficient, enhancing the adaptability and practicality of EL methods. Our study highlights the practical advantages of using sampling techniques over traditional optimization methods for EL problems, showing rapid convergence to the global optima of posterior distributions and ensuring the effective resolution of complex statistical inference challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17354v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Cheng Yong Tang, Yuanzheng Zhu</dc:creator>
    </item>
  </channel>
</rss>
