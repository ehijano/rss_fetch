<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Oct 2024 04:01:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Renormalization-Group Analysis of the Many-Body Localization Transition in the Random-Field XXZ Chain</title>
      <link>https://arxiv.org/abs/2410.12430</link>
      <description>arXiv:2410.12430v1 Announce Type: new 
Abstract: We analyze the spectral properties of the Heisenberg spin-1/2 chain with random fields in light of recent works of the renormalization-group flow of the Anderson model in infinite dimension. We reconstruct the beta function of the order parameter from the numerical data, and show that it does not admit a one-parameter scaling form and a simple Wilson-Fisher fixed point. Rather, it is compatible with a two-parameter, Berezinskii-Kosterlitz-Thouless-like flow with a line of fixed points (the many-body localized phase), which terminates into the localization transition critical point. Therefore, we argue that previous studies, which assumed the existence of an isolated Wilson- Fisher fixed point and performed one-parameter finite-size scaling analysis, could not explain the numerical data in a coherent way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12430v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacopo Niedda, Giacomo Bracci Testasecca, Giuseppe Magnifico, Federico Balducci, Carlo Vanoni, Antonello Scardicchio</dc:creator>
    </item>
    <item>
      <title>Electronic glasses from a broken gauge symmetry in disorder-free systems</title>
      <link>https://arxiv.org/abs/2410.12482</link>
      <description>arXiv:2410.12482v1 Announce Type: new 
Abstract: Glass phases can be stabilized by quenched disorders, as in most spin-glass materials, or self-generated through kinetic freezing in disorder-free systems. A canonical example of the latter is structural glasses, which have been extensively studied for many decades. Yet, how the rugged energy landscape of a glass phase is spontaneously generated in disorder-free systems remains one of the key questions in glass physics. Here we present a general electronic mechanism for the emergence of glassy phase using the example of itinerant electrons coupled to XY spins on a lattice. This model can also be be viewed as the mean-field theory of a superconducting system with attractive density-density interactions. Intriguingly, the electron gauge symmetry in the strong pairing limit gives rise to a macroscopic degeneracy of XY spins. In the presence of electron hopping that breaks the gauge symmetry, the lifting of the extensive degeneracy leads to a glass phase with disordered pairings. Our findings highlight a novel scenario in which a glassy state originates from the breaking of quantum gauge symmetry without quenched disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12482v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.str-el</category>
      <category>cond-mat.supr-con</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingyu Yang, Gia-Wei Chern</dc:creator>
    </item>
    <item>
      <title>Statistical field theory of random graphs with prescribed degrees</title>
      <link>https://arxiv.org/abs/2410.11191</link>
      <description>arXiv:2410.11191v1 Announce Type: cross 
Abstract: Statistical field theory methods have been very successful with a number of random graph and random matrix problems, but it is challenging to apply these methods to graphs with prescribed degree sequences due to the extensive number of constraints that enforce the desired degree at each vertex. Building on top of recent results where similar methods are applied to random regular graph counting, we develop an accurate statistical field theory description for the adjacency matrix spectra of graphs with prescribed degrees. For large graphs, the expectation values are dominated by functional saddle points satisfying explicit equations. For the case of regular graphs, this immediately leads to the known McKay distribution. We then consider mixed-regular graphs with N1 vertices of degree d1, N2 vertices of degree d2, etc, such that the ratios N_i/N are kept fixed as N goes to infinity. For such graphs, the eigenvalue densities are governed by nonlinear integral equations of the Hammerstein type. Solving these equations numerically reproduces with an excellent accuracy the empirical eigenvalue distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11191v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>math-ph</category>
      <category>math.FA</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pawat Akara-pipattana, Oleg Evnin</dc:creator>
    </item>
    <item>
      <title>On the time complexity analysis of numerical percolation threshold estimation</title>
      <link>https://arxiv.org/abs/2410.11874</link>
      <description>arXiv:2410.11874v1 Announce Type: cross 
Abstract: The main purpose of percolation theory is to model phase transitions in a variety of random systems, which is highly valuable in fields related to materials physics, biology, or otherwise unrelated areas like oil extraction or even quantum computing. Thus, one of the problems encountered is the calculation of the threshold at which such transition occurs, known as percolation threshold. Since there are no known closed forms to determine the threshold in an exact manner in systems with particular properties, it is decided to rely on numerical methods as the Monte Carlo approach, which provides a sufficiently accurate approximation to serve as a valid estimate in the projects or research where it is involved. However, in order to achieve an exact characterization of the threshold in two-dimensional systems with site percolation, in this work it is performed an analysis of the complexity, both temporal and spatial, of an algorithm that implements its computation from the aforementioned numerical method. Specifically, the conduction of an accurate analysis of the cost of such algorithm implies a deep enough knowledge about certain metrics regarding its duration, or work completed per iteration, which along with its formalization may contribute to the determination of the threshold based on these metrics. Nevertheless, as a result, various bounds are achieved for the best, average and worst cases of the execution on systems spanning several dimensions, revealing that in 1 and 2 the complexity is directly conditioned by the duration, although from 3 onwards no proof for this point has been found, notwithstanding the evidence suggesting its compliance. Furthermore, based on the average case, several methods are proposed that could be applied to characterize the threshold, although they have not been thoroughly explored beyond what is necessary for the complexity analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11874v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.CC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Garc\'ia Solla</dc:creator>
    </item>
    <item>
      <title>The Persian Rug: solving toy models of superposition using large-scale symmetries</title>
      <link>https://arxiv.org/abs/2410.12101</link>
      <description>arXiv:2410.12101v1 Announce Type: cross 
Abstract: We present a complete mechanistic description of the algorithm learned by a minimal non-linear sparse data autoencoder in the limit of large input dimension. The model, originally presented in arXiv:2209.10652, compresses sparse data vectors through a linear layer and decompresses using another linear layer followed by a ReLU activation. We notice that when the data is permutation symmetric (no input feature is privileged) large models reliably learn an algorithm that is sensitive to individual weights only through their large-scale statistics. For these models, the loss function becomes analytically tractable. Using this understanding, we give the explicit scalings of the loss at high sparsity, and show that the model is near-optimal among recently proposed architectures. In particular, changing or adding to the activation function any elementwise or filtering operation can at best improve the model's performance by a constant factor. Finally, we forward-engineer a model with the requisite symmetries and show that its loss precisely matches that of the trained models. Unlike the trained model weights, the low randomness in the artificial weights results in miraculous fractal structures resembling a Persian rug, to which the algorithm is oblivious. Our work contributes to neural network interpretability by introducing techniques for understanding the structure of autoencoders. Code to reproduce our results can be found at https://github.com/KfirD/PersianRug .</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12101v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Cowsik, Kfir Dolev, Alex Infanger</dc:creator>
    </item>
    <item>
      <title>An informal introduction to the Parisi formula</title>
      <link>https://arxiv.org/abs/2410.12364</link>
      <description>arXiv:2410.12364v1 Announce Type: cross 
Abstract: This note is an informal presentation of spin glasses and of the Parisi formula. We also discuss some models for which the Parisi formula is not well-understood, and some partial progress that relies upon a connection with partial differential equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12364v1</guid>
      <category>math.PR</category>
      <category>cond-mat.dis-nn</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean-Christophe Mourrat</dc:creator>
    </item>
    <item>
      <title>PRefLexOR: Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning and Agentic Thinking</title>
      <link>https://arxiv.org/abs/2410.12375</link>
      <description>arXiv:2410.12375v1 Announce Type: cross 
Abstract: PRefLexOR (Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning) combines preference optimization with concepts from Reinforcement Learning to enable models to self-teach through iterative reasoning improvements. We propose a recursive learning approach that engages the model in multi-step reasoning, revisiting, and refining intermediate steps before producing a final output in training and inference phases. Through multiple training stages, the model first learns to align its reasoning with accurate decision paths by optimizing the log odds between preferred and non-preferred responses. During this process, PRefLexOR builds a dynamic knowledge graph by generating questions from random text chunks and retrieval-augmentation to contextualize relevant details from the entire training corpus. In the second stage, preference optimization enhances model performance by using rejection sampling to fine-tune reasoning quality by continually producing in-situ training data while masking the reasoning steps. Recursive optimization within a thinking token framework introduces iterative feedback loops, where the model refines reasoning, achieving deeper coherence, consistency, and adaptability. Implemented in small language models with only 3 billion parameters, we should that even tiny models can iteratively teach themselves to reason with greater depth and reflectivity. Our implementation is straightforward and can be incorporated into any existing pretrained LLM. We focus our examples on applications in biological materials science and demonstrate the method in a variety of case studies that range from in-domain to cross-domain applications. Using reasoning strategies that include thinking and reflection modalities we build a multi-agent recursive self-improving inference approach to successively improve responses via repeated sampling in inference time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12375v1</guid>
      <category>cs.AI</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mes-hall</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.CL</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Markus J. Buehler</dc:creator>
    </item>
    <item>
      <title>Fate of localization features in a one-dimensional non-Hermitian flat-band lattice with quasiperiodic modulations</title>
      <link>https://arxiv.org/abs/2311.03166</link>
      <description>arXiv:2311.03166v3 Announce Type: replace 
Abstract: We investigate the influence of quasiperiodic modulations on one-dimensional non-Hermitian diamond lattices with an artificial magnetic flux $\theta$ that possess flat bands. Our study shows that the symmetry of these modulations and the magnetic flux $\theta$ play a pivotal role in shaping the localization properties of the system. When $\theta=0$, the non-Hermitian lattice exhibits a single flat band in the crystalline case, and symmetric as well as antisymmetric modulations can induce accurate mobility edges. In contrast, when $\theta=\pi$, the clean diamond lattice manifests three dispersionless bands referred to as an "all-band-flat" (ABF) structure, irrespective of the non-Hermitian parameter. The ABF structure restricts the transition from delocalized to localized states, as all states remain localized for any finite symmetric modulation. Our numerical calculations further unveil that the ABF system subjected to antisymmetric modulations exhibits multifractal-to-localized edges. Multifractal states are predominantly concentrated in the internal region of the spectrum. Additionally, we explore the case where $\theta$ lies within the range of $(0, \pi)$, revealing a diverse array of complex localization features. Finally, we propose a classical electrical circuit scheme to realize the non-Hermitian flat-band chain with quasiperiodic modulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03166v3</guid>
      <category>cond-mat.dis-nn</category>
      <category>quant-ph</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1367-2630/ad7529</arxiv:DOI>
      <arxiv:journal_reference>New J. Phys. 26 (2024) 093007</arxiv:journal_reference>
      <dc:creator>Hui Liu, Zhanpeng Lu, Xu Xia, Zhihao Xu</dc:creator>
    </item>
    <item>
      <title>Low-resolution descriptions of model neural activity reveal hidden features and underlying system properties</title>
      <link>https://arxiv.org/abs/2405.14531</link>
      <description>arXiv:2405.14531v3 Announce Type: replace 
Abstract: The analysis of complex systems such as neural networks is made particularly difficult by the overwhelming number of their interacting components. In the absence of prior knowledge, identifying a small but informative subset of network nodes on which the analysis should focus is a rather challenging task. In this work, we address this problem in the context of a Hopfield model, which is observed through the lenses of low-resolution representations, or decimation mappings, consisting of subgroups of its neurons. The optimal, most informative mappings of the network are defined through a recently developed methodology, the mapping entropy optimisation workflow (MEOW), which performs an unsupervised analysis of the states sampled by the network and identifies those subgroups of spins whose configuration distribution is closest to that of the full, high-resolution model. Which neurons are retained in an optimal mapping is found to critically depend on the properties of the interaction matrix of the network and the level of detail employed to describe the system; by these means, it is thus possible to extract quantitative insight about the underlying properties of the high-resolution model through the analysis of its optimal low-resolution representations. These results show a tight and potentially fruitful relation between the level of detail at which the network is inspected and the type and amount of information that can be gathered from it, and showcase the MEOW approach as a practical, enabling tool for the study of complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14531v3</guid>
      <category>cond-mat.dis-nn</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Aldrigo, Roberto Menichetti, Raffaello Potestio</dc:creator>
    </item>
    <item>
      <title>An exactly solvable model for emergence and scaling laws in the multitask sparse parity problem</title>
      <link>https://arxiv.org/abs/2404.17563</link>
      <description>arXiv:2404.17563v3 Announce Type: replace-cross 
Abstract: Deep learning models can exhibit what appears to be a sudden ability to solve a new problem as training time, training data, or model size increases, a phenomenon known as emergence. In this paper, we present a framework where each new ability (a skill) is represented as a basis function. We solve a simple multi-linear model in this skill-basis, finding analytic expressions for the emergence of new skills, as well as for scaling laws of the loss with training time, data size, model size, and optimal compute. We compare our detailed calculations to direct simulations of a two-layer neural network trained on multitask sparse parity, where the tasks in the dataset are distributed according to a power-law. Our simple model captures, using a single fit parameter, the sigmoidal emergence of multiple new skills as training time, data size or model size increases in the neural network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17563v3</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoonsoo Nam, Nayara Fonseca, Seok Hyeong Lee, Chris Mingard, Ard A. Louis</dc:creator>
    </item>
  </channel>
</rss>
