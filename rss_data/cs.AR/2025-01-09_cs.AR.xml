<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Jan 2025 02:33:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A 65 nm Bayesian Neural Network Accelerator with 360 fJ/Sample In-Word GRNG for AI Uncertainty Estimation</title>
      <link>https://arxiv.org/abs/2501.04577</link>
      <description>arXiv:2501.04577v1 Announce Type: new 
Abstract: Uncertainty estimation is an indispensable capability for AI-enabled, safety-critical applications, e.g. autonomous vehicles or medical diagnosis. Bayesian neural networks (BNNs) use Bayesian statistics to provide both classification predictions and uncertainty estimation, but they suffer from high computational overhead associated with random number generation and repeated sample iterations. Furthermore, BNNs are not immediately amenable to acceleration through compute-in-memory architectures due to the frequent memory writes necessary after each RNG operation. To address these challenges, we present an ASIC that integrates 360 fJ/Sample Gaussian RNG directly into the SRAM memory words. This integration reduces RNG overhead and enables fully-parallel compute-in-memory operations for BNNs. The prototype chip achieves 5.12 GSa/s RNG throughput and 102 GOp/s neural network throughput while occupying 0.45 mm2, bringing AI uncertainty estimation to edge computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04577v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zephan M. Enciso, Boyang Cheng, Likai Pei, Jianbo Liu, Steven Davis, Ningyuan Cao, Michael Niemier</dc:creator>
    </item>
    <item>
      <title>Design of a 6-bit Threshold Inverter Quantization (TIQ) Flash Analog to Digital Converter (ADC)</title>
      <link>https://arxiv.org/abs/2501.04627</link>
      <description>arXiv:2501.04627v1 Announce Type: new 
Abstract: An ADC is used to convert analog signals into binary signals. Compared with many other types of ADCs, flash converters are incredibly quick. A typical Flash ADC consists of 2n resistors, 2n-1 op-amp comparators, and an encoder which requires more area. The resistors and comparators can be eliminated by using threshold inverter quantization (TIQ) comparators. As a voltage comparator, TIQ technique uses two cascaded CMOS inverters. So that there will be no variation in the fabrication process, and temperature. A 6-bit flash ADC based on threshold inverter quantization (TIQ) comparator was designed and software implementation was performed employing a fat tree encoder with 0.25 micrometer CMOS technology. The design consists of 2n-1 TIQ comparator arrays, a gain booster, a 1-out-of-n code generator, and a fat tree encoder. This TIQ flash ADC is simulated with the Tanner EDA Tool. Here the supply voltage is 2.5 V, input frequency of 10 kHz and 10 MHz. The power consumption of the ADC is 6.25 mW, and the propagation delay is 1.07 microseconds for 10 kHz input frequency. For 10 MHz input frequency, power consumption is 12.12 mW and propagation delay is 947.14 ms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04627v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noyon Kumar Sarkar, Moumita Roy, Md. Tariq Hasan</dc:creator>
    </item>
    <item>
      <title>FedKD-hybrid: Federated Hybrid Knowledge Distillation for Lithography Hotspot Detection</title>
      <link>https://arxiv.org/abs/2501.04066</link>
      <description>arXiv:2501.04066v1 Announce Type: cross 
Abstract: Federated Learning (FL) provides novel solutions for machine learning (ML)-based lithography hotspot detection (LHD) under distributed privacy-preserving settings. Currently, two research pipelines have been investigated to aggregate local models and achieve global consensus, including parameter/nonparameter based (also known as knowledge distillation, namely KD). While these two kinds of methods show effectiveness in specific scenarios, we note they have not fully utilized and transferred the information learned, leaving the potential of FL-based LDH remains unexplored. Thus, we propose FedKDhybrid in this study to mitigate the research gap. Specifically, FedKD-hybrid clients agree on several identical layers across all participants and a public dataset for achieving global consensus. During training, the trained local model will be evaluated on the public dataset, and the generated logits will be uploaded along with the identical layer parameters. The aggregated information is consequently used to update local models via the public dataset as a medium. We compare our proposed FedKD-hybrid with several state-of-the-art (SOTA) FL methods under ICCAD-2012 and FAB (real-world collected) datasets with different settings; the experimental results demonstrate the superior performance of the FedKD-hybrid algorithm. Our code is available at https://github.com/itsnotacie/NN-FedKD-hybrid</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04066v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuqi Li, Xingyou Lin, Kai Zhang, Chuanguang Yang, Zhongliang Guo, Jianping Gou, Yanli Li</dc:creator>
    </item>
    <item>
      <title>Modern Hardware Security: A Review of Attacks and Countermeasures</title>
      <link>https://arxiv.org/abs/2501.04394</link>
      <description>arXiv:2501.04394v1 Announce Type: cross 
Abstract: With the exponential rise in the use of cloud services, smart devices, and IoT devices, advanced cyber attacks have become increasingly sophisticated and ubiquitous. Furthermore, the rapid evolution of computing architectures and memory technologies has created an urgent need to understand and address hardware security vulnerabilities. In this paper, we review the current state of vulnerabilities and mitigation strategies in contemporary computing systems. We discuss cache side-channel attacks (including Spectre and Meltdown), power side-channel attacks (such as Simple Power Analysis, Differential Power Analysis, Correlation Power Analysis, and Template Attacks), and advanced techniques like Voltage Glitching and Electromagnetic Analysis to help understand and build robust cybersecurity defense systems and guide further research. We also examine memory encryption, focusing on confidentiality, granularity, key management, masking, and re-keying strategies. Additionally, we cover Cryptographic Instruction Set Architectures, Secure Boot, Root of Trust mechanisms, Physical Unclonable Functions, and hardware fault injection techniques. The paper concludes with an analysis of the RISC-V architecture's unique security challenges. The comprehensive analysis presented in this paper is essential for building resilient hardware security solutions that can protect against both current and emerging threats in an increasingly challenging security landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04394v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jyotiprakash Mishra, Sanjay K. Sahay</dc:creator>
    </item>
    <item>
      <title>Histogram-Equalized Quantization for logic-gated Residual Neural Networks</title>
      <link>https://arxiv.org/abs/2501.04517</link>
      <description>arXiv:2501.04517v2 Announce Type: cross 
Abstract: Adjusting the quantization according to the data or to the model loss seems mandatory to enable a high accuracy in the context of quantized neural networks. This work presents Histogram-Equalized Quantization (HEQ), an adaptive framework for linear symmetric quantization. HEQ automatically adapts the quantization thresholds using a unique step size optimization. We empirically show that HEQ achieves state-of-the-art performances on CIFAR-10. Experiments on the STL-10 dataset even show that HEQ enables a proper training of our proposed logic-gated (OR, MUX) residual networks with a higher accuracy at a lower hardware complexity than previous work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04517v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ISCAS48785.2022.9937290</arxiv:DOI>
      <arxiv:journal_reference>2022 IEEE International Symposium on Circuits and Systems (ISCAS), Austin, TX, USA, 2022, pp. 1289-1293</arxiv:journal_reference>
      <dc:creator>Van Thien Nguyen, William Guicquero, Gilles Sicard</dc:creator>
    </item>
  </channel>
</rss>
