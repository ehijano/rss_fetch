<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Aug 2025 04:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Dissecting Microbial Community Structure and Heterogeneity via Multivariate Covariate-Adjusted Clustering</title>
      <link>https://arxiv.org/abs/2508.11036</link>
      <description>arXiv:2508.11036v1 Announce Type: new 
Abstract: In microbiome studies, it is often of great interest to identify clusters or partitions of microbiome profiles within a study population and to characterize the distinctive attributes of each resulting microbial community. While raw counts or relative compositions are commonly used for such analysis, variations between clusters may be driven or distorted by subject-level covariates, reflecting underlying biological and clinical heterogeneity across individuals. Simultaneously detecting latent communities and identifying covariates that differentiate them can enhance our understanding of the microbiome and its association with health outcomes. To this end, we propose a Dirichlet-multinomial mixture regression (DMMR) model that enables joint clustering of microbiome profiles while accounting for covariates with either homogeneous or heterogeneous effects across clusters. A novel symmetric link function is introduced to facilitate covariate modeling through the compositional parameters. We develop efficient algorithms with convergence guarantees for parameter estimation and establish theoretical properties of the proposed estimators. Extensive simulation studies demonstrate the effectiveness of the method in clustering, feature selection, and heterogeneity detection. We illustrate the utility of DMMR through a comprehensive application to upper-airway microbiota data from a pediatric asthma study, uncovering distinct microbial subtypes and their associations with clinical characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11036v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongmao Liu, Xiaohui Yin, Yanjiao Zhou, Gen Li, Kun Chen</dc:creator>
    </item>
    <item>
      <title>Estimating effects of longitudinal modified treatment policies (LMTPs) on rates of change in health outcomes</title>
      <link>https://arxiv.org/abs/2508.11131</link>
      <description>arXiv:2508.11131v1 Announce Type: new 
Abstract: Longitudinal data often contains time-varying outcomes measured at multiple visits and scientific interest may lie in quantifying the effect of an intervention on an outcome's rate of change. For example, one may wish to study the progression (or trajectory) of a disease over time under different hypothetical interventions. We extend the longitudinal modified treatment policy (LMTP) methodology introduced in D\'iaz et al. (2023) to estimate effects of complex interventions on rates of change in an outcome over time. We exploit the theoretical properties of a nonparametric efficient influence function (EIF)-based estimator to introduce a novel inference framework that can be used to construct simultaneous confidence intervals for a variety of causal effects of interest and to formally test relevant global and local hypotheses about rates of change. We illustrate the utility of our framework in investigating whether a longitudinal shift intervention affects an outcome's counterfactual trajectory, as compared with no intervention. We present results from a simulation study to illustrate the performance of our inference framework in a longitudinal setting with time-varying confounding and a continuous exposure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11131v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anja Shahu, Daniel Malinsky</dc:creator>
    </item>
    <item>
      <title>Estimating Covariate Effects on Functional Connectivity using Voxel-Level fMRI Data</title>
      <link>https://arxiv.org/abs/2508.11213</link>
      <description>arXiv:2508.11213v1 Announce Type: new 
Abstract: Functional connectivity (FC) analysis of resting-state fMRI data provides a framework for characterizing brain networks and their association with participant-level covariates. Due to the high dimensionality of neuroimaging data, standard approaches often average signals within regions of interest (ROIs), which ignores the underlying spatiotemporal dependence among voxels and can lead to biased or inefficient inference. We propose to use a summary statistic -- the empirical voxel-wise correlations between ROIs -- and, crucially, model the complex covariance structure among these correlations through a new positive definite covariance function. Building on this foundation, we develop a computationally efficient two-step estimation procedure that enables statistical inference on covariate effects on region-level connectivity. Simulation studies show calibrated uncertainty quantification, and substantial gains in validity of the statistical inference over the standard averaging method. With data from the Autism Brain Imaging Data Exchange, we show that autism spectrum disorder is associated with altered FC between attention-related ROIs after adjusting for age and gender. The proposed framework offers an interpretable and statistically rigorous approach to estimation of covariate effects on FC suitable for large-scale neuroimaging studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11213v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Zhao, Brian J. Reich, Emily C. Hector</dc:creator>
    </item>
    <item>
      <title>Two-Sample Testing with Missing Data via Energy Distance: Weighting and Imputation Approaches</title>
      <link>https://arxiv.org/abs/2508.11421</link>
      <description>arXiv:2508.11421v1 Announce Type: new 
Abstract: In this paper, we address the problem of two-sample testing in the presence of missing data under a variety of missingness mechanisms. Our focus is on the well-known energy distance-based two-sample test. In addition to the standard complete-case approach, we propose a modification of the test statistic that incorporates all available data, utilizing appropriate weights. The asymptotic null distribution of the test statistic is derived and two resampling procedures for approximating the corresponding p-values are proposed. We also propose a new bootstrap method specifically designed for a test statistic based on samples completed via common imputation methods. Through an extensive simulation study, we compare all methods in terms of type I error control and statistical power across a set of sample sizes, dimensions, distributions, missingness mechanisms, and missingness rates. Based on these results, we provide general recommendations for each considered scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11421v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danijel G. Aleksi\'c, Bojana Milo\v{s}evi\'c</dc:creator>
    </item>
    <item>
      <title>Simulation-based inference using splitting schemes for partially observed diffusions in chemical reaction networks</title>
      <link>https://arxiv.org/abs/2508.11438</link>
      <description>arXiv:2508.11438v1 Announce Type: new 
Abstract: We address the problem of simulation and parameter inference for chemical reaction networks described by the chemical Langevin equation, a stochastic differential equation (SDE) representation of the dynamics of the chemical species. This is challenging for two main reasons. First, the (multi-dimensional) SDEs cannot be explicitly solved and are driven by multiplicative and non-commutative noise, requiring the development of advanced numerical schemes for their approximation and simulation. Second, not all components of the SDEs are directly observed, as the available discrete-time data are typically incomplete and/or perturbed with measurement error. We tackle these issues via three contributions. First, we show that these models can be rewritten as perturbed conditionally Cox-Ingersoll-Ross-type SDEs, i.e., each coordinate, conditioned on all other coordinates being fixed, follows an SDE with linear drift and square root diffusion coefficient perturbed by additional Brownian motions. Second, for this class of SDEs, we develop a numerical splitting scheme that preserves structural properties of the model, such as oscillations, state space and invariant distributions, unlike the commonly used Euler-Maruyama scheme. Our numerical method is robust for large integration time steps. Third, we propose a sequential Monte Carlo approximate Bayesian computation algorithm incorporating "data-conditional" simulation and sequential learning of summary statistics, allowing inference for multidimensional partially observed systems, further developing previous results on fully observed systems based on the Euler-Maruyama scheme. We validate our approach on models of interest in chemical reaction networks, such as the stochastic Repressilator, Lotka-Volterra, and two-pool systems, demonstrating its effectiveness, in terms of both numerical and inferential accuracy, and reduced computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11438v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Petar Jovanovski, Andrew Golightly, Umberto Picchini, Massimiliano Tamborrino</dc:creator>
    </item>
    <item>
      <title>Contrastive CUR: Interpretable Joint Feature and Sample Selection for Case-Control Studies</title>
      <link>https://arxiv.org/abs/2508.11557</link>
      <description>arXiv:2508.11557v1 Announce Type: new 
Abstract: Dimension reduction is an essential tool for analyzing high dimensional data. Most existing methods, including principal component analysis (PCA), as well as their extensions, provide principal components that are often linear combinations of features, which are often challenging to interpret. CUR decomposition, another matrix decomposition technique, is a more interpretable and efficient alternative, offers simultaneous feature and sample selection. Despite this, many biomedical studies involve two groups: a foreground (treatment or case) group and a background (control) group, where the objective is to identify features unique to or enriched in the foreground. This need for contrastive dimension reduction is not well addressed by existing CUR methods, nor by contrastive approaches rooted in PCAs. Furthermore, they fail to address a key challenge in biomedical studies: the need for selecting samples unique to the foreground. In this paper, we address this gap by proposing a Contrastive CUR (CCUR), a novel method specifically designed for case-control studies. Through extensive experiments, we demonstrate that CCUR outperforms existing techniques in isolating biologically relevant features as well as identifying sample-specific responses unique to the foreground, offering deeper insights into case-control biomedical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11557v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Zhang, Michael Love, Didong Li</dc:creator>
    </item>
    <item>
      <title>Approximate Factor Model with S-vine Copula Structure</title>
      <link>https://arxiv.org/abs/2508.11619</link>
      <description>arXiv:2508.11619v1 Announce Type: new 
Abstract: We propose a novel framework for approximate factor models that integrates an S-vine copula structure to capture complex dependencies among common factors. Our estimation procedure proceeds in two steps: first, we apply principal component analysis (PCA) to extract the factors; second, we employ maximum likelihood estimation that combines kernel density estimation for the margins with an S-vine copula to model the dependence structure. Jointly fitting the S-vine copula with the margins yields an oblique factor rotation without resorting to ad hoc restrictions or traditional projection pursuit methods. Our theoretical contributions include establishing the consistency of the rotation and copula parameter estimators, developing asymptotic theory for the factor-projected empirical process under dependent data, and proving the uniform consistency of the projected entropy estimators. Simulation studies demonstrate convergence with respect to both the dimensionality and the sample size. We further assess model performance through Value-at-Risk (VaR) estimation via Monte Carlo methods and apply our methodology to the daily returns of S&amp;P 500 Index constituents to forecast the VaR of S&amp;P 500 index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11619v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jialing Han, Yu-Ning Li</dc:creator>
    </item>
    <item>
      <title>Deconfounding via Profiled Transfer Learning</title>
      <link>https://arxiv.org/abs/2508.11622</link>
      <description>arXiv:2508.11622v1 Announce Type: new 
Abstract: Unmeasured confounders are a major source of bias in regression-based effect estimation and causal inference. In this paper, we advocate a new profiled transfer learning framework, ProTrans, to address confounding effects in the target dataset, when additional source datasets that possess similar confounding structures are available. We introduce the concept of profiled residuals to characterize the shared confounding patterns between source and target datasets. By incorporating these profiled residuals into the target debiasing step, we effectively mitigates the latent confounding effects. We also propose a source selection strategy to enhance robustness of ProTrans against noninformative sources. As a byproduct, ProTrans can also be utilized to estimate treatment effects when potential confounders exist, without the use of auxiliary features such as instrumental or proxy variables, which are often challenging to select in practice. Theoretically, we prove that the resulting estimated model shift from sources to target is confounding-free without any assumptions imposed on the true confounding structure, and that the target parameter estimation achieves the minimax optimal rate under mild conditions. Simulated and real-world experiments validate the effectiveness of ProTrans and support the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11622v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyuan Chen, Yifan Jiang, Jingyuan Liu, Fang Yao</dc:creator>
    </item>
    <item>
      <title>Counterfactual Survival Q Learning for Longitudinal Randomized Trials via Buckley James Boosting</title>
      <link>https://arxiv.org/abs/2508.11060</link>
      <description>arXiv:2508.11060v1 Announce Type: cross 
Abstract: We propose a Buckley James (BJ) Boost Q learning framework for estimating optimal dynamic treatment regimes under right censored survival data, tailored for longitudinal randomized clinical trial settings. The method integrates accelerated failure time models with iterative boosting techniques, including componentwise least squares and regression trees, within a counterfactual Q learning framework. By directly modeling conditional survival time, BJ Boost Q learning avoids the restrictive proportional hazards assumption and enables unbiased estimation of stage specific Q functions. Grounded in potential outcomes, this framework ensures identifiability of the optimal treatment regime under standard causal assumptions. Compared to Cox based Q learning, which relies on hazard modeling and may suffer from bias under misspecification, our approach provides robust and flexible estimation. Simulation studies and analysis of the ACTG175 HIV trial demonstrate that BJ Boost Q learning yields higher accuracy in treatment decision making, especially in multistage settings where bias can accumulate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11060v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeongjin Lee, Jong-Min Kim</dc:creator>
    </item>
    <item>
      <title>Functional Analysis of Variance for Association Studies</title>
      <link>https://arxiv.org/abs/2508.11069</link>
      <description>arXiv:2508.11069v1 Announce Type: cross 
Abstract: While progress has been made in identifying common genetic variants associated with human diseases, for most of common complex diseases, the identified genetic variants only account for a small proportion of heritability. Challenges remain in finding additional unknown genetic variants predisposing to complex diseases. With the advance in next-generation sequencing technologies, sequencing studies have become commonplace in genetic research. The ongoing exome-sequencing and whole-genome-sequencing studies generate a massive amount of sequencing variants and allow researchers to comprehensively investigate their role in human diseases. The discovery of new disease-associated variants can be enhanced by utilizing powerful and computationally efficient statistical methods. In this paper, we propose a functional analysis of variance (FANOVA) method for testing an association of sequence variants in a genomic region with a qualitative trait. The FANOVA has a number of advantages: (1) it tests for a joint effect of gene variants, including both common and rare; (2) it fully utilizes linkage disequilibrium and genetic position information; and (3) allows for either protective or risk-increasing causal variants. Through simulations, we show that FANOVA outperform two popularly used methods - SKAT and a previously proposed method based on functional linear models (FLM), - especially if a sample size of a study is small and/or sequence variants have low to moderate effects. We conduct an empirical study by applying three methods (FANOVA, SKAT and FLM) to sequencing data from Dallas Heart Study. While SKAT and FLM respectively detected ANGPTL 4 and ANGPTL 3 associated with obesity, FANOVA was able to identify both genes associated with obesity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11069v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1371/journal.pone.0105074</arxiv:DOI>
      <dc:creator>Olga A. Vsevolozhskaya, Dmitri V. Zaykin, Mark C. Greenwood, Changshuai Wei, Qing Lu</dc:creator>
    </item>
    <item>
      <title>Nonparametric learning of stochastic differential equations from sparse and noisy data</title>
      <link>https://arxiv.org/abs/2508.11597</link>
      <description>arXiv:2508.11597v1 Announce Type: cross 
Abstract: The paper proposes a systematic framework for building data-driven stochastic differential equation (SDE) models from sparse, noisy observations. Unlike traditional parametric approaches, which assume a known functional form for the drift, our goal here is to learn the entire drift function directly from data without strong structural assumptions, making it especially relevant in scientific disciplines where system dynamics are partially understood or highly complex. We cast the estimation problem as minimization of the penalized negative log-likelihood functional over a reproducing kernel Hilbert space (RKHS). In the sparse observation regime, the presence of unobserved trajectory segments makes the SDE likelihood intractable. To address this, we develop an Expectation-Maximization (EM) algorithm that employs a novel Sequential Monte Carlo (SMC) method to approximate the filtering distribution and generate Monte Carlo estimates of the E-step objective. The M-step then reduces to a penalized empirical risk minimization problem in the RKHS, whose minimizer is given by a finite linear combination of kernel functions via a generalized representer theorem. To control model complexity across EM iterations, we also develop a hybrid Bayesian variant of the algorithm that uses shrinkage priors to identify significant coefficients in the kernel expansion. We establish important theoretical convergence results for both the exact and approximate EM sequences. The resulting EM-SMC-RKHS procedure enables accurate estimation of the drift function of stochastic dynamical systems in low-data regimes and is broadly applicable across domains requiring continuous-time modeling under observational constraints. We demonstrate the effectiveness of our method through a series of numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11597v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnab Ganguly, Riten Mitra, Jinpu Zhou</dc:creator>
    </item>
    <item>
      <title>Enhancing Dose Selection in Phase I Cancer Trials: Extending the Bayesian Logistic Regression Model with Non-DLT Adverse Events Integration</title>
      <link>https://arxiv.org/abs/2405.13767</link>
      <description>arXiv:2405.13767v4 Announce Type: replace 
Abstract: This work introduces the Burdened Bayesian Logistic Regression Model (BBLRM), an enhancement of the Bayesian Logistic Regression Model (BLRM) for dose-finding in phase I oncology trials. The BLRM determines the maximum tolerated dose (MTD) based on dose limiting toxicities (DLTs). However, clinicians often perceive model-based designs like BLRM as complex and less conservative than rule-based designs, such as the widely used 3+3 method. To address these concerns, BBLRM incorporates non-DLT adverse events (nDLTAEs), which, although not severe enough to be DLTs, indicate potential toxicity risks at higher doses. BBLRM introduces an additional parameter {\delta} to account for nDLTAEs, adjusting toxicity probability estimates to make dose escalation more conservative while maintaining accurate MTD allocation. This parameter, generated basing on the proportion of patients experiencing nDLTAEs, is tuned to balance conservatism with model performance, reducing the risk of selecting overly toxic doses. Additionally, involving clinicians in identifying nDLTAEs enhances their engagement in the dose-finding process. A simulation study compares BBLRM with two other BLRM methods and a two-stage Continual Reassessment Method (CRM) incorporating nDLTAEs. Results show that BBLRM reduces the proportion of toxic doses selected as MTD without compromising the accuracy in MTD identification. These findings suggest that integrating nDLTAEs can improve the safety and acceptance of model-based designs in phase I oncology trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13767v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Nizzardo, Luca Genetti, Marco Pergher</dc:creator>
    </item>
    <item>
      <title>On MCMC mixing for predictive inference under unidentified transformation models</title>
      <link>https://arxiv.org/abs/2411.01382</link>
      <description>arXiv:2411.01382v2 Announce Type: replace 
Abstract: Reliable Bayesian predictive inference has long been an open problem under unidentified transformation models, since the Markov Chain Monte Carlo (MCMC) chains of posterior predictive distribution (PPD) values are generally poorly mixed. We address the poorly mixed PPD value chains under unidentified transformation models through an adaptive scheme for prior adjustment. Specifically, we originate a conception of sufficient informativeness, which explicitly quantifies the information level provided by nonparametric priors, and assesses MCMC mixing by comparison with the within-chain MCMC variance. We formulate the prior information level by a set of hyperparameters induced from the nonparametric prior elicitation with an analytic expression, which is guaranteed by asymptotic theory for the posterior variance under unidentified transformation models. The analytic prior information level consequently drives a hyperparameter tuning procedure to achieve MCMC mixing. The proposed method is general enough to cover various data domains through a multiplicative error working model. Comprehensive simulations and real-world data analysis demonstrate that our method successfully achieves MCMC mixing and outperforms state-of-the-art competitors in predictive capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01382v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chong Zhong, Jin Yang, Junshan Shen, Zhaohai Li, Catherine C. Liu</dc:creator>
    </item>
    <item>
      <title>Ordinal regression for meta-analysis of test accuracy: a flexible approach for utilising all threshold data</title>
      <link>https://arxiv.org/abs/2505.23393</link>
      <description>arXiv:2505.23393v2 Announce Type: replace 
Abstract: Standard methods for meta-analysis and network-meta-analysis of test accuracy do not fully utilise available evidence, as they analyse thresholds separately, resulting in a loss of data unless every study reports all thresholds - which rarely occurs. Furthermore, previously proposed "multiple threshold" models introduce different problems: making overly restrictive assumptions, or failing to provide summary sensitivity and specificity estimates across thresholds.
  To address this, we proposed a series of ordinal regression-based models, representing a natural extension of established frameworks. Our approach offers notable advantages: (i) complete data utilisation: rather than discarding information like standard methods, we incorporate all threshold data; (ii) threshold-specific inference: by providing summary accuracy estimates across thresholds, our models deliver critical information for clinical decision-making; (iii) enhanced flexibility: unlike previous "multiple thresholds" approaches, our methodology imposes fewer assumptions, leading to better accuracy estimates; (iv) our models use an induced-Dirichlet framework, allowing for either fixed-effects or random-effects cutpoint parameters, whilst also allowing for intuitive cutpoint priors.
  Our (ongoing) simulation study - based on real-world anxiety and depression screening data - demonstrates notably better accuracy estimates than previous approaches, even when the number of categories is high.
  Furthermore, we implemented these models in a user-friendly R package - MetaOrdDTA (https://github.com/CerulloE1996/MetaOrdDTA). The package uses Stan and produces MCMC summaries, sROC plots with credible/prediction regions, and meta-regression.
  Overall, our approach establishes a more comprehensive framework for synthesising test accuracy data, better serving systematic reviewers and clinical decision-makers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23393v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enzo Cerullo, Haley E. Jones, Tim Lucas, Nicola J. Cooper, Alex J. Sutton</dc:creator>
    </item>
    <item>
      <title>Coverage correlation: detecting singular dependencies between random variables</title>
      <link>https://arxiv.org/abs/2508.06402</link>
      <description>arXiv:2508.06402v2 Announce Type: replace 
Abstract: We introduce the coverage correlation coefficient, a novel nonparametric measure of statistical association designed to quantifies the extent to which two random variables have a joint distribution concentrated on a singular subset with respect to the product of the marginals. Our correlation statistic consistently estimates an $f$-divergence between the joint distribution and the product of the marginals, which is 0 if and only if the variables are independent and 1 if and only if the copula is singular. Using Monge--Kantorovich ranks, the coverage correlation naturally extends to measure association between random vectors. It is distribution-free, admits an analytically tractable asymptotic null distribution, and can be computed efficiently, making it well-suited for detecting complex, potentially nonlinear associations in large-scale pairwise testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06402v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuzhi Yang, Mona Azadkia, Tengyao Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian Models for Joint Selection of Features and Auto-Regressive Lags: Theory and Applications in Environmental and Financial Forecasting</title>
      <link>https://arxiv.org/abs/2508.10055</link>
      <description>arXiv:2508.10055v2 Announce Type: replace 
Abstract: We develop a Bayesian framework for variable selection in linear regression with autocorrelated errors, accommodating lagged covariates and autoregressive structures. This setting occurs in time series applications where responses depend on contemporaneous or past explanatory variables and persistent stochastic shocks, including financial modeling, hydrological forecasting, and meteorological applications requiring temporal dependency capture. Our methodology uses hierarchical Bayesian models with spike-and-slab priors to simultaneously select relevant covariates and lagged error terms. We propose an efficient two-stage MCMC algorithm separating sampling of variable inclusion indicators and model parameters to address high-dimensional computational challenges. Theoretical analysis establishes posterior selection consistency under mild conditions, even when candidate predictors grow exponentially with sample size, common in modern time series with many potential lagged variables. Through simulations and real applications (groundwater depth prediction, S&amp;P 500 log returns modeling), we demonstrate substantial gains in variable selection accuracy and predictive performance. Compared to existing methods, our framework achieves lower MSPE, improved true model component identification, and greater robustness with autocorrelated noise, underscoring practical utility for model interpretation and forecasting in autoregressive settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10055v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Alokesh Manna, Sujit K. Ghosh</dc:creator>
    </item>
    <item>
      <title>Synthesizing Evidence: Data-Pooling as a Tool for Treatment Selection in Online Experiments</title>
      <link>https://arxiv.org/abs/2508.10331</link>
      <description>arXiv:2508.10331v2 Announce Type: replace 
Abstract: Randomized experiments are the gold standard for causal inference but face significant challenges in business applications, including limited traffic allocation, the need for heterogeneous treatment effect estimation, and the complexity of managing overlapping experiments. These factors lead to high variability in treatment effect estimates, making data-driven policy roll out difficult. To address these issues, we introduce the data pooling treatment roll-out (DPTR) framework, which enhances policy roll-out by pooling data across experiments rather than focusing narrowly on individual ones. DPTR can effectively accommodate both overlapping and non-overlapping traffic scenarios, regardless of linear or nonlinear model specifications. We demonstrate the framework's robustness through a three-pronged validation: (a) theoretical analysis shows that DPTR surpasses the traditional difference-in-mean and ordinary least squares methods under non-overlapping experiments, particularly when the number of experiments is large; (b) synthetic simulations confirm its adaptability in complex scenarios with overlapping traffic, rich covariates and nonlinear specifications; and (c) empirical applications to two experimental datasets from real world platforms, demonstrating its effectiveness in guiding customized policy roll-outs for subgroups within a single experiment, as well as in coordinating policy deployments across multiple experiments with overlapping scenarios. By reducing estimation variability to improve decision-making effectiveness, DPTR provides a scalable, practical solution for online platforms to better leverage their experimental data in today's increasingly complex business environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10331v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenkang Peng, Chengzhang Li, Ying Rong, Renyu Zhang</dc:creator>
    </item>
    <item>
      <title>How Much Does Home Field Advantage Matter in Soccer Games? A Causal Inference Approach for English Premier League Analysis</title>
      <link>https://arxiv.org/abs/2205.07193</link>
      <description>arXiv:2205.07193v2 Announce Type: replace-cross 
Abstract: In many sports, it is commonly believed that the home team has an advantage over the visiting team, known as the home field advantage. Yet its causal effect on team performance is largely unknown. In this paper, we propose a novel causal inference approach to study the causal effect of home field advantage in English Premier League. We develop a hierarchical causal model and show that both league level and team level causal effects are identifiable and can be conveniently estimated. We further develop an inference procedure for the proposed estimators and demonstrate its excellent numerical performance via simulation studies. We implement our method on the 2020-21 English Premier League data and assess the causal effect of home advantage on eleven summary statistics that measure the offensive and defensive performance and referee-assessed disciplinary outcomes. We find that the home field advantage resides more heavily in offensive statistics than it does in defensive or referee statistics. We also find evidence that teams that had lower rankings retain a higher home field advantage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.07193v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Wang, Katherine Price, Hengrui Cai, Weining Shen, Zhanrui Cai, Guanyu Hu</dc:creator>
    </item>
    <item>
      <title>A time warping model for seasonal data with application to age estimation from narwhal tusks</title>
      <link>https://arxiv.org/abs/2410.05843</link>
      <description>arXiv:2410.05843v2 Announce Type: replace-cross 
Abstract: Signals with varying periodicity frequently appear in real-world phenomena, necessitating the development of efficient modelling techniques to map the measured nonlinear timeline to linear time. Here we propose a regression model that allows for a representation of periodic and dynamic patterns observed in time series data. The model incorporates a hidden strictly positive stochastic process that represents the instantaneous frequency, allowing the model to adapt and accurately capture varying time scales. A case study focusing on age estimation of narwhal tusks is presented, where cyclic element signals associated with annual growth layer groups are analyzed. We apply the methodology to data from one such tusk collected in West Greenland and use the fitted model to estimate the age of the narwhal. The proposed method is validated using simulated signals with known cycle counts and practical considerations and modelling challenges are discussed in detail. This research contributes to the field of time series analysis, providing a tool and valuable insights for understanding and modeling complex cyclic patterns in diverse domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05843v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars N{\o}rtoft Reiter, Mads Peter Heide-J{\o}rgensen, Eva Garde, Adeline Samson, Susanne Ditlevsen</dc:creator>
    </item>
    <item>
      <title>Vecchia Gaussian Processes: Probabilistic Properties, Minimax Rates and Methodological Developments</title>
      <link>https://arxiv.org/abs/2410.10649</link>
      <description>arXiv:2410.10649v3 Announce Type: replace-cross 
Abstract: Gaussian Processes (GPs) are widely used to model dependencies in spatial statistics and machine learning; however, exact inference is computationally intractable, with a time complexity of $O(n^3)$. Vecchia approximation allows scalable Bayesian inference of GPs by introducing sparsity in the spatial dependency structure characterized by a directed acyclic graph (DAG). Despite its practical popularity, the approach lacks rigorous theoretical foundations, and the choice of DAG structure remains an open question. In this paper, we systematically study Vecchia GPs as standalone stochastic processes, uncovering key probabilistic and statistical properties. For probabilistic properties, we prove that the conditional distributions of the Mat\'{e}rn GPs, as well as their Vecchia approximations, can be characterized by polynomial interpolations. This allows us to prove a series of results regarding small ball probabilities and Reproducing Kernel Hilbert Spaces (RKHSs) of Vecchia GPs. For the statistical methodology, we provide a principled guideline for selecting parent sets as norming sets with fixed cardinality, and we develop algorithms along these guidelines. In terms of theoretical guarantees, we establish posterior contraction rates for Vecchia GPs in the nonparametric regression model and show that minimax-optimal rates are attained under oracle rescaling or hierarchical Bayesian tuning. We demonstrate our theoretical results and methodology through numerical studies and provide an efficient implementation of our methods in C++, with interfaces in R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10649v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Botond Szabo, Yichen Zhu</dc:creator>
    </item>
    <item>
      <title>Beyond algorithm hyperparameters: on preprocessing hyperparameters and associated pitfalls in machine learning applications</title>
      <link>https://arxiv.org/abs/2412.03491</link>
      <description>arXiv:2412.03491v2 Announce Type: replace-cross 
Abstract: Adequately generating and evaluating prediction models based on supervised machine learning (ML) is often challenging, especially for less experienced users in applied research areas. Special attention is required in settings where the model generation process involves hyperparameter tuning, i.e. data-driven optimization of different types of hyperparameters to improve the predictive performance of the resulting model. Discussions about tuning typically focus on the hyperparameters of the ML algorithm (e.g., the minimum number of observations in each terminal node for a tree-based algorithm). In this context, it is often neglected that hyperparameters also exist for the preprocessing steps that are applied to the data before it is provided to the algorithm (e.g., how to handle missing feature values in the data). As a consequence, users experimenting with different preprocessing options to improve model performance may be unaware that this constitutes a form of hyperparameter tuning, albeit informal and unsystematic, and thus may fail to report or account for this optimization. To illuminate this issue, this paper reviews and empirically illustrates different procedures for generating and evaluating prediction models, explicitly addressing the different ways algorithm and preprocessing hyperparameters are typically handled by applied ML users. By highlighting potential pitfalls, especially those that may lead to exaggerated performance claims, this review aims to further improve the quality of predictive modeling in ML applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03491v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christina Sauer, Anne-Laure Boulesteix, Luzia Han{\ss}um, Farina Hodiamont, Claudia Bausewein, Theresa Ullmann</dc:creator>
    </item>
    <item>
      <title>Adaptive Bayesian Optimization for Robust Identification of Stochastic Dynamical Systems</title>
      <link>https://arxiv.org/abs/2503.06381</link>
      <description>arXiv:2503.06381v2 Announce Type: replace-cross 
Abstract: This paper deals with the identification of linear stochastic dynamical systems, where the unknowns include system coefficients and noise variances. Conventional approaches that rely on the maximum likelihood estimation (MLE) require nontrivial gradient computations and are prone to local optima. To overcome these limitations, a sample-efficient global optimization method based on Bayesian optimization (BO) is proposed, using an ensemble Gaussian process (EGP) surrogate with weighted kernels from a predefined dictionary. This ensemble enables a richer function space and improves robustness over single-kernel BO. Each objective evaluation is efficiently performed via Kalman filter recursion. Extensive experiments across parameter settings and sampling intervals show that the EGP-based BO consistently outperforms MLE via steady-state filtering and expectation-maximization (whose derivation is a side contribution) in terms of RMSE and statistical consistency. Unlike the ensemble variant, single-kernel BO does not always yield such gains, underscoring the benefits of model averaging. Notably, the BO-based estimator achieves RMSE below the classical Cramer-Rao bound, particularly for the inverse time constant, long considered difficult to estimate. This counterintuitive outcome is attributed to a data-driven prior implicitly induced by the GP surrogate in BO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06381v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinwen Xu, Qin Lu, Yaakov Bar-Shalom</dc:creator>
    </item>
  </channel>
</rss>
