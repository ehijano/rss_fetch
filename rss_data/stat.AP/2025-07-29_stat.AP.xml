<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Jul 2025 01:29:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Consistency and Central Limit Results for the Maximum Likelihood Estimator in the Admixture Model</title>
      <link>https://arxiv.org/abs/2507.19564</link>
      <description>arXiv:2507.19564v1 Announce Type: new 
Abstract: In the Admixture Model, the probability of an individual having a certain number of alleles at a specific marker depends on the allele frequencies in $K$ ancestral populations and the fraction of the individual's genome originating from these ancestral populations.
  This study investigates consistency and central limit results of maximum likelihood estimators (MLEs) for the ancestry and the allele frequencies in the Admixture Model, complimenting previous work by \cite{pfaff2004information, pfaffelhuber2022central}. Specifically, we prove consistency of the MLE, if we estimate the allele frequencies and the ancestries. Furthermore, we prove central limit theorems, if we estimate the ancestry of a finite number of individuals and the allele frequencies of finitely many markers, also addressing the case where the true ancestry lies on the boundary of the parameter space.
  Finally, we use the new theory to quantify the uncertainty of the MLEs for the data of \citet{10002015global}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19564v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carola Sophia Heinzel</dc:creator>
    </item>
    <item>
      <title>A Bayesian Additive Regression Trees Model for zero and one inflated data for Predicting Individual Treatment Effects in Alcohol Use Disorder Trials</title>
      <link>https://arxiv.org/abs/2507.19848</link>
      <description>arXiv:2507.19848v1 Announce Type: new 
Abstract: Alcohol Use Disorder (AUD) treatment presents high individual-level heterogeneity, with outcomes ranging from complete abstinence to persistent heavy drinking. This variability-driven by complex behavioral, social, and environmental factors-poses major challenges for treatment evaluation and individualized decision-making. In particular, accurately modeling bounded semicontinuous outcomes and estimating predictive individual treatment effects (PITEs) remains methodologically demanding.
  For the pre-registered PITE analysis of Project MATCH, we developed HOBZ-BART, a novel Bayesian nonparametric model tailored for semicontinuous outcomes concentrated at clinically meaningful boundary values (0 and 1). The model decomposes the outcome into three components-abstinence, partial drinking, and persistent use-via a sequential hurdle structure, offering interpretability aligned with clinical reasoning. A shared Bayesian Additive Regression Tree (BART) ensemble captures nonlinear effects and covariate interactions across components, while a scalable Beta-likelihood approximation enables efficient, conjugate-friendly posterior computation.
  Through extensive simulations we demonstrate that HOBZ-BART outperforms traditional zero-one inflated Beta (ZOIB) model in predictive accuracy, computational efficiency, and PITE estimation. We then present the primary PITE analysis of the MATCH trial using HOBZ-BART which enables clinically meaningful comparisons of Cognitive Behavioral Therapy (CBT), Motivational Enhancement Therapy (MET), and Twelve Step Facilitation (TSF), offering personalized treatment insights.
  HOBZ-BART combines statistical rigor with clinical interpretability, addressing a critical need in addiction research for models that support individualized, data-driven care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19848v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pamela Solano, M Lee Van Horn, Kyle Walters, Philipp Besendorfer, Alena Kuhlemeier, Manel Mart\'inez-Ram\'on, Thomas Jaki</dc:creator>
    </item>
    <item>
      <title>Computation of Optimal Type-II Progressing Censoring Scheme Using Genetic Algorithm Approach</title>
      <link>https://arxiv.org/abs/2507.20001</link>
      <description>arXiv:2507.20001v1 Announce Type: new 
Abstract: The experimenter must perform a legitimate search in the entire set of feasible censoring schemes to identify the optimal type II progressive censoring scheme, when applied to a life-testing experiment. Current recommendations are limited to small sample sizes. Exhaustive search strategies are not practically feasible for large sample sizes. This paper proposes a meta-heuristic algorithm based on the genetic algorithm for large sample sizes. The algorithm is found to provide optimal or near-optimal solutions for small sample sizes and large sample sizes. Our suggested optimal criterion is based on the cost function and is scale-invariant for both location-scale and log-location-scale distribution families. To investigate how inaccurate parameter values or cost coefficients may affect the optimal solution, a sensitivity analysis is also taken into account.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20001v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ujjwal Roy, Ritwik Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Exploring Causal Mediation Analysis in Bacterial Vaginosis Challenges</title>
      <link>https://arxiv.org/abs/2507.20547</link>
      <description>arXiv:2507.20547v1 Announce Type: new 
Abstract: Bacterial Vaginosis (BV) affects nearly 23-29% of women worldwide and increases risk of miscarriage, preterm birth, and sexually transmitted infections. It involves a shift in the vaginal microbiome from Lactobacillus dominance to a diverse bacterial composition. Understanding causal pathways linking behavioral factors to BV risk is essential for effective intervention. Observational studies have identified pathogenic bacteria associated with BV, and causal mediation analysis can clarify how behaviors like sexual activity influence the microbiome. Analyzing microbiome data is complex due to its high-dimensional and compositional nature, often challenging traditional statistical methods, especially with small samples. This article presents various approaches to measure causal mediation effects, emphasizing the benefits of an empirical distribution method for small samples, and outlines models for mediators, exposure, and outcomes, aiming to identify taxa that mediate the exposure-outcome relationship in BV, concluding with a revisit of the motivational example and model identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20547v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Debarghya Nandi, Soumya Sahu, Supriya Mehta, Dulal K. Bhaumik</dc:creator>
    </item>
    <item>
      <title>Time-to-Event Modeling with Pseudo-Observations in Federated Settings</title>
      <link>https://arxiv.org/abs/2507.20558</link>
      <description>arXiv:2507.20558v1 Announce Type: new 
Abstract: In multi-center clinical studies, concerns about patient privacy often prohibit pooling individual-level time-to-event data. We propose a non-iterative, one-shot federated framework using distributed pseudo-observations, derived from a sequentially updated Kaplan-Meier estimator and fitted with renewable generalized linear models. This framework enables the estimation of survival probabilities at specified landmark times and accommodates both time-invariant and time-varying covariate effects. To capture site-level heterogeneity, we introduce a soft-thresholding debiasing procedure that adaptively shrinks local estimates toward the global fit. Through extensive simulations across varying event rates and site-size distributions, our method demonstrates performance comparable to pooled Cox and the one-shot Optimal Distributed Aggregation (ODAC) models, with added flexibility to capture non-proportional hazards. Applied to pediatric obesity data from the Chicago Area Patient-Centered Outcomes Research Network (CAPriCORN), which comprises four different sites and includes a total of 45,865 patients. The federated pseudo value regression model produced estimates of both time-constant and time-varying hazard ratios that closely aligned with those obtained from the pooled analysis, demonstrating its utility as a robust and privacy-preserving alternative for collaborative survival research. To further address potential heterogeneity across sites, we applied a covariate-wise debiasing algorithm, enabling site-level adjustments while preserving consistency with the global model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20558v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyojung Jang, Malcolm Risk, Yaojie Wang, Norrina Bai Allen, Xu Shi, Lili Zhao</dc:creator>
    </item>
    <item>
      <title>RestoreAI -- Pattern-based Risk Estimation Of Remaining Explosives</title>
      <link>https://arxiv.org/abs/2507.19873</link>
      <description>arXiv:2507.19873v1 Announce Type: cross 
Abstract: Landmine removal is a slow, resource-intensive process affecting over 60 countries. While AI has been proposed to enhance explosive ordnance (EO) detection, existing methods primarily focus on object recognition, with limited attention to prediction of landmine risk based on spatial pattern information. This work aims to answer the following research question: How can AI be used to predict landmine risk from landmine patterns to improve clearance time efficiency? To that effect, we introduce RestoreAI, an AI system for pattern-based risk estimation of remaining explosives. RestoreAI is the first AI system that leverages landmine patterns for risk prediction, improving the accuracy of estimating the residual risk of missing EO prior to land release. We particularly focus on the implementation of three instances of RestoreAI, respectively, linear, curved and Bayesian pattern deminers. First, the linear pattern deminer uses linear landmine patterns from a principal component analysis (PCA) for the landmine risk prediction. Second, the curved pattern deminer uses curved landmine patterns from principal curves. Finally, the Bayesian pattern deminer incorporates prior expert knowledge by using a Bayesian pattern risk prediction. Evaluated on real-world landmine data, RestoreAI significantly boosts clearance efficiency. The top-performing pattern-based deminers achieved a 14.37 percentage point increase in the average share of cleared landmines per timestep and required 24.45% less time than the best baseline deminer to locate all landmines. Interestingly, linear and curved pattern deminers showed no significant performance difference, suggesting that more efficient linear patterns are a viable option for risk prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19873v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bj\"orn Kischelewski, Benjamin Guedj, David Wahl</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Circular Data</title>
      <link>https://arxiv.org/abs/2507.19889</link>
      <description>arXiv:2507.19889v2 Announce Type: cross 
Abstract: In causal inference, a fundamental task is to estimate the effect resulting from a specific treatment, which is often handled with inverse probability weighting. Despite an abundance of attention to the advancement of this task, most articles have focused on linear data rather than circular data, which are measured in angles. In this article, we extend the causal inference framework to accommodate circular data. Specifically, two new treatment effects, average direction treatment effect (ADTE) and average length treatment effect (ALTE), are introduced to offer a proper causal explanation for these data. As the average direction and average length describe the location and concentration of a random sample of circular data, the ADTE and ALTE measure the change in direction and length between two counterfactual outcomes. With inverse probability weighting, we propose estimators that exhibit ideal theoretical properties, which are validated by a simulation study. To illustrate the practical utility of our estimator, we analyze the effect of different job types on dispatchers' sleep patterns using data from Federal Railroad Administration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19889v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuan-Hsun Wu</dc:creator>
    </item>
    <item>
      <title>Effective Bayesian Modeling of Large Spatiotemporal Count Data Using Autoregressive Gamma Processes</title>
      <link>https://arxiv.org/abs/2507.19915</link>
      <description>arXiv:2507.19915v1 Announce Type: cross 
Abstract: We put forward a new Bayesian modeling strategy for spatiotemporal count data that enables efficient posterior sampling. Most previous models for such data decompose logarithms of the response Poisson rates into fixed effects and spatial random effects, where the latter is typically assumed to follow a latent Gaussian process, the conditional autoregressive model, or the intrinsic conditional autoregressive model. Since log-Gaussian is not conjugate to Poisson, such implementations must resort to either approximation methods like INLA or Metropolis moves on latent states in MCMC algorithms for model fitting and exhibit several approximation and posterior sampling challenges. Instead of modeling logarithms of spatiotemporal frailties jointly as a Gaussian process, we construct a spatiotemporal autoregressive gamma process guaranteed stationary across the time dimension. We decompose latent Poisson variables to permit fully conjugate Gibbs sampling of spatiotemporal frailties and design a sparse spatial dependence structure to get a linear computational complexity that facilitates efficient posterior computation. Our model permits convenient Bayesian predictive machinery based on posterior samples that delivers satisfactory performance in predicting at new spatial locations and time intervals. We have performed extensive simulation experiments and real data analyses, which corroborated our model's accurate parameter estimation, model fitting, and out-of-sample prediction capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19915v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Cheng, Cheng Li</dc:creator>
    </item>
    <item>
      <title>Predicting Parkinson's Disease Progression Using Statistical and Neural Mixed Effects Models: A Comparative Study on Longitudinal Biomarkers</title>
      <link>https://arxiv.org/abs/2507.20058</link>
      <description>arXiv:2507.20058v1 Announce Type: cross 
Abstract: Predicting Parkinson's Disease (PD) progression is crucial, and voice biomarkers offer a non-invasive method for tracking symptom severity (UPDRS scores) through telemonitoring. Analyzing this longitudinal data is challenging due to within-subject correlations and complex, nonlinear patient-specific progression patterns. This study benchmarks LMMs against two advanced hybrid approaches: the Generalized Neural Network Mixed Model (GNMM) (Mandel 2021), which embeds a neural network within a GLMM structure, and the Neural Mixed Effects (NME) model (Wortwein 2023), allowing nonlinear subject-specific parameters throughout the network. Using the Oxford Parkinson's telemonitoring voice dataset, we evaluate these models' performance in predicting Total UPDRS to offer practical guidance for PD research and clinical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20058v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ran Tong, Lanruo Wang, Tong Wang, Wei Yan</dc:creator>
    </item>
    <item>
      <title>A Markov switching discrete-time Hawkes process: application to the monitoring of bats behavior</title>
      <link>https://arxiv.org/abs/2507.20153</link>
      <description>arXiv:2507.20153v1 Announce Type: cross 
Abstract: Over the past few decades, the Hawkes process has become a popular framework for modeling temporal events thanks to its flexibility to capture different dependency structures. The objective of this work is to model call sequences emitted by bats for echolocation, whose patterns are known to change depending on the animal's activity. The novelty of the model lies in the combination of a Hawkes-type dependency from past events, as well as a latent variable that encodes changes in bat behavior. More precisely, we consider a discrete-time version of the Hawkes process, with an exponential kernel, where the immigration term varies according to a latent Markov chain. We prove that this model is identifiable and can be reformulated in terms of a Hidden Markov Model, with Poisson emissions. Based on these properties, we show that maximum likelihood inference of the model parameters can be performed using an EM algorithm, which involves a recursive M-step. A simulation study demonstrates the performance of our approach method for estimating the parameters, recovering the number of hidden states and classifying each bin of the trajectory. Finally, we illustrate the use of the proposed modeling to distinguish different behaviors of bats, based on the recording of their cries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20153v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Bonnet, St\'ephane Robin</dc:creator>
    </item>
    <item>
      <title>Sparse Regression Codes for Secret Key Agreement: Achieving Strong Secrecy and Near-Optimal Rates for Gaussian Sources</title>
      <link>https://arxiv.org/abs/2507.20157</link>
      <description>arXiv:2507.20157v1 Announce Type: cross 
Abstract: Secret key agreement from correlated physical layer observations is a cornerstone of information-theoretic security. This paper proposes and rigorously analyzes a complete, constructive protocol for secret key agreement from Gaussian sources using Sparse Regression Codes (SPARCs). Our protocol systematically leverages the known optimality of SPARCs for both rate-distortion and Wyner-Ziv (WZ) coding, facilitated by their inherent nested structure. The primary contribution of this work is a comprehensive end-to-end analysis demonstrating that the proposed scheme achieves near-optimal secret key rates with strong secrecy guarantees, as quantified by a vanishing variational distance. We explicitly characterize the gap to the optimal rate, revealing a fundamental trade-off between the key rate and the required public communication overhead, which is governed by a tunable quantization parameter. Furthermore, we uncover a non-trivial constrained optimization for this parameter, showing that practical constraints on the SPARC code parameters induce a peak in the achievable secret key rate. This work establishes SPARCs as a viable and theoretically sound framework for secure key generation, providing a compelling low-complexity alternative to existing schemes and offering new insights into the practical design of such protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20157v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emmanouil M. Athanasakos, Hariprasad Manjunath</dc:creator>
    </item>
    <item>
      <title>Post-estimation Adjustments in Data-driven Decision-making with Applications in Pricing</title>
      <link>https://arxiv.org/abs/2507.20501</link>
      <description>arXiv:2507.20501v1 Announce Type: cross 
Abstract: The predict-then-optimize (PTO) framework is a standard approach in data-driven decision-making, where a decision-maker first estimates an unknown parameter from historical data and then uses this estimate to solve an optimization problem. While widely used for its simplicity and modularity, PTO can lead to suboptimal decisions because the estimation step does not account for the structure of the downstream optimization problem. We study a class of problems where the objective function, evaluated at the PTO decision, is asymmetric with respect to estimation errors. This asymmetry causes the expected outcome to be systematically degraded by noise in the parameter estimate, as the penalty for underestimation differs from that of overestimation. To address this, we develop a data-driven post-estimation adjustment that improves decision quality while preserving the practicality and modularity of PTO. We show that when the objective function satisfies a particular curvature condition, based on the ratio of its third and second derivatives, the adjustment simplifies to a closed-form expression. This condition holds for a broad range of pricing problems, including those with linear, log-linear, and power-law demand models. Under this condition, we establish theoretical guarantees that our adjustment uniformly and asymptotically outperforms standard PTO, and we precisely characterize the resulting improvement. Additionally, we extend our framework to multi-parameter optimization and settings with biased estimators. Numerical experiments demonstrate that our method consistently improves revenue, particularly in small-sample regimes where estimation uncertainty is most pronounced. This makes our approach especially well-suited for pricing new products or in settings with limited historical price variation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20501v1</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Albert, Max Biggs, Ningyuan Chen, Guan Wang</dc:creator>
    </item>
    <item>
      <title>Nullstrap-DE: A General Framework for Calibrating FDR and Preserving Power in DE Methods, with Applications to DESeq2 and edgeR</title>
      <link>https://arxiv.org/abs/2507.20598</link>
      <description>arXiv:2507.20598v1 Announce Type: cross 
Abstract: Differential expression (DE) analysis is a key task in RNA-seq studies, aiming to identify genes with expression differences across conditions. A central challenge is balancing false discovery rate (FDR) control with statistical power. Parametric methods such as DESeq2 and edgeR achieve high power by modeling gene-level counts using negative binomial distributions and applying empirical Bayes shrinkage. However, these methods may suffer from FDR inflation when model assumptions are mildly violated, especially in large-sample settings. In contrast, non-parametric tests like Wilcoxon offer more robust FDR control but often lack power and do not support covariate adjustment. We propose Nullstrap-DE, a general add-on framework that combines the strengths of both approaches. Designed to augment tools like DESeq2 and edgeR, Nullstrap-DE calibrates FDR while preserving power, without modifying the original method's implementation. It generates synthetic null data from a model fitted under the gene-specific null (no DE), applies the same test statistic to both observed and synthetic data, and derives a threshold that satisfies the target FDR level. We show theoretically that Nullstrap-DE asymptotically controls FDR while maintaining power consistency. Simulations confirm that it achieves reliable FDR control and high power across diverse settings, where DESeq2, edgeR, or Wilcoxon often show inflated FDR or low power. Applications to real datasets show that Nullstrap-DE enhances statistical rigor and identifies biologically meaningful genes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20598v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chenxin Jiang, Changhu Wang, Jingyi Jessica Li</dc:creator>
    </item>
    <item>
      <title>Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks</title>
      <link>https://arxiv.org/abs/2507.20708</link>
      <description>arXiv:2507.20708v1 Announce Type: cross 
Abstract: Proving the compliance of AI algorithms has become an important challenge with the growing deployment of such algorithms for real-life applications. Inspecting possible biased behaviors is mandatory to satisfy the constraints of the regulations of the EU Artificial Intelligence's Act. Regulation-driven audits increasingly rely on global fairness metrics, with Disparate Impact being the most widely used. Yet such global measures depend highly on the distribution of the sample on which the measures are computed. We investigate first how to manipulate data samples to artificially satisfy fairness criteria, creating minimally perturbed datasets that remain statistically indistinguishable from the original distribution while satisfying prescribed fairness constraints. Then we study how to detect such manipulation. Our analysis (i) introduces mathematically sound methods for modifying empirical distributions under fairness constraints using entropic or optimal transport projections, (ii) examines how an auditee could potentially circumvent fairness inspections, and (iii) offers recommendations to help auditors detect such data manipulations. These results are validated through experiments on classical tabular datasets in bias detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20708v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentin Lafargue, Adriana Laurindo Monteiro, Emmanuelle Claeys, Laurent Risser, Jean-Michel Loubes</dc:creator>
    </item>
    <item>
      <title>Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI</title>
      <link>https://arxiv.org/abs/2507.20714</link>
      <description>arXiv:2507.20714v1 Announce Type: cross 
Abstract: Prostate cancer, the second most prevalent male malignancy, requires advanced diagnostic tools. We propose an explainable AI system combining BERT (for textual clinical notes) and Random Forest (for numerical lab data) through a novel multimodal fusion strategy, achieving superior classification performance on PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is established, our work demonstrates that a simple yet interpretable BERT+RF pipeline delivers clinically significant improvements - particularly for intermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824 numerical/0.725 textual). SHAP analysis provides transparent feature importance rankings, while ablation studies prove textual features' complementary value. This accessible approach offers hospitals a balance of high performance (F1=89%), computational efficiency, and clinical interpretability - addressing critical needs in prostate cancer diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20714v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Asma Sadia Khan, Fariba Tasnia Khan, Tanjim Mahmud, Salman Karim Khan, Rishita Chakma, Nahed Sharmen, Mohammad Shahadat Hossain, Karl Andersson</dc:creator>
    </item>
    <item>
      <title>BuildSTG: A Multi-building Energy Load Forecasting Method using Spatio-Temporal Graph Neural Network</title>
      <link>https://arxiv.org/abs/2507.20838</link>
      <description>arXiv:2507.20838v1 Announce Type: cross 
Abstract: Due to the extensive availability of operation data, data-driven methods show strong capabilities in predicting building energy loads. Buildings with similar features often share energy patterns, reflected by spatial dependencies in their operational data, which conventional prediction methods struggle to capture. To overcome this, we propose a multi-building prediction approach using spatio-temporal graph neural networks, comprising graph representation, graph learning, and interpretation. First, a graph is built based on building characteristics and environmental factors. Next, a multi-level graph convolutional architecture with attention is developed for energy prediction. Lastly, a method interpreting the optimized graph structure is introduced. Experiments on the Building Data Genome Project 2 dataset confirm superior performance over baselines such as XGBoost, SVR, FCNN, GRU, and Naive, highlighting the method's robustness, generalization, and interpretability in capturing meaningful building similarities and spatial relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20838v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongzheng Liu, Yiming Wang, Po Xu, Yingjie Xu, Yuntian Chen, Dongxiao Zhang</dc:creator>
    </item>
    <item>
      <title>Precision spectral estimation at sub-Hz frequencies: closed-form posteriors and Bayesian noise projection</title>
      <link>https://arxiv.org/abs/2507.20846</link>
      <description>arXiv:2507.20846v1 Announce Type: cross 
Abstract: We present a Bayesian method for estimating spectral quantities in multivariate Gaussian time series. The approach, based on periodograms and Wishart statistics, yields closed-form expressions at any given frequency for the marginal posterior distributions of the individual power spectral densities, the pairwise coherence, and the multiple coherence, as well as for the joint posterior distribution of the full cross-spectral density matrix. In the context of noise projection - where one series is modeled as a linear combination of filtered versions of the others, plus a background component - the method also provides closed-form posteriors for both the susceptibilities, i.e., the filter transfer functions, and the power spectral density of the background. Originally developed for the analysis of the data from the European Space Agency's LISA Pathfinder mission, the method is particularly well-suited to very-low-frequency data, where long observation times preclude averaging over large sets of periodograms, which would otherwise allow these to be treated as approximately normally distributed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20846v1</guid>
      <category>astro-ph.IM</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Sala, Stefano Vitale</dc:creator>
    </item>
    <item>
      <title>A Simulated Reconstruction and Reidentification Attack on the 2010 U.S. Census</title>
      <link>https://arxiv.org/abs/2312.11283</link>
      <description>arXiv:2312.11283v3 Announce Type: replace 
Abstract: We show that individual, confidential microdata records from the 2010 U.S. Census of Population and Housing can be accurately reconstructed from the published tabular summaries. Ninety-seven million person records (every resident in 70% of all census blocks) are exactly reconstructed with provable certainty using only public information. We further show that a hypothetical attacker using our methods can reidentify with 95% accuracy population unique individuals who are perfectly reconstructed and not in the modal race and ethnicity category in their census block (3.4 million persons)--a result that is only possible because their confidential records were used in the published tabulations. Finally, we show that the methods used for the 2020 Census, based on a differential privacy framework, provide better protection against this type of attack, with better published data accuracy, than feasible alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11283v3</guid>
      <category>stat.AP</category>
      <category>cs.CR</category>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1162/99608f92.4a1ebf70</arxiv:DOI>
      <dc:creator>John M. Abowd, Tamara Adams, Robert Ashmead, David Darais, Sourya Dey, Simson L. Garfinkel, Nathan Goldschlag, Michael B. Hawes, Daniel Kifer, Philip Leclerc, Ethan Lew, Scott Moore, Rolando A. Rodr\'iguez, Ramy N. Tadros, Lars Vilhuber</dc:creator>
    </item>
    <item>
      <title>Quantifying uncertainty in climate projections with conformal ensembles</title>
      <link>https://arxiv.org/abs/2408.06642</link>
      <description>arXiv:2408.06642v3 Announce Type: replace 
Abstract: Ensembles of General Circulation Models (GCMs) are the primary tools for investigating climate sensitivity, projecting future climate states, and quantifying uncertainty. GCM ensembles are subject to substantial uncertainty due to model inadequacies, resolution limits, internal variability, and inter-model variability, meaning rigorous climate risk assessments and informed decision-making require reliable and accurate uncertainty quantification (UQ). We introduce conformal ensembles (CE), a new approach to climate UQ that quantifies and constrains projection uncertainty with conformal prediction sets and observational data. CE seamlessly integrates climate model ensembles and observational data across a range of scales to generate statistically rigorous, easy-to-interpret uncertainty estimates. CE can be applied to any climatic variable using any ensemble analysis method and outperforms existing inter-model variability methods in uncertainty quantification across all time horizons and most spatial locations under SSP2-4.5. CE is also computationally efficient, requires minimal assumptions, and is highly robust to the conformity measure. Experiments show that it is effective when conditioning future projections on historical reanalysis data compared with standard ensemble averaging approaches, yielding more physically consistent projections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06642v3</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Trevor Harris, Ryan Sriver</dc:creator>
    </item>
    <item>
      <title>Quantifying sleep apnea heterogeneity using hierarchical Bayesian modeling</title>
      <link>https://arxiv.org/abs/2503.11599</link>
      <description>arXiv:2503.11599v4 Announce Type: replace 
Abstract: Obstructive Sleep Apnea (OSA) is a breathing disorder during sleep that affects millions of people worldwide. The diagnosis of OSA often occurs through an overnight polysomnogram (PSG) sleep study that generates a massive amount of physiological data. However, despite the evidence of substantial heterogeneity in the expression and symptoms of OSA, diagnosis and scientific analysis of severity typically focus on a single summary statistic, the Apnea-Hypopnea Index (AHI). We address the limitations of this approach through hierarchical Bayesian modeling of PSG data. Our approach produces interpretable random effects for each patient, which govern sleep-stage dynamics, rates of OSA events, and impacts of OSA events on subsequent sleep-stage dynamics. We propose a novel approach for using these random effects to produce a Bayes optimal clustering of patients. We use the proposed approach to analyze data from the APPLES study. Our analysis produces clinically interesting groups of patients with sleep apnea and a novel finding of an association between OSA expression and cognitive performance that is missed by an AHI-based analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11599v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Glenn Palmer, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>What Quality Engineers Need to Know about Degradation Models</title>
      <link>https://arxiv.org/abs/2507.14666</link>
      <description>arXiv:2507.14666v2 Announce Type: replace 
Abstract: Degradation models play a critical role in quality engineering by enabling the assessment and prediction of system reliability based on data. The objective of this paper is to provide an accessible introduction to degradation models. We explore commonly used degradation data types, including repeated measures degradation data and accelerated destructive degradation test data, and review modeling approaches such as general path models and stochastic process models. Key inference problems, including reliability estimation and prediction, are addressed. Applications across diverse fields, including material science, renewable energy, civil engineering, aerospace, and pharmaceuticals, illustrate the broad impact of degradation models in industry. We also discuss best practices for quality engineers, software implementations, and challenges in applying these models. This paper aims to provide quality engineers with a foundational understanding of degradation models, equipping them with the knowledge necessary to apply these techniques effectively in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14666v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jared M. Clark, Jie Min, Mingyang Li, Richard L. Warr, Stephanie P. DeHart, Caleb B. King, Lu Lu, Yili Hong</dc:creator>
    </item>
    <item>
      <title>Bayesian Mixed Multidimensional Scaling for Auditory Processing</title>
      <link>https://arxiv.org/abs/2209.00102</link>
      <description>arXiv:2209.00102v3 Announce Type: replace-cross 
Abstract: The human brain distinguishes speech sounds by mapping acoustic signals into a latent perceptual space. This space can be estimated via multidimensional scaling (MDS), preserving the similarity structure in lower dimensions. However, individual and group-level heterogeneity, especially between native and non-native listeners, remains poorly understood. Prior approaches often ignore such variability or cannot capture shared structure, limiting principled comparison. Moreover, the literature typically focuses on latent distances rather than the underlying features themselves. To address these issues, we develop a Bayesian mixed MDS method that accounts for both subject- and group-level heterogeneity, enabling recovery of biologically interpretable latent features. Simulations and an auditory neuroscience application demonstrate how these features reconstruct observed distances and vary with individual and language background, revealing novel insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00102v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Rebaudo, Fernando Llanos, Bharath Chandrasekaran, Abhra Sarkar</dc:creator>
    </item>
    <item>
      <title>A semi-parametric model for assessing the effect of temperature on ice accumulation rate from Antarctic ice core data</title>
      <link>https://arxiv.org/abs/2309.03782</link>
      <description>arXiv:2309.03782v2 Announce Type: replace-cross 
Abstract: In this paper, we present a semiparametric model for describing the effect of temperature on Antarctic ice accumulation on a paleoclimatic time scale. The model is motivated by sharp ups and downs in the rate of ice accumulation apparent from ice core data records, which are synchronous with movements of temperature. We prove strong consistency of the estimators under reasonable conditions. We conduct extensive simulations to assess the performance of the estimators and bootstrap based standard errors and confidence limits for the requisite range of sample sizes. Analysis of ice core data from two Antarctic locations over several hundred thousand years shows a reasonable fit. The apparent accumulation rate exhibits a thinning pattern that should facilitate the understanding of ice condensation, transformation and flow over the ages. There is a very strong linear relationship between temperature and the apparent accumulation rate adjusted for thinning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03782v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Radhendushka Srivastava, Debasis Sengupta</dc:creator>
    </item>
    <item>
      <title>Prediction of microstructural representativity from a single image</title>
      <link>https://arxiv.org/abs/2410.19568</link>
      <description>arXiv:2410.19568v2 Announce Type: replace-cross 
Abstract: In this study, we present a method for predicting the representativity of the phase fraction observed in a single image (2D or 3D) of a material. Traditional approaches often require large datasets and extensive statistical analysis to estimate the Integral Range, a key factor in determining the variance of microstructural properties. Our method leverages the Two-Point Correlation function to directly estimate the variance from a single image, thereby enabling phase fraction prediction with associated confidence levels. We validate our approach using open-source datasets, demonstrating its efficacy across diverse microstructures. This technique significantly reduces the data requirements for representativity analysis, providing a practical tool for material scientists and engineers working with limited microstructural data. To make the method easily accessible, we have created a web-application, www.imagerep.io, for quick, simple and informative use of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19568v2</guid>
      <category>stat.CO</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/advs.202414149</arxiv:DOI>
      <arxiv:journal_reference>Advanced Science 2025</arxiv:journal_reference>
      <dc:creator>Amir Dahari, Ronan Docherty, Steve Kench, Samuel J. Cooper</dc:creator>
    </item>
    <item>
      <title>Analysis of multivariate event times under informative censoring using vine copula</title>
      <link>https://arxiv.org/abs/2502.20608</link>
      <description>arXiv:2502.20608v2 Announce Type: replace-cross 
Abstract: The study of times to nonterminal events of different types and their interrelation is a compelling area of interest. The primary challenge in analyzing such multivariate event times is the presence of informative censoring by the terminal event. While numerous statistical methods have been proposed for a single nonterminal event, i.e., semi-competing risks data, there remains a dearth of tools for analyzing times to multiple nonterminal events. This article introduces a novel analysis framework that leverages the vine copula to directly estimate the joint density of multivariate times to nonterminal and terminal events. Unlike the few existing methods based on multivariate or nested copulas, the developed approach excels in capturing the heterogeneous dependence between each pair of event times (nonterminal-terminal and between-nonterminal) in terms of strength and structure. We propose a likelihood-based estimation and inference procedure, which can be implemented efficiently in sequential stages. Through extensive simulation studies, we demonstrate the satisfactory finite-sample performance of our proposed stage-wise estimators and analytical variance estimators, as well as their advantages over existing methods. We apply the developed approach to data from a crowdfunding platform to investigate the relationship between various types of creator-backer interactions and a creator's lifetime on the platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20608v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Yiwei Li, Qian M. Zhou</dc:creator>
    </item>
    <item>
      <title>Nonparametric regression for cost-effectiveness analyses with observational data -- a tutorial</title>
      <link>https://arxiv.org/abs/2507.03511</link>
      <description>arXiv:2507.03511v2 Announce Type: replace-cross 
Abstract: Healthcare decision-making often requires selecting among treatment options under budget constraints, particularly when one option is more effective but also more costly. Cost-effectiveness analysis (CEA) provides a framework for evaluating whether the health benefits of a treatment justify its additional costs. A key component of CEA is the estimation of treatment effects on both health outcomes and costs, which becomes challenging when using observational data, due to potential confounding. While advanced causal inference methods exist for use in such circumstances, their adoption in CEAs remains limited, with many studies relying on overly simplistic methods such as linear regression or propensity score matching. We believe that this is mainly due to health economists being generally unfamiliar with superior methodology. In this paper, we address this gap by introducing cost-effectiveness researchers to modern nonparametric regression models, with a particular focus on Bayesian Additive Regression Trees (BART). We provide practical guidance on how to implement BART in CEAs, including code examples, and discuss its advantages in producing more robust and credible estimates from observational data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03511v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Esser, Mateus Maia, Judith Bosmans, Johanna van Dongen</dc:creator>
    </item>
    <item>
      <title>Nonparametric Linear Discriminant Analysis for High Dimensional Matrix-Valued Data</title>
      <link>https://arxiv.org/abs/2507.19028</link>
      <description>arXiv:2507.19028v2 Announce Type: replace-cross 
Abstract: This paper addresses classification problems with matrix-valued data, which commonly arises in applications such as neuroimaging and signal processing. Building on the assumption that the data from each class follows a matrix normal distribution, we propose a novel extension of Fisher's Linear Discriminant Analysis (LDA) tailored for matrix-valued observations. To effectively capture structural information while maintaining estimation flexibility, we adopt a nonparametric empirical Bayes framework based on Nonparametric Maximum Likelihood Estimation (NPMLE), applied to vectorized and scaled matrices. The NPMLE method has been shown to provide robust, flexible, and accurate estimates for vector-valued data with various structures in the mean vector or covariance matrix. By leveraging its strengths, our method is effectively generalized to the matrix setting, thereby improving classification performance. Through extensive simulation studies and real data applications, including electroencephalography (EEG) and magnetic resonance imaging (MRI) analysis, we demonstrate that the proposed method consistently outperforms existing approaches across a variety of data structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19028v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seungyeon Oh, Seongoh Park, Hoyoung Park</dc:creator>
    </item>
  </channel>
</rss>
