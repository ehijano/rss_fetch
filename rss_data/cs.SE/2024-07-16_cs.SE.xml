<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Jul 2024 01:51:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Toward Regulatory Compliance: A few-shot Learning Approach to Extract Processing Activities</title>
      <link>https://arxiv.org/abs/2407.09592</link>
      <description>arXiv:2407.09592v1 Announce Type: new 
Abstract: The widespread use of mobile applications has driven the growth of the industry, with companies relying heavily on user data for services like targeted advertising and personalized offerings. In this context, privacy regulations such as the General Data Protection Regulation (GDPR) play a crucial role. One of the GDPR requirements is the maintenance of a Record of Processing Activities (RoPA) by companies. RoPA encompasses various details, including the description of data processing activities, their purposes, types of data involved, and other relevant external entities. Small app-developing companies face challenges in meeting such compliance requirements due to resource limitations and tight timelines. To aid these developers and prevent fines, we propose a method to generate segments of RoPA from user-authored usage scenarios using large language models (LLMs). Our method employs few-shot learning with GPT-3.5 Turbo to summarize usage scenarios and generate RoPA segments. We evaluate different factors that can affect few-shot learning performance consistency for our summarization task, including the number of examples in few-shot learning prompts, repetition, and order permutation of examples in the prompts. Our findings highlight the significant influence of the number of examples in prompts on summarization F1 scores, while demonstrating negligible variability in F1 scores across multiple prompt repetitions. Our prompts achieve successful summarization of processing activities with an average 70% ROUGE-L F1 score. Finally, we discuss avenues for improving results through manual evaluation of the generated summaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09592v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pragyan K C, Rambod Ghandiparsi, Rocky Slavin, Sepideh Ghanavati, Travis Breaux, Mitra Bokaei Hosseini</dc:creator>
    </item>
    <item>
      <title>OXN -- Automated Observability Assessments for Cloud-Native Applications</title>
      <link>https://arxiv.org/abs/2407.09644</link>
      <description>arXiv:2407.09644v1 Announce Type: new 
Abstract: Observability is important to ensure the reliability of microservice applications. These applications are often prone to failures, since they have many independent services deployed on heterogeneous environments. When employed "correctly", observability can help developers identify and troubleshoot faults quickly. However, instrumenting and configuring the observability of a microservice application is not trivial but tool-dependent and tied to costs. Practitioners need to understand observability-related trade-offs in order to weigh between different observability design alternatives. Still, these architectural design decisions are not supported by systematic methods and typically just rely on "professional intuition".
  To assess observability design trade-offs with concrete evidence, we advocate for conducting experiments that compare various design alternatives. Achieving a systematic and repeatable experiment process necessitates automation. We present a proof-of-concept implementation of an experiment tool - Observability eXperiment eNgine (OXN). OXN is able to inject arbitrary faults into an application, similar to Chaos Engineering, but also possesses the unique capability to modify the observability configuration, allowing for the straightforward assessment of design decisions that were previously left unexplored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09644v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maria C. Borges, Joshua Bauer, Sebastian Werner</dc:creator>
    </item>
    <item>
      <title>On Mitigating Code LLM Hallucinations with API Documentation</title>
      <link>https://arxiv.org/abs/2407.09726</link>
      <description>arXiv:2407.09726v1 Announce Type: new 
Abstract: In this study, we address the issue of API hallucinations in various software engineering contexts. We introduce CloudAPIBench, a new benchmark designed to measure API hallucination occurrences. CloudAPIBench also provides annotations for frequencies of API occurrences in the public domain, allowing us to study API hallucinations at various frequency levels. Our findings reveal that Code LLMs struggle with low frequency APIs: for e.g., GPT-4o achieves only 38.58% valid low frequency API invocations. We demonstrate that Documentation Augmented Generation (DAG) significantly improves performance for low frequency APIs (increase to 47.94% with DAG) but negatively impacts high frequency APIs when using sub-optimal retrievers (a 39.02% absolute drop). To mitigate this, we propose to intelligently trigger DAG where we check against an API index or leverage Code LLMs' confidence scores to retrieve only when needed. We demonstrate that our proposed methods enhance the balance between low and high frequency API performance, resulting in more reliable API invocations (8.20% absolute improvement on CloudAPIBench for GPT-4o).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09726v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nihal Jain, Robert Kwiatkowski, Baishakhi Ray, Murali Krishna Ramanathan, Varun Kumar</dc:creator>
    </item>
    <item>
      <title>Uncovering Weaknesses in Neural Code Generation</title>
      <link>https://arxiv.org/abs/2407.09793</link>
      <description>arXiv:2407.09793v1 Announce Type: new 
Abstract: Code generation, the task of producing source code from prompts, has seen significant advancements with the advent of pre-trained large language models (PLMs). Despite these achievements, there lacks a comprehensive taxonomy of weaknesses about the benchmark and the generated code, which risks the community's focus on known issues at the cost of under-explored areas.
  Our systematic study aims to fill this gap by evaluating five state-of-the-art PLMs: three larger models, CodeGen2.5 with 7 billion parameters, CodeGeeX2 with 6 billion parameters, GPT-4 Turbo, and two smaller ones, UnixCoder with 110 million parameters and CodeT5 base with 220 million parameters, across three popular datasets, CoNaLa, HumanEval Plus, and DS-1000. We assess the quality of generated code using match-based and execution-based metrics, then conduct thematic analysis to develop a taxonomy of nine types of weaknesses.
  We dissected weakness distributions in both larger and smaller models, applying an extensive methodology that encompasses model-specific as well as collective analysis (union and intersection) across models. Our research uncovers three salient findings: 1. In the CoNaLa dataset, inaccurate prompts are a notable problem, causing all large models to fail in 26.84% of cases, with even higher failure rates of 40% for smaller models; 2. Missing pivotal semantics is a pervasive issue across benchmarks, with one or more large models omitting key semantics in 65.78% of CoNaLa tasks, and similarly high occurrences in HumanEval Plus (66.09%) and DS-1000 (80.51%); 3. All models struggle with proper API usage, a challenge amplified by vague or complex prompts.
  Our findings aim to steer researchers towards addressing specific weaknesses and challenges in code generation. Furthermore, our annotations can offer a targeted benchmark subset for detailed analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09793v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoli Lian, Shuaisong Wang, Jieping Ma, Fang Liu, Xin Tan, Lin Shi, Li Zhang</dc:creator>
    </item>
    <item>
      <title>Teaching Design Science as a Method for Effective Research Development</title>
      <link>https://arxiv.org/abs/2407.09844</link>
      <description>arXiv:2407.09844v1 Announce Type: new 
Abstract: Applying Design Science Research (DSR) methodology is becoming a popular working resource for most Information Systems (IS) and Software engineering studies. The research and/or practical design problems that must be faced aim to answer the question of how to create or investigate an artifact in a given context. Precisely characterizing both artifact and context is essential for effective research development. While various design science guidelines and frameworks have been created by experts in IS engineering, emerging researchers and postgraduate students still find it challenging to apply this research methodology correctly. There is limited literature and materials that guide and support teaching novice researchers about the types of artifacts that can be developed to address a particular problem and decision-making in DSR. To address this gap in DSR, in this chapter, we explore DSR from an educational perspective, explaining both the concept of DSR and an effective method for teaching it. This chapter includes examples of DSR, a teaching methodology, learning objectives, and recommendations. Moreover, we have created a survey artifact intended to gather data on the experiences of design science users. The goal is to discover insights into contemporary issues and needs for DSR best practices (and, in consequence, evaluate the methodology we aim to teach). Our survey results offer a comprehensive overview of participants' experiences with DSR in the SE and IS Engineering domain, highlighting broad engagement primarily initiated during PhD studies and driven by supervisory guidance and research problems. We finally disclose the artifact to the community so that it can be used by other educators as a preparation when planning to teach DSR in tune with the experiences of their target audience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09844v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oscar Pastor, Mmatshuene Anna Segooa, Jose Ignacio Panach</dc:creator>
    </item>
    <item>
      <title>A Systematic Literature Review on Task Recommendation Systems for Crowdsourced Software Engineering</title>
      <link>https://arxiv.org/abs/2407.09872</link>
      <description>arXiv:2407.09872v1 Announce Type: new 
Abstract: Context: Crowdsourced Software Engineering CSE offers outsourcing work to software practitioners by leveraging a global online workforce. However these software practitioners struggle to identify suitable tasks due to the variety of options available. Hence there have been a growing number of studies on introducing recommendation systems to recommend CSE tasks to software practitioners. Objective: The goal of this study is to analyze the existing CSE task recommendation systems, investigating their extracted data, recommendation methods, key advantages and limitations, recommended task types, the use of human factors in recommendations, popular platforms, and features used to make recommendations. Method: This SLR was conducted according to the Kitchenham and Charters guidelines. We used both manual and automatic search strategies without putting any time limitation for searching the relevant papers. Results: We selected 63 primary studies for data extraction, analysis, and synthesis based on our predefined inclusion and exclusion criteria. From the results of the data analysis, we classified the extracted data into 4 categories based on the data extraction source, categorized the proposed recommendation systems to fit into a taxonomy, and identified the key advantages and limitations of these systems. Our results revealed that human factors play a major role in CSE task recommendation. Further we identified five popular task types recommended, popular platforms, and their features used in task recommendation. We also provided recommendations for future research directions. Conclusion: This SLR provides insights into current trends gaps and future research directions in CSE task recommendation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09872v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shashiwadana Nirmani, Mojtaba Shahin, Hourieh Khalajzadeh, Xiao Liu</dc:creator>
    </item>
    <item>
      <title>EATXT: A textual concrete syntax for EAST-ADL</title>
      <link>https://arxiv.org/abs/2407.09895</link>
      <description>arXiv:2407.09895v1 Announce Type: new 
Abstract: Blended modeling is an approach that enables users to interact with a model via multiple notations. In this context, there is a growing need for open-source industry-grade exemplars of languages with available language engineering artifacts, in particular, editors and notations for supporting the creation of models based on a single metamodel in different representations (e.g., textual, graphical, and tabular ones). These exemplars can support the development of advanced solutions to address the practical challenges posed by blended modeling requirements.
  As one such exemplar, this paper introduces EATXT, a textual concrete syntax for automotive architecture modeling with EAST-ADL, developed in cooperation with an industry partner in the automotive domain. The EATXT editor is based on Xtext and provides basic and advanced features, such as an improved content-assist and serialization specifically addressing blended modeling requirements. We present the editor features and architecture, the implementation approach, and previous use of EATXT in research. The EATXT editor is publicly available, rendering it a valuable resource for language developers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09895v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Weixing Zhang, J\"org Holtmann, Daniel Str\"uber, Jan-Philipp Stegh\"ofer</dc:creator>
    </item>
    <item>
      <title>Reproducibility of Issues Reported in Stack Overflow Questions: Challenges, Impact &amp; Estimation</title>
      <link>https://arxiv.org/abs/2407.10023</link>
      <description>arXiv:2407.10023v1 Announce Type: new 
Abstract: Software developers often submit questions to technical Q&amp;A sites like Stack Overflow (SO) to resolve code-level problems. In practice, they include example code snippets with questions to explain the programming issues. Existing research suggests that users attempt to reproduce the reported issues using given code snippets when answering questions. Unfortunately, such code snippets could not always reproduce the issues due to several unmet challenges that prevent questions from receiving appropriate and prompt solutions. One previous study investigated reproducibility challenges and produced a catalog. However, how the practitioners perceive this challenge catalog is unknown. Practitioners' perspectives are inevitable in validating these challenges and estimating their severity. This study first surveyed 53 practitioners to understand their perspectives on reproducibility challenges. We attempt to (a) see whether they agree with these challenges, (b) determine the impact of each challenge on answering questions, and (c) identify the need for tools to promote reproducibility. Survey results show that - (a) about 90% of the participants agree with the challenges, (b) "missing an important part of code" most severely hurt reproducibility, and (c) participants strongly recommend introducing automated tool support to promote reproducibility. Second, we extract \emph{nine} code-based features (e.g., LOC, compilability) and build five Machine Learning (ML) models to predict issue reproducibility. Early detection might help users improve code snippets and their reproducibility. Our models achieve 84.5% precision, 83.0% recall, 82.8% F1-score, and 82.8% overall accuracy, which are highly promising. Third, we systematically interpret the ML model and explain how code snippets with reproducible issues differ from those with irreproducible issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10023v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Saikat Mondal, Banani Roy</dc:creator>
    </item>
    <item>
      <title>OpenTracer: A Dynamic Transaction Trace Analyzer for Smart Contract Invariant Generation and Beyond</title>
      <link>https://arxiv.org/abs/2407.10039</link>
      <description>arXiv:2407.10039v1 Announce Type: new 
Abstract: Smart contracts, self-executing programs on the blockchain, facilitate reliable value exchanges without centralized oversight. Despite the recent focus on dynamic analysis of their transaction histories in both industry and academia, no open-source tool currently offers comprehensive tracking of complete transaction information to extract user-desired data such as invariant-related data. This paper introduces OpenTracer, designed to address this gap. OpenTracer guarantees comprehensive tracking of every execution step, providing complete transaction information. OpenTracer has been employed to analyze 350,800 Ethereum transactions, successfully inferring 23 different types of invariant from predefined templates. The tool is fully open-sourced, serving as a valuable resource for developers and researchers aiming to study transaction behaviors or extract and validate new invariants from transaction traces. The source code of OpenTracer is available at https://github.com/jeffchen006/OpenTracer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10039v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyang Chen, Ye Liu, Sidi Mohamed Beillahi, Yi Li, Fan Long</dc:creator>
    </item>
    <item>
      <title>DistillSeq: A Framework for Safety Alignment Testing in Large Language Models using Knowledge Distillation</title>
      <link>https://arxiv.org/abs/2407.10106</link>
      <description>arXiv:2407.10106v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have showcased their remarkable capabilities in diverse domains, encompassing natural language understanding, translation, and even code generation. The potential for LLMs to generate harmful content is a significant concern. This risk necessitates rigorous testing and comprehensive evaluation of LLMs to ensure safe and responsible use. However, extensive testing of LLMs requires substantial computational resources, making it an expensive endeavor. Therefore, exploring cost-saving strategies during the testing phase is crucial to balance the need for thorough evaluation with the constraints of resource availability. To address this, our approach begins by transferring the moderation knowledge from an LLM to a small model. Subsequently, we deploy two distinct strategies for generating malicious queries: one based on a syntax tree approach, and the other leveraging an LLM-based method. Finally, our approach incorporates a sequential filter-test process designed to identify test cases that are prone to eliciting toxic responses. Our research evaluated the efficacy of DistillSeq across four LLMs: GPT-3.5, GPT-4.0, Vicuna-13B, and Llama-13B. In the absence of DistillSeq, the observed attack success rates on these LLMs stood at 31.5% for GPT-3.5, 21.4% for GPT-4.0, 28.3% for Vicuna-13B, and 30.9% for Llama-13B. However, upon the application of DistillSeq, these success rates notably increased to 58.5%, 50.7%, 52.5%, and 54.4%, respectively. This translated to an average escalation in attack success rate by a factor of 93.0% when compared to scenarios without the use of DistillSeq. Such findings highlight the significant enhancement DistillSeq offers in terms of reducing the time and resource investment required for effectively testing LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10106v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mingke Yang, Yuqi Chen, Yi Liu, Ling Shi</dc:creator>
    </item>
    <item>
      <title>KAT: Dependency-aware Automated API Testing with Large Language Models</title>
      <link>https://arxiv.org/abs/2407.10227</link>
      <description>arXiv:2407.10227v1 Announce Type: new 
Abstract: API testing has increasing demands for software companies. Prior API testing tools were aware of certain types of dependencies that needed to be concise between operations and parameters. However, their approaches, which are mostly done manually or using heuristic-based algorithms, have limitations due to the complexity of these dependencies. In this paper, we present KAT (Katalon API Testing), a novel AI-driven approach that leverages the large language model GPT in conjunction with advanced prompting techniques to autonomously generate test cases to validate RESTful APIs. Our comprehensive strategy encompasses various processes to construct an operation dependency graph from an OpenAPI specification and to generate test scripts, constraint validation scripts, test cases, and test data. Our evaluation of KAT using 12 real-world RESTful services shows that it can improve test coverage, detect more undocumented status codes, and reduce false positives in these services in comparison with a state-of-the-art automated test generation tool. These results indicate the effectiveness of using the large language model for generating test scripts and data for API testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10227v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICST60714.2024.00017</arxiv:DOI>
      <dc:creator>Tri Le, Thien Tran, Duy Cao, Vy Le, Tien Nguyen, Vu Nguyen</dc:creator>
    </item>
    <item>
      <title>PLACIDUS: Engineering Product Lines of Rigorous Assurance Cases</title>
      <link>https://arxiv.org/abs/2407.10345</link>
      <description>arXiv:2407.10345v1 Announce Type: new 
Abstract: In critical software engineering, structured assurance cases (ACs) are used to demonstrate how key properties (e.g., safety, security) are supported by evidence artifacts (e.g., test results, proofs). ACs can also be studied as formal objects in themselves, such that formal methods can be used to establish their correctness. Creating rigorous ACs is particularly challenging in the context of software product lines (SPLs), wherein a family of related software products is engineered simultaneously. Since creating individual ACs for each product is infeasible, AC development must be lifted to the level of product lines. In this work, we propose PLACIDUS, a methodology for integrating formal methods and software product line engineering to develop provably correct ACs for SPLs. To provide rigorous foundations for PLACIDUS, we define a variability-aware AC language and formalize its semantics using the proof assistant Lean. We provide tool support for PLACIDUS as part of an Eclipse-based model management framework. Finally, we demonstrate the feasibility of PLACIDUS by developing an AC for a product line of medical devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10345v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Logan Murphy, Torin Viger, Alessio Di Sandro, Marsha Chechik</dc:creator>
    </item>
    <item>
      <title>A Framework for QoS of Integration Testing in Satellite Edge Clouds</title>
      <link>https://arxiv.org/abs/2407.10402</link>
      <description>arXiv:2407.10402v1 Announce Type: new 
Abstract: The diversification of satellite communication services imposes varied requirements on network service quality, making quality of service (QoS) testing for microservices running on satellites more complex. Existing testing tools have limitations, potentially offering only single-functionality testing, thus failing to meet the requirements of QoS testing for edge cloud services in mobile satellite scenarios. In this paper, we propose a framework for integrating quality of service testing in satellite edge clouds. More precisely, the framework can integrate changes in satellite network topology, create and manage satellite edge cloud cluster testing environments on heterogeneous edge devices, customize experiments for users, support deployment and scaling of various integrated testing tools, and publish and visualize test results. Our experimental results validate the framework's ability to test key service quality metrics in a satellite edge cloud cluster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10402v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guogen Zeng, Juan Luo, Yufeng Zhang, Ying Qiao, Shuyang Teng</dc:creator>
    </item>
    <item>
      <title>AirDnD -- Asynchronous In-Range Dynamic and Distributed Network Orchestration Framework</title>
      <link>https://arxiv.org/abs/2407.10500</link>
      <description>arXiv:2407.10500v1 Announce Type: new 
Abstract: The increasing usage of IoT devices has generated an extensive volume of data which resulted in the establishment of data centers with well-structured computing infrastructure. Reducing underutilized resources of such data centers can be achieved by monitoring the tasks and offloading them across various compute units. This approach can also be used in mini mobile data ponds generated by edge devices and smart vehicles. This research aims to improve and utilize the usage of computing resources in distributed edge devices by forming a dynamic mesh network. The nodes in the mesh network shall share their computing tasks with another node that possesses unused computing resources. This proposed method ensures the minimization of data transfer between entities. The proposed AirDnD vision will be applied to a practical scenario relevant to an autonomous vehicle that approaches an intersection commonly known as ``looking around the corner'' in related literature, collecting essential computational results from nearby vehicles to enhance its perception. The proposed solution consists of three models that transform growing amounts of geographically distributed edge devices into a living organism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10500v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICDCS57875.2023.00108</arxiv:DOI>
      <arxiv:journal_reference>M.A.M Dona 2023 IEEE 43rd International Conference on Distributed Computing Systems (ICDCS)(2023) 953-954</arxiv:journal_reference>
      <dc:creator>Malsha Ashani Mahawatta Dona, Christian Berger, Yinan Yu</dc:creator>
    </item>
    <item>
      <title>Mitigating Data Imbalance for Software Vulnerability Assessment: Does Data Augmentation Help?</title>
      <link>https://arxiv.org/abs/2407.10722</link>
      <description>arXiv:2407.10722v1 Announce Type: new 
Abstract: Background: Software Vulnerability (SV) assessment is increasingly adopted to address the ever-increasing volume and complexity of SVs. Data-driven approaches have been widely used to automate SV assessment tasks, particularly the prediction of the Common Vulnerability Scoring System (CVSS) metrics such as exploitability, impact, and severity. SV assessment suffers from the imbalanced distributions of the CVSS classes, but such data imbalance has been hardly understood and addressed in the literature. Aims: We conduct a large-scale study to quantify the impacts of data imbalance and mitigate the issue for SV assessment through the use of data augmentation. Method: We leverage nine data augmentation techniques to balance the class distributions of the CVSS metrics. We then compare the performance of SV assessment models with and without leveraging the augmented data. Results: Through extensive experiments on 180k+ real-world SVs, we show that mitigating data imbalance can significantly improve the predictive performance of models for all the CVSS tasks, by up to 31.8% in Matthews Correlation Coefficient. We also discover that simple text augmentation like combining random text insertion, deletion, and replacement can outperform the baseline across the board. Conclusions: Our study provides the motivation and the first promising step toward tackling data imbalance for effective SV assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10722v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Triet H. M. Le, M. Ali Babar</dc:creator>
    </item>
    <item>
      <title>An Exploratory Study on Just-in-Time Multi-Programming-Language Bug Prediction</title>
      <link>https://arxiv.org/abs/2407.10906</link>
      <description>arXiv:2407.10906v1 Announce Type: new 
Abstract: Context: An increasing number of software systems are written in multiple programming languages (PLs), which are called multi-programming-language (MPL) systems. MPL bugs (MPLBs) refers to the bugs whose resolution involves multiple PLs. Despite high complexity of MPLB resolution, there lacks MPLB prediction methods. Objective: This work aims to construct just-in-time (JIT) MPLB prediction models with selected prediction metrics, analyze the significance of the metrics, and then evaluate the performance of cross-project JIT MPLB prediction. Method: We develop JIT MPLB prediction models with the selected metrics using machine learning algorithms and evaluate the models in within-project and cross-project contexts with our constructed dataset based on 18 Apache MPL projects. Results: Random Forest is appropriate for JIT MPLB prediction. Changed LOC of all files, added LOC of all files, and the total number of lines of all files of the project currently are the most crucial metrics in JIT MPLB prediction. The prediction models can be simplified using a few top-ranked metrics. Training on the dataset from multiple projects can yield significantly higher AUC than training on the dataset from a single project for cross-project JIT MPLB prediction. Conclusions: JIT MPLB prediction models can be constructed with the selected set of metrics, which can be reduced to build simplified JIT MPLB prediction models, and cross-project JIT MPLB prediction is feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10906v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zengyang Li, Jiabao Ji, Peng Liang, Ran Mo, Hui Liu</dc:creator>
    </item>
    <item>
      <title>Aligning Models with Their Realization through Model-based Systems Engineering</title>
      <link>https://arxiv.org/abs/2407.09513</link>
      <description>arXiv:2407.09513v1 Announce Type: cross 
Abstract: In this paper, we propose a method for aligning models with their realization through the application of model-based systems engineering. Our approach is divided into three steps. (1) Firstly, we leverage domain expertise and the Unified Architecture Framework to establish a reference model that fundamentally describes some domain. (2) Subsequently, we instantiate the reference model as specific models tailored to different scenarios within the domain. (3) Finally, we incorporate corresponding run logic directly into both the reference model and the specific models. In total, we thus provide a practical means to ensure that every implementation result is justified by business demand. We demonstrate our approach using the example of maritime object detection as a specific application (specific model / implementation element) of automatic target recognition as a service reoccurring in various forms (reference model element). Our approach facilitates a more seamless integration of models and implementation, fostering enhanced Business-IT alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09513v1</guid>
      <category>eess.SY</category>
      <category>cs.MA</category>
      <category>cs.SE</category>
      <category>cs.SY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/AEIS61544.2023.00018</arxiv:DOI>
      <arxiv:journal_reference>Conference on Advanced Enterprise Information System (AEIS 2023)</arxiv:journal_reference>
      <dc:creator>Lovis Justin Immanuel Zenz, Erik Heiland, Peter Hillmann, Andreas Karcher</dc:creator>
    </item>
    <item>
      <title>Building Collaborative Learning: Exploring Social Annotation in Introductory Programming</title>
      <link>https://arxiv.org/abs/2407.10322</link>
      <description>arXiv:2407.10322v1 Announce Type: cross 
Abstract: The increasing demand for software engineering education presents learning challenges in courses due to the diverse range of topics that require practical applications, such as programming or software design, all of which are supported by group work and interaction. Social Annotation (SA) is an approach to teaching that can enhance collaborative learning among students. In SA, both students and teachers utilize platforms like Feedback Fruits, Perusall, and Diigo to collaboratively annotate and discuss course materials. This approach encourages students to share their thoughts and answers with their peers, fostering a more interactive learning environment. We share our experience of implementing social annotation via Perusall as a preparatory tool for lectures in an introductory programming course aimed at undergraduate students in Software Engineering. We report the impact of Perusall on the examination results of 112 students. Our results show that 81% of students engaged in meaningful social annotation successfully passed the course. Notably, the proportion of students passing the exam tends to rise as they complete more Perusall assignments. In contrast, only 56% of students who did not participate in Perusall discussions managed to pass the exam. We did not enforce mandatory Perusall participation in the course. Yet, the feedback from our course evaluation questionnaire reveals that most students ranked Perusall among their favorite components of the course and that their interest in the subject has increased.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10322v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francisco Gomes de Oliveira Neto, Felix Dobslaw</dc:creator>
    </item>
    <item>
      <title>Comprehensive Review of Performance Optimization Strategies for Serverless Applications on AWS Lambda</title>
      <link>https://arxiv.org/abs/2407.10397</link>
      <description>arXiv:2407.10397v1 Announce Type: cross 
Abstract: This review paper synthesizes the latest research on performance optimization strategies for serverless applications deployed on AWS Lambda. By examining recent studies, we highlight the challenges, solutions, and best practices for enhancing the performance, cost efficiency, and scalability of serverless applications. The review covers a range of optimization techniques including resource management, runtime selection, observability improvements, and workload aware operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10397v1</guid>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed Lemine El Bechir, Cheikh Sad Bouh, Abobakr Shuwail</dc:creator>
    </item>
    <item>
      <title>Early Career Developers' Perceptions of Code Understandability. A Study of Complexity Metrics</title>
      <link>https://arxiv.org/abs/2303.07722</link>
      <description>arXiv:2303.07722v2 Announce Type: replace 
Abstract: Context. Code understandability is fundamental. Developers need to understand the code they are modifying clearly. A low understandability can increase the amount of coding effort, and misinterpreting code impacts the entire development process. Ideally, developers should write clear and understandable code with the least effort. Aim. Our work investigates whether the McCabe Cyclomatic Complexity or the Cognitive Complexity can be a good predictor for the developers' perceived code understandability to understand which of the two complexities can be used as criteria to evaluate if a piece of code is understandable. Method. We designed and conducted an empirical study among 216 early career developers with professional experience ranging from one to four years. We asked them to manually inspect and rate the understandability of 12 Java classes that exhibit different levels of Cyclomatic and Cognitive Complexity. Results. Our findings showed that while the old-fashioned McCabe Cyclomatic Complexity and the most recent Cognitive Complexity are modest predictors for code understandability when considering the complexity perceived by early-career developers, they are not for problem severity. Conclusions. Based on our results, early-career developers should not be left alone when performing code-reviewing tasks due to their scarce experience. Moreover, low complexity measures indicate good understandability, but having either CoC or CyC high makes understandability unpredictable. Nevertheless, there is no evidence that CyC or CoC are indicators of early-career perceived severity.Future research efforts will focus on expanding the population to experienced developers to confront whether seniority influences the predictive power of the chosen metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07722v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Esposito, Andrea Janes, Terhi Kilamo, Valentina Lenarduzzi</dc:creator>
    </item>
    <item>
      <title>iSNEAK: Partial Ordering as Heuristics for Model-Based Reasoning in Software Engineering</title>
      <link>https://arxiv.org/abs/2310.19125</link>
      <description>arXiv:2310.19125v2 Announce Type: replace 
Abstract: A "partial ordering" is a way to heuristically order a set of examples (partial orderings are a set where, for certain pairs of elements, one precedes the other). While these orderings may only be approximate, they can be useful for guiding a search towards better regions of the data. To illustrate the value of that technique, this paper presents iSNEAK, an incremental human-in-the-loop AI problem solver. iSNEAK uses partial orderings and feedback from humans to prune the space of options. Further, in experiments with a dozen software models of increasing size and complexity (with up to 10,000 variables), iSNEAK only asked a handful of questions to return human-acceptable solutions that outperformed the prior state-of-the-art. We propose the use of partial orderings and tools like iSNEAK to solve the information overload problem where human experts grow fatigued and make mistakes when they are asked too many questions. iSNEAK mitigates the information overload problem since it allows humans to explore complex problem spaces in far less time, with far less effort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19125v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andre Lustosa, Tim Menzies</dc:creator>
    </item>
    <item>
      <title>"Add more config detail": A Taxonomy of Installation Instruction Changes</title>
      <link>https://arxiv.org/abs/2312.03250</link>
      <description>arXiv:2312.03250v2 Announce Type: replace 
Abstract: README files play an important role in providing installation-related instructions to software users and are widely used in open source software systems on platforms such as GitHub. However, these files often suffer from various documentation issues, leading to challenges in comprehension and potential errors in content. Despite their significance, there is a lack of systematic understanding regarding the documentation efforts invested in README files, especially in the context of installation-related instructions, which are crucial for users to start with a software project. To fill the research gap, we conducted a qualitative study, investigating 400 GitHub repositories with 1,163 README commits that focused on updates in installation-related sections. Our research revealed six major categories of changes in the README commits, namely pre-installation instructions, installation instructions, post-installation instructions, help information updates, document presentation, and external resource management. We further provide detailed insights into modification behaviours and offer examples of these updates. Based on our findings, we propose a README template tailored to cover the installation-related sections for documentation maintainers to reference when updating documents. We further validate this template by conducting an online survey, identifying that documentation readers find the augmented documents based on our template are generally of better quality. We further provide recommendations to practitioners for maintaining their README files, as well as motivations for future research directions... (too long for arxiv)</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03250v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyu Gao, Christoph Treude, Mansooreh Zahedi</dc:creator>
    </item>
    <item>
      <title>An empirical study of testing machine learning in the wild</title>
      <link>https://arxiv.org/abs/2312.12604</link>
      <description>arXiv:2312.12604v2 Announce Type: replace 
Abstract: Recently, machine and deep learning (ML/DL) algorithms have been increasingly adopted in many software systems. Due to their inductive nature, ensuring the quality of these systems remains a significant challenge for the research community. Unlike traditional software built deductively by writing explicit rules, ML/DL systems infer rules from training data. Recent research in ML/DL quality assurance has adapted concepts from traditional software testing, such as mutation testing, to improve reliability. However, it is unclear if these proposed testing techniques are adopted in practice, or if new testing strategies have emerged from real-world ML deployments. There is little empirical evidence about the testing strategies.
  To fill this gap, we perform the first fine-grained empirical study on ML testing in the wild to identify the ML properties being tested, the testing strategies, and their implementation throughout the ML workflow.
  We conducted a mixed-methods study to understand ML software testing practices. We analyzed test files and cases from 11 open-source ML/DL projects on GitHub. Using open coding, we manually examined the testing strategies, tested ML properties, and implemented testing methods to understand their practical application in building and releasing ML/DL software systems.
  Our findings reveal several key insights: 1.) The most common testing strategies, accounting for less than 40%, are Grey-box and White-box methods, such as Negative Testing, Oracle Approximation and Statistical Testing. 2.) A wide range of 17 ML properties are tested, out of which only 20% to 30% are frequently tested, including Consistency, Correctness}, and Efficiency. 3.) Bias and Fairness is more tested in Recommendation, while Security &amp; Privacy is tested in Computer Vision (CV) systems, Application Platforms, and Natural Language Processing (NLP) systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12604v2</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moses Openja (Jack), Foutse Khomh (Jack), Armstrong Foundjem (Jack), Zhen Ming (Jack),  Jiang, Mouna Abidi, Ahmed E. Hassan</dc:creator>
    </item>
    <item>
      <title>Informed and Assessable Observability Design Decisions in Cloud-native Microservice Applications</title>
      <link>https://arxiv.org/abs/2403.00633</link>
      <description>arXiv:2403.00633v2 Announce Type: replace 
Abstract: Observability is important to ensure the reliability of microservice applications. These applications are often prone to failures, since they have many independent services deployed on heterogeneous environments. When employed "correctly", observability can help developers identify and troubleshoot faults quickly. However, instrumenting and configuring the observability of a microservice application is not trivial but tool-dependent and tied to costs. Architects need to understand observability-related trade-offs in order to weigh between different observability design alternatives. Still, these architectural design decisions are not supported by systematic methods and typically just rely on "professional intuition". In this paper, we argue for a systematic method to arrive at informed and continuously assessable observability design decisions. Specifically, we focus on fault observability of cloud-native microservice applications, and turn this into a testable and quantifiable property. Towards our goal, we first model the scale and scope of observability design decisions across the cloud-native stack. Then, we propose observability metrics which can be determined for any microservice application through so-called observability experiments. We present a proof-of-concept implementation of our experiment tool OXN. OXN is able to inject arbitrary faults into an application, similar to Chaos Engineering, but also possesses the unique capability to modify the observability configuration, allowing for the assessment of design decisions that were previously left unexplored. We demonstrate our approach using a popular open source microservice application and show the trade-offs involved in different observability design decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00633v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maria C. Borges, Joshua Bauer, Sebastian Werner, Michael Gebauer, Stefan Tai</dc:creator>
    </item>
    <item>
      <title>Digital Twin Evolution for Sustainable Smart Ecosystems</title>
      <link>https://arxiv.org/abs/2403.07162</link>
      <description>arXiv:2403.07162v2 Announce Type: replace 
Abstract: Smart ecosystems are the drivers of modern society. They control infrastructures of socio-techno-economic importance, ensuring their stable and sustainable operation. Smart ecosystems are governed by digital twins -- real-time virtual representations of physical infrastructure. To support the open-ended and reactive traits of smart ecosystems, digital twins need to be able to evolve in reaction to changing conditions. However, digital twin evolution is challenged by the intertwined nature of physical and software components, and their individual evolution. As a consequence, software practitioners find a substantial body of knowledge on software evolution hard to apply in digital twin evolution scenarios and a lack of knowledge on the digital twin evolution itself. The aim of this paper, consequently, is to provide software practitioners with tangible leads toward understanding and managing the evolutionary concerns of digital twins. We use four distinct digital twin evolution scenarios, contextualized in a citizen energy community case to illustrate the usage of the 7R taxonomy of digital twin evolution. By that, we aim to bridge a significant gap in leveraging software engineering practices to develop robust smart ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07162v2</guid>
      <category>cs.SE</category>
      <category>cs.ET</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Istvan David, Judith Michael, Dominik Bork</dc:creator>
    </item>
    <item>
      <title>Graph Neural Networks for Vulnerability Detection: A Counterfactual Explanation</title>
      <link>https://arxiv.org/abs/2404.15687</link>
      <description>arXiv:2404.15687v2 Announce Type: replace 
Abstract: Vulnerability detection is crucial for ensuring the security and reliability of software systems. Recently, Graph Neural Networks (GNNs) have emerged as a prominent code embedding approach for vulnerability detection, owing to their ability to capture the underlying semantic structure of source code. However, GNNs face significant challenges in explainability due to their inherently black-box nature. To this end, several factual reasoning-based explainers have been proposed. These explainers provide explanations for the predictions made by GNNs by analyzing the key features that contribute to the outcomes. We argue that these factual reasoning-based explanations cannot answer critical what-if questions: What would happen to the GNN's decision if we were to alter the code graph into alternative structures? Inspired by advancements of counterfactual reasoning in artificial intelligence, we propose CFExplainer, a novel counterfactual explainer for GNN-based vulnerability detection. Unlike factual reasoning-based explainers, CFExplainer seeks the minimal perturbation to the input code graph that leads to a change in the prediction, thereby addressing the what-if questions for vulnerability detection. We term this perturbation a counterfactual explanation, which can pinpoint the root causes of the detected vulnerability and furnish valuable insights for developers to undertake appropriate actions for fixing the vulnerability. Extensive experiments on four GNN-based vulnerability detection models demonstrate the effectiveness of CFExplainer over existing state-of-the-art factual reasoning-based explainers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15687v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3650212.3652136</arxiv:DOI>
      <dc:creator>Zhaoyang Chu, Yao Wan, Qian Li, Yang Wu, Hongyu Zhang, Yulei Sui, Guandong Xu, Hai Jin</dc:creator>
    </item>
    <item>
      <title>SyDRA: An Approach to Understand Game Engine Architecture</title>
      <link>https://arxiv.org/abs/2406.05487</link>
      <description>arXiv:2406.05487v2 Announce Type: replace 
Abstract: Game engines are tools to facilitate video game development. They provide graphics, sound, and physics simulation features, which would have to be otherwise implemented by developers. Even though essential for modern commercial video game development, game engines are complex and developers often struggle to understand their architecture, leading to maintainability and evolution issues that negatively affect video game productions. In this paper, we present the Subsystem-Dependency Recovery Approach (SyDRA), which helps game engine developers understand game engine architecture and therefore make informed game engine development choices. By applying this approach to 10 open-source game engines, we obtain architectural models that can be used to compare game engine architectures and identify and solve issues of excessive coupling and folder nesting. Through a controlled experiment, we show that the inspection of the architectural models derived from SyDRA enables developers to complete tasks related to architectural understanding and impact analysis in less time and with higher correctness than without these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05487v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel C. Ullmann, Yann-Ga\"el Gu\'eh\'eneuc, Fabio Petrillo, Nicolas Anquetil, Cristiano Politowski</dc:creator>
    </item>
    <item>
      <title>AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology</title>
      <link>https://arxiv.org/abs/2406.11912</link>
      <description>arXiv:2406.11912v2 Announce Type: replace 
Abstract: Software agents have emerged as promising tools for addressing complex software engineering tasks. Existing works, on the other hand, frequently oversimplify software development workflows, despite the fact that such workflows are typically more complex in the real world. Thus, we propose AgileCoder, a multi agent system that integrates Agile Methodology (AM) into the framework. This system assigns specific AM roles - such as Product Manager, Developer, and Tester to different agents, who then collaboratively develop software based on user inputs. AgileCoder enhances development efficiency by organizing work into sprints, focusing on incrementally developing software through sprints. Additionally, we introduce Dynamic Code Graph Generator, a module that creates a Code Dependency Graph dynamically as updates are made to the codebase. This allows agents to better comprehend the codebase, leading to more precise code generation and modifications throughout the software development process. AgileCoder surpasses existing benchmarks, like ChatDev and MetaGPT, establishing a new standard and showcasing the capabilities of multi agent systems in advanced software engineering environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11912v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Minh Huynh Nguyen, Thang Phan Chau, Phong X. Nguyen, Nghi D. Q. Bui</dc:creator>
    </item>
    <item>
      <title>On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards</title>
      <link>https://arxiv.org/abs/2407.04065</link>
      <description>arXiv:2407.04065v2 Announce Type: replace 
Abstract: Foundation models (FM), such as large language models (LLMs), which are large-scale machine learning (ML) models, have demonstrated remarkable adaptability in various downstream software engineering (SE) tasks, such as code completion, code understanding, and software development. As a result, FM leaderboards, especially those hosted on cloud platforms, have become essential tools for SE teams to compare and select the best third-party FMs for their specific products and purposes. However, the lack of standardized guidelines for FM evaluation and comparison threatens the transparency of FM leaderboards and limits stakeholders' ability to perform effective FM selection. As a first step towards addressing this challenge, our research focuses on understanding how these FM leaderboards operate in real-world scenarios ("leaderboard operations") and identifying potential leaderboard pitfalls and areas for improvement ("leaderboard smells"). In this regard, we perform a multivocal literature review to collect up to 721 FM leaderboards, after which we examine their documentation and engage in direct communication with leaderboard operators to understand their workflow patterns. Using card sorting and negotiated agreement, we identify 5 unique workflow patterns and develop a domain model that outlines the essential components and their interaction within FM leaderboards. We then identify 8 unique types of leaderboard smells in LBOps. By mitigating these smells, SE teams can improve transparency, accountability, and collaboration in current LBOps practices, fostering a more robust and responsible ecosystem for FM comparison and selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04065v2</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhimin Zhao, Abdul Ali Bangash, Filipe Roseiro C\^ogo, Bram Adams, Ahmed E. Hassan</dc:creator>
    </item>
    <item>
      <title>SynCode: LLM Generation with Grammar Augmentation</title>
      <link>https://arxiv.org/abs/2403.01632</link>
      <description>arXiv:2403.01632v3 Announce Type: replace-cross 
Abstract: LLMs are widely used in complex AI applications. These applications underscore the need for LLM outputs to adhere to a specific format, for their integration with other components in the systems. Typically the format rules e.g., for data serialization formats such as JSON, YAML, or Code in Programming Language are expressed as context-free grammar (CFG). Due to the hallucinations and unreliability of LLMs, instructing LLMs to adhere to specified syntax becomes an increasingly important challenge.
  We present SynCode, a novel framework for efficient and general syntactical decoding with LLMs, to address this challenge. SynCode ensures soundness and completeness with respect to the CFG of a formal language, effectively retaining valid tokens while filtering out invalid ones. SynCode uses an offline-constructed, efficient lookup table, the DFA mask store, derived from the DFA of the language's grammar for efficient generation. SynCode seamlessly integrates with any language defined by CFG, as evidenced by experiments focusing on generating JSON, Python, and Go outputs. Our experiments evaluating the effectiveness of SynCode for JSON generation demonstrate that SynCode eliminates all syntax errors and significantly outperforms state-of-the-art baselines. Furthermore, our results underscore how SynCode significantly reduces 96.07% of syntax errors in generated Python and Go code, showcasing its substantial impact on enhancing syntactical precision in LLM generation. Our code is available at https://github.com/uiuc-focal-lab/syncode</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01632v3</guid>
      <category>cs.LG</category>
      <category>cs.FL</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, Gagandeep Singh</dc:creator>
    </item>
    <item>
      <title>Demystifying Invariant Effectiveness for Securing Smart Contracts</title>
      <link>https://arxiv.org/abs/2404.14580</link>
      <description>arXiv:2404.14580v2 Announce Type: replace-cross 
Abstract: Smart contract transactions associated with security attacks often exhibit distinct behavioral patterns compared with historical benign transactions before the attacking events. While many runtime monitoring and guarding mechanisms have been proposed to validate invariants and stop anomalous transactions on the fly, the empirical effectiveness of the invariants used remains largely unexplored. In this paper, we studied 23 prevalent invariants of 8 categories, which are either deployed in high-profile protocols or endorsed by leading auditing firms and security experts. Using these well-established invariants as templates, we developed a tool Trace2Inv which dynamically generates new invariants customized for a given contract based on its historical transaction data.
  We evaluated Trace2Inv on 42 smart contracts that fell victim to 27 distinct exploits on the Ethereum blockchain. Our findings reveal that the most effective invariant guard alone can successfully block 18 of the 27 identified exploits with minimal gas overhead. Our analysis also shows that most of the invariants remain effective even when the experienced attackers attempt to bypass them. Additionally, we studied the possibility of combining multiple invariant guards, resulting in blocking up to 23 of the 27 benchmark exploits and achieving false positive rates as low as 0.32%. Trace2Inv outperforms current state-of-the-art works on smart contract invariant mining and transaction attack detection in terms of both practicality and accuracy. Though Trace2Inv is not primarily designed for transaction attack detection, it surprisingly found two previously unreported exploit transactions, earlier than any reported exploit transactions against the same victim contracts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14580v2</guid>
      <category>cs.CR</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/366078</arxiv:DOI>
      <dc:creator>Zhiyang Chen, Ye Liu, Sidi Mohamed Beillahi, Yi Li, Fan Long</dc:creator>
    </item>
    <item>
      <title>Enhancing Automata Learning with Statistical Machine Learning: A Network Security Case Study</title>
      <link>https://arxiv.org/abs/2405.11141</link>
      <description>arXiv:2405.11141v2 Announce Type: replace-cross 
Abstract: Intrusion detection systems are crucial for network security. Verification of these systems is complicated by various factors, including the heterogeneity of network platforms and the continuously changing landscape of cyber threats. In this paper, we use automata learning to derive state machines from network-traffic data with the objective of supporting behavioural verification of intrusion detection systems. The most innovative aspect of our work is addressing the inability to directly apply existing automata learning techniques to network-traffic data due to the numeric nature of such data. Specifically, we use interpretable machine learning (ML) to partition numeric ranges into intervals that strongly correlate with a system's decisions regarding intrusion detection. These intervals are subsequently used to abstract numeric ranges before automata learning. We apply our ML-enhanced automata learning approach to a commercial network intrusion detection system developed by our industry partner, RabbitRun Technologies. Our approach results in an average 67.5% reduction in the number of states and transitions of the learned state machines, while achieving an average 28% improvement in accuracy compared to using expertise-based numeric data abstraction. Furthermore, the resulting state machines help practitioners in verifying system-level security requirements and exploring previously unknown system behaviours through model checking and temporal query checking. We make our implementation and experimental data available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11141v2</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Negin Ayoughi, Shiva Nejati, Mehrdad Sabetzadeh, Patricio Saavedra</dc:creator>
    </item>
  </channel>
</rss>
