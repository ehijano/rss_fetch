<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Sep 2025 04:05:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Adversarial Decision-Making in Partially Observable Multi-Agent Systems: A Sequential Hypothesis Testing Approach</title>
      <link>https://arxiv.org/abs/2509.03727</link>
      <description>arXiv:2509.03727v1 Announce Type: new 
Abstract: Adversarial decision-making in partially observable multi-agent systems requires sophisticated strategies for both deception and counter-deception. This paper presents a sequential hypothesis testing (SHT)-driven framework that captures the interplay between strategic misdirection and inference in adversarial environments. We formulate this interaction as a partially observable Stackelberg game, where a follower agent (blue team) seeks to fulfill its primary task while actively misleading an adversarial leader (red team). In opposition, the red team, leveraging leaked information, instills carefully designed patterns to manipulate the blue team's behavior, mitigating the misdirection effect. Unlike conventional approaches that focus on robust control under adversarial uncertainty, our framework explicitly models deception as a dynamic optimization problem, where both agents strategically adapt their policies in response to inference and counter-inference. We derive a semi-explicit optimal control solution for the blue team within a linear-quadratic setting and develop iterative and machine learning-based methods to characterize the red team's optimal response. Numerical experiments demonstrate how deception-driven strategies influence adversarial interactions and reveal the impact of leaked information in shaping equilibrium behaviors. These results provide new insights into strategic deception in multi-agent systems, with potential applications in cybersecurity, autonomous decision-making, and financial markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03727v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haosheng Zhou, Daniel Ralston, Xu Yang, Ruimeng Hu</dc:creator>
    </item>
    <item>
      <title>A minimization principle behind the diffusion bridge of diurnal fish migration</title>
      <link>https://arxiv.org/abs/2509.03824</link>
      <description>arXiv:2509.03824v1 Announce Type: new 
Abstract: Fish migration is a mass movement that affects the hydrosphere and ecosystems. While it occurs on multiple temporal scales, including daily and intraday fluctuations, the latter remains less studied. In this study, for a stochastic differential equation model of the intraday unit-time fish count at a fixed observation point, we demonstrate that the model can be derived from a minimization problem in the form of a stochastic control problem. The control problem assumes the form of the Schr\"odinger Bridge but differs from classical formulations by involving a degenerate diffusion process and an objective function with a novel time-dependent weight coefficient. The well-posedness of the control problem and its solution are discussed in detail, using a penalized formulation. The proposed theory is applied to juvenile upstream migration events of the diadromous fish species Plecoglossus altivelis altivelis commonly called Ayu in Japan. We also conduct sensitivity analysis of the models identified from real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03824v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H. Yoshioka</dc:creator>
    </item>
    <item>
      <title>Frugal forward-backward splitting methods with deviations</title>
      <link>https://arxiv.org/abs/2509.03865</link>
      <description>arXiv:2509.03865v1 Announce Type: new 
Abstract: The deviation vectors provide additional degrees of freedom and effectively enhance the flexibility of algorithms. In the literature, the iterative schemes with deviations are constructed and their convergence analyses are performed on an inefficient, algorithm-by-algorithm basis. In this paper, we address these by providing a general framework of frugal forward-backward splitting methods with deviations for finding zeros in the sum of a finite number of maximally monotone operators and cocoercive operators. Our framework encompasses the Douglas--Rachford splitting method with deviations. A unified weak convergence analysis is made under mild conditions. Numerical experiments on Markowitz portfolio optimization problem are given to demonstrate the effectiveness of deviations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03865v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yongyu Fu, Haowen Zheng, Qiao-Li Dong, Xiaolong Qin, Jing Zhao</dc:creator>
    </item>
    <item>
      <title>On the $p$-order Semismoothness of the Metric Projection onto Slices of the Positive Semidefinite Cone</title>
      <link>https://arxiv.org/abs/2509.03977</link>
      <description>arXiv:2509.03977v1 Announce Type: new 
Abstract: The metric projection onto the positive semidefinite (PSD) cone is strongly semismooth, a property that guarantees local quadratic convergence for many powerful algorithms in semidefinite programming. In this paper, we investigate whether this essential property holds for the metric projection onto an affine slice of the PSD cone, which is the operator implicitly used by many algorithms that handle linear constraints directly. Although this property is known to be preserved for the second-order cone, we conclusively demonstrate that this is not the case for the PSD cone. Specifically, we provide a constructive example that for any $p &gt; 0$, there exists an affine slice of a PSD cone for which the metric projection operator fails to be $p$-order semismooth. This finding establishes a fundamental difference between the geometry of the second-order cone and the PSD cone and necessitates new approaches for both analysis and algorithm design for linear semidefinite programming problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03977v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoning Chen, Jiaming Ma, Defeng Sun</dc:creator>
    </item>
    <item>
      <title>Towards understanding Accelerated Stein Variational Gradient Flow -- Analysis of Generalized Bilinear Kernels for Gaussian target distributions</title>
      <link>https://arxiv.org/abs/2509.04008</link>
      <description>arXiv:2509.04008v1 Announce Type: new 
Abstract: Stein variational gradient descent (SVGD) is a kernel-based and non-parametric particle method for sampling from a target distribution, such as in Bayesian inference and other machine learning tasks. Different from other particle methods, SVGD does not require estimating the score, which is the gradient of the log-density. However, in practice, SVGD can be slow compared to score-estimation-based sampling algorithms. To design a fast and efficient high-dimensional sampling algorithm with the advantages of SVGD, we introduce accelerated SVGD (ASVGD), based on an accelerated gradient flow in a metric space of probability densities following Nesterov's method. We then derive a momentum-based discrete-time sampling algorithm, which evolves a set of particles deterministically. To stabilize the particles' position update, we also include a Wasserstein metric regularization. This paper extends the conference version \cite{SL2025}. For the bilinear kernel and Gaussian target distributions, we study the kernel parameter and damping parameters with an optimal convergence rate of the proposed dynamics. This is achieved by analyzing the linearized accelerated gradient flows at the equilibrium. Interestingly, the optimal parameter is a constant, which does not depend on the covariance of the target distribution. For the generalized kernel functions, such as the Gaussian kernel, numerical examples with varied target distributions demonstrate the effectiveness of ASVGD compared to SVGD and other popular sampling methods. Furthermore, we show that in the setting of Bayesian neural networks, ASVGD outperforms SVGD significantly in terms of log-likelihood and total iteration times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04008v1</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Viktor Stein, Wuchen Li</dc:creator>
    </item>
    <item>
      <title>Distance Between Stochastic Linear Systems</title>
      <link>https://arxiv.org/abs/2509.04014</link>
      <description>arXiv:2509.04014v1 Announce Type: new 
Abstract: This manuscript proposes a distance measure between stochastic linear dynamical systems. While the existing stochastic control theory is well equipped to handle dynamical systems with stochastic uncertainties, a paradigm shift using distance measure based decision making is required for the effective further exploration of the field. As a first step, a distance measure between two linear time invariant stochastic dynamical systems is proposed here, extending the existing distance metrics between deterministic linear dynamical systems. Distance measure for stochastic systems is proposed for the frequency domain setting as the worst-case point-wise in frequency Wasserstein distance between distributions characterising the uncertainties using inverse stereographic projection on the Riemann sphere. For the time domain setting, the proposed distance corresponds to the gap metric induced type-$q$ Wasserstein distance between the push-forward measures under both systems' corresponding measurable maps from the parameter space to their respective space of system plants. It is proved and demonstrated using numerical simulation that the proposed frequency domain distance measure shall never exceed the proposed time domain distance measure counterpart. Lower and upper bounds are provided for the proposed distance measures in both frequency and time domain settings. The proposed distance measures induce a topology in the corresponding (frequency/time) domain space of stochastic dynamical systems and will facilitate the provision of probabilistic guarantees on system robustness and controller performances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04014v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Venkatraman Renganathan, Sei Zhen Khong</dc:creator>
    </item>
    <item>
      <title>Gromov-Wasserstein and optimal transport: from assignment problems to probabilistic numeric</title>
      <link>https://arxiv.org/abs/2509.04089</link>
      <description>arXiv:2509.04089v1 Announce Type: new 
Abstract: The assignment problem, a cornerstone of operations research, seeks an optimal one-to-one mapping between agents and tasks to minimize total cost. This work traces its evolution from classical formulations and algorithms to modern optimal transport (OT) theory, positioning the Quadratic Assignment Problem (QAP) and related structural matching tasks within this framework. We connect the linear assignment problem to Monge's transport problem, Kantorovich's relaxation, and Wasserstein distances, then extend to cases where source and target lie in different metric-measure spaces requiring Gromov-Wasserstein (GW) distances. GW formulations, including the fused GW variant that integrates structural and feature information, naturally address QAP-like problems by optimizing alignment based on both intra-domain distances and cross-domain attributes. Applications include graph matching, keypoint correspondence, and feature-based assignments. We present exact solvers, Genetic Algorithms (GA), and multiple GW variants, including a proposed multi-initialization strategy (GW-MultiInit) that mitigates the risk of getting stuck in local optima alongside entropic Sinkhorn-based approximations and fused GW. Computational experiments on capacitated QAP instances show that GW-MultiInit consistently achieves near-optimal solutions and scales efficiently to large problems where exact methods become impractical, while parameterized EGW and FGW variants provide flexible trade-offs between accuracy and runtime. Our findings provide theoretical foundations, computational insights, and practical guidelines for applying OT and GW methods to QAP and other real-world matching problems, such as those in machine learning and logistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04089v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iman Seyedi, Antonio Candelieri, Enza Messina, Francesco Archetti</dc:creator>
    </item>
    <item>
      <title>Optimal Control for Minimizing Inescapable Ellipsoids in Linear Periodically Time-Varying Systems Under Bounded Disturbances</title>
      <link>https://arxiv.org/abs/2509.04090</link>
      <description>arXiv:2509.04090v1 Announce Type: new 
Abstract: This letter addresses optimal controller design for periodic linear time-varying systems under unknown-but-bounded disturbances. We introduce differential Lyapunov-type equations to describe time-varying inescapable ellipsoids and define an integral-based measure of their size. To minimize this measure, we develop a differential Riccati equation-based approach that provides exact solutions for state-feedback, observer synthesis, and output-feedback control. A key component is a systematic procedure for determining the optimal time-varying parameter, reducing an infinite-dimensional optimization to a simple iterative process. A numerical example validates the method's effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04090v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/LCSYS.2025.3570559</arxiv:DOI>
      <arxiv:journal_reference>E. Dogadin and A. Peregudin, "Optimal Control for Minimizing Inescapable Ellipsoids in Linear Periodically Time-Varying Systems Under Bounded Disturbances," in IEEE Control Systems Letters, vol. 9, pp. 228-233, 2025</arxiv:journal_reference>
      <dc:creator>Egor Dogadin, Alexey Peregudin</dc:creator>
    </item>
    <item>
      <title>Duality between polyhedral approximation of value functions and optimal quantization of measures</title>
      <link>https://arxiv.org/abs/2509.04101</link>
      <description>arXiv:2509.04101v1 Announce Type: new 
Abstract: Approximating a convex function by a polyhedral function that has a limited number of facets is a fundamental problem with applications in various fields, from mitigating the curse of dimensionality in optimal control to bi-level optimization. We establish a connection between this problem and the optimal quantization of a positive measure. Building on recent stability results in optimal transport, by Delalande and M\'erigot, we deduce that the polyhedral approximation of a convex function is equivalent to the quantization of the Monge-Amp\`ere measure of its Legendre-Fenchel dual. This duality motivates a simple greedy method for computing a parsimonious approximation of a polyhedral convex function, by clustering the vertices of a Newton polytope. We evaluate our algorithm on two applications: 1) A high-dimensional optimal control problem (quantum gate synthesis), leveraging McEneaney's max-plus-based curse-of-dimensionality attenuation method; 2) A bi-level optimization problem in electricity pricing. Numerical results demonstrate the efficiency of this approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04101v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdellah Bulaich Mehamdi, Wim van Ackooij, Luce Brotcorne, St\'ephane Gaubert, Quentin Jacquet</dc:creator>
    </item>
    <item>
      <title>Some Remarks on the $l_1$-Robust Solution of LexRank Problem</title>
      <link>https://arxiv.org/abs/2509.04131</link>
      <description>arXiv:2509.04131v1 Announce Type: new 
Abstract: Graph-based ranking methods, such as LexRank, are fundamental in Natural Language Processing (NLP) applications like text summarization, as they measure the relative importance of textual units. Building on recent advances in ranking methods for growing and dynamic graphs, we develop a robust variant of LexRank that operates on stochastic similarity graphs with uncertain and expanding structure. Our approach introduces a novel $l_1$-based formulation that captures ambiguity in both transition probabilities and graph size, while maintaining sparsity. The resulting non-convex problem is upper-bounded by a linear program, providing a tractable and interpretable approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04131v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anna Timonina-Farkas</dc:creator>
    </item>
    <item>
      <title>Shuffling Heuristic in Variational Inequalities: Establishing New Convergence Guarantees</title>
      <link>https://arxiv.org/abs/2509.04133</link>
      <description>arXiv:2509.04133v1 Announce Type: new 
Abstract: Variational inequalities have gained significant attention in machine learning and optimization research. While stochastic methods for solving these problems typically assume independent data sampling, we investigate an alternative approach -- the shuffling heuristic. This strategy involves permuting the dataset before sequential processing, ensuring equal consideration of all data points. Despite its practical utility, theoretical guarantees for shuffling in variational inequalities remain unexplored. We address this gap by providing the first theoretical convergence estimates for shuffling methods in this context. Our analysis establishes rigorous bounds and convergence rates, extending the theoretical framework for this important class of algorithms. We validate our findings through extensive experiments on diverse benchmark variational inequality problems, demonstrating faster convergence of shuffling methods compared to independent sampling approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04133v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniil Medyakov, Gleb Molodtsov, Grigoriy Evseev, Egor Petrov, Aleksandr Beznosikov</dc:creator>
    </item>
    <item>
      <title>Safe Navigation in the Presence of Range-Limited Pursuers</title>
      <link>https://arxiv.org/abs/2509.04258</link>
      <description>arXiv:2509.04258v1 Announce Type: new 
Abstract: This paper examines the degree to which an evader seeking a safe and efficient path to a target location can benefit from increasing levels of knowledge regarding one or more range-limited pursuers seeking to intercept it. Unlike previous work, this research considers the time of flight of the pursuers actively attempting interception. It is shown that additional knowledge allows the evader to safely steer closer to the threats, shortening paths without accepting additional risk of capture. A control heuristic is presented, suitable for real time implementation, which capitalizes on all knowledge available to the evader.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04258v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thomas Chapman, Alexander Von Moll, Isaac E. Weintraub</dc:creator>
    </item>
    <item>
      <title>On computing sparse universal solvers for key problems in statistics</title>
      <link>https://arxiv.org/abs/2509.04264</link>
      <description>arXiv:2509.04264v1 Announce Type: new 
Abstract: We give sparsity results and present algorithms for calculating minimum (vector) 1-norm universal solvers connected to least-squares problems. In particular, besides universal least-squares solvers, we consider minimum-rank universal least-squares solvers, and simultaneous universal minimum-norm/least-squares solvers. For all of these, we present and compare several new alternative linear-optimization formulations and very effective proximal-point algorithms. Overall, we found that our new Douglas-Rachford splitting algorithms for these problems performed best.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04264v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ananias Sousa Machado, Marcia Fampa, Jon Lee</dc:creator>
    </item>
    <item>
      <title>Extending Linear Convergence of the Proximal Point Algorithm: The Quasar-Convex Case</title>
      <link>https://arxiv.org/abs/2509.04375</link>
      <description>arXiv:2509.04375v1 Announce Type: new 
Abstract: This work investigates the properties of the proximity operator for quasar-convex functions and establishes the convergence of the proximal point algorithm to a global minimizer with a particular focus on its convergence rate. In particular, we demonstrate: (i) the generated sequence is mi\-ni\-mi\-zing and achieves an $\mathcal{O}(\varepsilon^{-1})$ complexity rate for quasar-convex functions; (ii) under strong quasar-convexity, the sequence converges linearly and attains an $\mathcal{O}(\ln(\varepsilon^{-1}))$ complexity rate. These results extend known convergence rates from the (strongly) convex to the (strongly) quasar-convex setting. To the best of our knowledge, some findings are novel even for the special case of (strongly) star-convex functions. Numerical experiments corroborate our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04375v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jos\'e de Brito, Felipe Lara, Di Liu</dc:creator>
    </item>
    <item>
      <title>Global Convergence and Acceleration for Single Observation Gradient Free Optimization</title>
      <link>https://arxiv.org/abs/2509.04424</link>
      <description>arXiv:2509.04424v1 Announce Type: new 
Abstract: Simultaneous perturbation stochastic approximation (SPSA) is an approach to gradient-free optimization introduced by Spall as a simplification of the approach of Kiefer and Wolfowitz. In many cases the most attractive option is the single-sample version known as 1SPSA, which is the focus of the present paper, containing two major contributions: a modification of the algorithm designed to ensure convergence from arbitrary initial condition, and a new approach to exploration to dramatically accelerate the rate of convergence. Examples are provided to illustrate the theory, and to demonstrate that estimates from unmodified 1SPSA may diverge even for a quadratic objective function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04424v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caio Kalil Lauand, Sean Meyn</dc:creator>
    </item>
    <item>
      <title>Solving Zero-Sum Games with Fewer Matrix-Vector Products</title>
      <link>https://arxiv.org/abs/2509.04426</link>
      <description>arXiv:2509.04426v1 Announce Type: new 
Abstract: In this paper we consider the problem of computing an $\epsilon$-approximate Nash Equilibrium of a zero-sum game in a payoff matrix $A \in \mathbb{R}^{m \times n}$ with $O(1)$-bounded entries given access to a matrix-vector product oracle for $A$ and its transpose $A^\top$. We provide a deterministic algorithm that solves the problem using $\tilde{O}(\epsilon^{-8/9})$-oracle queries, where $\tilde{O}(\cdot)$ hides factors polylogarithmic in $m$, $n$, and $\epsilon^{-1}$. Our result improves upon the state-of-the-art query complexity of $\tilde{O}(\epsilon^{-1})$ established by [Nemirovski, 2004] and [Nesterov, 2005]. We obtain this result through a general framework that yields improved deterministic query complexities for solving a broader class of minimax optimization problems which includes computing a linear classifier (hard-margin support vector machine) as well as linear regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04426v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ishani Karmarkar, Liam O'Carroll, Aaron Sidford</dc:creator>
    </item>
    <item>
      <title>The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric</title>
      <link>https://arxiv.org/abs/2509.03594</link>
      <description>arXiv:2509.03594v1 Announce Type: cross 
Abstract: We present a class of novel optimisers for training neural networks that makes use of the Riemannian metric naturally induced when the loss landscape is embedded in higher-dimensional space. This is the same metric that underlies common visualisations of loss landscapes. By taking this geometric perspective literally and using the induced metric, we develop a new optimiser and compare it to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of tasks and architectures. Empirically, we conclude that this new class of optimisers is highly effective in low dimensional examples, and provides slight improvement over state-of-the-art methods for training neural networks. These new optimisers have theoretically desirable properties. In particular, the effective learning rate is automatically decreased in regions of high curvature acting as a smoothed out form of gradient clipping. Similarly, one variant of these optimisers can also be viewed as inducing an effective scheduled learning rate and decoupled weight decay is the natural choice from our geometric perspective. The basic method can be used to modify any existing preconditioning method. The new optimiser has a computational complexity comparable to that of Adam.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03594v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas R. Harvey</dc:creator>
    </item>
    <item>
      <title>Mean-Variance Stackelberg Games with Asymmetric Information</title>
      <link>https://arxiv.org/abs/2509.03669</link>
      <description>arXiv:2509.03669v1 Announce Type: cross 
Abstract: This paper considers two investors who perform mean-variance portfolio selection with asymmetric information: one knows the true stock dynamics, while the other has to infer the true dynamics from observed stock evolution. Their portfolio selection is interconnected through relative performance concerns, i.e., each investor is concerned about not only her terminal wealth, but how it compares to the average terminal wealth of both investors. We model this as Stackelberg competition: the partially-informed investor (the "follower") observes the trading behavior of the fully-informed investor (the "leader") and decides her trading strategy accordingly; the leader, anticipating the follower's response, in turn selects a trading strategy that best suits her objective. To prevent information leakage, the leader adopts a randomized strategy selected under an entropy-regularized mean-variance objective, where the entropy regularizer quantifies the randomness of a chosen strategy. The follower, on the other hand, observes only the actual trading actions of the leader (sampled from the randomized strategy), but not the randomized strategy itself. Her mean-variance objective is thus a random field, in the form of an expectation conditioned on a realized path of the leader's trading actions. In the idealized case of continuous sampling of the leader's trading actions, we derive a Stackelberg equilibrium where the follower's trading strategy depends linearly on the actual trading actions of the leader and the leader samples her trading actions from Gaussian distributions. In the realistic case of discrete sampling of the leader's trading actions, the above becomes an $\epsilon$-Stackelberg equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03669v1</guid>
      <category>q-fin.MF</category>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu-Jui Huang, Shihao Zhu</dc:creator>
    </item>
    <item>
      <title>Avoidance of an unexpected obstacle without reinforcement learning: Why not using advanced control-theoretic tools?</title>
      <link>https://arxiv.org/abs/2509.03721</link>
      <description>arXiv:2509.03721v1 Announce Type: cross 
Abstract: This communication on collision avoidance with unexpected obstacles is motivated by some critical appraisals on reinforcement learning (RL) which "requires ridiculously large numbers of trials to learn any new task" (Yann LeCun). We use the classic Dubins' car in order to replace RL with flatness-based control, combined with the HEOL feedback setting, and the latest model-free predictive control approach. The two approaches lead to convincing computer experiments where the results with the model-based one are only slightly better. They exhibit a satisfactory robustness with respect to randomly generated mismatches/disturbances, which become excellent in the model-free case. Those properties would have been perhaps difficult to obtain with today's popular machine learning techniques in AI. Finally, we should emphasize that our two methods require a low computational burden.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03721v1</guid>
      <category>eess.SY</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C\'edric Join, Michel Fliess</dc:creator>
    </item>
    <item>
      <title>Reservoir Predictive Path Integral Control for Unknown Nonlinear Dynamics</title>
      <link>https://arxiv.org/abs/2509.03839</link>
      <description>arXiv:2509.03839v1 Announce Type: cross 
Abstract: Neural networks capable of approximating complex nonlinearities have found extensive application in data-driven control of nonlinear dynamical systems. However, fast online identification and control of unknown dynamics remain central challenges. This paper integrates echo-state networks (ESNs) -- reservoir computing models implemented with recurrent neural networks -- and model predictive path integral (MPPI) control -- sampling-based variants of model predictive control -- to meet these challenges. The proposed reservoir predictive path integral (RPPI) enables fast learning of nonlinear dynamics with ESN and exploits the learned nonlinearities directly in parallelized MPPI control computation without linearization approximations. The framework is further extended to uncertainty-aware RPPI (URPPI), which leverages ESN uncertainty to balance exploration and exploitation: exploratory inputs dominate during early learning, while exploitative inputs prevail as model confidence grows. Experiments on controlling the Duffing oscillator and four-tank systems demonstrate that URPPI improves control performance, reducing control costs by up to 60% compared to traditional quadratic programming-based model predictive control methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03839v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <category>nlin.CD</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daisuke Inoue, Tadayoshi Matsumori, Gouhei Tanaka, Yuji Ito</dc:creator>
    </item>
    <item>
      <title>Regulation or Competition:Major-Minor Optimal Liquidation across Dark and Lit Pools</title>
      <link>https://arxiv.org/abs/2509.03916</link>
      <description>arXiv:2509.03916v1 Announce Type: cross 
Abstract: We study the optimal liquidation problem in both lit and dark pools for investors facing execution uncertainty in a continuous-time setting with market impact. First, we design an optimal make--take fee policy for a large investor liquidating her position across both pools, interacting with small investors who pay trading fees. We explicitly characterize the large investor's optimal liquidation strategies in both lit and dark pools using BSDEs under a compensation scheme proposed by an exchange to mitigate market impact in the lit venue. Second, we consider a purely competitive model with major--minor traders in the absence of regulation. We provide explicit solutions to the associated HJB--Fokker--Planck system. Finally, we illustrate our results through numerical experiments, comparing market impact under a regulated market with a strategic large investor to that in a purely competitive market with both small and large investors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03916v1</guid>
      <category>q-fin.MF</category>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thibaut Mastrolia, Hao Wang</dc:creator>
    </item>
    <item>
      <title>Counterfactual simulations for large scale systems with burnout variables</title>
      <link>https://arxiv.org/abs/2509.04038</link>
      <description>arXiv:2509.04038v1 Announce Type: cross 
Abstract: We consider large-scale systems influenced by burnout variables - state variables that start active, shape dynamics, and irreversibly deactivate once certain conditions are met. Simulating what-if scenarios in such systems is computationally demanding, as alternative trajectories often require sequential processing, which does not scale very well. This challenge arises in settings like online advertising, because of campaigns budgets, complicating counterfactual analysis despite rich data availability. We introduce a new type of algorithms based on what we refer to as uncertainty relaxation, that enables efficient parallel computation, significantly improving scalability for counterfactual estimation in systems with burnout variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04038v1</guid>
      <category>cs.DC</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Heymann</dc:creator>
    </item>
    <item>
      <title>Compatibility of Multiple Control Barrier Functions for Constrained Nonlinear Systems</title>
      <link>https://arxiv.org/abs/2509.04220</link>
      <description>arXiv:2509.04220v1 Announce Type: cross 
Abstract: Control barrier functions (CBFs) are a powerful tool for the constrained control of nonlinear systems; however, the majority of results in the literature focus on systems subject to a single CBF constraint, making it challenging to synthesize provably safe controllers that handle multiple state constraints. This paper presents a framework for constrained control of nonlinear systems subject to box constraints on the systems' vector-valued outputs using multiple CBFs. Our results illustrate that when the output has a vector relative degree, the CBF constraints encoding these box constraints are compatible, and the resulting optimization-based controller is locally Lipschitz continuous and admits a closed-form expression. Additional results are presented to characterize the degradation of nominal tracking objectives in the presence of safety constraints. Simulations of a planar quadrotor are presented to demonstrate the efficacy of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04220v1</guid>
      <category>eess.SY</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max H. Cohen, Eugene Lavretsky, Aaron D. Ames</dc:creator>
    </item>
    <item>
      <title>Polynomial Stability of Non-Linearly Damped Contraction Semigroups</title>
      <link>https://arxiv.org/abs/2509.04275</link>
      <description>arXiv:2509.04275v1 Announce Type: cross 
Abstract: We investigate the stability properties of an abstract class of semi-linear systems. Our main result establishes rational rates of decay for classical solutions assuming a certain non-uniform observability estimate for the linear part and suitable conditions on the non-linearity. We illustrate the strength of our abstract results by applying them to a one-dimensional wave equation with weak non-linear damping and to an Euler-Bernoulli beam with a tip mass subject to non-linear damping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04275v1</guid>
      <category>math.FA</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lassi Paunonen, David Seifert</dc:creator>
    </item>
    <item>
      <title>SAFE--MA--RRT: Multi-Agent Motion Planning with Data-Driven Safety Certificates</title>
      <link>https://arxiv.org/abs/2509.04413</link>
      <description>arXiv:2509.04413v1 Announce Type: cross 
Abstract: This paper proposes a fully data-driven motion-planning framework for homogeneous linear multi-agent systems that operate in shared, obstacle-filled workspaces without access to explicit system models. Each agent independently learns its closed-loop behavior from experimental data by solving convex semidefinite programs that generate locally invariant ellipsoids and corresponding state-feedback gains. These ellipsoids, centered along grid-based waypoints, certify the dynamic feasibility of short-range transitions and define safe regions of operation. A sampling-based planner constructs a tree of such waypoints, where transitions are allowed only when adjacent ellipsoids overlap, ensuring invariant-to-invariant transitions and continuous safety. All agents expand their trees simultaneously and are coordinated through a space-time reservation table that guarantees inter-agent safety by preventing simultaneous occupancy and head-on collisions. Each successful edge in the tree is equipped with its own local controller, enabling execution without re-solving optimization problems at runtime. The resulting trajectories are not only dynamically feasible but also provably safe with respect to both environmental constraints and inter-agent collisions. Simulation results demonstrate the effectiveness of the approach in synthesizing synchronized, safe trajectories for multiple agents under shared dynamics and constraints, using only data and convex optimization tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04413v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Babak Esmaeili, Hamidreza Modares</dc:creator>
    </item>
    <item>
      <title>FastPart: Over-Parameterized Stochastic Gradient Descent for Sparse optimisation on Measures</title>
      <link>https://arxiv.org/abs/2312.05993</link>
      <description>arXiv:2312.05993v2 Announce Type: replace 
Abstract: This paper presents a novel algorithm that leverages Stochastic Gradient Descent strategies in conjunction with Random Features to augment the scalability of Conic Particle Gradient Descent (CPGD) specifically tailored for solving sparse optimization problems on measures. By formulating the CPGD steps within a variational framework, we provide rigorous mathematical proofs demonstrating the following key findings: $\mathrm{(i)}$ The total variation norms of the solution measures along the descent trajectory remain bounded, ensuring stability and preventing undesirable divergence; $\mathrm{(ii)}$ We establish a global convergence guarantee with a convergence rate of ${O}(\log(K)/\sqrt{K})$ over $K$ iterations, showcasing the efficiency and effectiveness of our algorithm, $\mathrm{(iii)}$ Additionally, we analyse and establish local control over the first-order condition discrepancy, contributing to a deeper understanding of the algorithm's behaviour and reliability in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05993v2</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yohann De Castro, S\'ebastien Gadat, Cl\'ement Marteau</dc:creator>
    </item>
    <item>
      <title>The best approximation pair problem relative to two subsets in a normed space</title>
      <link>https://arxiv.org/abs/2403.18767</link>
      <description>arXiv:2403.18767v2 Announce Type: replace 
Abstract: In the classical best approximation pair (BAP) problem, one is given two nonempty, closed, convex and disjoint subsets in a finite- or an infinite-dimensional Hilbert space, and the goal is to find a pair of points, each from each subset, which realizes the distance between the subsets. We discuss the problem in more general normed spaces and with possibly non-convex subsets, and focus our attention on the issues of uniqueness and existence of the solution to the problem. As far as we know, these fundamental issues have not received much attention. We present several sufficient geometric conditions for the (at most) uniqueness of a BAP. These conditions are related to the structure and the relative orientation of the boundaries of the subsets and to the norm. We also present many sufficient conditions for the existence of a BAP. Our results significantly extend the horizon of a recent algorithm for solving the BAP problem [Censor, Mansour, Reem, J. Approx. Theory (2024)]. The paper also shows, perhaps for the first time, how wide is the scope of the BAP problem in terms of the scientific communities which are involved in it (frequently independently) and in terms of its applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18767v2</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <category>math.MG</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Reem, Yair Censor</dc:creator>
    </item>
    <item>
      <title>Observer-Based Realization of Control Systems</title>
      <link>https://arxiv.org/abs/2404.15688</link>
      <description>arXiv:2404.15688v2 Announce Type: replace 
Abstract: A novel model reduction framework for large-scale complex systems is proposed by introducing function-type dynamic control systems via the dimension-keeping semi-tensor product (DK-STP) of matrices. Utilizing bridge matrices, the DK-STP facilitates the construction of an approximate observer-based realization (OR) of a linear control system in the form of a function-type control system, where the functions serve as observers. A necessary and sufficient condition is established for the OR-system to admit exact observer dynamics. When an exact OR-system does not exist, an extended OR-system is developed by incorporating the original system's observers into its state. Furthermore, a minimal feedback extended OR-system is constructed, and its relationship to Kalman's minimal realization is analyzed. Finally, the proposed approach is extended to nonlinear control-affine systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15688v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daizhan Cheng, Xiao Zhang, Zhengping Ji, Changxi Li</dc:creator>
    </item>
    <item>
      <title>Quantum Computing for Discrete Optimization: A Highlight of Three Technologies</title>
      <link>https://arxiv.org/abs/2409.01373</link>
      <description>arXiv:2409.01373v2 Announce Type: replace 
Abstract: Quantum optimization has emerged as a promising frontier of quantum computing, providing novel numerical approaches to mathematical optimization problems. The main goal of this paper is to facilitate interdisciplinary research between the Operations Research (OR) and Quantum Computing communities by helping OR scientists to build initial intuition for-, and offering them a hands-on gateway to quantum-powered methods in the context of discrete optimization. To this end, we consider three quantum-powered optimization approaches that make use of different types of quantum hardware available on the market. To illustrate these approaches, we solve three classical optimization problems: the Traveling Salesperson Problem, Weighted Maximum Cut, and Maximum Independent Set. With a general OR audience in mind, we attempt to provide an intuition behind each approach along with key references, describe the corresponding high-level workflow, and highlight crucial practical considerations. In particular, we emphasize the importance of problem formulations and device-specific configurations, and their impact on the amount of resources required for computation (where we focus on the number of qubits). These points are illustrated with a series of experiments on three types of quantum computers: a neutral atom machine from QuEra, a quantum annealer from D-Wave, and gate-based devices from IBM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01373v2</guid>
      <category>math.OC</category>
      <category>quant-ph</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ejor.2025.07.063</arxiv:DOI>
      <dc:creator>Alexey Bochkarev (RPTU Kaiserslautern-Landau), Raoul Heese (Fraunhofer Institute of Industrial Mathematics ITWM), Sven J\"ager (RPTU Kaiserslautern-Landau), Philine Schiewe (Aalto University), Anita Sch\"obel (RPTU Kaiserslautern-Landau, Fraunhofer Institute of Industrial Mathematics ITWM)</dc:creator>
    </item>
    <item>
      <title>New results related to cutters and to an extrapolated block-iterative method for finding a common fixed point of a collection of them</title>
      <link>https://arxiv.org/abs/2410.20448</link>
      <description>arXiv:2410.20448v3 Announce Type: replace 
Abstract: Given a Hilbert space and a finite family of operators defined on the space, the common fixed point problem (CFPP) is to find a point in the intersection of the fixed point sets of these operators. Instances of the problem have numerous applications in science and engineering. We consider an extrapolated block-iterative method with dynamic weights for solving the CFPP assuming the operators belong to a wide class of operators called cutters. Global convergence is proved in two different scenarios, one of them is under a seemingly new condition on the weights which is less restrictive than a condition suggested in previous works. In order to establish convergence, we derive various new results of independent interest related to cutters, some of them extend, generalize and clarify previously published results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20448v3</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.37193/CJM.2025.04.01</arxiv:DOI>
      <arxiv:journal_reference>Carpathian Journal of Mathematics 41 (2025), pp. 863--884</arxiv:journal_reference>
      <dc:creator>Yair Censor, Daniel Reem, Maroun Zaknoon</dc:creator>
    </item>
    <item>
      <title>Stochastic LQR Design With Disturbance Preview</title>
      <link>https://arxiv.org/abs/2412.06662</link>
      <description>arXiv:2412.06662v4 Announce Type: replace 
Abstract: This paper considers the discrete-time, stochastic LQR problem with $p$ steps of disturbance preview information where $p$ is finite. We first derive the solution for this problem on a finite horizon with linear, time-varying dynamics and time-varying costs. Next, we derive the solution on the infinite horizon with linear, time-invariant dynamics and time-invariant costs. Our proofs rely on the well-known principle of optimality. We provide an independent proof for the principle of optimality that relies only on nested information structure. Finally, we show that the finite preview controller converges to the optimal noncausal controller as the preview horizon $p$ tends to infinity. We also provide a simple example to illustrate both the finite and infinite horizon results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06662v4</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jietian Liu, Laurent Lessard, Peter Seiler</dc:creator>
    </item>
    <item>
      <title>Semilinear Dynamic Programming: Analysis, Algorithms, and Certainty Equivalence Properties</title>
      <link>https://arxiv.org/abs/2501.04668</link>
      <description>arXiv:2501.04668v2 Announce Type: replace 
Abstract: We consider a broad class of dynamic programming (DP) problems that involve a partially linear structure and some positivity properties in their system equation and cost function. We address deterministic and stochastic problems, possibly with Markov jump parameters. We focus primarily on infinite horizon problems and prove that under our assumptions, the optimal cost function is linear, and that an optimal policy can be computed efficiently with standard DP algorithms. Moreover, we show that forms of certainty equivalence hold for our stochastic problems, in analogy with the classical linear quadratic optimal control problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04668v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchao Li, Dimitri Bertsekas</dc:creator>
    </item>
    <item>
      <title>Increasing competitiveness by imbalanced groups: The example of the 48-team FIFA World Cup</title>
      <link>https://arxiv.org/abs/2502.08565</link>
      <description>arXiv:2502.08565v2 Announce Type: replace 
Abstract: A match played in a sports tournament can be called stakeless if at least one team is indifferent to its outcome because it already has qualified or has been eliminated. Such a game threatens fairness since teams may not exert full effort without incentives. This paper suggests a novel classification for stakeless matches based on their expected outcome: they are more costly if the indifferent team is more likely to win by playing honestly. Our approach is illustrated with the 2026 FIFA World Cup, the first edition of the competition with 48 teams. We propose a novel format based on imbalanced groups, which substantially reduces the probability of stakeless matches played by the strongest teams according to Monte Carlo simulations. The new design also increases the uncertainty of match outcomes and requires fewer matches. Governing bodies in sports are encouraged to consider our innovative idea in order to enhance the competitiveness of their tournaments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08565v2</guid>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, Andr\'as Gyimesi</dc:creator>
    </item>
    <item>
      <title>System Identification from Partial Observations under Adversarial Attacks</title>
      <link>https://arxiv.org/abs/2504.00244</link>
      <description>arXiv:2504.00244v3 Announce Type: replace 
Abstract: This paper is concerned with the partially observed linear system identification, where the goal is to obtain reasonably accurate estimation of the balanced truncation of the true system up to order $k$ from output measurements. We consider the challenging case of system identification under adversarial attacks, where the probability of having an attack at each time is $\Theta(1/k)$ while the value of the attack is arbitrary. We first show that the $\ell_1$-norm estimator exactly identifies the true Markov parameter matrix for nilpotent systems under any type of attack. We then build on this result to extend it to general systems and show that the estimation error exponentially decays as $k$ grows. The estimated balanced truncation model accordingly shows an exponentially decaying error for the identification of the true system up to a similarity transformation. This work is the first to provide the input-output analysis of the system with partial observations under arbitrary attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00244v3</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jihun Kim, Javad Lavaei</dc:creator>
    </item>
    <item>
      <title>Bounding Escape Rates and Approximating Quasi-Stationary Distributions of Brownian Dynamics</title>
      <link>https://arxiv.org/abs/2504.00729</link>
      <description>arXiv:2504.00729v2 Announce Type: replace 
Abstract: Throughout physics Brownian dynamics are used to describe the behaviour of molecular systems. When the Brownian particle is confined to a bounded domain, a particularly important question arises around determining how long it takes the particle to encounter certain regions of the boundary from which it can escape. Termed the first passage time, it sets the natural timescale of the chemical, biological, and physical processes that are described by the stochastic differential equation. Probabilistic information about the first passage time can be studied using spectral properties of the deterministic generator of the stochastic process. In this work we introduce a framework for bounding the leading eigenvalue of the generator which determines the exponential rate of escape of the particle from the domain. The method employs sum-of-squares programming to produce nearly sharp numerical upper and lower bounds on the leading eigenvalue, while also giving good numerical approximations of the associated leading eigenfunction, the quasi-stationary distribution of the process. To demonstrate utility, the method is applied to prototypical low-dimensional problems from the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00729v2</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <category>math.DS</category>
      <category>math.PR</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason J. Bramburger</dc:creator>
    </item>
    <item>
      <title>Closed-Loop Neural Operator-Based Observer of Traffic Density</title>
      <link>https://arxiv.org/abs/2504.04873</link>
      <description>arXiv:2504.04873v2 Announce Type: replace 
Abstract: We consider the problem of traffic density estimation with sparse measurements from stationary roadside sensors. Our approach uses Fourier neural operators to learn macroscopic traffic flow dynamics from high-fidelity data. During inference, the operator functions as an open-loop predictor of traffic evolution. To close the loop, we couple the open-loop operator with a correction operator that combines the predicted density with sparse measurements from the sensors. Simulations with the SUMO software indicate that, compared to open-loop observers, the proposed closed-loop observer exhibits classical closed-loop properties such as robustness to noise and ultimate boundedness of the error. This shows the advantages of combining learned physics with real-time corrections, and opens avenues for accurate, efficient, and interpretable data-driven observers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04873v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alice Harting, Karl Henrik Johansson, Matthieu Barreau</dc:creator>
    </item>
    <item>
      <title>A dynamic view of some anomalous phenomena in SGD</title>
      <link>https://arxiv.org/abs/2505.01751</link>
      <description>arXiv:2505.01751v2 Announce Type: replace 
Abstract: It has been observed by Belkin et al.\ that over-parametrized neural networks exhibit a `double descent' phenomenon. That is, as the model complexity (as reflected in the number of features) increases, the test error initially decreases, then increases, and then decreases again. A counterpart of this phenomenon in the time domain has been noted in the context of epoch-wise training, viz., the test error decreases with the number of iterates, then increases, then decreases again. Another anomalous phenomenon is that of \textit{grokking} wherein two regimes of descent are interrupted by a third regime wherein the mean loss remains almost constant. This note presents a plausible explanation for these and related phenomena by using the theory of two time scale stochastic approximation, applied to the continuous time limit of the gradient dynamics. This gives a novel perspective for an already well studied theme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01751v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivek Shripad Borkar</dc:creator>
    </item>
    <item>
      <title>Collective steering: Tracer-informed dynamics</title>
      <link>https://arxiv.org/abs/2505.01975</link>
      <description>arXiv:2505.01975v2 Announce Type: replace 
Abstract: We consider control and inference problems where control protocols and internal dynamics are informed by two types of constraints. Our data consist of i) statistics on the ensemble and ii) trajectories or final disposition of selected tracer particles embedded in the flow. Our aim is i') to specify a control protocol to realize a flow that meets such constraints or ii') to recover the internal dynamics that are consistent with such a data set. We analyze these problems in the setting of linear flows and Gaussian distributions. The control cost is taken to be a suitable action integral constrained by either the trajectories of tracer particles or their terminal placements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01975v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asmaa Eldesoukey, Mahmoud Abdelgalil, Tryphon T. Georgiou</dc:creator>
    </item>
    <item>
      <title>Compact R-Continuity with Applications to Solving Inclusions and Convergence of Algorithms</title>
      <link>https://arxiv.org/abs/2509.01872</link>
      <description>arXiv:2509.01872v2 Announce Type: replace 
Abstract: This paper investigates the notion of compact R-continuity and its specifications for set-valued mappings between Banach spaces. We reveal several important properties of compact R-continuity in general settings and show that in finite dimensions, this notion is supported by the classical Lojasiewicz inequality for analytic functions. An application of compact R-continuity and the obtained results is given to convergence analysis for a broad class of descent algorithms in nonsmooth optimization. We also show that this notion is instrumental for the design and justification of a novel R-class of algorithms to solve inclusion problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01872v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ba Khiet Le, Boris S. Mordukhovich, Michel A. Thera</dc:creator>
    </item>
    <item>
      <title>Safe Sequences via Dominators in DAGs for Path-Covering Problems</title>
      <link>https://arxiv.org/abs/2411.03871</link>
      <description>arXiv:2411.03871v4 Announce Type: replace-cross 
Abstract: A path-covering problem on a directed acyclic graph (DAG) requires finding a set of source-to-sink paths that cover all the nodes, all the arcs, or subsets thereof, and additionally they are optimal with respect to some function. In this paper we study safe sequences of nodes or arcs, namely sequences that appear in some path of every path cover of a DAG.
  We show that safe sequences admit a simple characterization via cutnodes. Moreover, we establish a connection between maximal safe sequences and leaf-to-root paths in the source- and sink-dominator trees of the DAG, which may be of independent interest in the extensive literature on dominators. With dominator trees, safe sequences admit an O(n)-size representation and a linear-time output-sensitive enumeration algorithm running in time O(m + o), where n and m are the number of nodes and arcs, respectively, and o is the total length of the maximal safe sequences.
  We then apply maximal safe sequences to simplify Integer Linear Programs (ILPs) for two path-covering problems, LeastSquares and MinPathError, which are at the core of RNA transcript assembly problems from bioinformatics. On various datasets, maximal safe sequences can be computed in under 0.1 seconds per graph, on average, and ILP solvers whose search space is reduced in this manner exhibit significant speed-ups. For example on graphs with a large width, average speed-ups are in the range 50-250x for MinPathError and in the range 80-350x for LeastSquares. Optimizing ILPs using safe sequences can thus become a fast building block of practical RNA transcript assembly tools, and more generally, of path-covering problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03871v4</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <category>q-bio.GN</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francisco Sena, Romeo Rizzi, Alexandru I. Tomescu</dc:creator>
    </item>
    <item>
      <title>MARS: Unleashing the Power of Variance Reduction for Training Large Models</title>
      <link>https://arxiv.org/abs/2411.10438</link>
      <description>arXiv:2411.10438v4 Announce Type: replace-cross 
Abstract: Training deep neural networks--and more recently, large models demands efficient and scalable optimizers. Adaptive gradient algorithms like Adam, AdamW, and their variants have been central to this task. Despite the development of numerous variance reduction algorithms in the past decade aimed at accelerating stochastic optimization in both convex and nonconvex settings, variance reduction has not found widespread success in training deep neural networks or large language models. Consequently, it has remained a less favored approach in modern AI. In this paper, to unleash the power of variance reduction for efficient training of large models, we propose a unified optimization framework, MARS (Make vAriance Reduction Shine), which reconciles preconditioned gradient methods with variance reduction via a scaled stochastic recursive momentum technique. Within our framework, we introduce three instances of MARS that leverage preconditioned gradient updates based on AdamW, Lion, and Shampoo, respectively. We also draw a connection between our algorithms and existing optimizers. Experimental results on training GPT-2 models indicate that MARS consistently outperforms AdamW by a large margin. The implementation of MARS is available at https://github.com/AGI-Arena/MARS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10438v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huizhuo Yuan, Yifeng Liu, Shuang Wu, Xun Zhou, Quanquan Gu</dc:creator>
    </item>
    <item>
      <title>ACING: Actor-Critic for Instruction Learning in Black-Box LLMs</title>
      <link>https://arxiv.org/abs/2411.12736</link>
      <description>arXiv:2411.12736v2 Announce Type: replace-cross 
Abstract: The effectiveness of Large Language Models (LLMs) in solving tasks depends significantly on the quality of their instructions, which often require substantial human effort to craft. This underscores the need for automated instruction optimization. However, optimizing instructions is particularly challenging when working with black-box LLMs, where model parameters and gradients are inaccessible. We introduce ACING, an actor-critic reinforcement learning framework that formulates instruction optimization as a stateless, continuous-action problem, enabling exploration of infinite instruction spaces using only black-box feedback. ACING automatically discovers prompts that outperform human-written prompts in 76% of instruction-induction tasks, with gains of up to 33 points and a 10-point median improvement over the best automatic baseline in 33 tasks spanning instruction-induction, summarization, and chain-of-thought reasoning. Extensive ablations highlight its robustness and efficiency. An implementation of ACING is available at https://github.com/salmakh1/ACING.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12736v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Salma Kharrat, Fares Fourati, Marco Canini</dc:creator>
    </item>
    <item>
      <title>On Mean Field Games in Infinite Dimension</title>
      <link>https://arxiv.org/abs/2411.14604</link>
      <description>arXiv:2411.14604v3 Announce Type: replace-cross 
Abstract: We study a Mean Field Games (MFG) system in a real, separable infinite dimensional Hilbert space. The system consists of a second order parabolic type equation, called Hamilton-Jacobi-Bellman (HJB) equation in the paper, coupled with a nonlinear Fokker-Planck (FP) equation. Both equations contain a Kolmogorov operator. Solutions to the HJB equation are interpreted in the mild solution sense and solutions to the FP equation are interpreted in an appropriate weak sense. We prove well-posedness of the considered MFG system under certain conditions. The existence of a solution to the MFG system is proved using Tikhonov's fixed point theorem in a proper space. Uniqueness of solutions is obtained under typical separability and Lasry-Lions type monotonicity conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14604v3</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Salvatore Federico, Fausto Gozzi, Andrzej \'Swi\k{e}ch</dc:creator>
    </item>
    <item>
      <title>Dataset Distillation as Pushforward Optimal Quantization</title>
      <link>https://arxiv.org/abs/2501.07681</link>
      <description>arXiv:2501.07681v2 Announce Type: replace-cross 
Abstract: Dataset distillation aims to find a synthetic training set such that training on the synthetic data achieves similar performance to training on real data, with orders of magnitude less computational requirements. Existing methods can be broadly categorized as either bi-level optimization problems that have neural network training heuristics as the lower level problem, or disentangled methods that bypass the bi-level optimization by matching distributions of data. The latter method has the major advantages of speed and scalability in terms of size of both training and distilled datasets. We demonstrate that when equipped with an encoder-decoder structure, the empirically successful disentangled methods can be reformulated as an optimal quantization problem, where a finite set of points is found to approximate the underlying probability measure by minimizing the expected projection distance. In particular, we link existing disentangled dataset distillation methods to the classical optimal quantization and Wasserstein barycenter problems, demonstrating consistency of distilled datasets for diffusion-based generative priors. We propose Dataset Distillation by Optimal Quantization, based on clustering in a latent space. Compared to the previous SOTA method D\textsuperscript{4}M, we achieve better performance and inter-model generalization on the ImageNet-1K dataset with trivial additional computation, and SOTA performance in higher image-per-class settings. Using the distilled noise initializations in a stronger diffusion transformer model, we obtain SOTA distillation performance on ImageNet-1K and its subsets, outperforming diffusion guidance methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07681v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong Ye Tan, Emma Slade</dc:creator>
    </item>
    <item>
      <title>Strong Lyapunov functions for rough systems</title>
      <link>https://arxiv.org/abs/2508.14559</link>
      <description>arXiv:2508.14559v2 Announce Type: replace-cross 
Abstract: We introduce the concept of {\it strong Lyapunov functions} to investigate the long term behavior of autonomous ordinary differential equations under a multiplicative noise of H\"older continuity, using rough path calculus and the framework of random dynamical systems. We conclude that if such a function exists for the drift then the perturbed system admits the global random pullback attractor which is upper semi-continuous w.r.t. the noise intensity coefficient and the dyadic approximation of the noise. Moreover, in case the drift is globally Lipschitz continuous, then there exists also a numerical attractor for the discritization which is also upper semi-continuous w.r.t. the noise intensity and also converges to the continuous attractor as the step size tends to zero. Several applications are studied, including the pendulum and the Fitzhugh Nagumo neuro-system. We also prove that strong Lyapunov functions could be approximated in practice by Lyapunov neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14559v2</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luu Hoang Duc, J\"urgen Jost</dc:creator>
    </item>
  </channel>
</rss>
