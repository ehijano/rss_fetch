<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Sep 2025 01:28:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Complete Decentralization of Linear Quadratic Gaussian Control for the Discrete Wave Equation</title>
      <link>https://arxiv.org/abs/2509.13446</link>
      <description>arXiv:2509.13446v1 Announce Type: new 
Abstract: The linear quadratic Gaussian (LQG) control problem for the linear wave equation on the unit circle with fully distributed actuation and partial state measurements is considered. An analytical solution to a spatial discretization of the problem is obtained. The main result of this work illustrates that for specific parameter values, the optimal LQG policy is completely decentralized, meaning only a measurement at spatial location $i$ is needed to compute an optimal control signal to actuate at this location. The relationship between performance and decentralization as a function of parameters is explored. Conditions for complete decentralization are related to metrics of kinetic and potential energy quantities and control effort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13446v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Addie McCurdy, Emily Jensen</dc:creator>
    </item>
    <item>
      <title>Topological Entropy of Nonlinear Time-Varying Systems</title>
      <link>https://arxiv.org/abs/2509.13537</link>
      <description>arXiv:2509.13537v1 Announce Type: new 
Abstract: Two general upper bounds on the topological entropy of nonlinear time-varying systems are established: one using the matrix measure of the system Jacobian, the other using the largest real part of the eigenvalues of the Jacobian matrix with off-diagonal entries replaced by their absolute values. A general lower bound is constructed using the trace of the Jacobian matrix. For interconnected systems, an upper bound is first derived by adapting one of the general upper bounds, using the matrix measure of an interconnection matrix function. A new upper bound is then developed using the largest real part of the eigenvalues of this function. This new bound is closely related to the individual upper bounds for subsystems and implies each of the two general upper bounds when the system is viewed as one of two suitable interconnections. These entropy bounds all depend only on upper or lower limits of the Jacobian matrix along trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13537v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.DS</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Guosong Yang, Daniel Liberzon</dc:creator>
    </item>
    <item>
      <title>Complexity Bounds for Smooth Convex Multiobjective Optimization</title>
      <link>https://arxiv.org/abs/2509.13550</link>
      <description>arXiv:2509.13550v1 Announce Type: new 
Abstract: We study the oracle complexity of finding $\varepsilon$-Pareto stationary points in smooth multiobjective optimization with $m$ objectives. The progress metric is the Pareto stationarity gap $\mathcal{G}(x)$ (the norm of an optimal convex combination of gradients). Our contributions are fourfold. (i) For strongly convex objectives, any span first-order method (iterates lie in the span of past gradients) exhibits linear convergence no faster than $\exp(-\Theta(T/\sqrt{\kappa}))$ after $T$ oracle calls, where $\kappa$ is the condition number, implying $\Theta(\sqrt{\kappa}\log(1/\varepsilon))$ iterations; this matches classical accelerated upper bounds. (ii) For convex problems and oblivious one-step methods (a fixed scalarization with pre-scheduled step sizes), we prove a lower bound of order $1/T$ on the best gradient norm among the first $T$ iterates. (iii) Although accelerated gradient descent is outside this restricted class, it is an oblivious span method and attains the same $1/T$ upper rate on a fixed scalarization. (iv) For convex problems and general span methods with adaptive scalarizations, we establish a universal lower bound of order $1/T^{2}$ on the gradient norm of the final iterate after $T$ steps, highlighting a gap between known upper bounds and worst-case guarantees. All bounds hold on non-degenerate instances with distinct objectives and non-singleton Pareto fronts; rates are stated up to universal constants and natural problem scaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13550v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Phillipe R. Sampaio</dc:creator>
    </item>
    <item>
      <title>Accelerated Gradient Methods with Biased Gradient Estimates: Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds</title>
      <link>https://arxiv.org/abs/2509.13628</link>
      <description>arXiv:2509.13628v1 Announce Type: new 
Abstract: We study trade-offs between convergence rate and robustness to gradient errors in first-order methods. Our focus is on generalized momentum methods (GMMs), a class that includes Nesterov's accelerated gradient, heavy-ball, and gradient descent. We allow stochastic gradient errors that may be adversarial and biased, and quantify robustness via the risk-sensitive index (RSI) from robust control theory. For quadratic objectives with i.i.d. Gaussian noise, we give closed-form expressions for RSI using 2x2 Riccati equations, revealing a Pareto frontier between RSI and convergence rate over stepsize and momentum choices. We prove a large-deviation principle for time-averaged suboptimality and show that the rate function is, up to scaling, the convex conjugate of the RSI. We further connect RSI to the $H_{\infty}$-norm, showing that stronger worst-case robustness (smaller $H_{\infty}$ norm) yields sharper decay of tail probabilities. Beyond quadratics, under biased sub-Gaussian gradient errors, we derive non-asymptotic bounds on a finite-time analogue of the RSI, giving finite-time high-probability guarantees and large-deviation bounds. We also observe an analogous trade-off between RSI and convergence-rate bounds for smooth strongly convex functions. To our knowledge, these are the first non-asymptotic guarantees and risk-sensitive analysis of GMMs with biased gradients. Numerical experiments on robust regression illustrate the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13628v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mert G\"urb\"uzbalaban, Yasa Syed, Necdet Serhat Aybat</dc:creator>
    </item>
    <item>
      <title>On the Moreau envelope properties of weakly convex functions</title>
      <link>https://arxiv.org/abs/2509.13960</link>
      <description>arXiv:2509.13960v1 Announce Type: new 
Abstract: In this document, we present the main properties satisfied by the Moreau envelope of weakly convex functions. The Moreau envelope has been introduced in convex optimization to regularize convex functionals while preserving their global minimizers. However, the Moreau envelope is also defined for the more general class of weakly convex function and can be a useful tool for optimization in this context. The main properties of the Moreau envelope have been demonstrated for convex functions and are generalized to weakly convex function in various works. This document summarizes the vast literature on the properties of the Moreau envelope and provides the associated proofs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13960v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marien Renaud, Arthur Leclaire, Nicolas Papadakis</dc:creator>
    </item>
    <item>
      <title>On Tackling High-Dimensional Nonconvex Stochastic Optimization via Stochastic First-Order Methods with Non-smooth Proximal Terms and Variance Reduction</title>
      <link>https://arxiv.org/abs/2509.13992</link>
      <description>arXiv:2509.13992v1 Announce Type: new 
Abstract: When the nonconvex problem is complicated by stochasticity, the sample complexity of stochastic first-order methods may depend linearly on the problem dimension, which is undesirable for large-scale problems. To alleviate this linear dependence, we adopt non-Euclidean settings and propose two choices of nonsmooth proximal terms when taking the stochastic gradient steps. This approach leads to stronger convergence metric, incremental computational overhead, and potentially dimension-insensitive sample complexity. We also consider acceleration through variance reduction which achieves near optimal sample complexity and, to our knowledge, state-of-art in the non-Euclidean setting ($\ell_1/\ell_\infty$). Since the use of nonsmooth proximal terms is unconventional, the convergence analysis deviates much from approaches in Euclidean settings or employing Bregman divergence, providing tools for analyzing other non-Euclidean choices of distance functions. Efficient resolution of the subproblem in various scenarios are also discussed and simulated. We illustrate the dimension-insensitive property of the proposed methods via preliminary numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13992v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Xie, Jiawen Bi, Hongcheng Liu</dc:creator>
    </item>
    <item>
      <title>Multi-Source Neural Activity Indices and Spatial Filters for EEG/MEG Inverse Problem: An Extension to MNE-Python</title>
      <link>https://arxiv.org/abs/2509.14118</link>
      <description>arXiv:2509.14118v1 Announce Type: new 
Abstract: Accurate EEG/MEG source localization is essential for understanding brain function, yet remains challenging because the inverse problem is inherently ill-posed. In spatial filtering (beamforming) approaches, single-source LCMV spatial filters, though widely used, suffer from source cancellation when sources are correlated - a common experimental scenario. Multi-source frameworks, such as the multi-source minimum-variance pseudo-unbiased reduced-rank (MV-PURE) method, offer improved reconstruction and robust neural activity indices, yet their adoption has been limited by incomplete theory and lack of accessible implementations. In this paper, we present a rigorous derivation of multi-source neural activity indices and spatial filters, establishing a complete analytical framework with automated parameter selection. The resulting compact algebraic forms enable straightforward implementation. To facilitate adoption, we provide a full implementation extending MNE-Python, along with an accompanying tutorial, and demonstrate its utility on EEG experimental data, highlighting the practical advantages of multi-source spatial filtering for source localization and reconstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14118v1</guid>
      <category>math.OC</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julia Jurkowska, Joanna Dreszer, Monika Lewandowska, Krzysztof To{\l}pa, Tomasz Piotrowski</dc:creator>
    </item>
    <item>
      <title>HYCO: Hybrid-Cooperative Learning for Data-Driven PDE Modeling</title>
      <link>https://arxiv.org/abs/2509.14123</link>
      <description>arXiv:2509.14123v1 Announce Type: new 
Abstract: We present Hybrid-Cooperative Learning (HYCO), a hybrid modeling framework that iteratively integrates physics-based and data-driven models through a mutual regularization mechanism. Unlike traditional approaches that impose physical constraints directly on synthetic models, HYCO treats the physical and synthetic components as co-trained agents: the physical and synthetic models are nudged toward agreement, while the synthetic model is enhanced to better fit the available data. This cooperative learning scheme is naturally parallelizable and improves robustness to noise as well as to sparse or heterogeneous data. Extensive numerical experiments on both static and time-dependent problems demonstrate that HYCO outperforms classical physics-based and data-driven methods, recovering accurate solutions and model parameters even under ill-posed conditions. The method also admits a natural game-theoretic interpretation, enabling alternating optimization and paving the way for future theoretical developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14123v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Liverani, Matthys Steynberg, Enrique Zuazua</dc:creator>
    </item>
    <item>
      <title>The Aubin Property for Generalized Equations over $C^2$-cone Reducible Sets</title>
      <link>https://arxiv.org/abs/2509.14194</link>
      <description>arXiv:2509.14194v1 Announce Type: new 
Abstract: This paper establishes the equivalence of the Aubin property and the strong regularity for generalized equations over $C^2$-cone reducible sets. This result resolves a long-standing question in variational analysis and extends the well-known equivalence theorem for polyhedral sets to a significantly broader class of non-polyhedral cases. Our proof strategy departs from traditional variational techniques, integrating insights from convex geometry with powerful tools from algebraic topology. A cornerstone of our analysis is a new fundamental lemma concerning the local structure of the normal cone map for arbitrary closed convex sets, which reveals how the dimension of normal cones varies in the neighborhood of a boundary point. This geometric insight is the key to applying degree theory, allowing us to prove that a crucial function associated with the problem has a topological index of $\pm1$. This, via a homological version of the inverse mapping theorem, implies that the function is a local homeomorphism, which in turn yields the strong regularity of the original solution map. This result unifies and extends several existing stability results for problems such as conventional nonlinear programming, nonlinear second-order cone programming, and nonlinear semidefinite programming under a single general framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14194v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaming Ma, Defeng Sun</dc:creator>
    </item>
    <item>
      <title>Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain</title>
      <link>https://arxiv.org/abs/2509.14203</link>
      <description>arXiv:2509.14203v1 Announce Type: new 
Abstract: Learning and optimal control under robust Markov decision processes (MDPs) have received increasing attention, yet most existing theory, algorithms, and applications focus on finite-horizon or discounted models. The average-reward formulation, while natural in many operations research and management contexts, remains underexplored. This is primarily because the dynamic programming foundations are technically challenging and only partially understood, with several fundamental questions remaining open. This paper steps toward a general framework for average-reward robust MDPs by analyzing the constant-gain setting. We study the average-reward robust control problem with possible information asymmetries between the controller and an S-rectangular adversary. Our analysis centers on the constant-gain robust Bellman equation, examining both the existence of solutions and their relationship to the optimal average reward. Specifically, we identify when solutions to the robust Bellman equation characterize the optimal average reward and stationary policies, and we provide sufficient conditions ensuring solutions' existence. These findings expand the dynamic programming theory for average-reward robust MDPs and lay a foundation for robust dynamic decision making under long-run average criteria in operational environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14203v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengbo Wang, Nian Si</dc:creator>
    </item>
    <item>
      <title>The column number for 3-modular matrices</title>
      <link>https://arxiv.org/abs/2509.13463</link>
      <description>arXiv:2509.13463v1 Announce Type: cross 
Abstract: An integer-valued matrix $\mathbf{A}$ is $\Delta$-modular if each $\text{rank}(\mathbf{A}) \times \text{rank}(\mathbf{A})$ submatrix has determinant at most $\Delta$ in absolute value. The column number problem is to determine the maximum number of pairwise non-parallel columns of a rank-$r$, $\Delta$-modular matrix. Exact values for the column number are only known for $r \le 2$ or $\Delta \le 2$. We prove that if $r$ is sufficiently large, then the maximum number of pairwise non-parallel columns of a rank-$r$, $3$-modular matrix is $\binom{r+1}{2} + 2(r-1)$. This settles a conjecture by Lee, Paat, Stallknecht, and Xu on the column number in the case $\Delta = 3$. We complement this main result by showing that there are at least three $3$-modular matrices with pairwise non-isomorphic vector matroids that attain this upper bound. More generally, we show that if $r &gt; \Delta$, then the number of $\Delta$-modular matrices with $\binom{r+1}{2} + (\Delta-1)(r-1)$ pairwise non-parallel columns and pairwise non-isomorphic vector matroids is at least exponential in $\sqrt{\Delta}$; previously only one matrix was known due to Lee et al.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13463v1</guid>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Paat, Zach Walsh, Luze Xu</dc:creator>
    </item>
    <item>
      <title>Identifying Network Structure of Nonlinear Dynamical Systems: Contraction and Kuramoto Oscillators</title>
      <link>https://arxiv.org/abs/2509.13505</link>
      <description>arXiv:2509.13505v1 Announce Type: cross 
Abstract: In this work, we study the identifiability of network topologies for networked nonlinear systems when partial measurements of the nodes are taken. We explore scenarios where different candidate topologies can yield similar measurements, thus limiting identifiability. To do so, we apply the contraction theory framework to facilitate comparisons between candidate topologies. We show that semicontraction in the observable space is a sufficient condition for two systems to become indistinguishable from one another based on partial measurements. We apply this framework to study networks of Kuramoto oscillators, and discuss scenarios in which different topologies (both connected and disconnected) become indistinguishable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13505v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaidev Gill, Jing Shuang Li</dc:creator>
    </item>
    <item>
      <title>The impact of modeling approaches on controlling safety-critical, highly perturbed systems: the case for data-driven models</title>
      <link>https://arxiv.org/abs/2509.13531</link>
      <description>arXiv:2509.13531v1 Announce Type: cross 
Abstract: This paper evaluates the impact of three system models on the reference trajectory tracking error of the LQR optimal controller, in the challenging problem of guidance and control of the state of a system under strong perturbations and reconfiguration. We compared a smooth Linear Time Variant system learned from data (DD-LTV) with state of the art Linear Time Variant (LTV) system identification methods, showing its superiority in the task of state propagation. Moreover, we have found that DD-LTV allows for better performance in terms of trajectory tracking error than the standard solutions of a Linear Time Invariant (LTI) system model, and comparable performance to a linearized Linear Time Variant (L-LTV) system model. We tested the three approaches on the perturbed and time varying spring-mass-damper systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13531v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piotr {\L}aszkiewicz, Maria Carvalho, Cl\'audia Soares, Pedro Louren\c{c}o</dc:creator>
    </item>
    <item>
      <title>Circuit realization and hardware linearization of monotone operator equilibrium networks</title>
      <link>https://arxiv.org/abs/2509.13793</link>
      <description>arXiv:2509.13793v1 Announce Type: cross 
Abstract: It is shown that the port behavior of a resistor-diode network corresponds to the solution of a ReLU monotone operator equilibrium network (a neural network in the limit of infinite depth), giving a parsimonious construction of a neural network in analog hardware. We furthermore show that the gradient of such a circuit can be computed directly in hardware, using a procedure we call hardware linearization. This allows the network to be trained in hardware, which we demonstrate with a device-level circuit simulation. We extend the results to cascades of resistor-diode networks, which can be used to implement feedforward and other asymmetric networks. We finally show that different nonlinear elements give rise to different activation functions, and introduce the novel diode ReLU which is induced by a non-ideal diode model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13793v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Thomas Chaffey</dc:creator>
    </item>
    <item>
      <title>On the Rate of Gaussian Approximation for Linear Regression Problems</title>
      <link>https://arxiv.org/abs/2509.14039</link>
      <description>arXiv:2509.14039v1 Announce Type: cross 
Abstract: In this paper, we consider the problem of Gaussian approximation for the online linear regression task. We derive the corresponding rates for the setting of a constant learning rate and study the explicit dependence of the convergence rate upon the problem dimension $d$ and quantities related to the design matrix. When the number of iterations $n$ is known in advance, our results yield the rate of normal approximation of order $\sqrt{\log{n}/n}$, provided that the sample size $n$ is large enough.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14039v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marat Khusainov, Marina Sheshukova, Alain Durmus, Sergey Samsonov</dc:creator>
    </item>
    <item>
      <title>Identifying Network Structure of Linear Dynamical Systems: Observability and Edge Misclassification</title>
      <link>https://arxiv.org/abs/2509.14065</link>
      <description>arXiv:2509.14065v1 Announce Type: cross 
Abstract: This work studies the limitations of uniquely identifying a linear network's topology from partial measurements of its nodes. We show that the set of networks that are consistent with the measurements are related through the nullspace of the observability matrix for the true network. In doing so, we illustrate how potentially many networks are fully consistent with the measurements despite having topologies that are structurally inconsistent with each other, an often neglected consideration in the design of topology inference methods. We then provide an aggregate characterization of the space of possible networks by analytically solving for the most structurally dissimilar network. We find that when observing over 6% of nodes in random network models (e.g., Erd\H{o}s-R\'{e}nyi and Watts-Strogatz) the rate of edge misclassification drops to ~1%. Extending this discussion, we construct a family of networks that keep measurements $\epsilon$-"close" to each other, and connect the identifiability of these networks to the spectral properties of an augmented observability Gramian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14065v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaidev Gill, Jing Shuang Li</dc:creator>
    </item>
    <item>
      <title>A Compositional Kernel Model for Feature Learning</title>
      <link>https://arxiv.org/abs/2509.14158</link>
      <description>arXiv:2509.14158v1 Announce Type: cross 
Abstract: We study a compositional variant of kernel ridge regression in which the predictor is applied to a coordinate-wise reweighting of the inputs. Formulated as a variational problem, this model provides a simple testbed for feature learning in compositional architectures. From the perspective of variable selection, we show how relevant variables are recovered while noise variables are eliminated. We establish guarantees showing that both global minimizers and stationary points discard noise coordinates when the noise variables are Gaussian distributed. A central finding is that $\ell_1$-type kernels, such as the Laplace kernel, succeed in recovering features contributing to nonlinear effects at stationary points, whereas Gaussian kernels recover only linear ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14158v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feng Ruan, Keli Liu, Michael Jordan</dc:creator>
    </item>
    <item>
      <title>A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning</title>
      <link>https://arxiv.org/abs/2509.14198</link>
      <description>arXiv:2509.14198v1 Announce Type: cross 
Abstract: Residual-based adaptive strategies are widely used in scientific machine learning but remain largely heuristic. We introduce a unifying variational framework that formalizes these methods by integrating convex transformations of the residual. Different transformations correspond to distinct objective functionals: exponential weights target the minimization of uniform error, while linear weights recover the minimization of quadratic error. Within this perspective, adaptive weighting is equivalent to selecting sampling distributions that optimize the primal objective, thereby linking discretization choices directly to error metrics. This principled approach yields three benefits: (1) it enables systematic design of adaptive schemes across norms, (2) reduces discretization error through variance reduction of the loss estimator, and (3) enhances learning dynamics by improving the gradient signal-to-noise ratio. Extending the framework to operator learning, we demonstrate substantial performance gains across optimizers and architectures. Our results provide a theoretical justification of residual-based adaptivity and establish a foundation for principled discretization and training strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14198v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Juan Diego Toscano, Daniel T. Chen, Vivek Oommen, George Em Karniadakis</dc:creator>
    </item>
    <item>
      <title>Concave tents: a new tool for constructing concave reformulations of a large class of nonconvex optimization problems</title>
      <link>https://arxiv.org/abs/2409.18451</link>
      <description>arXiv:2409.18451v2 Announce Type: replace 
Abstract: Optimizing a nonlinear function over nonconvex sets is challenging since solving convex relaxations may lead to substantial relaxation gaps and infeasible solutions that must be "rounded" to feasible ones, often with uncontrollable losses in objective function performance. For this reason, these convex hulls are especially useful if the objective function is linear or even concave, since concave optimization is invariant to taking the convex hull of the feasible set. We propose the notion of concave tents, which are concave approximations of the original objective function that agree with this objective function on the feasible set, and allow for concave reformulations of the problem. Concave tents, therefore, are special cases of concave extensions. In this text, we derive such concave tents for a large class of objective functions as the optimal value functions of conic optimization problems. Hence, evaluating our concave tents requires solving a conic problem. We can find supergradients by solving the conic dual problem, so that differentiation is of the same complexity as evaluation. For feasible sets that are contained in the extreme points of their convex hull, we construct these concave tents in the original space of variables. For general feasible sets, we propose a double lifting strategy, where the original optimization problem is lifted into a higher-dimensional space in which the concave tent can be constructed easily. We investigate the relation of the so-constructed concave tents to concave envelopes and naive concave tents based on concave quadratic updates. Based on these ideas, we propose a primal heuristic for a class of robust discrete quadratic optimization problems that can be used instead of classical rounding techniques. Numerical experiments suggest that our techniques can be beneficial as an upper-bounding procedure in a branch-and-bound solution scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18451v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Gabl</dc:creator>
    </item>
    <item>
      <title>Tensor train solution to uncertain optimization problems with shared sparsity penalty</title>
      <link>https://arxiv.org/abs/2411.03989</link>
      <description>arXiv:2411.03989v2 Announce Type: replace 
Abstract: We develop both first and second order numerical optimization methods to solve non-smooth optimization problems featuring a shared sparsity penalty, constrained by differential equations with uncertainty. To alleviate the curse of dimensionality we use tensor product approximations. To handle the non-smoothness of the objective function we employ a smoothed version of the shared sparsity objective. We consider both a benchmark elliptic PDE constraint, and a more realistic topology optimization problem in engineering. We demonstrate that the error converges linearly in iterations and the smoothing parameter, and faster than algebraically in the number of degrees of freedom, consisting of the number of quadrature points in one variable and tensor ranks. Moreover, in the topology optimization problem, the smoothed shared sparsity penalty actually reduces the tensor ranks compared to the unpenalised solution. This enables us to find a sparse high-resolution design under a high-dimensional uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03989v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harbir Antil, Sergey Dolgov, Akwum Onwunta</dc:creator>
    </item>
    <item>
      <title>The Small-Gain Condition for Infinite Networks Modeled on $\ell^{\infty}$-Spaces</title>
      <link>https://arxiv.org/abs/2503.03925</link>
      <description>arXiv:2503.03925v4 Announce Type: replace 
Abstract: In recent years, attempts have been made to extend nonlinear small-gain theorems for input-to-state stability (ISS) from finite networks to countably infinite networks with finite indegrees. Under specific assumptions about the interconnection gains and the ISS formulation, corresponding infinite-dimensional small-gain results have been proven. However, concerning these assumptions, the results are still too narrow to be considered a full extension of the state-of-the-art for finite networks. We take a step to closing this gap by developing a general technical framework within which the small-gain condition for both finite and infinite networks can be analyzed. This includes a thorough investigation of various monotone operators associated with a network and a specific ISS formulation. Our results extend and generalize the existing theory for finite networks, yield complete characterizations of the small-gain condition for specific ISS formulations, and show which obstacles still have to be overcome to obtain a complete theory for the most general infinite case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03925v4</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoph Kawan</dc:creator>
    </item>
    <item>
      <title>Data-Driven Distributed Optimization via Aggregative Tracking and Deep-Learning</title>
      <link>https://arxiv.org/abs/2503.04668</link>
      <description>arXiv:2503.04668v2 Announce Type: replace 
Abstract: In this paper, we propose a novel distributed data-driven optimization scheme. In detail, we focus on the so-called aggregative framework, a scenario in which a set of agents aim to cooperatively minimize the sum of local costs, each depending on both local decision variables and an aggregation of all of them. We consider a data-driven setup where each objective function is unknown and can be sampled at a single point per iteration (thanks to, e.g., feedback from users or sensors). We address this scenario through a distributed algorithm combining three components: (i) a learning part leveraging neural networks to learn the local costs descent direction, (ii) an optimization routine steering the estimates according to the learned direction to minimize the global cost, and (iii) a tracking mechanism locally reconstructing the unavailable global quantities. Using tools from system theory, i.e., timescale separation and averaging theory, we formally prove that in strongly convex setups, the distributed scheme linearly converges to a neighborhood of the optimum, whose radius depends on the accuracy of the neural networks. Finally, numerical simulations validate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04668v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Brumali, Guido Carnevale, Giuseppe Notarstefano</dc:creator>
    </item>
    <item>
      <title>Welfare and Cost Aggregation for Multi-Agent Control: When to Choose Which Social Cost Function, and Why?</title>
      <link>https://arxiv.org/abs/2503.20772</link>
      <description>arXiv:2503.20772v2 Announce Type: replace 
Abstract: Many multi-agent socio-technical systems rely on aggregating heterogeneous agents' costs into a social cost function (SCF) to coordinate resource allocation in domains like energy grids, water allocation, or traffic management. The choice of SCF often entails implicit assumptions and may lead to undesirable outcomes if not rigorously justified. In this paper, we demonstrate that what determines which SCF ought to be used is the degree to which individual costs can be compared across agents and which axioms the aggregation shall fulfill. Drawing on the results from social choice theory, we provide guidance on how this process can be used in control applications. We demonstrate which assumptions about interpersonal utility comparability - ranging from ordinal level comparability to full cardinal comparability - together with a choice of desirable axioms, inform the selection of a correct SCF, be it the classical utilitarian sum, the Nash SCF, or maximin. We then demonstrate how the proposed framework can be applied for principled allocations of water and transportation resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20772v2</guid>
      <category>math.OC</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilia Shilov, Ezzat Elokda, Sophie Hall, Heinrich H. Nax, Saverio Bolognani</dc:creator>
    </item>
    <item>
      <title>A primal-dual price-optimization method for computing equilibrium prices in mean-field games models</title>
      <link>https://arxiv.org/abs/2506.04169</link>
      <description>arXiv:2506.04169v2 Announce Type: replace 
Abstract: We develop a simple yet efficient Lagrangian method for computing equilibrium prices in a mean-field game price-formation model. We prove that equilibrium prices are optimal in terms of a suitable criterion and derive a primal-dual gradient-based algorithm for computing them. One of the highlights of our computational framework is the efficient, simple, and flexible implementation of the algorithm using modern automatic differentiation techniques. Our implementation is modular and admits a seamless extension to high-dimensional settings with more complex dynamics, costs, and equilibrium conditions. Additionally, automatic differentiation enables a versatile algorithm that requires only coding the cost functions of agents. It automatically handles the gradients of the costs, thereby eliminating the need to manually form the adjoint equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04169v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>econ.TH</category>
      <category>math.NA</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xu Wang, Samy Wu Fung, Levon Nurbekyan</dc:creator>
    </item>
    <item>
      <title>Feedback Optimization of Dynamical Systems in Time-Varying Environments: An Internal Model Principle Approach</title>
      <link>https://arxiv.org/abs/2508.03503</link>
      <description>arXiv:2508.03503v2 Announce Type: replace 
Abstract: Feedback optimization has emerged as a promising approach for regulating dynamical systems to optimal steady states that are implicitly defined by underlying optimization problems. Despite their effectiveness, existing methods face two key limitations: (i) reliable performance is restricted to time-invariant or slowly varying settings, and (ii) convergence rates are limited by the need for the controller to operate orders of magnitude slower than the plant. These limitations can be traced back to the reliance of existing techniques on numerical optimization algorithms. In this paper, we propose a novel perspective on the design of feedback optimization algorithms, by framing these objectives as an output regulation problem. We place particular emphasis on time-varying optimization problems, and show that an algorithm can track time-varying optimizers if and only if it incorporates a model of the temporal variability inherent to the optimization - a requirement that we term the internal model principle of feedback optimization. Building on this insight, we introduce a new design methodology that couples output-feedback stabilization with a control component that drives the system toward the critical points of the optimization problem. This framework enables feedback optimization algorithms to overcome the classical limitations of slow tracking and poor adaptability to time variations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03503v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gianluca Bianchin, Bryan Van Scoy</dc:creator>
    </item>
    <item>
      <title>Linear Convergence of Gradient Descent for Quadratically Regularized Optimal Transport</title>
      <link>https://arxiv.org/abs/2509.08547</link>
      <description>arXiv:2509.08547v2 Announce Type: replace 
Abstract: In optimal transport, quadratic regularization is an alternative to entropic regularization when sparse couplings or small regularization parameters are desired. Here quadratic regularization means that transport couplings are penalized by the squared $L^2$ norm, or equivalently the $\chi^2$ divergence. While a number of computational approaches have been shown to work in practice, quadratic regularization is analytically less tractable than entropic, and we are not aware of a previous theoretical convergence rate analysis. We focus on the gradient descent algorithm for the dual transport problem in continuous and semi-discrete settings. This problem is convex but not strongly convex; its solutions are the potential functions that approximate the Kantorovich potentials of unregularized optimal transport. The gradient descent steps are straightforward to implement, and stable for small regularization parameter -- in contrast to Sinkhorn's algorithm in the entropic setting. Our main result is that gradient descent converges linearly; that is, the $L^2$ distance between the iterates and the limiting potentials decreases exponentially fast. Our analysis centers on the linearization of the gradient descent operator at the optimum and uses functional-analytic arguments to bound its spectrum. These techniques seem to be novel in this area and are substantially different from the approaches familiar in entropic optimal transport.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08547v2</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alberto Gonz\'alez-Sanz, Marcel Nutz, Andr\'es Riveros Valdevenito</dc:creator>
    </item>
    <item>
      <title>JD.com Improves Fulfillment Efficiency with Data-driven Integrated Assortment Planning and Inventory Allocation</title>
      <link>https://arxiv.org/abs/2509.12183</link>
      <description>arXiv:2509.12183v2 Announce Type: replace 
Abstract: This paper presents data-driven approaches for integrated assortment planning and inventory allocation that significantly improve fulfillment efficiency at JD\,.com, a leading E-commerce company. JD\,.com uses a two-level distribution network that includes regional distribution centers (RDCs) and front distribution centers (FDCs). Selecting products to stock at FDCs and then optimizing daily inventory allocation from RDCs to FDCs is critical to improving fulfillment efficiency, which is crucial for enhancing customer experiences. For assortment planning, we propose efficient algorithms to maximize the number of orders that can be fulfilled by FDCs (local fulfillment). For inventory allocation, we develop a novel end-to-end algorithm that integrates forecasting, optimization, and simulation to minimize lost sales and inventory transfer costs. Numerical experiments demonstrate that our methods outperform existing approaches, increasing local order fulfillment rates by 0.54% and our inventory allocation algorithm increases FDC demand satisfaction rates by 1.05%. Considering the high-volume operations of JD\,.com, with millions of weekly orders per region, these improvements yield substantial benefits beyond the company's established supply chain system. Implementation across JD\,.com's network has reduced costs, improved stock availability, and increased local order fulfillment rates for millions of orders annually.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12183v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zuo-Jun Max Shen, Shuo Sun, Yongzhi Qi, Hao Hu, Ningxuan Kang, Jianshen Zhang, Xin Wang, Xiaoming Lin</dc:creator>
    </item>
    <item>
      <title>Rich Vehicle Routing Problem in Disaster Management enabling Temporally-causal Transhipments across Multi-Modal Transportation Network</title>
      <link>https://arxiv.org/abs/2509.13227</link>
      <description>arXiv:2509.13227v2 Announce Type: replace 
Abstract: A rich vehicle routing problem is considered, allowing multiple trips of heterogeneous vehicles stationed at geographically distributed vehicle depots having access to different modes of transportation. The problem arises from the real-world requirement of optimizing the disaster response time by minimizing the makespan of vehicular routes. Multiple diversely-functional vertices are considered, including Transhipment Ports as inter-modal resource transfer stations. Both simultaneous and split pickup and delivery are considered, for multiple cargo types, along with Vehicle-Cargo and Transhipment Port-Cargo compatibilities. The superiority of the proposed cascaded minimization approach is demonstrated over the existing makespan minimization approaches through our developed Mixed-Integer Linear Programming formulation. To solve the problem quickly for practical implementation in a Disaster Management-specific Decision Support System, an extensive Heuristic Algorithm is devised which utilizes Decision Tree based structuring of possible routes; the Decision Tree approach helps to inherently capture the compatibility issues, while also explore the solution space through stochastic weights. Preferential generation of small route elements is performed, which are integrated into route clusters; we consider multiple different logical integration approaches, as well as shuffling the logics to simultaneously produce multiple independent solutions. Finally, perturbations of the different solutions are done to find better neighbouring solutions. The computational performance of the PSR-GIP Heuristic, on our created novel datasets, indicates that it is able to give good solutions swiftly for practical problems involving large integer instances that the MILP is unable to solve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13227v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santanu Banerjee, Goutam Sen, Siddhartha Mukhopadhyay</dc:creator>
    </item>
    <item>
      <title>Blind Network Revenue Management and Bandits with Knapsacks under Limited Switches</title>
      <link>https://arxiv.org/abs/1911.01067</link>
      <description>arXiv:1911.01067v5 Announce Type: replace-cross 
Abstract: This paper studies the impact of limited switches on resource-constrained dynamic pricing with demand learning. We focus on the classical price-based blind network revenue management problem and extend our results to the bandits with knapsacks problem. In both settings, a decision maker faces stochastic and distributionally unknown demand, and must allocate finite initial inventory across multiple resources over time. In addition to standard resource constraints, we impose a switching constraint that limits the number of action changes over the time horizon. We establish matching upper and lower bounds on the optimal regret and develop computationally efficient limited-switch algorithms that achieve it. We show that the optimal regret rate is fully characterized by a piecewise-constant function of the switching budget, which further depends on the number of resource constraints. Our results highlight the fundamental role of resource constraints in shaping the statistical complexity of online learning under limited switches. Extensive simulations demonstrate that our algorithms maintain strong cumulative reward performance while significantly reducing the number of switches.</description>
      <guid isPermaLink="false">oai:arXiv.org:1911.01067v5</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Simchi-Levi, Yunzong Xu, Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>Optimality Conditions for Multivariate Chebyshev Approximation: A Survey</title>
      <link>https://arxiv.org/abs/2310.01851</link>
      <description>arXiv:2310.01851v5 Announce Type: replace-cross 
Abstract: Uniform polynomial approximation, also called minimax approximation or Chebyshev approximation, consists in searching polynomial approximation that minimizes the worst case error. Optimality conditions for the uniform approximation of univariate functions defined in an interval are governed by the equioscillation theorem, which is also a key ingredient in algorithms for computing best uniform approximation, like Remez's algorithm and the two phase method. Multivariate polynomial approximation is more complicated, and several optimality conditions for uniform multivariate polynomial approximation generalize the equioscillation theorem. We review these conditions, including, from oldest to newest, Kirchberger's kernel condition, Kolmogorov criteria, Rivlin and Shapiro's annihilating measures. An emphasis is given to conditions for strong optimality, which has some strong theoretical and practical importance, including Bartelt's and Smarzewsky's conditions. Optimality conditions related to more general relative Chebyshev centers are also presented, including Tanimoto's and Levis et al.'s conditions. In a second step, conditions obtained by standard convex analysis, subdifferential and directional derivative, applied to uniform approximation are formulated. Their relationship to previous conditions is investigated, providing sometimes enlightening interpretations of the laters, e.g., relating Kolmogorov criterion with directional derivative, and strong uniqueness with sharp minimizers. Finally, numerical applications of the two-step approach to three uniform approximation problems are presented, namely the approximation of the multidimensional Runge function, the approximation of the two dimensional inverse model of the DexTAR parallel robot, and the approximation problem consisting in minimizing the sum of both the polynomial approximation error and the polynomial evaluation error in Horner form.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01851v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Goldsztejn (LS2N, LS2N - \'equipe OGRE)</dc:creator>
    </item>
    <item>
      <title>An Optimal Algorithm for the Stacker Crane Problem on Fixed Topologies</title>
      <link>https://arxiv.org/abs/2410.06764</link>
      <description>arXiv:2410.06764v2 Announce Type: replace-cross 
Abstract: The Stacker Crane Problem (SCP) is a variant of the Traveling Salesman Problem. In SCP, pairs of pickup and delivery points are designated on a graph, and a crane must visit these points to move objects from each pickup location to its respective delivery point. The goal is to minimize the total distance traveled. SCP is known to be NP-hard, even on trees. The only positive results, in terms of polynomial-time solvability, apply to graphs that are topologically equivalent to a path or a cycle.
  We propose an algorithm that is optimal for each fixed topology, running in near-linear time. This is achieved by demonstrating that the problem is fixed-parameter tractable (FPT) when parameterized by both the cycle rank and the number of branch vertices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06764v2</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yike Chen, Ke Shi, Chao Xu</dc:creator>
    </item>
    <item>
      <title>Duality for Evolutionary Equations with Applications to Null Controllability</title>
      <link>https://arxiv.org/abs/2411.14239</link>
      <description>arXiv:2411.14239v2 Announce Type: replace-cross 
Abstract: We study evolutionary equations in exponentially weighted $\mathrm{L}^{2}$-spaces as introduced by Picard in 2009. First, for a given evolutionary equation, we explicitly describe the $\nu$-adjoint system, which turns out to describe a system backwards in time. We prove well-posedness for the $\nu$-adjoint system. We then apply the thus obtained duality to introduce and study notions of null-controllability for evolutionary equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14239v2</guid>
      <category>math.AP</category>
      <category>math.FA</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Buchinger, Christian Seifert</dc:creator>
    </item>
    <item>
      <title>Global Search for Optimal Low Thrust Spacecraft Trajectories using Diffusion Models and the Indirect Method</title>
      <link>https://arxiv.org/abs/2501.07005</link>
      <description>arXiv:2501.07005v2 Announce Type: replace-cross 
Abstract: Long time-duration low-thrust nonlinear optimal spacecraft trajectory global search is a computationally and time expensive problem characterized by clustering patterns in locally optimal solutions. During preliminary mission design, mission parameters are subject to frequent changes, necessitating that trajectory designers efficiently generate high-quality control solutions for these new scenarios. Generative machine learning models can be trained to learn how the solution structure varies with respect to a conditional parameter, thereby accelerating the global search for missions with updated parameters. In this work, state-of-the-art diffusion models are integrated with the indirect approach for trajectory optimization within a global search framework. This framework is tested on two low-thrust transfers of different complexity in the circular restricted three-body problem. By generating and analyzing a training data set, we develop mathematical relations and techniques to understand the complex structures in the costate domain of locally optimal solutions for these problems. A diffusion model is trained on this data and successfully accelerates the global search for both problems. The model predicts how the costate solution structure changes, based on the maximum spacecraft thrust magnitude. Warm-starting a numerical solver with diffusion model samples for the costates at the initial time increases the number of solutions generated per minute for problems with unseen thrust magnitudes by one to two orders of magnitude in comparison to samples from a uniform distribution and from an adjoint control transformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07005v2</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jannik Graebner, Ryne Beeson</dc:creator>
    </item>
    <item>
      <title>Learning AC Power Flow Solutions using a Data-Dependent Variational Quantum Circuit</title>
      <link>https://arxiv.org/abs/2509.03495</link>
      <description>arXiv:2509.03495v2 Announce Type: replace-cross 
Abstract: Interconnection studies require solving numerous instances of the AC load or power flow (AC PF) problem to simulate diverse scenarios as power systems navigate the ongoing energy transition. To expedite such studies, this work leverages recent advances in quantum computing to find or predict AC PF solutions using a variational quantum circuit (VQC). VQCs are trainable models that run on modern-day noisy intermediate-scale quantum (NISQ) hardware to accomplish elaborate optimization and machine learning (ML) tasks. Our first contribution is to pose a single instance of the AC PF as a nonlinear least-squares fit over the VQC trainable parameters (weights) and solve it using a hybrid classical/quantum computing approach. The second contribution is to feed PF specifications as features into a data-embedded VQC and train the resultant quantum ML (QML) model to predict general PF solutions. The third contribution is to develop a novel protocol to efficiently measure AC-PF quantum observables by exploiting the graph structure of a power network. Preliminary numerical tests indicate that the proposed VQC models attain enhanced prediction performance over a deep neural network despite using much fewer weights. The proposed quantum AC-PF framework sets the foundations for addressing more elaborate grid tasks via quantum computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03495v2</guid>
      <category>quant-ph</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thinh Viet Le, Md Obaidur Rahman, Vassilis Kekatos</dc:creator>
    </item>
    <item>
      <title>Privacy-Preserving Uncertainty Disclosure for Facilitating Enhanced Energy Storage Dispatch</title>
      <link>https://arxiv.org/abs/2509.11022</link>
      <description>arXiv:2509.11022v2 Announce Type: replace-cross 
Abstract: This paper proposes a novel privacy-preserving uncertainty disclosure framework, enabling system operators to release marginal value function bounds to reduce the conservativeness of interval forecast and mitigate excessive withholding, thereby enhancing storage dispatch and social welfare. We develop a risk-averse storage arbitrage model based on stochastic dynamic programming, explicitly accounting for uncertainty intervals in value function training. Real-time marginal value function bounds are derived using a rolling-horizon chance-constrained economic dispatch formulation. We rigorously prove that the bounds reliably cap the true opportunity cost and dynamically converge to the hindsight value. We verify that both the marginal value function and its bounds monotonically decrease with the state of charge (SoC) and increase with uncertainty, providing a theoretical basis for risk-averse strategic behaviors and SoC-dependent designs. An adjusted storage dispatch algorithm is further designed using these bounds. We validate the effectiveness of the proposed framework via an agent-based simulation on the ISO-NE test system. Under 50% renewable capacity and 35% storage capacity, the proposed bounds enhance storage response by 38.91% and reduce the optimality gap to 3.91% through improved interval predictions. Additionally, by mitigating excessive withholding, the bounds yield an average system cost reduction of 0.23% and an average storage profit increase of 13.22%. These benefits further scale with higher prediction conservativeness, storage capacity, and system uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11022v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ning Qi, Xiaolong Jin, Kai Hou, Zeyu Liu, Hongjie Jia, Wei Wei</dc:creator>
    </item>
  </channel>
</rss>
