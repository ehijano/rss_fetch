<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Jun 2025 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Analysis and conditional optimization of projection estimates for the distribution of random variable using Legendre polynomials</title>
      <link>https://arxiv.org/abs/2506.14822</link>
      <description>arXiv:2506.14822v1 Announce Type: new 
Abstract: Algorithms for jointly obtaining projection estimates of the density and distribution function of a random variable using the Legendre polynomials are proposed. For these algorithms, a problem of the conditional optimization is solved. Such an optimization allows one increasing the approximation accuracy with a minimum computational costs. The proposed algorithms are tested on examples with different degree of smoothness of the density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14822v1</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatyana A. Averina, Konstantin A. Rybakov</dc:creator>
    </item>
    <item>
      <title>Dynamic guessing for Hamiltonian Monte Carlo with embedded numerical root-finding</title>
      <link>https://arxiv.org/abs/2506.15423</link>
      <description>arXiv:2506.15423v1 Announce Type: new 
Abstract: Modern implementations of Hamiltonian Monte Carlo and related MCMC algorithms support sampling of probability functions that embed numerical root-finding algorithms, thereby allowing fitting of statistical models involving analytically intractable algebraic constraints. However the application of these models in practice is limited by the computational cost of computing large numbers of numerical solutions. We identify a key limitation of previous approaches to HMC with embedded root-finding, which require the starting guess to be the same at all points on the same simulated Hamiltonian trajectory. We demonstrate that this requirement can be relaxed, so that the starting guess depends on the previous integrator state. To choose a good guess using this information we propose two heuristics: use the previous solution and extrapolate the previous solution using implicit differentiation. Both heuristics yield substantial performance improvements on a range of representative models compared with static guessing. We also present grapevine, a JAX-based Python package providing easy access to an implementation of the No-U-Turn sampler augmented with dynamic guessing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15423v1</guid>
      <category>stat.CO</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teddy Groves, Nicholas Luke Cowie, Lars Keld Nielsen</dc:creator>
    </item>
    <item>
      <title>Faster Computation of Entropic Optimal Transport via Stable Low Frequency Modes</title>
      <link>https://arxiv.org/abs/2506.14780</link>
      <description>arXiv:2506.14780v1 Announce Type: cross 
Abstract: In this paper, we propose an accelerated version for the Sinkhorn algorithm, which is the reference method for computing the solution to Entropic Optimal Transport.
  Its main draw-back is the exponential slow-down of convergence as the regularization weakens $\varepsilon \rightarrow 0$.
  Thanks to spectral insights on the behavior of the Hessian, we propose to mitigate the problem via an original spectral warm-start strategy. This leads to faster convergence compared to the reference method, as also demonstrated in our numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14780v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reda Chhaibi, Serge Gratton, Samuel Vaiter</dc:creator>
    </item>
    <item>
      <title>GLASD: A Loss-Function-Agnostic Global Optimizer for Robust Correlation Estimation under Data Contamination and Heavy Tails</title>
      <link>https://arxiv.org/abs/2506.14801</link>
      <description>arXiv:2506.14801v1 Announce Type: cross 
Abstract: Robust correlation estimation is essential in high-dimensional settings, particularly when data are contaminated by outliers or exhibit heavy-tailed behavior. Many robust loss functions of practical interest-such as those involving truncation or redescending M-estimators-lead to objective functions that are inherently non-convex and non-differentiable. Traditional methods typically focus on a single loss function tailored to a specific contamination model and develop custom algorithms tightly coupled with that loss, limiting generality and adaptability. We introduce GLASD (Global Adaptive Stochastic Descent), a general-purpose black-box optimization algorithm designed to operate over the manifold of positive definite correlation matrices. Unlike conventional solvers, GLASD requires no gradient information and imposes no assumptions of convexity or smoothness, making it ideally suited for optimizing a wide class of loss functions-including non-convex, non-differentiable, or discontinuous objectives. This flexibility allows GLASD to serve as a unified framework for robust estimation under arbitrary user-defined criteria. We demonstrate its effectiveness through extensive simulations involving contaminated and heavy-tailed distributions, as well as a real-data application to breast cancer proteomic network inference, where GLASD successfully identifies biologically plausible interactions despite the presence of outliers. The proposed method is scalable, constraint-aware, and available as open-source software at GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14801v1</guid>
      <category>stat.AP</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Priyam Das</dc:creator>
    </item>
    <item>
      <title>Simulating Diffusion Bridges with Score Matching</title>
      <link>https://arxiv.org/abs/2111.07243</link>
      <description>arXiv:2111.07243v3 Announce Type: replace 
Abstract: We consider the problem of simulating diffusion bridges, which are diffusion processes that are conditioned to initialize and terminate at two given states. The simulation of diffusion bridges has applications in diverse scientific fields and plays a crucial role in the statistical inference of discretely-observed diffusions. This is known to be a challenging problem that has received much attention in the last two decades. This article contributes to this rich body of literature by presenting a new avenue to obtain diffusion bridge approximations. Our approach is based on a backward time representation of a diffusion bridge, which may be simulated if one can time-reverse the unconditioned diffusion. We introduce a variational formulation to learn this time-reversal with function approximation and rely on a score matching method to circumvent intractability. Another iteration of our proposed methodology approximates the Doob's $h$-transform defining the forward time representation of a diffusion bridge. We discuss algorithmic considerations and extensions, and present numerical results on an Ornstein--Uhlenbeck process, a model from financial econometrics for interest rates, and a model from genetics for cell differentiation and development to illustrate the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.07243v3</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Heng, Valentin De Bortoli, Arnaud Doucet, James Thornton</dc:creator>
    </item>
    <item>
      <title>Sublinear Algorithms for Wasserstein and Total Variation Distances: Applications to Fairness and Privacy Auditing</title>
      <link>https://arxiv.org/abs/2503.07775</link>
      <description>arXiv:2503.07775v2 Announce Type: replace-cross 
Abstract: Resource-efficiently computing representations of probability distributions and the distances between them while only having access to the samples is a fundamental and useful problem across mathematical sciences. In this paper, we propose a generic framework to learn the probability and cumulative distribution functions (PDFs and CDFs) of a sub-Weibull, i.e. almost any light- or heavy-tailed, distribution while the samples from it arrive in a stream. The idea is to reduce these problems into estimating the frequency of an \textit{appropriately chosen subset} of the support of a \textit{properly discretised distribution}. We leverage this reduction to compute mergeable summaries of distributions from the stream of samples while requiring only sublinear space relative to the number of observed samples. This allows us to estimate Wasserstein and Total Variation (TV) distances between any two distributions while samples arrive in streams and from multiple sources. Our algorithms significantly improves on the existing methods for distance estimation incurring super-linear time and linear space complexities, and further extend the mergeable summaries framework to continuous distributions with possibly infinite support. Our results are tight with respect to the existing lower bounds for bounded discrete distributions. In addition, we leverage our proposed estimators of Wasserstein and TV distances to tightly audit the fairness and privacy of algorithms. We empirically demonstrate the efficiency of proposed algorithms across synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07775v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.DS</category>
      <category>stat.CO</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debabrota Basu, Debarshi Chanda</dc:creator>
    </item>
  </channel>
</rss>
