<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.med-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.med-ph</link>
    <description>physics.med-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.med-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Mar 2024 04:03:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 28 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A deep neural network for positioning and inter-crystal scatter identification in multiplexed PET detectors</title>
      <link>https://arxiv.org/abs/2403.18240</link>
      <description>arXiv:2403.18240v1 Announce Type: new 
Abstract: Objective: Conventional event positioning algorithms in light-sharing PET detectors are often limited by edge effects and the impact of inter-crystal scattering (ICS). This study explores the feasibility of deep neural network (DNN) techniques for more precise event positioning in finely segmented and highly multiplexed PET detectors with light-sharing. Approach: A DNN was designed for crystal localisation, and trained/tested with light distributions of photoelectric (P) and Compton/photoelectric (CP) events simulated using optical GATE and an efficient analytical method. Using the statistical properties of ICS events from simulation, an energy-guided positioning algorithm was built into the DNN, enabling selection of the unique or first crystal of interaction in P and CP events, respectively. Performance of the DNN was compared with Anger logic using light distributions from simulated 511-keV point sources near the PET detector. Results: Despite coarse photodetector data due to signal multiplexing, the DNN demonstrated a crystal classification accuracy of 90% for P events and 82% for CP events. For crystal positioning, the DNN outperformed Anger logic by at least 34% and 14% for P and CP events, respectively. Further improvement is somewhat constrained by the physics, specifically, the ratio of backward to forward scattering of gamma rays within the crystal array being close to 1. This prevents selecting the first crystal of interaction in CP events with a high degree of certainty. Significance: Light-sharing and multiplexed PET detectors are common in high-resolution PET, yet event positioning can be poor due to edge effects and ICS events. Our study shows that DNN-based event positioning can enhance 2D coincidence event positioning accuracy by nearly a factor of 2 compared to Anger logic. However, further improvements are difficult to foresee without timing information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18240v1</guid>
      <category>physics.med-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco E Enriquez-Mier-y-Teran, Luping Zhou, Steven R Meikle, Andre Z Kyme</dc:creator>
    </item>
    <item>
      <title>Automated Report Generation for Lung Cytological Images Using a CNN Vision Classifier and Multiple-Transformer Text Decoders: Preliminary Study</title>
      <link>https://arxiv.org/abs/2403.18151</link>
      <description>arXiv:2403.18151v1 Announce Type: cross 
Abstract: Cytology plays a crucial role in lung cancer diagnosis. Pulmonary cytology involves cell morphological characterization in the specimen and reporting the corresponding findings, which are extremely burdensome tasks. In this study, we propose a report-generation technique for lung cytology images. In total, 71 benign and 135 malignant pulmonary cytology specimens were collected. Patch images were extracted from the captured specimen images, and the findings were assigned to each image as a dataset for report generation. The proposed method consists of a vision model and a text decoder. In the former, a convolutional neural network (CNN) is used to classify a given image as benign or malignant, and the features related to the image are extracted from the intermediate layer. Independent text decoders for benign and malignant cells are prepared for text generation, and the text decoder switches according to the CNN classification results. The text decoder is configured using a Transformer that uses the features obtained from the CNN for report generation. Based on the evaluation results, the sensitivity and specificity were 100% and 96.4%, respectively, for automated benign and malignant case classification, and the saliency map indicated characteristic benign and malignant areas. The grammar and style of the generated texts were confirmed as correct and in better agreement with gold standard compared to existing LLM-based image-captioning methods and single-text-decoder ablation model. These results indicate that the proposed method is useful for pulmonary cytology classification and reporting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18151v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atsushi Teramoto, Ayano Michiba, Yuka Kiriyama, Tetsuya Tsukamoto, Kazuyoshi Imaizumi, Hiroshi Fujita</dc:creator>
    </item>
  </channel>
</rss>
