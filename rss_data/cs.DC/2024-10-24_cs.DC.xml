<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Oct 2024 01:45:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Enhancing Real-Time Master Data Management with Complex Match and Merge Algorithms</title>
      <link>https://arxiv.org/abs/2410.17279</link>
      <description>arXiv:2410.17279v1 Announce Type: new 
Abstract: Master Data Management (MDM) ensures data integrity, consistency, and reliability across an organization's systems. I introduce a novel complex match and merge algorithm optimized for real-time MDM solutions. The proposed method accurately identifies duplicates and consolidates records in large-scale datasets by combining deterministic matching, fuzzy matching, and machine learning-based conflict resolution. I implemented it using PySpark and Databricks; the algorithm benefits from distributed computing and Delta Lake for scalable and reliable data processing. Comprehensive performance evaluations demonstrate a 90% accuracy on datasets of up to 10 million records while maintaining low latency and high throughput, significantly improving upon existing MDM approaches. The method shows strong potential in domains such as healthcare and finance, with an overall 30% improvement in latency compared to traditional MDM systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17279v1</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Durai Rajamanickam</dc:creator>
    </item>
    <item>
      <title>Efficiently Scheduling Parallel DAG Tasks on Identical Multiprocessors</title>
      <link>https://arxiv.org/abs/2410.17563</link>
      <description>arXiv:2410.17563v1 Announce Type: new 
Abstract: Parallel real-time embedded applications can be modelled as directed acyclic graphs (DAGs) whose nodes model subtasks and whose edges model precedence constraints among subtasks. Efficiently scheduling such parallel tasks can be challenging in itself, particularly in hard real-time systems where it must be ensured offline that the deadlines of the parallel applications will be met at run time. In this paper, we tackle the problem of scheduling DAG tasks on identical multiprocessor systems efficiently, in terms of processor utilisation. We propose a new algorithm that attempts to use dedicated processor clusters for high-utilisation tasks, as in federated scheduling, but is also capable of reclaiming the processing capacity lost to fragmentation, by splitting the execution of parallel tasks over different existing clusters, in a manner inspired by semi-partitioned C=D scheduling (originally devised for non-parallel tasks). In the experiments with synthetic DAG task sets, our Segmented-Flattened-and-Split scheduling approach achieves a significantly higher scheduling success ratio than federated scheduling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17563v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shardul Lendve (CISTER Research Centre, Faculdade da Engenharia da Universidade do Porto), Konstantinos Bletsas (CISTER Research Centre, ISEP/IPP), Pedro F. Souto (Faculdade da Engenharia da Universidade do Porto, CISTER Research Centre)</dc:creator>
    </item>
    <item>
      <title>Signature-based IaaS Performance Change Detection</title>
      <link>https://arxiv.org/abs/2410.17623</link>
      <description>arXiv:2410.17623v1 Announce Type: new 
Abstract: We propose a novel change detection framework to identify changes in the long-term performance behavior of an IaaS service. An IaaS service's long-term performance behavior is represented by an IaaS performance signature. The proposed framework leverages time series similarity measures and a sliding window technique to detect changes in IaaS performance signatures. We introduce a new IaaS performance noise model that enables the proposed framework to distinguish between performance noise and actual changes in performance. The proposed framework utilizes a novel Signal-to-Noise Ratio (SNR) based approach to detect changes when prior knowledge about performance noise is available. A set of experiments is conducted using real-world datasets to demonstrate the effectiveness of the proposed change detection framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17623v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sheik Mohammad Mostakim Fattah, Athman Bouguettaya</dc:creator>
    </item>
    <item>
      <title>Optimal Fault-Tolerant Dispersion on Oriented Grids</title>
      <link>https://arxiv.org/abs/2410.17813</link>
      <description>arXiv:2410.17813v1 Announce Type: new 
Abstract: Dispersion of mobile robots over the nodes of an anonymous graph is an important problem and turns out to be a crucial subroutine for designing efficient algorithms for many fundamental graph problems via mobile robots. In this problem, starting from an arbitrary initial distribution of $n$ robots across the $n$ nodes, the goal is to achieve a final configuration where each node holds at most one robot. This paper investigates the dispersion problem on an oriented grid, considering the possibility of robot failures (crashes) at any time during the algorithm's execution. We present a crash-tolerant dispersion algorithm that solves the dispersion problem on an anonymous oriented grid in $O(\sqrt{n})$ time and using $O(\log n)$ bits of memory per robot. The algorithm is optimal in terms of both time and memory per robot. We further extend this algorithm to deal with weak Byzantine robots. The weak Byzantine fault dispersion algorithm takes optimal $O(\sqrt{n})$ rounds but requires $O(n\log n)$ bits of memory per robot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17813v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rik Banerjee, Manish Kumar, Anisur Rahaman Molla</dc:creator>
    </item>
    <item>
      <title>AMUSD: Asynchronous Multi-Device Speculative Decoding for LLM Acceleration</title>
      <link>https://arxiv.org/abs/2410.17375</link>
      <description>arXiv:2410.17375v1 Announce Type: cross 
Abstract: Large language models typically generate tokens autoregressively, using each token as input for the next. Recent work on Speculative Decoding has sought to accelerate this process by employing a smaller, faster draft model to more quickly generate candidate tokens. These candidates are then verified in parallel by the larger (original) verify model, resulting in overall speedup compared to using the larger model by itself in an autoregressive fashion. In this work, we introduce AMUSD (Asynchronous Multi-device Speculative Decoding), a system that further accelerates generation by decoupling the draft and verify phases into a continuous, asynchronous approach. Unlike conventional speculative decoding, where only one model (draft or verify) performs token generation at a time, AMUSD enables both models to perform predictions independently on separate devices (e.g., GPUs). We evaluate our approach over multiple datasets and show that AMUSD achieves an average 29% improvement over speculative decoding and up to 1.96$\times$ speedup over conventional autoregressive decoding, while achieving identical output quality. Our system is open-source and available at https://github.com/BradMcDanel/AMUSD/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17375v1</guid>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bradley McDanel</dc:creator>
    </item>
    <item>
      <title>AI-focused HPC Data Centers Can Provide More Power Grid Flexibility and at Lower Cost</title>
      <link>https://arxiv.org/abs/2410.17435</link>
      <description>arXiv:2410.17435v1 Announce Type: cross 
Abstract: The recent growth of Artificial Intelligence (AI), particularly large language models, requires energy-demanding high-performance computing (HPC) data centers, which poses a significant burden on power system capacity. Scheduling data center computing jobs to manage power demand can alleviate network stress with minimal infrastructure investment and contribute to fast time-scale power system balancing. This study, for the first time, comprehensively analyzes the capability and cost of grid flexibility provision by GPU-heavy AI-focused HPC data centers, along with a comparison with CPU-heavy general-purpose HPC data centers traditionally used for scientific computing. Using real-world data from 7 AI-focused HPC data centers, 7 general-purpose HPC data centers, and 3 cloud platforms, we find that AI-focused HPC data centers can offer greater flexibility at 50% lower cost for a range of power system services. By comparing the cost to flexibility market prices, we illustrate the financial profitability of flexibility provision for AI-focused HPC data centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17435v1</guid>
      <category>eess.SY</category>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yihong Zhou, Angel Paredes, Chaimaa Essayeh, Thomas Morstyn</dc:creator>
    </item>
    <item>
      <title>Deoxys: A Causal Inference Engine for Unhealthy Node Mitigation in Large-scale Cloud Infrastructure</title>
      <link>https://arxiv.org/abs/2410.17709</link>
      <description>arXiv:2410.17709v1 Announce Type: cross 
Abstract: The presence of unhealthy nodes in cloud infrastructure signals the potential failure of machines, which can significantly impact the availability and reliability of cloud services, resulting in negative customer experiences. Effectively addressing unhealthy node mitigation is therefore vital for sustaining cloud system performance. This paper introduces Deoxys, a causal inference engine tailored to recommending mitigation actions for unhealthy node in cloud systems to minimize virtual machine downtime and interruptions during unhealthy events. It employs double machine learning combined with causal forest to produce precise and reliable mitigation recommendations based solely on limited observational data collected from the historical unhealthy events. To enhance the causal inference model, Deoxys further incorporates a policy fallback mechanism based on model uncertainty and action overriding mechanisms to (i) improve the reliability of the system, and (ii) strike a good tradeoff between downtime reduction and resource utilization, thereby enhancing the overall system performance.
  After deploying Deoxys in a large-scale cloud infrastructure at Microsoft, our observations demonstrate that Deoxys significantly reduces average VM downtime by 53% compared to a legacy policy, while leading to 49.5% lower VM interruption rate. This substantial improvement enhances the reliability and stability of cloud platforms, resulting in a seamless customer experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17709v1</guid>
      <category>eess.SY</category>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaoyun Zhang, Randolph Yao, Si Qin, Ze Li, Shekhar Agrawal, Binit R. Mishra, Tri Tran, Minghua Ma, Qingwei Lin, Murali Chintalapati, Dongmei Zhang</dc:creator>
    </item>
    <item>
      <title>POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference</title>
      <link>https://arxiv.org/abs/2410.18038</link>
      <description>arXiv:2410.18038v1 Announce Type: cross 
Abstract: Each request in LLM inference goes through two phases: compute-bound prefill and memory-bandwidth-bound decode. To improve GPU utilization, recent systems use hybrid batching that combines the prefill and decode phases of different requests into the same batch. Hybrid batching works well for linear operations as it amortizes the cost of loading model weights from HBM. However, attention computation in hybrid batches remains inefficient because existing attention kernels are optimized for either prefill or decode.
  In this paper, we present POD-Attention -- the first GPU kernel that efficiently computes attention for hybrid batches. POD-Attention aims to maximize the utilization of both compute and memory bandwidth by carefully allocating the GPU's resources such that prefill and decode operations happen concurrently on the same multiprocessor. We integrate POD-Attention in a state-of-the-art LLM inference scheduler Sarathi-Serve. POD-Attention speeds up attention computation by up to 75% (mean 28%) and increases LLM serving throughput by up to 22% in offline inference. In online inference, POD-Attention enables lower time-to-first-token (TTFT), time-between-tokens (TBT), and request execution latency versus Sarathi-Serve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18038v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya K Kamath, Ramya Prabhu, Jayashree Mohan, Simon Peter, Ramachandran Ramjee, Ashish Panwar</dc:creator>
    </item>
    <item>
      <title>Cold Start Latency in Serverless Computing: A Systematic Review, Taxonomy, and Future Directions</title>
      <link>https://arxiv.org/abs/2310.08437</link>
      <description>arXiv:2310.08437v2 Announce Type: replace 
Abstract: Recently, academics and the corporate sector have paid attention to serverless computing, which enables dynamic scalability and an economic model. In serverless computing, users only pay for the time they actually use resources, enabling zero scaling to optimise cost and resource utilisation. However, this approach also introduces the serverless cold start problem. Researchers have developed various solutions to address the cold start problem, yet it remains an unresolved research area. In this article, we propose a systematic literature review on clod start latency in serverless computing. Furthermore, we create a detailed taxonomy of approaches to cold start latency, which we use to investigate existing techniques for reducing the cold start time and frequency. We have classified the current studies on cold start latency into several categories such as caching and application-level optimisation-based solutions, as well as Artificial Intelligence (AI)/Machine Learning (ML)-based solutions. Moreover, we have analyzed the impact of cold start latency on quality of service, explored current cold start latency mitigation methods, datasets, and implementation platforms, and classified them into categories based on their common characteristics and features. Finally, we outline the open challenges and highlight the possible future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08437v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3700875</arxiv:DOI>
      <arxiv:journal_reference>ACM Computing Surveys 2024</arxiv:journal_reference>
      <dc:creator>Muhammed Golec, Guneet Kaur Walia, Mohit Kumar, Felix Cuadrado, Sukhpal Singh Gill, Steve Uhlig</dc:creator>
    </item>
    <item>
      <title>Autobahn: Seamless high speed BFT</title>
      <link>https://arxiv.org/abs/2401.10369</link>
      <description>arXiv:2401.10369v3 Announce Type: replace 
Abstract: Today's practical, high performance Byzantine Fault Tolerant (BFT) consensus protocols operate in the partial synchrony model. However, existing protocols are inefficient when deployments are indeed partially synchronous. They deliver either low latency during fault-free, synchronous periods (good intervals) or robust recovery from events that interrupt progress (blips). At one end, traditional, view-based BFT protocols optimize for latency during good intervals, but, when blips occur, can suffer from performance degradation (hangovers) that can last beyond the return of a good interval. At the other end, modern DAG-based BFT protocols recover more gracefully from blips, but exhibit lackluster latency during good intervals. To close the gap, this work presents Autobahn, a novel high-throughput BFT protocol that offers both low latency and seamless recovery from blips. By combining a highly parallel asynchronous data dissemination layer with a low-latency, partially synchronous consensus mechanism, Autobahn (i) avoids the hangovers incurred by traditional BFT protocols and (ii) matches the throughput of state of the art DAG-based BFT protocols while cutting their latency in half, matching the latency of traditional BFT protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10369v3</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil Giridharan, Florian Suri-Payer, Ittai Abraham, Lorenzo Alvisi, Natacha Crooks</dc:creator>
    </item>
    <item>
      <title>Large Scale Multi-GPU Based Parallel Traffic Simulation for Accelerated Traffic Assignment and Propagation</title>
      <link>https://arxiv.org/abs/2406.08496</link>
      <description>arXiv:2406.08496v2 Announce Type: replace 
Abstract: Traffic propagation simulation is crucial for urban planning, enabling congestion analysis, travel time estimation, and route optimization. Traditional micro-simulation frameworks are limited to main roads due to the complexity of urban mobility and large-scale data. We introduce the Large Scale Multi-GPU Parallel Computing based Regional Scale Traffic Simulation Framework (LPSim), a scalable tool that leverages GPU parallel computing to simulate extensive traffic networks with high fidelity and reduced computation time. LPSim performs millions of vehicle dynamics simulations simultaneously, outperforming CPU-based methods. It can complete simulations of 2.82 million trips in 6.28 minutes using a single GPU, and 9.01 million trips in 21.16 minutes on dual GPUs. LPSim is also tested on dual NVIDIA A100 GPUs, achieving simulations about 113 times faster than traditional CPU methods. This demonstrates its scalability and efficiency for large-scale applications, making LPSim a valuable resource for researchers and planners. Code: https://github.com/Xuan-1998/LPSim</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08496v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuan Jiang, Raja Sengupta, James Demmel, Samuel Williams</dc:creator>
    </item>
    <item>
      <title>I've Got 99 Problems But FLOPS Ain't One</title>
      <link>https://arxiv.org/abs/2407.12819</link>
      <description>arXiv:2407.12819v2 Announce Type: replace 
Abstract: Hyperscalers dominate the landscape of large network deployments, yet they rarely share data or insights about the challenges they face. In light of this supremacy, what problems can we find to solve in this space? We take an unconventional approach to find relevant research directions, starting from public plans to build a $100 billion datacenter for machine learning applications. Leveraging the language models scaling laws, we discover what workloads such a datacenter might carry and explore the challenges one may encounter in doing so, with a focus on networking research. We conclude that building the datacenter and training such models is technically possible, but this requires novel wide-area transports for inter-DC communication, a multipath transport and novel datacenter topologies for intra-datacenter communication, high speed scale-up networks and transports, outlining a rich research agenda for the networking community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12819v2</guid>
      <category>cs.DC</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3696348.3696893</arxiv:DOI>
      <dc:creator>Alexandru M. Gherghescu, Vlad-Andrei B\u{a}doiu, Alexandru Agache, Mihai-Valentin Dumitru, Iuliu Vasilescu, Radu Mantu, Costin Raiciu</dc:creator>
    </item>
    <item>
      <title>CrashEventLLM: Predicting System Crashes with Large Language Models</title>
      <link>https://arxiv.org/abs/2407.15716</link>
      <description>arXiv:2407.15716v3 Announce Type: replace 
Abstract: As the dependence on computer systems expands across various domains, focusing on personal, industrial, and large-scale applications, there arises a compelling need to enhance their reliability to sustain business operations seamlessly and ensure optimal user satisfaction. System logs generated by these devices serve as valuable repositories of historical trends and past failures. The use of machine learning techniques for failure prediction has become commonplace, enabling the extraction of insights from past data to anticipate future behavior patterns. Recently, large language models have demonstrated remarkable capabilities in tasks including summarization, reasoning, and event prediction. Therefore, in this paper, we endeavor to investigate the potential of large language models in predicting system failures, leveraging insights learned from past failure behavior to inform reasoning and decision-making processes effectively. Our approach involves leveraging data from the Intel Computing Improvement Program (ICIP) system crash logs to identify significant events and develop CrashEventLLM. This model, built upon a large language model framework, serves as our foundation for crash event prediction. Specifically, our model utilizes historical data to forecast future crash events, informed by expert annotations. Additionally, it goes beyond mere prediction, offering insights into potential causes for each crash event. This work provides the preliminary insights into prompt-based large language models for the log-based event prediction task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15716v3</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Priyanka Mudgal, Bijan Arbab, Swaathi Sampath Kumar</dc:creator>
    </item>
    <item>
      <title>MoC-System: Efficient Fault Tolerance for Sparse Mixture-of-Experts Model Training</title>
      <link>https://arxiv.org/abs/2408.04307</link>
      <description>arXiv:2408.04307v2 Announce Type: replace 
Abstract: As large language models continue to scale up, distributed training systems have expanded beyond 10k nodes, intensifying the importance of fault tolerance. Checkpoint has emerged as the predominant fault tolerance strategy, with extensive studies dedicated to optimizing its efficiency. However, the advent of the sparse Mixture-of-Experts (MoE) model presents new challenges due to the substantial increase in model size, despite comparable computational demands to dense models.
  In this work, we propose the Mixture-of-Checkpoint System (MoC-System) to orchestrate the vast array of checkpoint shards produced in distributed training systems. MoC-System features a novel Partial Experts Checkpointing (PEC) mechanism, an algorithm-system co-design that strategically saves a selected subset of experts, effectively reducing the MoE checkpoint size to levels comparable with dense models. Incorporating hybrid parallel strategies, MoC-System involves fully sharded checkpointing strategies to evenly distribute the workload across distributed ranks. Furthermore, MoC-System introduces a two-level checkpointing management method that asynchronously handles in-memory snapshots and persistence processes.
  We build MoC-System upon the Megatron-DeepSpeed framework, achieving up to a 98.9% reduction in overhead for each checkpointing process compared to the original method, during MoE model training with ZeRO-2 data parallelism and expert parallelism. Additionally, extensive empirical analyses substantiate that our methods enhance efficiency while maintaining comparable model accuracy, even achieving an average accuracy increase of 1.08% on downstream tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04307v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weilin Cai, Le Qin, Jiayi Huang</dc:creator>
    </item>
    <item>
      <title>TiMePReSt: Time and Memory Efficient Pipeline Parallel DNN Training with Removed Staleness</title>
      <link>https://arxiv.org/abs/2410.14312</link>
      <description>arXiv:2410.14312v2 Announce Type: replace 
Abstract: DNN training is time-consuming and requires efficient multi-accelerator parallelization, where a single training iteration is split over available accelerators. Current approaches often parallelize training using intra-batch parallelization. Combining inter-batch and intra-batch pipeline parallelism is common to further improve training throughput. In this article, we develop a system, called TiMePReSt, that combines them in a novel way which helps to better overlap computation and communication, and limits the amount of communication. The traditional pipeline-parallel training of DNNs maintains similar working principle as sequential or conventional training of DNNs by maintaining consistent weight versions in forward and backward passes of a mini-batch. Thus, it suffers from high GPU memory footprint during training. In this paper, experimental study demonstrates that compromising weight consistency doesn't decrease prediction capability of a parallelly trained DNN. Moreover, TiMePReSt overcomes GPU memory overhead and achieves zero weight staleness. State-of-the-art techniques often become costly in terms of training time. In order to address this issue, TiMePReSt introduces a variant of intra-batch parallelism that parallelizes the forward pass of each mini-batch by decomposing it into smaller micro-batches. A novel synchronization method between forward and backward passes reduces training time in TiMePReSt. The occurrence of multiple sequence problem and its relation with version difference have been observed in TiMePReSt. This paper presents a mathematical relationship between the number of micro-batches and worker machines, highlighting the variation in version difference. A mathematical expression has been developed to calculate version differences for various combinations of these two without creating diagrams for all combinations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14312v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ankita Dutta, Nabendu Chaki, Rajat K. De</dc:creator>
    </item>
    <item>
      <title>Portable, Massively Parallel Implementation of a Material Point Method for Compressible Flows</title>
      <link>https://arxiv.org/abs/2404.17057</link>
      <description>arXiv:2404.17057v3 Announce Type: replace-cross 
Abstract: The recent evolution of software and hardware technologies is leading to a renewed computational interest in Particle-In-Cell (PIC) methods such as the Material Point Method (MPM). Indeed, provided some critical aspects are properly handled, PIC methods can be cast in formulations suitable for the requirements of data locality and fine-grained parallelism of modern hardware accelerators such as Graphics Processing Units (GPUs). Such a rapid and continuous technological development increases also the importance of generic and portable implementations. While the capabilities of MPM on a wide range continuum mechanics problem have been already well assessed, the use of the method in compressible fluid dynamics has received less attention. In this paper we present a portable, highly parallel, GPU based MPM solver for compressible gas dynamics. The implementation aims to reach a good compromise between portability and efficiency in order to provide a first assessment of the potential of this approach in solving strongly compressible gas flow problems, also taking into account solid obstacles. The numerical model considered constitutes a first step towards the development of a monolithic MPM solver for Fluid-Structure Interaction (FSI) problems at all Mach numbers up to the supersonic regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17057v3</guid>
      <category>physics.comp-ph</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paolo Joseph Baioni, Tommaso Benacchio, Luigi Capone, Carlo de Falco</dc:creator>
    </item>
    <item>
      <title>Harnessing Your DRAM and SSD for Sustainable and Accessible LLM Inference with Mixed-Precision and Multi-level Caching</title>
      <link>https://arxiv.org/abs/2410.14740</link>
      <description>arXiv:2410.14740v2 Announce Type: replace-cross 
Abstract: Although Large Language Models (LLMs) have demonstrated remarkable capabilities, their massive parameter counts and associated extensive computing make LLMs' deployment the main part of carbon emission from nowadays AI applications. Compared to modern GPUs like H$100$, it would be significantly carbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as shown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for LLM servings. However, the limited High Bandwidth Memory (HBM) available on such GPU often cannot support the loading of LLMs due to the gigantic model size and intermediate activation data, making their serving challenging. For instance, a LLaMA2 model with $70$B parameters typically requires $128$GB for inference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains infeasible even considering the additional $64$GB DRAM. To address this challenge, this paper proposes a mixed-precision with a model modularization algorithm to enable LLM inference on outdated hardware with resource constraints. (The precision denotes the numerical precision like FP16, INT8, INT4) and multi-level caching (M2Cache).)
  Specifically, our M2Cache first modulizes neurons in LLM and creates their importance ranking. Then, it adopts a dynamic sparse mixed-precision quantization mechanism in weight space to reduce computational demands and communication overhead at each decoding step. It collectively lowers the operational carbon emissions associated with LLM inference. Moreover, M2Cache introduces a three-level cache management system with HBM, DRAM, and SSDs that complements the dynamic sparse mixed-precision inference. To enhance communication efficiency, M2Cache maintains a neuron-level mixed-precision LRU cache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14740v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jie Peng, Zhang Cao, Huaizhi Qu, Zhengyu Zhang, Chang Guo, Yanyong Zhang, Zhichao Cao, Tianlong Chen</dc:creator>
    </item>
  </channel>
</rss>
