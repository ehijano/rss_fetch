<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Apr 2024 04:03:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>DiffFit: Visually-Guided Differentiable Fitting of Molecule Structures to Cryo-EM Map</title>
      <link>https://arxiv.org/abs/2404.02465</link>
      <description>arXiv:2404.02465v1 Announce Type: new 
Abstract: We introduce DiffFit, a differentiable algorithm for fitting protein atomistic structures into experimental reconstructed Cryo-Electron Microscopy (cryo-EM) volume map. This process is essential in structural biology to semi-automatically reconstruct large meso-scale models of complex protein assemblies and complete cellular structures that are based on measured cryo-EM data. Current approaches require manual fitting in 3D that already results in approximately aligned structures followed by an automated fine-tuning of the alignment. With our DiffFit approach, we enable domain scientists to automatically fit new structures and visualize the fitting results for inspection and interactive revision. Our fitting begins with differentiable 3D rigid transformations of the protein atom coordinates, followed by sampling the density values at its atom coordinates from the target cryo-EM volume. To ensure a meaningful correlation between the sampled densities and the protein structure, we propose a novel loss function based on a multi-resolution volume-array approach and the exploitation of the negative space. Such loss function serves as a critical metric for assessing the fitting quality, ensuring both fitting accuracy and improved visualization of the results. We assessed the placement quality of DiffFit with several large, realistic datasets and found its quality to be superior to that of previous methods. We further evaluated our method in two use cases. First, we demonstrate its use in the process of automating the integration of known composite structures into larger protein complexes. Second, we show that it facilitates the fitting of predicted protein domains into volume densities to aid researchers in the identification of unknown proteins. We open-sourced (github.com/nanovis/DiffFitViewer) DiffFit as a plugin in ChimeraX. All supplemental materials are available at osf.io/5tx4q.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02465v1</guid>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deng Luo, Zainab Alsuwaykit, Dawar Khan, Ond\v{r}ej Strnad, Tobias Isenberg, Ivan Viola</dc:creator>
    </item>
    <item>
      <title>New methods for drug synergy prediction</title>
      <link>https://arxiv.org/abs/2404.02484</link>
      <description>arXiv:2404.02484v1 Announce Type: cross 
Abstract: In this mini-review, we explore the new prediction methods for drug combination synergy relying on high-throughput combinatorial screens. The fast progress of the field is witnessed in the more than thirty original machine learning methods published since 2021, a clear majority of them based on deep learning techniques. We aim to put these papers under a unifying lens by highlighting the core technologies, the data sources, the input data types and synergy scores used in the methods, as well as the prediction scenarios and evaluation protocols that the papers deal with. Our finding is that the best methods accurately solve the synergy prediction scenarios involving known drugs or cell lines while the scenarios involving new drugs or cell lines still fall short of an accurate prediction level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02484v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatemeh Abbasi, Juho Rousu</dc:creator>
    </item>
    <item>
      <title>Assessing the Utility of Large Language Models for Phenotype-Driven Gene Prioritization in Rare Genetic Disorder Diagnosis</title>
      <link>https://arxiv.org/abs/2403.14801</link>
      <description>arXiv:2403.14801v2 Announce Type: replace 
Abstract: Phenotype-driven gene prioritization is a critical process in the diagnosis of rare genetic disorders for identifying and ranking potential disease-causing genes based on observed physical traits or phenotypes. While traditional approaches rely on curated knowledge graphs with phenotype-gene relations, recent advancements in large language models have opened doors to the potential of AI predictions through extensive training on diverse corpora and complex models. This study conducted a comprehensive evaluation of five large language models, including two Generative Pre-trained Transformers series, and three Llama2 series, assessing their performance across three key metrics: task completeness, gene prediction accuracy, and adherence to required output structures. Various experiments explored combinations of models, prompts, input types, and task difficulty levels. Our findings reveal that even the best-performing LLM, GPT-4, achieved an accuracy of 16.0%, which still lags behind traditional bioinformatics tools. Prediction accuracy increased with the parameter/model size. A similar increasing trend was observed for the task completion rate, with complicated prompts more likely to increase task completeness in models smaller than GPT-4. However, complicated prompts are more likely to decrease the structure compliance rate, but no prompt effects on GPT-4. Compared to HPO term-based input, LLM was also able to achieve better than random prediction accuracy by taking free-text input, but slightly lower than with the HPO input. Bias analysis showed that certain genes, such as MECP2, CDKL5, and SCN1A, are more likely to be top-ranked, potentially explaining the variances observed across different datasets. This study provides valuable insights into the integration of LLMs within genomic analysis, contributing to the ongoing discussion on the utilization of advanced LLMs in clinical workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14801v2</guid>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyoung Kim (Department of Biomedical Informatics, Columbia University, New York, NY, USA), Jingye Yang (Raymond G. Perelman Center for Cellular and Molecular Therapeutics, Children's Hospital of Philadelphia, Philadelphia, USA, Department of Mathematics, University of Pennsylvania, Philadelphia, USA), Kai Wang (Raymond G. Perelman Center for Cellular and Molecular Therapeutics, Children's Hospital of Philadelphia, Philadelphia, USA, Department of Pathology and Laboratory Medicine, University of Pennsylvania, Philadelphia, USA), Chunhua Weng (Department of Biomedical Informatics, Columbia University, New York, NY, USA), Cong Liu (Department of Biomedical Informatics, Columbia University, New York, NY, USA)</dc:creator>
    </item>
    <item>
      <title>BioImage.IO Chatbot: A Community-Driven AI Assistant for Advanced Bioimage Analysis and Tool Integration</title>
      <link>https://arxiv.org/abs/2310.18351</link>
      <description>arXiv:2310.18351v4 Announce Type: replace-cross 
Abstract: We introduce the BioImage$.$IO Chatbot, an AI assistant underpinned by Large Language Models and enriched by a community-driven knowledge base and tools. It facilitates customized interactions across a spectrum of user requirements via a flexible extension mechanism, from data retrieval to AI-enhanced analysis. Adhering to open-source values, the chatbot is in constant development with input from the bioimage community, improving its dependability and collaboratively tackling AI-related challenges. This tool streamlines the exploration of the complex bioimage analysis landscape, enabling life sciences to advance by harnessing the collective ingenuity of its community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18351v4</guid>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.10032227</arxiv:DOI>
      <dc:creator>Wanlu Lei, Caterina Fuster-Barcel\'o, Gabriel Reder, Arrate Mu\~noz-Barrutia, Wei Ouyang</dc:creator>
    </item>
    <item>
      <title>MolBind: Multimodal Alignment of Language, Molecules, and Proteins</title>
      <link>https://arxiv.org/abs/2403.08167</link>
      <description>arXiv:2403.08167v2 Announce Type: replace-cross 
Abstract: Recent advancements in biology and chemistry have leveraged multi-modal learning, integrating molecules and their natural language descriptions to enhance drug discovery. However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them. In this work, we propose MolBind, a framework that trains encoders for multiple modalities through contrastive learning, mapping all modalities to a shared feature space for multi-modal semantic alignment. To facilitate effective pre-training of MolBind on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including graph-language, conformation-language, graph-conformation, and conformation-protein paired data. MolBind shows superior zero-shot learning performance across a wide range of tasks, demonstrating its strong capability of capturing the underlying semantics of multiple modalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08167v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Teng Xiao, Chao Cui, Huaisheng Zhu, Vasant G. Honavar</dc:creator>
    </item>
  </channel>
</rss>
