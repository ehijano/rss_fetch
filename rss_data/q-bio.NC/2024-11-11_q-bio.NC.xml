<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Nov 2024 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Time in a bottle. A psychophysics study of human time perception through aging</title>
      <link>https://arxiv.org/abs/2411.05017</link>
      <description>arXiv:2411.05017v1 Announce Type: new 
Abstract: Time perception is crucial for a coherent human experience. As life progresses, our perception of the passage of time becomes increasingly non-uniform, often feeling as though it accelerates with age. While various causes for this phenomenon have been theorized, a comprehensive mathematical and theoretical framework remains underexplored. This study aims to elucidate the mechanisms behind perceived time dilation by integrating classical and revised psychophysical theorems with a novel mathematical approach. Utilizing Weber-Fechner laws as foundational elements, we develop a model that transitions from exponential to logarithmic functions to represent changes in time perception across the human lifespan. Our results indicate that the perception of time shifts significantly around the age of mental maturity, aligning with a proposed inversion point where sensitivity to temporal stimuli decreases, eventually plateauing out at a constant rate. This model not only explains the underlying causes of time perception changes but also provides analytical values to quantify this acceleration. These findings offer valuable insights into the cognitive and neurological processes influencing how we experience time as we go through life.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05017v1</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Enric Espel Sanchez</dc:creator>
    </item>
    <item>
      <title>Electro-diffusive modeling and the role of spine geometry on action potential propagation in neurons</title>
      <link>https://arxiv.org/abs/2411.05329</link>
      <description>arXiv:2411.05329v1 Announce Type: new 
Abstract: Electrical signaling in the brain plays a vital role to our existence but at the same time, the fundamental mechanism of this propagation is undeciphered. Notable advancements have been made in the numerical modeling supplementing the related experimental findings. Cable theory based models provided a significant breakthrough in understanding the mechanism of electrical propagation in the neuronal axons. Cable theory, however, fails for thin geometries such as a spine or a dendrite of a neuron, amongst its other limitations. Recently, the spatiotemporal propagation has been precisely modeled using the Poisson-Nernst-Planck (PNP) electro-diffusive theory in the neuronal axons as well as the dendritic spines respectively. Patch clamp and voltage imaging experiments have extensively aided the study of action potential propagation exclusively for the neuronal axons but not the dendritic spines because of the challenges linked with their thin geometry. Assisted by the super-resolution microscopes and the voltage dyeing experiments, it has become possible to precisely measure the voltage in the dendritic spines. This has facilitated the requirement of a high fidelity numerical frame that is capable of acting as a digital twin. Here, using the PNP theory, we integrate the dendritic spine, soma and the axon region to numerically model the propagation of excitatory synaptic potential in a complete neuronal geometry with the synaptic input at the spines, potential initiating at the axon hillock and propagating through the neuronal axon. The model outputs the forward propagation of the action potential along the neuronal axons as well as the back propagation into the spines. We point out the significance of the intricate geometry of the dendritic spines, namely the spine neck length and radius, and the ion channel density in the axon hillock to the action potential initiation and propagation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05329v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Gulati, Shiva Rudraraju</dc:creator>
    </item>
    <item>
      <title>Relationships between the degrees of freedom in the affine Gaussian derivative model for visual receptive fields and 2-D affine image transformations, with application to covariance properties of simple cells in the primary visual cortex</title>
      <link>https://arxiv.org/abs/2411.05673</link>
      <description>arXiv:2411.05673v1 Announce Type: new 
Abstract: When observing the surface patterns of objects delimited by smooth surfaces, the projections of the surface patterns to the image domain will be subject to substantial variabilities, as induced by variabilities in the geometric viewing conditions, and as generated by either monocular or binocular imaging conditions, or by relative motions between the object and the observer over time. To first order of approximation, the image deformations of such projected surface patterns can be modelled as local linearizations in terms of local 2-D spatial affine transformations.
  This paper presents a theoretical analysis of relationships between the degrees of freedom in 2-D spatial affine image transformations and the degrees of freedom in the affine Gaussian derivative model for visual receptive fields. For this purpose, we first describe a canonical decomposition of 2-D affine transformations on a product form, closely related to a singular value decomposition, while in closed form, and which reveals the degrees of freedom in terms of (i) uniform scaling transformations, (ii) an overall amount of global rotation, (iii) a complementary non-uniform scaling transformation and (iv) a relative normalization to a preferred symmetry orientation in the image domain. Then, we show how these degrees of freedom relate to the degrees of freedom in the affine Gaussian derivative model.
  Finally, we use these theoretical results to consider whether we could regard the biological receptive fields in the primary visual cortex of higher mammals as being able to span the degrees of freedom of 2-D spatial affine transformations, based on interpretations of existing neurophysiological experimental results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05673v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tony Lindeberg</dc:creator>
    </item>
    <item>
      <title>AGE2HIE: Transfer Learning from Brain Age to Predicting Neurocognitive Outcome for Infant Brain Injury</title>
      <link>https://arxiv.org/abs/2411.05188</link>
      <description>arXiv:2411.05188v1 Announce Type: cross 
Abstract: Hypoxic-Ischemic Encephalopathy (HIE) affects 1 to 5 out of every 1,000 newborns, with 30% to 50% of cases resulting in adverse neurocognitive outcomes. However, these outcomes can only be reliably assessed as early as age 2. Therefore, early and accurate prediction of HIE-related neurocognitive outcomes using deep learning models is critical for improving clinical decision-making, guiding treatment decisions and assessing novel therapies. However, a major challenge in developing deep learning models for this purpose is the scarcity of large, annotated HIE datasets. We have assembled the first and largest public dataset, however it contains only 156 cases with 2-year neurocognitive outcome labels. In contrast, we have collected 8,859 normal brain black Magnetic Resonance Imagings (MRIs) with 0-97 years of age that are available for brain age estimation using deep learning models. In this paper, we introduce AGE2HIE to transfer knowledge learned by deep learning models from healthy controls brain MRIs to a diseased cohort, from structural to diffusion MRIs, from regression of continuous age estimation to prediction of the binary neurocognitive outcomes, and from lifespan age (0-97 years) to infant (0-2 weeks). Compared to training from scratch, transfer learning from brain age estimation significantly improves not only the prediction accuracy (3% or 2% improvement in same or multi-site), but also the model generalization across different sites (5% improvement in cross-site validation).</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05188v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rina Bao, Sheng He, Ellen Grant, Yangming Ou</dc:creator>
    </item>
    <item>
      <title>Scaling Laws for Task-Optimized Models of the Primate Visual Ventral Stream</title>
      <link>https://arxiv.org/abs/2411.05712</link>
      <description>arXiv:2411.05712v1 Announce Type: cross 
Abstract: When trained on large-scale object classification datasets, certain artificial neural network models begin to approximate core object recognition (COR) behaviors and neural response patterns in the primate visual ventral stream (VVS). While recent machine learning advances suggest that scaling model size, dataset size, and compute resources improve task performance, the impact of scaling on brain alignment remains unclear. In this study, we explore scaling laws for modeling the primate VVS by systematically evaluating over 600 models trained under controlled conditions on benchmarks spanning V1, V2, V4, IT and COR behaviors. We observe that while behavioral alignment continues to scale with larger models, neural alignment saturates. This observation remains true across model architectures and training datasets, even though models with stronger inductive bias and datasets with higher-quality images are more compute-efficient. Increased scaling is especially beneficial for higher-level visual areas, where small models trained on few samples exhibit only poor alignment. Finally, we develop a scaling recipe, indicating that a greater proportion of compute should be allocated to data samples over model size. Our results suggest that while scaling alone might suffice for alignment with human core object recognition behavior, it will not yield improved models of the brain's visual ventral stream with current architectures and datasets, highlighting the need for novel strategies in building brain-like models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05712v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abdulkadir Gokce, Martin Schrimpf</dc:creator>
    </item>
    <item>
      <title>Consciousness is entailed by compositional learning of new causal structures in deep predictive processing systems</title>
      <link>https://arxiv.org/abs/2301.07016</link>
      <description>arXiv:2301.07016v3 Announce Type: replace 
Abstract: Machine learning algorithms have achieved superhuman performance in specific complex domains. However, learning online from few examples and compositional learning for efficient generalization across domains remain elusive. In humans, such learning includes specific declarative memory formation and is closely associated with consciousness. Predictive processing has been advanced as a principled Bayesian framework for understanding the cortex as implementing deep generative models for both sensory perception and action control. However, predictive processing offers little direct insight into fast compositional learning or of the separation between conscious and unconscious contents. Here, propose that access consciousness arises as a consequence of a particular learning mechanism operating within a predictive processing system. We extend predictive processing by adding online, single-example new structure learning via hierarchical binding of unpredicted inferences. This system learns new causes by quickly connecting together novel combinations of perceptions, which manifests as working memories that can become short- and long-term declarative memories retrievable by associative recall. The contents of such bound representations are unified yet differentiated, can be maintained by selective attention and are globally available. The proposed learning process explains contrast and masking manipulations, postdictive perceptual integration, and other paradigm cases of consciousness research. 'Phenomenal conscious experience' is how the learning system transparently models its own functioning, giving rise to perceptual illusions underlying the meta-problem of consciousness. Our proposal naturally unifies the feature binding, recurrent processing, predictive processing, and global workspace theories of consciousness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.07016v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.RO</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V. A. Aksyuk</dc:creator>
    </item>
    <item>
      <title>Anti-seizure medication tapering correlates with daytime delta band power reduction in the cortex</title>
      <link>https://arxiv.org/abs/2405.01385</link>
      <description>arXiv:2405.01385v2 Announce Type: replace 
Abstract: Anti-seizure medications (ASMs) are the primary treatment for epilepsy, yet medication tapering effects have not been investigated in a dose, region, and time-dependent manner, despite their potential impact on research and clinical practice.
  We examined over 3000 hours of intracranial EEG recordings in 32 subjects during long-term monitoring, of which 22 underwent concurrent ASM tapering. We estimated ASM plasma levels based on known pharmaco-kinetics of all the major ASM types.
  We found an overall decrease in the power of delta band (${\delta}$) activity around the period of maximum medication withdrawal in most (80%) subjects, independent of their epilepsy type or medication combination. The degree of withdrawal correlated positively with the magnitude of ${\delta}$ power decrease. This dose-dependent effect was evident across all recorded cortical regions during daytime; but not in sub-cortical regions, or during night time. We found no evidence of a differential effect in seizure onset, spiking, or pathological brain regions.
  The finding of decreased ${\delta}$ band power during ASM tapering agrees with previous literature. Our observed dose-dependent effect indicates that monitoring ASM levels in cortical regions may be feasible for applications such as medication reminder systems, or closed-loop ASM delivery systems. ASMs are also used in other neurological and psychiatric conditions, making our findings relevant to a general neuroscience and neurology audience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01385v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillermo M. Besne, Nathan Evans, Mariella Panagiotopoulou, Billy Smith, Fahmida A Chowdhury, Beate Diehl, John S Duncan, Andrew W McEvoy, Anna Miserocchi, Jane de Tisi, Mathew Walker, Peter N. Taylor, Chris Thornton, Yujiang Wang</dc:creator>
    </item>
    <item>
      <title>Lattice physics approaches for neural networks</title>
      <link>https://arxiv.org/abs/2405.12022</link>
      <description>arXiv:2405.12022v4 Announce Type: replace 
Abstract: Modern neuroscience has evolved into a frontier field that draws on numerous disciplines, resulting in the flourishing of novel conceptual frames primarily inspired by physics and complex systems science. Contributing in this direction, we recently introduced a mathematical framework to describe the spatiotemporal interactions of systems of neurons using lattice field theory, the reference paradigm for theoretical particle physics. In this note, we provide a concise summary of the basics of the theory, aiming to be intuitive to the interdisciplinary neuroscience community. We contextualize our methods, illustrating how to readily connect the parameters of our formulation to experimental variables using well-known renormalization procedures. This synopsis yields the key concepts needed to describe neural networks using lattice physics. Such classes of methods are attention-worthy in an era of blistering improvements in numerical computations, as they can facilitate relating the observation of neural activity to generative models underpinned by physical principles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12022v4</guid>
      <category>q-bio.NC</category>
      <category>physics.app-ph</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giampiero Bardella, Simone Franchini, Pierpaolo Pani, Stefano Ferraina</dc:creator>
    </item>
    <item>
      <title>Topological and Graph Theoretical Analysis of Dynamic Functional Connectivity for Autism Spectrum Disorder</title>
      <link>https://arxiv.org/abs/2410.16874</link>
      <description>arXiv:2410.16874v2 Announce Type: replace 
Abstract: Autism Spectrum Disorder (ASD) is a prevalent neurological disorder. However, the multi-faceted symptoms and large individual differences among ASD patients are hindering the diagnosis process, which largely relies on subject descriptions and lacks quantitative biomarkers. To remediate such problems, this paper explores the use of graph theory and topological data analysis (TDA) to study brain activity in ASD patients and normal controls. We employ the Mapper algorithm in TDA and the distance correlation graphical model (DCGM) in graph theory to create brain state networks, then innovatively adopt complex network metrics in Graph signal processing (GSP) and physical quantities to analyze brain activities over time. Our findings reveal statistical differences in network characteristics between ASD and control groups. Compared to normal subjects, brain state networks of ASD patients tend to have decreased modularity, higher von Neumann entropy, increased Betti-0 numbers, and decreased Betti-1 numbers. These findings attest to the biological traits of ASD, suggesting less organized and more variable brain dynamics. These findings offer potential biomarkers for ASD diagnosis and deepen our understanding of its neural correlations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16874v2</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuzhe Chen, Dayu Qin, Ercan Engin Kuruoglu</dc:creator>
    </item>
    <item>
      <title>Inferring stochastic low-rank recurrent neural networks from neural data</title>
      <link>https://arxiv.org/abs/2406.16749</link>
      <description>arXiv:2406.16749v3 Announce Type: replace-cross 
Abstract: A central aim in computational neuroscience is to relate the activity of large populations of neurons to an underlying dynamical system. Models of these neural dynamics should ideally be both interpretable and fit the observed data well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability by having tractable dynamics. However, it is unclear how to best fit low-rank RNNs to data consisting of noisy observations of an underlying stochastic system. Here, we propose to fit stochastic low-rank RNNs with variational sequential Monte Carlo methods. We validate our method on several datasets consisting of both continuous and spiking neural data, where we obtain lower dimensional latent dynamics than current state of the art methods. Additionally, for low-rank models with piecewise linear nonlinearities, we show how to efficiently identify all fixed points in polynomial rather than exponential cost in the number of units, making analysis of the inferred dynamics tractable for large RNNs. Our method both elucidates the dynamical systems underlying experimental recordings and provides a generative model whose trajectories match observed variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16749v3</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS) 2024</arxiv:journal_reference>
      <dc:creator>Matthijs Pals, A Erdem Sa\u{g}tekin, Felix Pei, Manuel Gloeckler, Jakob H Macke</dc:creator>
    </item>
  </channel>
</rss>
