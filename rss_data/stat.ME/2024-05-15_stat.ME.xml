<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 May 2024 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Best practices for estimating and reporting epidemiological delay distributions of infectious diseases using public health surveillance and healthcare data</title>
      <link>https://arxiv.org/abs/2405.08841</link>
      <description>arXiv:2405.08841v1 Announce Type: new 
Abstract: Epidemiological delays, such as incubation periods, serial intervals, and hospital lengths of stay, are among key quantities in infectious disease epidemiology that inform public health policy and clinical practice. This information is used to inform mathematical and statistical models, which in turn can inform control strategies. There are three main challenges that make delay distributions difficult to estimate. First, the data are commonly censored (e.g., symptom onset may only be reported by date instead of the exact time of day). Second, delays are often right truncated when being estimated in real time (not all events that have occurred have been observed yet). Third, during a rapidly growing or declining outbreak, overrepresentation or underrepresentation, respectively, of recently infected cases in the data can lead to bias in estimates. Studies that estimate delays rarely address all these factors and sometimes report several estimates using different combinations of adjustments, which can lead to conflicting answers and confusion about which estimates are most accurate. In this work, we formulate a checklist of best practices for estimating and reporting epidemiological delays with a focus on the incubation period and serial interval. We also propose strategies for handling common biases and identify areas where more work is needed. Our recommendations can help improve the robustness and utility of reported estimates and provide guidance for the evaluation of estimates for downstream use in transmission models or other analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08841v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kelly Charniga (IP), Sang Woo Park (NTU), Andrei R Akhmetzhanov (NTU), Anne Cori (LSHTM), Jonathan Dushoff (LSHTM), Sebastian Funk (LSHTM), Katelyn M Gostic (CDC), Natalie M Linton (UKHSA), Adrian Lison (UKHSA), Christopher E Overton (UKHSA), Juliet R C Pulliam (CDC), Thomas Ward (UKHSA), Simon Cauchemez (IP), Sam Abbott (LSHTM)</dc:creator>
    </item>
    <item>
      <title>Evaluating the Uncertainty in Mean Residual Times: Estimators Based on Residence Times from Discrete Time Processes</title>
      <link>https://arxiv.org/abs/2405.08853</link>
      <description>arXiv:2405.08853v1 Announce Type: new 
Abstract: In this work, we propose estimators for the uncertainty in mean residual times that require, for their evaluation, statistically independent individual residence times obtained from a discrete time process. We examine their performance through numerical experiments involving well-known probability distributions, and an application example using molecular dynamics simulation results, from an aqueous NaCl solution, is provided. These computationally inexpensive estimators, capable of achieving very accurate outcomes, serve as useful tools for assessing and reporting uncertainties in mean residual times across a wide range of simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08853v1</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hern\'an R. S\'anchez, Javier Garcia</dc:creator>
    </item>
    <item>
      <title>High dimensional test for functional covariates</title>
      <link>https://arxiv.org/abs/2405.08912</link>
      <description>arXiv:2405.08912v1 Announce Type: new 
Abstract: As medical devices become more complex, they routinely collect extensive and complicated data. While classical regressions typically examine the relationship between an outcome and a vector of predictors, it becomes imperative to identify the relationship with predictors possessing functional structures. In this article, we introduce a novel inference procedure for examining the relationship between outcomes and large-scale functional predictors. We target testing the linear hypothesis on the functional parameters under the generalized functional linear regression framework, where the number of the functional parameters grows with the sample size. We develop the estimation procedure for the high dimensional generalized functional linear model incorporating B-spline functional approximation and amenable regularization. Furthermore, we construct a procedure that is able to test the local alternative hypothesis on the linear combinations of the functional parameters. We establish the statistical guarantees in terms of non-asymptotic convergence of the parameter estimation and the oracle property and asymptotic normality of the estimators. Moreover, we derive the asymptotic distribution of the test statistic. We carry out intensive simulations and illustrate with a new dataset from an Alzheimer's disease magnetoencephalography study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08912v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huaqing Jin, Fei Jiang</dc:creator>
    </item>
    <item>
      <title>Nonparametric Inference on Dose-Response Curves Without the Positivity Condition</title>
      <link>https://arxiv.org/abs/2405.09003</link>
      <description>arXiv:2405.09003v1 Announce Type: new 
Abstract: Existing statistical methods in causal inference often rely on the assumption that every individual has some chance of receiving any treatment level regardless of its associated covariates, which is known as the positivity condition. This assumption could be violated in observational studies with continuous treatments. In this paper, we present a novel integral estimator of the causal effects with continuous treatments (i.e., dose-response curves) without requiring the positivity condition. Our approach involves estimating the derivative function of the treatment effect on each observed data sample and integrating it to the treatment level of interest so as to address the bias resulting from the lack of positivity condition. The validity of our approach relies on an alternative weaker assumption that can be satisfied by additive confounding models. We provide a fast and reliable numerical recipe for computing our estimator in practice and derive its related asymptotic theory. To conduct valid inference on the dose-response curve and its derivative, we propose using the nonparametric bootstrap and establish its consistency. The practical performances of our proposed estimators are validated through simulation studies and an analysis of the effect of air pollution exposure (PM$_{2.5}$) on cardiovascular mortality rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09003v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yikun Zhang, Yen-Chi Chen, Alexander Giessing</dc:creator>
    </item>
    <item>
      <title>Causal Inference for a Hidden Treatment</title>
      <link>https://arxiv.org/abs/2405.09080</link>
      <description>arXiv:2405.09080v1 Announce Type: new 
Abstract: In many empirical settings, directly observing a treatment variable may be infeasible although an error-prone surrogate measurement of the latter will often be available. Causal inference based solely on the observed surrogate measurement of the hidden treatment may be particularly challenging without an additional assumption or auxiliary data. To address this issue, we propose a method that carefully incorporates the surrogate measurement together with a proxy of the hidden treatment to identify its causal effect on any scale for which identification would in principle be feasible had contrary to fact the treatment been observed error-free. Beyond identification, we provide general semiparametric theory for causal effects identified using our approach, and we derive a large class of semiparametric estimators with an appealing multiple robustness property. A significant obstacle to our approach is the estimation of nuisance functions involving the hidden treatment, which prevents the direct application of standard machine learning algorithms. To resolve this, we introduce a novel semiparametric EM algorithm, thus adding a practical dimension to our theoretical contributions. This methodology can be adapted to analyze a large class of causal parameters in the proposed hidden treatment model, including the population average treatment effect, the effect of treatment on the treated, quantile treatment effects, and causal effects under marginal structural models. We examine the finite-sample performance of our method using simulations and an application which aims to estimate the causal effect of Alzheimer's disease on hippocampal volume using data from the Alzheimer's Disease Neuroimaging Initiative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09080v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Zhou, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Exploring uniformity and maximum entropy distribution on torus through intrinsic geometry: Application to protein-chemistry</title>
      <link>https://arxiv.org/abs/2405.09149</link>
      <description>arXiv:2405.09149v1 Announce Type: new 
Abstract: A generic family of distributions, defined on the surface of a curved torus is introduced using the area element of it. The area uniformity and the maximum entropy distribution are identified using the trigonometric moments of the proposed family. A marginal distribution is obtained as a three-parameter modification of the von Mises distribution that encompasses the von Mises, Cardioid, and Uniform distributions as special cases. The proposed family of the marginal distribution exhibits both symmetric and asymmetric, unimodal or bimodal shapes, contingent upon parameters. Furthermore, we scrutinize a two-parameter symmetric submodel, examining its moments, measure of variation, Kullback-Leibler divergence, and maximum likelihood estimation, among other properties. In addition, we introduce a modified acceptance-rejection sampling with a thin envelope obtained from the upper-Riemann-sum of a circular density, achieving a high rate of acceptance. This proposed sampling scheme will accelerate the empirical studies for a large-scale simulation reducing the processing time. Furthermore, we extend the Uniform, Wrapped Cauchy, and Kato-Jones distributions to the surface of the curved torus and implemented the proposed bivariate toroidal distribution for different groups of protein data, namely, $\alpha$-helix, $\beta$-sheet, and their mixture. A marginal of this proposed distribution is fitted to the wind direction data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09149v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Surojit Biswas, Buddhananda Banerjee</dc:creator>
    </item>
    <item>
      <title>Multi-Source Conformal Inference Under Distribution Shift</title>
      <link>https://arxiv.org/abs/2405.09331</link>
      <description>arXiv:2405.09331v1 Announce Type: new 
Abstract: Recent years have experienced increasing utilization of complex machine learning models across multiple sources of data to inform more generalizable decision-making. However, distribution shifts across data sources and privacy concerns related to sharing individual-level data, coupled with a lack of uncertainty quantification from machine learning predictions, make it challenging to achieve valid inferences in multi-source environments. In this paper, we consider the problem of obtaining distribution-free prediction intervals for a target population, leveraging multiple potentially biased data sources. We derive the efficient influence functions for the quantiles of unobserved outcomes in the target and source populations, and show that one can incorporate machine learning prediction algorithms in the estimation of nuisance functions while still achieving parametric rates of convergence to nominal coverage probabilities. Moreover, when conditional outcome invariance is violated, we propose a data-adaptive strategy to upweight informative data sources for efficiency gain and downweight non-informative data sources for bias reduction. We highlight the robustness and efficiency of our proposals for a variety of conformal scores and data-generating mechanisms via extensive synthetic experiments. Hospital length of stay prediction intervals for pediatric patients undergoing a high-risk cardiac surgical procedure between 2016-2022 in the U.S. illustrate the utility of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09331v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Liu, Alexander W. Levis, Sharon-Lise Normand, Larry Han</dc:creator>
    </item>
    <item>
      <title>Predicting Future Change-points in Time Series</title>
      <link>https://arxiv.org/abs/2405.09485</link>
      <description>arXiv:2405.09485v1 Announce Type: new 
Abstract: Change-point detection and estimation procedures have been widely developed in the literature. However, commonly used approaches in change-point analysis have mainly been focusing on detecting change-points within an entire time series (off-line methods), or quickest detection of change-points in sequentially observed data (on-line methods). Both classes of methods are concerned with change-points that have already occurred. The arguably more important question of when future change-points may occur, remains largely unexplored. In this paper, we develop a novel statistical model that describes the mechanism of change-point occurrence. Specifically, the model assumes a latent process in the form of a random walk driven by non-negative innovations, and an observed process which behaves differently when the latent process belongs to different regimes. By construction, an occurrence of a change-point is equivalent to hitting a regime threshold by the latent process. Therefore, by predicting when the latent process will hit the next regime threshold, future change-points can be forecasted. The probabilistic properties of the model such as stationarity and ergodicity are established. A composite likelihood-based approach is developed for parameter estimation and model selection. Moreover, we construct the predictor and prediction interval for future change points based on the estimated model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09485v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chak Fung Choi, Chunxue Li, Chun Yip Yau, Zifeng Zhao</dc:creator>
    </item>
    <item>
      <title>Wasserstein Gradient Boosting: A General Framework with Applications to Posterior Regression</title>
      <link>https://arxiv.org/abs/2405.09536</link>
      <description>arXiv:2405.09536v1 Announce Type: new 
Abstract: Gradient boosting is a sequential ensemble method that fits a new base learner to the gradient of the remaining loss at each step. We propose a novel family of gradient boosting, Wasserstein gradient boosting, which fits a new base learner to an exactly or approximately available Wasserstein gradient of a loss functional on the space of probability distributions. Wasserstein gradient boosting returns a set of particles that approximates a target probability distribution assigned at each input. In probabilistic prediction, a parametric probability distribution is often specified on the space of output variables, and a point estimate of the output-distribution parameter is produced for each input by a model. Our main application of Wasserstein gradient boosting is a novel distributional estimate of the output-distribution parameter, which approximates the posterior distribution over the output-distribution parameter determined pointwise at each data point. We empirically demonstrate the superior performance of the probabilistic prediction by Wasserstein gradient boosting in comparison with various existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09536v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takuo Matsubara</dc:creator>
    </item>
    <item>
      <title>Properties of stationary cyclical processes</title>
      <link>https://arxiv.org/abs/2405.08907</link>
      <description>arXiv:2405.08907v1 Announce Type: cross 
Abstract: The paper investigates the theoretical properties of zero-mean stationary time series with cyclical components, admitting the representation $y_t=\alpha_t \cos \lambda t + \beta_t \sin \lambda t$, with $\lambda \in (0,\pi]$ and $[\alpha_t\,\, \beta_t]$ following some bivariate process. We diagnose that in the extant literature on cyclic time series, a prevalent assumption of Gaussianity for $[\alpha_t\,\, \beta_t]$ imposes inadvertently a severe restriction on the amplitude of the process. Moreover, it is shown that other common distributions may suffer from either similar defects or fail to guarantee the stationarity of $y_t$. To address both of the issues, we propose to introduce a direct stochastic modulation of the amplitude and phase shift in an almost periodic function. We prove that this novel approach may lead, in general, to a stationary (up to any order) time series, and specifically, to a zero-mean stationary time series featuring cyclicity, with a pseudo-cyclical autocovariance function that may even decay at a very slow rate. The proposed process fills an important gap in this type of models and allows for flexible modeling of amplitude and phase shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08907v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz Lenart</dc:creator>
    </item>
    <item>
      <title>Enhancing Airline Customer Satisfaction: A Machine Learning and Causal Analysis Approach</title>
      <link>https://arxiv.org/abs/2405.09076</link>
      <description>arXiv:2405.09076v1 Announce Type: cross 
Abstract: This study explores the enhancement of customer satisfaction in the airline industry, a critical factor for retaining customers and building brand reputation, which are vital for revenue growth. Utilizing a combination of machine learning and causal inference methods, we examine the specific impact of service improvements on customer satisfaction, with a focus on the online boarding pass experience. Through detailed data analysis involving several predictive and causal models, we demonstrate that improvements in the digital aspects of customer service significantly elevate overall customer satisfaction. This paper highlights how airlines can strategically leverage these insights to make data-driven decisions that enhance customer experiences and, consequently, their market competitiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09076v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tejas Mirthipati (Georgia Institute Of Technology)</dc:creator>
    </item>
    <item>
      <title>A new set of tools for goodness-of-fit validation</title>
      <link>https://arxiv.org/abs/2209.07295</link>
      <description>arXiv:2209.07295v2 Announce Type: replace 
Abstract: We introduce two new tools to assess the validity of statistical distributions. These tools are based on components derived from a new statistical quantity, the $comparison$ $curve$. The first tool is a graphical representation of these components on a $bar$ $plot$ (B plot), which can provide a detailed appraisal of the validity of the statistical model, in particular when supplemented by acceptance regions related to the model. The knowledge gained from this representation can sometimes suggest an existing $goodness$-$of$-$fit$ test to supplement this visual assessment with a control of the type I error. Otherwise, an adaptive test may be preferable and the second tool is the combination of these components to produce a powerful $\chi^2$-type goodness-of-fit test. Because the number of these components can be large, we introduce a new selection rule to decide, in a data driven fashion, on their proper number to take into consideration. In a simulation, our goodness-of-fit tests are seen to be powerwise competitive with the best solutions that have been recommended in the context of a fully specified model as well as when some parameters must be estimated. Practical examples show how to use these tools to derive principled information about where the model departs from the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.07295v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gilles R. Ducharme, Teresa Ledwina</dc:creator>
    </item>
    <item>
      <title>Powerful Partial Conjunction Hypothesis Testing via Conditioning</title>
      <link>https://arxiv.org/abs/2212.11304</link>
      <description>arXiv:2212.11304v3 Announce Type: replace 
Abstract: A Partial Conjunction Hypothesis (PCH) test combines information across a set of base hypotheses to determine whether some subset is non-null. PCH tests arise in a diverse array of fields, but standard PCH testing methods can be highly conservative, leading to low power especially in low signal settings commonly encountered in applications. In this paper, we introduce the conditional PCH (cPCH) test, a new method for testing a single PCH that directly corrects the conservativeness of standard approaches by conditioning on certain order statistics of the base p-values. Under distributional assumptions commonly encountered in PCH testing, the cPCH test is valid and produces nearly uniformly distributed p-values under the null (i.e., cPCH p-values are only very slightly conservative). We demonstrate that the cPCH test matches or outperforms existing single PCH tests with particular power gains in low signal settings, maintains Type I error control even under model misspecification, and can be used to outperform state-of-the-art multiple PCH testing procedures in certain settings, particularly when side information is present. Finally, we illustrate an application of the cPCH test through a replicability analysis across DNA microarray studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.11304v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Biyonka Liang, Lu Zhang, Lucas Janson</dc:creator>
    </item>
    <item>
      <title>Semiparametrically Efficient Score for the Survival Odds Ratio</title>
      <link>https://arxiv.org/abs/2310.14448</link>
      <description>arXiv:2310.14448v2 Announce Type: replace 
Abstract: We consider a general proportional odds model for survival data under binary treatment, where the functional form of the covariates is left unspecified. We derive the efficient score for the conditional survival odds ratio given the covariates using modern semiparametric theory. The efficient score may be useful in the development of doubly robust estimators, although computational challenges remain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14448v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Denise Rava, Jelena Bradic, Ronghui Xu</dc:creator>
    </item>
    <item>
      <title>A review of regularised estimation methods and cross-validation in spatiotemporal statistics</title>
      <link>https://arxiv.org/abs/2402.00183</link>
      <description>arXiv:2402.00183v2 Announce Type: replace 
Abstract: This review article focuses on regularised estimation procedures applicable to geostatistical and spatial econometric models. These methods are particularly relevant in the case of big geospatial data for dimensionality reduction or model selection. To structure the review, we initially consider the most general case of multivariate spatiotemporal processes (i.e., $g &gt; 1$ dimensions of the spatial domain, a one-dimensional temporal domain, and $q \geq 1$ random variables). Then, the idea of regularised/penalised estimation procedures and different choices of shrinkage targets are discussed. Finally, guided by the elements of a mixed-effects model setup, which allows for a variety of spatiotemporal models, we show different regularisation procedures and how they can be used for the analysis of geo-referenced data, e.g. for selection of relevant regressors, dimensionality reduction of the covariance matrices, detection of conditionally independent locations, or the estimation of a full spatial interaction matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00183v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Otto, Alessandro Fass\`o, Paolo Maranzano</dc:creator>
    </item>
    <item>
      <title>On foundation of generative statistics with F-entropy: a gradient-based approach</title>
      <link>https://arxiv.org/abs/2405.05389</link>
      <description>arXiv:2405.05389v4 Announce Type: replace 
Abstract: This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can {\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g. missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisher's divergence. (The F-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for generative model selection. Possible future projects include discrete data and Bayesian variational inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05389v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bing Cheng, Howell Tong</dc:creator>
    </item>
    <item>
      <title>Classification by sparse generalized additive models</title>
      <link>https://arxiv.org/abs/2212.01792</link>
      <description>arXiv:2212.01792v4 Announce Type: replace-cross 
Abstract: We consider (nonparametric) sparse (generalized) additive models (SpAM) for classification. The design of a SpAM classifier is based on minimizing the logistic loss with a sparse group Lasso/Slope-type penalties on the coefficients of univariate additive components' expansions in orthonormal series (e.g., Fourier or wavelets). The resulting classifier is inherently adaptive to the unknown sparsity and smoothness. We show that under certain sparse group restricted eigenvalue condition it is nearly-minimax (up to log-factors) simultaneously across the entire range of analytic, Sobolev and Besov classes. The performance of the proposed classifier is illustrated on a simulated and a real-data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.01792v4</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Felix Abramovich</dc:creator>
    </item>
    <item>
      <title>Asymptotically Unbiased Synthetic Control Methods by Distribution Matching</title>
      <link>https://arxiv.org/abs/2307.11127</link>
      <description>arXiv:2307.11127v3 Announce Type: replace-cross 
Abstract: Synthetic Control Methods (SCMs) have become an essential tool for comparative case studies. The fundamental idea of SCMs is to estimate the counterfactual outcomes of a treated unit using a weighted sum of the observed outcomes of untreated units. The accuracy of the synthetic control (SC) is critical for evaluating the treatment effect of a policy intervention; therefore, the estimation of SC weights has been the focus of extensive research. In this study, we first point out that existing SCMs suffer from an endogeneity problem, the correlation between the outcomes of untreated units and the error term of the synthetic control, which yields a bias in the treatment effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the joint density of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matching the moments of treated outcomes with the weighted sum of moments of untreated outcomes. Our proposed method has three advantages over existing methods: first, our estimator is asymptotically unbiased under the assumption of the mixture model; second, due to the asymptotic unbiasedness, we can reduce the mean squared error in counterfactual predictions; third, our method generates full densities of the treatment effect, not merely expected values, which broadens the applicability of SCMs. We provide experimental results to demonstrate the effectiveness of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11127v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato, Akari Ohda, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>A new adaptive local polynomial density estimation procedure on complicated domains</title>
      <link>https://arxiv.org/abs/2308.01156</link>
      <description>arXiv:2308.01156v2 Announce Type: replace-cross 
Abstract: This paper presents a novel approach for pointwise estimation of multivariate density functions on known domains of arbitrary dimensions using nonparametric local polynomial estimators. Our method is highly flexible, as it applies to both simple domains, such as open connected sets, and more complicated domains that are not star-shaped around the point of estimation. This enables us to handle domains with sharp concavities, holes, and local pinches, such as polynomial sectors. Additionally, we introduce a data-driven selection rule based on the general ideas of Goldenshluger and Lepski. Our results demonstrate that the local polynomial estimators are minimax under a $L^2$ risk across a wide range of H\"older-type functional classes. In the adaptive case, we provide oracle inequalities and explicitly determine the convergence rate of our statistical procedure. Simulations on polynomial sectors show that our oracle estimates outperform those of the most popular alternative method, found in the sparr package for the R software. Our statistical procedure is implemented in an online R package which is readily accessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01156v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karine Bertin, Nicolas Klutchnikoff, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach</title>
      <link>https://arxiv.org/abs/2402.01454</link>
      <description>arXiv:2402.01454v2 Announce Type: replace-cross 
Abstract: In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)'' for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, by using an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve SCD on this dataset, even if this dataset has never been included in the training data of the LLM. The proposed approach can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01454v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masayuki Takayama, Tadahisa Okuda, Thong Pham, Tatsuyoshi Ikenoue, Shingo Fukuma, Shohei Shimizu, Akiyoshi Sannai</dc:creator>
    </item>
  </channel>
</rss>
