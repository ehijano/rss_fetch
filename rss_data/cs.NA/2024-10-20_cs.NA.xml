<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Oct 2024 04:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Stochastic Convergence Analysis of Inverse Potential Problem</title>
      <link>https://arxiv.org/abs/2410.14106</link>
      <description>arXiv:2410.14106v1 Announce Type: new 
Abstract: In this work, we investigate the inverse problem of recovering a potential in an elliptic problem from random pointwise observations in the domain. We employ a regularized output-least squares formulation with an $H^1(\Omega)$ penalty for the numerical reconstruction, and the Galerkin finite element method for the spatial discretization. Under mild regularity assumptions on the problem data, we provide a stochastic $L^2(\Omega)$ convergence analysis on the regularized solution and the finite element approximation in a high probability sense. The obtained error bounds depend explicitly on the regularization parameter $\gamma$, the number $n$ of observation points and the mesh size $h$. These estimates provide a useful guideline for choosing relevant algorithmic parameters. Furthermore, we develop a monotonically convergent adaptive algorithm for determining a suitable regularization parameter in the absence of \textit{a priori} knowledge. Numerical experiments are provided to complement the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14106v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bangti Jin, Qimeng Quan, Wenlong Zhang</dc:creator>
    </item>
    <item>
      <title>A hybrid approach for singularly perturbed parabolic problem with discontinuous data</title>
      <link>https://arxiv.org/abs/2410.14125</link>
      <description>arXiv:2410.14125v1 Announce Type: new 
Abstract: In this article, we study a two-dimensional singularly perturbed parabolic equation of the convection-diffusion type, characterized by discontinuities in the source term and convection coefficient at a specific point in the domain. These discontinuities lead to the development of interior layers. To address these layers and ensure uniform convergence, we propose a hybrid monotone difference scheme that combines the central difference and midpoint upwind schemes for spatial discretization, applied on a piecewise-uniform Shishkin mesh. For temporal discretization, we employ the Crank-Nicolson method on a uniform mesh. The resulting scheme is proven to be uniformly convergent, order achieving almost two in space and two in time. Numerical experiments validate the theoretical error estimates, demonstrating superior accuracy and convergence when compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14125v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nirmali Roy, Anuradha Jha</dc:creator>
    </item>
    <item>
      <title>Fine-Tuning DeepONets to Enhance Physics-informed Neural Networks for solving Partial Differential Equations</title>
      <link>https://arxiv.org/abs/2410.14134</link>
      <description>arXiv:2410.14134v1 Announce Type: new 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as powerful tools for solving partial differential equations (PDEs). However, training PINNs from scratch is often computationally intensive and time-consuming. To address this problem, we propose a parameter-efficient approach that fine-tunes pre-trained DeepONet models within the PINN framework (FTO-PINN), enabling more efficient meshless PDE solving. Specifically, we freeze the weights of the pre-trained DeepONet model and fine-tune the output of the branch net by incorporating a small number of new trainable parameters, which can be quickly determined using least-squares techniques. Additionally, we introduce trunk net expansions and low-rank adaptation strategies to further enhance the performance of FTO-PINN. The effectiveness of our proposed method is demonstrated through a series of numerical experiments across various types of PDEs. FTO-PINN significantly reduces the training time of vanilla PINNs while maintaining comparable accuracy, and outperforms DeepONet, which is pre-trained on general function data, in both fidelity and generalization capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14134v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sidi Wu</dc:creator>
    </item>
    <item>
      <title>Intrinsic mixed finite element methods for linear Cosserat elasticity and couple stress problem</title>
      <link>https://arxiv.org/abs/2410.14176</link>
      <description>arXiv:2410.14176v1 Announce Type: new 
Abstract: We propose two parameter-robust mixed finite element methods for linear Cosserat elasticity. The Cosserat coupling constant $\mu_c$, connecting the displacement $u$ and rotation vector $\omega$, leads to possible locking phenomena in finite element methods. The formal limit of $\mu_c\to\infty$ enforces the constraint $\frac{1}{2}\operatorname{curl}u = \omega$ and leads to the fourth order couple stress problem. Viewing the linear Cosserat model as the Hodge-Laplacian problem of a twisted de Rham complex, we derive structure-preserving distributional finite element spaces, where the limit constraint is fulfilled in the discrete setting. Applying the mass conserving mixed stress (MCS) method for the rotations, the resulting scheme is robust in $\mu_c$. Combining it with the tangential-displacement normal-normal-stress (TDNNS) method for the displacement part we obtain additional robustness in the nearly incompressible regime and for anisotropic structures. Using a post-processing scheme for the rotations, we prove optimal convergence rates independent of the Cosserat coupling constant $\mu_c$. Further, we propose a mixed method for the couple stress problem based on the MCS scheme. We demonstrate the performance of the proposed methods in several numerical benchmark examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14176v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Dziubek, Kaibo Hu, Michael Karow, Michael Neunteufel</dc:creator>
    </item>
    <item>
      <title>Physics Informed Neural Networks for heat conduction with phase change</title>
      <link>https://arxiv.org/abs/2410.14216</link>
      <description>arXiv:2410.14216v1 Announce Type: new 
Abstract: We study numerical algorithms to solve a specific Partial Differential Equation (PDE), namely the Stefan problem, using Physics Informed Neural Networks (PINNs). This problem describes the heat propagation in a liquid-solid phase change system. It implies a heat equation and a discontinuity at the interface where the phase change occurs. In the context of PINNs, this model leads to difficulties in the learning process, especially near the interface of phase change. We present different strategies that can be used in this context. We illustrate our results and compare with classical solvers for PDEs (finite differences).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14216v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bahae-Eddine Madir (LMRS), Francky Luddens (LMRS), Corentin Lothod\'e (LAREMA), Ionut Danaila (LMRS)</dc:creator>
    </item>
    <item>
      <title>Mixed finite element projection methods for the unsteady Stokes equations</title>
      <link>https://arxiv.org/abs/2410.14266</link>
      <description>arXiv:2410.14266v1 Announce Type: new 
Abstract: We develop $H$(div)-conforming mixed finite element methods for the unsteady Stokes equations modeling single-phase incompressible fluid flow. A projection method in the framework of the incremental pressure correction methodology is applied, where a predictor and a corrector problems are sequentially solved, accounting for the viscous effects and incompressibility, respectively. The predictor problem is based on a stress-velocity mixed formulation, while the corrector projection problem uses a velocity-pressure mixed formulation. The scheme results in pointwise divergence-free velocity computed at the end of each time step. We establish unconditional stability and first order in time accuracy. In the implementation we focus on generally unstructured triangular grids. We employ a second order multipoint flux mixed finite element method based on the next-to-the-lowest order Raviart-Thomas space $RT_1$ and a suitable quadrature rule. In the predictor problem this approach allows for a local stress elimination, resulting in element-based systems for each velocity component with three degrees of freedom per element. Similarly, in the corrector problem, the velocity is locally eliminated and an element-based system for the pressure is solved. At the end of each time step we obtain a second order accurate $H$(div)-conforming piecewise linear velocity, which is pointwise divergence free. We present a series of numerical tests to illustrate the performance of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14266v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Costanza Aric\`o, Rainer Helmig, Ivan Yotov</dc:creator>
    </item>
    <item>
      <title>Bound preserving Point-Average-Moment PolynomiAl-interpreted (PAMPA) scheme: one-dimensional case</title>
      <link>https://arxiv.org/abs/2410.14292</link>
      <description>arXiv:2410.14292v1 Announce Type: new 
Abstract: We propose a bound preserving Point-Average-Moment PolynomiAl-interpreted (PAMPA) method by blending the third-order construction and first-order construction. The originality of the present construction is that it does not need any explicit reconstruction within each element, and therefore the construction is very flexible. The construction uses a classical blending approach between a first order bound preserving scheme and a high order scheme that is not bound preserving.
  We show its efficiency on many problems, ranging from scalar to system cases (Euler equations). In both cases, we provide optimal values of the blending parameter. In the system case, we use the recent geometric quasi-linearisation of [Wu and Shu, SIAM Review, 65 (2023), pp. 1031--1073].</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14292v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R\'emi Abgrall, Miaosen Jiao, Yongle Liu, Kailiang Wu</dc:creator>
    </item>
    <item>
      <title>A class of kernel-based scalable algorithms for data science</title>
      <link>https://arxiv.org/abs/2410.14323</link>
      <description>arXiv:2410.14323v1 Announce Type: new 
Abstract: We present several generative and predictive algorithms based on the RKHS (reproducing kernel Hilbert spaces) methodology, which most importantly are scalable in the following sense. It is well recognized that the RKHS methodology leads one to efficient and robust algorithms for numerous tasks in data science, statistics, and scientific computations.However, the standard implementations remain difficult to scale to encompass large data sets. In this paper, we introduce a simple and robust, divide-and-conquer approach which applies to large scale data sets and relies on suitably formulated, kernel-based algorithms: we distinguish between extrapolation, interpolation, and optimal transport steps. We explain how to select the best algorithms in specific applications thanks to some feedback, mainly consisting of perfomance criteria. Our main focus for the applications and challenging problems arising in industrial applications, such as the generation of mesh for efficient numerical simulations, the design of generators of conditional distributions, the transition probability matrix for statistic or stochastic applications, as well as various tasks of interest to the artificial intelligence community. Indeed, the proposed algorithms are relevant for supervised or unsupervised learning, generative methods, and reinforcement learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14323v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe G. LeFloch, Jean-Marc Mercier, Shohruh Miryusupov</dc:creator>
    </item>
    <item>
      <title>Convergence of the Dirichlet-Neumann method for semilinear elliptic equations</title>
      <link>https://arxiv.org/abs/2410.14339</link>
      <description>arXiv:2410.14339v1 Announce Type: new 
Abstract: The Dirichlet-Neumann method is a common domain decomposition method for nonoverlapping domain decomposition and the method has been studied extensively for linear elliptic equations. However, for nonlinear elliptic equations, there are only convergence results for some specific cases in one spatial dimension. The aim of this manuscript is therefore to prove that the Dirichlet-Neumann method converges for a class of semilinear elliptic equations on Lipschitz continuous domains in two and three spatial dimensions. This is achieved by first proving a new result on the convergence of nonlinear iterations in Hilbert spaces and then applying this result to the Steklov-Poincar\'e formulation of the Dirichlet-Neumann method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14339v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emil Engstr\"om</dc:creator>
    </item>
    <item>
      <title>Explicit T -coercivity for the Stokes problem: a coercive finite element discretization</title>
      <link>https://arxiv.org/abs/2410.14444</link>
      <description>arXiv:2410.14444v1 Announce Type: new 
Abstract: Using the T-coercivity theory as advocated in [Chesnel, Ciarlet, T -coercivity and continuous Galerkin methods: application to transmission problems with sign changing coefficients (2013)], we propose a new variational formulation of the Stokes problem which does not involve nonlocal operators. With this new formulation, unstable finite element pairs are stabilized. In addition, the numerical scheme is easy to implement, and a better approximation of the velocity and the pressure is observed numerically when the viscosity is small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14444v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Patrick Ciarlet Jr, Erell Jamelot</dc:creator>
    </item>
    <item>
      <title>A chiseling algorithm for low-rank Grassmann decomposition of skew-symmetric tensors</title>
      <link>https://arxiv.org/abs/2410.14486</link>
      <description>arXiv:2410.14486v1 Announce Type: new 
Abstract: A numerical algorithm to decompose a low-rank skew-symmetric tensor into a sum of elementary (rank-$1$) skew-symmetric tensors is introduced. The algorithm uncovers this Grassmann decomposition based on linear relations that are encoded by the kernel of the differential of the natural action of the general linear group on the tensor, following the ideas of [Brooksbank, Kassabov, and Wilson, Detecting cluster patterns in tensor data, arXiv:2408.17425, 2024]. The Grassmann decomposition can be recovered, up to scale, from a diagonalization of a generic element in this kernel. Numerical experiments illustrate that the algorithm is computationally efficient and quite accurate for mathematically low-rank tensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14486v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick Vannieuwenhoven</dc:creator>
    </item>
    <item>
      <title>Mimetic Metrics for the DGSEM</title>
      <link>https://arxiv.org/abs/2410.14502</link>
      <description>arXiv:2410.14502v1 Announce Type: new 
Abstract: Free-stream preservation is an essential property for numerical solvers on curvilinear grids. Key to this property is that the metric terms of the curvilinear mapping satisfy discrete metric identities, i.e., have zero divergence. Divergence-free metric terms are furthermore essential for entropy stability on curvilinear grids. We present a new way to compute the metric terms for discontinuous Galerkin spectral element methods (DGSEMs) that guarantees they are divergence-free. Our proposed mimetic approach uses projections that fit within the de Rham Cohomology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14502v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Bach, Andr\'es Rueda-Ram\'irez, David A. Kopriva, Gregor J. Gassner</dc:creator>
    </item>
    <item>
      <title>A Localized Orthogonal Decomposition Method for Heterogeneous Stokes Problems</title>
      <link>https://arxiv.org/abs/2410.14514</link>
      <description>arXiv:2410.14514v1 Announce Type: new 
Abstract: In this paper, we propose a multiscale method for heterogeneous Stokes problems. The method is based on the Localized Orthogonal Decomposition (LOD) methodology and has approximation properties independent of the regularity of the coefficients. We apply the LOD to an appropriate reformulation of the Stokes problem, which allows us to construct exponentially decaying basis functions for the velocity approximation while using a piecewise constant pressure approximation. The exponential decay motivates a localization of the basis computation, which is essential for the practical realization of the method. We perform a rigorous a priori error analysis and prove optimal convergence rates for the velocity approximation and a post-processed pressure approximation, provided that the supports of the basis functions are logarithmically increased with the desired accuracy. Numerical experiments support the theoretical results of this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14514v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Moritz Hauck, Alexei Lozinski</dc:creator>
    </item>
    <item>
      <title>Discrete empirical interpolation in the tensor t-product framework</title>
      <link>https://arxiv.org/abs/2410.14519</link>
      <description>arXiv:2410.14519v1 Announce Type: new 
Abstract: The discrete empirical interpolation method (DEIM) is a well-established approach, widely used for state reconstruction using sparse sensor/measurement data, nonlinear model reduction, and interpretable feature selection. We introduce the tensor t-product Q-DEIM (t-Q-DEIM), an extension of the DEIM framework for dealing with tensor-valued data. The proposed approach seeks to overcome one of the key drawbacks of DEIM, viz., the need for matricizing the data, which can distort any structural and/or geometric information. Our method leverages the recently developed tensor t-product algebra to avoid reshaping the data. In analogy with the standard DEIM, we formulate and solve a tensor-valued least-squares problem, whose solution is achieved through an interpolatory projection. We develop a rigorous, computable upper bound for the error resulting from the t-Q-DEIM approximation. Using five different tensor-valued datasets, we numerically illustrate the better approximation properties of t-Q-DEIM and the significant computational cost reduction it offers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14519v1</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sridhar Chellappa, Lihong Feng, Peter Benner</dc:creator>
    </item>
    <item>
      <title>Semi-Implicit Lagrangian Voronoi Approximation for Compressible Viscous Fluid Flows</title>
      <link>https://arxiv.org/abs/2410.14564</link>
      <description>arXiv:2410.14564v1 Announce Type: new 
Abstract: This paper contributes to the recent investigations of Lagrangian methods based on Voronoi meshes. The aim is to design a new conservative numerical scheme that can simulate complex flows and multi-phase problems with more accuracy than SPH (Smoothed Particle Hydrodynamics) methods but, unlike diffuse interface models on fixed grid topology, does not suffer from the deteriorating quality of the computational grid. The numerical solution is stored at particles, which move with the fluid velocity and also play the role of the generators of the computational mesh, that is efficiently re-constructed at each time step. The main novelty stems from combining a Lagrangian Voronoi scheme with a semi-implicit integrator for compressible flows. This allows to model low-Mach number flows without the extremely stringent stability constraint on the time step and with the correct scaling of numerical viscosity. The implicit linear system for the unknown pressure is obtained by splitting the reversible from the irreversible (viscous) part of the dynamics, and then using entropy conservation of the reversible sub-system to derive an auxiliary elliptic equation. The method, called SILVA (Semi-Implicit Lagrangian Voronoi Approximation), is validated in a variety of test cases that feature diverse Mach numbers, shocks and multi-phase flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14564v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ond\v{r}ej Kincl, Ilya Peshkov, Walter Boscheri</dc:creator>
    </item>
    <item>
      <title>Tensor Decomposition with Unaligned Observations</title>
      <link>https://arxiv.org/abs/2410.14046</link>
      <description>arXiv:2410.14046v1 Announce Type: cross 
Abstract: This paper presents a canonical polyadic (CP) tensor decomposition that addresses unaligned observations. The mode with unaligned observations is represented using functions in a reproducing kernel Hilbert space (RKHS). We introduce a versatile loss function that effectively accounts for various types of data, including binary, integer-valued, and positive-valued types. Additionally, we propose an optimization algorithm for computing tensor decompositions with unaligned observations, along with a stochastic gradient method to enhance computational efficiency. A sketching algorithm is also introduced to further improve efficiency when using the $\ell_2$ loss function. To demonstrate the efficacy of our methods, we provide illustrative examples using both synthetic data and an early childhood human microbiome dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14046v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Runshi Tang, Tamara Kolda, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Accelerating operator Sinkhorn iteration with overrelaxation</title>
      <link>https://arxiv.org/abs/2410.14104</link>
      <description>arXiv:2410.14104v1 Announce Type: cross 
Abstract: We propose accelerated versions of the operator Sinkhorn iteration for operator scaling using successive overrelaxation. We analyze the local convergence rates of these accelerated methods via linearization, which allows us to determine the asymptotically optimal relaxation parameter based on Young's SOR theorem. Using the Hilbert metric on positive definite cones, we also obtain a global convergence result for a geodesic version of overrelaxation in a specific range of relaxation parameters. These techniques generalize corresponding results obtained for matrix scaling by Thibault et al. (Algorithms, 14(5):143, 2021) and Lehmann et al. (Optim. Lett., 16(8):2209--2220, 2022). Numerical experiments demonstrate that the proposed methods outperform the original operator Sinkhorn iteration in certain applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14104v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tasuku Soma, Andr\'e Uschmajew</dc:creator>
    </item>
    <item>
      <title>wavScalogram: an R package with wavelet scalogram tools for time series analysis</title>
      <link>https://arxiv.org/abs/2410.14274</link>
      <description>arXiv:2410.14274v1 Announce Type: cross 
Abstract: In this work we present the wavScalogram R package, which contains methods based on wavelet scalograms for time series analysis. These methods are related to two main wavelet tools: the windowed scalogram difference and the scale index. The windowed scalogram difference compares two time series, identifying if their scalograms follow similar patterns at different scales and times, and it is thus a useful complement to other comparison tools such as the squared wavelet coherence. On the other hand, the scale index provides a numerical estimation of the degree of non-periodicity of a time series and it is widely used in many scientific areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14274v1</guid>
      <category>physics.data-an</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>nlin.CD</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.32614/RJ-2022-031</arxiv:DOI>
      <arxiv:journal_reference>The R Journal, 14/2 (2022), 164-185</arxiv:journal_reference>
      <dc:creator>Vicente J. Bolos, Rafael Benitez</dc:creator>
    </item>
    <item>
      <title>Extra-Gradient Method with Flexible Anchoring: Strong Convergence and Fast Residual Decay</title>
      <link>https://arxiv.org/abs/2410.14369</link>
      <description>arXiv:2410.14369v1 Announce Type: cross 
Abstract: In this paper, we introduce a novel Extra-Gradient method with anchor term governed by general parameters. Our method is derived from an explicit discretization of a Tikhonov-regularized monotone flow in Hilbert space, which provides a theoretical foundation for analyzing its convergence properties. We establish strong convergence to specific points within the solution set, as well as convergence rates expressed in terms of the regularization parameters. Notably, our approach recovers the fast residual decay rate $O(k^{-1})$ for standard parameter choices. Numerical experiments highlight the competitiveness of the method and demonstrate how its flexible design enhances practical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14369v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radu Ioan Bo\c{t}, Enis Chenchene</dc:creator>
    </item>
    <item>
      <title>Learning to Control the Smoothness of Graph Convolutional Network Features</title>
      <link>https://arxiv.org/abs/2410.14604</link>
      <description>arXiv:2410.14604v1 Announce Type: cross 
Abstract: The pioneering work of Oono and Suzuki [ICLR, 2020] and Cai and Wang [arXiv:2006.13318] initializes the analysis of the smoothness of graph convolutional network (GCN) features. Their results reveal an intricate empirical correlation between node classification accuracy and the ratio of smooth to non-smooth feature components. However, the optimal ratio that favors node classification is unknown, and the non-smooth features of deep GCN with ReLU or leaky ReLU activation function diminish. In this paper, we propose a new strategy to let GCN learn node features with a desired smoothness -- adapting to data and tasks -- to enhance node classification. Our approach has three key steps: (1) We establish a geometric relationship between the input and output of ReLU or leaky ReLU. (2) Building on our geometric insights, we augment the message-passing process of graph convolutional layers (GCLs) with a learnable term to modulate the smoothness of node features with computational efficiency. (3) We investigate the achievable ratio between smooth and non-smooth feature components for GCNs with the augmented message-passing scheme. Our extensive numerical results show that the augmented message-passing schemes significantly improve node classification for GCN and some related models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14604v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shih-Hsin Wang, Justin Baker, Cory Hauck, Bao Wang</dc:creator>
    </item>
    <item>
      <title>Mean Square Temporal error estimates for the 2D stochastic Navier-Stokes equations with transport noise</title>
      <link>https://arxiv.org/abs/2305.10999</link>
      <description>arXiv:2305.10999v2 Announce Type: replace 
Abstract: We study the 2D Navier-Stokes equation with transport noise subject to periodic boundary conditions. Our main result is an error estimate for the time-discretisation showing a convergence rate of order (up to) 1/2. It holds with respect to mean square error convergence, whereas previously such a rate for the stochastic Navier-Stokes equations was only known with respect to convergence in probability. Our result is based on uniform-in-probability estimates for the continuous as well as the time-discrete solution exploiting the particular structure of the noise.
  Eventually, we perform numerical simulations for the corresponding problem on bounded domains with no-slip boundary conditions. They suggest the same convergence rate as proved for the periodic problem hinging sensitively on the compatibility of the data. We also compare the energy profiles with those for corresponding problems with additive or multiplicative It\^o-type noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.10999v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominic Breit, Thamsanqa Castern Moyo, Andreas Prohl, J\"orn Wichmann</dc:creator>
    </item>
    <item>
      <title>A reduced-order model for segregated fluid-structure interaction solvers based on an ALE approach</title>
      <link>https://arxiv.org/abs/2305.13613</link>
      <description>arXiv:2305.13613v2 Announce Type: replace 
Abstract: This article presents a Galerkin projection model-order reduction approach for segregated fluid-structure interaction in an Arbitrary Lagrangian Eulerian (ALE) approach at low Reynolds number using the Finite Volume Method (FVM). The reduced-order model (ROM) is based on the proper orthogonal decomposition (POD), with a data-driven technique that combines the classical Galerkin projection and radial basis networks. The results show the stability and accuracy of the proposed method with respect to the high-dimensional model by capturing transient flow fields and, more importantly, the forces acting on the moving object. The effectiveness of this approach is demonstrated in the case study of vortex-induced vibrations (VIV) of a cylinder at Reynolds number Re = 200. The mixing up technique results to an accurate algorithm for resolving fluid-structure interaction problems with moving meshes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.13613v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Valentin Nkana Ngan, Giovanni Stabile, Andrea Mola, Gianluigi Rozza</dc:creator>
    </item>
    <item>
      <title>Approximating Sparse Matrices and their Functions using Matrix-vector products</title>
      <link>https://arxiv.org/abs/2310.05625</link>
      <description>arXiv:2310.05625v2 Announce Type: replace 
Abstract: The computation of a matrix function $f(A)$ is an important task in scientific computing appearing in machine learning, network analysis and the solution of partial differential equations. In this work, we use only matrix-vector products $x\mapsto Ax$ to approximate functions of sparse matrices and matrices with similar structures such as sparse matrices $A$ themselves or matrices that have a similar decay property as matrix functions. We show that when $A$ is a sparse matrix with an unknown sparsity pattern, techniques from compressed sensing can be used under natural assumptions. Moreover, if $A$ is a banded matrix then certain deterministic matrix-vector products can efficiently recover the large entries of $f(A)$. We describe an algorithm for each of the two cases and give error analysis based on the decay bound for the entries of $f(A)$. We finish with numerical experiments showing the accuracy of our algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05625v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taejun Park, Yuji Nakatsukasa</dc:creator>
    </item>
    <item>
      <title>Data-driven Morozov regularization of inverse problems</title>
      <link>https://arxiv.org/abs/2310.14290</link>
      <description>arXiv:2310.14290v3 Announce Type: replace 
Abstract: The solution of inverse problems is crucial in various fields such as medicine, biology, and engineering, where one seeks to find a solution from noisy observations. These problems often exhibit non-uniqueness and ill-posedness, resulting in instability under noise with standard methods. To address this, regularization techniques have been developed to balance data fitting and prior information. Recently, data-driven variational regularization methods have emerged, mainly analyzed within the framework of Tikhonov regularization, termed Network Tikhonov (NETT). This paper introduces Morozov regularization combined with a learned regularizer, termed DD-Morozov regularization. Our approach employs neural networks to define non-convex regularizers tailored to training data, enabling a convergence analysis in the non-convex context with noise-dependent regularizers. We also propose a refined training strategy that improves adaptation to ill-posed problems compared to NETT's original strategy, which primarily focuses on addressing non-uniqueness. We present numerical results for attenuation correction in photoacoustic tomography, comparing DD-Morozov regularization with NETT using the same trained regularizer, both with and without an additional total variation regularizer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14290v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Markus Haltmeier, Richard Kowar, Markus Tiefenthaler</dc:creator>
    </item>
    <item>
      <title>Data-Driven Closure Parametrizations with Metrics: Dispersive Transport</title>
      <link>https://arxiv.org/abs/2311.13975</link>
      <description>arXiv:2311.13975v2 Announce Type: replace 
Abstract: This work presents a data-driven framework for multi-scale parametrization of velocity-dependent dispersive transport in porous media. Pore-scale flow and transport simulations are conducted on periodic pore geometries, and volume-averaging is used to isolate dispersive transport, producing parameters for the dispersive closure term at the Representative Elementary Volume (REV) scale. After validation on unit cells with symmetric and asymmetric geometries, a convolutional neural network (CNN) is trained to predict dispersivity directly from pore-geometry images. Descriptive metrics are also introduced to better understand the parameter space and are used to build a neural network that predicts dispersivity based solely on these metrics. While the models predict longitudinal dispersivity well, transversal dispersivity remains difficult to capture, likely requiring more advanced models to fully describe pore-scale transversal dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13975v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edward Coltman, Martin Schneider, Rainer Helmig</dc:creator>
    </item>
    <item>
      <title>Convergence and stability of randomized implicit two-stage Runge-Kutta schemes</title>
      <link>https://arxiv.org/abs/2404.19059</link>
      <description>arXiv:2404.19059v2 Announce Type: replace 
Abstract: We randomize the implicit two-stage Runge-Kutta scheme to improve the rate of convergence (compared to a deterministic scheme) and stability of the approximate solution (contrasted to the solution generated by the explicit scheme). For stability analysis, we use Dahlquist's concept of A-stability, adopted to randomized schemes by considering three notions of stability: asymptotic, mean-square, and in probability. The randomized implicit RK2 scheme is A-stable asymptotically and in probability but not in the mean-square sense.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19059v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomasz Bochacik, Pawe{\l} Przyby{\l}owicz, {\L}ukasz St\k{e}pie\'n</dc:creator>
    </item>
    <item>
      <title>Partial-differential-algebraic equations of nonlinear dynamics by Physics-Informed Neural-Network: (I) Operator splitting and framework assessment</title>
      <link>https://arxiv.org/abs/2408.01914</link>
      <description>arXiv:2408.01914v3 Announce Type: replace 
Abstract: Several forms for constructing novel physics-informed neural-networks (PINN) for the solution of partial-differential-algebraic equations based on derivative operator splitting are proposed, using the nonlinear Kirchhoff rod as a prototype for demonstration. The open-source DeepXDE is likely the most well documented framework with many examples. Yet, we encountered some pathological problems and proposed novel methods to resolve them. Among these novel methods are the PDE forms, which evolve from the lower-level form with fewer unknown dependent variables to higher-level form with more dependent variables, in addition to those from lower-level forms. Traditionally, the highest-level form, the balance-of-momenta form, is the starting point for (hand) deriving the lowest-level form through a tedious (and error prone) process of successive substitutions. The next step in a finite element method is to discretize the lowest-level form upon forming a weak form and linearization with appropriate interpolation functions, followed by their implementation in a code and testing. The time-consuming tedium in all of these steps could be bypassed by applying the proposed novel PINN directly to the highest-level form. We developed a script based on JAX. While our JAX script did not show the pathological problems of DDE-T (DDE with TensorFlow backend), it is slower than DDE-T. That DDE-T itself being more efficient in higher-level form than in lower-level form makes working directly with higher-level form even more attractive in addition to the advantages mentioned further above. Since coming up with an appropriate learning-rate schedule for a good solution is more art than science, we systematically codified in detail our experience running optimization through a normalization/standardization of the network-training process so readers can reproduce our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01914v3</guid>
      <category>math.NA</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1002/nme.7586</arxiv:DOI>
      <arxiv:journal_reference>International Journal for Numerical Methods in Engineering, 2024;e7586</arxiv:journal_reference>
      <dc:creator>Loc Vu-Quoc, Alexander Humer</dc:creator>
    </item>
    <item>
      <title>DeepTV: A neural network approach for total variation minimization</title>
      <link>https://arxiv.org/abs/2409.05569</link>
      <description>arXiv:2409.05569v2 Announce Type: replace 
Abstract: Neural network approaches have been demonstrated to work quite well to solve partial differential equations in practice. In this context approaches like physics-informed neural networks and the Deep Ritz method have become popular. In this paper, we propose a similar approach to solve an infinite-dimensional total variation minimization problem using neural networks. We illustrate that the resulting neural network problem does not have a solution in general. To circumvent this theoretic issue, we consider an auxiliary neural network problem, which indeed has a solution, and show that it converges in the sense of $\Gamma$-convergence to the original problem. For computing a numerical solution we further propose a discrete version of the auxiliary neural network problem and again show its $\Gamma$-convergence to the original infinite-dimensional problem. In particular, the $\Gamma$-convergence proof suggests a particular discretization of the total variation. Moreover, we connect the discrete neural network problem to a finite difference discretization of the infinite-dimensional total variation minimization problem. Numerical experiments are presented supporting our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05569v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andreas Langer, Sara Behnamian</dc:creator>
    </item>
    <item>
      <title>A novel Mortar Method Integration using Radial Basis Functions</title>
      <link>https://arxiv.org/abs/2409.11735</link>
      <description>arXiv:2409.11735v2 Announce Type: replace 
Abstract: The growing availability of computational resources has significantly increased the interest of the scientific community in performing complex multi-physics and multi-domain simulations. However, the generation of appropriate computational grids for such problems often remains one of the main bottlenecks. The use of a domain partitioning with non-conforming grids is a possible solution, which, however, requires the development of robust and efficient inter-grid interpolation operators to transfer a scalar or a vector field from one domain to another. This work presents a novel approach for interpolating quantities across non-conforming meshes within the framework of the classical mortar method, where weak continuity conditions are enforced. The key contribution is the introduction of a novel strategy that uses mesh-free Radial Basis Function (RBF) interpolations to compute the mortar integral, offering a compelling alternative to traditional projection-based methods. We propose an efficient algorithm tailored for complex three-dimensional settings allowing for potentially significant savings in the overall computational cost and ease of implementation, with no detrimental effects on the numerical accuracy. The formulation, analysis, and validation of the proposed RBF-based algorithm is discussed with the aid of a set of numerical examples, demonstrating its effectiveness. Furthermore, the details of the implementation are discussed and a test case involving a complex geometry is presented, to illustrate the applicability and advantages of our approach in real-world problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11735v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniele Moretto, Andrea Franceschini, Massimiliano Ferronato</dc:creator>
    </item>
    <item>
      <title>Data Complexity Estimates for Operator Learning</title>
      <link>https://arxiv.org/abs/2405.15992</link>
      <description>arXiv:2405.15992v2 Announce Type: replace-cross 
Abstract: Operator learning has emerged as a new paradigm for the data-driven approximation of nonlinear operators. Despite its empirical success, the theoretical underpinnings governing the conditions for efficient operator learning remain incomplete. The present work develops theory to study the data complexity of operator learning, complementing existing research on the parametric complexity. We investigate the fundamental question: How many input/output samples are needed in operator learning to achieve a desired accuracy $\epsilon$? This question is addressed from the point of view of $n$-widths, and this work makes two key contributions. The first contribution is to derive lower bounds on $n$-widths for general classes of Lipschitz and Fr\'echet differentiable operators. These bounds rigorously demonstrate a ``curse of data-complexity'', revealing that learning on such general classes requires a sample size exponential in the inverse of the desired accuracy $\epsilon$. The second contribution of this work is to show that ``parametric efficiency'' implies ``data efficiency''; using the Fourier neural operator (FNO) as a case study, we show rigorously that on a narrower class of operators, efficiently approximated by FNO in terms of the number of tunable parameters, efficient operator learning is attainable in data complexity as well. Specifically, we show that if only an algebraically increasing number of tunable parameters is needed to reach a desired approximation accuracy, then an algebraically bounded number of data samples is also sufficient to achieve the same accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15992v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikola B. Kovachki, Samuel Lanthaler, Hrushikesh Mhaskar</dc:creator>
    </item>
    <item>
      <title>A Mixed Tree-Cotree Gauge for the Reduced Basis Approximation of Maxwell's Eigenvalue Problem</title>
      <link>https://arxiv.org/abs/2406.11276</link>
      <description>arXiv:2406.11276v2 Announce Type: replace-cross 
Abstract: Model order reduction methods are a powerful tool to drastically reduce the computational effort of problems which need to be evaluated repeatedly, i.e., when computing the same system for various parameter values. When applying a reduced basis approximation algorithm to the Maxwell eigenvalue problem, we encounter spurious solutions in the reduced system which hence need to be removed during the basis construction. In this paper, we discuss two tree-cotree gauge-based methods for the removal of the spurious eigenmodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11276v2</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Ziegler, Sebastian Sch\"ops</dc:creator>
    </item>
  </channel>
</rss>
