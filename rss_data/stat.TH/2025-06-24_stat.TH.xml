<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 14:51:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Asymptotic theory for the likelihood-based block maxima method in time series</title>
      <link>https://arxiv.org/abs/2506.17448</link>
      <description>arXiv:2506.17448v1 Announce Type: new 
Abstract: This paper develops a rigorous asymptotic framework for likelihood-based inference in the Block Maxima (BM) method for stationary time series. While Bayesian inference under the BM approach has been widely studied in the independence setting, no asymptotic theory currently exists for time series. Further results are needed to establish that BM method can be applied with the kind of dependent time series models relevant to applied fields. To address this gap we first establish a comprehensive likelihood theory for the misspecified Generalized Extreme Value (GEV) model under serial dependence. Our results include uniform convergence of the empirical log-likelihood process, contraction rates for the Maximum Likelihood Estimator, and a local asymptotically Gaussian expansion. Building on this foundation, we develop the asymptotic theory of Bayesian inference for the GEV parameters, the extremal index, $T$-time-horizon return levels, and extreme quantiles (Value at Risk). Under general conditions on the prior, we prove posterior consistency, $\sqrt{k}$-contraction rates, Bernstein-von Mises theorems, and asymptotic coverage properties for credible intervals. For inference on the extremal index, we propose an adjusted posterior distribution that corrects for poor coverage exhibited by a naive Bayesian approach. Simulations show excellent inferential performances for the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17448v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David L. Carl, Simone A. Padoan, Stefano Rizzelli</dc:creator>
    </item>
    <item>
      <title>Testing Separability of High-Dimensional Covariance Matrices</title>
      <link>https://arxiv.org/abs/2506.17463</link>
      <description>arXiv:2506.17463v1 Announce Type: new 
Abstract: Due to their parsimony, separable covariance models have been popular in modeling matrix-variate data. However, the inference from such a model may be misleading if the population covariance matrix $\Sigma$ is actually non-separable, motivating the use of statistical tests of separability. Likelihood ratio tests have tractable null distributions and good power when the sample size $n$ is larger than the number of variables $p$, but are not well-defined otherwise. Other existing separability tests for the $p&gt;n$ case have low power for small sample sizes, and have null distributions that depend on unknown parameters, preventing exact error rate control. To address these issues, we propose novel invariant tests leveraging the core covariance matrix, a complementary notion to a separable covariance matrix. We show that testing separability of $\Sigma$ is equivalent to testing sphericity of its core component. With this insight, we construct test statistics that are well-defined in high-dimensional settings and have distributions that are invariant under the null hypothesis of separability, allowing for exact simulation of null distributions. We study asymptotic null distributions and prove consistency of our tests in a $p/n\rightarrow\gamma\in(0,\infty)$ asymptotic regime. The large power of our proposed tests relative to existing procedures is demonstrated numerically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17463v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bongjung Sung, Peter D. Hoff</dc:creator>
    </item>
    <item>
      <title>Detection and Reconstruction of a Random Hypergraph from Noisy Graph Projection</title>
      <link>https://arxiv.org/abs/2506.17527</link>
      <description>arXiv:2506.17527v1 Announce Type: new 
Abstract: For a $d$-uniform random hypergraph on $n$ vertices in which hyperedges are included i.i.d.\ so that the average degree in the hypergraph is $n^{\delta+o(1)}$, the projection of such a hypergraph is a graph on the same $n$ vertices where an edge connects two vertices if and only if they belong to a same hyperedge. In this work, we study the inference problem where the observation is a \emph{noisy} version of the graph projection where each edge in the projection is kept with probability $p=n^{-1+\alpha+o(1)}$ and each edge not in the projection is added with probability $q=n^{-1+\beta+o(1)}$. For all constant $d$, we establish sharp thresholds for both detection (distinguishing the noisy projection from an Erd\H{o}s-R\'enyi random graph with edge density $q$) and reconstruction (estimating the original hypergraph). Notably, our results reveal a \emph{detection-reconstruction gap} phenomenon in this problem. Our work also answers a problem raised in \cite{BGPY25+}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17527v1</guid>
      <category>math.ST</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyang Gong, Zhangsong Li, Qiheng Xu</dc:creator>
    </item>
    <item>
      <title>Estimating quantile treatments without strict overlap</title>
      <link>https://arxiv.org/abs/2506.18215</link>
      <description>arXiv:2506.18215v1 Announce Type: new 
Abstract: We consider the problem of estimating quantile treatment effects without assuming strict overlap , i.e., we do not assume that the propensity score is bounded away from zero. More specifically, we consider an inverse probability weighting (IPW) approach for estimating quantiles in the potential outcomes framework and pay special attention to scenarios where the propensity scores can tend to zero as a regularly varying function. Our approach effectively considers a heavy-tailed objective function for estimating the quantile process. We introduce a truncated IPW estimator that is shown to outperform the standard quantile IPW estimator when strict overlap does not hold. We show that the limiting distribution of the estimated quantile process follows a stable distribution and converges at the rate $n^{1-1/\gamma}$, where $\gamma&gt;1$ is the tail index of the propensity scores when they tend to zero. We illustrate the performance of our estimators in numerical experiments and in a dataset that exhibits the presence of extreme propensity scores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18215v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Avella-Medina, Richard Davis, Gennady Samorodnitsky</dc:creator>
    </item>
    <item>
      <title>Gaussian Processes and Reproducing Kernels: Connections and Equivalences</title>
      <link>https://arxiv.org/abs/2506.17366</link>
      <description>arXiv:2506.17366v1 Announce Type: cross 
Abstract: This monograph studies the relations between two approaches using positive definite kernels: probabilistic methods using Gaussian processes, and non-probabilistic methods using reproducing kernel Hilbert spaces (RKHS). They are widely studied and used in machine learning, statistics, and numerical analysis. Connections and equivalences between them are reviewed for fundamental topics such as regression, interpolation, numerical integration, distributional discrepancies, and statistical dependence, as well as for sample path properties of Gaussian processes. A unifying perspective for these equivalences is established, based on the equivalence between the Gaussian Hilbert space and the RKHS. The monograph serves as a basis to bridge many other methods based on Gaussian processes and reproducing kernels, which are developed in parallel by the two research communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17366v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, Bharath K. Sriperumbudur</dc:creator>
    </item>
    <item>
      <title>The Zeta Tail Distribution: A Novel Event-Count Model</title>
      <link>https://arxiv.org/abs/2506.17496</link>
      <description>arXiv:2506.17496v1 Announce Type: cross 
Abstract: We introduce the Zeta Tail(a) probability distribution as a new model for random damage-event counts in risk analysis. Although readily motivated through a natural relationship with the Geometric(p) distribution, Zeta Tail(a) has received little attention in the scholarly literature. In the present work, we begin by deriving various fundamental properties of this novel distribution. We then assess its usefulness as an alternative to Geometric(p) for event-count data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17496v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Powers</dc:creator>
    </item>
    <item>
      <title>Efficient Difference-in-Differences and Event Study Estimators</title>
      <link>https://arxiv.org/abs/2506.17729</link>
      <description>arXiv:2506.17729v1 Announce Type: cross 
Abstract: This paper investigates efficient Difference-in-Differences (DiD) and Event Study (ES) estimation using short panel data sets within the heterogeneous treatment effect framework, free from parametric functional form assumptions and allowing for variation in treatment timing. We provide an equivalent characterization of the DiD potential outcome model using sequential conditional moment restrictions on observables, which shows that the DiD identification assumptions typically imply nonparametric overidentification restrictions. We derive the semiparametric efficient influence function (EIF) in closed form for DiD and ES causal parameters under commonly imposed parallel trends assumptions. The EIF is automatically Neyman orthogonal and yields the smallest variance among all asymptotically normal, regular estimators of the DiD and ES parameters. Leveraging the EIF, we propose simple-to-compute efficient estimators. Our results highlight how to optimally explore different pre-treatment periods and comparison groups to obtain the tightest (asymptotic) confidence intervals, offering practical tools for improving inference in modern DiD and ES applications even in small samples. Calibrated simulations and an empirical application demonstrate substantial precision gains of our efficient estimators in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17729v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaohong Chen, Pedro H. C. Sant'Anna, Haitian Xie</dc:creator>
    </item>
    <item>
      <title>Curse of Dimensionality in Bayesian Model Updating</title>
      <link>https://arxiv.org/abs/2506.17744</link>
      <description>arXiv:2506.17744v1 Announce Type: cross 
Abstract: Bayesian approach provides a coherent framework to address the model updating problem in structural health monitoring. The current practice, however, only focuses on low-dimension model (generally no more than 20 parameters), which limits the accuracy and predictability of the updated model. This paper aims at understanding the curse of dimensionality in Bayesian model updating, and thus proposing feasible strategies to overcome it. An analytical investigation is conducted, which allows us to answer fundamental questions in Bayesian analysis, e.g., where the posterior mass locates and how large of it comparing to the prior volume. The key concept here is the distance from the prior to the posterior, which makes the parameter estimation really difficult in high-dimension problems. In this sense, not only the dimensionality matters, but also the multi-modality, the pronounced degeneracy, and other factors that influence the prior-posterior distance matter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17744v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li Binbin, Liao Zihan</dc:creator>
    </item>
    <item>
      <title>Identifiable Convex-Concave Regression via Sub-gradient Regularised Least Squares</title>
      <link>https://arxiv.org/abs/2506.18078</link>
      <description>arXiv:2506.18078v1 Announce Type: cross 
Abstract: We propose a novel nonparametric regression method that models complex input-output relationships as the sum of convex and concave components. The method-Identifiable Convex-Concave Nonparametric Least Squares (ICCNLS)-decomposes the target function into additive shape-constrained components, each represented via sub-gradient-constrained affine functions. To address the affine ambiguity inherent in convex-concave decompositions, we introduce global statistical orthogonality constraints, ensuring that residuals are uncorrelated with both intercept and input variables. This enforces decomposition identifiability and improves interpretability. We further incorporate L1, L2 and elastic net regularisation on sub-gradients to enhance generalisation and promote structural sparsity. The proposed method is evaluated on synthetic and real-world datasets, including healthcare pricing data, and demonstrates improved predictive accuracy and model simplicity compared to conventional CNLS and difference-of-convex (DC) regression approaches. Our results show that statistical identifiability, when paired with convex-concave structure and sub-gradient regularisation, yields interpretable models suited for forecasting, benchmarking, and policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18078v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Chung</dc:creator>
    </item>
    <item>
      <title>Distributed Poisson multi-Bernoulli filtering via generalised covariance intersection</title>
      <link>https://arxiv.org/abs/2506.18397</link>
      <description>arXiv:2506.18397v1 Announce Type: cross 
Abstract: This paper presents the distributed Poisson multi-Bernoulli (PMB) filter based on the generalised covariance intersection (GCI) fusion rule for distributed multi-object filtering. Since the exact GCI fusion of two PMB densities is intractable, we derive a principled approximation. Specifically, we approximate the power of a PMB density as an unnormalised PMB density, which corresponds to an upper bound of the PMB density. Then, the GCI fusion rule corresponds to the normalised product of two unnormalised PMB densities. We show that the result is a Poisson multi-Bernoulli mixture (PMBM), which can be expressed in closed form. Future prediction and update steps in each filter preserve the PMBM form, which can be projected back to a PMB density before the next fusion step. Experimental results show the benefits of this approach compared to other distributed multi-object filters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18397v1</guid>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Angel F. Garc\'ia-Fern\'andez, Giorgio Battistelli</dc:creator>
    </item>
    <item>
      <title>A Random Matrix Analysis of In-context Memorization for Nonlinear Attention</title>
      <link>https://arxiv.org/abs/2506.18656</link>
      <description>arXiv:2506.18656v1 Announce Type: cross 
Abstract: Attention mechanisms have revolutionized machine learning (ML) by enabling efficient modeling of global dependencies across inputs. Their inherently parallelizable structures allow for efficient scaling with the exponentially increasing size of both pretrained data and model parameters. Yet, despite their central role as the computational backbone of modern large language models (LLMs), the theoretical understanding of Attentions, especially in the nonlinear setting, remains limited.
  In this paper, we provide a precise characterization of the \emph{in-context memorization error} of \emph{nonlinear Attention}, in the high-dimensional proportional regime where the number of input tokens $n$ and their embedding dimension $p$ are both large and comparable. Leveraging recent advances in the theory of large kernel random matrices, we show that nonlinear Attention typically incurs higher memorization error than linear ridge regression on random inputs. However, this gap vanishes, and can even be reversed, when the input exhibits statistical structure, particularly when the Attention weights align with the input signal direction. Our results reveal how nonlinearity and input structure interact with each other to govern the memorization performance of nonlinear Attention. The theoretical insights are supported by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18656v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyu Liao, Jiaqing Liu, TianQi Hou, Difan Zou, Zenan Ling</dc:creator>
    </item>
    <item>
      <title>Asymptotic Expansions of Gaussian and Laguerre Ensembles at the Soft Edge III: Generating Functions</title>
      <link>https://arxiv.org/abs/2506.18673</link>
      <description>arXiv:2506.18673v1 Announce Type: cross 
Abstract: We conclude our work [arXiv:2403.07628, arXiv:2503.12644] on asymptotic expansions at the soft edge for the classical $n$-dimensional Gaussian and Laguerre ensembles, now studying the gap-probability generating functions. We show that the correction terms in the asymptotic expansion are multilinear forms of the higher-order derivatives of the leading-order term, with certain rational polynomial coefficients that are independent of the dummy variable. In this way, the same multilinear structure, with the same polynomial coefficients, is inherited by the asymptotic expansion of any linearly induced quantity such as the distribution of the $k$-th largest level. Whereas the results for the unitary ensembles are presented with proof, the discussion of the orthogonal and symplectic ones is based on some hypotheses. To substantiate the hypotheses, we check the result for the $k$-th largest level in the orthogonal ensembles against simulation data for choices of $n$ and $k$ that require as many as four correction terms to achieve satisfactory accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18673v1</guid>
      <category>math.PR</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Folkmar Bornemann</dc:creator>
    </item>
    <item>
      <title>Likelihood Ratio test for Poisson graph</title>
      <link>https://arxiv.org/abs/2506.18778</link>
      <description>arXiv:2506.18778v1 Announce Type: cross 
Abstract: Directed acyclic graphs are widely used to describe the causal effects among random variables, and the inference of those causal effects has become an popular topic in statistics and machine learning, and has wide applications in neuroinformatics, bioinformatics and so on. However, most studies focus on the estimation or inference of the directional relations among continuous random variables, those among discrete random variables have not gained much attentions. In this article we focus on the inference of directed linkages and directed pathways in a Poisson directed graphical model. We employ likelihood ratio tests subject to non-convex acyclicity constraints, and derive the asymptotic distributions of the test statistic under the null hypothesis is true in high-dimensional situations. The power analysis and simulations suggest that the tests achieve the desired objectives of inference. An analysis of a basketball statistics dataset of NBA players during 2016-2017 season illustrates the utility of the proposed method to infer directed linkages and directed pathways in player's statistics network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18778v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Shuyan, Liu Xin, Wang Shaoli</dc:creator>
    </item>
    <item>
      <title>Stable and consistent density-based clustering via multiparameter persistence</title>
      <link>https://arxiv.org/abs/2005.09048</link>
      <description>arXiv:2005.09048v4 Announce Type: replace 
Abstract: We consider the degree-Rips construction from topological data analysis, which provides a density-sensitive, multiparameter hierarchical clustering algorithm. We analyze its stability to perturbations of the input data using the correspondence-interleaving distance, a metric for hierarchical clusterings that we introduce. Taking certain one-parameter slices of degree-Rips recovers well-known methods for density-based clustering, but we show that these methods are unstable. However, we prove that degree-Rips, as a multiparameter object, is stable, and we propose an alternative approach for taking slices of degree-Rips, which yields a one-parameter hierarchical clustering algorithm with better stability properties. We prove that this algorithm is consistent, using the correspondence-interleaving distance. We provide an algorithm for extracting a single clustering from one-parameter hierarchical clusterings, which is stable with respect to the correspondence-interleaving distance. And, we integrate these methods into a pipeline for density-based clustering, which we call Persistable. Adapting tools from multiparameter persistent homology, we propose visualization tools that guide the selection of all parameters of the pipeline. We demonstrate Persistable on benchmark data sets, showing that it identifies multi-scale cluster structure in data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2005.09048v4</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5555/3722577.3722835</arxiv:DOI>
      <arxiv:journal_reference>Journal of Machine Learning Research, 25(258):1-74, 2024</arxiv:journal_reference>
      <dc:creator>Alexander Rolle, Luis Scoccola</dc:creator>
    </item>
    <item>
      <title>On estimation of skewed stable linear regression</title>
      <link>https://arxiv.org/abs/2404.10448</link>
      <description>arXiv:2404.10448v3 Announce Type: replace 
Abstract: We study the parameter estimation method for linear regression models with possibly skewed stable distributed errors. Our estimation procedure consists of two stages: first, for the regression coefficients, the Cauchy quasi-maximum likelihood estimator (CQMLE) is considered after taking the differences to remove the skewness of noise, and we prove its asymptotic normality and tail-probability estimate; second, as for stable-distribution parameters, we consider the moment estimators based on the symmetrized and centered residuals and prove their $\sqrt{n}$-consistency. To derive the $\sqrt{n}$-consistency, we essentially used the tail-probability estimate of the CQMLE. The proposed estimation procedure has a very low computational load and is much less time-consuming compared with the maximum-likelihood estimator. Further, our estimator can be effectively used as an initial value of the numerical optimization of the log-likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10448v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eitaro Kawamo, Hiroki Masuda</dc:creator>
    </item>
    <item>
      <title>The saddlepoint approximation for averages of conditionally independent random variables</title>
      <link>https://arxiv.org/abs/2407.08915</link>
      <description>arXiv:2407.08915v3 Announce Type: replace 
Abstract: Motivated by the application of saddlepoint approximations to resampling-based statistical tests, we prove that the Lugannani-Rice formula has vanishing relative error when applied to approximate conditional tail probabilities of averages of conditionally independent random variables. In a departure from existing work, this result is valid under only sub-exponential assumptions on the summands, and does not require any assumptions on their smoothness or lattice structure. The derived saddlepoint approximation result can be directly applied to resampling-based hypothesis tests, including bootstrap, sign-flipping and conditional randomization tests. We exemplify this by providing the first rigorous justification of a saddlepoint approximation for the sign-flipping test of symmetry about the origin, initially proposed in 1955. On the way to our main result, we establish a conditional Berry-Esseen inequality for sums of conditionally independent random variables, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08915v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Niu, Jyotishka Ray Choudhury, Eugene Katsevich</dc:creator>
    </item>
    <item>
      <title>Error Bounds for a Kernel-Based Constrained Optimal Smoothing Approximation</title>
      <link>https://arxiv.org/abs/2407.09040</link>
      <description>arXiv:2407.09040v2 Announce Type: replace 
Abstract: This paper establishes error bounds for the convergence of a piecewise linear approximation of the constrained optimal smoothing problem posed in a reproducing kernel Hilbert space (RKHS). This problem can be reformulated as a Bayesian estimation problem involving a Gaussian process related to the kernel of the RKHS. Consequently, error bounds can be interpreted as a quantification of the maximum a posteriori (MAP) accuracy. To our knowledge, no error bounds have been proposed for this type of problem so far. The convergence results are provided as a function of the grid size, the regularity of the kernel, and the distance from the kernel interpolant of the approximation to the set of constraints. Inspired by the MaxMod algorithm from recent literature, which sequentially allocates knots for the piecewise linear approximation, we conduct our analysis for non-equispaced knots. These knots are even allowed to be non-dense, which impacts the definition of the optimal smoothing solution and our error bound quantifiers. Finally, we illustrate our theorems through several numerical experiments involving constraints such as boundedness and monotonicity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09040v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laurence Grammont (UJM, CNRS, ECL, INSA Lyon, UCBL, ICJ), Fran\c{c}ois Bachoc (IMT, RT-UQ), Andr\'es F. L\'opez-Lopera (UPHF, CERAMATHS)</dc:creator>
    </item>
    <item>
      <title>On the pointwise supremum of the set of copulas with a given curvilinear section</title>
      <link>https://arxiv.org/abs/2412.20629</link>
      <description>arXiv:2412.20629v4 Announce Type: replace 
Abstract: Making use of the total variation of particular functions, we give an explicit formula for the pointwise supremum of the set of all copulas with a given curvilinear section. When the pointwise supremum is a copula is characterized. We also characterize the coincidence of the pointwise supremum and the greatest quasi-copula with the same curvilinear section.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20629v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yao Ouyang, Yonghui Sun, Hua-Peng Zhang</dc:creator>
    </item>
    <item>
      <title>Minimax rates for learning kernels in operators</title>
      <link>https://arxiv.org/abs/2502.20368</link>
      <description>arXiv:2502.20368v2 Announce Type: replace 
Abstract: Learning kernels in operators from data lies at the intersection of inverse problems and statistical learning, providing a powerful framework for capturing non-local dependencies in function spaces and high-dimensional settings. In contrast to classical nonparametric regression, where the inverse problem is well-posed, kernel estimation involves a compact normal operator and an ill-posed deconvolution. To address these challenges, we introduce adaptive spectral Sobolev spaces, which unify Sobolev spaces and reproducing kernel Hilbert spaces, automatically discarding non-identifiable components and controlling terms with small eigenvalues. Within this framework, we establish the minimax convergence rates for the mean squared error under both polynomial and exponential spectral decay regimes. Methodologically, we develop a tamed least squares estimator achieving the minimax upper rates via controlling the left-tail probability for eigenvalues of the random normal matrix; and for the minimax lower rates, we resolve challenges from infinite-dimensional measures through their projections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20368v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sichong Zhang, Xiong Wang, Fei Lu</dc:creator>
    </item>
    <item>
      <title>Weak Signals and Heavy Tails: Machine-learning meets Extreme Value Theory</title>
      <link>https://arxiv.org/abs/2504.06984</link>
      <description>arXiv:2504.06984v2 Announce Type: replace 
Abstract: The masses of data now available have opened up the prospect of discovering weak signals using machine-learning algorithms, with a view to predictive or interpretation tasks. As this survey of recent results attempts to show, bringing multivariate extreme value theory and statistical learning theory together in a common, non-parametric and non-asymptotic framework makes it possible to design and analyze new methods for exploiting the scarce information located in distribution tails in these purposes. This article reviews recently proved theoretical tools for establishing guarantees for supervised or unsupervised algorithms learning from a fraction of extreme data. These are mainly exponential maximal deviation inequalities tailored to low-probability regions and concentration results for stochastic processes empirically describing the behavior of extreme observations, their dependence structure in particular. Under appropriate assumptions of regular variation, several illustrative applications are then examined: classification, regression, anomaly detection, model selection via cross-validation. For these, generalization results are established inspired by the classical bounds in statistical learning theory. In the same spirit, it is also shown how to adapt the popular high-dimensional lasso technique in the context of extreme values for the covariates with generalization guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06984v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Stephan Cl\'emen\c{c}on, Anne Sabourin</dc:creator>
    </item>
    <item>
      <title>Conformal changepoint localization</title>
      <link>https://arxiv.org/abs/2505.00292</link>
      <description>arXiv:2505.00292v2 Announce Type: replace 
Abstract: Changepoint localization is the problem of estimating the index at which a change occurred in the data generating distribution of an ordered list of data, or declaring that no change occurred. We present the broadly applicable CONCH (CONformal CHangepoint localization) algorithm, which uses a matrix of conformal p-values to produce a confidence interval for a (single) changepoint under the mild assumption that the pre-change and post-change distributions are each exchangeable. We exemplify the CONCH algorithm on a variety of synthetic and real-world datasets, including using black-box pre-trained classifiers to detect changes in sequences of images or text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00292v2</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Nelson-Aalen kernel estimator to the tail index of right censored Pareto-type data</title>
      <link>https://arxiv.org/abs/2505.09152</link>
      <description>arXiv:2505.09152v2 Announce Type: replace 
Abstract: On the basis of Nelson-Aalen product-limit estimator of a randomly censored distribution function, we introduce a kernel estimator to the tail index of right-censored Pareto-like data. Under some regularity assumptions, the consistency and asymptotic normality of the proposed estimator are established. A small simulation study shows that the proposed estimator performs much better, in terms of bias and stability, than the existing ones with, a slight increase in the mean squared error. The results are applied to insurance loss data to illustrate the practical effectiveness of our estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09152v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nour Elhouda Guesmia, Abdelhakim Necir, Djamel Meraghni</dc:creator>
    </item>
    <item>
      <title>$\beta$-integrated local depth and corresponding partitioned local depth representation</title>
      <link>https://arxiv.org/abs/2506.14108</link>
      <description>arXiv:2506.14108v2 Announce Type: replace 
Abstract: A novel local depth definition, $\beta$-integrated local depth ($\beta$-ILD), is proposed as a generalization of the local depth introduced by Paindaveine and Van Bever \cite{paindaveine2013depth}, designed to quantify the local centrality of data points. $\beta$-ILD inherits desirable properties from global data depth and remains robust across varying locality levels. A partitioning approach for $\beta$-ILD is introduced, leading to the construction of a matrix that quantifies the contribution of one point to another's local depth, providing a new interpretable measure of local centrality. These concepts are applied to classification and outlier detection tasks, demonstrating significant improvements in the performance of depth-based algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14108v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyi Wang, Alexandre Leblanc, Paul D. McNicholas</dc:creator>
    </item>
    <item>
      <title>Indeterminate Probability Theory</title>
      <link>https://arxiv.org/abs/2303.11536</link>
      <description>arXiv:2303.11536v2 Announce Type: replace-cross 
Abstract: Complex continuous or mixed joint distributions (e.g., P(Y | z_1, z_2, ..., z_N)) generally lack closed-form solutions, often necessitating approximations such as MCMC. This paper proposes Indeterminate Probability Theory (IPT), which makes the following contributions: (1) An observer-centered framework in which experimental outcomes are represented as distributions combining ground truth with observation error; (2) The introduction of three independence candidate axioms that enable a two-phase probabilistic inference framework; (3) The derivation of closed-form solutions for arbitrary complex joint distributions under this framework. Both the Indeterminate Probability Neural Network (IPNN) model and the non-neural multivariate time series forecasting application demonstrate IPT's effectiveness in modeling high-dimensional distributions, with successful validation up to 1000 dimensions. Importantly, IPT is consistent with classical probability theory and subsumes the frequentist equation in the limit of vanishing observation error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.11536v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tao Yang, Chuang Liu, Xiaofeng Ma, Weijia Lu, Ning Wu, Bingyang Li, Zhifei Yang, Peng Liu, Lin Sun, Xiaodong Zhang, Can Zhang</dc:creator>
    </item>
    <item>
      <title>Structural restrictions in local causal discovery: identifying direct causes of a target variable</title>
      <link>https://arxiv.org/abs/2307.16048</link>
      <description>arXiv:2307.16048v4 Announce Type: replace-cross 
Abstract: We consider the problem of learning a set of direct causes of a target variable from an observational joint distribution. Learning directed acyclic graphs (DAGs) that represent the causal structure is a fundamental problem in science. Several results are known when the full DAG is identifiable from the distribution, such as assuming a nonlinear Gaussian data-generating process. Here, we are only interested in identifying the direct causes of one target variable (local causal structure), not the full DAG. This allows us to relax the identifiability assumptions and develop possibly faster and more robust algorithms. In contrast to the Invariance Causal Prediction framework, we only assume that we observe one environment without any interventions. We discuss different assumptions for the data-generating process of the target variable under which the set of direct causes is identifiable from the distribution. While doing so, we put essentially no assumptions on the variables other than the target variable. In addition to the novel identifiability results, we provide two practical algorithms for estimating the direct causes from a finite random sample and demonstrate their effectiveness on several benchmark and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16048v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/biomet/asaf042</arxiv:DOI>
      <arxiv:journal_reference>Biometrika (2025)</arxiv:journal_reference>
      <dc:creator>Juraj Bodik, Val\'erie Chavez-Demoulin</dc:creator>
    </item>
    <item>
      <title>The Empirical Spectral Distribution of i.i.d. Random Matrices with Random Perturbations</title>
      <link>https://arxiv.org/abs/2410.21919</link>
      <description>arXiv:2410.21919v3 Announce Type: replace-cross 
Abstract: A large i.i.d. random matrix with deterministic low-rank perturbation has been extensively studied, particularly in the aspects of the ESD (Empirical Spectral Distribution) and the outliers of eigenvalues. In this work, we investigate the analogous scenario where the perturbation is random and extend the previous results from the deterministic perturbation to the random case. Specifically, we consider an i.i.d. matrix with random perturbation, $\mathbf{M}$. Our results show that: (i) the eigenvalue outliers of $\mathbf{M}$ converge to the eigenvalues of its perturbation; (ii) the ESD of $\mathbf{M}$ converges to the circular law; (iii) the eigenvector alignment holds for specific perturbations.
  As an application of the above random matrices, we present the first optimal query complexity lower bound for approximating the top eigenvector of asymmetric matrices. In the inverse polynomial accuracy regime, the complexity matches the upper bounds that can be obtained via the power method. As far as we know, it is the first lower bound for approximating the eigenvector of an asymmetric matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21919v3</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Chen, Zhihua Zhang</dc:creator>
    </item>
    <item>
      <title>Self-Normalized Inference in (Quantile, Expected Shortfall) Regressions for Time Series</title>
      <link>https://arxiv.org/abs/2502.10065</link>
      <description>arXiv:2502.10065v2 Announce Type: replace-cross 
Abstract: This paper proposes valid inference tools, based on self-normalization, in time series expected shortfall regressions and, as a corollary, also in quantile regressions. Extant methods for such time series regressions, based on a bootstrap or direct estimation of the long-run variance, are computationally more involved, require the choice of tuning parameters and have serious size distortions when the regression errors are strongly serially dependent. In contrast, our inference tools only require estimates of the (quantile, expected shortfall) regression parameters that are computed on an expanding window, and are correctly sized as we show in simulations. Two empirical applications to stock return predictability and to Growth-at-Risk demonstrate the practical usefulness of the developed inference tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10065v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yannick Hoga, Christian Schulz</dc:creator>
    </item>
    <item>
      <title>Optimal Adjustment Sets for Nonparametric Estimation of Weighted Controlled Direct Effect</title>
      <link>https://arxiv.org/abs/2506.09871</link>
      <description>arXiv:2506.09871v2 Announce Type: replace-cross 
Abstract: The weighted controlled direct effect (WCDE) generalizes the standard controlled direct effect (CDE) by averaging over the mediator distribution, providing a robust estimate when treatment effects vary across mediator levels. This makes the WCDE especially relevant in fairness analysis, where it isolates the direct effect of an exposure on an outcome, independent of mediating pathways. This work establishes three fundamental advances for WCDE in observational studies: First, we establish necessary and sufficient conditions for the unique identifiability of the WCDE, clarifying when it diverges from the CDE. Next, we consider nonparametric estimation of the WCDE and derive its influence function, focusing on the class of regular and asymptotically linear estimators. Lastly, we characterize the optimal covariate adjustment set that minimizes the asymptotic variance, demonstrating how mediator-confounder interactions introduce distinct requirements compared to average treatment effect estimation. Our results offer a principled framework for efficient estimation of direct effects in complex causal systems, with practical applications in fairness and mediation analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09871v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruiyang Lin (University of Science,Technology of China), Yongyi Guo (University of Wisconsin-Madison), Kyra Gan (Cornell Tech)</dc:creator>
    </item>
    <item>
      <title>A Random Matrix Theory of Pauli Tomography</title>
      <link>https://arxiv.org/abs/2506.12010</link>
      <description>arXiv:2506.12010v2 Announce Type: replace-cross 
Abstract: Quantum state tomography (QST), the process of reconstructing some unknown quantum state $\hat\rho$ from repeated measurements on copies of said state, is a foundationally important task in the context of quantum computation and simulation. For this reason, a detailed characterization of the error $\Delta\hat\rho = \hat\rho-\hat\rho^\prime$ in a QST reconstruction $\hat\rho^\prime$ is of clear importance to quantum theory and experiment. In this work, we develop a fully random matrix theory (RMT) treatment of state tomography in informationally-complete bases; and in doing so we reveal deep connections between QST errors $\Delta\hat\rho$ and the gaussian unitary ensemble (GUE). By exploiting this connection we prove that wide classes of functions of the spectrum of $\Delta\hat\rho$ can be evaluated by substituting samples of an appropriate GUE for realizations of $\Delta\hat\rho$. This powerful and flexible result enables simple analytic treatments of the mean value and variance of the error as quantified by the trace distance $\|\Delta\hat\rho\|_\mathrm{Tr}$ (which we validate numerically for common tomographic protocols), allows us to derive a bound on the QST sample complexity, and subsequently demonstrate that said bound doesn't change under the most widely-used rephysicalization procedure. These results collectively demonstrate the flexibility, strength, and broad applicability of our approach; and lays the foundation for broader studies of RMT treatments of QST in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12010v2</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Keenan, John Goold, Alex Nico-Katz</dc:creator>
    </item>
  </channel>
</rss>
