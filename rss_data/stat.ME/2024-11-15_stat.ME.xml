<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Nov 2024 05:00:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Power and Sample Size Calculations for Cluster Randomized Hybrid Type 2 Effectiveness-Implementation Studies</title>
      <link>https://arxiv.org/abs/2411.08929</link>
      <description>arXiv:2411.08929v1 Announce Type: new 
Abstract: Hybrid studies allow investigators to simultaneously study an intervention effectiveness outcome and an implementation research outcome. In particular, type 2 hybrid studies support research that places equal importance on both outcomes rather than focusing on one and secondarily on the other (i.e., type 1 and type 3 studies). Hybrid 2 studies introduce the statistical issue of multiple testing, complicated by the fact that they are typically also cluster randomized trials. Standard statistical methods do not apply in this scenario. Here, we describe the design methodologies available for validly powering hybrid type 2 studies and producing reliable sample size calculations in a cluster-randomized design with a focus on binary outcomes. Through a literature search, 18 publications were identified that included methods relevant to the design of hybrid 2 studies. Five methods were identified, two of which did not account for clustering but are extended in this article to do so, namely the combined outcomes approach and the single 1-degree of freedom combined test. Procedures for powering hybrid 2 studies using these five methods are described and illustrated using input parameters inspired by a study from the Community Intervention to Reduce CardiovascuLar Disease in Chicago (CIRCL-Chicago) Implementation Research Center. In this illustrative example, the intervention effectiveness outcome was controlled blood pressure, and the implementation outcome was reach. The conjunctive test resulted in higher power than the popular p-value adjustment methods, and the newly extended combined outcomes and single 1-DF test were found to be the most powerful among all of the tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08929v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Melody A. Owen, Geoffrey M. Curran, Justin D. Smith, Yacob Tedla, Chao Cheng, Donna Spiegelman</dc:creator>
    </item>
    <item>
      <title>Using Principal Progression Rate to Quantify and Compare Disease Progression in Comparative Studies</title>
      <link>https://arxiv.org/abs/2411.08984</link>
      <description>arXiv:2411.08984v1 Announce Type: new 
Abstract: In comparative studies of progressive diseases, such as randomized controlled trials (RCTs), the mean Change From Baseline (CFB) of a continuous outcome at a pre-specified follow-up time across subjects in the target population is a standard estimand used to summarize the overall disease progression. Despite its simplicity in interpretation, the mean CFB may not efficiently capture important features of the trajectory of the mean outcome relevant to the evaluation of the treatment effect of an intervention. Additionally, the estimation of the mean CFB does not use all longitudinal data points. To address these limitations, we propose a class of estimands called Principal Progression Rate (PPR). The PPR is a weighted average of local or instantaneous slope of the trajectory of the population mean during the follow-up. The flexibility of the weight function allows the PPR to cover a broad class of intuitive estimands, including the mean CFB, the slope of ordinary least-square fit to the trajectory, and the area under the curve. We showed that properly chosen PPRs can enhance statistical power over the mean CFB by amplifying the signal of treatment effect and/or improving estimation precision. We evaluated different versions of PPRs and the performance of their estimators through numerical studies. A real dataset was analyzed to demonstrate the advantage of using alternative PPR over the mean CFB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08984v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changyu Shen, Menglan Pang, Ling Zhu, Lu Tian</dc:creator>
    </item>
    <item>
      <title>Debiased machine learning for counterfactual survival functionals based on left-truncated right-censored data</title>
      <link>https://arxiv.org/abs/2411.09017</link>
      <description>arXiv:2411.09017v1 Announce Type: new 
Abstract: Learning causal effects of a binary exposure on time-to-event endpoints can be challenging because survival times may be partially observed due to censoring and systematically biased due to truncation. In this work, we present debiased machine learning-based nonparametric estimators of the joint distribution of a counterfactual survival time and baseline covariates for use when the observed data are subject to covariate-dependent left truncation and right censoring and when baseline covariates suffice to deconfound the relationship between exposure and survival time. Our inferential procedures explicitly allow the integration of flexible machine learning tools for nuisance estimation, and enjoy certain robustness properties. The approach we propose can be directly used to make pointwise or uniform inference on smooth summaries of the joint counterfactual survival time and covariate distribution, and can be valuable even in the absence of interventions, when summaries of a marginal survival distribution are of interest. We showcase how our procedures can be used to learn a variety of inferential targets and illustrate their performance in simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09017v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric R. Morenz, Charles J. Wolock, Marco Carone</dc:creator>
    </item>
    <item>
      <title>On the Selection Stability of Stability Selection and Its Applications</title>
      <link>https://arxiv.org/abs/2411.09097</link>
      <description>arXiv:2411.09097v1 Announce Type: new 
Abstract: Stability selection is a widely adopted resampling-based framework for high-dimensional structure estimation and variable selection. However, the concept of 'stability' is often narrowly addressed, primarily through examining selection frequencies, or 'stability paths'. This paper seeks to broaden the use of an established stability estimator to evaluate the overall stability of the stability selection framework, moving beyond single-variable analysis. We suggest that the stability estimator offers two advantages: it can serve as a reference to reflect the robustness of the outcomes obtained and help identify an optimal regularization value to improve stability. By determining this value, we aim to calibrate key stability selection parameters, namely, the decision threshold and the expected number of falsely selected variables, within established theoretical bounds. Furthermore, we explore a novel selection criterion based on this regularization value. With the asymptotic distribution of the stability estimator previously established, convergence to true stability is ensured, allowing us to observe stability trends over successive sub-samples. This approach sheds light on the required number of sub-samples addressing a notable gap in prior studies. The 'stabplot' package is developed to facilitate the use of the plots featured in this manuscript, supporting their integration into further statistical analysis and research workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09097v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Nouraie, Samuel Muller</dc:creator>
    </item>
    <item>
      <title>Propensity Score Matching: Should We Use It in Designing Observational Studies?</title>
      <link>https://arxiv.org/abs/2411.09579</link>
      <description>arXiv:2411.09579v1 Announce Type: new 
Abstract: Propensity Score Matching (PSM) stands as a widely embraced method in comparative effectiveness research. PSM crafts matched datasets, mimicking some attributes of randomized designs, from observational data. In a valid PSM design where all baseline confounders are measured and matched, the confounders would be balanced, allowing the treatment status to be considered as if it were randomly assigned. Nevertheless, recent research has unveiled a different facet of PSM, termed "the PSM paradox." As PSM approaches exact matching by progressively pruning matched sets in order of decreasing propensity score distance, it can paradoxically lead to greater covariate imbalance, heightened model dependence, and increased bias, contrary to its intended purpose. Methods: We used analytic formula, simulation, and literature to demonstrate that this paradox stems from the misuse of metrics for assessing chance imbalance and bias. Results: Firstly, matched pairs typically exhibit different covariate values despite having identical propensity scores. However, this disparity represents a "chance" difference and will average to zero over a large number of matched pairs. Common distance metrics cannot capture this ``chance" nature in covariate imbalance, instead reflecting increasing variability in chance imbalance as units are pruned and the sample size diminishes. Secondly, the largest estimate among numerous fitted models, because of uncertainty among researchers over the correct model, was used to determine statistical bias. This cherry-picking procedure ignores the most significant benefit of matching design-reducing model dependence based on its robustness against model misspecification bias. Conclusions: We conclude that the PSM paradox is not a legitimate concern and should not stop researchers from using PSM designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09579v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fei Wan</dc:creator>
    </item>
    <item>
      <title>Microfoundation Inference for Strategic Prediction</title>
      <link>https://arxiv.org/abs/2411.08998</link>
      <description>arXiv:2411.08998v1 Announce Type: cross 
Abstract: Often in prediction tasks, the predictive model itself can influence the distribution of the target variable, a phenomenon termed performative prediction. Generally, this influence stems from strategic actions taken by stakeholders with a vested interest in predictive models. A key challenge that hinders the widespread adaptation of performative prediction in machine learning is that practitioners are generally unaware of the social impacts of their predictions. To address this gap, we propose a methodology for learning the distribution map that encapsulates the long-term impacts of predictive models on the population. Specifically, we model agents' responses as a cost-adjusted utility maximization problem and propose estimates for said cost. Our approach leverages optimal transport to align pre-model exposure (ex ante) and post-model exposure (ex post) distributions. We provide a rate of convergence for this proposed estimate and assess its quality through empirical demonstrations on a credit-scoring dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08998v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Bracale, Subha Maity, Felipe Maia Polo, Seamus Somerstep, Moulinath Banerjee, Yuekai Sun</dc:creator>
    </item>
    <item>
      <title>General linear threshold models with application to influence maximization</title>
      <link>https://arxiv.org/abs/2411.09100</link>
      <description>arXiv:2411.09100v1 Announce Type: cross 
Abstract: A number of models have been developed for information spread through networks, often for solving the Influence Maximization (IM) problem. IM is the task of choosing a fixed number of nodes to "seed" with information in order to maximize the spread of this information through the network, with applications in areas such as marketing and public health. Most methods for this problem rely heavily on the assumption of known strength of connections between network members (edge weights), which is often unrealistic. In this paper, we develop a likelihood-based approach to estimate edge weights from the fully and partially observed information diffusion paths. We also introduce a broad class of information diffusion models, the general linear threshold (GLT) model, which generalizes the well-known linear threshold (LT) model by allowing arbitrary distributions of node activation thresholds. We then show our weight estimator is consistent under the GLT and some mild assumptions. For the special case of the standard LT model, we also present a much faster expectation-maximization approach for weight estimation. Finally, we prove that for the GLT models, the IM problem can be solved by a natural greedy algorithm with standard optimality guarantees if all node threshold distributions have concave cumulative distribution functions. Extensive experiments on synthetic and real-world networks demonstrate that the flexibility in the choice of threshold distribution combined with the estimation of edge weights significantly improves the quality of IM solutions, spread prediction, and the estimates of the node activation probabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09100v1</guid>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Kagan, Elizaveta Levina, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>fdesigns: Bayesian Optimal Designs of Experiments for Functional Models in R</title>
      <link>https://arxiv.org/abs/2411.09225</link>
      <description>arXiv:2411.09225v1 Announce Type: cross 
Abstract: This paper describes the R package fdesigns that implements a methodology for identifying Bayesian optimal experimental designs for models whose factor settings are functions, known as profile factors. This type of experiments which involve factors that vary dynamically over time, presenting unique challenges in both estimation and design due to the infinite-dimensional nature of functions. The package fdesigns implements a dimension reduction method leveraging basis functions of the B-spline basis system. The package fdesigns contains functions that effectively reduce the design problem to the optimisation of basis coefficients for functional linear functional generalised linear models, and it accommodates various options. Applications of the fdesigns package are demonstrated through a series of examples that showcase its capabilities in identifying optimal designs for functional linear and generalised linear models. The examples highlight how the package's functions can be used to efficiently design experiments involving both profile and scalar factors, including interactions and polynomial effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09225v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damianos Michaelides, Antony Overstall, Dave Woods</dc:creator>
    </item>
    <item>
      <title>Monitoring time to event in registry data using CUSUMs based on excess hazard models</title>
      <link>https://arxiv.org/abs/2411.09353</link>
      <description>arXiv:2411.09353v1 Announce Type: cross 
Abstract: An aspect of interest in surveillance of diseases is whether the survival time distribution changes over time. By following data in health registries over time, this can be monitored, either in real time or retrospectively. With relevant risk factors registered, these can be taken into account in the monitoring as well. A challenge in monitoring survival times based on registry data is that data on cause of death might either be missing or uncertain. To quantify the burden of disease in such cases, excess hazard methods can be used, where the total hazard is modelled as the population hazard plus the excess hazard due to the disease.
  We propose a CUSUM procedure for monitoring for changes in the survival time distribution in cases where use of excess hazard models is relevant. The procedure is based on a survival log-likelihood ratio and extends previously suggested methods for monitoring of time to event to the excess hazard setting. The procedure takes into account changes in the population risk over time, as well as changes in the excess hazard which is explained by observed covariates. Properties, challenges and an application to cancer registry data will be presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09353v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jimmy Huy Tran, Jan Terje Kval{\o}y, Hartwig K{\o}rner</dc:creator>
    </item>
    <item>
      <title>On importance sampling and independent Metropolis-Hastings with an unbounded weight function</title>
      <link>https://arxiv.org/abs/2411.09514</link>
      <description>arXiv:2411.09514v1 Announce Type: cross 
Abstract: Importance sampling and independent Metropolis-Hastings (IMH) are among the fundamental building blocks of Monte Carlo methods. Both require a proposal distribution that globally approximates the target distribution. The Radon-Nikodym derivative of the target distribution relative to the proposal is called the weight function. Under the weak assumption that the weight is unbounded but has a number of finite moments under the proposal distribution, we obtain new results on the approximation error of importance sampling and of the particle independent Metropolis-Hastings algorithm (PIMH), which includes IMH as a special case. For IMH and PIMH, we show that the common random numbers coupling is maximal. Using that coupling we derive bounds on the total variation distance of a PIMH chain to the target distribution. The bounds are sharp with respect to the number of particles and the number of iterations. Our results allow a formal comparison of the finite-time biases of importance sampling and IMH. We further consider bias removal techniques using couplings of PIMH, and provide conditions under which the resulting unbiased estimators have finite moments. We compare the asymptotic efficiency of regular and unbiased importance sampling estimators as the number of particles goes to infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09514v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Deligiannidis (University of Oxford), Pierre E. Jacob (ESSEC Business School), El Mahdi Khribch (ESSEC Business School), Guanyang Wang (Rutgers University)</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for aggregated Hawkes processes</title>
      <link>https://arxiv.org/abs/2211.16552</link>
      <description>arXiv:2211.16552v4 Announce Type: replace 
Abstract: The Hawkes process, a self-exciting point process, has a wide range of applications in modeling earthquakes, social networks and stock markets. The established estimation process requires that researchers have access to the exact time stamps and spatial information. However, available data are often rounded or aggregated. We develop a Bayesian estimation procedure for the parameters of a Hawkes process based on aggregated data. Our approach is developed for temporal, spatio-temporal, and mutually exciting Hawkes processes where data are available over discrete time periods and regions. We show theoretically that the parameters of the Hawkes process are identifiable from aggregated data under general specifications. We demonstrate the method on simulated data under various model specifications in the presence of one or more interacting processes, and under varying coarseness of data aggregation. Finally, we examine the internal and cross-excitation effects of airstrikes and insurgent violence events from February 2007 to June 2008, with some data aggregated by day.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16552v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lingxiao Zhou, Georgia Papadogeorgou</dc:creator>
    </item>
    <item>
      <title>dynamite: An R Package for Dynamic Multivariate Panel Models</title>
      <link>https://arxiv.org/abs/2302.01607</link>
      <description>arXiv:2302.01607v3 Announce Type: replace 
Abstract: dynamite is an R package for Bayesian inference of intensive panel (time series) data comprising multiple measurements per multiple individuals measured in time. The package supports joint modeling of multiple response variables, time-varying and time-invariant effects, a wide range of discrete and continuous distributions, group-specific random effects, latent factors, and customization of prior distributions of the model parameters. Models in the package are defined via a user-friendly formula interface, and estimation of the posterior distribution of the model parameters takes advantage of state-of-the-art Markov chain Monte Carlo methods. The package enables efficient computation of both individual-level and aggregated predictions and offers a comprehensive suite of tools for visualization and model diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.01607v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santtu Tikka, Jouni Helske</dc:creator>
    </item>
    <item>
      <title>Personalised dynamic super learning: an application in predicting hemodiafiltration convection volumes</title>
      <link>https://arxiv.org/abs/2310.08479</link>
      <description>arXiv:2310.08479v2 Announce Type: replace 
Abstract: Obtaining continuously updated predictions is a major challenge for personalised medicine. Leveraging combinations of parametric regressions and machine learning approaches, the personalised online super learner (POSL) can achieve such dynamic and personalised predictions. We adapt POSL to predict a repeated continuous outcome dynamically and propose a new way to validate such personalised or dynamic prediction models. We illustrate its performance by predicting the convection volume of patients undergoing hemodiafiltration. POSL outperformed its candidate learners with respect to median absolute error, calibration-in-the-large, discrimination, and net benefit. We finally discuss the choices and challenges underlying the use of POSL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08479v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arthur Chatton, Mich\`ele Bally, Ren\'ee L\'evesque, Ivana Malenica, Robert W. Platt, Mireille E. Schnitzer</dc:creator>
    </item>
    <item>
      <title>Fourier analysis of spatial point processes</title>
      <link>https://arxiv.org/abs/2401.06403</link>
      <description>arXiv:2401.06403v2 Announce Type: replace 
Abstract: In this article, we develop comprehensive frequency domain methods for estimating and inferring the second-order structure of spatial point processes. The main element here is on utilizing the discrete Fourier transform (DFT) of the point pattern and its tapered counterpart. Under second-order stationarity, we show that both the DFTs and the tapered DFTs are asymptotically jointly independent Gaussian even when the DFTs share the same limiting frequencies. Based on these results, we establish an $\alpha$-mixing central limit theorem for a statistic formulated as a quadratic form of the tapered DFT. As applications, we derive the asymptotic distribution of the kernel spectral density estimator and establish a frequency domain inferential method for parametric stationary point processes. For the latter, the resulting model parameter estimator is computationally tractable and yields meaningful interpretations even in the case of model misspecification. We investigate the finite sample performance of our estimator through simulations, considering scenarios of both correctly specified and misspecified models. Furthermore, we extend our proposed DFT-based frequency domain methods to a class of non-stationary spatial point processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06403v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junho Yang, Yongtao Guan</dc:creator>
    </item>
    <item>
      <title>Robust Genomic Prediction and Heritability Estimation using Density Power Divergence</title>
      <link>https://arxiv.org/abs/2401.07344</link>
      <description>arXiv:2401.07344v2 Announce Type: replace 
Abstract: This manuscript delves into the intersection of genomics and phenotypic prediction, focusing on the statistical innovation required to navigate the complexities introduced by noisy covariates and confounders. The primary emphasis is on the development of advanced robust statistical models tailored for genomic prediction from single nucleotide polymorphism data in plant and animal breeding and multi-field trials. The manuscript highlights the significance of incorporating all estimated effects of marker loci into the statistical framework and aiming to reduce the high dimensionality of data while preserving critical information. This paper introduces a new robust statistical framework for genomic prediction, employing one-stage and two-stage linear mixed model analyses along with utilizing the popular robust minimum density power divergence estimator (MDPDE) to estimate genetic effects on phenotypic traits. The study illustrates the superior performance of the proposed MDPDE-based genomic prediction and associated heritability estimation procedures over existing competitors through extensive empirical experiments on artificial datasets and application to a real-life maize breeding dataset. The results showcase the robustness and accuracy of the proposed MDPDE-based approaches, especially in the presence of data contamination, emphasizing their potential applications in improving breeding programs and advancing genomic prediction of phenotyping traits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07344v2</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Upama Paul Chowdhury, Ronit Bhattacharjee, Susmita Das, Abhik Ghosh</dc:creator>
    </item>
    <item>
      <title>Merging uncertainty sets via majority vote</title>
      <link>https://arxiv.org/abs/2401.09379</link>
      <description>arXiv:2401.09379v5 Announce Type: replace 
Abstract: Given $K$ uncertainty sets that are arbitrarily dependent -- for example, confidence intervals for an unknown parameter obtained with $K$ different estimators, or prediction sets obtained via conformal prediction based on $K$ different algorithms on shared data -- we address the question of how to efficiently combine them in a black-box manner to produce a single uncertainty set. We present a simple and broadly applicable majority vote procedure that produces a merged set with nearly the same error guarantee as the input sets. We then extend this core idea in a few ways: we show that weighted averaging can be a powerful way to incorporate prior information, and a simple randomization trick produces strictly smaller merged sets without altering the coverage guarantee. Further improvements can be obtained if the sets are exchangeable. We also show that many modern methods, like split conformal prediction, median of means, HulC and cross-fitted ``double machine learning'', can be effectively derandomized using these ideas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09379v5</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Gasparin, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Enhancing Statistical Validity and Power in Hybrid Controlled Trials: A Randomization Inference Approach with Conformal Selective Borrowing</title>
      <link>https://arxiv.org/abs/2410.11713</link>
      <description>arXiv:2410.11713v2 Announce Type: replace 
Abstract: Randomized controlled trials (RCTs) are the gold standard for causal inference but may lack power because of small populations in rare diseases and limited participation in common diseases due to equipoise concerns. Hybrid controlled trials, which integrate external controls (ECs) from historical studies or large observational data, improve statistical efficiency and are appealing for drug evaluations. However, non-randomized ECs can introduce biases and inflate the type I error rate, especially when the RCT sample size is small. To address this, we propose a Fisher randomization test (FRT) that employs a semiparametric efficient test statistic combining RCT and EC data, with assignments resampled using the actual randomization procedure. The proposed FRT controls the type I error rate even with unmeasured confounding among ECs. However, borrowing biased ECs can reduce FRT power, so we introduce conformal selective borrowing (CSB) to individually borrow comparable ECs. We propose an adaptive procedure to determine the selection threshold, minimizing the mean squared error of a class of CSB estimators and enhancing FRT power. The advantages of our method are demonstrated through simulations and an application to a lung cancer RCT with ECs from the National Cancer Database. Our method is available in the R package intFRT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11713v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ke Zhu, Shu Yang, Xiaofei Wang</dc:creator>
    </item>
    <item>
      <title>Joint Estimation of Conditional Mean and Covariance for Unbalanced Panels</title>
      <link>https://arxiv.org/abs/2410.21858</link>
      <description>arXiv:2410.21858v3 Announce Type: replace 
Abstract: We propose a nonparametric, kernel-based joint estimator for conditional mean and covariance matrices in large unbalanced panels. Our estimator, with proven consistency and finite-sample guarantees, is applied to a comprehensive panel of monthly US stock excess returns from 1962 to 2021, conditioned on macroeconomic and firm-specific covariates. The estimator captures time-varying cross-sectional dependencies effectively, demonstrating robust statistical performance. In asset pricing, it generates conditional mean-variance efficient portfolios with out-of-sample Sharpe ratios that substantially exceed those of equal-weighted benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21858v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Damir Filipovic, Paul Schneider</dc:creator>
    </item>
    <item>
      <title>Comment on 'Sparse Bayesian Factor Analysis when the Number of Factors is Unknown' by S. Fr\"uhwirth-Schnatter, D. Hosszejni, and H. Freitas Lopes</title>
      <link>https://arxiv.org/abs/2411.02531</link>
      <description>arXiv:2411.02531v3 Announce Type: replace 
Abstract: The techniques suggested in Fr\"uhwirth-Schnatter et al. (2024) concern sparsity and factor selection and have enormous potential beyond standard factor analysis applications. We show how these techniques can be applied to Latent Space (LS) models for network data. These models suffer from well-known identification issues of the latent factors due to likelihood invariance to factor translation, reflection, and rotation (see Hoff et al., 2002). A set of observables can be instrumental in identifying the latent factors via auxiliary equations (see Liu et al., 2021). These, in turn, share many analogies with the equations used in factor modeling, and we argue that the factor loading restrictions may be beneficial for achieving identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02531v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Casarin (Ca' Foscari University of Venice), Antonio Peruzzi (Ca' Foscari University of Venice)</dc:creator>
    </item>
    <item>
      <title>M-Variance Asymptotics and Uniqueness of Descriptors</title>
      <link>https://arxiv.org/abs/2011.14762</link>
      <description>arXiv:2011.14762v2 Announce Type: replace-cross 
Abstract: Asymptotic theory for M-estimation problems usually focuses on the asymptotic convergence of the sample descriptor, defined as the minimizer of the sample loss function. Here, we explore a related question and formulate asymptotic theory for the minimum value of sample loss, the M-variance. Since the loss function value is always a real number, the asymptotic theory for the M-variance is comparatively simple. M-variance often satisfies a standard central limit theorem, even in situations where the asymptotics of the descriptor is more complicated as for example in case of smeariness, or if no asymptotic distribution can be given as can be the case if the descriptor space is a general metric space. We use the asymptotic results for the M-variance to formulate a hypothesis test to systematically determine for a given sample whether the underlying population loss function may have multiple global minima. We discuss three applications of our test to data, each of which presents a typical scenario in which non-uniqueness of descriptors may occur. These model scenarios are the mean on a non-euclidean space, non-linear regression and Gaussian mixture clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.14762v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Eltzner</dc:creator>
    </item>
    <item>
      <title>Enhancing Maritime Trajectory Forecasting via H3 Index and Causal Language Modelling (CLM)</title>
      <link>https://arxiv.org/abs/2405.09596</link>
      <description>arXiv:2405.09596v2 Announce Type: replace-cross 
Abstract: The prediction of ship trajectories is a growing field of study in artificial intelligence. Traditional methods rely on the use of LSTM, GRU networks, and even Transformer architectures for the prediction of spatio-temporal series. This study proposes a viable alternative for predicting these trajectories using only GNSS positions. It considers this spatio-temporal problem as a natural language processing problem. The latitude/longitude coordinates of AIS messages are transformed into cell identifiers using the H3 index. Thanks to the pseudo-octal representation, it becomes easier for language models to learn the spatial hierarchy of the H3 index. The method is compared with a classical Kalman filter, widely used in the maritime domain, and introduces the Fr\'echet distance as the main evaluation metric. We show that it is possible to predict ship trajectories quite precisely up to 8 hours ahead with 30 minutes of context, using solely GNSS positions, without relying on any additional information such as speed, course, or external conditions - unlike many traditional methods. We demonstrate that this alternative works well enough to predict trajectories worldwide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09596v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicolas Drapier, Aladine Chetouani, Aur\'elien Chateigner</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing with e-values</title>
      <link>https://arxiv.org/abs/2410.23614</link>
      <description>arXiv:2410.23614v2 Announce Type: replace-cross 
Abstract: This book is written to offer a humble, but unified, treatment of e-values in hypothesis testing. The book is organized into three parts: Fundamental Concepts, Core Ideas, and Advanced Topics. The first part includes three chapters that introduce the basic concepts. The second part includes five chapters of core ideas such as universal inference, log-optimality, e-processes, operations on e-values, and e-values in multiple testing. The third part contains five chapters of advanced topics. We hope that, by putting the materials together in this book, the concept of e-values becomes more accessible for educational, research, and practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23614v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaditya Ramdas, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Causal Discovery and Classification Using Lempel-Ziv Complexity</title>
      <link>https://arxiv.org/abs/2411.01881</link>
      <description>arXiv:2411.01881v2 Announce Type: replace-cross 
Abstract: Inferring causal relationships in the decision-making processes of machine learning algorithms is a crucial step toward achieving explainable Artificial Intelligence (AI). In this research, we introduce a novel causality measure and a distance metric derived from Lempel-Ziv (LZ) complexity. We explore how the proposed causality measure can be used in decision trees by enabling splits based on features that most strongly \textit{cause} the outcome. We further evaluate the effectiveness of the causality-based decision tree and the distance-based decision tree in comparison to a traditional decision tree using Gini impurity. While the proposed methods demonstrate comparable classification performance overall, the causality-based decision tree significantly outperforms both the distance-based decision tree and the Gini-based decision tree on datasets generated from causal models. This result indicates that the proposed approach can capture insights beyond those of classical decision trees, especially in causally structured data. Based on the features used in the LZ causal measure based decision tree, we introduce a causal strength for each features in the dataset so as to infer the predominant causal variables for the occurrence of the outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01881v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Dhruthi, Nithin Nagaraj, Harikrishnan N B</dc:creator>
    </item>
  </channel>
</rss>
