<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Mar 2024 04:02:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Secrecy Outage Probability Analysis for Downlink RIS-NOMA Networks with On-Off Control</title>
      <link>https://arxiv.org/abs/2403.11097</link>
      <description>arXiv:2403.11097v1 Announce Type: cross 
Abstract: Reconfigurable intelligent surface (RIS) has been regarded as a promising technology since it has ability to create the favorable channel conditions. This paper investigates the secure communications of RIS assisted non-orthogonal multiple access (NOMA) networks, where both external and internal eavesdropping scenarios are taken into consideration. More specifically, novel approximate and asymptotic expressions of secrecy outage probability (SOP) for the k-th legitimate user (LU) are derived by invoking imperfect successive interference cancellation (ipSIC) and perfect successive interference cancellation (pSIC). To characterize the secrecy performance of RIS-NOMA networks, the diversity order of the k-th LU with ipSIC/pSIC is obtained in the high signal-to-noise ratio region. The secrecy system throughput of RIS-NOMA networks is discussed in delay-limited transmission mode. Numerical results are presented to verify theoretical analysis that: i) The SOP of RIS-NOMA networks is superior to that of RIS assisted orthogonal multiple access (OMA) and conventional cooperative communication schemes; ii) As the number of reflecting elements increases, the RIS-NOMA networks are capable of achieving the enhanced secrecy performance; and iii) The RIS-NOMA networks have better secrecy system throughput than that of RIS-OMA networks and conventional cooperative communication schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11097v1</guid>
      <category>cs.IT</category>
      <category>cs.PF</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TVT.2023.3267531</arxiv:DOI>
      <arxiv:journal_reference>vol. 72, no. 9, pp. 11772-11786, Sep. 2023</arxiv:journal_reference>
      <dc:creator>Yingjie Pei, Xinwei Yue, Wenqiang Yi, Yuanwei Liu, Xuehua Li, Zhiguo Ding</dc:creator>
    </item>
    <item>
      <title>FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines</title>
      <link>https://arxiv.org/abs/2403.11421</link>
      <description>arXiv:2403.11421v1 Announce Type: cross 
Abstract: Cost of serving large language models (LLM) is high, but the expensive and scarce GPUs are poorly efficient when generating tokens sequentially, unless the batch of sequences is enlarged. However, the batch size is limited by some constantly reused intermediate results, namely KV-Cache. They occupy too much memory to fit more sequences into a GPU simultaneously. While they could be offloaded to host memory, the CPU-GPU bandwidth is an inevitable bottleneck.
  We find a way to decompose the transformer models into two parts of different characteristics, one of which includes the memory-bound KV-Cache accessing. Our key insight is that the aggregated memory capacity, bandwidth, and computing power of CPUs across multiple nodes is an efficient option to process this part. Performance improvement comes from reduced data transmission overhead and boosted GPU throughput to process the other model part. Moreover, we address efficiency challenges brought by heterogeneity at both temporal and inter-device scopes using scheduling and performance modeling techniques. Evaluation results show that our system achieves 1.88x - 5.04x the throughput of vLLM when serving modern LLMs with the same GPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11421v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiaao He, Jidong Zhai</dc:creator>
    </item>
    <item>
      <title>The Power of Training: How Different Neural Network Setups Influence the Energy Demand</title>
      <link>https://arxiv.org/abs/2401.01851</link>
      <description>arXiv:2401.01851v2 Announce Type: replace-cross 
Abstract: This work offers a heuristic evaluation of the effects of variations in machine learning training regimes and learning paradigms on the energy consumption of computing, especially HPC hardware with a life-cycle aware perspective. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also fosters the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to raise awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter configurations are evaluated on three different hardware systems. Among many results, we have found out that even with the same model and hardware to reach the same accuracy, improperly set training hyperparameters consume up to 5 times the energy of the optimal setup. We also extensively examined the energy-saving benefits of learning paradigms including recycling knowledge through pretraining and sharing knowledge through multitask training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01851v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Gei{\ss}ler, Bo Zhou, Mengxi Liu, Sungho Suh, Paul Lukowicz</dc:creator>
    </item>
  </channel>
</rss>
