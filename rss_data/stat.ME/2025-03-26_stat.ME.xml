<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Mar 2025 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian Semi-Parametric Spatial Dispersed Count Model for Precipitation Analysis</title>
      <link>https://arxiv.org/abs/2503.19117</link>
      <description>arXiv:2503.19117v1 Announce Type: new 
Abstract: The appropriateness of the Poisson model is frequently challenged when examining spatial count data marked by unbalanced distributions, over-dispersion, or under-dispersion. Moreover, traditional parametric models may inadequately capture the relationships among variables when covariates display ambiguous functional forms or when spatial patterns are intricate and indeterminate. To tackle these issues, we propose an innovative Bayesian hierarchical modeling system. This method combines non-parametric techniques with an adapted dispersed count model based on renewal theory, facilitating the effective management of unequal dispersion, non-linear correlations, and complex geographic dependencies in count data. We illustrate the efficacy of our strategy by applying it to lung and bronchus cancer mortality data from Iowa, emphasizing environmental and demographic factors like ozone concentrations, PM2.5, green space, and asthma prevalence. Our analysis demonstrates considerable regional heterogeneity and non-linear relationships, providing important insights into the impact of environmental and health-related factors on cancer death rates. This application highlights the significance of our methodology in public health research, where precise modeling and forecasting are essential for guiding policy and intervention efforts. Additionally, we performed a simulation study to assess the resilience and accuracy of the suggested method, validating its superiority in managing dispersion and capturing intricate spatial patterns relative to conventional methods. The suggested framework presents a flexible and robust instrument for geographical count analysis, offering innovative insights for academics and practitioners in disciplines such as epidemiology, environmental science, and spatial statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19117v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahsa Nadifar, Andriette Bekker, Mohammad Arashi, Abel Ramoelo</dc:creator>
    </item>
    <item>
      <title>Exact identifiability analysis for a class of partially observed near-linear stochastic differential equation models</title>
      <link>https://arxiv.org/abs/2503.19241</link>
      <description>arXiv:2503.19241v1 Announce Type: new 
Abstract: Stochasticity plays a key role in many biological systems, necessitating the calibration of stochastic mathematical models to interpret associated data. For model parameters to be estimated reliably, it is typically the case that they must be structurally identifiable. Yet, while theory underlying structural identifiability analysis for deterministic differential equation models is highly developed, there are currently no tools for the general assessment of stochastic models. In this work, we extend the well-established differential algebra framework for structural identifiability analysis to linear and a class of near-linear, two-dimensional, partially observed stochastic differential equation (SDE) models. Our framework is based on a deterministic recurrence relation that describes the dynamics of the statistical moments of the system of SDEs. From this relation, we iteratively form a series of necessarily satisfied equations involving only the observed moments, from which we are able to establish structurally identifiable parameter combinations. We demonstrate our framework for a suite of linear (two- and $n$-dimensional) and non-linear (two-dimensional) models. Most importantly, we define the notion of structural identifiability for SDE models and establish the effect of the initial condition on identifiability. We conclude with a discussion on the applicability and limitations of our approach, and potential future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19241v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander P Browning, Michael J Chappell, Hamid Rahkooy, Torkel E Loman, Ruth E Baker</dc:creator>
    </item>
    <item>
      <title>A general joint latent class model of longitudinal and survival data with the covariance modelling</title>
      <link>https://arxiv.org/abs/2503.19246</link>
      <description>arXiv:2503.19246v1 Announce Type: new 
Abstract: Based on the proposed time-varying JLCM (Miao and Charalambous, 2022), the heterogeneous random covariance matrix can also be considered, and a regression submodel for the variance-covariance matrix of the multivariate latent random effects can be added to the joint latent class model. A general joint latent class model with heterogeneous random-effects modelling is a natural extension of the time-varying JLCM, which consists of the linear and the log link functions to model the covariance matrices as the variance-covariance regression submodel based on the modified Cholesky decomposition, longitudinal submodel, survival submodel as well as the membership probability. It can help to get more information from the random covariance matrix through the regression submodel and get unbiased estimates for all parameters by modelling the variance-covariance matrix. By adding the regression model, the homogeneous random effects assumption can be tested and the issue of high-dimensional heterogeneous random effects can be easily solved. The Bayesian approach will be used to estimate the data. DIC value is the criterion for deciding the optimal k value. We illustrate our general JLCM on a real data set of AIDS study and we are interested in the prospective accuracy of our proposed JLCM as well as doing the dynamic predictions for time-to-death in the joint model using the longitudinal CD4 cell count measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19246v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>A general joint latent class model of longitudinal and survival data with a time-varying membership probability and covariance modelling Miao, R. (Author). 1 Aug 2023 Student thesis: Phd</arxiv:journal_reference>
      <dc:creator>Ruoyu Miao, Christiana Charalambous</dc:creator>
    </item>
    <item>
      <title>Multilevel Monte Carlo Metamodeling for Variance Function Estimation</title>
      <link>https://arxiv.org/abs/2503.19294</link>
      <description>arXiv:2503.19294v1 Announce Type: new 
Abstract: This work introduces a novel multilevel Monte Carlo (MLMC) metamodeling approach for variance function estimation. Although devising an efficient experimental design for simulation metamodeling can be elusive, the MLMC-based approach addresses this challenge by dynamically adjusting the number of design points and budget allocation at each level, thereby automatically creating an efficient design. Theoretical analyses show that, under mild conditions, the proposed MLMC metamodeling approach for variance function estimation can achieve superior computational efficiency compared to standard Monte Carlo metamodeling while achieving the desired level of accuracy. Additionally, this work establishes the asymptotic normality of the MLMC metamodeling estimator under certain sufficient conditions, providing valuable insights for uncertainty quantification. Finally, two MLMC metamodeling procedures are proposed for variance function estimation: one to achieve a target accuracy level and another to efficiently utilize a fixed computational budget. Numerical evaluations support the theoretical results and demonstrate the potential of the proposed approach in facilitating global sensitivity analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19294v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingtao Zhang, Xi Chen</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for High-dimensional Matrix-variate Factor Models with Missing Observations</title>
      <link>https://arxiv.org/abs/2503.19304</link>
      <description>arXiv:2503.19304v1 Announce Type: new 
Abstract: This paper develops an inferential theory for high-dimensional matrix-variate factor models with missing observations. We propose an easy-to-use all-purpose method that involves two straightforward steps. First, we perform principal component analysis on two re-weighted covariance matrices to obtain the row and column loadings. Second, we utilize these loadings along with the matrix-variate data to derive the factors. We develop an inferential theory that establishes the consistency and the rate of convergence under general conditions and missing patterns. The simulation results demonstrate the adequacy of the asymptotic results in approximating the properties of a finite sample. Finally, we illustrate the application of our method using a real numerical dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19304v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yongxia Zhang, Jinwen Liang, Liwen Xu, Keming Yu, Maozai Tian</dc:creator>
    </item>
    <item>
      <title>Bayesian Outlier Detection for Matrix-variate Models</title>
      <link>https://arxiv.org/abs/2503.19515</link>
      <description>arXiv:2503.19515v1 Announce Type: new 
Abstract: Bayes Factor (BF) is one of the tools used in Bayesian analysis for model selection. The predictive BF finds application in detecting outliers, which are relevant sources of estimation and forecast errors. An efficient framework for outlier detection is provided and purposely designed for large multidimensional datasets. Online detection and analytical tractability guarantee the procedure's efficiency. The proposed sequential Bayesian monitoring extends the univariate setup to a matrix--variate one. Prior perturbation based on power discounting is applied to obtain tractable predictive BFs. This way, computationally intensive procedures used in Bayesian Analysis are not required. The conditions leading to inconclusive responses in outlier identification are derived, and some robust approaches are proposed that exploit the predictive BF's variability to improve the standard discounting method. The effectiveness of the procedure is studied using simulated data. An illustration is provided through applications to relevant benchmark datasets from macroeconomics and finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19515v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Monica Billio, Roberto Casarin, Fausto Corradin, Antonio Peruzzi</dc:creator>
    </item>
    <item>
      <title>An Approximate Monte Carlo Simulation Method for Estimating Uncertainty and Constructing Confidence Intervals for 2020 Census Statistics</title>
      <link>https://arxiv.org/abs/2503.19714</link>
      <description>arXiv:2503.19714v1 Announce Type: new 
Abstract: To protect the confidentiality of the 2020 Census, the U.S. Census Bureau adopted a statistical disclosure limitation framework based on the principles of differential privacy. A key component was the TopDown Algorithm, which applied differentially-private noise to an extensive series of counts from the confidential 2020 Census data and transformed the resulting noisy measurements into privacy-protected microdata which were then tabulated to produce official data products. Though the codebase is publicly available, currently there is no way to estimate the uncertainty for statistics included in the published data products that accounts for both the noisy measurements and the post-processing performed within the TopDown Algorithm. We propose an approximate Monte Carlo Simulation method that allows for the estimation of statistical quantities like mean squared error, bias, and variance, as well as the construction of confidence intervals. The method uses the output of the production iteration of the TopDown Algorithm as the input to additional iterations, allowing for statistical quantities to be estimated without impacting the formal privacy protections of the 2020 Census. The results show that, in general, the quantities estimated by this approximate method closely match their intended targets and that the resulting confidence intervals are statistically valid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19714v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Ashmead, Michael B. Hawes, Mary Pritts, Pavel Zhuravlev, Sallie Ann Keller</dc:creator>
    </item>
    <item>
      <title>No-prior Bayesian inference reIMagined: probabilistic approximations of inferential models</title>
      <link>https://arxiv.org/abs/2503.19748</link>
      <description>arXiv:2503.19748v1 Announce Type: new 
Abstract: When prior information is lacking, the go-to strategy for probabilistic inference is to combine a "default prior" and the likelihood via Bayes's theorem. Objective Bayes, (generalized) fiducial inference, etc. fall under this umbrella. This construction is natural, but the corresponding posterior distributions generally only offer limited, approximately valid uncertainty quantification. The present paper takes a reimagined approach offering posterior distributions with stronger reliability properties. The proposed construction starts with an inferential model (IM), one that takes the mathematical form of a data-driven possibility measure and features exactly valid uncertainty quantification, and then returns a so-called inner probabilistic approximation thereof. This inner probabilistic approximation inherits many of the original IM's desirable properties, including credible sets with exact coverage and asymptotic efficiency. The approximation also agrees with the familiar Bayes/fiducial solution obtained in applications where the model has a group transformation structure. A Monte Carlo method for evaluating the probabilistic approximation is presented, along with numerical illustrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19748v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>A burn-in(g) question: How long should an initial equal randomization stage be before Bayesian response-adaptive randomization?</title>
      <link>https://arxiv.org/abs/2503.19795</link>
      <description>arXiv:2503.19795v1 Announce Type: new 
Abstract: Response-adaptive (RA) trials offer the potential to enhance participant benefit but also complicate valid statistical analysis and potentially lead to a higher proportion of participants receiving an inferior treatment. A common approach to mitigate these disadvantages is to introduce a fixed non-adaptive randomization stage at the start of the RA design, known as the burn-in period. Currently, investigations and guidance on the effect of the burn-in length are scarce. To this end, this paper provides an exact evaluation approach to investigate how the burn-in length impacts the statistical properties of two-arm binary RA designs. We show that (1) for commonly used calibration and asymptotic tests an increase in the burn-in length reduces type I error rate inflation but does not lead to strict type I error rate control, necessitating exact tests; (2) the burn-in length substantially influences the power and participant benefit, and these measures are often not maximized at the maximum or minimum possible burn-in length; (3) the conditional exact test conditioning on total successes provides the highest average and minimum power for both small and moderate burn-in lengths compared to other tests. Using our exact analysis method, we re-design the ARREST trial to improve its statistical properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19795v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edwin Y. N. Tang, Stef Baas, Daniel Kaddaj, Lukas Pin, David S. Robertson, Sof\'ia S. Villar</dc:creator>
    </item>
    <item>
      <title>Minimum Volume Conformal Sets for Multivariate Regression</title>
      <link>https://arxiv.org/abs/2503.19068</link>
      <description>arXiv:2503.19068v1 Announce Type: cross 
Abstract: Conformal prediction provides a principled framework for constructing predictive sets with finite-sample validity. While much of the focus has been on univariate response variables, existing multivariate methods either impose rigid geometric assumptions or rely on flexible but computationally expensive approaches that do not explicitly optimize prediction set volume. We propose an optimization-driven framework based on a novel loss function that directly learns minimum-volume covering sets while ensuring valid coverage. This formulation naturally induces a new nonconformity score for conformal prediction, which adapts to the residual distribution and covariates. Our approach optimizes over prediction sets defined by arbitrary norm balls, including single and multi-norm formulations. Additionally, by jointly optimizing both the predictive model and predictive uncertainty, we obtain prediction sets that are tight, informative, and computationally efficient, as demonstrated in our experiments on real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19068v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sacha Braun, Liviu Aolaritei, Michael I. Jordan, Francis Bach</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes shrinkage (mostly) does not correct the measurement error in regression</title>
      <link>https://arxiv.org/abs/2503.19095</link>
      <description>arXiv:2503.19095v1 Announce Type: cross 
Abstract: In the value-added literature, it is often claimed that regressing on empirical Bayes shrinkage estimates corrects for the measurement error problem in linear regression. We clarify the conditions needed; we argue that these conditions are stronger than the those needed for classical measurement error correction, which we advocate for instead. Moreover, we show that the classical estimator cannot be improved without stronger assumptions. We extend these results to regressions on nonlinear transformations of the latent attribute and find generically slow minimax estimation rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19095v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiafeng Chen, Jiaying Gu, Soonwoo Kwon</dc:creator>
    </item>
    <item>
      <title>Identification of Average Treatment Effects in Nonparametric Panel Models</title>
      <link>https://arxiv.org/abs/2503.19873</link>
      <description>arXiv:2503.19873v1 Announce Type: cross 
Abstract: This paper studies identification of average treatment effects in a panel data setting. It introduces a novel nonparametric factor model and proves identification of average treatment effects. The identification proof is based on the introduction of a consistent estimator. Underlying the proof is a result that there is a consistent estimator for the expected outcome in the absence of the treatment for each unit and time period; this result can be applied more broadly, for example in problems of decompositions of group-level differences in outcomes, such as the much-studied gender wage gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19873v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susan Athey, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>Conditional partial exchangeability: a probabilistic framework for multi-view clustering</title>
      <link>https://arxiv.org/abs/2307.01152</link>
      <description>arXiv:2307.01152v2 Announce Type: replace 
Abstract: Standard clustering techniques assume a common configuration for all features in a dataset. However, when dealing with multi-view or longitudinal data, the clusters' number, frequencies, and shapes may need to vary across features to accurately capture dependence structures and heterogeneity. In this setting, classical model-based clustering fails to account for within-subject dependence across domains. We introduce conditional partial exchangeability, a novel probabilistic paradigm for dependent random partitions of the same objects across distinct domains. Additionally, we study a wide class of Bayesian clustering models based on conditional partial exchangeability, which allows for flexible dependent clustering of individuals across features, capturing the specific contribution of each feature and the within-subject dependence, while ensuring computational feasibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01152v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Beatrice Franzolini, Maria De Iorio, Johan Eriksson</dc:creator>
    </item>
    <item>
      <title>Multiple Testing of Local Extrema for Detection of Structural Breaks in Piecewise Linear Models</title>
      <link>https://arxiv.org/abs/2308.04368</link>
      <description>arXiv:2308.04368v4 Announce Type: replace 
Abstract: In this paper, we propose a new generic method for detecting the number and locations of structural breaks or change points in piecewise linear models under stationary Gaussian noise. Our method transforms the change point detection problem into identifying local extrema (local maxima and local minima) through kernel smoothing and differentiation of the data sequence. By computing p-values for all local extrema based on peak height distributions of smooth Gaussian processes, we utilize the Benjamini-Hochberg procedure to identify significant local extrema as the detected change points. Our method can distinguish between two types of change points: continuous breaks (Type I) and jumps (Type II). We study three scenarios of piecewise linear signals, namely pure Type I, pure Type II and a mixture of Type I and Type II change points. The results demonstrate that our proposed method ensures asymptotic control of the False Discover Rate (FDR) and power consistency, as sequence length, slope changes, and jump size increase. Furthermore, compared to traditional change point detection methods based on recursive segmentation, our approach only requires a single test for all candidate local extrema, thereby achieving the smallest computational complexity proportionate to the data sequence length. Additionally, numerical studies illustrate that our method maintains FDR control and power consistency, even in non-asymptotic cases when the size of slope changes or jumps is not large. We have implemented our method in the R package "dSTEM" (available from https://cran.r-project.org/web/packages/dSTEM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.04368v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhibing He, Dan Cheng, Yunpeng Zhao</dc:creator>
    </item>
    <item>
      <title>Causal Panel Analysis under Parallel Trends: Lessons from A Large Reanalysis Study</title>
      <link>https://arxiv.org/abs/2309.15983</link>
      <description>arXiv:2309.15983v5 Announce Type: replace 
Abstract: Two-way fixed effects (TWFE) models are widely used in political science to establish causality, but recent methodological discussions highlight their limitations under heterogeneous treatment effects (HTE) and violations of the parallel trends (PT) assumption. This growing literature has introduced numerous new estimators and procedures, causing confusion among researchers about the reliability of existing results and best practices. To address these concerns, we replicated and reanalyzed 49 studies from leading journals using TWFE models for observational panel data with binary treatments. Using six HTE-robust estimators, diagnostic tests, and sensitivity analyses, we find: (i) HTE-robust estimators yield qualitatively similar but highly variable results; (ii) while a few studies show clear signs of PT violations, many lack evidence to support this assumption; and (iii) many studies are underpowered when accounting for HTE and potential PT violations. We emphasize the importance of strong research designs and rigorous validation of key identifying assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15983v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Albert Chiu, Xingchen Lan, Ziyi Liu, Yiqing Xu</dc:creator>
    </item>
    <item>
      <title>Robust integration of external control data in randomized trials</title>
      <link>https://arxiv.org/abs/2406.17971</link>
      <description>arXiv:2406.17971v2 Announce Type: replace 
Abstract: One approach for increasing the efficiency of randomized trials is the use of "external controls" -- individuals who received the control treatment studied in the trial during routine practice or in prior experimental studies. Existing external control methods, however, can be biased if the populations underlying the trial and the external control data are not exchangeable. Here, we characterize a randomization-aware class of treatment effect estimators in the population underlying the trial that remain consistent and asymptotically normal when using external control data, even when exchangeability does not hold. We consider two members of this class of estimators: the well-known augmented inverse probability weighting trial-only estimator, which is the efficient estimator when only trial data are used; and a potentially more efficient member of the class when exchangeability holds and external control data are available, which we refer to as the optimized randomization-aware estimator. To achieve robust integration of external control data in trial analyses, we then propose a combined estimator based on the efficient trial-only estimator and the optimized randomization-aware estimator. We show that the combined estimator is consistent and no less efficient than the most efficient of the two component estimators, whether the exchangeability assumption holds or not. We examine the estimators' performance in simulations and we illustrate their use with data from two trials of paliperidone extended-release for schizophrenia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17971v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rickard Karlsson, Guanbo Wang, Piersilvio De Bartolomeis, Jesse H. Krijthe, Issa J. Dahabreh</dc:creator>
    </item>
    <item>
      <title>Huber-robust likelihood ratio tests for composite nulls and alternatives</title>
      <link>https://arxiv.org/abs/2408.14015</link>
      <description>arXiv:2408.14015v4 Announce Type: replace 
Abstract: We propose an e-value based framework for testing composite nulls against composite alternatives when an $\epsilon$ fraction of the data can be arbitrarily corrupted. Our tests are inherently sequential, being valid at arbitrary data-dependent stopping times, but they are new even for fixed sample sizes, giving type-I error control without any regularity conditions. We achieve this by modifying and extending a proposal by Huber (1965) in the point null versus point alternative case. Our test statistic is a nonnegative supermartingale under the null, even with a sequentially adaptive contamination model where the conditional distribution of each observation given the past data lies within an $\epsilon$ (total variation) ball of the null. The test is powerful within an $\epsilon$ ball of the alternative. As a consequence, one obtains anytime-valid p-values that enable continuous monitoring of the data, and adaptive stopping. We analyze the growth rate of our test supermartingale and demonstrate that as $\epsilon\to 0$, it approaches a certain Kullback-Leibler divergence between the null and alternative, which is the optimal non-robust growth rate. A key step is the derivation of a robust Reverse Information Projection (RIPr). Simulations validate the theory and demonstrate excellent practical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14015v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aytijhya Saha, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>A general framework for probabilistic model uncertainty</title>
      <link>https://arxiv.org/abs/2410.17108</link>
      <description>arXiv:2410.17108v2 Announce Type: replace 
Abstract: Existing approaches to model uncertainty typically either compare models using a quantitative model selection criterion or evaluate posterior model probabilities having set a prior. In this paper, we propose an alternative strategy which views missing observations as the source of model uncertainty, where the true model would be identified with the complete data. To quantify model uncertainty, it is then necessary to provide a probability distribution for the missing observations conditional on what has been observed. This can be set sequentially using one-step-ahead predictive densities, which recursively sample from the best model according to some consistent model selection criterion. Repeated predictive sampling of the missing data, to give a complete dataset and hence a best model each time, provides our measure of model uncertainty. This approach bypasses the need for subjective prior specification or integration over parameter spaces, addressing issues with standard methods such as the Bayes factor. Predictive resampling also suggests an alternative view of hypothesis testing as a decision problem based on a population statistic, where we directly index the probabilities of competing models. In addition to hypothesis testing, we demonstrate our approach on illustrations from density estimation and variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17108v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vik Shirvaikar, Stephen G. Walker, Chris Holmes</dc:creator>
    </item>
    <item>
      <title>Adding covariates to bounds: What is the question?</title>
      <link>https://arxiv.org/abs/2502.03156</link>
      <description>arXiv:2502.03156v2 Announce Type: replace 
Abstract: Symbolic nonparametric bounds for partial identification of causal effects now have a long history in the causal literature. Sharp bounds, bounds that use all available information to make the range of values as narrow as possible, are often the goal. For this reason, many publications have focused on deriving sharp bounds, but the concept of sharp bounds is nuanced and can be misleading. In settings with ancillary covariates, the situation becomes more complex. We provide clear definitions for pointwise and uniform sharpness of covariate-conditional bounds, that we then use to prove some general and some specific to the IV setting results about the relationship between these two concepts. As we demonstrate, general conditions are much more difficult to determine and thus, we urge authors to be clear when including ancillary covariates in bounds via conditioning about the setting of interest and the assumptions made.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03156v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gustav Jonzon, Erin E Gabriel, Arvid Sj\"olander, Michael C Sachs</dc:creator>
    </item>
    <item>
      <title>Variational inference for hierarchical models with conditional scale and skewness corrections</title>
      <link>https://arxiv.org/abs/2503.18075</link>
      <description>arXiv:2503.18075v2 Announce Type: replace 
Abstract: Gaussian variational approximations are widely used for summarizing posterior distributions in Bayesian models, especially in high-dimensional settings. However, a drawback of such approximations is the inability to capture skewness or more complex features of the posterior. Recent work suggests applying skewness corrections to existing Gaussian or other symmetric approximations to address this limitation. We propose to incorporate the skewness correction into the definition of an approximating variational family. We consider approximating the posterior for hierarchical models, in which there are ``global'' and ``local'' parameters. A baseline variational approximation is defined as the product of a Gaussian marginal posterior for global parameters and a Gaussian conditional posterior for local parameters given the global ones. Skewness corrections are then considered. The adjustment of the conditional posterior term for local variables is adaptive to the global parameter value. Optimization of baseline variational parameters is performed jointly with the skewness correction. Our approach allows the location, scale and skewness to be captured separately, without using additional parameters for skewness adjustments. The proposed method substantially improves accuracy for only a modest increase in computational cost compared to state-of-the-art Gaussian approximations. Good performance is demonstrated in generalized linear mixed models and multinomial logit discrete choice models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18075v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Kock, Linda S. L. Tan, Prateek Bansal, David J. Nott</dc:creator>
    </item>
    <item>
      <title>Quantifying the Internal Validity of Weighted Estimands</title>
      <link>https://arxiv.org/abs/2404.14603</link>
      <description>arXiv:2404.14603v2 Announce Type: replace-cross 
Abstract: In this paper we study a class of weighted estimands, which we define as parameters that can be expressed as weighted averages of the underlying heterogeneous treatment effects. The popular ordinary least squares (OLS), two-stage least squares (2SLS), and two-way fixed effects (TWFE) estimands are all special cases within our framework. Our focus is on answering two questions concerning weighted estimands. First, under what conditions can they be interpreted as the average treatment effect for some (possibly latent) subpopulation? Second, when these conditions are satisfied, what is the upper bound on the size of that subpopulation, either in absolute terms or relative to a target population of interest? We argue that this upper bound provides a valuable diagnostic for empirical research. When a given weighted estimand corresponds to the average treatment effect for a small subset of the population of interest, we say its internal validity is low. Our paper develops practical tools to quantify the internal validity of weighted estimands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14603v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Poirier, Tymon S{\l}oczy\'nski</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Estimators based on the Block Maxima Method</title>
      <link>https://arxiv.org/abs/2409.05529</link>
      <description>arXiv:2409.05529v2 Announce Type: replace-cross 
Abstract: The block maxima method is a standard approach for analyzing the extremal behavior of a potentially multivariate time series. It has recently been found that the classical approach based on disjoint block maxima may be universally improved by considering sliding block maxima instead. However, the asymptotic variance formula for estimators based on sliding block maxima involves an integral over the covariance of a certain family of multivariate extreme value distributions, which makes its estimation, and inference in general, an intricate problem. As an alternative, one may rely on bootstrap approximations: we show that naive block-bootstrap approaches from time series analysis are inconsistent even in i.i.d.\ situations, and provide a consistent alternative based on resampling circular block maxima. As a by-product, we show consistency of the classical resampling bootstrap for disjoint block maxima, and that estimators based on circular block maxima have the same asymptotic variance as their sliding block maxima counterparts. The finite sample properties are illustrated by Monte Carlo experiments, and the methods are demonstrated by a case study of precipitation extremes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05529v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axel B\"ucher, Torben Staud</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of Hawkes processes with RKHSs</title>
      <link>https://arxiv.org/abs/2411.00621</link>
      <description>arXiv:2411.00621v2 Announce Type: replace-cross 
Abstract: This paper addresses nonparametric estimation of nonlinear multivariate Hawkes processes, where the interaction functions are assumed to lie in a reproducing kernel Hilbert space (RKHS). Motivated by applications in neuroscience, the model allows complex interaction functions, in order to express exciting and inhibiting effects, but also a combination of both (which is particularly interesting to model the refractory period of neurons), and considers in return that conditional intensities are rectified by the ReLU function. The latter feature incurs several methodological challenges, for which workarounds are proposed in this paper. In particular, it is shown that a representer theorem can be obtained for approximated versions of the log-likelihood and the least-squares criteria. Based on it, we propose an estimation method, that relies on two common approximations (of the ReLU function and of the integral operator). We provide a bound that controls the impact of these approximations. Numerical results on synthetic data confirm this fact as well as the good asymptotic behavior of the proposed estimator. It also shows that our method achieves a better performance compared to related nonparametric estimation techniques and suits neuronal applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00621v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Bonnet, Maxime Sangnier</dc:creator>
    </item>
    <item>
      <title>Cheap Permutation Testing</title>
      <link>https://arxiv.org/abs/2502.07672</link>
      <description>arXiv:2502.07672v2 Announce Type: replace-cross 
Abstract: Permutation tests are a popular choice for distinguishing distributions and testing independence, due to their exact, finite-sample control of false positives and their minimax optimality when paired with U-statistics. However, standard permutation tests are also expensive, requiring a test statistic to be computed hundreds or thousands of times to detect a separation between distributions. In this work, we offer a simple approach to accelerate testing: group your datapoints into bins and permute only those bins. For U and V-statistics, we prove that these cheap permutation tests have two remarkable properties. First, by storing appropriate sufficient statistics, a cheap test can be run in time comparable to evaluating a single test statistic. Second, cheap permutation power closely approximates standard permutation power. As a result, cheap tests inherit the exact false positive control and minimax optimality of standard permutation tests while running in a fraction of the time. We complement these findings with improved power guarantees for standard permutation testing and experiments demonstrating the benefits of cheap permutations over standard maximum mean discrepancy (MMD), Hilbert-Schmidt independence criterion (HSIC), random Fourier feature, Wilcoxon-Mann-Whitney, cross-MMD, and cross-HSIC tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07672v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carles Domingo-Enrich, Raaz Dwivedi, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>Locally Private Nonparametric Contextual Multi-armed Bandits</title>
      <link>https://arxiv.org/abs/2503.08098</link>
      <description>arXiv:2503.08098v2 Announce Type: replace-cross 
Abstract: Motivated by privacy concerns in sequential decision-making on sensitive data, we address the challenge of nonparametric contextual multi-armed bandits (MAB) under local differential privacy (LDP). We develop a uniform-confidence-bound-type estimator, showing its minimax optimality supported by a matching minimax lower bound. We further consider the case where auxiliary datasets are available, subject also to (possibly heterogeneous) LDP constraints. Under the widely-used covariate shift framework, we propose a jump-start scheme to effectively utilize the auxiliary data, the minimax optimality of which is further established by a matching lower bound. Comprehensive experiments on both synthetic and real-world datasets validate our theoretical results and underscore the effectiveness of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08098v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuheng Ma, Feiyu Jiang, Zifeng Zhao, Hanfang Yang, Yi Yu</dc:creator>
    </item>
  </channel>
</rss>
