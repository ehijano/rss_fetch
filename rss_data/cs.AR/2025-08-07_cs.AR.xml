<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Aug 2025 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent Memory Subsystems</title>
      <link>https://arxiv.org/abs/2508.03837</link>
      <description>arXiv:2508.03837v1 Announce Type: new 
Abstract: Designing and validating efficient cache-coherent memory subsystems is a critical yet complex task in the development of modern multi-core system-on-chip architectures. Rhea is a unified framework that streamlines the design and system-level validation of RTL cache-coherent memory subsystems. On the design side, Rhea generates synthesizable, highly configurable RTL supporting various architectural parameters. On the validation side, Rhea integrates Verilator's cycle-accurate RTL simulation with gem5's full-system simulation, allowing realistic workloads and operating systems to run alongside the actual RTL under test. We apply Rhea to design MSI-based RTL memory subsystems with one and two levels of private caches and scaling up to sixteen cores. Their evaluation with 22 applications from state-of-the-art benchmark suites shows intermediate performance relative to gem5 Ruby's MI and MOESI models. The hybrid gem5-Verilator co-simulation flow incurs a moderate simulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher fidelity by simulating real RTL hardware. This overhead decreases with scale, down to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's effectiveness and scalability in enabling fast development of RTL cache-coherent memory subsystem designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03837v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Zoni, Andrea Galimberti, Adriano Guarisco</dc:creator>
    </item>
    <item>
      <title>FlashVault: Versatile In-NAND Self-Encryption with Zero Area Overhead</title>
      <link>https://arxiv.org/abs/2508.03866</link>
      <description>arXiv:2508.03866v1 Announce Type: new 
Abstract: We present FlashVault, an in-NAND self-encryption architecture that embeds a reconfigurable cryptographic engine into the unused silicon area of a state-of-the-art 4D V-NAND structure. FlashVault supports not only block ciphers for data encryption but also public-key and post-quantum algorithms for digital signatures, all within the NAND flash chip. This design enables each NAND chip to operate as a self-contained enclave without incurring area overhead, while eliminating the need for off-chip encryption. We implement FlashVault at the register-transfer level (RTL) and perform place-and-route (P&amp;R) for accurate power/area evaluation. Our analysis shows that the power budget determines the number of cryptographic engines per NAND chip. We integrate this architectural choice into a full-system simulation and evaluate its performance on a wide range of cryptographic algorithms. Our results show that FlashVault consistently outperforms both CPU-based encryption (1.46~3.45x) and near-core processing architecture (1.02~2.01x), demonstrating its effectiveness as a secure SSD architecture that meets diverse cryptographic requirements imposed by regulatory standards and enterprise policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03866v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seock-Hwan Noh, Hoyeon Lee, Junkyum Kim, Junsu Im, Jay H. Park, Sungjin Lee, Sam H. Noh, Yeseong Kim, Jaeha Kung</dc:creator>
    </item>
    <item>
      <title>TROOP: At-the-Roofline Performance for Vector Processors on Low Operational Intensity Workloads</title>
      <link>https://arxiv.org/abs/2508.03900</link>
      <description>arXiv:2508.03900v1 Announce Type: new 
Abstract: The fast evolution of Machine Learning (ML) models requires flexible and efficient hardware solutions as hardwired accelerators face rapid obsolescence. Vector processors are fully programmable and achieve high energy efficiencies by exploiting data parallelism, amortizing instruction fetch and decoding costs. Hence, a promising design choice is to build accelerators based on shared L1-memory clusters of streamlined Vector Processing Elements (VPEs). However, current state-of-the-art VPEs are limited in L1 memory bandwidth and achieve high efficiency only for computational kernels with high data reuse in the Vector Register File (VRF), such as General Matrix Multiplication (GEMM). Performance is suboptimal for workloads with lower data reuse like General Matrix-Vector Multiplication (GEMV). To fully exploit available bandwidth at the L1 memory interface, the VPE micro-architecture must be optimized to achieve near-ideal utilization, i.e., to be as close as possible to the L1 memory roofline (at-the-roofline). In this work, we propose TROOP, a set of hardware optimizations that include decoupled load-store interfaces, improved vector chaining, shadow buffers to hide VRF conflicts, and address scrambling techniques to achieve at-the-roofline performance for VPEs without compromising their area and energy efficiency. We implement TROOP on an open-source streamlined vector processor in a 12nm FinFET technology. TROOP achieves significant speedups of 1.5x, 2.2x, and 2.6x, respectively, for key memory-intensive kernels such as GEMV, DOTP and AXPY, achieving at-the-roofline performance. Additionally, TROOP enhances the energy efficiency by up to 45%, reaching 38 DP-GFLOPs/W (1 GHz, TT, 0.8V) for DOTP while maintaining a high energy efficiency of 61 DP-GFLOPs/W for GEMMs, incurring only a minor area overhead of less than 7%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03900v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Navaneeth Kunhi Purayil, Diyou Shen, Matteo Perotti, Luca Benini</dc:creator>
    </item>
    <item>
      <title>OpenYield: An Open-Source SRAM Yield Analysis and Optimization Benchmark Suite</title>
      <link>https://arxiv.org/abs/2508.04106</link>
      <description>arXiv:2508.04106v1 Announce Type: new 
Abstract: Static Random-Access Memory (SRAM) yield analysis is essential for semiconductor innovation, yet research progress faces a critical challenge: the significant disconnect between simplified academic models and complex industrial realities. The absence of open, realistic benchmarks has created a reproducibility crisis, where promising academic techniques often fail to translate to industrial practice. We present \textit{OpenYield}, a comprehensive open-source ecosystem designed to address this critical gap through three core contributions: (1) A realistic SRAM circuit generator that uniquely incorporates critical second-order-effect parasitics, inter-cell leakage coupling, and peripheral circuit variations, which are typically omitted in academic studies but decisive in industrial designs. (2) A standardized evaluation platform with a simple interface and implemented baseline yield analysis algorithms, enabling fair comparisons and reproducible research. (3) A standardized SRAM optimization platform, demonstrating OpenYield's utility in enhancing SRAM design robustness and efficiency, providing a comprehensive benchmark for optimization algorithms. OpenYield creates a foundation for meaningful academia-industry collaboration, accelerating innovation in memory design. The framework is publicly available on \href{https://github.com/ShenShan123/OpenYield}{OpenYield:URL}</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04106v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shan Shen, Xingyang Li, Zhuohua Liu, Yikai Wang, Yiheng Wu, Junhao Ma, Yuquan Sun, Wei W. Xing</dc:creator>
    </item>
    <item>
      <title>ECOLogic: Enabling Circular, Obfuscated, and Adaptive Logic via eFPGA-Augmented SoCs</title>
      <link>https://arxiv.org/abs/2508.04516</link>
      <description>arXiv:2508.04516v1 Announce Type: new 
Abstract: Traditional hardware platforms - ASICs and FPGAs - offer competing trade-offs among performance, flexibility, and sustainability. ASICs provide high efficiency but are inflexible post-fabrication, require costly re-spins for updates, and expose IPs to piracy risks. FPGAs offer reconfigurability and reuse, yet suffer from substantial area, power, and performance overheads, resulting in higher carbon footprints. We present ECOLogic, a hybrid design paradigm that embeds lightweight eFPGA fabric within ASICs to enable secure, updatable, and resource-aware computation. Central to this architecture is ECOScore, a quantitative scoring framework that evaluates IPs based on adaptability, piracy threat, performance tolerance, and resource fit to guide RTL partitioning. Evaluated across six diverse SoC modules, ECOLogic retains an average of 90 percent ASIC-level performance (up to 2 GHz), achieves 9.8 ns timing slack (versus 5.1 ns in FPGA), and reduces power by 480 times on average. Moreover, sustainability analysis shows a 99.7 percent reduction in deployment carbon footprint and 300 to 500 times lower emissions relative to FPGA-only implementations. These results position ECOLogic as a high-performance, secure, and environmentally sustainable solution for next-generation reconfigurable systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04516v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ishraq Tashdid, Dewan Saiham, Nafisa Anjum, Tasnuva Farheen, Sazadur Rahman</dc:creator>
    </item>
    <item>
      <title>Near instantaneous O(1) Analog Solver Circuit for Linear Symmetric Positive-Definite Systems</title>
      <link>https://arxiv.org/abs/2508.04609</link>
      <description>arXiv:2508.04609v1 Announce Type: new 
Abstract: Accelerating the solution of linear systems of equations is critical due to their central role in numerous applications, such as scientific simulations, data analytics, and machine learning. This paper presents a general-purpose analog direct solver circuit designed to accelerate the solution of positive definite symmetric linear systems of equations. The proposed design leverages non-inverting operational amplifier configurations to create a negative resistance circuit, effectively modeling any symmetric system. The paper details the principles behind the design, optimizations of the system architecture, and numerical results that demonstrate the robustness of the design. The findings reveal that the proposed system solves diagonally dominant symmetric matrices with O(1) complexity, achieving the theoretical maximum speed as the circuit relies solely on resistors. For non-diagonally dominant symmetric positive-definite systems, the solution speed depends on matrix properties such as eigenvalues and the maximum off-diagonal term, but remains independent of matrix size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04609v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Osama Abdelaleim, Arun Prakash, Ayhan Irfanoglu, Veljko Milutinovic</dc:creator>
    </item>
    <item>
      <title>Channel-Coherence-Adaptive Two-Stage Fully Digital Combining for mmWave MIMO Systems</title>
      <link>https://arxiv.org/abs/2508.04214</link>
      <description>arXiv:2508.04214v1 Announce Type: cross 
Abstract: This paper considers a millimeter-wave wideband point-to-point MIMO system with fully digital transceivers at the base station and the user equipment (UE), focusing on mobile UE scenarios. A main challenge when building a digital UE combining is the large volume of baseband samples to handle. To mitigate computational and hardware complexity, we propose a novel two-stage digital combining scheme at the UE. The first stage reduces the $N_{\text{r}}$ received signals to $N_{\text{c}}$ streams before baseband processing, leveraging channel geometry for dimension reduction and updating at the beam coherence time, which is longer than the channel coherence time of the small-scale fading. By contrast, the second-stage combining is updated per fading realization. We develop a pilot-based channel estimation framework for this hardware setup based on maximum likelihoodestimation in both uplink and downlink. Digital precoding and combining designs are proposed, and a spectral efficiency expression that incorporates imperfect channel knowledge is derived. The numerical results demonstrate that the proposed approach outperforms hybrid beamforming, showcasing the attractiveness of using two-stage fully digital transceivers in future systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04214v1</guid>
      <category>eess.SP</category>
      <category>cs.AR</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasaman Khorsandmanesh, Emil Bj\"ornson, Joakim Jald\'en, Bengt Lindoff</dc:creator>
    </item>
    <item>
      <title>Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs</title>
      <link>https://arxiv.org/abs/2504.06211</link>
      <description>arXiv:2504.06211v2 Announce Type: replace 
Abstract: Zero-Knowledge Proofs (ZKPs) are rapidly gaining importance in privacy-preserving and verifiable computing. ZKPs enable a proving party to prove the truth of a statement to a verifying party without revealing anything else. ZKPs have applications in blockchain technologies, verifiable machine learning, and electronic voting, but have yet to see widespread adoption due to the computational complexity of the proving process. Recent works have accelerated the key primitives of state-of-the-art ZKP protocols on GPU and ASIC. However, the protocols accelerated thus far face one of two challenges: they either require a trusted setup for each application, or they generate larger proof sizes with higher verification costs, limiting their applicability in scenarios with numerous verifiers or strict verification time constraints. This work presents an accelerator, zkSpeed, for HyperPlonk, a state-of-the-art ZKP protocol that supports both one-time, universal setup and small proof sizes for typical ZKP applications in publicly verifiable, consensus-based systems. We accelerate the entire protocol, including two major primitives: SumCheck and Multi-scalar Multiplications (MSMs). We develop a full-chip architecture using 366.46 mm$^2$ and 2 TB/s of bandwidth to accelerate the entire proof generation process, achieving geometric mean speedups of 801$\times$ over CPU baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06211v2</guid>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3695053.3731021</arxiv:DOI>
      <dc:creator>Alhad Daftardar, Jianqiao Mo, Joey Ah-kiow, Benedikt B\"unz, Ramesh Karri, Siddharth Garg, Brandon Reagen</dc:creator>
    </item>
    <item>
      <title>RTLCoder: Outperforming GPT-3.5 in Design RTL Generation with Our Open-Source Dataset and Lightweight Solution</title>
      <link>https://arxiv.org/abs/2312.08617</link>
      <description>arXiv:2312.08617v5 Announce Type: replace-cross 
Abstract: The automatic generation of RTL code (e.g., Verilog) using natural language instructions and large language models (LLMs) has attracted significant research interest recently. However, most existing approaches heavily rely on commercial LLMs such as ChatGPT, while open-source LLMs tailored for this specific design generation task exhibit notably inferior performance. The absence of high-quality open-source solutions restricts the flexibility and data privacy of this emerging technique. In this study, we present a new customized LLM solution with a modest parameter count of only 7B, achieving better performance than GPT-3.5 on all representative benchmarks for RTL code generation. Especially, it outperforms GPT-4 in VerilogEval Machine benchmark. This remarkable balance between accuracy and efficiency is made possible by leveraging our new RTL code dataset and a customized LLM algorithm, both of which have been made fully open-source.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08617v5</guid>
      <category>cs.PL</category>
      <category>cs.AR</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/LAD62341.2024.10691788</arxiv:DOI>
      <dc:creator>Shang Liu, Wenji Fang, Yao Lu, Qijun Zhang, Hongce Zhang, Zhiyao Xie</dc:creator>
    </item>
    <item>
      <title>Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications</title>
      <link>https://arxiv.org/abs/2405.15877</link>
      <description>arXiv:2405.15877v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) significantly enhance the performance of various applications, but they are computationally intensive and energy-demanding. This makes it challenging to deploy them on devices with limited resources, such as personal computers and mobile/wearable devices, and results in substantial inference costs in resource-rich environments like cloud servers. To extend the use of LLMs, we introduce a low-rank decomposition approach to effectively compress these models, tailored to the requirements of specific applications. We observe that LLMs pretrained on general datasets contain many redundant components not needed for particular applications. Our method focuses on identifying and removing these redundant parts, retaining only the necessary elements for the target applications. Specifically, we represent the weight matrices of LLMs as a linear combination of base components. We then prune the irrelevant bases and enhance the model with new bases beneficial for specific applications. Deep compression results on the Llama 2-7b and -13B models, conducted on target applications including mathematical reasoning and code generation, show that our method significantly reduces model size while maintaining comparable accuracy to state-of-the-art low-rank compression techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15877v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>cs.CL</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Li, Daniel Agyei Asante, Changsheng Zhao, Ernie Chang, Yangyang Shi, Vikas Chandra</dc:creator>
    </item>
    <item>
      <title>Ultra Memory-Efficient On-FPGA Training of Transformers via Tensor-Compressed Optimization</title>
      <link>https://arxiv.org/abs/2501.06663</link>
      <description>arXiv:2501.06663v2 Announce Type: replace-cross 
Abstract: Transformer models have achieved state-of-the-art performance across a wide range of machine learning tasks. There is growing interest in training transformers on resource-constrained edge devices due to considerations such as privacy, domain adaptation, and on-device scientific machine learning. However, the significant computational and memory demands required for transformer training often exceed the capabilities of an edge device. Leveraging low-rank tensor compression, this paper presents the first on-FPGA accelerator for end-to-end transformer training. On the algorithm side, we present a bi-directional contraction flow for tensorized transformer training, significantly reducing the computational FLOPS and intra-layer memory costs compared to existing tensor operations. On the hardware side, we store all highly compressed model parameters and gradient information on chip, creating an on-chip-memory-only framework for each stage in training. This reduces off-chip communication and minimizes latency and energy costs. Additionally, we implement custom computing kernels for each training stage and employ intra-layer parallelism and pipe-lining to further enhance run-time and memory efficiency. Through experiments on transformer models within $36.7$ to $93.5$ MB using FP-32 data formats on the ATIS dataset, our tensorized FPGA accelerator could conduct single-batch end-to-end training on the AMD Alevo U50 FPGA, with a memory budget of less than $6$-MB BRAM and $22.5$-MB URAM. Compared to uncompressed training on the NVIDIA RTX 3090 GPU, our on-FPGA training achieves a memory reduction of $30\times$ to $51\times$. Our FPGA accelerator also achieves up to $3.6\times$ less energy cost per epoch compared with tensor Transformer training on an NVIDIA RTX 3090 GPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06663v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>cs.CL</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayi Tian, Jinming Lu, Hai Li, Xiangwei Wang, Cong Hao, Ian Young, Zheng Zhang</dc:creator>
    </item>
    <item>
      <title>GenEDA: Towards Generative Netlist Functional Reasoning via Cross-Modal Circuit Encoder-Decoder Alignment</title>
      <link>https://arxiv.org/abs/2504.09485</link>
      <description>arXiv:2504.09485v2 Announce Type: replace-cross 
Abstract: The success of foundation AI has motivated the research of circuit foundation models, which are customized to assist the integrated circuit (IC) design process. However, existing pre-trained circuit foundation models are typically limited to standalone encoders for predictive tasks or decoders for generative tasks. These two model types are developed independently, operate on different circuit modalities, and reside in separate latent spaces. This restricts their ability to complement each other for more advanced capabilities. In this work, we present GenEDA, the first framework that cross-modally aligns circuit encoders with decoders within a shared latent space. GenEDA bridges the gap between graph-based circuit representation learning and text-based large language models (LLMs), enabling communication between their respective latent spaces. To achieve the alignment, we propose two paradigms to support both open-source trainable LLMs and commercial frozen LLMs. We leverage this aligned architecture to develop the first generative foundation model for netlists, unleashing LLMs' generative reasoning capability on the low-level and bit-blasted netlists. GenEDA enables three unprecedented generative netlist functional reasoning tasks, where it reversely generates high-level functionalities such as specifications and RTL code from low-level netlists. These tasks move beyond traditional gate function classification to direct generation of full-circuit functionality. Experiments demonstrate that GenEDA significantly boosts advanced LLMs' (e.g., GPT and DeepSeek series) performance in all tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09485v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wenji Fang, Jing Wang, Yao Lu, Shang Liu, Zhiyao Xie</dc:creator>
    </item>
  </channel>
</rss>
