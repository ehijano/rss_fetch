<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Jun 2024 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Second-Order Algorithms for Finding Local Nash Equilibria in Zero-Sum Games</title>
      <link>https://arxiv.org/abs/2406.03565</link>
      <description>arXiv:2406.03565v1 Announce Type: new 
Abstract: Zero-sum games arise in a wide variety of problems, including robust optimization and adversarial learning. However, algorithms deployed for finding a local Nash equilibrium in these games often converge to non-Nash stationary points. This highlights a key challenge: for any algorithm, the stability properties of its underlying dynamical system can cause non-Nash points to be potential attractors. To overcome this challenge, algorithms must account for subtleties involving the curvatures of players' costs. To this end, we leverage dynamical system theory and develop a second-order algorithm for finding a local Nash equilibrium in the smooth, possibly nonconvex-nonconcave, zero-sum game setting. First, we prove that this novel method guarantees convergence to only local Nash equilibria with a local linear convergence rate. We then interpret a version of this method as a modified Gauss-Newton algorithm with local superlinear convergence to the neighborhood of a point that satisfies first-order local Nash equilibrium conditions. In comparison, current related state-of-the-art methods do not offer convergence rate guarantees. Furthermore, we show that this approach naturally generalizes to settings with convex and potentially coupled constraints while retaining earlier guarantees of convergence to only local (generalized) Nash equilibria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03565v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kushagra Gupta, Xinjie Liu, Ufuk Topcu, David Fridovich-Keil</dc:creator>
    </item>
    <item>
      <title>Balancing rationality and social influence: Alpha-rational Nash equilibrium in games with herding</title>
      <link>https://arxiv.org/abs/2406.03928</link>
      <description>arXiv:2406.03928v1 Announce Type: new 
Abstract: The classical game theory models rational players and proposes Nash equilibrium (NE) as the solution. However, real-world scenarios rarely feature rational players; instead, players make inconsistent and irrational decisions. Often, irrational players exhibit herding behaviour by simply following the majority.
  In this paper, we consider the mean-field game with $\alpha$-fraction of rational players and the rest being herding-irrational players. For such a game, we introduce a novel concept of equilibrium named $\alpha$-Rational NE (in short, $\alpha$-RNE). The $\alpha$-RNEs and their implications are extensively analyzed in the game with two actions. Due to herding-irrational players, new equilibria may arise, and some classical NEs may be deleted.
  The rational players are not harmed but benefit from the presence of irrational players. Notably, we demonstrate through examples that rational players leverage upon the herding behaviour of irrational players and may attain higher utility (under $\alpha$-RNE) than social optimal utility (in the classical setting).
  Interestingly, the irrational players may also benefit by not being rational. We observe that irrational players do not lose compared to some classical NEs for participation and bandwidth sharing games. More importantly, in bandwidth sharing game, irrational players receive utility that approaches the social optimal utility. Such examples indicate that it may sometimes be `rational' to be irrational.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03928v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khushboo Agarwal, Konstantin Avrachenkov, Veeraruna Kavitha, Raghupati Vyas</dc:creator>
    </item>
    <item>
      <title>Online Learning in Betting Markets: Profit versus Prediction</title>
      <link>https://arxiv.org/abs/2406.04062</link>
      <description>arXiv:2406.04062v1 Announce Type: new 
Abstract: We examine two types of binary betting markets, whose primary goal is for profit (such as sports gambling) or to gain information (such as prediction markets). We articulate the interplay between belief and price-setting to analyse both types of markets, and show that the goals of maximising bookmaker profit and eliciting information are fundamentally incompatible. A key insight is that profit hinges on the deviation between (the distribution of) bettor and true beliefs, and that heavier tails in bettor belief distribution imply higher profit. Our algorithmic contribution is to introduce online learning methods for price-setting. Traditionally bookmakers update their prices rather infrequently, we present two algorithms that guide price updates upon seeing each bet, assuming very little of bettor belief distributions. The online pricing algorithm achieves stochastic regret of $\mathcal{O}(\sqrt{T})$ against the worst local maximum, or $ \mathcal{O}(\sqrt{T \log T}) $ with high probability against the global maximum under fair odds. More broadly, the inherent trade-off between profit and information-seeking in binary betting may inspire new understandings of large-scale multi-agent behaviour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04062v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haiqing Zhu, Alexander Soen, Yun Kuen Cheung, Lexing Xie</dc:creator>
    </item>
    <item>
      <title>Graphon Mean Field Games with a Representative Player: Analysis and Learning Algorithm</title>
      <link>https://arxiv.org/abs/2405.08005</link>
      <description>arXiv:2405.08005v2 Announce Type: cross 
Abstract: We propose a discrete time graphon game formulation on continuous state and action spaces using a representative player to study stochastic games with heterogeneous interaction among agents. This formulation admits both philosophical and mathematical advantages, compared to a widely adopted formulation using a continuum of players. We prove the existence and uniqueness of the graphon equilibrium with mild assumptions, and show that this equilibrium can be used to construct an approximate solution for finite player game on networks, which is challenging to analyze and solve due to curse of dimensionality. An online oracle-free learning algorithm is developed to solve the equilibrium numerically, and sample complexity analysis is provided for its convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08005v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fuzhong Zhou, Chenyu Zhang, Xu Chen, Xuan Di</dc:creator>
    </item>
    <item>
      <title>Bidding in Uniform Price Auctions for Value Maximizing Buyers</title>
      <link>https://arxiv.org/abs/2406.03674</link>
      <description>arXiv:2406.03674v1 Announce Type: cross 
Abstract: We study the problem of bidding in uniform price auctions widely used in practice. Although these auctions are non-truthful for bidders with quasilinear utility functions, several empirical findings suggest that the auction format induces truthful bidding from the bidders. We attribute this difference in theory and practice to the assumption of the behavioral model of the bidders. In this pursuit, we study uniform price auctions in a repeated setting from the perspective of a value-maximizing buyer who aims to maximize their acquired cumulative value across $T$ rounds, subject to per-round return-on-investment (RoI) constraints. For a RoI-constrained, value-maximizing buyer, we study a generalized version of the uniform bidding format, commonly used in practice, which we term as $m$-uniform bidding. To characterize the optimal $m$-uniform bid, we introduce and study the notion of universally feasible (UF) bidding policies, which are robust, meaning that RoI feasibility is obtained regardless of the competitors' bids. We show that the optimal class of UF bidding policies is essentially a generalization of truthful bidding policies, which depends only on the valuation curve of the bidder and target RoI. To measure the performance of UF bidding policies against the optimal bidding policy that is not necessarily UF, we introduce a metric called the Price of Universal Feasibility (PoUF) and establish that PoUF is at most 2, irrespective of $m$ and the upper bound is tight. We further compare the generalized $m$-uniform bidding interface against the classical uniform bidding format under which $m=1$, showing the total value under $m$-uniform bidding increases at most by a factor of $m$. Numerical simulations on semi-synthetic data demonstrate that UF bidding policies perform significantly better than the derived theoretical bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03674v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Negin Golrezaei, Sourav Sahoo</dc:creator>
    </item>
    <item>
      <title>Quantifying Misalignment Between Agents</title>
      <link>https://arxiv.org/abs/2406.04231</link>
      <description>arXiv:2406.04231v1 Announce Type: cross 
Abstract: Growing concerns about the AI alignment problem have emerged in recent years, with previous work focusing mainly on (1) qualitative descriptions of the alignment problem; (2) attempting to align AI actions with human interests by focusing on value specification and learning; and/or (3) focusing on a single agent or on humanity as a singular unit. Recent work in sociotechnical AI alignment has made some progress in defining alignment inclusively, but the field as a whole still lacks a systematic understanding of how to specify, describe, and analyze misalignment among entities, which may include individual humans, AI agents, and complex compositional entities such as corporations, nation-states, and so forth. Previous work on controversy in computational social science offers a mathematical model of contention among populations (of humans). In this paper, we adapt this contention model to the alignment problem, and show how misalignment can vary depending on the population of agents (human or otherwise) being observed, the domain in question, and the agents' probability-weighted preferences between possible outcomes. Our model departs from value specification approaches and focuses instead on the morass of complex, interlocking, sometimes contradictory goals that agents may have in practice. We apply our model by analyzing several case studies ranging from social media moderation to autonomous vehicle behavior. By applying our model with appropriately representative value data, AI engineers can ensure that their systems learn values maximally aligned with diverse human interests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04231v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aidan Kierans, Avijit Ghosh, Hananel Hazan, Shiri Dori-Hacohen</dc:creator>
    </item>
    <item>
      <title>Competitive Demand Learning: A Non-cooperative Pricing Algorithm with Coordinated Price Experimentation</title>
      <link>https://arxiv.org/abs/2008.05195</link>
      <description>arXiv:2008.05195v2 Announce Type: replace 
Abstract: We consider a periodical equilibrium pricing problem for multiple firms over a planning horizon of T periods. At each period, firms set their selling prices and receive stochastic demand from consumers. Firms do not know their underlying demand curve, but they wish to determine the selling prices to maximize total revenue under competition. Hence, they have to do some price experiments such that the observed demand data are informative to make price decisions. However, uncoordinated price updating can render the demand information gathered by price experimentation less informative or inaccurate. We design a nonparametric learning algorithm to facilitate coordinated dynamic pricing, in which competitive firms estimate their demand functions based on observations and adjust their pricing strategies in a prescribed manner. We show that the pricing decisions, determined by estimated demand functions, converge to underlying equilibrium as time progresses. We obtain a bound of the revenue difference that has an order of O(F^2 T^3/4) and a regret bound that has an order of O(F T^1/2) with respect to the number of the competitive firms F and T . We also develop a modified algorithm to handle the situation where some firms may have the knowledge of the demand curve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.05195v2</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Production and Operations Management 2024. Vol. 33(1)</arxiv:journal_reference>
      <dc:creator>Yongge Yang, Yu-Ching Lee, Po-An Chen</dc:creator>
    </item>
    <item>
      <title>Improving Sample Efficiency of Model-Free Algorithms for Zero-Sum Markov Games</title>
      <link>https://arxiv.org/abs/2308.08858</link>
      <description>arXiv:2308.08858v2 Announce Type: replace-cross 
Abstract: The problem of two-player zero-sum Markov games has recently attracted increasing interests in theoretical studies of multi-agent reinforcement learning (RL). In particular, for finite-horizon episodic Markov decision processes (MDPs), it has been shown that model-based algorithms can find an $\epsilon$-optimal Nash Equilibrium (NE) with the sample complexity of $O(H^3SAB/\epsilon^2)$, which is optimal in the dependence of the horizon $H$ and the number of states $S$ (where $A$ and $B$ denote the number of actions of the two players, respectively). However, none of the existing model-free algorithms can achieve such an optimality. In this work, we propose a model-free stage-based Q-learning algorithm and show that it achieves the same sample complexity as the best model-based algorithm, and hence for the first time demonstrate that model-free algorithms can enjoy the same optimality in the $H$ dependence as model-based algorithms. The main improvement of the dependency on $H$ arises by leveraging the popular variance reduction technique based on the reference-advantage decomposition previously used only for single-agent RL. However, such a technique relies on a critical monotonicity property of the value function, which does not hold in Markov games due to the update of the policy via the coarse correlated equilibrium (CCE) oracle. Thus, to extend such a technique to Markov games, our algorithm features a key novel design of updating the reference value functions as the pair of optimistic and pessimistic value functions whose value difference is the smallest in the history in order to achieve the desired improvement in the sample efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.08858v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songtao Feng, Ming Yin, Yu-Xiang Wang, Jing Yang, Yingbin Liang</dc:creator>
    </item>
    <item>
      <title>From Stream to Pool: Dynamic Pricing Beyond i.i.d. Arrivals</title>
      <link>https://arxiv.org/abs/2310.19220</link>
      <description>arXiv:2310.19220v2 Announce Type: replace-cross 
Abstract: Dynamic pricing models often posit that a $\textbf{stream}$ of customer interactions occur sequentially, where customers' valuations are drawn independently. However, this model is not entirely reflective of the real world, as it overlooks a critical aspect, the law of diminishing marginal utility, which states that a customer's marginal utility from each additional unit declines. This causes the valuation distribution to shift towards the lower end, which is not captured by the stream model. This motivates us to study a pool-based model, where a $\textbf{pool}$ of customers repeatedly interacts with a monopolist seller, each of whose valuation diminishes in the number of purchases made according to a discount function. In particular, when the discount function is constant, our pool model recovers the stream model. We focus on the most fundamental special case, where a customer's valuation becomes zero once a purchase is made. Given $k$ prices, we present a non-adaptive, detail-free (i.e., does not "know" the valuations) policy that achieves a $1/k$ competitive ratio, which is optimal among non-adaptive policies. Furthermore, based on a novel debiasing technique, we propose an adaptive learn-then-earn policy with a $\tilde O(k^{2/3} n^{2/3})$ regret.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19220v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Titing Cui, Su Jia, Thomas Lavastida</dc:creator>
    </item>
  </channel>
</rss>
