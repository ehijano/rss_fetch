<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Nov 2025 05:01:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>General Intelligence-based Fragmentation (GIF): A framework for peak-labeled spectra simulation</title>
      <link>https://arxiv.org/abs/2511.09571</link>
      <description>arXiv:2511.09571v1 Announce Type: new 
Abstract: Despite growing reference libraries and advanced computational tools, progress in the field of metabolomics remains constrained by low rates of annotating measured spectra. The recent developments of large language models (LLMs) have led to strong performance across a wide range of generation and reasoning tasks, spurring increased interest in LLMs' application to domain-specific scientific challenges, such as mass spectra annotation. Here, we present a novel framework, General Intelligence-based Fragmentation (GIF), that guides pretrained LLMs through spectra simulation using structured prompting and reasoning. GIF utilizes tagging, structured inputs/outputs, system prompts, instruction-based prompts, and iterative refinement. Indeed, GIF offers a structured alternative to ad hoc prompting, underscoring the need for systematic guidance of LLMs on complex scientific tasks. Using GIF, we evaluate current generalist LLMs' ability to use reasoning towards fragmentation and to perform intensity prediction after fine-tuning. We benchmark performance on a novel QA dataset, the MassSpecGym QA-sim dataset, that we derive from the MassSpecGym dataset. Through these implementations of GIF, we find that GPT-4o and GPT-4o-mini achieve a cosine similarity of 0.36 and 0.35 between the simulated and true spectra, respectively, outperforming other pretrained models including GPT-5, Llama-3.1, and ChemDFM, despite GPT-5's recency and ChemDFM's domain specialization. GIF outperforms several deep learning baselines. Our evaluation of GIF highlights the value of using LLMs not only for spectra simulation but for enabling human-in-the-loop workflows and structured, explainable reasoning in molecular fragmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09571v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Margaret R. Martin, Soha Hassoun</dc:creator>
    </item>
    <item>
      <title>Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification</title>
      <link>https://arxiv.org/abs/2511.09576</link>
      <description>arXiv:2511.09576v1 Announce Type: new 
Abstract: Variants of Uncertain Significance (VUS) limit the clinical utility of prostate cancer genomics by delaying diagnosis and therapy when evidence for pathogenicity or benignity is incomplete. Progress is further limited by inconsistent annotations across sources and the absence of a prostate-specific benchmark for fair comparison. We introduce Prostate-VarBench, a curated pipeline for creating prostate-specific benchmarks that integrates COSMIC (somatic cancer mutations), ClinVar (expert-curated clinical variants), and TCGA-PRAD (prostate tumor genomics from The Cancer Genome Atlas) into a harmonized dataset of 193,278 variants supporting patient- or gene-aware splits to prevent data leakage. To ensure data integrity, we corrected a Variant Effect Predictor (VEP) issue that merged multiple transcript records, introducing ambiguity in clinical significance fields. We then standardized 56 interpretable features across eight clinically relevant tiers, including population frequency, variant type, and clinical context. AlphaMissense pathogenicity scores were incorporated to enhance missense variant classification and reduce VUS uncertainty. Building on this resource, we trained an interpretable TabNet model to classify variant pathogenicity, whose step-wise sparse masks provide per-case rationales consistent with molecular tumor board review practices. On the held-out test set, the model achieved 89.9% accuracy with balanced class metrics, and the VEP correction yields an 6.5% absolute reduction in VUS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09576v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abraham Francisco Arellano Tavara, Umesh Kumar, Jathurshan Pradeepkumar, Jimeng Sun</dc:creator>
    </item>
    <item>
      <title>Exhaustive Investigation of CBC-Derived Biomarker Ratios for Clinical Outcome Prediction: The RDW-to-MCHC Ratio as a Novel Mortality Predictor in Critical Care</title>
      <link>https://arxiv.org/abs/2511.09583</link>
      <description>arXiv:2511.09583v1 Announce Type: new 
Abstract: Ratios of common biomarkers and blood analytes are well established for early detection and predictive purposes. Early risk stratification in critical care is often limited by the delayed availability of complex severity scores. Complete blood count (CBC) parameters, available within hours of admission, may enable rapid prognostication. We conducted an exhaustive and systematic evaluation of CBC-derived ratios for mortality prediction to identify robust, accessible, and generalizable biomarkers. We generated all feasible two-parameter CBC ratios with unit checks and plausibility filters on more than 90,000 ICU admissions (MIMIC-IV). Discrimination was assessed via cross-validated and external AUC, calibration via isotonic regression, and clinical utility with decision-curve analysis. Retrospective validation was performed on eICU-CRD (n = 156530) participants. The ratio of Red Cell Distribution Width (RDW) to Mean Corpuscular Hemoglobin Concentration (MCHC), denoted RDW:MCHC, emerged as the top biomarker (AUC = 0.699 discovery; 0.662 validation), outperforming RDW and NLR. It achieved near-universal availability (99.9\% vs.\ 35.0\% for NLR), excellent calibration (Hosmer--Lemeshow $p = 1.0$; $\mathrm{ECE} &lt; 0.001$), and preserved performance across diagnostic groups, with only modest attenuation in respiratory cases. Expressed as a logistic odds ratio, each one standard deviation increase in RDW:MCHC nearly quadrupled 30-day mortality odds (OR = 3.81, 95\% CI [3.70, 3.95]). Decision-curve analysis showed positive net benefit at high-risk triage thresholds. A simple, widely available CBC-derived feature (RDW:MCHC) provides consistent, externally validated signal for early mortality risk. While not a substitute for multivariable scores, it offers a pragmatic adjunct for rapid triage when full scoring is impractical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09583v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dmytro Leontiev, Abicumaran Uthamacumaran, Riya Nagar, Hector Zenil</dc:creator>
    </item>
    <item>
      <title>fastbmRAG: A Fast Graph-Based RAG Framework for Efficient Processing of Large-Scale Biomedical Literature</title>
      <link>https://arxiv.org/abs/2511.10014</link>
      <description>arXiv:2511.10014v1 Announce Type: new 
Abstract: Large language models (LLMs) are rapidly transforming various domains, including biomedicine and healthcare, and demonstrate remarkable potential from scientific research to new drug discovery. Graph-based retrieval-augmented generation (RAG) systems, as a useful application of LLMs, can improve contextual reasoning through structured entity and relationship identification from long-context knowledge, e.g. biomedical literature. Even though many advantages over naive RAGs, most of graph-based RAGs are computationally intensive, which limits their application to large-scale dataset. To address this issue, we introduce fastbmRAG, an fast graph-based RAG optimized for biomedical literature. Utilizing well organized structure of biomedical papers, fastbmRAG divides the construction of knowledge graph into two stages, first drafting graphs using abstracts; and second, refining them using main texts guided by vector-based entity linking, which minimizes redundancy and computational load. Our evaluations demonstrate that fastbmRAG is over 10x faster than existing graph-RAG tools and achieve superior coverage and accuracy to input knowledge. FastbmRAG provides a fast solution for quickly understanding, summarizing, and answering questions about biomedical literature on a large scale. FastbmRAG is public available in https://github.com/menggf/fastbmRAG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10014v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Guofeng Meng, Li Shen, Qiuyan Zhong, Wei Wang, Haizhou Zhang, Xiaozhen Wang</dc:creator>
    </item>
    <item>
      <title>HAMscope: a snapshot Hyperspectral Autofluorescence Miniscope for real-time molecular imaging</title>
      <link>https://arxiv.org/abs/2511.09574</link>
      <description>arXiv:2511.09574v1 Announce Type: cross 
Abstract: We introduce HAMscope, a compact, snapshot hyperspectral autofluorescence miniscope that enables real-time, label-free molecular imaging in a wide range of biological systems. By integrating a thin polymer diffuser into a widefield miniscope, HAMscope spectrally encodes each frame and employs a probabilistic deep learning framework to reconstruct 30-channel hyperspectral stacks (452 to 703 nm) or directly infer molecular composition maps from single images. A scalable multi-pass U-Net architecture with transformer-based attention and per-pixel uncertainty estimation enables high spatio-spectral fidelity (mean absolute error ~ 0.0048) at video rates. While initially demonstrated in plant systems, including lignin, chlorophyll, and suberin imaging in intact poplar and cork tissues, the platform is readily adaptable to other applications such as neural activity mapping, metabolic profiling, and histopathology. We show that the system generalizes to out-of-distribution tissue types and supports direct molecular mapping without the need for spectral unmixing. HAMscope establishes a general framework for compact, uncertainty-aware spectral imaging that combines minimal optics with advanced deep learning, offering broad utility for real-time biochemical imaging across neuroscience, environmental monitoring, and biomedicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09574v1</guid>
      <category>physics.optics</category>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Ingold, Richard G. Baird, Dasmeet Kaur, Nidhi Dwivedi, Reed Sorenson, Leslie Sieburth, Chang-Jun Liu, Rajesh Menon</dc:creator>
    </item>
    <item>
      <title>Clinically-aligned Multi-modal Chest X-ray Classification</title>
      <link>https://arxiv.org/abs/2511.09581</link>
      <description>arXiv:2511.09581v1 Announce Type: cross 
Abstract: Radiology is essential to modern healthcare, yet rising demand and staffing shortages continue to pose major challenges. Recent advances in artificial intelligence have the potential to support radiologists and help address these challenges. Given its widespread use and clinical importance, chest X-ray classification is well suited to augment radiologists' workflows. However, most existing approaches rely solely on single-view, image-level inputs, ignoring the structured clinical information and multi-image studies available at the time of reporting. In this work, we introduce CaMCheX, a multimodal transformer-based framework that aligns multi-view chest X-ray studies with structured clinical data to better reflect how clinicians make diagnostic decisions. Our architecture employs view-specific ConvNeXt encoders for frontal and lateral chest radiographs, whose features are fused with clinical indications, history, and vital signs using a transformer fusion module. This design enables the model to generate context-aware representations that mirror reasoning in clinical practice. Our results exceed the state of the art for both the original MIMIC-CXR dataset and the more recent CXR-LT benchmarks, highlighting the value of clinically grounded multimodal alignment for advancing chest X-ray classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09581v1</guid>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phillip Sloan, Edwin Simpson, Majid Mirmehdi</dc:creator>
    </item>
    <item>
      <title>Diffusion-Based Quality Control of Medical Image Segmentations across Organs</title>
      <link>https://arxiv.org/abs/2511.09588</link>
      <description>arXiv:2511.09588v1 Announce Type: cross 
Abstract: Medical image segmentation using deep learning (DL) has enabled the development of automated analysis pipelines for large-scale population studies. However, state-of-the-art DL methods are prone to hallucinations, which can result in anatomically implausible segmentations. With manual correction impractical at scale, automated quality control (QC) techniques have to address the challenge. While promising, existing QC methods are organ-specific, limiting their generalizability and usability beyond their original intended task. To overcome this limitation, we propose no-new Quality Control (nnQC), a robust QC framework based on a diffusion-generative paradigm that self-adapts to any input organ dataset. Central to nnQC is a novel Team of Experts (ToE) architecture, where two specialized experts independently encode 3D spatial awareness, represented by the relative spatial position of an axial slice, and anatomical information derived from visual features from the original image. A weighted conditional module dynamically combines the pair of independent embeddings, or opinions to condition the sampling mechanism within a diffusion process, enabling the generation of a spatially aware pseudo-ground truth for predicting QC scores. Within its framework, nnQC integrates fingerprint adaptation to ensure adaptability across organs, datasets, and imaging modalities. We evaluated nnQC on seven organs using twelve publicly available datasets. Our results demonstrate that nnQC consistently outperforms state-of-the-art methods across all experiments, including cases where segmentation masks are highly degraded or completely missing, confirming its versatility and effectiveness across different organs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09588v1</guid>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincenzo Marcian\`o, Hava Chaptoukaev, Virginia Fernandez, M. Jorge Cardoso, S\'ebastien Ourselin, Michela Antonelli, Maria A. Zuluaga</dc:creator>
    </item>
    <item>
      <title>Segment Any Tumour: An Uncertainty-Aware Vision Foundation Model for Whole-Body Analysis</title>
      <link>https://arxiv.org/abs/2511.09592</link>
      <description>arXiv:2511.09592v1 Announce Type: cross 
Abstract: Prompt-driven vision foundation models, such as the Segment Anything Model, have recently demonstrated remarkable adaptability in computer vision. However, their direct application to medical imaging remains challenging due to heterogeneous tissue structures, imaging artefacts, and low-contrast boundaries, particularly in tumours and cancer primaries leading to suboptimal segmentation in ambiguous or overlapping lesion regions. Here, we present Segment Any Tumour 3D (SAT3D), a lightweight volumetric foundation model designed to enable robust and generalisable tumour segmentation across diverse medical imaging modalities. SAT3D integrates a shifted-window vision transformer for hierarchical volumetric representation with an uncertainty-aware training pipeline that explicitly incorporates uncertainty estimates as prompts to guide reliable boundary prediction in low-contrast regions. Adversarial learning further enhances model performance for the ambiguous pathological regions. We benchmark SAT3D against three recent vision foundation models and nnUNet across 11 publicly available datasets, encompassing 3,884 tumour and cancer cases for training and 694 cases for in-distribution evaluation. Trained on 17,075 3D volume-mask pairs across multiple modalities and cancer primaries, SAT3D demonstrates strong generalisation and robustness. To facilitate practical use and clinical translation, we developed a 3D Slicer plugin that enables interactive, prompt-driven segmentation and visualisation using the trained SAT3D model. Extensive experiments highlight its effectiveness in improving segmentation accuracy under challenging and out-of-distribution scenarios, underscoring its potential as a scalable foundation model for medical image analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09592v1</guid>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Himashi Peiris, Sizhe Wang, Gary Egan, Mehrtash Harandi, Meng Law, Zhaolin Chen</dc:creator>
    </item>
    <item>
      <title>TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice Representations and Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2511.09605</link>
      <description>arXiv:2511.09605v1 Announce Type: cross 
Abstract: The growing number of medical tomography examinations has necessitated the development of automated methods capable of extracting comprehensive imaging features to facilitate downstream tasks such as tumor characterization, while assisting physicians in managing their growing workload. However, 3D medical image classification remains a challenging task due to the complex spatial relationships and long-range dependencies inherent in volumetric data. Training models from scratch suffers from low data regimes, and the absence of 3D large-scale multimodal datasets has limited the development of 3D medical imaging foundation models. Recent studies, however, have highlighted the potential of 2D vision foundation models, originally trained on natural images, as powerful feature extractors for medical image analysis. Despite these advances, existing approaches that apply 2D models to 3D volumes via slice-based decomposition remain suboptimal. Conventional volume slicing strategies, which rely on canonical planes such as axial, sagittal, or coronal, may inadequately capture the spatial extent of target structures when these are misaligned with standardized viewing planes. Furthermore, existing slice-wise aggregation strategies rarely account for preserving the volumetric structure, resulting in a loss of spatial coherence across slices. To overcome these limitations, we propose TomoGraphView, a novel framework that integrates omnidirectional volume slicing with spherical graph-based feature aggregation. We publicly share our accessible code base at http://github.com/compai-lab/2025-MedIA-kiechle and provide a user-friendly library for omnidirectional volume slicing at https://pypi.org/project/OmniSlicer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09605v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Kiechle, Stefan M. Fischer, Daniel M. Lang, Cosmin I. Bercea, Matthew J. Nyflot, Lina Felsner, Julia A. Schnabel, Jan C. Peeken</dc:creator>
    </item>
    <item>
      <title>EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization</title>
      <link>https://arxiv.org/abs/2511.10165</link>
      <description>arXiv:2511.10165v1 Announce Type: cross 
Abstract: Accurate exploration of protein conformational ensembles is essential for uncovering function but remains hard because molecular-dynamics (MD) simulations suffer from high computational costs and energy-barrier trapping. This paper presents Energy Preference Optimization (EPO), an online refinement algorithm that turns a pretrained protein ensemble generator into an energy-aware sampler without extra MD trajectories. Specifically, EPO leverages stochastic differential equation sampling to explore the conformational landscape and incorporates a novel energy-ranking mechanism based on list-wise preference optimization. Crucially, EPO introduces a practical upper bound to efficiently approximate the intractable probability of long sampling trajectories in continuous-time generative models, making it easily adaptable to existing pretrained generators. On Tetrapeptides, ATLAS, and Fast-Folding benchmarks, EPO successfully generates diverse and physically realistic ensembles, establishing a new state-of-the-art in nine evaluation metrics. These results demonstrate that energy-only preference signals can efficiently steer generative models toward thermodynamically consistent conformational ensembles, providing an alternative to long MD simulations and widening the applicability of learned potentials in structural biology and drug discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10165v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuancheng Sun, Yuxuan Ren, Zhaoming Chen, Xu Han, Kang Liu, Qiwei Ye</dc:creator>
    </item>
    <item>
      <title>Histology-informed tiling of whole tissue sections improves the interpretability and predictability of cancer relapse and genetic alterations</title>
      <link>https://arxiv.org/abs/2511.10432</link>
      <description>arXiv:2511.10432v1 Announce Type: cross 
Abstract: Histopathologists establish cancer grade by assessing histological structures, such as glands in prostate cancer. Yet, digital pathology pipelines often rely on grid-based tiling that ignores tissue architecture. This introduces irrelevant information and limits interpretability. We introduce histology-informed tiling (HIT), which uses semantic segmentation to extract glands from whole slide images (WSIs) as biologically meaningful input patches for multiple-instance learning (MIL) and phenotyping. Trained on 137 samples from the ProMPT cohort, HIT achieved a gland-level Dice score of 0.83 +/- 0.17. By extracting 380,000 glands from 760 WSIs across ICGC-C and TCGA-PRAD cohorts, HIT improved MIL models AUCs by 10% for detecting copy number variation (CNVs) in genes related to epithelial-mesenchymal transitions (EMT) and MYC, and revealed 15 gland clusters, several of which were associated with cancer relapse, oncogenic mutations, and high Gleason. Therefore, HIT improved the accuracy and interpretability of MIL predictions, while streamlining computations by focussing on biologically meaningful structures during feature extraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10432v1</guid>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <category>q-bio.TO</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Willem Bonnaff\'e, Yang Hu, Andrea Chatrian, Mengran Fan, Stefano Malacrino, Sandy Figiel, CRUK ICGC Prostate Group, Srinivasa R. Rao, Richard Colling, Richard J. Bryant, Freddie C. Hamdy, Dan J. Woodcock, Ian G. Mills, Clare Verrill, Jens Rittscher</dc:creator>
    </item>
    <item>
      <title>A reanalysis of the FDA's benefit-risk assessment of Moderna's mRNA-1273 COVID vaccine stratified not only based on age and sex but also on prior-infection and comorbidity status</title>
      <link>https://arxiv.org/abs/2410.11811</link>
      <description>arXiv:2410.11811v3 Announce Type: replace 
Abstract: The United States Food and Drug Administration (FDA) conducted a benefit-risk assessment for Moderna's COVID vaccine mRNA-1273 prior to its full approval, announced 1/31/2022. The FDA's assessment focused on males of ages 18-64 years because the agency's risk analysis was limited to vaccine-attributable myocarditis/pericarditis (VAM/P) given the excess risk among males. The FDA's analysis concluded that vaccine benefits clearly outweighed risks, even for 18-25-year-old males (those at highest VAM/P risk). We reanalyze the FDA's benefit-risk assessment using information available through the third week of January 2022 and focusing on 18-25-year-old males. We use the FDA's framework but extend its model by accounting for protection derived from prior COVID infection, finer age-stratification in COVID-hospitalization rates, and incidental hospitalizations (those of patients who test positive for COVID but are being treated for something else). We also use more realistic projections of Omicron-infection rates and more accurate rates of VAM/P. With hospitalizations as the principal endpoint of the analysis (those prevented by vaccination vs. those caused by VAM/P), our model finds vaccine risks outweighed benefits for 18-25-year-old males, except in scenarios projecting implausibly high Omicron-infection prevalence. Our assessment suggests that mRNA-1273 vaccination of 18-25-year-old males generated between 16% and 63% more hospitalizations from vaccine-attributable myocarditis/pericarditis alone compared to COVID hospitalizations prevented (over a five-month period of vaccine protection assumed by the FDA). The preceding assessment derives from model inputs based on data available at the time of the FDA's mRNA-1273 assessment. Moreover, these inputs as well as model outputs are validated by subsequently available data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11811v3</guid>
      <category>q-bio.QM</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul S. Bourdon, Ram Duriseti, H. Christian Gromoll, Dyana K. Dalton, Kevin Bardosh, Allison E. Krug</dc:creator>
    </item>
    <item>
      <title>CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment</title>
      <link>https://arxiv.org/abs/2511.03826</link>
      <description>arXiv:2511.03826v2 Announce Type: replace 
Abstract: Accurate and efficient registration of whole slide images (WSIs) is essential for high-resolution, nuclei-level analysis in multi-stained tissue slides. We propose a novel coarse-to-fine framework CORE for accurate nuclei-level registration across diverse multimodal whole-slide image (WSI) datasets. The coarse registration stage leverages prompt-based tissue mask extraction to effectively filter out artefacts and non-tissue regions, followed by global alignment using tissue morphology and ac- celerated dense feature matching with a pre-trained feature extractor. From the coarsely aligned slides, nuclei centroids are detected and subjected to fine-grained rigid registration using a custom, shape-aware point-set registration model. Finally, non-rigid alignment at the cellular level is achieved by estimating a non-linear dis- placement field using Coherent Point Drift (CPD). Our approach benefits from automatically generated nuclei that enhance the accuracy of deformable registra- tion and ensure precise nuclei-level correspondence across modalities. The pro- posed model is evaluated on three publicly available WSI registration datasets, and two private datasets. We show that CORE outperforms current state-of-the-art methods in terms of generalisability, precision, and robustness in bright-field and immunofluorescence microscopy WSIs</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03826v2</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Esha Sadia Nasir, Behnaz Elhaminia, Mark Eastwood, Catherine King, Owen Cain, Lorraine Harper, Paul Moss, Dimitrios Chanouzas, David Snead, Nasir Rajpoot, Adam Shephard, Shan E Ahmed Raza</dc:creator>
    </item>
    <item>
      <title>BATIS: Bayesian Approaches for Targeted Improvement of Species Distribution Models</title>
      <link>https://arxiv.org/abs/2510.19749</link>
      <description>arXiv:2510.19749v2 Announce Type: replace-cross 
Abstract: Species distribution models (SDMs), which aim to predict species occurrence based on environmental variables, are widely used to monitor and respond to biodiversity change. Recent deep learning advances for SDMs have been shown to perform well on complex and heterogeneous datasets, but their effectiveness remains limited by spatial biases in the data. In this paper, we revisit deep SDMs from a Bayesian perspective and introduce BATIS, a novel and practical framework wherein prior predictions are updated iteratively using limited observational data. Models must appropriately capture both aleatoric and epistemic uncertainty to effectively combine fine-grained local insights with broader ecological patterns. We benchmark an extensive set of uncertainty quantification approaches on a novel dataset including citizen science observations from the eBird platform. Our empirical study shows how Bayesian deep learning approaches can greatly improve the reliability of SDMs in data-scarce locations, which can contribute to ecological understanding and conservation efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19749v2</guid>
      <category>cs.LG</category>
      <category>q-bio.PE</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Catherine Villeneuve, Benjamin Akera, M\'elisande Teng, David Rolnick</dc:creator>
    </item>
    <item>
      <title>Dynamics of menopause from deconvolution of millions of lab tests</title>
      <link>https://arxiv.org/abs/2511.05906</link>
      <description>arXiv:2511.05906v2 Announce Type: replace-cross 
Abstract: Menopause reshapes female physiology, yet its full temporal footprint is obscured by uncertainty in the age of the final menstrual period (FMP). Here we analyse cross-sectional data on 300 million laboratory tests from more than a million women in two population-scale cohorts (Israel-Clalit and US-NHANES). We apply a deconvolution algorithm inspired by astronomical image "de-blurring" to align each test to time-from-FMP rather than chronological age. Nearly every assay - spanning endocrine, bone, hepatic, lipid, osmolality, inflammatory and muscular systems - exhibits a jump at FMP that is absent in males and highly concordant between cohorts. Jumps were largest in the sex hormones, followed by bone, toxins, red blood cells, liver, iron, lipids, kidney, and muscle. Changes are mostly detrimental except iron indices and anemia that improve post-menopause, and depression scores that spike only transiently. Hormone-replacement therapy attenuates many of the step-like changes. Sex hormone dysregulation occurs more than 10 years prior to FMP. These findings reveal the step-like dysregulation across physiology caused by loss of sex hormones and establish deconvolution as a general strategy for disentangling age-related transitions in large, noisy datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05906v2</guid>
      <category>q-bio.TO</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Glen Pridham, Yoav Hayut, Noa Lavi-Shoseyov, Michal Neeman, Noa Hovav, Yoel Toledano, Uri Alon</dc:creator>
    </item>
  </channel>
</rss>
