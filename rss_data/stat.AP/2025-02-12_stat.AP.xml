<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Feb 2025 05:01:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Enhanced Renewable Energy Forecasting and Operations through Probabilistic Forecast Aggregation</title>
      <link>https://arxiv.org/abs/2502.07010</link>
      <description>arXiv:2502.07010v1 Announce Type: new 
Abstract: Accurate and reliable forecasting of renewable energy generation is crucial for the efficient integration of renewable sources into the power grid. In particular, probabilistic forecasts are becoming essential for managing the intrinsic variability and uncertainty of renewable energy production, especially wind and solar generation. This paper considers the setting where probabilistic forecasts are provided for individual renewable energy sites using, e.g., quantile regression models, but without any correlation information between sites. This setting is common if, e.g., such forecasts are provided by each individual site, or by multiple vendors. However, to effectively manage a fleet of renewable generators, it is necessary to aggregate these individual forecasts to the fleet level, while ensuring that the aggregated probabilistic forecast is statistically consistent and reliable. To address this challenge, this paper presents the integrated use of Copula and Monte-Carlo methods to aggregate individual probabilistic forecasts into a statistically calibrated, probabilistic forecast at the fleet level. The proposed framework is validated using synthetic data from several large-scale systems in the United States. This work has important implications for grid operators and energy planners, providing them with better tools to manage the variability and uncertainty inherent in renewable energy production.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07010v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alireza Moradi, Mathieu Tanneau, Reza Zandehshahvar, Pascal Van Hentenryck</dc:creator>
    </item>
    <item>
      <title>Towards a Principled Framework for Disclosure Avoidance</title>
      <link>https://arxiv.org/abs/2502.07105</link>
      <description>arXiv:2502.07105v1 Announce Type: new 
Abstract: Responsible disclosure limitation is an iterative exercise in risk assessment and mitigation. From time to time, as disclosure risks grow and evolve and as data users' needs change, agencies must consider redesigning the disclosure avoidance system(s) they use. Discussions about candidate systems often conflate inherent features of those systems with implementation decisions independent of those systems. For example, a system's ability to calibrate the strength of protection to suit the underlying disclosure risk of the data (e.g., by varying suppression thresholds), is a worthwhile feature regardless of the independent decision about how much protection is actually necessary. Having a principled discussion of candidate disclosure avoidance systems requires a framework for distinguishing these inherent features of the systems from the implementation decisions that need to be made independent of the system selected. For statistical agencies, this framework must also reflect the applied nature of these systems, acknowledging that candidate systems need to be adaptable to requirements stemming from the legal, scientific, resource, and stakeholder environments within which they would be operating. This paper proposes such a framework. No approach will be perfectly adaptable to every potential system requirement. Because the selection of some methodologies over others may constrain the resulting systems' efficiency and flexibility to adapt to particular statistical product specifications, data user needs, or disclosure risks, agencies may approach these choices in an iterative fashion, adapting system requirements, product specifications, and implementation parameters as necessary to ensure the resulting quality of the statistical product.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07105v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael B Hawes, Evan M Brassell, Anthony Caruso, Ryan Cumings-Menon, Jason Devine, Cassandra Dorius, David Evans, Kenneth Haase, Michele C Hedrick, Alexandra Krause, Philip Leclerc, James Livsey, Rolando A Rodriguez, Luke T Rogers, Matthew Spence, Victoria Velkoff, Michael Walsh, James Whitehorne, Sallie Ann Keller</dc:creator>
    </item>
    <item>
      <title>A Framework for Supervised and Unsupervised Segmentation and Classification of Materials Microstructure Images</title>
      <link>https://arxiv.org/abs/2502.07107</link>
      <description>arXiv:2502.07107v1 Announce Type: new 
Abstract: Microstructure of materials is often characterized through image analysis to understand processing-structure-properties linkages. We propose a largely automated framework that integrates unsupervised and supervised learning methods to classify micrographs according to microstructure phase/class and, for multiphase microstructures, segments them into different homogeneous regions. With the advance of manufacturing and imaging techniques, the ultra-high resolution of imaging that reveals the complexity of microstructures and the rapidly increasing quantity of images (i.e., micrographs) enables and necessitates a more powerful and automated framework to extract materials characteristics and knowledge. The framework we propose can be used to gradually build a database of microstructure classes relevant to a particular process or group of materials, which can help in analyzing and discovering/identifying new materials. The framework has three steps: (1) segmentation of multiphase micrographs through a recently developed score-based method so that different microstructure homogeneous regions can be identified in an unsupervised manner; (2) {identification and classification of} homogeneous regions of micrographs through an uncertainty-aware supervised classification network trained using the segmented micrographs from Step $1$ with their identified labels verified via the built-in uncertainty quantification and minimal human inspection; (3) supervised segmentation (more powerful than the segmentation in Step $1$) of multiphase microstructures through a segmentation network trained with micrographs and the results from Steps $1$-$2$ using a form of data augmentation. This framework can iteratively characterize/segment new homogeneous or multiphase materials while expanding the database to enhance performance. The framework is demonstrated on various sets of materials and texture images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07107v1</guid>
      <category>stat.AP</category>
      <category>cs.CV</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kungang Zhang, Daniel W. Apley, Wei Chen, Wing K. Liu, L. Catherine Brinson</dc:creator>
    </item>
    <item>
      <title>Valeriepieris Circles Reveal City and Regional Boundaries in England and Wales</title>
      <link>https://arxiv.org/abs/2502.07451</link>
      <description>arXiv:2502.07451v1 Announce Type: new 
Abstract: We propose a new method of determining regional and city boundaries based on the Valeriepieris circle, the smallest circle containing a given fraction of the data. By varying the fraction in the circle we can map complex spatial data to a simple model of concentric rings which we then fit to determine natural density cutoffs. We apply this method to population, occupation, economic and transport data from England and Wales, finding that the regions determined by this method affirm well known social facts such as the disproportionate wealth of London or the relative isolation of the North East and South West of England. We then show how different data sets give us different views of the same cities, providing insight into their development and dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07451v1</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rudy Arthur, Federico Botta</dc:creator>
    </item>
    <item>
      <title>Applications of robust statistics for cyclostationarity detection in non-Gaussian signals for local damage detection in bearings</title>
      <link>https://arxiv.org/abs/2502.07478</link>
      <description>arXiv:2502.07478v1 Announce Type: new 
Abstract: Signals with periodic characteristics are ubiquitous in real-world applications. One of these areas is condition monitoring, where the vibration signals from rotating machines naturally display periodic behavior. Thus, the cyclostationary analysis has evolved into the investigation of such signals. For the traditional cyclostationary approaches, the autocovariance function (ACVF) and its bi-frequency representation, spectral coherence (SC), are regarded as the base. However, recent research has revealed that real vibration signals increasingly exhibit impulsive behavior in addition to periodicity. As a result, there was a need for new methods to identify periodic behavior that take into account the impulsiveness of the data. In this article, we provide a way to improve the SC method by using its robust variants in place of the classical ACVF estimator (sample ACVF). The suggested concept is intuitive and relatively simple. We create robust versions of the SC algorithm that more accurately detect periodic behavior in signals with significant disruptions in contrast to the classical techniques. The efficiency of the proposed approach is demonstrated for simulated signals with three different types of non-Gaussian noise distribution and different levels of periodic impulses imitating a local damage. The introduced approach is also validated on real vibration signal from the rolling element bearings operating in a crushing machine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07478v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ymssp.2024.111367</arxiv:DOI>
      <arxiv:journal_reference>Mechanical Systems and Signal Processing 214, 111367, 2024</arxiv:journal_reference>
      <dc:creator>Wojciech \.Zu{\l}awi\'nski, J\'er\^ome Antoni, Rados{\l}aw Zimroz, Agnieszka Wy{\l}oma\'nska</dc:creator>
    </item>
    <item>
      <title>Quantitative evaluation of unsupervised clustering algorithms for dynamic total-body PET image analysis</title>
      <link>https://arxiv.org/abs/2502.07511</link>
      <description>arXiv:2502.07511v1 Announce Type: new 
Abstract: Background. Recently, dynamic total-body positron emission tomography (PET) imaging has become possible due to new scanner devices. While clustering algorithms have been proposed for PET analysis already earlier, there is still little research systematically evaluating these algorithms for processing of dynamic total-body PET images. Materials and methods. Here, we compare the performance of 15 unsupervised clustering methods, including K-means either by itself or after principal component analysis (PCA) or independent component analysis (ICA), Gaussian mixture model (GMM), fuzzy c-means (FCM), agglomerative clustering, spectral clustering, and several newer clustering algorithms, for classifying time activity curves (TACs) in dynamic PET images. We use dynamic total-body $^{15}$O-water PET images collected from 30 patients with suspected or confirmed coronary artery disease. To evaluate the clustering algorithms in a quantitative way, we use them to classify 5000 TACs from each image based on whether the curve is taken from brain, right heart ventricle, right kidney, lower right lung lobe, or urinary bladder. Results. According to our results, the best methods are GMM, FCM, and ICA combined with mini batch K-means, which classified the TACs with a median accuracies of 89\%, 83\%, and 81\%, respectively, in a processing time of half a second or less on average for each image. Conclusion. GMM, FCM, and ICA with mini batch K-means show promise for dynamic total-body PET analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07511v1</guid>
      <category>stat.AP</category>
      <category>cs.CV</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/03091902.2025.2466834</arxiv:DOI>
      <dc:creator>Oona Rainio, Maria K. Jaakkola, Riku Kl\'en</dc:creator>
    </item>
    <item>
      <title>Forecasting the future development in quality and value of professional football players for applications in team management</title>
      <link>https://arxiv.org/abs/2502.07528</link>
      <description>arXiv:2502.07528v1 Announce Type: new 
Abstract: Transfers in professional football (soccer) are risky investments because of the large transfer fees and high risks involved. Although data-driven models can be used to improve transfer decisions, existing models focus on describing players' historical progress, leaving their future performance unknown. Moreover, recent developments have called for the use of explainable models combined with uncertainty quantification of predictions. This paper assesses explainable machine learning models based on predictive accuracy and uncertainty quantification methods for the prediction of the future development in quality and transfer value of professional football players. Using a historical data set of data-driven indicators describing player quality and the transfer value of a football player, the models are trained to forecast player quality and player value one year ahead. These two prediction problems demonstrate the efficacy of tree-based models, particularly random forest and XGBoost, in making accurate predictions. In general, the random forest model is found to be the most suitable model because it provides accurate predictions as well as an uncertainty quantification method that naturally arises from the bagging procedure of the random forest model. Additionally, our research shows that the development of player performance contains nonlinear patterns and interactions between variables, and that time series information can provide useful information for the modeling of player performance metrics. Our research provides models to help football clubs make more informed, data-driven transfer decisions by forecasting player quality and transfer value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07528v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Koen W. van Arem, Floris Goes-Smit, Jakob S\"ohl</dc:creator>
    </item>
    <item>
      <title>Response rate estimation in single-stage basket trials: A comparison of estimators that allow for borrowing across cohorts</title>
      <link>https://arxiv.org/abs/2502.07639</link>
      <description>arXiv:2502.07639v1 Announce Type: new 
Abstract: Therapeutic advancements in oncology have shifted towards targeted therapy based on genomic aberrations. This necessitates innovative statistical approaches in clinical trials, particularly in master protocol studies. Basket trials, a type of master protocol, evaluate a single treatment across cohorts sharing a genomic aberration but differing in tumor histology. While offering operational advantages, basket trial analysis presents statistical inference challenges. These trials help determine for which tumor histology the treatment is promising enough to advance to confirmatory evaluation and often use Bayesian designs to support decisions. Beyond decision-making, estimating cohort-specific response rates is crucial for designing subsequent trials. This study compares seven Bayesian estimation methods for basket trials with binary outcomes against the (frequentist) sample proportion estimate through simulations. The goal is to estimate cohort-specific response rates, focusing on bias, mean squared error, and information borrowing. Various scenarios are examined, including homogeneous, heterogeneous, and clustered response rates across cohorts. Results show trade-offs in bias and precision, highlighting the importance of method selection. Berry's method performs best with limited heterogeneity. No clear winner emerges in general cases, with performance affected by shrinkage, bias, and the choice of priors and tuning parameters. Challenges include computational complexity, parameter tuning, and the lack of clear guidance on selection. Researchers should consider these factors when designing and analyzing basket trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07639v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonios Daletzakis, Rutger van den Bor, Vincent van der Noort, Kit CB Roes</dc:creator>
    </item>
    <item>
      <title>Evaluating the impact of racial disproportionality in Stop &amp; Search on expressive crimes in London</title>
      <link>https://arxiv.org/abs/2502.07695</link>
      <description>arXiv:2502.07695v1 Announce Type: new 
Abstract: Racial disproportionality in Stop &amp; Search practices elicits substantial concerns about its societal and behavioral impacts. This paper aims to investigate the effect of disproportionality, particularly on the black community, on expressive crimes in London using data from January 2019 to December 2023. We focus on a semi-parametric partially linear structural regression method and introduce a Bayesian empirical likelihood procedure combined with double machine learning techniques to control for high-dimensional confounding and to accommodate the strong prior assumption. In addition, we show that the proposed procedure generates a valid posterior in terms of coverage. Applying this approach to the Stop &amp; Search dataset, we find that racial disproportionality aimed at the Black community may be alleviated by taking into account the proportion of the Black population when focusing on expressive crimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07695v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Luo, Yijing Li</dc:creator>
    </item>
    <item>
      <title>Analyzing Geospatial and Socioeconomic Disparities in Breast Cancer Screening Among Populations in the United States: Machine Learning Approach</title>
      <link>https://arxiv.org/abs/2502.06800</link>
      <description>arXiv:2502.06800v1 Announce Type: cross 
Abstract: Breast cancer screening plays a pivotal role in early detection and subsequent effective management of the disease, impacting patient outcomes and survival rates. This study aims to assess breast cancer screening rates nationwide in the United States and investigate the impact of social determinants of health on these screening rates. Data on mammography screening at the census tract level for 2018 and 2020 were collected from the Behavioral Risk Factor Surveillance System. We developed a large dataset of social determinants of health, comprising 13 variables for 72337 census tracts. Spatial analysis employing Getis-Ord Gi statistics was used to identify clusters of high and low breast cancer screening rates. To evaluate the influence of these social determinants, we implemented a random forest model, with the aim of comparing its performance to linear regression and support vector machine models. The models were evaluated using R2 and root mean squared error metrics. Shapley Additive Explanations values were subsequently used to assess the significance of variables and direction of their influence. Geospatial analysis revealed elevated screening rates in the eastern and northern United States, while central and midwestern regions exhibited lower rates. The random forest model demonstrated superior performance, with an R2=64.53 and root mean squared error of 2.06 compared to linear regression and support vector machine models. Shapley Additive Explanations values indicated that the percentage of the Black population, the number of mammography facilities within a 10-mile radius, and the percentage of the population with at least a bachelor's degree were the most influential variables, all positively associated with mammography screening rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06800v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2196/59882</arxiv:DOI>
      <arxiv:journal_reference>JMIR Cancer 2025;11:e59882</arxiv:journal_reference>
      <dc:creator>Soheil Hashtarkhani, Yiwang Zhou, Fekede Asefa Kumsa, Shelley White-Means, David L Schwartz, Arash Shaban-Nejad</dc:creator>
    </item>
    <item>
      <title>Global Ease of Living Index: a machine learning framework for longitudinal analysis of major economies</title>
      <link>https://arxiv.org/abs/2502.06866</link>
      <description>arXiv:2502.06866v1 Announce Type: cross 
Abstract: The drastic changes in the global economy, geopolitical conditions, and disruptions such as the COVID-19 pandemic have impacted the cost of living and quality of life. It is important to understand the long-term nature of the cost of living and quality of life in major economies. A transparent and comprehensive living index must include multiple dimensions of living conditions. In this study, we present an approach to quantifying the quality of life through the Global Ease of Living Index that combines various socio-economic and infrastructural factors into a single composite score. Our index utilises economic indicators that define living standards, which could help in targeted interventions to improve specific areas. We present a machine learning framework for addressing the problem of missing data for some of the economic indicators for specific countries. We then curate and update the data and use a dimensionality reduction approach (principal component analysis) to create the Ease of Living Index for major economies since 1970. Our work significantly adds to the literature by offering a practical tool for policymakers to identify areas needing improvement, such as healthcare systems, employment opportunities, and public safety. Our approach with open data and code can be easily reproduced and applied to various contexts. This transparency and accessibility make our work a valuable resource for ongoing research and policy development in quality-of-life assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06866v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanay Panat, Rohitash Chandra</dc:creator>
    </item>
    <item>
      <title>A multi-arm multi-stage design for trials with all pairwise testing</title>
      <link>https://arxiv.org/abs/2502.07013</link>
      <description>arXiv:2502.07013v1 Announce Type: cross 
Abstract: Multi-arm multi-stage (MAMS) trials have gained popularity to enhance the efficiency of clinical trials, potentially reducing both duration and costs. This paper focuses on designing MAMS trials where no control treatment exists. This can arise when multiple standard treatments are already established or no treatment is available for a severe disease, making it unethical to withhold a potentially helpful option. The proposed design incorporates interim analyses to allow early termination of notably worst treatments and stops the trial entirely if all remaining treatments are performing similarly. The proposed design controls the familywise error rate (FWER) for all pairwise comparisons and provides the conditions guaranteeing FWER control in the strong sense. The FWER and power are used to calculate both the stopping boundaries and the sample size required. Analytic solutions to compute the expected sample size are also derived. A trial motivated by a study conducted in sepsis, where there was no control treatment, is shown. The multi-arm multi-stage all pairwise (MAMSAP) design proposed here is compared to multiple different approaches. For the trial studied, the proposed method yields the lowest required maximum and expected sample size when controlling the FWER and power at the desired levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07013v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Greenstreet, Thomas Jaki, Alun Bedding, Pavel Mozgunov</dc:creator>
    </item>
    <item>
      <title>Likelihood-Free Estimation for Spatiotemporal Hawkes processes with missing data and application to predictive policing</title>
      <link>https://arxiv.org/abs/2502.07111</link>
      <description>arXiv:2502.07111v1 Announce Type: cross 
Abstract: With the growing use of AI technology, many police departments use forecasting software to predict probable crime hotspots and allocate patrolling resources effectively for crime prevention. The clustered nature of crime data makes self-exciting Hawkes processes a popular modeling choice. However, one significant challenge in fitting such models is the inherent missingness in crime data due to non-reporting, which can bias the estimated parameters of the predictive model, leading to inaccurate downstream hotspot forecasts, often resulting in over or under-policing in various communities, especially the vulnerable ones. Our work introduces a Wasserstein Generative Adversarial Networks (WGAN) driven likelihood-free approach to account for unreported crimes in Spatiotemporal Hawkes models. We demonstrate through empirical analysis how this methodology improves the accuracy of parametric estimation in the presence of data missingness, leading to more reliable and efficient policing strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07111v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pramit Das, Moulinath Banerjee, Yuekai Sun</dc:creator>
    </item>
    <item>
      <title>The Variable Multiple Bandpass Periodic Block Bootstrap for Time Series with Multiple Periodic Correlations</title>
      <link>https://arxiv.org/abs/2502.07462</link>
      <description>arXiv:2502.07462v1 Announce Type: cross 
Abstract: This work introduces a novel block bootstrap method for time series with multiple periodically correlated (MPC) components called the Variable Multiple Bandpass Periodic Block Bootstrap (VMBPBB). While past methodological advancements permitted bootstrapping time series to preserve certain correlations, and then periodically correlated (PC) structures, there does not appear to be adequate or efficient methods to bootstrap estimate the sampling distribution of estimators for MPC time series. Current methods that preserve the PC correlation structure resample the original time series, selecting block size to preserve one PC component frequency while simultaneously and unnecessarily resampling all frequencies. This destroys PC components at other frequencies. VMBPBB uses bandpass filters to separate each PC component, creating a set of PC component time series each composed principally of one component. VMBPBB then resamples each PC component time series, not the original MPC time series, with the respective block size preserving the correlation structure of each PC component. Finally, VMBPBB aggregates the PC component bootstraps to form a bootstrap of the MPC time series, successfully preserving all correlations. A simulation study across a wide range of different MPC component frequencies and signal-to-noise ratios is presented and reveals that VMBPBB almost universally outperforms existing methods that fail to bandpass filter the MPC time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07462v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>A Nonparametric and Functional Wombling Methodology</title>
      <link>https://arxiv.org/abs/2502.07740</link>
      <description>arXiv:2502.07740v1 Announce Type: cross 
Abstract: Wombling methods, first introduced in 1951, have been widely applied to detect boundaries and variations across spatial domains, particularly in biological, public health and meteorological studies. Traditional applications focus on finite-dimensional observations, where significant changes in measurable traits indicate structural boundaries. In this work, wombling methodologies are extended to functional data, enabling the identification of spatial variation in infinite-dimensional settings. Proposed is a nonparametric approach that accommodates functional observations without imposing strict distributional assumptions. This methodology successfully captures complex spatial structures and discontinuities, demonstrating superior sensitivity and robustness compared to existing finite-dimensional techniques. This methodology is then applied to analyse regional epidemiological disparities between London and the rest of the UK, identifying key spatial boundaries in the shape of the first trajectory of Covid-19 incidence in 2020. Through extensive simulations and empirical validation, demonstrated is the method's effectiveness in uncovering meaningful spatial variations, with potential applications in a wide variety of fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07740v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke A. Barratt (Statistical Laboratory, DPMMS, University of Cambridge, UK), John A. D. Aston (Statistical Laboratory, DPMMS, University of Cambridge, UK)</dc:creator>
    </item>
    <item>
      <title>Bayesian Learning of Clinically Meaningful Sepsis Phenotypes in Northern Tanzania</title>
      <link>https://arxiv.org/abs/2405.01746</link>
      <description>arXiv:2405.01746v2 Announce Type: replace 
Abstract: Sepsis is a life-threatening condition caused by a dysregulated host response to infection. Recently, researchers have hypothesized that sepsis consists of a heterogeneous spectrum of distinct subtypes, motivating several studies to identify clusters of sepsis patients that correspond to subtypes, with the long-term goal of using these clusters to design subtype-specific treatments. Therefore, clinicians rely on clusters having a concrete medical interpretation, usually corresponding to clinically meaningful regions of the sample space that have a concrete implication to practitioners. In this article, we propose Clustering Around Meaningful Regions (CLAMR), a Bayesian clustering approach that explicitly models the medical interpretation of each cluster center. CLAMR favors clusterings that can be summarized via meaningful feature values, leading to medically significant sepsis patient clusters. We also provide details on measuring the effect of each feature on the clustering using Bayesian hypothesis tests, so one can assess what features are relevant for cluster interpretation. Our focus is on clustering sepsis patients from Moshi, Tanzania, where patients are younger and the prevalence of HIV infection is higher than in previous sepsis subtyping cohorts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01746v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Dombowsky, David B. Dunson, Deng B. Madut, Matthew P. Rubach, Amy H. Herring</dc:creator>
    </item>
    <item>
      <title>Ensemble Kalman Inversion for Geothermal Reservoir Modelling</title>
      <link>https://arxiv.org/abs/2410.09017</link>
      <description>arXiv:2410.09017v4 Announce Type: replace 
Abstract: Numerical models of geothermal reservoirs typically depend on hundreds or thousands of unknown parameters, which must be estimated using sparse, noisy data. However, these models capture complex physical processes, which frequently results in long run-times and simulation failures, making the process of estimating the unknown parameters a challenging task. Conventional techniques for parameter estimation and uncertainty quantification, such as Markov chain Monte Carlo (MCMC), can require tens of thousands of simulations to provide accurate results and are therefore challenging to apply in this context. In this paper, we study the ensemble Kalman inversion (EKI) algorithm as an alternative technique for approximate parameter estimation and uncertainty quantification for geothermal reservoir models. EKI possesses several characteristics that make it well-suited to a geothermal setting; it is derivative-free, parallelisable, robust to simulation failures, and in many cases requires far fewer simulations to provide an accurate characterisation of the posterior than conventional uncertainty quantification techniques such as MCMC. We illustrate the use of EKI in a reservoir modelling context using a combination of synthetic and real-world case studies. Through these case studies, we also demonstrate how EKI can be paired with flexible parametrisation techniques capable of accurately representing prior knowledge of the characteristics of a reservoir and adhering to geological constraints, and how the algorithm can be made robust to simulation failures. Our results demonstrate that EKI provides a reliable and efficient means of obtaining accurate parameter estimates for large-scale, two-phase geothermal reservoir models, with appropriate characterisation of uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09017v4</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex de Beer, Elvar K Bjarkason, Michael Gravatt, Ruanui Nicholson, John P O'Sullivan, Michael J O'Sullivan, Oliver J Maclaren</dc:creator>
    </item>
    <item>
      <title>The Global Carbon Budget as a cointegrated system</title>
      <link>https://arxiv.org/abs/2412.09226</link>
      <description>arXiv:2412.09226v3 Announce Type: replace 
Abstract: The Global Carbon Budget, maintained by the Global Carbon Project, summarizes Earth's global carbon cycle through four annual time series beginning in 1959: atmospheric CO$_2$ concentrations, anthropogenic CO$_2$ emissions, and CO$_2$ uptake by land and ocean. We analyze these four time series as a multivariate (cointegrated) system. Statistical tests show that the four time series are cointegrated with rank three and identify anthropogenic CO$_2$ emissions as the single stochastic trend driving the nonstationary dynamics of the system. The three cointegrated relations correspond to the physical relations that the sinks are linearly related to atmospheric concentrations and that the change in concentrations equals emissions minus the combined uptake by land and ocean. Furthermore, likelihood ratio tests show that a parametrically restricted error-correction model that embodies these physical relations and accounts for the El Ni\~no/Southern Oscillation cannot be rejected on the data. The model can be used for both in-sample and out-of-sample analysis. In an application of the latter, we demonstrate that projections based on this model, using Shared Socioeconomic Pathways scenarios, yield results consistent with established climate science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09226v3</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mikkel Bennedsen, Eric Hillebrand, Morten {\O}rregaard Nielsen</dc:creator>
    </item>
    <item>
      <title>A Tutorial on Markov Renewal and Semi-Markov Proportional Hazards Model</title>
      <link>https://arxiv.org/abs/2502.03479</link>
      <description>arXiv:2502.03479v3 Announce Type: replace 
Abstract: Transition probability estimation plays a critical role in multi-state modeling, especially in clinical research. This paper investigates the application of semi-Markov and Markov renewal frameworks to the EBMT dataset, focusing on six clinical states encountered during hematopoietic stem cell transplantation. By comparing Aalen-Johansen (AJ) and Dabrowska-Sun-Horowitz (DSH) estimators, we demonstrate that semi-Markov models, which incorporate sojourn times, provide a more nuanced and temporally sensitive depiction of patient trajectories compared to memoryless Markov models. The DSH estimator consistently yields smoother probability curves, particularly for transitions involving prolonged states. We use empirical process theory and Burkholder-Davis-Gundy inequality to show weak convergence of the estimator. Future work includes extending the framework to accommodate advanced covariate structures and non-Markovian dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03479v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eliuvish Cuicizion, Itsugo Ri, Elaine Holmes, Jawad Chern</dc:creator>
    </item>
    <item>
      <title>Correcting Selection Bias in Standardized Test Comparisons</title>
      <link>https://arxiv.org/abs/2309.10642</link>
      <description>arXiv:2309.10642v5 Announce Type: replace-cross 
Abstract: This paper tackles the critical issue of sample selection bias in cross-country comparisons using international assessments such as PISA (Program for International Student Assessment). While PISA is widely used to rank educational performance, its reliance on samples of students still in school at age 15 introduces survival bias, potentially distorting comparisons. To address this, I developed an econometric framework based on a quantile selection model. Under a stochastic dominance assumption, the selection-corrected quantile function is partially identified, with point identification achieved under parametric restrictions on the joint distribution of test scores and selection. Applying this method to PISA 2018 data, I demonstrate that correcting for selection bias leads to significant changes in country rankings based on mean performance. These findings underscore the importance of accounting for sample selection bias to ensure accurate and meaningful international educational comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10642v5</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onil Boussim</dc:creator>
    </item>
    <item>
      <title>Zero-inflated stochastic volatility model for disaggregated inflation data with exact zeros</title>
      <link>https://arxiv.org/abs/2403.10945</link>
      <description>arXiv:2403.10945v2 Announce Type: replace-cross 
Abstract: The disaggregated time-series data for Consumer Price Index (CPI) often exhibits frequent instances of exact zero price changes, stemming from measurement errors inherent in the data collection process. However, the currently prominent stochastic volatility model of trend inflation is designed for aggregate measures of price inflation, where exact zero price changes rarely occur. We formulate a zero-inflated stochastic volatility model applicable to such non-stationary real-valued multivariate time-series data with exact zeros. The Bayesian dynamic generalized linear model jointly specifies the dynamic zero-generating process. We construct an efficient custom Gibbs sampler, leveraging the P\'{o}lya-Gamma augmentation. Applying the model to disaggregated CPI data in four advanced economies -- US, UK, Germany, and Japan -- we find that the zero-inflated model provides more sensible and informative estimates of time-varying trend and volatility. Through an out-of-sample forecasting exercise, we find that the zero-inflated model delivers improved in point forecasts and better-calibrated interval forecasts, particularly when zero-inflation is prevalent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10945v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geonhee Han, Kaoru Irie</dc:creator>
    </item>
    <item>
      <title>Data Assimilation with Machine Learning Surrogate Models: A Case Study with FourCastNet</title>
      <link>https://arxiv.org/abs/2405.13180</link>
      <description>arXiv:2405.13180v2 Announce Type: replace-cross 
Abstract: Modern data-driven surrogate models for weather forecasting provide accurate short-term predictions but inaccurate and nonphysical long-term forecasts. This paper investigates online weather prediction using machine learning surrogates supplemented with partial and noisy observations. We empirically demonstrate and theoretically justify that, despite the long-time instability of the surrogates and the sparsity of the observations, filtering estimates can remain accurate in the long-time horizon. As a case study, we integrate FourCastNet, a weather surrogate model, within a variational data assimilation framework using partial, noisy ERA5 data. Our results show that filtering estimates remain accurate over a year-long assimilation window and provide effective initial conditions for forecasting tasks, including extreme event prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13180v2</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>nlin.CD</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melissa Adrian, Daniel Sanz-Alonso, Rebecca Willett</dc:creator>
    </item>
    <item>
      <title>Ensemble quantile-based deep learning framework for streamflow and flood prediction in Australian catchments</title>
      <link>https://arxiv.org/abs/2407.15882</link>
      <description>arXiv:2407.15882v2 Announce Type: replace-cross 
Abstract: In recent years, climate extremes such as floods have created significant environmental and economic hazards for Australia. Deep learning methods have been promising for predicting extreme climate events; however, large flooding events present a critical challenge due to factors such as model calibration and missing data. We present an ensemble quantile-based deep learning framework that addresses large-scale streamflow forecasts using quantile regression for uncertainty projections in prediction. We evaluate selected univariate and multivariate deep learning models and catchment strategies. Furthermore, we implement a multistep time-series prediction model using the CAMELS dataset for selected catchments across Australia. The ensemble model employs a set of quantile deep learning models for streamflow determined by historical streamflow data. We utilise the streamflow prediction and obtain flood probability using flood frequency analysis and compare it with historical flooding events for selected catchments. Our results demonstrate notable efficacy and uncertainties in streamflow forecasts with varied catchment properties. Our flood probability estimates show good accuracy in capturing the historical floods from the selected catchments. This underscores the potential for our deep learning framework to revolutionise flood forecasting across diverse regions and be implemented as an early warning system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15882v2</guid>
      <category>cs.LG</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohitash Chandra, Arpit Kapoor, Siddharth Khedkar, Jim Ng, R. Willem Vervoort</dc:creator>
    </item>
    <item>
      <title>Overcoming data challenges to measure whole-person health in electronic health records</title>
      <link>https://arxiv.org/abs/2502.05380</link>
      <description>arXiv:2502.05380v2 Announce Type: replace-cross 
Abstract: The allostatic load index (ALI) is a composite measure of whole-person health. Data from electronic health records (EHR) present a huge opportunity to operationalize the ALI in the learning health system, except they are prone to missingness and errors. Validation of EHR data (e.g., through chart reviews) can provide better-quality data, but realistically, only a subset of patients' data can be validated, and most protocols do not recover missing data. Using a representative sample of 1000 patients from the EHR at an extensive learning health system (100 of whom could be validated), we propose methods to design, conduct, and analyze statistically efficient and robust studies of the ALI and healthcare utilization. With semiparametric maximum likelihood estimation, we robustly incorporate all available data into statistical models. Using targeted design strategies, we examine ways to select the most informative patients for validation. Incorporating clinical expertise, we devise a novel validation protocol to promote the quality and completeness of EHR data. Validating the EHR data uncovered relatively low error rates and recovered some missing data. Through simulation studies based on preliminary data, residual sampling was identified as the most informative strategy for completing our validation study. Statistical models of partially validated data indicated higher odds of engaging in the healthcare system were associated with worse whole-person health (i.e., higher ALI), adjusting for age. Targeted validation with an enriched protocol allowed us to ensure the quality and promote the completeness of the EHR. Findings from our validation study were incorporated into analyses as we operationalize the ALI as a whole-person health measure intended to predict healthcare utilization in the academic learning health system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05380v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Sheetal Kedar, Rabeya Tahir, Aidan D. Keleghan, Amelia Miranda, Stephany N. Duda, Michael P. Bancks, Brian J. Wells, Ashish K. Khanna, Joseph Rigdon</dc:creator>
    </item>
    <item>
      <title>A state-space framework for causal detection of hippocampal ripple-replay events</title>
      <link>https://arxiv.org/abs/2502.05394</link>
      <description>arXiv:2502.05394v2 Announce Type: replace-cross 
Abstract: Hippocampal ripple-replay events are typically identified using a two-step process that at each time point uses past and future data to determine whether an event is occurring. This prevents researchers from identifying these events in real time for closed-loop experiments. It also prevents the identification of periods of nonlocal representation that are not accompanied by large changes in the spectral content of the local field potentials (LFPs). In this work, we present a new state-space model framework that is able to detect concurrent changes in the rhythmic structure of LFPs with nonlocal activity in place cells to identify ripple-replay events in a causal manner. The model combines latent factors related to neural oscillations, represented space, and switches between coding properties to explain simultaneously the spiking activity from multiple units and the rhythmic content of LFPs recorded from multiple sources. The model is temporally causal, meaning that estimates of the switching state can be made at each instant using only past information from the spike and LFP signals, or can be combined with future data to refine those estimates. We applied this model framework to simulated and real hippocampal data to demonstrate its performance in identifying ripple-replay events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05394v2</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sirui Zeng, Uri T. Eden</dc:creator>
    </item>
  </channel>
</rss>
