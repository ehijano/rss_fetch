<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Mar 2025 05:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Time-Irreversible Quantum-Classical Dynamics of Molecular Models in the Brain</title>
      <link>https://arxiv.org/abs/2503.00016</link>
      <description>arXiv:2503.00016v1 Announce Type: new 
Abstract: This manuscript aims to illustrate a quantum-classical dissipative theory (suited to be converted to effective algorithms for numerical simulations) within the long-term project of studying molecular processes in the brain. Other approaches, briefly sketched in the text, have advocated the need to deal with both quantum and classical dynamic variables when studying the brain. At variance with these other frameworks, the manuscript's formalism allows us to explicitly treat the classical dynamical variables. The theory must be dissipative not because of formal requirements but because brain processes appear to be dissipative at the molecular, physiological, and high functional levels. We discuss theoretically that using Brownian dynamics or the Nos\`e-Hoover-Chain thermostat to perform computer simulations provides an effective way to introduce an arrow of time for open quantum systems in a classical environment. In the future, We plan to study classical models of neurons and astrocytes, as well as their networks, coupled to quantum dynamical variables describing, e.g., nuclear and electron spins, HOMO and LUMO orbitals of phenyl and indole rings, ion channels, and tunneling protons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00016v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.other</category>
      <category>physics.bio-ph</category>
      <category>quant-ph</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/sym17020285</arxiv:DOI>
      <arxiv:journal_reference>Symmetry 2025, 17, 285</arxiv:journal_reference>
      <dc:creator>Alessandro Sergi, Antonino Messina, Rosalba Saija, Gabriella Martino, Maria Teresa Caccamo, Min-Fang Kuo, Michael A. Nitsche</dc:creator>
    </item>
    <item>
      <title>Implicit Generative Modeling by Kernel Similarity Matching</title>
      <link>https://arxiv.org/abs/2503.00655</link>
      <description>arXiv:2503.00655v1 Announce Type: new 
Abstract: Understanding how the brain encodes stimuli has been a fundamental problem in computational neuroscience. Insights into this problem have led to the design and development of artificial neural networks that learn representations by incorporating brain-like learning abilities. Recently, learning representations by capturing similarity between input samples has been studied to tackle this problem. This approach, however, has thus far been used to only learn downstream features from an input and has not been studied in the context of a generative paradigm, where one can map the representations back to the input space, incorporating not only bottom-up interactions (stimuli to latent) but also learning features in a top-down manner (latent to stimuli). We investigate a kernel similarity matching framework for generative modeling. Starting with a modified sparse coding objective for learning representations proposed in prior work, we demonstrate that representation learning in this context is equivalent to maximizing similarity between the input kernel and a latent kernel. We show that an implicit generative model arises from learning the kernel structure in the latent space and show how the framework can be adapted to learn manifold structures, potentially providing insights as to how task representations can be encoded in the brain. To solve the objective, we propose a novel Alternate Direction Method of Multipliers (ADMM) based algorithm and discuss the interpretation of the optimization process. Finally, we discuss how this representation learning problem can lead towards a biologically plausible architecture to learn the model parameters that ties together representation learning using similarity matching (a bottom-up approach) with predictive coding (a top-down approach).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00655v1</guid>
      <category>q-bio.NC</category>
      <category>eess.SP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shubham Choudhary, Paul Masset, Demba Ba</dc:creator>
    </item>
    <item>
      <title>Dementia Insights: A Context-Based MultiModal Approach</title>
      <link>https://arxiv.org/abs/2503.01226</link>
      <description>arXiv:2503.01226v1 Announce Type: new 
Abstract: Dementia, a progressive neurodegenerative disorder, affects memory, reasoning, and daily functioning, creating challenges for individuals and healthcare systems. Early detection is crucial for timely interventions that may slow disease progression. Large pre-trained models (LPMs) for text and audio, such as Generative Pre-trained Transformer (GPT), Bidirectional Encoder Representations from Transformers (BERT), and Contrastive Language-Audio Pretraining (CLAP), have shown promise in identifying cognitive impairments. However, existing studies generally rely heavily on expert-annotated datasets and unimodal approaches, limiting robustness and scalability. This study proposes a context-based multimodal method, integrating both text and audio data using the best-performing LPMs in each modality. By incorporating contextual embeddings, our method improves dementia detection performance. Additionally, motivated by the effectiveness of contextual embeddings, we further experimented with a context-based In-Context Learning (ICL) as a complementary technique. Results show that GPT-based embeddings, particularly when fused with CLAP audio features, achieve an F1-score of $83.33\%$, surpassing state-of-the-art dementia detection models. Furthermore, raw text data outperforms expert-annotated datasets, demonstrating that LPMs can extract meaningful linguistic and acoustic patterns without extensive manual labeling. These findings highlight the potential for scalable, non-invasive diagnostic tools that reduce reliance on costly annotations while maintaining high accuracy. By integrating multimodal learning with contextual embeddings, this work lays the foundation for future advancements in personalized dementia detection and cognitive health research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01226v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahar Sinene Mehdoui, Abdelhamid Bouzid, Daniel Sierra-Sosa, Adel Elmaghraby</dc:creator>
    </item>
    <item>
      <title>A comprehensive and reliable protocol for manual segmentation of the human claustrum using high-resolution MRI</title>
      <link>https://arxiv.org/abs/2503.01761</link>
      <description>arXiv:2503.01761v1 Announce Type: new 
Abstract: The claustrum is a thin gray matter structure in each brain hemisphere, characterized by exceptionally high connectivity with nearly all brain regions. Despite extensive animal studies on its anatomy and function and growing evidence of claustral deficits in neuropsychiatric disorders, its specific roles in normal and abnormal human brain function remain largely unknown. This is primarily due to its thin and complex morphology, which limits accurate anatomical delineation and neural activity isolation in conventional in vivo neuroimaging. To facilitate future neuroimaging studies, we developed a comprehensive and reliable manual segmentation protocol based on a cellular-resolution brain atlas and high-resolution (0.7^3 mm) MRI data. The protocols involve detailed guidelines to delineate the entire claustrum, including the inferior parts that have not been clearly described in earlier MRI studies. Additionally, we propose a geometric method to parcellate the claustrum into three subregions (the dorsal, ventral, and temporal claustrum) along the superior-to-inferior axis. The mean bilateral claustrum volume in 10 young adults was 3307.5 mm^3, approximately 0.21% of total intracranial volume. Our segmentation protocol demonstrated high inter- and intra-rater reliability (ICC &gt; 0.89, DSC &gt; 0.85), confirming its replicability. This comprehensive and reliable claustrum segmentation protocols will provide a cornerstone for future neuroimaging studies of systematic, large-scale investigations of the anatomy and the functions of the human claustrum in normal and pathological populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01761v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Seung-Suk Kang, Joseph Bodenheimer, Kayley Morris, Tracey Butler</dc:creator>
    </item>
    <item>
      <title>Forecasting Whole-Brain Neuronal Activity from Volumetric Video</title>
      <link>https://arxiv.org/abs/2503.00073</link>
      <description>arXiv:2503.00073v1 Announce Type: cross 
Abstract: Large-scale neuronal activity recordings with fluorescent calcium indicators are increasingly common, yielding high-resolution 2D or 3D videos. Traditional analysis pipelines reduce this data to 1D traces by segmenting regions of interest, leading to inevitable information loss. Inspired by the success of deep learning on minimally processed data in other domains, we investigate the potential of forecasting neuronal activity directly from volumetric videos. To capture long-range dependencies in high-resolution volumetric whole-brain recordings, we design a model with large receptive fields, which allow it to integrate information from distant regions within the brain. We explore the effects of pre-training and perform extensive model selection, analyzing spatio-temporal trade-offs for generating accurate forecasts. Our model outperforms trace-based forecasting approaches on ZAPBench, a recently proposed benchmark on whole-brain activity prediction in zebrafish, demonstrating the advantages of preserving the spatial structure of neuronal activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00073v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Immer, Jan-Matthis Lueckmann, Alex Bo-Yuan Chen, Peter H. Li, Mariela D. Petkova, Nirmala A. Iyer, Aparna Dev, Gudrun Ihrke, Woohyun Park, Alyson Petruncio, Aubrey Weigel, Wyatt Korff, Florian Engert, Jeff W. Lichtman, Misha B. Ahrens, Viren Jain, Micha{\l} Januszewski</dc:creator>
    </item>
    <item>
      <title>Exploring the Potential of QEEGNet for Cross-Task and Cross-Dataset Electroencephalography Encoding with Quantum Machine Learning</title>
      <link>https://arxiv.org/abs/2503.00080</link>
      <description>arXiv:2503.00080v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) is widely used in neuroscience and clinical research for analyzing brain activity. While deep learning models such as EEGNet have shown success in decoding EEG signals, they often struggle with data complexity, inter-subject variability, and noise robustness. Recent advancements in quantum machine learning (QML) offer new opportunities to enhance EEG analysis by leveraging quantum computing's unique properties. In this study, we extend the previously proposed Quantum-EEGNet (QEEGNet), a hybrid neural network incorporating quantum layers into EEGNet, to investigate its generalization ability across multiple EEG datasets. Our evaluation spans a diverse set of cognitive and motor task datasets, assessing QEEGNet's performance in different learning scenarios. Experimental results reveal that while QEEGNet demonstrates competitive performance and maintains robustness in certain datasets, its improvements over traditional deep learning methods remain inconsistent. These findings suggest that hybrid quantum-classical architectures require further optimization to fully leverage quantum advantages in EEG processing. Despite these limitations, our study provides new insights into the applicability of QML in EEG research and highlights challenges that must be addressed for future advancements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00080v1</guid>
      <category>quant-ph</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi-Sheng Chen, Samuel Yen-Chi Chen, Huan-Hsin Tseng</dc:creator>
    </item>
    <item>
      <title>Statistical Mechanics of Semantic Compression</title>
      <link>https://arxiv.org/abs/2503.00612</link>
      <description>arXiv:2503.00612v1 Announce Type: cross 
Abstract: The basic problem of semantic compression is to minimize the length of a message while preserving its meaning. This differs from classical notions of compression in that the distortion is not measured directly at the level of bits, but rather in an abstract semantic space. In order to make this precise, we take inspiration from cognitive neuroscience and machine learning and model semantic space as a continuous Euclidean vector space. In such a space, stimuli like speech, images, or even ideas, are mapped to high-dimensional real vectors, and the location of these embeddings determines their meaning relative to other embeddings. This suggests that a natural metric for semantic similarity is just the Euclidean distance, which is what we use in this work. We map the optimization problem of determining the minimal-length, meaning-preserving message to a spin glass Hamiltonian and solve the resulting statistical mechanics problem using replica theory. We map out the replica symmetric phase diagram, identifying distinct phases of semantic compression: a first-order transition occurs between lossy and lossless compression, whereas a continuous crossover is seen from extractive to abstractive compression. We conclude by showing numerical simulations of compressions obtained by simulated annealing and greedy algorithms, and argue that while the problem of finding a meaning-preserving compression is computationally hard in the worst case, there exist efficient algorithms which achieve near optimal performance in the typical case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00612v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tankut Can</dc:creator>
    </item>
    <item>
      <title>Range, not Independence, Drives Modularity in Biologically Inspired Representations</title>
      <link>https://arxiv.org/abs/2410.06232</link>
      <description>arXiv:2410.06232v3 Announce Type: replace 
Abstract: Why do biological and artificial neurons sometimes modularise, each encoding a single meaningful variable, and sometimes entangle their representation of many variables? In this work, we develop a theory of when biologically inspired networks -- those that are nonnegative and energy efficient -- modularise their representation of source variables (sources). We derive necessary and sufficient conditions on a sample of sources that determine whether the neurons in an optimal biologically-inspired linear autoencoder modularise. Our theory applies to any dataset, extending far beyond the case of statistical independence studied in previous work. Rather we show that sources modularise if their support is ``sufficiently spread''. From this theory, we extract and validate predictions in a variety of empirical studies on how data distribution affects modularisation in nonlinear feedforward and recurrent neural networks trained on supervised and unsupervised tasks. Furthermore, we apply these ideas to neuroscience data, showing that range independence can be used to understand the mixing or modularising of spatial and reward information in entorhinal recordings in seemingly conflicting experiments. Further, we use these results to suggest alternate origins of mixed-selectivity, beyond the predominant theory of flexible nonlinear classification. In sum, our theory prescribes precise conditions on when neural activities modularise, providing tools for inducing and elucidating modular representations in brains and machines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06232v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Will Dorrell, Kyle Hsu, Luke Hollingsworth, Jin Hwa Lee, Jiajun Wu, Chelsea Finn, Peter E Latham, Tim EJ Behrens, James CR Whittington</dc:creator>
    </item>
    <item>
      <title>Predictive Strategies for the Control of Complex Motor Skills: Recent Insights into Individual and Joint Actions</title>
      <link>https://arxiv.org/abs/2412.04191</link>
      <description>arXiv:2412.04191v2 Announce Type: replace 
Abstract: Humans perform exquisite sensorimotor skills, both individually and in teams, from athletes performing rhythmic gymnastics to everyday tasks like carrying a cup of coffee. The "predictive brain" framework suggests that mastering these skills relies on predictive mechanisms, raising the question of how we deploy predictions for real-time control and coordination. This review highlights two research lines, showing that during the control of complex objects people make the interaction with 'tools' predictable; and that during dyadic coordination people make their behavior predictable and legible for their partners. These studies demonstrate that to achieve sophisticated motor skills, we play "prediction tricks": we select subspaces of predictable solutions and make sensorimotor interactions more predictable and legible by and for others. This synthesis underscores the critical role of predictability in optimizing control strategies across contexts. Furthermore, it emphasizes the need for novel studies on the scope and limits of predictive mechanisms in motor control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04191v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marta Russo, Antonella Maselli, Dagmar Sternad, Giovanni Pezzulo</dc:creator>
    </item>
    <item>
      <title>Artificial Neural Networks for Magnetoencephalography: A review of an emerging field</title>
      <link>https://arxiv.org/abs/2501.11566</link>
      <description>arXiv:2501.11566v2 Announce Type: replace 
Abstract: Magnetoencephalography (MEG) is a cutting-edge neuroimaging technique that measures the intricate brain dynamics underlying cognitive processes with an unparalleled combination of high temporal and spatial precision. MEG data analytics has always relied on advanced signal processing and mathematical and statistical tools for various tasks ranging from data cleaning to probing the signals' rich dynamics and estimating the neural sources underlying the surface-level recordings. Like in most domains, the surge in Artificial Intelligence (AI) has led to the increased use of Machine Learning (ML) methods for MEG data classification. More recently, an emerging trend in this field is using Artificial Neural Networks (ANNs) to address many MEG-related tasks. This review provides a comprehensive overview of how ANNs are being used with MEG data from three vantage points: First, we review work that employs ANNs for MEG signal classification, i.e., for brain decoding. Second, we report on work that has used ANNs as putative models of information processing in the human brain. Finally, we examine studies that use ANNs as techniques to tackle methodological questions in MEG, including artifact correction and source estimation. Furthermore, we assess the current strengths and limitations of using ANNs with MEG and discuss future challenges and opportunities in this field. Finally, by establishing a detailed portrait of the field and providing practical recommendations for the future, this review seeks to provide a helpful reference for both seasoned MEG researchers and newcomers to the field who are interested in using ANNs to enhance the exploration of the complex dynamics of the human brain with MEG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11566v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arthur Dehgan, Hamza Abdelhedi, Vanessa Hadid, Irina Rish, Karim Jerbi</dc:creator>
    </item>
    <item>
      <title>Disentangling Representations through Multi-task Learning</title>
      <link>https://arxiv.org/abs/2407.11249</link>
      <description>arXiv:2407.11249v3 Announce Type: replace-cross 
Abstract: Intelligent perception and interaction with the world hinges on internal representations that capture its underlying structure (''disentangled'' or ''abstract'' representations). Disentangled representations serve as world models, isolating latent factors of variation in the world along approximately orthogonal directions, thus facilitating feature-based generalization. We provide experimental and theoretical results guaranteeing the emergence of disentangled representations in agents that optimally solve multi-task evidence accumulation classification tasks, canonical in the neuroscience literature. The key conceptual finding is that, by producing accurate multi-task classification estimates, a system implicitly represents a set of coordinates specifying a disentangled representation of the underlying latent state of the data it receives. The theory provides conditions for the emergence of these representations in terms of noise, number of tasks, and evidence accumulation time. We experimentally validate these predictions in RNNs trained to multi-task, which learn disentangled representations in the form of continuous attractors, leading to zero-shot out-of-distribution (OOD) generalization in predicting latent factors. We demonstrate the robustness of our framework across autoregressive architectures, decision boundary geometries and in tasks requiring classification confidence estimation. We find that transformers are particularly suited for disentangling representations, which might explain their unique world understanding abilities. Overall, our framework establishes a formal link between competence at multiple tasks and the formation of disentangled, interpretable world models in both biological and artificial systems, and helps explain why ANNs often arrive at human-interpretable concepts, and how they both may acquire exceptional zero-shot generalization capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11249v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>International Conference on Learning Representations, 2025 https://openreview.net/forum?id=yVGGtsOgc7</arxiv:journal_reference>
      <dc:creator>Pantelis Vafidis, Aman Bhargava, Antonio Rangel</dc:creator>
    </item>
    <item>
      <title>A Computational Framework for Modeling Emergence of Color Vision in the Human Brain</title>
      <link>https://arxiv.org/abs/2408.16916</link>
      <description>arXiv:2408.16916v2 Announce Type: replace-cross 
Abstract: It is a mystery how the brain decodes color vision purely from the optic nerve signals it receives, with a core inferential challenge being how it disentangles internal perception with the correct color dimensionality from the unknown encoding properties of the eye. In this paper, we introduce a computational framework for modeling this emergence of human color vision by simulating both the eye and the cortex. Existing research often overlooks how the cortex develops color vision or represents color space internally, assuming that the color dimensionality is known a priori; however, we argue that the visual cortex has the capability and the challenge of inferring the color dimensionality purely from fluctuations in the optic nerve signals. To validate our theory, we introduce a simulation engine for biological eyes based on established vision science and generate optic nerve signals resulting from looking at natural images. Further, we propose a bio-plausible model of cortical learning based on self-supervised prediction of optic nerve signal fluctuations under natural eye motions. We show that this model naturally learns to generate color vision by disentangling retinal invariants from the sensory signals. When the retina contains N types of color photoreceptors, our simulation shows that N-dimensional color vision naturally emerges, verified through formal colorimetry. Using this framework, we also present the first simulation work that successfully boosts the color dimensionality, as observed in gene therapy on squirrel monkeys, and demonstrates the possibility of enhancing human color vision from 3D to 4D.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16916v2</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Atsunobu Kotani, Ren Ng</dc:creator>
    </item>
    <item>
      <title>Generative causal testing to bridge data-driven models and scientific theories in language neuroscience</title>
      <link>https://arxiv.org/abs/2410.00812</link>
      <description>arXiv:2410.00812v2 Announce Type: replace-cross 
Abstract: Representations from large language models are highly effective at predicting BOLD fMRI responses to language stimuli. However, these representations are largely opaque: it is unclear what features of the language stimulus drive the response in each brain area. We present generative causal testing (GCT), a framework for generating concise explanations of language selectivity in the brain from predictive models and then testing those explanations in follow-up experiments using LLM-generated stimuli.This approach is successful at explaining selectivity both in individual voxels and cortical regions of interest (ROIs), including newly identified microROIs in prefrontal cortex. We show that explanatory accuracy is closely related to the predictive power and stability of the underlying predictive models. Finally, we show that GCT can dissect fine-grained differences between brain areas with similar functional selectivity. These results demonstrate that LLMs can be used to bridge the widening gap between data-driven models and formal scientific theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00812v2</guid>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard Antonello, Chandan Singh, Shailee Jain, Aliyah Hsu, Sihang Guo, Jianfeng Gao, Bin Yu, Alexander Huth</dc:creator>
    </item>
    <item>
      <title>Tracking objects that change in appearance with phase synchrony</title>
      <link>https://arxiv.org/abs/2410.02094</link>
      <description>arXiv:2410.02094v3 Announce Type: replace-cross 
Abstract: Objects we encounter often change appearance as we interact with them. Changes in illumination (shadows), object pose, or the movement of non-rigid objects can drastically alter available image features. How do biological visual systems track objects as they change? One plausible mechanism involves attentional mechanisms for reasoning about the locations of objects independently of their appearances -- a capability that prominent neuroscience theories have associated with computing through neural synchrony. Here, we describe a novel deep learning circuit that can learn to precisely control attention to features separately from their location in the world through neural synchrony: the complex-valued recurrent neural network (CV-RNN). Next, we compare object tracking in humans, the CV-RNN, and other deep neural networks (DNNs), using FeatureTracker: a large-scale challenge that asks observers to track objects as their locations and appearances change in precisely controlled ways. While humans effortlessly solved FeatureTracker, state-of-the-art DNNs did not. In contrast, our CV-RNN behaved similarly to humans on the challenge, providing a computational proof-of-concept for the role of phase synchronization as a neural substrate for tracking appearance-morphing objects as they move about.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02094v3</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sabine Muzellec, Drew Linsley, Alekh K. Ashok, Ennio Mingolla, Girik Malik, Rufin VanRullen, Thomas Serre</dc:creator>
    </item>
    <item>
      <title>Dissociating Artificial Intelligence from Artificial Consciousness</title>
      <link>https://arxiv.org/abs/2412.04571</link>
      <description>arXiv:2412.04571v2 Announce Type: replace-cross 
Abstract: Developments in machine learning and computing power suggest that artificial general intelligence is within reach. This raises the question of artificial consciousness: if a computer were to be functionally equivalent to a human, being able to do all we do, would it experience sights, sounds, and thoughts, as we do when we are conscious? Answering this question in a principled manner can only be done on the basis of a theory of consciousness that is grounded in phenomenology and that states the necessary and sufficient conditions for any system, evolved or engineered, to support subjective experience. Here we employ Integrated Information Theory (IIT), which provides principled tools to determine whether a system is conscious, to what degree, and the content of its experience. We consider pairs of systems constituted of simple Boolean units, one of which -- a basic stored-program computer -- simulates the other with full functional equivalence. By applying the principles of IIT, we demonstrate that (i) two systems can be functionally equivalent without being phenomenally equivalent, and (ii) that this conclusion is not dependent on the simulated system's function. We further demonstrate that, according to IIT, it is possible for a digital computer to simulate our behavior, possibly even by simulating the neurons in our brain, without replicating our experience. This contrasts sharply with computational functionalism, the thesis that performing computations of the right kind is necessary and sufficient for consciousness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04571v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Graham Findlay, William Marshall, Larissa Albantakis, Isaac David, William GP Mayner, Christof Koch, Giulio Tononi</dc:creator>
    </item>
    <item>
      <title>CBraMod: A Criss-Cross Brain Foundation Model for EEG Decoding</title>
      <link>https://arxiv.org/abs/2412.07236</link>
      <description>arXiv:2412.07236v4 Announce Type: replace-cross 
Abstract: Electroencephalography (EEG) is a non-invasive technique to measure and record brain electrical activity, widely used in various BCI and healthcare applications. Early EEG decoding methods rely on supervised learning, limited by specific tasks and datasets, hindering model performance and generalizability. With the success of large language models, there is a growing body of studies focusing on EEG foundation models. However, these studies still leave challenges: Firstly, most of existing EEG foundation models employ full EEG modeling strategy. It models the spatial and temporal dependencies between all EEG patches together, but ignores that the spatial and temporal dependencies are heterogeneous due to the unique structural characteristics of EEG signals. Secondly, existing EEG foundation models have limited generalizability on a wide range of downstream BCI tasks due to varying formats of EEG data, making it challenging to adapt to. To address these challenges, we propose a novel foundation model called CBraMod. Specifically, we devise a criss-cross transformer as the backbone to thoroughly leverage the structural characteristics of EEG signals, which can model spatial and temporal dependencies separately through two parallel attention mechanisms. And we utilize an asymmetric conditional positional encoding scheme which can encode positional information of EEG patches and be easily adapted to the EEG with diverse formats. CBraMod is pre-trained on a very large corpus of EEG through patch-based masked EEG reconstruction. We evaluate CBraMod on up to 10 downstream BCI tasks (12 public datasets). CBraMod achieves the state-of-the-art performance across the wide range of tasks, proving its strong capability and generalizability. The source code is publicly available at https://github.com/wjq-learning/CBraMod.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07236v4</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiquan Wang, Sha Zhao, Zhiling Luo, Yangxuan Zhou, Haiteng Jiang, Shijian Li, Tao Li, Gang Pan</dc:creator>
    </item>
    <item>
      <title>Asymmetric coupling of nonchaotic Rulkov neurons: Fractal attractors, quasimultistability, and final state sensitivity</title>
      <link>https://arxiv.org/abs/2412.16189</link>
      <description>arXiv:2412.16189v3 Announce Type: replace-cross 
Abstract: Although neuron models have been well studied for their rich dynamics and biological properties, limited research has been done on the complex geometries that emerge from the basins of attraction and basin boundaries of multistable neuron systems. In this paper, we investigate the geometrical properties of the strange attractors, four-dimensional basins, and fractal basin boundaries of an asymmetrically electrically coupled system of two identical nonchaotic Rulkov neurons. We discover a quasimultistability in the system emerging from the existence of a chaotic spiking-bursting pseudo-attractor, and we classify and quantify the system's basins of attraction, which are found to have complex fractal geometries. Using the method of uncertainty exponents, we also find that the system exhibits extreme final state sensitivity, which results in a dynamical uncertainty that could have important applications in neurobiology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16189v3</guid>
      <category>nlin.CD</category>
      <category>math.DS</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevE.111.034201</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. E 111, 034201 (2025)</arxiv:journal_reference>
      <dc:creator>Brandon B. Le</dc:creator>
    </item>
  </channel>
</rss>
