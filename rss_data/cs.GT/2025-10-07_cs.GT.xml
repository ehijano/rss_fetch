<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Oct 2025 01:41:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Downside Risk-Aware Equilibria for Strategic Decision-Making</title>
      <link>https://arxiv.org/abs/2510.03446</link>
      <description>arXiv:2510.03446v1 Announce Type: new 
Abstract: Game theory has traditionally had a relatively limited view of risk based on how a player's expected reward is impacted by the uncertainty of the actions of other players. Recently, a new game-theoretic approach provides a more holistic view of risk also considering the reward-variance. However, these variance-based approaches measure variance of the reward on both the upside and downside. In many domains, such as finance, downside risk only is of key importance, as this represents the potential losses associated with a decision. In contrast, large upside "risk" (e.g. profits) are not an issue. To address this restrictive view of risk, we propose a novel solution concept, downside risk aware equilibria (DRAE) based on lower partial moments. DRAE restricts downside risk, while placing no restrictions on upside risk, and additionally, models higher-order risk preferences. We demonstrate the applicability of DRAE on several games, successfully finding equilibria which balance downside risk with expected reward, and prove the existence and optimality of this equilibria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03446v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver Slumbers, Benjamin Patrick Evans, Sumitra Ganesh, Leo Ardon</dc:creator>
    </item>
    <item>
      <title>On the $O(1/T)$ Convergence of Alternating Gradient Descent-Ascent in Bilinear Games</title>
      <link>https://arxiv.org/abs/2510.03855</link>
      <description>arXiv:2510.03855v1 Announce Type: new 
Abstract: We study the alternating gradient descent-ascent (AltGDA) algorithm in two-player zero-sum games. Alternating methods, where players take turns to update their strategies, have long been recognized as simple and practical approaches for learning in games, exhibiting much better numerical performance than their simultaneous counterparts. However, our theoretical understanding of alternating algorithms remains limited, and results are mostly restricted to the unconstrained setting. We show that for two-player zero-sum games that admit an interior Nash equilibrium, AltGDA converges at an $O(1/T)$ ergodic convergence rate when employing a small constant stepsize. This is the first result showing that alternation improves over the simultaneous counterpart of GDA in the constrained setting. For games without an interior equilibrium, we show an $O(1/T)$ local convergence rate with a constant stepsize that is independent of any game-specific constants. In a more general setting, we develop a performance estimation programming (PEP) framework to jointly optimize the AltGDA stepsize along with its worst-case convergence rate. The PEP results indicate that AltGDA may achieve an $O(1/T)$ convergence rate for a finite horizon $T$, whereas its simultaneous counterpart appears limited to an $O(1/\sqrt{T})$ rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03855v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianlong Nan, Shuvomoy Das Gupta, Garud Iyengar, Christian Kroer</dc:creator>
    </item>
    <item>
      <title>Robust Optimality of Bundling Goods Beyond Finite Variance</title>
      <link>https://arxiv.org/abs/2510.04343</link>
      <description>arXiv:2510.04343v1 Announce Type: new 
Abstract: When selling many goods with independent valuations, we develop a distributionally robust framework, consisting of a two-player game between seller and nature. The seller has only limited knowledge about the value distribution. The seller selects a revenue-maximizing mechanism, after which nature chooses a revenue-minimizing distribution from all distributions that comply with the limited knowledge. When the seller knows the mean and variance of valuations, bundling is known to be an asymptotically optimal deterministic mechanism, achieving a normalized revenue close to the mean. Moving beyond this variance assumption, we assume knowledge of the mean absolute deviation (MAD), accommodating more dispersion and heavy-tailed valuations with infinite variance. We show for a large range of MAD values that bundling remains optimal, but the seller can only guarantee a revenue strictly smaller than the mean. Another noteworthy finding is indifference to the order of play, as both the max-min and min-max versions of the problem yield identical values. This contrasts with deterministic mechanisms and the separate sale of goods, where the order of play significantly impacts outcomes. We further underscore the universality of the optimal bundling price by demonstrating its efficacy in optimizing not only absolute revenue but also the absolute regret and ratio objective among all bundling prices</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04343v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim S. G. van Eck, Pieter Kleer, Johan S. H. van Leeuwaarden</dc:creator>
    </item>
    <item>
      <title>Scale-Invariant Regret Matching and Online Learning with Optimal Convergence: Bridging Theory and Practice in Zero-Sum Games</title>
      <link>https://arxiv.org/abs/2510.04407</link>
      <description>arXiv:2510.04407v1 Announce Type: new 
Abstract: A considerable chasm has been looming for decades between theory and practice in zero-sum game solving through first-order methods. Although a convergence rate of $T^{-1}$ has long been established since Nemirovski's mirror-prox algorithm and Nesterov's excessive gap technique in the early 2000s, the most effective paradigm in practice is *counterfactual regret minimization*, which is based on *regret matching* and its modern variants. In particular, the state of the art across most benchmarks is *predictive* regret matching$^+$ (PRM$^+$), in conjunction with non-uniform averaging. Yet, such algorithms can exhibit slower $\Omega(T^{-1/2})$ convergence even in self-play.
  In this paper, we close the gap between theory and practice. We propose a new scale-invariant and parameter-free variant of PRM$^+$, which we call IREG-PRM$^+$. We show that it achieves $T^{-1/2}$ best-iterate and $T^{-1}$ (i.e., optimal) average-iterate convergence guarantees, while also being on par with PRM$^+$ on benchmark games. From a technical standpoint, we draw an analogy between IREG-PRM$^+$ and optimistic gradient descent with *adaptive* learning rate. The basic flaw of PRM$^+$ is that the ($\ell_2$-)norm of the regret vector -- which can be thought of as the inverse of the learning rate -- can decrease. By contrast, we design IREG-PRM$^+$ so as to maintain the invariance that the norm of the regret vector is nondecreasing. This enables us to derive an RVU-type bound for IREG-PRM$^+$, the first such property that does not rely on introducing additional hyperparameters to enforce smoothness.
  Furthermore, we find that IREG-PRM$^+$ performs on par with an adaptive version of optimistic gradient descent that we introduce whose learning rate depends on the misprediction error, demystifying the effectiveness of the regret matching family *vis-a-vis* more standard optimization techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04407v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Hu Zhang, Ioannis Anagnostides, Tuomas Sandholm</dc:creator>
    </item>
    <item>
      <title>Bin Packing and Covering: Pushing the Frontier on the Maximin Share Fairness</title>
      <link>https://arxiv.org/abs/2510.04425</link>
      <description>arXiv:2510.04425v1 Announce Type: new 
Abstract: We study a fundamental fair allocation problem, where the agent's value is determined by the number of bins either used to pack or cover the items allocated to them. Fairness is evaluated using the maximin share (MMS) criterion. This problem is not only motivated by practical applications, but also serves as a natural framework for studying group fairness. As MMS is not always satisfiable, we consider two types of approximations: cardinal and ordinal. For cardinal approximation, we relax the requirements of being packed or covered for a bin, and for ordinal approximation, we relax the number of bins that are packed or covered. For all models of interest, we provide constant approximation algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04425v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo Li, Ankang Sun, Zunyu Wang, Yu Zhou</dc:creator>
    </item>
    <item>
      <title>Fairness in Repeated Matching: A Maximin Perspective</title>
      <link>https://arxiv.org/abs/2510.04624</link>
      <description>arXiv:2510.04624v1 Announce Type: new 
Abstract: We study a sequential decision-making model where a set of items is repeatedly matched to the same set of agents over multiple rounds. The objective is to determine a sequence of matchings that either maximizes the utility of the least advantaged agent at the end of all rounds (optimal) or at the end of every individual round (anytime optimal). We investigate the computational challenges associated with finding (anytime) optimal outcomes and demonstrate that these problems are generally computationally intractable. However, we provide approximation algorithms, fixed-parameter tractable algorithms, and identify several special cases whereby the problem(s) can be solved efficiently. Along the way, we also establish characterizations of Pareto-optimal/maximum matchings, which may be of independent interest to works in matching theory and house allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04624v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eugene Lim, Tzeh Yuan Neoh, Nicholas Teh</dc:creator>
    </item>
    <item>
      <title>A Fixed Point Framework for the Existence of EFX Allocations</title>
      <link>https://arxiv.org/abs/2510.04915</link>
      <description>arXiv:2510.04915v1 Announce Type: new 
Abstract: We consider the problem of the existence of an envy-free allocation up to any good (EFX) for linear valuations and establish new results by connecting this problem to a fixed point framework. Specifically, we first use randomized rounding to extend the discrete EFX constraints into a continuous space and show that an EFX allocation exists if and only if the optimal value of the continuously extended objective function is nonpositive. In particular, we demonstrate that this optimization problem can be formulated as an unconstrained difference of convex (DC) program, which can be further simplified to the minimization of a piecewise linear concave function over a polytope. Leveraging this connection, we show that the proposed DC program has a nonpositive optimal objective value if and only if a well-defined continuous vector map admits a fixed point. Crucially, we prove that the reformulated fixed point problem satisfies all the conditions of Brouwer's fixed point theorem, except that self-containedness is violated by an arbitrarily small positive constant. To address this, we propose a slightly perturbed continuous map that always admits a fixed point. This fixed point serves as a proxy for the fixed point (if it exists) of the original map, and hence for an EFX allocation through an appropriate transformation. Our results offer a new approach to establishing the existence of EFX allocations through fixed point theorems. Moreover, the equivalence with DC programming enables a more efficient and systematic method for computing such allocations (if one exists) using tools from nonlinear optimization. Our findings bridge the discrete problem of finding an EFX allocation with two continuous frameworks: solving an unconstrained DC program and identifying a fixed point of a continuous vector map.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04915v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Rasoul Etesami</dc:creator>
    </item>
    <item>
      <title>The Hidden Game Problem</title>
      <link>https://arxiv.org/abs/2510.03845</link>
      <description>arXiv:2510.03845v1 Announce Type: cross 
Abstract: This paper investigates a class of games with large strategy spaces, motivated by challenges in AI alignment and language games. We introduce the hidden game problem, where for each player, an unknown subset of strategies consistently yields higher rewards compared to the rest. The central question is whether efficient regret minimization algorithms can be designed to discover and exploit such hidden structures, leading to equilibrium in these subgames while maintaining rationality in general. We answer this question affirmatively by developing a composition of regret minimization techniques that achieve optimal external and swap regret bounds. Our approach ensures rapid convergence to correlated equilibria in hidden subgames, leveraging the hidden game structure for improved computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03845v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gon Buzaglo, Noah Golowich, Elad Hazan</dc:creator>
    </item>
    <item>
      <title>RLRF: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback</title>
      <link>https://arxiv.org/abs/2510.04096</link>
      <description>arXiv:2510.04096v1 Announce Type: cross 
Abstract: Competitive search is a setting where document publishers modify them to improve their ranking in response to a query. Recently, publishers have increasingly leveraged LLMs to generate and modify competitive content. We introduce Reinforcement Learning from Ranker Feedback (RLRF), a framework that trains LLMs using preference datasets derived from ranking competitions. The goal of a publisher (LLM-based) agent is to optimize content for improved ranking while accounting for the strategies of competing agents. We generate the datasets using approaches that do not rely on human-authored data. We show that our proposed agents consistently and substantially outperform previously suggested approaches for LLM-based competitive document modification. We further show that our agents are effective with ranking functions they were not trained for (i.e., out of distribution) and they adapt to strategic opponents. These findings provide support to the significant potential of using reinforcement learning in competitive search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04096v1</guid>
      <category>cs.IR</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tommy Mordo, Sagie Dekel, Omer Madmon, Moshe Tennenholtz, Oren Kurland</dc:creator>
    </item>
    <item>
      <title>Look-ahead Reasoning with a Learned Model in Imperfect Information Games</title>
      <link>https://arxiv.org/abs/2510.05048</link>
      <description>arXiv:2510.05048v1 Announce Type: cross 
Abstract: Test-time reasoning significantly enhances pre-trained AI agents' performance. However, it requires an explicit environment model, often unavailable or overly complex in real-world scenarios. While MuZero enables effective model learning for search in perfect information games, extending this paradigm to imperfect information games presents substantial challenges due to more nuanced look-ahead reasoning techniques and large number of states relevant for individual decisions. This paper introduces an algorithm LAMIR that learns an abstracted model of an imperfect information game directly from the agent-environment interaction. During test time, this trained model is used to perform look-ahead reasoning. The learned abstraction limits the size of each subgame to a manageable size, making theoretically principled look-ahead reasoning tractable even in games where previous methods could not scale. We empirically demonstrate that with sufficient capacity, LAMIR learns the exact underlying game structure, and with limited capacity, it still learns a valuable abstraction, which improves game playing performance of the pre-trained agents even in large games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05048v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ond\v{r}ej Kub\'i\v{c}ek, Viliam Lis\'y</dc:creator>
    </item>
    <item>
      <title>Weakly-Popular and Super-Popular Matchings with Ties and Their Connection to Stable Matchings</title>
      <link>https://arxiv.org/abs/2310.12269</link>
      <description>arXiv:2310.12269v3 Announce Type: replace 
Abstract: In this paper, we study a slightly different definition of popularity in bipartite graphs $G=(U,W,E)$ with two-sided preferences, when ties are present in the preference lists. This is motivated by the observation that if an agent $u$ is indifferent between his original partner $w$ in matching $M$ and his new partner $w'\ne w$ in matching $N$, then he may probably still prefer to stay with his original partner, as change requires effort, so he votes for $M$ in this case, instead of being indifferent.
  We show that this alternative definition of popularity, which we call weak-popularity allows us to guarantee the existence of such a matching and also to find a weakly-popular matching in polynomial-time that has size at least $\frac{3}{4}$ the size of the maximum weakly popular matching. We also show that this matching is at least $\frac{4}{5}$ times the size of the maximum (weakly) stable matching, so may provide a more desirable solution than the current best (and tight under certain assumptions) $\frac{2}{3}$-approximation for such a stable matching. We also show that unfortunately, finding a maximum size weakly popular matching is NP-hard, even with one-sided ties and that assuming some complexity theoretic assumptions, the $\frac{3}{4}$-approximation bound is tight.
  Then, we study a more general model than weak-popularity, where for each edge, we can specify independently for both endpoints the size of improvement the endpoint needs to vote in favor of a new matching $N$. We show that even in this more general model, a so-called $\gamma$-popular matching always exists and that the same positive results still hold.
  Finally, we define an other, stronger variant of popularity, called super-popularity, where even a weak improvement is enough to vote in favor of a new matching. We show that for this case, even the existence problem is NP-hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12269v3</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gergely Cs\'aji</dc:creator>
    </item>
    <item>
      <title>Multiplayer General Lotto game</title>
      <link>https://arxiv.org/abs/2401.14613</link>
      <description>arXiv:2401.14613v5 Announce Type: replace 
Abstract: In this paper, we investigate the multiplayer General Lotto game across multiple battlefields, a significant variant of the Colonel Blotto game. In this version, each player employs a probability distribution for resource allocation, ensuring that their expected expenditure does not exceed their budget. We first establish the existence of the Nash equilibrium in a general setting, where players' budgets are asymmetric and the values of the battlefields are heterogeneous and asymmetric among players. Next, we provide a detailed characterization of the Nash equilibrium for multiple players on a single battlefield. In this characterization, we observe that the upper endpoints of the supports of players' equilibrium strategies coincide, and that the minimum value of a player's support above zero inversely correlates with his budget. We demonstrate the uniqueness of Nash equilibrium over a single battlefield in some scenarios. In the multi-battlefield setting, we prove that there is an upper bound on the average number of battlefields each player participates in. Additionally, we provide an example demonstrating the non-uniqueness of the Nash equilibrium in the context of multiple battlefields with multiple players. Finally, we present a solution for the Nash equilibrium in a symmetric case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14613v5</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Liu, Bonan Ni, Weiran Shen, Zihe Wang, Jie Zhang</dc:creator>
    </item>
    <item>
      <title>Algorithmic pricing with independent learners and relative experience replay</title>
      <link>https://arxiv.org/abs/2102.09139</link>
      <description>arXiv:2102.09139v3 Announce Type: replace-cross 
Abstract: In an infinitely repeated general-sum pricing game, independent reinforcement learners may exhibit collusive behavior without any communication, raising concerns about algorithmic collusion. To better understand the learning dynamics, we incorporate agents' relative performance (RP) among competitors using experience replay (ER) techniques. Experimental results indicate that RP considerations play a critical role in long-run outcomes. Agents that are averse to underperformance converge to the Bertrand-Nash equilibrium, while those more tolerant of underperformance tend to charge supra-competitive prices. This finding also helps mitigate the overfitting issue in independent Q-learning. Additionally, the impact of relative ER varies with the number of agents and the choice of algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.09139v3</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bingyan Han</dc:creator>
    </item>
    <item>
      <title>Learning Safe Strategies for Value Maximizing Buyers in Uniform Price Auctions</title>
      <link>https://arxiv.org/abs/2406.03674</link>
      <description>arXiv:2406.03674v4 Announce Type: replace-cross 
Abstract: We study the bidding problem in repeated uniform price multi-unit auctions from the perspective of a value-maximizing buyer. The buyer aims to maximize their cumulative value over $T$ rounds while adhering to per-round return-on-investment (RoI) constraints in a strategic (or adversarial) environment. Using an $m$-uniform bidding format, the buyer submits $m$ bid-quantity pairs $(b_i, q_i)$ to demand $q_i$ units at bid $b_i$, with $m \ll M$ in practice, where $M$ denotes the maximum demand of the buyer.
  We introduce the notion of safe bidding strategies as those that satisfy the RoI constraints irrespective of competing bids. Despite the stringent requirement, we show that these strategies satisfy a mild no-overbidding condition, depend only on the valuation curve of the bidder, and the bidder can focus on a finite subset without loss of generality. Though the subset size is $O(M^m)$, we design a polynomial-time learning algorithm that achieves sublinear regret, both in full-information and bandit settings, relative to the hindsight-optimal safe strategy.
  We assess the robustness of safe strategies against the hindsight-optimal strategy from a richer class. We define the richness ratio $\alpha \in (0,1]$ as the minimum ratio of the value of the optimal safe strategy to that of the optimal strategy from richer class and construct hard instances showing the tightness of $\alpha$. Our algorithm achieves $\alpha$-approximate sublinear regret against these stronger benchmarks. Simulations on semi-synthetic auction data show that empirical richness ratios significantly outperform the theoretical worst-case bounds. The proposed safe strategies and learning algorithm extend naturally to more nuanced buyer and competitor models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03674v4</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Negin Golrezaei, Sourav Sahoo</dc:creator>
    </item>
    <item>
      <title>Extending Stable and Popular Matching Algorithms from Bipartite to Arbitrary Instances</title>
      <link>https://arxiv.org/abs/2409.16173</link>
      <description>arXiv:2409.16173v3 Announce Type: replace-cross 
Abstract: We consider stable and popular matching problems in arbitrary graphs, which are referred to as stable roommates instances. We extend the 3/2-approximation algorithm for the maximum size weakly stable matching problem to the roommates case, which solves a more than 20 year old open question of Irving and Manlove about the approximability of maximum size weakly stable matchings in roommates instances with ties [Irving and Manlove 2002] and has nice applications for the problem of matching residents to hospitals in the presence of couples. We also extend the algorithm that finds a maximum size popular matching in bipartite graphs in the case of strict preferences and the algorithm to find a popular matching among maximum weight matchings. While previous attempts to extend the idea of promoting the agents or duplicating the edges from bipartite instances to arbitrary ones failed, these results show that with the help of a simple observation, we can indeed bridge the gap and extend these algorithms</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16173v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gergely Cs\'aji</dc:creator>
    </item>
    <item>
      <title>The Hive Mind is a Single Reinforcement Learning Agent</title>
      <link>https://arxiv.org/abs/2410.17517</link>
      <description>arXiv:2410.17517v4 Announce Type: replace-cross 
Abstract: Decision-making is an essential attribute of any intelligent agent or group. Natural systems are known to converge to optimal strategies through at least two distinct mechanisms: collective decision-making via imitation of others, and individual trial-and-error. This paper establishes an equivalence between these two paradigms by drawing from the well-established collective decision-making model of nest-hunting in swarms of honey bees. We show that the emergent distributed cognition (sometimes referred to as the $\textit{hive mind}$) arising from individual bees following simple, local imitation-based rules is that of a single online reinforcement learning (RL) agent interacting with many parallel environments. The update rule through which this macro-agent learns is a bandit algorithm that we coin $\textit{Maynard-Cross Learning}$. Our analysis implies that a group of cognition-limited organisms can be equivalent to a more complex, reinforcement-enabled entity, substantiating the idea that group-level intelligence may explain how seemingly simple and blind individual behaviors are selected in nature.
  From a biological perspective, this analysis suggests how such imitation strategies evolved: they constitute a scalable form of reinforcement learning at the group level, aligning with theories of kin and group selection. Beyond biology, the framework offers new tools for analyzing economic and social systems where individuals imitate successful strategies, effectively participating in a collective learning process. In swarm intelligence, our findings will inform the design of scalable collective systems in artificial domains, enabling RL-inspired mechanisms for coordination and adaptability at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17517v4</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik Soma, Yann Bouteiller, Heiko Hamann, Giovanni Beltrame</dc:creator>
    </item>
    <item>
      <title>Learning to Bid in Non-Stationary Repeated First-Price Auctions</title>
      <link>https://arxiv.org/abs/2501.13358</link>
      <description>arXiv:2501.13358v3 Announce Type: replace-cross 
Abstract: First-price auctions have recently gained significant traction in digital advertising markets, exemplified by Google's transition from second-price to first-price auctions. Unlike in second-price auctions, where bidding one's private valuation is a dominant strategy, determining an optimal bidding strategy in first-price auctions is more complex. From a learning perspective, the learner (a specific bidder) can interact with the environment (other bidders, i.e., opponents) sequentially to infer their behaviors. Existing research often assumes specific environmental conditions and benchmarks performance against the best fixed policy (static benchmark). While this approach ensures strong learning guarantees, the static benchmark can deviate significantly from the optimal strategy in environments with even mild non-stationarity. To address such scenarios, a dynamic benchmark--representing the sum of the highest achievable rewards at each time step--offers a more suitable objective. However, achieving no-regret learning with respect to the dynamic benchmark requires additional constraints. By inspecting reward functions in online first-price auctions, we introduce two metrics to quantify the regularity of the sequence of opponents' highest bids, which serve as measures of non-stationarity. We provide a minimax-optimal characterization of the dynamic regret for the class of sequences of opponents' highest bids that satisfy either of these regularity constraints. Our main technical tool is the Optimistic Mirror Descent (OMD) framework with a novel optimism configuration, which is well-suited for achieving minimax-optimal dynamic regret rates in this context. We then use synthetic datasets to validate our theoretical guarantees and demonstrate that our methods outperform existing ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13358v3</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihao Hu, Xiaoyu Fan, Yuan Yao, Jiheng Zhang, Zhengyuan Zhou</dc:creator>
    </item>
    <item>
      <title>Learning Closed-Loop Parametric Nash Equilibria of Multi-Agent Collaborative Field Coverage</title>
      <link>https://arxiv.org/abs/2503.11829</link>
      <description>arXiv:2503.11829v2 Announce Type: replace-cross 
Abstract: Multi-agent reinforcement learning is a challenging and active field of research due to the inherent nonstationary property and coupling between agents. A popular approach to modeling the multi-agent interactions underlying the multi-agent RL problem is the Markov Game. There is a special type of Markov Game, termed Markov Potential Game, which allows us to reduce the Markov Game to a single-objective optimal control problem where the objective function is a potential function. In this work, we prove that a multi-agent collaborative field coverage problem, which is found in many engineering applications, can be formulated as a Markov Potential Game, and we can learn a parameterized closed-loop Nash Equilibrium by solving an equivalent single-objective optimal control problem. As a result, our algorithm is 10x faster during training compared to a game-theoretic baseline and converges faster during policy execution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11829v2</guid>
      <category>cs.MA</category>
      <category>cs.GT</category>
      <category>cs.RO</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jushan Chen, Santiago Paternain</dc:creator>
    </item>
  </channel>
</rss>
