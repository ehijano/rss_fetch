<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Feb 2026 05:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>MPL-HMC: A Tunable Parameterized Leapfrog Framework for Robust Hamiltonian Monte Carlo</title>
      <link>https://arxiv.org/abs/2602.14061</link>
      <description>arXiv:2602.14061v1 Announce Type: new 
Abstract: This article introduces the Modified Parameterized Leapfrog Hamiltonian Monte Carlo (MPL-HMC) method, a novel extension of HMC addressing key limitations through tunable integration parameters $\alpha(\delta t)$ and $\beta(\delta t)$, enabling controlled perturbations to Hamiltonian dynamics. Theoretical analysis demonstrates MPL-HMC maintains approximate detailed balance. Extensive empirical evaluation reveals systematic performance improvements. The damping variant ($\alpha_2=-0.1$, $\beta_2=-0.05$) achieves a 14-fold increase in effective sample size for Neal's funnel and 27\% better efficiency for pharmacokinetic models. The anti-damping variant ($\alpha_2=0.1$, $\beta_2=0.05$) achieves $\hat{R}=1.026$ for Bayesian neural networks versus $\hat{R}=1.981$ for standard HMC. We introduce aggressive MPL-HMC for multimodal distributions, employing extreme parameters ($\alpha_2=8.0$--$15.0$, $\beta_2=5.0$--$8.0$) with enhanced sampling to achieve full mode exploration where standard methods fail. All variants maintain computational efficiency identical to standard HMC while providing systematic control over damping, exploration, stability, and accuracy. The article provides rigorous mathematical foundations, implementation specifications, parameter tuning strategies, and comprehensive performance comparisons, extending HMC's applicability to previously challenging domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14061v1</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourabh Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Fast Compute for ML Optimization</title>
      <link>https://arxiv.org/abs/2602.14280</link>
      <description>arXiv:2602.14280v1 Announce Type: new 
Abstract: We study optimization for losses that admit a variance-mean scale-mixture representation. Under this representation, each EM iteration is a weighted least squares update in which latent variables determine observation and parameter weights; these play roles analogous to Adam's second-moment scaling and AdamW's weight decay, but are derived from the model. The resulting Scale Mixture EM (SM-EM) algorithm removes user-specified learning-rate and momentum schedules. On synthetic ill-conditioned logistic regression benchmarks with $p \in \{20, \ldots, 500\}$, SM-EM with Nesterov acceleration attains up to $13\times$ lower final loss than Adam tuned by learning-rate grid search. For a 40-point regularization path, sharing sufficient statistics across penalty values yields a $10\times$ runtime reduction relative to the same tuned-Adam protocol. For the base (non-accelerated) algorithm, EM monotonicity guarantees nonincreasing objective values; adding Nesterov extrapolation trades this guarantee for faster empirical convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14280v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Polson, Vadim Sokolov</dc:creator>
    </item>
    <item>
      <title>Higher-Order Hit-&amp;-Run Samplers for Linearly Constrained Densities</title>
      <link>https://arxiv.org/abs/2602.14616</link>
      <description>arXiv:2602.14616v1 Announce Type: new 
Abstract: Markov chain Monte Carlo (MCMC) sampling of densities restricted to linearly constrained domains is an important task arising in Bayesian treatment of inverse problems in the natural sciences. While efficient algorithms for uniform polytope sampling exist, much less work has dealt with more complex constrained densities. In particular, gradient information as used in unconstrained MCMC is not necessarily helpful in the constrained case, where the gradient may push the proposal's density out of the polytope. In this work, we propose a novel constrained sampling algorithm, which combines strengths of higher-order information, like the target's log-density's gradients and curvature, with the Hit-&amp;-Run proposal, a simple mechanism which guarantees the generation of feasible proposals, fulfilling the linear constraints. Our extensive experiments demonstrate improved sampling efficiency on complex constrained densities over various constrained and unconstrained samplers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14616v1</guid>
      <category>stat.CO</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Richard D. Paul, Anton Stratmann, Johann F. Jadebeck, Martin Bey{\ss}, Hanno Scharr, David R\"ugamer, Katharina N\"oh</dc:creator>
    </item>
    <item>
      <title>Weak Poincar\'{e} inequalities for Deterministic-scan Metropolis-within-Gibbs samplers</title>
      <link>https://arxiv.org/abs/2602.14692</link>
      <description>arXiv:2602.14692v1 Announce Type: new 
Abstract: Using the framework of weak Poincar\'{e} inequalities, we analyze the convergence properties of deterministic-scan Metropolis-within-Gibbs samplers, an important class of Markov chain Monte Carlo algorithms. Our analysis applies to nonreversible Markov chains and yields explicit (subgeometric) convergence bounds through novel comparison techniques based on Dirichlet forms. We show that the joint chain inherits the convergence behavior of the marginal chain and conversely. In addition, we establish several fundamental results for weak Poincar\'{e} inequalities for discrete-time Markov chains, such as a tensorization property for independent chains. We apply our theoretical results through applications to algorithms for Bayesian inference for a hierarchical regression model and a diffusion model under discretely-observed data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14692v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengxi Gao, Gareth O. Roberts, Andi Q. Wang</dc:creator>
    </item>
    <item>
      <title>Mixture-of-experts Wishart model for covariance matrices with an application to Cancer drug screening</title>
      <link>https://arxiv.org/abs/2602.13888</link>
      <description>arXiv:2602.13888v1 Announce Type: cross 
Abstract: Covariance matrices arise naturally in different scientific fields, including finance, genomics, and neuroscience, where they encode dependence structures and reveal essential features of complex multivariate systems. In this work, we introduce a comprehensive Bayesian framework for analyzing heterogeneous covariance data through both classical mixture models and a novel mixture-of-experts Wishart (MoE-Wishart) model. The proposed MoE-Wishart model extends standard Wishart mixtures by allowing mixture weights to depend on predictors through a multinomial logistic gating network. This formulation enables the model to capture complex, nonlinear heterogeneity in covariance structures and to adapt subpopulation membership probabilities to covariate-dependent patterns. To perform inference, we develop an efficient Gibbs-within-Metropolis-Hastings sampling algorithm tailored to the geometry of the Wishart likelihood and the gating network. We additionally derive an Expectation-Maximization algorithm for maximum likelihood estimation in the mixture-of-experts setting. Extensive simulation studies demonstrate that the proposed Bayesian and maximum likelihood estimators achieve accurate subpopulation recovery and estimation under a range of heterogeneous covariance scenarios. Finally, we present an innovative application of our methodology to a challenging dataset: cancer drug sensitivity profiles, illustrating the ability of the MoE-Wishart model to leverage covariance across drug dosages and replicate measurements.
  Our methods are implemented in the \texttt{R} package \texttt{moewishart} available at https://github.com/zhizuio/moewishart .</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13888v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>The Tien Mai, Zhi Zhao</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach to Low-Discrepancy Subset Selection</title>
      <link>https://arxiv.org/abs/2602.14607</link>
      <description>arXiv:2602.14607v1 Announce Type: cross 
Abstract: Low-discrepancy designs play a central role in quasi-Monte Carlo methods and are increasingly influential in other domains such as machine learning, robotics and computer graphics, to name a few. In recent years, one such low-discrepancy construction method called subset selection has received a lot of attention. Given a large population, one optimally selects a small low-discrepancy subset with respect to a discrepancy-based objective. Versions of this problem are known to be NP-hard. In this text, we establish, for the first time, that the subset selection problem with respect to kernel discrepancies is also NP-hard. Motivated by this intractability, we propose a Bayesian Optimization procedure for the subset selection problem utilizing the recent notion of deep embedding kernels. We demonstrate the performance of the BO algorithm to minimize discrepancy measures and note that the framework is broadly applicable any design criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14607v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Kirk</dc:creator>
    </item>
    <item>
      <title>NISQ-compatible quantum cryptography based on Parrondo dynamics in discrete-time quantum walks</title>
      <link>https://arxiv.org/abs/2602.14678</link>
      <description>arXiv:2602.14678v1 Announce Type: cross 
Abstract: Compatibility with noisy intermediate-scale quantum (NISQ) devices is crucial for the realistic implementation of quantum cryptographic protocols. We investigate a cryptographic scheme based on discrete-time quantum walks (DTQWs) on cyclic graphs that exploits Parrondo dynamics, wherein periodic evolution emerges from a deterministic sequence of individually chaotic coin operators. We construct an explicit quantum circuit realization tailored to NISQ architectures and analyze its performance through numerical simulations in Qiskit under both ideal and noisy conditions. Protocol performance is quantified using probability distributions, Hellinger fidelity, and total variation distance. To assess security at the circuit level, we model intercept-resend and man-in-the-middle attacks and evaluate the resulting quantum bit error rate. In the absence of adversarial intervention, the protocol enables reliable message recovery, whereas eavesdropping induces characteristic disturbances that disrupt the periodic reconstruction mechanism. We further examine hardware feasibility on contemporary NISQ processors, specifically $ibm\_torino$, incorporating qubit connectivity and state-transfer constraints into the circuit design. Our analysis demonstrates that communication between spatially separated logical modules increases circuit depth via SWAP operations, leading to cumulative noise effects. By exploring hybrid state-transfer strategies, we show that qubit selection and connectivity play a decisive role in determining fidelity and overall protocol performance, highlighting hardware-dependent trade-offs in NISQ implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14678v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aditi Rath, Dinesh Kumar Panda, Colin Benjamin</dc:creator>
    </item>
    <item>
      <title>Block Empirical Likelihood Inference for Longitudinal Generalized Partially Linear Single-Index Models</title>
      <link>https://arxiv.org/abs/2602.14981</link>
      <description>arXiv:2602.14981v1 Announce Type: cross 
Abstract: Generalized partially linear single-index models (GPLSIMs) provide a flexible and interpretable semiparametric framework for longitudinal outcomes by combining a low-dimensional parametric component with a nonparametric index component. For repeated measurements, valid inference is challenging because within-subject correlation induces nuisance parameters and variance estimation can be unstable in semiparametric settings. We propose a profile estimating-equation approach based on spline approximation of the unknown link function and construct a subject-level block empirical likelihood (BEL) for joint inference on the parametric coefficients and the single-index direction. The resulting BEL ratio statistic enjoys a Wilks-type chi-square limit, yielding likelihood-free confidence regions without explicit sandwich variance estimation. We also discuss practical implementation, including constrained optimization for the index parameter, working-correlation choices, and bootstrap-based confidence bands for the nonparametric component. Simulation studies and an application to the epilepsy longitudinal study illustrate the finite-sample performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14981v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianni Zhang, Yuyao Wang, Yu Lu, and Mengfei Ran</dc:creator>
    </item>
    <item>
      <title>An ILUES-based adaptive Gaussian process method for multimodal Bayesian inverse problems</title>
      <link>https://arxiv.org/abs/2409.15307</link>
      <description>arXiv:2409.15307v2 Announce Type: replace 
Abstract: Inverse problems are prevalent in both scientific research and engineering applications. In the context of Bayesian inverse problems, sampling from the posterior distribution can be particularly challenging when the forward models are computationally expensive. This challenge is further compounded when the posterior distribution is multimodal. To address this issue, we propose a Gaussian process (GP)-based method to indirectly build surrogates for the forward model. Specifically, the unnormalized posterior density is expressed as a product of an auxiliary density and an exponential GP surrogate. Iteratively, the auxiliary density converges to the posterior distribution, starting from an arbitrary initial density. However, the efficiency of GP regression is highly influenced by the quality of the training data. Therefore, we utilize the iterative local updating ensemble smoother (ILUES) to generate high-quality samples that are concentrated in regions with high posterior probability. Subsequently, based on the surrogate model and mode information extracted using a clustering method, Markov chain Monte Carlo (MCMC) with a Gaussian mixed (GM) proposal is used to draw samples from the auxiliary density. Through numerical examples, we demonstrate that the proposed method can accurately and efficiently represent the posterior with a limited number of forward simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15307v2</guid>
      <category>stat.CO</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhihang Xu, Xiaoyu Zhu, Daoji Li, Qifeng Liao</dc:creator>
    </item>
    <item>
      <title>Interpretable contour level selection for heat maps for gridded data</title>
      <link>https://arxiv.org/abs/2505.16788</link>
      <description>arXiv:2505.16788v2 Announce Type: replace 
Abstract: Gridded data formats, where the observed multivariate data are aggregated into grid cells, ensure confidentiality and reduce storage requirements, with the trade-off that access to the underlying point data is lost. Heat maps are a highly pertinent visualisation for gridded data, and heat maps with a small number of well-selected contour levels offer improved interpretability over continuous contour levels. There are many possible contour level choices. Amongst them, density contour levels are highly suitable in many cases. Current methods for computing density contour levels requires access to the observed point data, so they are not applicable to gridded data. To remedy this, we introduce an approximation of density contour levels for gridded data. We then compare our proposed method to existing contour level selection methods, and conclude that our proposal provides improved interpretability for synthetic and experimental gridded data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16788v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarn Duong</dc:creator>
    </item>
    <item>
      <title>Are Statistical Methods Obsolete in the Era of Deep Learning? A Study of ODE Inverse Problems</title>
      <link>https://arxiv.org/abs/2505.21723</link>
      <description>arXiv:2505.21723v2 Announce Type: replace 
Abstract: In the era of AI, neural networks have become increasingly popular for modeling, inference, and prediction, largely due to their potential for universal approximation. With the proliferation of such deep learning models, a question arises: are leaner statistical methods still relevant? To shed insight on this question, we employ the mechanistic nonlinear ordinary differential equation (ODE) inverse problem as a testbed, using the physics-informed neural network (PINN) as a representative of the deep learning paradigm and manifold-constrained Gaussian process inference (MAGI) as a representative of statistically principled methods. Through case studies involving the SEIR model from epidemiology and the Lorenz model from chaotic dynamics, we demonstrate that statistical methods are far from obsolete, especially when working with sparse and noisy observations. On tasks such as parameter inference and trajectory reconstruction, statistically principled methods consistently achieve lower bias and variance, while using far fewer parameters and requiring less hyperparameter tuning. Statistical methods can also decisively outperform deep learning models on out-of-sample future prediction, where the absence of relevant data often leads overparameterized models astray. Additionally, we find that statistically principled approaches are more robust to accumulation of numerical imprecision and can represent the underlying system more faithfully to the true governing ODEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21723v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Skyler Wu, Shihao Yang, S. C. Kou</dc:creator>
    </item>
    <item>
      <title>Variational Transdimensional Inference</title>
      <link>https://arxiv.org/abs/2506.04749</link>
      <description>arXiv:2506.04749v3 Announce Type: replace 
Abstract: The expressiveness of flow-based models combined with stochastic variational inference (SVI) has expanded the application of optimization-based Bayesian inference to highly complex problems. However, despite the importance of multi-model Bayesian inference for problems defined on a transdimensional joint model and parameter space, such as Bayesian structure learning and model selection, flow-based SVI has been limited to problems defined on a fixed-dimensional parameter space. We introduce CoSMIC, normalizing flows (COntextually-Specified Masking for Identity-mapped Components), an extension to neural autoregressive conditional normalizing flow architectures that enables use of a single flow-based variational density for inference over a transdimensional (multi-model) conditional target distribution. We propose a combined stochastic variational transdimensional inference (VTI) approach to training CoSMIC, flows using ideas from Bayesian optimization and Monte Carlo gradient estimation. Numerical experiments show the performance of VTI on challenging problems that scale to high-cardinality model spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04749v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Laurence Davies, Dan Mackinlay, Rafael Oliveira, Scott A. Sisson</dc:creator>
    </item>
    <item>
      <title>VCDF: A Validated Consensus-Driven Framework for Time Series Causal Discovery</title>
      <link>https://arxiv.org/abs/2410.19412</link>
      <description>arXiv:2410.19412v2 Announce Type: replace-cross 
Abstract: Time series causal discovery is essential for understanding dynamic systems, yet many existing methods remain sensitive to noise, non-stationarity, and sampling variability. We propose the Validated Consensus-Driven Framework (VCDF), a simple and method-agnostic layer that improves robustness by evaluating the stability of causal relations across blocked temporal subsets. VCDF requires no modification to base algorithms and can be applied to methods such as VAR-LiNGAM and PCMCI. Experiments on synthetic datasets show that VCDF improves VAR-LiNGAM by approximately 0.08-0.12 in both window and summary F1 scores across diverse data characteristics, with gains most pronounced for moderate-to-long sequences. The framework also benefits from longer sequences, yielding up to 0.18 absolute improvement on time series of length 1000 and above. Evaluations on simulated fMRI data and IT-monitoring scenarios further demonstrate enhanced stability and structural accuracy under realistic noise conditions. VCDF provides an effective reliability layer for time series causal discovery without altering underlying modeling assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19412v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gene Yu, Ce Guo, Wayne Luk</dc:creator>
    </item>
    <item>
      <title>TabMGP: Martingale posterior with TabPFN</title>
      <link>https://arxiv.org/abs/2510.25154</link>
      <description>arXiv:2510.25154v2 Announce Type: replace-cross 
Abstract: Bayesian inference provides principled uncertainty quantification but is often limited by challenges of prior and likelihood elicitation. The martingale posterior (MGP) (Fong et al., 2023) offers an alternative by replacing these requirements with a predictive rule. Additionally MGP focuses inference on parameters defined through a loss function. This framework is especially resonant in the era of foundation transformers; practitioners increasingly leverage models like TabPFN for their state-of-the-art capabilities, yet often require epistemic uncertainty for a scientific estimand $\theta$ that need not parameterise the model's implicit latent model. The MGP provides the mechanism to recover these posterior distributions. We introduce TabMGP, an MGP built on TabPFN for tabular data. TabMGP produces credible sets with near-nominal coverage and often outperforms both handcrafted MGP constructions and standard Bayesian baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25154v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenyon Ng, Edwin Fong, David T. Frazier, Jeremias Knoblauch, Susan Wei</dc:creator>
    </item>
    <item>
      <title>Information-theoretic minimax and submodular optimization algorithms for multivariate Markov chains</title>
      <link>https://arxiv.org/abs/2511.00769</link>
      <description>arXiv:2511.00769v2 Announce Type: replace-cross 
Abstract: We study an information-theoretic minimax problem for finite multivariate Markov chains on $d$-dimensional product state spaces. Given a family $\mathcal B=\{P_1,\ldots,P_n\}$ of $\pi$-stationary transition matrices and a class $\mathcal F = \mathcal{F}(\mathbf{S})$ of factorizable models induced by a partition $\mathbf S$ of the coordinate set $[d]$, we seek to minimize the worst-case information loss by analyzing $$\min_{Q\in\mathcal F}\max_{P\in\mathcal B} D_{\mathrm{KL}}^{\pi}(P\|Q),$$ where $D_{\mathrm{KL}}^{\pi}(P\|Q)$ is the $\pi$-weighted KL divergence from $Q$ to $P$. We recast the above minimax problem into concave maximization over the $n$-probability-simplex via strong duality and Pythagorean identities that we derive. This leads us to formulate an information-theoretic game and show that a mixed strategy Nash equilibrium always exists; and propose a projected subgradient algorithm to approximately solve the minimax problem with provable guarantee. By transforming the minimax problem into an orthant submodular function in $\mathbf{S}$, this motivates us to consider a max-min-max submodular optimization problem and investigate a two-layer subgradient-greedy procedure to approximately solve this generalization. Numerical experiments for Markov chains on the Curie-Weiss and Bernoulli-Laplace models illustrate the practicality of these proposed algorithms and reveals sparse optimal structures in these examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00769v2</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheyuan Lai, Michael C. H. Choi</dc:creator>
    </item>
    <item>
      <title>Optimization and Regularization Under Arbitrary Objectives</title>
      <link>https://arxiv.org/abs/2511.19628</link>
      <description>arXiv:2511.19628v2 Announce Type: replace-cross 
Abstract: This study investigates the limitations of applying Markov Chain Monte Carlo (MCMC) methods to arbitrary objective functions, focusing on a two-block MCMC framework which alternates between Metropolis-Hastings and Gibbs sampling. While such approaches are often considered advantageous for enabling data-driven regularization, we show that their performance critically depends on the sharpness of the employed likelihood form. By introducing a sharpness parameter and exploring alternative likelihood formulations proportional to the target objective function, we demonstrate how likelihood curvature governs both in-sample performance and the degree of regularization inferred by the training data. Empirical applications are conducted on reinforcement learning tasks: including a navigation problem and the game of tic-tac-toe. The study concludes with a separate analysis examining the implications of extreme likelihood sharpness on arbitrary objective functions stemming from the classic game of blackjack, where the first block of the two-block MCMC framework is replaced with an iterative optimization step. The resulting hybrid approach achieves performance nearly identical to the original MCMC framework, indicating that excessive likelihood sharpness effectively collapses posterior mass onto a single dominant mode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19628v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared N. Lakhani, Etienne Pienaar</dc:creator>
    </item>
    <item>
      <title>Ergodicity of an Adaptive MCMC Sampler under a Probability Bound</title>
      <link>https://arxiv.org/abs/2602.06568</link>
      <description>arXiv:2602.06568v2 Announce Type: replace-cross 
Abstract: This paper provides sufficient conditions over the sequence of samples and parameters of an adaptive Markov Chain Monte Carlo (MCMC) algorithm to ensure ergodicity with respect to a target distribution that can have unbounded support. These conditions aim to make more easily usable the conditions of Containment and Diminishing Adaptation from Roberts and Rosenthal [2007] formulated over the transition kernels, without needing, as was done in other works, an artificial assumption of the compactness over both sample and parameter spaces. The paper shows that the condition of compactness can be relaxed to a more realistic bound in probability over the sequence of both samples and parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06568v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Chotard (LISIC)</dc:creator>
    </item>
  </channel>
</rss>
