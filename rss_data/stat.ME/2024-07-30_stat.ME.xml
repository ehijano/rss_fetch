<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 31 Jul 2024 01:50:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multimodal data integration and cross-modal querying via orchestrated approximate message passing</title>
      <link>https://arxiv.org/abs/2407.19030</link>
      <description>arXiv:2407.19030v1 Announce Type: new 
Abstract: The need for multimodal data integration arises naturally when multiple complementary sets of features are measured on the same sample. Under a dependent multifactor model, we develop a fully data-driven orchestrated approximate message passing algorithm for integrating information across these feature sets to achieve statistically optimal signal recovery. In practice, these reference data sets are often queried later by new subjects that are only partially observed. Leveraging on asymptotic normality of estimates generated by our data integration method, we further develop an asymptotically valid prediction set for the latent representation of any such query subject. We demonstrate the prowess of both the data integration and the prediction set construction algorithms on a tri-modal single-cell dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19030v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sagnik Nandy, Zongming Ma</dc:creator>
    </item>
    <item>
      <title>Partial Identification of the Average Treatment Effect with Stochastic Counterfactuals and Discordant Twins</title>
      <link>https://arxiv.org/abs/2407.19057</link>
      <description>arXiv:2407.19057v1 Announce Type: new 
Abstract: We develop a novel approach to partially identify causal estimands, such as the average treatment effect (ATE), from observational data. To better satisfy the stable unit treatment value assumption (SUTVA) we utilize stochastic counterfactuals within a propensity-prognosis model of the data generating process. For more precise identification we utilize knowledge of discordant twin outcomes as evidence for randomness in the data generating process. Our approach culminates with a constrained optimization problem; the solution gives upper and lower bounds for the ATE. We demonstrate the applicability of our introduced methodology with three example applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19057v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian Knaeble, Braxton Osting, Placede Tshiaba</dc:creator>
    </item>
    <item>
      <title>Bayesian Mapping of Mortality Clusters</title>
      <link>https://arxiv.org/abs/2407.19135</link>
      <description>arXiv:2407.19135v1 Announce Type: new 
Abstract: Disease mapping analyses the distribution of several diseases within a territory. Primary goals include identifying areas with unexpected changes in mortality rates, studying the relation among multiple diseases, and dividing the analysed territory into clusters based on the observed levels of disease incidence or mortality. In this work, we focus on detecting spatial mortality clusters, that occur when neighbouring areas within a territory exhibit similar mortality levels due to one or more diseases. When multiple death causes are examined together, it is relevant to identify both the spatial boundaries of the clusters and the diseases that lead to their formation. However, existing methods in literature struggle to address this dual problem effectively and simultaneously. To overcome these limitations, we introduce Perla, a multivariate Bayesian model that clusters areas in a territory according to the observed mortality rates of multiple death causes, also exploiting the information of external covariates. Our model incorporates the spatial data structure directly into the clustering probabilities by leveraging the stick-breaking formulation of the multinomial distribution. Additionally, it exploits suitable global-local shrinkage priors to ensure that the detection of clusters is driven by concrete differences across mortality levels while excluding spurious differences. We propose an MCMC algorithm for posterior inference that consists of closed-form Gibbs sampling moves for nearly every model parameter, without requiring complex tuning operations. This work is primarily motivated by a case study on the territory of the local unit ULSS6 Euganea within the Italian public healthcare system. To demonstrate the flexibility and effectiveness of our methodology, we also validate Perla with a series of simulation experiments and an extensive case study on mortality levels in U.S. counties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19135v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrea Sottosanti, Pietro Belloni, Enrico Bovo, Giovanna Boccuzzo</dc:creator>
    </item>
    <item>
      <title>Assessing Spatial Disparities: A Bayesian Linear Regression Approach</title>
      <link>https://arxiv.org/abs/2407.19171</link>
      <description>arXiv:2407.19171v1 Announce Type: new 
Abstract: Epidemiological investigations of regionally aggregated spatial data often involve detecting spatial health disparities between neighboring regions on a map of disease mortality or incidence rates. Analyzing such data introduces spatial dependence among the health outcomes and seeks to report statistically significant spatial disparities by delineating boundaries that separate neighboring regions with widely disparate health outcomes. However, current statistical methods are often inadequate for appropriately defining what constitutes a spatial disparity and for constructing rankings of posterior probabilities that are robust under changes to such a definition. More specifically, non-parametric Bayesian approaches endow spatial effects with discrete probability distributions using Dirichlet processes, or generalizations thereof, and rely upon computationally intensive methods for inferring on weakly identified parameters. In this manuscript, we introduce a Bayesian linear regression framework to detect spatial health disparities. This enables us to exploit Bayesian conjugate posterior distributions in a more accessible manner and accelerate computation significantly over existing Bayesian non-parametric approaches. Simulation experiments conducted over a county map of the entire United States demonstrate the effectiveness of our method and we apply our method to a data set from the Institute of Health Metrics and Evaluation (IHME) on age-standardized US county-level estimates of mortality rates across tracheal, bronchus, and lung cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19171v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Lin Wu, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach Toward Robust Multidimensional Ellipsoid-Specific Fitting</title>
      <link>https://arxiv.org/abs/2407.19269</link>
      <description>arXiv:2407.19269v1 Announce Type: new 
Abstract: This work presents a novel and effective method for fitting multidimensional ellipsoids to scattered data in the contamination of noise and outliers. We approach the problem as a Bayesian parameter estimate process and maximize the posterior probability of a certain ellipsoidal solution given the data. We establish a more robust correlation between these points based on the predictive distribution within the Bayesian framework. We incorporate a uniform prior distribution to constrain the search for primitive parameters within an ellipsoidal domain, ensuring ellipsoid-specific results regardless of inputs. We then establish the connection between measurement point and model data via Bayes' rule to enhance the method's robustness against noise. Due to independent of spatial dimensions, the proposed method not only delivers high-quality fittings to challenging elongated ellipsoids but also generalizes well to multidimensional spaces. To address outlier disturbances, often overlooked by previous approaches, we further introduce a uniform distribution on top of the predictive distribution to significantly enhance the algorithm's robustness against outliers. We introduce an {\epsilon}-accelerated technique to expedite the convergence of EM considerably. To the best of our knowledge, this is the first comprehensive method capable of performing multidimensional ellipsoid specific fitting within the Bayesian optimization paradigm under diverse disturbances. We evaluate it across lower and higher dimensional spaces in the presence of heavy noise, outliers, and substantial variations in axis ratios. Also, we apply it to a wide range of practical applications such as microscopy cell counting, 3D reconstruction, geometric shape approximation, and magnetometer calibration tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19269v1</guid>
      <category>stat.ME</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhao Mingyang, Jia Xiaohong, Ma Lei, Shi Yuke, Jiang Jingen, Li Qizhai, Yan Dong-Ming, Huang Tiejun</dc:creator>
    </item>
    <item>
      <title>Normality testing after transformation</title>
      <link>https://arxiv.org/abs/2407.19329</link>
      <description>arXiv:2407.19329v1 Announce Type: new 
Abstract: Transforming a random variable to improve its normality leads to a followup test for whether the transformed variable follows a normal distribution. Previous work has shown that the Anderson Darling test for normality suffers from resubstitution bias following Box-Cox transformation, and indicates normality much too often. The work reported here extends this by adding the Shapiro-Wilk statistic and the two-parameter Box Cox transformation, all of which show severe bias. We also develop a recalibration to correct the bias in all four settings. The methodology was motivated by finding reference ranges in biomarker studies where parametric analysis, possibly on a power-transformed measurand, can be much more informative than nonparametric. It is illustrated with a data set on biomarkers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19329v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Douglas Hawkins</dc:creator>
    </item>
    <item>
      <title>Penalized Principal Component Analysis for Large-dimension Factor Model with Group Pursuit</title>
      <link>https://arxiv.org/abs/2407.19378</link>
      <description>arXiv:2407.19378v1 Announce Type: new 
Abstract: This paper investigates the intrinsic group structures within the framework of large-dimensional approximate factor models, which portrays homogeneous effects of the common factors on the individuals that fall into the same group. To this end, we propose a fusion Penalized Principal Component Analysis (PPCA) method and derive a closed-form solution for the $\ell_2$-norm optimization problem. We also show the asymptotic properties of our proposed PPCA estimates. With the PPCA estimates as an initialization, we identify the unknown group structure by a combination of the agglomerative hierarchical clustering algorithm and an information criterion. Then the factor loadings and factor scores are re-estimated conditional on the identified latent groups. Under some regularity conditions, we establish the consistency of the membership estimators as well as that of the group number estimator derived from the information criterion. Theoretically, we show that the post-clustering estimators for the factor loadings and factor scores with group pursuit achieve efficiency gains compared to the estimators by conventional PCA method. Thorough numerical studies validate the established theory and a real financial example illustrates the practical usefulness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19378v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong He, Dong Liu, Guangming Pan, Yiming Wang</dc:creator>
    </item>
    <item>
      <title>Identifying arbitrary transformation between the slopes in functional regression</title>
      <link>https://arxiv.org/abs/2407.19502</link>
      <description>arXiv:2407.19502v1 Announce Type: new 
Abstract: In this article, we study whether the slope functions of two functional regression models in two samples are associated with any arbitrary transformation (barring constant and linear transformation) or not along the vertical axis. In order to address this issue, a statistical testing of the hypothesis problem is formalized, and the test statistic is formed based on the estimated second derivative of the unknown transformation. The asymptotic properties of the test statistics are investigated using some advanced techniques related to the empirical process. Moreover, to implement the test for small sample size data, a Bootstrap algorithm is proposed, and it is shown that the Bootstrap version of the test is as good as the original test for sufficiently large sample size. Furthermore, the utility of the proposed methodology is shown for simulated data sets, and DTI data is analyzed using this methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19502v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratim Guha Niyogi, Subhra Sankar Dhar</dc:creator>
    </item>
    <item>
      <title>Identification and Inference with Invalid Instruments</title>
      <link>https://arxiv.org/abs/2407.19558</link>
      <description>arXiv:2407.19558v1 Announce Type: new 
Abstract: Instrumental variables (IVs) are widely used to study the causal effect of an exposure on an outcome in the presence of unmeasured confounding. IVs require an instrument, a variable that is (A1) associated with the exposure, (A2) has no direct effect on the outcome except through the exposure, and (A3) is not related to unmeasured confounders. Unfortunately, finding variables that satisfy conditions (A2) or (A3) can be challenging in practice. This paper reviews works where instruments may not satisfy conditions (A2) or (A3), which we refer to as invalid instruments. We review identification and inference under different violations of (A2) or (A3), specifically under linear models, non-linear models, and heteroskedatic models. We conclude with an empirical comparison of various methods by re-analyzing the effect of body mass index on systolic blood pressure from the UK Biobank.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19558v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyunseung Kang, Zijian Guo, Zhonghua Liu, Dylan Small</dc:creator>
    </item>
    <item>
      <title>Metropolis--Hastings with Scalable Subsampling</title>
      <link>https://arxiv.org/abs/2407.19602</link>
      <description>arXiv:2407.19602v1 Announce Type: new 
Abstract: The Metropolis-Hastings (MH) algorithm is one of the most widely used Markov Chain Monte Carlo schemes for generating samples from Bayesian posterior distributions. The algorithm is asymptotically exact, flexible and easy to implement. However, in the context of Bayesian inference for large datasets, evaluating the likelihood on the full data for thousands of iterations until convergence can be prohibitively expensive. This paper introduces a new subsample MH algorithm that satisfies detailed balance with respect to the target posterior and utilises control variates to enable exact, efficient Bayesian inference on datasets with large numbers of observations. Through theoretical results, simulation experiments and real-world applications on certain generalised linear models, we demonstrate that our method requires substantially smaller subsamples and is computationally more efficient than the standard MH algorithm and other exact subsample MH algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19602v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Estev\~ao Prado, Christopher Nemeth, Chris Sherlock</dc:creator>
    </item>
    <item>
      <title>Experimenting on Markov Decision Processes with Local Treatments</title>
      <link>https://arxiv.org/abs/2407.19618</link>
      <description>arXiv:2407.19618v1 Announce Type: new 
Abstract: As service systems grow increasingly complex and dynamic, many interventions become localized, available and taking effect only in specific states. This paper investigates experiments with local treatments on a widely-used class of dynamic models, Markov Decision Processes (MDPs). Particularly, we focus on utilizing the local structure to improve the inference efficiency of the average treatment effect. We begin by demonstrating the efficiency of classical inference methods, including model-based estimation and temporal difference learning under a fixed policy, as well as classical A/B testing with general treatments. We then introduce a variance reduction technique that exploits the local treatment structure by sharing information for states unaffected by the treatment policy. Our new estimator effectively overcomes the variance lower bound for general treatments while matching the more stringent lower bound incorporating the local treatment structure. Furthermore, our estimator can optimally achieve a linear reduction with the number of test arms for a major part of the variance. Finally, we explore scenarios with perfect knowledge of the control arm and design estimators that further improve inference efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19618v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuze Chen, David Simchi-Levi, Chonghuan Wang</dc:creator>
    </item>
    <item>
      <title>Nonparametric independence tests in high-dimensional settings, with applications to the genetics of complex disease</title>
      <link>https://arxiv.org/abs/2407.19624</link>
      <description>arXiv:2407.19624v1 Announce Type: new 
Abstract: [PhD thesis of FCP.] Nowadays, genetics studies large amounts of very diverse variables. Mathematical statistics has evolved in parallel to its applications, with much recent interest high-dimensional settings. In the genetics of human common disease, a number of relevant problems can be formulated as tests of independence. We show how defining adequate premetric structures on the support spaces of the genetic data allows for novel approaches to such testing. This yields a solid theoretical framework, which reflects the underlying biology, and allows for computationally-efficient implementations. For each problem, we provide mathematical results, simulations and the application to real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19624v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fernando Castro-Prado</dc:creator>
    </item>
    <item>
      <title>Estimating heterogeneous treatment effects by W-MCM based on Robust reduced rank regression</title>
      <link>https://arxiv.org/abs/2407.19659</link>
      <description>arXiv:2407.19659v1 Announce Type: new 
Abstract: Recently, from the personalized medicine perspective, there has been an increased demand to identify subgroups of subjects for whom treatment is effective. Consequently, the estimation of heterogeneous treatment effects (HTE) has been attracting attention. While various estimation methods have been developed for a single outcome, there are still limited approaches for estimating HTE for multiple outcomes. Accurately estimating HTE remains a challenge especially for datasets where there is a high correlation between outcomes or the presence of outliers. Therefore, this study proposes a method that uses a robust reduced-rank regression framework to estimate treatment effects and identify effective subgroups. This approach allows the consideration of correlations between treatment effects and the estimation of treatment effects with an accurate low-rank structure. It also provides robust estimates for outliers. This study demonstrates that, when treatment effects are estimated using the reduced rank regression framework with an appropriate rank, the expected value of the estimator equals the treatment effect. Finally, we illustrate the effectiveness and interpretability of the proposed method through simulations and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19659v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryoma Hieda, Shintaro Yuki, Kensuke Tanioka, Hiroshi Yadohisa</dc:creator>
    </item>
    <item>
      <title>Robust classification via finite mixtures of matrix-variate skew t distributions</title>
      <link>https://arxiv.org/abs/2407.19744</link>
      <description>arXiv:2407.19744v1 Announce Type: new 
Abstract: Analysis of matrix-variate data is becoming increasingly common in the literature, particularly in the field of clustering and classification. It is well-known that real data, including real matrix-variate data, often exhibit high levels of asymmetry. To address this issue, one common approach is to introduce a tail or skewness parameter to a symmetric distribution. In this regard, we introduced here a new distribution called the matrix-variate skew t distribution (MVST), which provides flexibility in terms of heavy tail and skewness. We then conduct a thorough investigation of various characterizations and probabilistic properties of the MVST distribution. We also explore extensions of this distribution to a finite mixture model. To estimate the parameters of the MVST distribution, we develop an efficient EM-type algorithm that computes maximum likelihood (ML) estimates of the model parameters. To validate the effectiveness and usefulness of the developed models and associated methods, we perform empirical experiments using simulated data as well as three real data examples. Our results demonstrate the efficacy of the developed approach in handling asymmetric matrix-variate data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19744v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Abbas Mahdavi, Narayanaswamy Balakrishnan, Ahad Jamalizadeh</dc:creator>
    </item>
    <item>
      <title>Inferring High-Dimensional Dynamic Networks Changing with Multiple Covariates</title>
      <link>https://arxiv.org/abs/2407.19978</link>
      <description>arXiv:2407.19978v1 Announce Type: new 
Abstract: High-dimensional networks play a key role in understanding complex relationships. These relationships are often dynamic in nature and can change with multiple external factors (e.g., time and groups). Methods for estimating graphical models are often restricted to static graphs or graphs that can change with a single covariate (e.g., time). We propose a novel class of graphical models, the covariate-varying network (CVN), that can change with multiple external covariates.
  In order to introduce sparsity, we apply a $L_1$-penalty to the precision matrices of $m \geq 2$ graphs we want to estimate. These graphs often show a level of similarity. In order to model this 'smoothness', we introduce the concept of a 'meta-graph' where each node in the meta-graph corresponds to an individual graph in the CVN. The (weighted) adjacency matrix of the meta-graph represents the strength with which similarity is enforced between the $m$ graphs.
  The resulting optimization problem is solved by employing an alternating direction method of multipliers. We test our method using a simulation study and we show its applicability by applying it to a real-world data set, the gene expression networks from the study 'German Cancer in childhood and molecular-epidemiology' (KiKme). An implementation of the algorithm in R is publicly available under https://github.com/bips-hb/cvn</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19978v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louis Dijkstra, Arne Godt, Ronja Foraita</dc:creator>
    </item>
    <item>
      <title>Graphical tools for detection and control of selection bias with multiple exposures and samples</title>
      <link>https://arxiv.org/abs/2407.20027</link>
      <description>arXiv:2407.20027v1 Announce Type: new 
Abstract: Among recent developments in definitions and analysis of selection bias is the potential outcomes approach of Kenah (Epidemiology, 2023), which allows non-parametric analysis using single-world intervention graphs, linking selection of study participants to identification of causal effects. Mohan &amp; Pearl (JASA, 2021) provide a framework for missing data via directed acyclic graphs augmented with nodes indicating missingness for each sometimes-missing variable, which allows for analysis of more general missing data problems but cannot easily encode scenarios in which different groups of variables are observed in specific subsamples. We give an alternative formulation of the potential outcomes framework based on conditional separable effects and indicators for selection into subsamples. This is practical for problems between the single-sample scenarios considered by Kenah and the variable-wise missingness considered by Mohan &amp; Pearl. This simplifies identification conditions and admits generalizations to scenarios with multiple, potentially nested or overlapping study samples, as well as multiple or time-dependent exposures. We give examples of identifiability arguments for case-cohort studies, multiple or time-dependent exposures, and direct effects of selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20027v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick M. Schnell, Eben Kenah</dc:creator>
    </item>
    <item>
      <title>Estimating risk factors for pathogenic dose accrual from longitudinal data</title>
      <link>https://arxiv.org/abs/2407.20051</link>
      <description>arXiv:2407.20051v1 Announce Type: new 
Abstract: Estimating risk factors for incidence of a disease is crucial for understanding its etiology. For diseases caused by enteric pathogens, off-the-shelf statistical model-based approaches do not provide biological plausibility and ignore important sources of variability. We propose a new approach to estimating incidence risk factors built on established work in quantitative microbiological risk assessment. Excepting those risk factors which affect both dose accrual and within-host pathogen survival rates, our model's regression parameters are easily interpretable as the dose accrual rate ratio due to the risk factors under study. % So long as risk factors do not affect both dose accrual and within-host pathogen survival rates, our model parameters are easily interpretable as the dose accrual rate ratio due to the risk factors under study. We also describe a method for leveraging information across multiple pathogens. The proposed methods are available as an R package at \url{https://github.com/dksewell/ladie}. Our simulation study shows unacceptable coverage rates from generalized linear models, while the proposed approach maintains the nominal rate even when the model is misspecified. Finally, we demonstrated our proposed approach by applying our method to Nairobian infant data obtained through the PATHOME study (\url{https://reporter.nih.gov/project-details/10227256}), discovering the impact of various environmental factors on infant enteric infections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20051v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel K. Sewell, Kelly K. Baker</dc:creator>
    </item>
    <item>
      <title>Transfer Learning Targeting Mixed Population: A Distributional Robust Perspective</title>
      <link>https://arxiv.org/abs/2407.20073</link>
      <description>arXiv:2407.20073v1 Announce Type: new 
Abstract: Despite recent advances in transfer learning with multiple source data sets, there still lacks developments for mixture target populations that could be approximated through a composite of the sources due to certain key factors like ethnicity in practice. To address this open problem under distributional shifts of covariates and outcome models as well as the absence of accurate labels on target, we propose a novel approach for distributionally robust transfer learning targeting mixture population. It learns a set of covariate-specific weights to infer the target outcome model with multiple sources, relying on a joint source mixture assumption for the target population. Then our method incorporates a group adversarial learning step to enhance the robustness against moderate violation of the joint mixture assumption. In addition, our framework allows the use of side information like small labeled sample as a guidance to avoid over-conservative results. Statistical convergence and predictive accuracy of our method are quantified through asymptotic studies. Simulation and real-world studies demonstrate the out-performance of our method over existing multi-source and transfer learning approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20073v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keyao Zhan, Xin Xiong, Zijian Guo, Tianxi Cai, Molei Liu</dc:creator>
    </item>
    <item>
      <title>Local Level Dynamic Random Partition Models for Changepoint Detection</title>
      <link>https://arxiv.org/abs/2407.20085</link>
      <description>arXiv:2407.20085v1 Announce Type: new 
Abstract: Motivated by an increasing demand for models that can effectively describe features of complex multivariate time series, e.g. from sensor data in biomechanics, motion analysis, and sports science, we introduce a novel state-space modeling framework where the state equation encodes the evolution of latent partitions of the data over time. Building on the principles of dynamic linear models, our approach develops a random partition model capable of linking data partitions to previous ones over time, using a straightforward Markov structure that accounts for temporal persistence and facilitates changepoint detection. The selection of changepoints involves multiple dependent decisions, and we address this time-dependence by adopting a non-marginal false discovery rate control. This leads to a simple decision rule that ensures more stringent control of the false discovery rate compared to approaches that do not consider dependence. The method is efficiently implemented using a Gibbs sampling algorithm, leading to a straightforward approach compared to existing methods for dependent random partition models. Additionally, we show how the proposed method can be adapted to handle multi-view clustering scenarios. Simulation studies and the analysis of a human gesture phase dataset collected through various sensing technologies show the effectiveness of the method in clustering multivariate time series and detecting changepoints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20085v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alice Giampino, Michele Guindani, Bernardo Nipoti, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Boosted generalized normal distributions: Integrating machine learning with operations knowledge</title>
      <link>https://arxiv.org/abs/2407.19092</link>
      <description>arXiv:2407.19092v1 Announce Type: cross 
Abstract: Applications of machine learning (ML) techniques to operational settings often face two challenges: i) ML methods mostly provide point predictions whereas many operational problems require distributional information; and ii) They typically do not incorporate the extensive body of knowledge in the operations literature, particularly the theoretical and empirical findings that characterize specific distributions. We introduce a novel and rigorous methodology, the Boosted Generalized Normal Distribution ($b$GND), to address these challenges. The Generalized Normal Distribution (GND) encompasses a wide range of parametric distributions commonly encountered in operations, and $b$GND leverages gradient boosting with tree learners to flexibly estimate the parameters of the GND as functions of covariates. We establish $b$GND's statistical consistency, thereby extending this key property to special cases studied in the ML literature that lacked such guarantees. Using data from a large academic emergency department in the United States, we show that the distributional forecasting of patient wait and service times can be meaningfully improved by leveraging findings from the healthcare operations literature. Specifically, $b$GND performs 6% and 9% better than the distribution-agnostic ML benchmark used to forecast wait and service times respectively. Further analysis suggests that these improvements translate into a 9% increase in patient satisfaction and a 4% reduction in mortality for myocardial infarction patients. Our work underscores the importance of integrating ML with operations knowledge to enhance distributional forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19092v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ragip Gurlek, Francis de Vericourt, Donald K. K. Lee</dc:creator>
    </item>
    <item>
      <title>Network sampling based inference for subgraph counts and clustering coefficient in a Stochastic Block Model framework with some extensions to a sparse case</title>
      <link>https://arxiv.org/abs/2407.19191</link>
      <description>arXiv:2407.19191v1 Announce Type: cross 
Abstract: Sampling is frequently used to collect data from large networks. In this article we provide valid asymptotic prediction intervals for subgraph counts and clustering coefficient of a population network when a network sampling scheme is used to observe the population. The theory is developed under a model based framework, where it is assumed that the population network is generated by a Stochastic Block Model (SBM). We study the effects of induced and ego-centric network formation, following the initial selection of nodes by Bernoulli sampling, and establish asymptotic normality of sample based subgraph count and clustering coefficient statistic under both network formation methods. The asymptotic results are developed under a joint design and model based approach, where the effect of sampling design is not ignored. In case of the sample based clustering coefficient statistic, we find that a bias correction is required in the ego-centric case, but there is no such bias in the induced case. We also extend the asymptotic normality results for estimated subgraph counts to a mildly sparse SBM framework, where edge probabilities decay to zero at a slow rate. In this sparse setting we find that the scaling and the maximum allowable decay rate for edge probabilities depend on the choice of the target subgraph. We obtain an expression for this maximum allowable decay rate and our results suggest that the rate becomes slower if the target subgraph has more edges in a certain sense. The simulation results suggest that the proposed prediction intervals have excellent coverage, even when the node selection probability is small and unknown SBM parameters are replaced by their estimates. Finally, the proposed methodology is applied to a real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19191v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anirban Mandal, Arindam Chatterjee</dc:creator>
    </item>
    <item>
      <title>Large-scale Multiple Testing of Cross-covariance Functions with Applications to Functional Network Models</title>
      <link>https://arxiv.org/abs/2407.19399</link>
      <description>arXiv:2407.19399v1 Announce Type: cross 
Abstract: The estimation of functional networks through functional covariance and graphical models have recently attracted increasing attention in settings with high dimensional functional data, where the number of functional variables p is comparable to, and maybe larger than, the number of subjects. In this paper, we first reframe the functional covariance model estimation as a tuning-free problem of simultaneously testing p(p-1)/2 hypotheses for cross-covariance functions. Our procedure begins by constructing a Hilbert-Schmidt-norm-based test statistic for each pair, and employs normal quantile transformations for all test statistics, upon which a multiple testing step is proposed. We then explore the multiple testing procedure under a general error-contamination framework and establish that our procedure can control false discoveries asymptotically. Additionally, we demonstrate that our proposed methods for two concrete examples: the functional covariance model with partial observations and, importantly, the more challenging functional graphical model, can be seamlessly integrated into the general error-contamination framework, and, with verifiable conditions, achieve theoretical guarantees on effective false discovery control. Finally, we showcase the superiority of our proposals through extensive simulations and functional connectivity analysis of two neuroimaging datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19399v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qin Fang, Qing Jiang, Xinghao Qiao</dc:creator>
    </item>
    <item>
      <title>Causal effect estimation under network interference with mean-field methods</title>
      <link>https://arxiv.org/abs/2407.19613</link>
      <description>arXiv:2407.19613v1 Announce Type: cross 
Abstract: We study causal effect estimation from observational data under interference. The interference pattern is captured by an observed network. We adopt the chain graph framework of Tchetgen Tchetgen et. al. (2021), which allows (i) interaction among the outcomes of distinct study units connected along the graph and (ii) long range interference, whereby the outcome of an unit may depend on the treatments assigned to distant units connected along the interference network. For ``mean-field" interaction networks, we develop a new scalable iterative algorithm to estimate the causal effects. For gaussian weighted networks, we introduce a novel causal effect estimation algorithm based on Approximate Message Passing (AMP). Our algorithms are provably consistent under a ``high-temperature" condition on the underlying model. We estimate the (unknown) parameters of the model from data using maximum pseudo-likelihood and establish $\sqrt{n}$-consistency of this estimator in all parameter regimes. Finally, we prove that the downstream estimators obtained by plugging in estimated parameters into the aforementioned algorithms are consistent at high-temperature. Our methods can accommodate dense interactions among the study units -- a setting beyond reach using existing techniques. Our algorithms originate from the study of variational inference approaches in high-dimensional statistics; overall, we demonstrate the usefulness of these ideas in the context of causal effect estimation under interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19613v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sohom Bhattacharya, Subhabrata Sen</dc:creator>
    </item>
    <item>
      <title>Causal Interventional Prediction System for Robust and Explainable Effect Forecasting</title>
      <link>https://arxiv.org/abs/2407.19688</link>
      <description>arXiv:2407.19688v1 Announce Type: cross 
Abstract: Although the widespread use of AI systems in today's world is growing, many current AI systems are found vulnerable due to hidden bias and missing information, especially in the most commonly used forecasting system. In this work, we explore the robustness and explainability of AI-based forecasting systems. We provide an in-depth analysis of the underlying causality involved in the effect prediction task and further establish a causal graph based on treatment, adjustment variable, confounder, and outcome. Correspondingly, we design a causal interventional prediction system (CIPS) based on a variational autoencoder and fully conditional specification of multiple imputations. Extensive results demonstrate the superiority of our system over state-of-the-art methods and show remarkable versatility and extensibility in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19688v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3627673.3680073</arxiv:DOI>
      <dc:creator>Zhixuan Chu, Hui Ding, Guang Zeng, Shiyu Wang, Yiming Li</dc:creator>
    </item>
    <item>
      <title>Bag of DAGs: Inferring Directional Dependence in Spatiotemporal Processes</title>
      <link>https://arxiv.org/abs/2112.11870</link>
      <description>arXiv:2112.11870v5 Announce Type: replace 
Abstract: We propose a class of nonstationary processes to characterize space- and time-varying directional associations in point-referenced data. We are motivated by spatiotemporal modeling of air pollutants in which local wind patterns are key determinants of the pollutant spread, but information regarding prevailing wind directions may be missing or unreliable. We propose to map a discrete set of wind directions to edges in a sparse directed acyclic graph (DAG), accounting for uncertainty in directional correlation patterns across a domain. The resulting Bag of DAGs processes (BAGs) lead to interpretable nonstationarity and scalability for large data due to sparsity of DAGs in the bag. We outline Bayesian hierarchical models using BAGs and illustrate inferential and performance gains of our methods compared to other state-of-the-art alternatives. We analyze fine particulate matter using high-resolution data from low-cost air quality sensors in California during the 2020 wildfire season. An R package is available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.11870v5</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bora Jin, Michele Peruzzi, David Dunson</dc:creator>
    </item>
    <item>
      <title>Bayesian Clustering via Fusing of Localized Densities</title>
      <link>https://arxiv.org/abs/2304.00074</link>
      <description>arXiv:2304.00074v3 Announce Type: replace 
Abstract: Bayesian clustering typically relies on mixture models, with each component interpreted as a different cluster. After defining a prior for the component parameters and weights, Markov chain Monte Carlo (MCMC) algorithms are commonly used to produce samples from the posterior distribution of the component labels. The data are then clustered by minimizing the expectation of a clustering loss function that favours similarity to the component labels. Unfortunately, although these approaches are routinely implemented, clustering results are highly sensitive to kernel misspecification. For example, if Gaussian kernels are used but the true density of data within a cluster is even slightly non-Gaussian, then clusters will be broken into multiple Gaussian components. To address this problem, we develop Fusing of Localized Densities (FOLD), a novel clustering method that melds components together using the posterior of the kernels. FOLD has a fully Bayesian decision theoretic justification, naturally leads to uncertainty quantification, can be easily implemented as an add-on to MCMC algorithms for mixtures, and favours a small number of distinct clusters. We provide theoretical support for FOLD including clustering optimality under kernel misspecification. In simulated experiments and real data, FOLD outperforms competitors by minimizing the number of clusters while inferring meaningful group structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00074v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Dombowsky, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Instrumental Variable Approach to Estimating Individual Causal Effects in N-of-1 Trials: Application to ISTOP Study</title>
      <link>https://arxiv.org/abs/2306.14019</link>
      <description>arXiv:2306.14019v2 Announce Type: replace 
Abstract: An N-of-1 trial is a multiple crossover trial conducted in a single individual to provide evidence to directly inform personalized treatment decisions. Advancements in wearable devices greatly improved the feasibility of adopting these trials to identify optimal individual treatment plans, particularly when treatments differ among individuals and responses are highly heterogeneous. Our work was motivated by the I-STOP-AFib Study, which examined the impact of different triggers on atrial fibrillation (AF) occurrence. We described a causal framework for 'N-of-1' trial using potential treatment selection paths and potential outcome paths. Two estimands of individual causal effect were defined:(a) the effect of continuous exposure, and (b) the effect of an individual observed behavior. We addressed three challenges: (a) imperfect compliance to the randomized treatment assignment; (b) binary treatments and binary outcomes which led to the 'non-collapsibility' issue of estimating odds ratios; and (c) serial inference in the longitudinal observations. We adopted the Bayesian IV approach where the study randomization was the IV as it impacted the choice of exposure of a subject but not directly the outcome. Estimations were through a system of two parametric Bayesian models to estimate the individual causal effect. Our model got around the non-collapsibility and non-consistency by modeling the confounding mechanism through latent structural models and by inferring with Bayesian posterior of functionals. Autocorrelation present in the repeated measurements was also accounted for. The simulation study showed our method largely reduced bias and greatly improved the coverage of the estimated causal effect, compared to existing methods (ITT, PP, and AT). We applied the method to I-STOP-AFib Study to estimate the individual effect of alcohol on AF occurrence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14019v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kexin Qu, Christopher H. Schmid, Tao Liu</dc:creator>
    </item>
    <item>
      <title>Nonparametric Causal Decomposition of Group Disparities</title>
      <link>https://arxiv.org/abs/2306.16591</link>
      <description>arXiv:2306.16591v3 Announce Type: replace 
Abstract: We introduce a new nonparametric causal decomposition approach that identifies the mechanisms by which a treatment variable contributes to a group-based outcome disparity. Our approach distinguishes three mechanisms: group differences in 1) treatment prevalence, 2) average treatment effects, and 3) selection into treatment based on individual-level treatment effects. Our approach reformulates classic Kitagawa-Blinder-Oaxaca decompositions in causal and nonparametric terms, complements causal mediation analysis by explaining group disparities instead of group effects, and isolates conceptually distinct mechanisms conflated in recent random equalization decompositions. In contrast to all prior approaches, our framework uniquely identifies differential selection into treatment as a novel disparity-generating mechanism. Our approach can be used for both the retrospective causal explanation of disparities and the prospective planning of interventions to change disparities. We present both an unconditional and a conditional decomposition, where the latter quantifies the contributions of the treatment within levels of certain covariates. We develop nonparametric estimators that are $\sqrt{n}$-consistent, asymptotically normal, semiparametrically efficient, and multiply robust. We apply our approach to analyze the mechanisms by which college graduation causally contributes to intergenerational income persistence (the disparity in income attainment between parental income groups).</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16591v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ang Yu, Felix Elwert</dc:creator>
    </item>
    <item>
      <title>Bayesian Structure Learning in Undirected Gaussian Graphical Models: Literature Review with Empirical Comparison</title>
      <link>https://arxiv.org/abs/2307.02603</link>
      <description>arXiv:2307.02603v3 Announce Type: replace 
Abstract: Gaussian graphical models provide a powerful framework to reveal the conditional dependency structure between multivariate variables. The process of uncovering the conditional dependency network is known as structure learning. Bayesian methods can measure the uncertainty of conditional relationships and include prior information. However, frequentist methods are often preferred due to the computational burden of the Bayesian approach. Over the last decade, Bayesian methods have seen substantial improvements, with some now capable of generating accurate estimates of graphs up to a thousand variables in mere minutes. Despite these advancements, a comprehensive review or empirical comparison of all recent methods has not been conducted. This paper delves into a wide spectrum of Bayesian approaches used for structure learning and evaluates their efficacy through a comprehensive simulation study. We also demonstrate how to apply Bayesian structure learning to a real-world data set and provide directions for future research. This study gives an exhaustive overview of this dynamic field for newcomers, practitioners, and experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02603v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Lucas Vogels, Reza Mohammadi, Marit Schoonhoven, S. Ilker Birbil</dc:creator>
    </item>
    <item>
      <title>Structural restrictions in local causal discovery: identifying direct causes of a target variable</title>
      <link>https://arxiv.org/abs/2307.16048</link>
      <description>arXiv:2307.16048v2 Announce Type: replace 
Abstract: We consider the problem of learning a set of direct causes of a target variable from an observational joint distribution. Learning directed acyclic graphs (DAGs) that represent the causal structure is a fundamental problem in science. Several results are known when the full DAG is identifiable from the distribution, such as assuming a nonlinear Gaussian data-generating process. Here, we are only interested in identifying the direct causes of one target variable (local causal structure), not the full DAG. This allows us to relax the identifiability assumptions and develop possibly faster and more robust algorithms. In contrast to the Invariance Causal Prediction framework, we only assume that we observe one environment without any interventions. We discuss different assumptions for the data-generating process of the target variable under which the set of direct causes is identifiable from the distribution. While doing so, we put essentially no assumptions on the variables other than the target variable. In addition to the novel identifiability results, we provide two practical algorithms for estimating the direct causes from a finite random sample and demonstrate their effectiveness on several benchmark and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16048v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juraj Bodik, Val\'erie Chavez-Demoulin</dc:creator>
    </item>
    <item>
      <title>Transfer learning for piecewise-constant mean estimation: Optimality, $\ell_1$- and $\ell_0$-penalisation</title>
      <link>https://arxiv.org/abs/2310.05646</link>
      <description>arXiv:2310.05646v4 Announce Type: replace 
Abstract: We study transfer learning for estimating piecewise-constant signals when source data, which may be relevant but disparate, are available in addition to the target data. We first investigate transfer learning estimators that respectively employ $\ell_1$- and $\ell_0$-penalties for unisource data scenarios and then generalise these estimators to accommodate multisources. To further reduce estimation errors, especially when some sources significantly differ from the target, we introduce an informative source selection algorithm. We then examine these estimators with multisource selection and establish their minimax optimality. Unlike the common narrative in the transfer learning literature that the performance is enhanced through large source sample sizes, our approaches leverage higher observation frequencies and accommodate diverse frequencies across multiple sources. Our theoretical findings are supported by extensive numerical experiments, with the code available online, see https://github.com/chrisfanwang/transferlearning</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05646v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Wang, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian Adjustment of Unmeasured Confounders in Cox Proportional Hazards Models</title>
      <link>https://arxiv.org/abs/2312.02404</link>
      <description>arXiv:2312.02404v3 Announce Type: replace 
Abstract: In observational studies, unmeasured confounders present a crucial challenge in accurately estimating desired causal effects. To calculate the hazard ratio (HR) in Cox proportional hazard models for time-to-event outcomes, two-stage residual inclusion and limited information maximum likelihood are typically employed. However, these methods are known to entail difficulty in terms of potential bias of HR estimates and parameter identification. This study introduces a novel nonparametric Bayesian method designed to estimate an unbiased HR, addressing concerns that previous research methods have had. Our proposed method consists of two phases: 1) detecting clusters based on the likelihood of the exposure and outcome variables, and 2) estimating the hazard ratio within each cluster. Although it is implicitly assumed that unmeasured confounders affect outcomes through cluster effects, our algorithm is well-suited for such data structures. The proposed Bayesian estimator has good performance compared with some competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02404v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunichiro Oriharal, Shonosuke Sugasawa, Tomohiro Ohigashi, Tomoyuki Nakagawa, Masataka Taguri</dc:creator>
    </item>
    <item>
      <title>Design-based inference for generalized network experiments with stochastic interventions</title>
      <link>https://arxiv.org/abs/2312.03268</link>
      <description>arXiv:2312.03268v2 Announce Type: replace 
Abstract: A growing number of researchers are conducting randomized experiments to analyze causal relationships in network settings where units influence one another. A dominant methodology for analyzing these experiments is design-based, leveraging random treatment assignments as the basis for inference. In this paper, we generalize this design-based approach to accommodate complex experiments with a variety of causal estimands and different target populations. An important special case of such generalized network experiments is a bipartite network experiment, in which treatment is randomized among one set of units, and outcomes are measured on a separate set of units. We propose a broad class of causal estimands based on stochastic interventions for generalized network experiments. Using a design-based approach, we show how to estimate these causal quantities without bias and develop conservative variance estimators. We apply our methodology to a randomized experiment in education where participation in an anti-conflict promotion program is randomized among selected students. Our analysis estimates the causal effects of treating each student or their friends among different target populations in the network. We find that the program improves the overall conflict awareness among students but does not significantly reduce the total number of such conflicts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03268v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ambarish Chattopadhyay, Kosuke Imai, Jose R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to functional regression: theory and computation</title>
      <link>https://arxiv.org/abs/2312.14086</link>
      <description>arXiv:2312.14086v2 Announce Type: replace 
Abstract: We propose a novel Bayesian methodology for inference in functional linear and logistic regression models based on the theory of reproducing kernel Hilbert spaces (RKHS's). We introduce general models that build upon the RKHS generated by the covariance function of the underlying stochastic process, and whose formulation includes as particular cases all finite-dimensional models based on linear combinations of marginals of the process, which can collectively be seen as a dense subspace made of simple approximations. By imposing a suitable prior distribution on this dense functional space we can perform data-driven inference via standard Bayes methodology, estimating the posterior distribution through reversible jump Markov chain Monte Carlo methods. In this context, our contribution is two-fold. First, we derive a theoretical result that guarantees posterior consistency, based on an application of a classic theorem of Doob to our RKHS setting. Second, we show that several prediction strategies stemming from our Bayesian procedure are competitive against other usual alternatives in both simulations and real data sets, including a Bayesian-motivated variable selection method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14086v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jos\'e R. Berrendero, Antonio Co\'in, Antonio Cuevas</dc:creator>
    </item>
    <item>
      <title>Doublethink: simultaneous Bayesian-frequentist model-averaged hypothesis testing</title>
      <link>https://arxiv.org/abs/2312.17566</link>
      <description>arXiv:2312.17566v2 Announce Type: replace 
Abstract: Establishing the frequentist properties of Bayesian approaches widens their appeal and offers new understanding. In hypothesis testing, Bayesian model averaging addresses the problem that conclusions are sensitive to variable selection. But Bayesian false discovery rate (FDR) guarantees are contingent on prior assumptions that may be disputed. Here we show that Bayesian model-averaged hypothesis testing is a closed testing procedure that controls the frequentist familywise error rate (FWER) in the strong sense. The rate converges pointwise as the sample size grows and, under some conditions, uniformly. The `Doublethink' method computes simultaneous posterior odds and asymptotic p-values for model-averaged hypothesis testing. We explore its benefits, including post-hoc variable selection, and limitations, including finite-sample inflation, through a Mendelian randomization study and simulations comparing approaches like LASSO, stepwise regression, the Benjamini-Hochberg procedure and e-values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17566v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helen R. Fryer, Nicolas Arning, Daniel J. Wilson</dc:creator>
    </item>
    <item>
      <title>Privacy-Protected Spatial Autoregressive Model</title>
      <link>https://arxiv.org/abs/2403.16773</link>
      <description>arXiv:2403.16773v2 Announce Type: replace 
Abstract: Spatial autoregressive (SAR) models are important tools for studying network effects. However, with an increasing emphasis on data privacy, data providers often implement privacy protection measures that make classical SAR models inapplicable. In this study, we introduce a privacy-protected SAR model with noise-added response and covariates to meet privacy-protection requirements. However, in this scenario, the traditional quasi-maximum likelihood estimator becomes infeasible because the likelihood function cannot be directly formulated. To address this issue, we first consider an explicit expression for the likelihood function with only noise-added responses. Then, we develop techniques to correct the biases for derivatives introduced by noise. Correspondingly, a Newton-Raphson-type algorithm is proposed to obtain the estimator, leading to a corrected likelihood estimator. To further enhance computational efficiency, we introduce a corrected least squares estimator based on the idea of bias correction. These two estimation methods ensure both data security and the attainment of statistically valid estimators. Theoretical analysis of both estimators is carefully conducted, statistical inference methods and model extensions are discussed. The finite sample performances of different methods are demonstrated through extensive simulations and the analysis of a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16773v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danyang Huang, Ziyi Kong, Shuyuan Wu, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Unsupervised Training of Convex Regularizers using Maximum Likelihood Estimation</title>
      <link>https://arxiv.org/abs/2404.05445</link>
      <description>arXiv:2404.05445v2 Announce Type: replace 
Abstract: Imaging is a standard example of an inverse problem, where the task of reconstructing a ground truth from a noisy measurement is ill-posed. Recent state-of-the-art approaches for imaging use deep learning, spearheaded by unrolled and end-to-end models and trained on various image datasets. However, many such methods require the availability of ground truth data, which may be unavailable or expensive, leading to a fundamental barrier that can not be bypassed by choice of architecture. Unsupervised learning presents an alternative paradigm that bypasses this requirement, as they can be learned directly on noisy data and do not require any ground truths. A principled Bayesian approach to unsupervised learning is to maximize the marginal likelihood with respect to the given noisy measurements, which is intrinsically linked to classical variational regularization. We propose an unsupervised approach using maximum marginal likelihood estimation to train a convex neural network-based image regularization term directly on noisy measurements, improving upon previous work in both model expressiveness and dataset size. Experiments demonstrate that the proposed method produces priors that are near competitive when compared to the analogous supervised training method for various image corruption operators, maintaining significantly better generalization properties when compared to end-to-end methods. Moreover, we provide a detailed theoretical analysis of the convergence properties of our proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05445v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong Ye Tan, Ziruo Cai, Marcelo Pereyra, Subhadip Mukherjee, Junqi Tang, Carola-Bibiane Sch\"onlieb</dc:creator>
    </item>
    <item>
      <title>Two-stage Estimators for Spatial Confounding</title>
      <link>https://arxiv.org/abs/2404.09358</link>
      <description>arXiv:2404.09358v2 Announce Type: replace 
Abstract: Public health data are often spatially dependent, but standard spatial regression methods can suffer from bias and invalid inference when the independent variable is associated with spatially-correlated residuals. This could occur if, for example, there is an unmeasured environmental contaminant associated with the independent and outcome variables in a spatial regression analysis. Geoadditive structural equation modeling (gSEM), in which an estimated spatial trend is removed from both the explanatory and response variables before estimating the parameters of interest, has previously been proposed as a solution, but there has been little investigation of gSEM's properties with point-referenced data. We link gSEM to results on double machine learning and semiparametric regression based on two-stage procedures. We propose using these semiparametric estimators for spatial regression using Gaussian processes with Mat\`ern covariance to estimate the spatial trends, and term this class of estimators Double Spatial Regression (DSR). We derive regularity conditions for root-$n$ asymptotic normality and consistency and closed-form variance estimation, and show that in simulations where standard spatial regression estimators are highly biased and have poor coverage, DSR can mitigate bias more effectively than competitors and obtain nominal coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09358v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nate Wiecha, Jane A. Hoppin, Brian J. Reich</dc:creator>
    </item>
    <item>
      <title>Estimation of density functionals via cross-validation</title>
      <link>https://arxiv.org/abs/2404.13753</link>
      <description>arXiv:2404.13753v2 Announce Type: replace 
Abstract: In density estimation, the mean integrated squared error (MISE) is commonly used as a measure of performance. In that setting, the cross-validation criterion provides an unbiased estimator of the MISE minus the integral of the squared density. Since the minimum MISE is known to converge to zero, this suggests that the minimum value of the cross-validation criterion could be regarded as an estimator of minus the integrated squared density. This novel proposal presents the outstanding feature that, unlike all other existing estimators, it does not need the choice of any tuning parameter. Indeed, it is proved here that this approach results in a consistent and efficient estimator, with remarkable performance in practice. Moreover, apart from this base case, it is shown how several other problems on density functional estimation can be similarly handled using this new principle, thus demonstrating full potential for further applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13753v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jos\'e E. Chac\'on, Carlos Tenreiro</dc:creator>
    </item>
    <item>
      <title>Estimating Metocean Environments Associated with Extreme Structural Response to Demonstrate the Dangers of Environmental Contour Methods</title>
      <link>https://arxiv.org/abs/2404.16775</link>
      <description>arXiv:2404.16775v4 Announce Type: replace 
Abstract: Extreme value analysis (EVA) uses data to estimate long-term extreme environmental conditions for variables such as significant wave height and period, for the design of marine structures. Together with models for the short-term evolution of the ocean environment and for wave-structure interaction, EVA provides a basis for full probabilistic design analysis. Alternatively, environmental contours provide an approximate approach to estimating structural integrity, without requiring structural knowledge. These contour methods also exploit statistical models, including EVA, but avoid the need for structural modelling by making what are believed to be conservative assumptions about the shape of the structural failure boundary in the environment space. These assumptions, however, may not always be appropriate, or may lead to unnecessary wasted resources from over design. We demonstrate a methodology for efficient fully probabilistic analysis of structural failure. From this, we estimate the joint conditional probability density of the environment (CDE), given the occurrence of an extreme structural response. We use CDE as a diagnostic to highlight the deficiencies of environmental contour methods for design; none of the IFORM environmental contours considered characterise CDE well for three example structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16775v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Speers, David Randell, Jonathan Angus Tawn, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>Multicalibration for Modeling Censored Survival Data with Universal Adaptability</title>
      <link>https://arxiv.org/abs/2405.15948</link>
      <description>arXiv:2405.15948v2 Announce Type: replace 
Abstract: Traditional statistical and machine learning methods assume identical distribution for the training and test data sets. This assumption, however, is often violated in real applications, particularly in health care research, where the training data~(source) may underrepresent specific subpopulations in the testing or target domain. Such disparities, coupled with censored observations, present significant challenges for investigators aiming to make predictions for those minority groups. This paper focuses on target-independent learning under covariate shift, where we study multicalibration for survival probability and restricted mean survival time, and propose a black-box post-processing boosting algorithm designed for censored survival data. Our algorithm, leveraging the pseudo observations, yields a multicalibrated predictor competitive with propensity scoring regarding predictions on the unlabeled target domain, not just overall but across diverse subpopulations. Our theoretical analysis for pseudo observations relies on functional delta method and $p$-variational norm. We further investigate the algorithm's sample complexity and convergence properties, as well as the multicalibration guarantee for post-processed predictors. Our theoretical insights reveal the link between multicalibration and universal adaptability, suggesting that our calibrated function performs comparably to, if not better than, the inverse propensity score weighting estimator. The performance of our proposed methods is corroborated through extensive numerical simulations and a real-world case study focusing on prediction of cardiovascular disease risk in two large prospective cohort studies. These empirical results confirm its potential as a powerful tool for predictive analysis with censored outcomes in diverse and shifting populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15948v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanxuan Ye, Hongzhe Li</dc:creator>
    </item>
    <item>
      <title>Efficient combination of observational and experimental datasets under general restrictions on outcome mean functions</title>
      <link>https://arxiv.org/abs/2406.06941</link>
      <description>arXiv:2406.06941v2 Announce Type: replace 
Abstract: A researcher collecting data from a randomized controlled trial (RCT) often has access to an auxiliary observational dataset that may be confounded or otherwise biased for estimating causal effects. Common modeling assumptions impose restrictions on the outcome mean function - the conditional expectation of the outcome of interest given observed covariates - in the two datasets. Running examples from the literature include settings where the observational dataset is subject to outcome-mediated selection bias or to confounding bias taking an assumed parametric form. We propose a succinct framework to derive the efficient influence function for any identifiable pathwise differentiable estimand under a general class of restrictions on the outcome mean function. This uncovers surprising results that with homoskedastic outcomes and a constant propensity score in the RCT, even strong parametric assumptions cannot improve the semiparametric lower bound for estimating various average treatment effects. We then leverage double machine learning to construct a one-step estimator that achieves the semiparametric efficiency bound even in cases when the outcome mean function and other nuisance parameters are estimated nonparametrically. The goal is to empower a researcher with custom, previously unstudied modeling restrictions on the outcome mean function to systematically construct causal estimators that maximially leverage their assumptions for variance reduction. We demonstrate the finite sample precision gains of our estimator over existing approaches in extensions of various numerical studies and data examples from the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06941v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison H. Li</dc:creator>
    </item>
    <item>
      <title>Identifying macro conditional independencies and macro total effects in summary causal graphs with latent confounding</title>
      <link>https://arxiv.org/abs/2407.07934</link>
      <description>arXiv:2407.07934v3 Announce Type: replace 
Abstract: Understanding causal relations in dynamic systems is essential in epidemiology. While causal inference methods have been extensively studied, they often rely on fully specified causal graphs, which may not always be available in complex dynamic systems. Partially specified causal graphs, such as summary causal graphs (SCGs), provide a simplified representation of causal relations, omitting temporal information and focusing on high-level causal structures. This simplification introduces new challenges concerning the types of queries of interest: macro queries, which involve relationships between clusters represented as vertices in the graph, and micro queries, which pertain to relationships between variables that are not directly visible through the vertices of the graph. In this paper, we first clearly distinguish between macro conditional independencies and micro conditional independencies and between macro total effects and micro total effects. Then, we demonstrate the soundness and completeness of the d-separation to identify macro conditional independencies in SCGs. Furthermore, we establish that the do-calculus is sound and complete for identifying macro total effects in SCGs. Finally, we give a graphical characterization for the non-identifiability of macro total effects in SCGs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07934v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Ferreira, Charles K. Assaad</dc:creator>
    </item>
    <item>
      <title>CASTRO -- Efficient constrained sampling method for material and chemical experimental design</title>
      <link>https://arxiv.org/abs/2407.16567</link>
      <description>arXiv:2407.16567v2 Announce Type: replace-cross 
Abstract: The exploration of multicomponent material composition space requires significant time and financial investments, necessitating efficient use of resources for statistically relevant compositions. This article introduces a novel methodology, implemented in the open-source CASTRO (ConstrAined Sequential laTin hypeRcube sampling methOd) software package, to overcome equality-mixture constraints and ensure comprehensive design space coverage. Our approach leverages Latin hypercube sampling (LHS) and LHS with multidimensional uniformity (LHSMDU) using a divide-and-conquer strategy to manage high-dimensional problems effectively. By incorporating previous experimental knowledge within a limited budget, our method strategically recommends a feasible number of experiments to explore the design space. We validate our methodology with two examples: a four-dimensional problem with near-uniform distributions and a nine-dimensional problem with additional mixture constraints, yielding specific component distributions. Our constrained sequential LHS or LHSMDU approach enables thorough design space exploration, proving robustness for experimental design. This research not only advances material science but also offers promising solutions for efficiency challenges in pharmaceuticals and chemicals. CASTRO and the case studies are available for free download on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16567v2</guid>
      <category>stat.CO</category>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christina Schenk, Maciej Haranczyk</dc:creator>
    </item>
  </channel>
</rss>
