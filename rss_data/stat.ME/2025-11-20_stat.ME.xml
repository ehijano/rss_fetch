<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Nov 2025 05:01:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Extrinsic Total-Variance and Coplanarity via Oriented and Classical Projective Shape Analysis</title>
      <link>https://arxiv.org/abs/2511.14815</link>
      <description>arXiv:2511.14815v1 Announce Type: new 
Abstract: Projective shape analysis provides a geometric framework for studying digital images acquired by pinhole digital cameras. In the classical projective shape (PS) method, landmark configurations are represented in $(\RP^2)^{k-4}$, where $k$ is the number of landmarks observed. This representation is invariant under the action of the full projective group on this space and is sign-blind, so opposite directions in $\R^{3}$ determine the same projective point and front--back orientation of a surface is not recorded. Oriented projective shape ($\OPS$) restores this information by working on a product of $k-4$ spheres $\SP^2$ instead of projective space and restricting attention to the orientation-preserving subgroup of projective transformations. In this paper we introduce an extrinsic total-variance index for OPS, resulting in the extrinsic Fr\'echet framework for the m dimensional case from the inclusion $\jdir:(\SP^m)^q\hookrightarrow(\R^{m+1})^q,q=k-m-2$. In the planar pentad case ($m=2$, $q=1$) the sample total extrinsic variance has a closed form in terms of the mean of a random sample of size $n$ of oriented projective coordinates in $S^2$. As an illustration, using an oriented projective frame, we analyze the Sope Creek stone data set, a benchmark and nearly planar example with $41$ images and $5$ landmarks. Using a delta-method applied to a large sample and a generalized Slutsky theorem argument, for an OPS leave-two-out diagnostic, one identifies coplanarity at the $5\%$ level, confirming the concentrated data coplanarity PS result in Patrangenaru(2001)\cite{Patrangenaru2001}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14815v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Musab Alamoudi, Robert L. Paige, Vic Patrangenaru</dc:creator>
    </item>
    <item>
      <title>An Estimand-Focused Approach for AUC Estimation, Generalization, and Comparison: From Non-representative Samples to Target Population</title>
      <link>https://arxiv.org/abs/2511.14992</link>
      <description>arXiv:2511.14992v1 Announce Type: new 
Abstract: The area under the ROC curve (AUC) is the standard measure of a biomarker's discriminatory accuracy; however, naive AUC estimates can be misleading when validation cohorts differ from the intended target population. Such covariate shifts commonly arise under biased or non-random sampling, distorting AUC estimations and thus impeding both generalization and cross-study comparison of AUC. We develop an estimand-focused framework for valid AUC estimation and benchmarking under covariate shift. Leveraging balancing ideas from causal inference, we extend calibration weighting to the U-statistic framework for AUC estimation and introduce a family of estimators that accommodate both summary-level and patient-level information; in certain specifications, some of these estimators attain double robustness. Furthermore, we establish asymptotic properties and study their performances across a spectrum of covariate shift severities and calibration choices in comprehensive simulations. Finally, we demonstrate practical utility in the POWER trials by evaluating how baseline stair-climb power (SCP) predicts 6-month survival among advanced non-small-cell lung cancer (NSCLC) patients. Together, the results provide a principled toolkit for anchoring biomarker AUCs to clinically relevant target populations and for comparing them fairly across studies despite distributional differences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14992v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiajun Liu, Guangcai Mao, Xiaofei Wang</dc:creator>
    </item>
    <item>
      <title>The Sequential Nature of Science: Quantifying Learning from a Sequence of Studies</title>
      <link>https://arxiv.org/abs/2511.14996</link>
      <description>arXiv:2511.14996v1 Announce Type: new 
Abstract: Scientific progress is inherently sequential: collective knowledge is updated as new studies enter the literature. We propose the sequential meta-analysis research trace (SMART), which quantifies the influence of each study at the time it enters the literature. In contrast to classical meta-analysis, our method can capture how new studies may cast doubt on previously held beliefs, increasing collective uncertainty. For example, a new study may present a methodological critique of prior work and propose a superior method. Even small studies, which may not materially affect a retrospective meta-analysis, can be influential at the time they appeared. To contrast SMART with classical meta-analysis, we re-analyze two meta-analysis datasets, from psychology and labor economics. One assembles studies using a single methodology; the other contains studies that predate or follow an important methodological innovation. Our formalization of sequential learning highlights the importance of methodological innovation that might otherwise be overlooked by classical meta-analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14996v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas M. Mikhaeil, Donald P. Green, David Blei</dc:creator>
    </item>
    <item>
      <title>Hazard-Based Targeted Maximum Likelihood Estimation for Survival in Resampling Designs</title>
      <link>https://arxiv.org/abs/2511.15045</link>
      <description>arXiv:2511.15045v1 Announce Type: new 
Abstract: Survival is a key metric for evaluating standards of care for people living with HIV. In resource-limited settings, high rates of loss to follow-up (LTFU) often result in underestimation of mortality when only observed deaths are considered. Resampling, which tracks a subset of LTFU patients to ascertain their outcomes, mitigates bias and improves survival estimates. However, common estimators for survival in resampling designs, such as weighted Kaplan-Meier (KM), fail to leverage covariate information collected during repeated clinic visits, even though this information is highly predictive of survival. We propose a Targeted Maximum Likelihood Estimator (TMLE) for survival in resampling designs, which addresses these limitations by leveraging baseline and longitudinal covariates to achieve greater efficiency. Our TMLE is a plug-in estimator and is robust to misspecification of the initial model for the conditional hazard of death, guaranteeing consistency of our estimator due to known resampling probabilities. We present: (1) a fully efficient TMLE for data from resampling studies with fixed follow-up time for all participants and (2) an inverse probability of censoring weighted (IPCW) TMLE that accounts for varied follow-up times by stratifying on patients with sufficient follow-up to evaluate survival. This IPCW-TMLE can be made highly efficient through nonparametric or targeted estimation of the follow-up censoring mechanism. In simulations, our TMLE reduced variance by up to 55% compared with the commonly used weighted KM estimator while preserving nominal confidence interval coverage. These findings demonstrate the potential of our TMLE to improve survival estimation in resampling designs, offering a robust and resource-efficient framework for HIV research. Keywords: Resampling designs, Survival analysis, Targeted Maximum Likelihood Estimation, Inverse probability weighting</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15045v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kirsten E. Landsiedel (Division of Biostatistics, University of California, Berkeley), Rachael V. Phillips (Division of Biostatistics, University of California, Berkeley), Maya L. Petersen (Division of Biostatistics, University of California, Berkeley), Mark J. van der Laan (Division of Biostatistics, University of California, Berkeley)</dc:creator>
    </item>
    <item>
      <title>Classification Trees with Valid Inference via the Exponential Mechanism</title>
      <link>https://arxiv.org/abs/2511.15068</link>
      <description>arXiv:2511.15068v1 Announce Type: new 
Abstract: Decision trees are widely used for non-linear modeling, as they capture interactions between predictors while producing inherently interpretable models. Despite their popularity, performing inference on the non-linear fit remains largely unaddressed. This paper focuses on classification trees and makes two key contributions. First, we introduce a novel tree-fitting method that replaces the greedy splitting of the predictor space in standard tree algorithms with a probabilistic approach. Each split in our approach is selected according to sampling probabilities defined by an exponential mechanism, with a temperature parameter controlling its deviation from the deterministic choice given data. Second, while our approach can fit a tree that, with high probability, approximates the fit produced by standard tree algorithms at high temperatures, it is not merely predictive- unlike standard algorithms, it enables valid inference by taking into account the highly adaptive tree structure. Our method produces pivots directly from the sampling probabilities in the exponential mechanism. In theory, our pivots allow asymptotically valid inference on the parameters in the predictive fit, and in practice, our method delivers powerful inference without sacrificing predictive accuracy, in contrast to data splitting methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15068v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soham Bakshi, Snigdha Panigrahi</dc:creator>
    </item>
    <item>
      <title>Individualized Prediction Bands in Causal Inference with Continuous Treatments</title>
      <link>https://arxiv.org/abs/2511.15075</link>
      <description>arXiv:2511.15075v1 Announce Type: new 
Abstract: Individualized treatments are crucial for optimal decision making and treatment allocation, specifically in personalized medicine based on the estimation of an individual's dose-response curve across a continuum of treatment levels, e.g., drug dosage. Current works focus on conditional mean and median estimates, which are useful but do not provide the full picture. We propose viewing causal inference with a continuous treatment as a covariate shift. This allows us to leverage existing weighted conformal prediction methods with both quantile and point estimates to compute individualized uncertainty quantification for dose-response curves. Our method, individualized prediction bands (IPB), is demonstrated via simulations and a real data analysis, which demonstrates the additional medical expenditure caused by continued smoking for selected individuals. The results demonstrate that IPB provides an effective solution to a gap in individual dose-response uncertainty quantification literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15075v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Max Sampson, Kung-Sik Chan</dc:creator>
    </item>
    <item>
      <title>Debiasing hazard-based, time-varying vaccine effects using vaccine-irrelevant infections: An observational extension of a pivotal Phase 3 COVID-19 vaccine efficacy trial</title>
      <link>https://arxiv.org/abs/2511.15099</link>
      <description>arXiv:2511.15099v1 Announce Type: new 
Abstract: Understanding how vaccine effectiveness (VE) changes over time can provide evidence-based guidance for public health decision making. While commonly reported by practitioners, time-varying VE estimates obtained using Cox regression are vul- nerable to hidden biases. To address these limitations, we describe how to leverage vaccine-irrelevant infections to identify hazard-based, time-varying VE in the pres- ence of unmeasured confounding and selection bias. We articulate assumptions under which our approach identifies a causal effect of an intervention deferring vaccination and interaction with the community in which infections circulate. We develop sieve and efficient influence curve-based estimators and discuss imposing monotone shape constraints and estimating VE against multiple variants. As a case study, we examine the observational booster phase of the Coronavirus Vaccine Efficacy (COVE) trial of the Moderna mRNA-1273 COVID-19 vaccine which used symptom-triggered multi- plex PCR testing to identify acute respiratory illnesses (ARIs) caused by SARS-CoV-2 and 20 off-target pathogens previously identified as compelling negative controls for COVID-19. Accounting for vaccine-irrelevant ARIs supported that the mRNA-1273 booster was more effective and durable against Omicron COVID-19 than suggested by Cox regression. Our work offers an approach to mitigate bias in hazard-based, time- varying treatment effects in randomized and non-randomized studies using negative controls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15099v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ethan Ashby, Dean Follmann, Holly Janes, Peter B. Gilbert, Ting Ye, Lindsey R. Baden, Hana M. El Sahly, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Robust outlier-adjusted mean-shift estimation of state-space models</title>
      <link>https://arxiv.org/abs/2511.15155</link>
      <description>arXiv:2511.15155v1 Announce Type: new 
Abstract: State-space models (SSMs) provide a flexible framework for modelling time series data, but their reliance on Gaussian error assumptions makes them highly sensitive to outliers. We propose a robust estimation method, ROAMS, that mitigates the influence of additive outliers by introducing shift parameters at each timepoint in the observation equation of the SSM. These parameters allow the model to attribute non-zero shifts to outliers while leaving clean observations unaffected. ROAMS then enables automatic outlier detection, through the addition of a penalty term on the number of flagged outlying timepoints in the objective function, and simultaneous estimation of model parameters. We apply the method to robustly estimate SSMs on both simulated data and real-world animal location-tracking data, demonstrating its ability to produce more reliable parameter estimates than classical methods and other benchmark methods. In addition to improved robustness, ROAMS offers practical diagnostic tools, including BIC curves for selecting tuning parameters and visualising outlier structure. These features make our approach broadly useful for researchers and practitioners working with contaminated time series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15155v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rajan Shankar, Ines Wilms, Jakob Raymaekers, Garth Tarr</dc:creator>
    </item>
    <item>
      <title>Testing relevant difference in high-dimensional linear regression with applications to detect transferability</title>
      <link>https://arxiv.org/abs/2511.15236</link>
      <description>arXiv:2511.15236v1 Announce Type: new 
Abstract: Most of researchers on testing a significance of coefficient $\ubeta$ in high-dimensional linear regression models consider the classical hypothesis testing problem $H_0^{c}: \ubeta=\uzero \mbox{ versus } H_1^{c}: \ubeta \neq \uzero$. We take a different perspective and study the testing problem with the null hypothesis of no relevant difference between $\ubeta$ and $\uzero$, that is, $H_0: \|\ubeta\|\leq \delta_0 \mbox{ versus } H_1: \|\ubeta\|&gt; \delta_0$, where $\delta_0$ is a prespecified small constant. This testing problem is motivated by the urgent requirement to detect the transferability of source data in the transfer learning framework. We propose a novel test procedure incorporating the estimation of the largest eigenvalue of a high-dimensional covariance matrix with the assistance of the random matrix theory. In the more challenging setting in the presence of high-dimensional nuisance parameters, we establish the asymptotic normality for the proposed test statistics under both the null and alternative hypotheses. By applying the proposed test approaches to detect the transferability of source data, the unified transfer learning models simultaneously achieve lower estimation and prediction errors with comparison to existing methods. We study the finite-sample properties of the new test by means of simulation studies and illustrate its performance by analyzing the GTEx data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15236v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xu Liu</dc:creator>
    </item>
    <item>
      <title>Location--Scale Calibration for Generalized Posterior</title>
      <link>https://arxiv.org/abs/2511.15320</link>
      <description>arXiv:2511.15320v1 Announce Type: new 
Abstract: General Bayesian updating replaces the likelihood with a loss scaled by a learning rate, but posterior uncertainty can depend sharply on that scale. We propose a simple post-processing that aligns generalized posterior draws with their asymptotic target, yielding uncertainty quantification that is invariant to the learning rate. We prove total-variation convergence for generalized posteriors with an effective sample size, allowing sample-size-dependent priors, non-i.i.d. observations, and convex penalties under model misspecification. Within this framework, we justify and extend the open-faced sandwich adjustment (Shaby, 2014), provide general theoretical guarantees for its use within generalized Bayes, and extend it from covariance rescaling to a location--scale calibration whose draws converge in total variation to the target for any learning rate. In our empirical illustration, calibrated draws maintain stable coverage, interval width, and bias over orders of magnitude in the learning rate and closely track frequentist benchmarks, whereas uncalibrated posteriors vary markedly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15320v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shu Tamano, Yui Tomo</dc:creator>
    </item>
    <item>
      <title>BaGGLS: A Bayesian Shrinkage Framework for Interpretable Modeling of Interactions in High-Dimensional Biological Data</title>
      <link>https://arxiv.org/abs/2511.15330</link>
      <description>arXiv:2511.15330v1 Announce Type: new 
Abstract: Biological data sets are often high-dimensional, noisy, and governed by complex interactions among sparse signals. This poses major challenges for interpretability and reliable feature selection. Tasks such as identifying motif interactions in genomics exemplify these difficulties, as only a small subset of biologically relevant features (e.g., motifs) are typically active, and their effects are often non-linear and context-dependent. While statistical approaches often result in more interpretable models, deep learning models have proven effective in modeling complex interactions and prediction accuracy, yet their black-box nature limits interpretability. We introduce BaGGLS, a flexible and interpretable probabilistic binary regression model designed for high-dimensional biological inference involving feature interactions. BaGGLS incorporates a Bayesian group global-local shrinkage prior, aligned with the group structure introduced by interaction terms. This prior encourages sparsity while retaining interpretability, helping to isolate meaningful signals and suppress noise. To enable scalable inference, we employ a partially factorized variational approximation that captures posterior skewness and supports efficient learning even in large feature spaces. In extensive simulations, we can show that BaGGLS outperforms the other methods with regard to interaction detection and is many times faster than MCMC sampling under the horseshoe prior. We also demonstrate the usefulness of BaGGLS in the context of interaction discovery from motif scanner outputs and noisy attribution scores from deep learning models. This shows that BaGGLS is a promising approach for uncovering biologically relevant interaction patterns, with potential applicability across a range of high-dimensional tasks in computational biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15330v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.ML</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marta S. Lemanczyk, Lucas Kock, Johanna Schlimme, Nadja Klein, Bernhard Y. Renard</dc:creator>
    </item>
    <item>
      <title>Utilizing subgroup information in random-effects meta-analysis of few studies</title>
      <link>https://arxiv.org/abs/2511.15366</link>
      <description>arXiv:2511.15366v1 Announce Type: new 
Abstract: Random-effects meta-analyses are widely used for evidence synthesis in medical research. However, conventional methods based on large-sample approximations often exhibit poor performance in case of very few studies (e.g., 2 to 4), which is very common in practice. Existing methods aiming to improve small-sample performance either still suffer from poor estimates of heterogeneity or result in very wide confidence intervals. Motivated by meta-analyses evaluating surrogate outcomes, where units nested within a trial are often exploited when the number of trials is small, we propose an inference approach based on a common-effect estimator synthesizing data from the subgroup-level instead of the study-level. Two DerSimonian-Laird type heterogeneity estimators are derived using the subgroup-level data, and are incorporated into the Henmi-Copas type variance to adequately reflect variance components. We considered t-quantile based intervals to account for small-sample properties and used flexible degrees of freedom to reduce interval lengths. A comprehensive simulation is conducted to study the performance of our methods depending on various magnitudes of subgroup effects as well as subgroup prevalences. Some general recommendations are provided on how to select the subgroups, and methods are illustrated using two example applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15366v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ao Huang, Christian R\"over, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Testing Conditional Independence via the Spectral Generalized Covariance Measure: Beyond Euclidean Data</title>
      <link>https://arxiv.org/abs/2511.15453</link>
      <description>arXiv:2511.15453v1 Announce Type: new 
Abstract: We propose a conditional independence (CI) test based on a new measure, the \emph{spectral generalized covariance measure} (SGCM). The SGCM is constructed by approximating the basis expansion of the squared norm of the conditional cross-covariance operator, using data-dependent bases obtained via spectral decompositions of empirical covariance operators. This construction avoids direct estimation of conditional mean embeddings and reduces the problem to scalar-valued regressions, resulting in robust finite-sample size control. Theoretically, we derive the limiting distribution of the SGCM statistic, establish the validity of a wild bootstrap for inference, and obtain uniform asymptotic size control under doubly robust conditions. As an additional contribution, we show that exponential kernels induced by continuous semimetrics of negative type are characteristic on general Polish spaces -- with extensions to finite tensor products -- thereby providing a foundation for applying our test and other kernel methods to complex objects such as distribution-valued data and curves on metric spaces. Extensive simulations indicate that the SGCM-based CI test attains near-nominal size and exhibits power competitive with or superior to state-of-the-art alternatives across a range of challenging scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15453v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryunosuke Miyazaki, Yoshimasa Uematsu</dc:creator>
    </item>
    <item>
      <title>FDR Control via Neural Networks under Covariate-Dependent Symmetric Nulls</title>
      <link>https://arxiv.org/abs/2511.15495</link>
      <description>arXiv:2511.15495v1 Announce Type: new 
Abstract: In modern multiple hypothesis testing, the availability of covariate information alongside the primary test statistics has motivated the development of more powerful and adaptive inference methods. However, most existing approaches rely on p-values that are precomputed under the assumption that their null distributions are independent of the covariates. In this paper, we propose a framework that derives covariate-adaptive p-values from the assumption of a symmetric null distribution of the primary variable given the covariates, without imposing any parametric assumptions. Building on these data-driven p-values, we employ a neural network model to learn a covariate-adaptive rejection threshold via the mirror estimation principle, optimizing the number of discoveries while maintaining valid false discovery rate control. Furthermore, our estimation of the conditional null distribution enables the computation of p-values directly from the raw data. The proposed method provides a principled way to derive covariate-adjusted p-values from raw data and allows seamless integration with previously established p-value based procedures. Simulation studies show that the proposed method outperforms existing approaches in terms of power. We further illustrate its applicability through two real data analyses: age-specific blood pressure data and U.S. air pollution data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15495v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taehyoung Kim, Seohwa Hwang, Junyong Park</dc:creator>
    </item>
    <item>
      <title>Variance-reduced extreme value index estimators using control variates in a semi-supervised setting</title>
      <link>https://arxiv.org/abs/2511.15561</link>
      <description>arXiv:2511.15561v1 Announce Type: new 
Abstract: The estimation of the Extreme Value Index (EVI) is fundamental in extreme value analysis but suffers from high variance due to reliance on only a few extreme observations. We propose a control variates based transfer learning approach in a semi-supervised framework, where a small set of coupled target and source observations is combined with abundant unpaired source data. By expressing the Hill estimator of the target EVI as a ratio of means, we apply approximate control variates to both numerator and denominator, with jointly optimized coefficients that guarantee variance reduction without introducing bias. We show theoretically and through simulations that the asymptotic relative variance reduction of the transferred Hill estimator is proportional to the tail dependence between the target and source variables and independent of their EVI values. Thus, substantial variance reduction can be achieved even without similarity in tail heaviness of the target and source distributions. The proposed approach can be extended to other EVI estimators expressed with ratio of means, as demonstrated on the moment estimator. The practical value of the proposed method is illustrated on multi-fidelity water surge and ice accretion datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15561v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Louison Bocquet-Nouaille, J\'er\^ome Morio, Benjamin Bobbia</dc:creator>
    </item>
    <item>
      <title>Convex Clustering Redefined: Robust Learning with the Median of Means Estimator</title>
      <link>https://arxiv.org/abs/2511.14784</link>
      <description>arXiv:2511.14784v1 Announce Type: cross 
Abstract: Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14784v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav De, Koustav Chowdhury, Bibhabasu Mandal, Sagar Ghosh, Swagatam Das, Debolina Paul, Saptarshi Chakraborty</dc:creator>
    </item>
    <item>
      <title>The Asymptotic Equivalence of Level-Based and Share-Based Loss Functions</title>
      <link>https://arxiv.org/abs/2511.14812</link>
      <description>arXiv:2511.14812v1 Announce Type: cross 
Abstract: Level-based and share-based loss functions are asymptotically equivalent if, in the limit, their averages converge almost surely to a constant ratio. These loss functions take a target value and its realization as arguments and are often used to measure accuracy. The equivalence is proved for a large class of loss functions, the weighted exponentiated functions, when the weights are decomposable as a particular product form. An upshot is that when losses are averaged for a large number of units, differences in ratios and, hence, ranks, are negligible, when the average (or summed) difference between the target values and their realizations is around zero. This implies the almost sure asymptotic convergence of numerical and distributive accuracy when using these loss functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14812v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Charles D. Coleman</dc:creator>
    </item>
    <item>
      <title>Uncovering Treatment Effect Heterogeneity in Pragmatic Gerontology Trials</title>
      <link>https://arxiv.org/abs/2511.14893</link>
      <description>arXiv:2511.14893v1 Announce Type: cross 
Abstract: Detecting heterogeneity in treatment response enriches the interpretation of gerontologic trials. In aging research, estimating the effect of the intervention on clinically meaningful outcomes faces analytical challenges when it is truncated by death. For example, in the Whole Systems Demonstrator trial, a large cluster-randomized study evaluating telecare among older adults, the overall effect of the intervention on quality of life was found to be null. However, this marginal intervention estimate obscures potential heterogeneity of individuals responding to the intervention, particularly among those who survive to the end of follow-up. To explore this heterogeneity, we adopt a causal framework grounded in principal stratification, targeting the Survivor Average Causal Effect (SACE)-the treatment effect among "always-survivors," or those who would survive regardless of treatment assignment. We extend this framework using Bayesian Additive Regression Trees (BART), a nonparametric machine learning method, to flexibly model both latent principal strata and stratum-specific potential outcomes. This enables the estimation of the Conditional SACE (CSACE), allowing us to uncover variation in treatment effects across subgroups defined by baseline characteristics. Our analysis reveals that despite the null average effect, some subgroups experience distinct quality of life benefits (or lack thereof) from telecare, highlighting opportunities for more personalized intervention strategies. This study demonstrates how embedding machine learning methods, such as BART, within a principled causal inference framework can offer deeper insights into trial data with complex features including truncation by death and clustering-key considerations in analyzing pragmatic gerontology trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14893v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changjun Li, Heather Allore, Michael O. Harhay, Fan Li, Guangyu Tong</dc:creator>
    </item>
    <item>
      <title>Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis</title>
      <link>https://arxiv.org/abs/2511.14922</link>
      <description>arXiv:2511.14922v1 Announce Type: cross 
Abstract: Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14922v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pranay Kumar Peddi, Dhrubajyoti Ghosh</dc:creator>
    </item>
    <item>
      <title>Design-based finite-sample analysis for regression adjustment</title>
      <link>https://arxiv.org/abs/2511.15161</link>
      <description>arXiv:2511.15161v1 Announce Type: cross 
Abstract: In randomized experiments, regression adjustment leverages covariates to improve the precision of average treatment effect (ATE) estimation without requiring a correctly specified outcome model. Although well understood in low-dimensional settings, its behavior in high-dimensional regimes -- where the number of covariates $p$ may exceed the number of observations $n$ -- remains underexplored. Furthermore, existing theory is largely asymptotic, providing limited guidance for finite-sample inference. We develop a design-based, non-asymptotic analysis of the regression-adjusted ATE estimator under complete randomization. Specifically, we derive finite-sample-valid confidence intervals with explicit, instance-adaptive widths that remain informative even when $p &gt; n$. These intervals rely on oracle (population-level) quantities, and we also outline data-driven envelopes that are computable from observed data. Our approach hinges on a refined swap sensitivity analysis: stochastic fluctuation is controlled via a variance-adaptive Doob martingale and Freedman's inequality, while design bias is bounded using Stein's method of exchangeable pairs. The analysis suggests how covariate geometry governs concentration and bias through leverages and cross-leverages, shedding light on when and how regression adjustment improves on the difference-in-means baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15161v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dogyoon Song</dc:creator>
    </item>
    <item>
      <title>Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss</title>
      <link>https://arxiv.org/abs/2511.15332</link>
      <description>arXiv:2511.15332v1 Announce Type: cross 
Abstract: In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise.
  Our method is implemented in the \texttt{R} package \texttt{heavylasso} available on Github: https://github.com/tienmt/heavylasso</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15332v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>The Tien Mai</dc:creator>
    </item>
    <item>
      <title>Proximal Approximate Inference in State-Space Models</title>
      <link>https://arxiv.org/abs/2511.15409</link>
      <description>arXiv:2511.15409v1 Announce Type: cross 
Abstract: We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15409v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Hany Abdulsamad, \'Angel F. Garc\'ia-Fern\'andez, Simo S\"arkk\"a</dc:creator>
    </item>
    <item>
      <title>Tractable Estimation of Nonlinear Panels with Interactive Fixed Effects</title>
      <link>https://arxiv.org/abs/2511.15427</link>
      <description>arXiv:2511.15427v1 Announce Type: cross 
Abstract: Interactive fixed effects are routinely controlled for in linear panel models. While an analogous fixed effects (FE) estimator for nonlinear models has been available in the literature (Chen, Fernandez-Val and Weidner, 2021), it sees much more limited use in applied research because its implementation involves solving a high-dimensional non-convex problem. In this paper, we complement the theoretical analysis of Chen, Fernandez-Val and Weidner (2021) by providing a new computationally efficient estimator that is asymptotically equivalent to their estimator. Unlike the previously proposed FE estimator, our estimator avoids solving a high-dimensional optimization problem and can be feasibly computed in large nonlinear panels. Our proposed method involves two steps. In the first step, we convexify the optimization problem using nuclear norm regularization (NNR) and obtain preliminary NNR estimators of the parameters, including the fixed effects. Then, we find the global solution of the original optimization problem using a standard gradient descent method initialized at these preliminary estimates. Thus, in practice, one can simply combine our computationally efficient estimator with the inferential theory provided in Chen, Fernandez-Val and Weidner (2021) to construct confidence intervals and perform hypothesis testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15427v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrei Zeleneev, Weisheng Zhang</dc:creator>
    </item>
    <item>
      <title>A new non-parametric Kendall's tau for matrix-valued elliptical observations</title>
      <link>https://arxiv.org/abs/2207.09633</link>
      <description>arXiv:2207.09633v2 Announce Type: replace 
Abstract: In this article, we first propose generalized row/column matrix Kendall's tau for matrix-variate observations that are ubiquitous in areas such as finance and medical imaging. For a random matrix following a matrix-variate elliptically contoured distribution, we show that the eigenspaces of the proposed row/column matrix Kendall's tau coincide with those of the row/column scatter matrix respectively, with the same descending order of the eigenvalues. We perform eigenvalue decomposition to the generalized row/column matrix Kendall's tau for recovering the loading spaces of the matrix factor model. We also propose to estimate the pair of the factor numbers by exploiting the eigenvalue-ratios of the row/column matrix Kendall's tau. Theoretically, we derive the convergence rates of the estimators for loading spaces, factor scores and common components, and prove the consistency of the estimators for the factor numbers without any moment constraints on the idiosyncratic errors. Thorough simulation studies are conducted to show the higher degree of robustness of the proposed estimators over the existing ones. Analysis of a financial dataset of asset returns and a medical imaging dataset associated with COVID-19 illustrate the empirical usefulness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.09633v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong He, Yalin Wang, Long Yu, Wang Zhou, Wen-Xin Zhou</dc:creator>
    </item>
    <item>
      <title>Regression Discontinuity Designs Under Interference</title>
      <link>https://arxiv.org/abs/2410.02727</link>
      <description>arXiv:2410.02727v5 Announce Type: replace 
Abstract: We extend the continuity-based framework to Regression Discontinuity Designs (RDDs) to identify and estimate causal effects under interference when units are connected through a network. Assignment to an "effective treatment," combining the individual treatment and a summary of neighbors' treatments, is determined by the unit's score and those of interfering units, yielding a multiscore RDD with complex, multidimensional boundaries. We characterize these boundaries and derive assumptions to identify boundary causal effects. We develop a distance-based nonparametric estimator and establish its asymptotic properties under restrictions on the network degree distribution. We show that while direct effects converge at the standard rate, the rate for indirect effects depends on the number of scores fixed at the cutoff. Finally, we propose a variance estimator accounting for network correlation and apply our method to PROGRESA data to estimate the direct and indirect effects of cash transfers on school attendance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02727v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elena Dal Torrione, Tiziano Arduini, Laura Forastiere</dc:creator>
    </item>
    <item>
      <title>Median Based Unit Weibull Distribution (MBUW): Do the Higher Order Probability Weighted Moments (PWM) Add More Information over the Lower Order PWM in Parameter Estimation</title>
      <link>https://arxiv.org/abs/2412.07404</link>
      <description>arXiv:2412.07404v3 Announce Type: replace 
Abstract: In the present paper, Probability weighted moments (PWMs) method for parameter estimation of the median based unit weibull (MBUW) distribution is discussed. The most widely used first order PWMs is compared with the higher order PWMs for parameter estimation of (MBUW) distribution. Asymptotic distribution of this PWM estimator is derived. This comparison is illustrated using real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07404v3</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohammed Attia</dc:creator>
    </item>
    <item>
      <title>Joint Registration and Conformal Prediction for Partially Observed Functional Data</title>
      <link>https://arxiv.org/abs/2502.15000</link>
      <description>arXiv:2502.15000v2 Announce Type: replace 
Abstract: Predicting missing segments in partially observed functions is challenging due to infinite-dimensionality, complex dependence within and across observations, and irregular noise. These challenges are further exacerbated by the existence of two distinct sources of variation in functional data, termed amplitude (variation along the $y$-axis) and phase (variation along the $x$-axis). While registration can disentangle them from complete functional data, the process is more difficult for partial observations. Thus, existing methods for functional data prediction often ignore phase variation. Furthermore, they rely on strong parametric assumptions, and require either precise model specifications or computationally intensive techniques, such as bootstrapping, to construct prediction intervals. To tackle this problem, we propose a unified registration and prediction approach for partially observed functions under the conformal prediction framework, which separately focuses on the amplitude and phase components. By leveraging split conformal methods, our approach integrates registration and prediction while ensuring exchangeability through carefully constructed predictor-response pairs. Using a neighborhood smoothing algorithm, the framework produces pointwise prediction bands with finite-sample marginal coverage guarantees under weak assumptions. The method is easy to implement, computationally efficient, and suitable for parallelization. Numerical studies and real-world data examples clearly demonstrate the effectiveness and practical utility of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15000v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fangyi Wang, Sebastian Kurtek, Yuan Zhang</dc:creator>
    </item>
    <item>
      <title>A Statistician's Overview of Physics-Informed Neural Networks for Spatio-Temporal Data</title>
      <link>https://arxiv.org/abs/2507.14336</link>
      <description>arXiv:2507.14336v2 Announce Type: replace 
Abstract: The recent success of deep neural network models with physical constraints (so-called, Physics-Informed Neural Networks, PINNs) has led to renewed interest in the incorporation of mechanistic information in predictive models. Statisticians and others have long been interested in this problem, which has led to several practical and innovative solutions dating back decades. In this overview, we focus on the problem of data-driven prediction and inference of dynamic spatio-temporal processes that include mechanistic information, such as would be available from partial differential equations, with a strong focus on the quantification of uncertainty associated with data, process, and parameters. We give a brief review of several paradigms and focus our attention on Bayesian implementations given they naturally accommodate uncertainty quantification. We then show that it is straight-forward to include the Bayesian PINN (B-PINN) within the Bayesian hierarchical model (BHM) framework that has long been considered for modeling dynamic spatio-temporal processes. Such a BHM-PINN is illustrated via a simulation study in which a latent nonlinear Burgers' equation PDE governs the dynamics of Poisson distributed spatio-temporal data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14336v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher K. Wikle, Joshua North, Giri Gopalan, Myungsoo Yoo</dc:creator>
    </item>
    <item>
      <title>Small Area Estimation of General Indicators in Off-Census Years</title>
      <link>https://arxiv.org/abs/2510.10812</link>
      <description>arXiv:2510.10812v2 Announce Type: replace 
Abstract: We propose small area estimators of general indicators in off-census years, which avoid the use of deprecated census microdata, but are nearly optimal in census years. The procedure is based on replacing the obsolete census file with a larger unit-level survey that adequately covers the areas of interest and contains the values of useful auxiliary variables. However, the minimal data requirement of the proposed method is a single survey with microdata on the target variable and suitable auxiliary variables for the period of interest. We also develop an estimator of the mean squared error (MSE) that accounts for the uncertainty introduced by the large survey used to replace the census of auxiliary information. Our empirical results indicate that the proposed predictors perform clearly better than the alternative predictors when census data are outdated, and are very close to optimal ones when census data are correct. They also illustrate that the proposed total MSE estimator corrects for the bias of purely model-based MSE estimators that do not account for the large survey uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10812v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Acero, Isabel Molina, J. Miguel Mar\'in</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing for the dimension of random geometric graph</title>
      <link>https://arxiv.org/abs/2510.11844</link>
      <description>arXiv:2510.11844v2 Announce Type: replace 
Abstract: Random geometric graphs (RGGs) offer a powerful tool for analyzing the geometric and dependence structures in real-world networks. For example, it has been observed that RGGs are a good model for protein-protein interaction networks. In RGGs, nodes are randomly distributed over an $m$-dimensional metric space, and edges connect the nodes if and only if their distance is less than some threshold. When fitting RGGs to real-world networks, the first step is probably to input or estimate the dimension $m$. However, it is not clear whether the prespecified dimension is equal to the true dimension. In this paper, we investigate this problem using hypothesis testing. Under the null hypothesis, the dimension is equal to a specific value, while the alternative hypothesis asserts the dimension is not equal to that value. We propose the first statistical test. Under the null hypothesis, the proposed test statistic converges in law to the standard normal distribution, and under the alternative hypothesis, the test statistic is unbounded in probability. We derive the asymptotic distribution by leveraging the asymptotic theory of degenerate U-statistics with kernel function dependent on the number of nodes. This approach differs significantly from prevailing methods used in network hypothesis testing problems. Moreover, we also propose an efficient approach to compute the test statistic based on the adjacency matrix. Simulation studies show that the proposed test performs well. We also apply the proposed test to multiple real-world networks to test their dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11844v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mingao Yuan, Feng Yu</dc:creator>
    </item>
    <item>
      <title>From Global to Local Correlation: Geometric Decomposition of Statistical Inference</title>
      <link>https://arxiv.org/abs/2511.04599</link>
      <description>arXiv:2511.04599v4 Announce Type: replace 
Abstract: Understanding feature-outcome associations in high-dimensional data remains
  challenging when relationships vary across subpopulations, yet standard
  methods assuming global associations miss context-dependent patterns, reducing
  statistical power and interpretability. We develop a geometric decomposition
  framework offering two strategies for partitioning inference problems into
  regional analyses on data-derived Riemannian graphs. Gradient flow
  decomposition uses path-monotonicity-validated discrete Morse theory to
  partition samples into gradient flow cells where outcomes exhibit monotonic
  behavior. Co-monotonicity decomposition utilizes vertex-level coefficients
  that provide context-dependent versions of the classical Pearson correlation:
  these coefficients measure edge-based directional concordance between outcome
  and features, or between feature pairs, defining embeddings of samples into
  association space. These embeddings induce Riemannian k-NN graphs on which
  biclustering identifies co-monotonicity cells (coherent regions) and feature
  modules. This extends naturally to multi-modal integration across multiple
  feature sets. Both strategies apply independently or jointly, with Bayesian
  posterior sampling providing credible intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04599v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pawel Gajer, Jacques Ravel</dc:creator>
    </item>
    <item>
      <title>On robust Bayesian causal inference</title>
      <link>https://arxiv.org/abs/2511.13895</link>
      <description>arXiv:2511.13895v2 Announce Type: replace 
Abstract: This paper develops a Bayesian framework for robust causal inference from longitudinal observational data. Many contemporary methods rely on structural assumptions, such as factor models, to adjust for unobserved confounding, but they can lead to biased causal estimands when mis-specified. We focus on directly estimating time--unit--specific causal effects and use generalised Bayesian inference to quantify model mis-specification and adjust for it, while retaining interpretable posterior inference. We select the learning rate~$\omega$ based on a proper scoring rule that jointly evaluates point and interval accuracy of the causal estimand, thus providing a coherent, decision-theoretic foundation for tuning~$\omega$. Simulation studies and applications to real data demonstrate improved calibration, sharpness, and robustness in estimating causal effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13895v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angelos Alexopoulos, Nikolaos Demiris</dc:creator>
    </item>
    <item>
      <title>Explicit modeling of density dependence in spatial capture-recapture models</title>
      <link>https://arxiv.org/abs/2412.09431</link>
      <description>arXiv:2412.09431v2 Announce Type: replace-cross 
Abstract: Density dependence occurs at the individual level and thus is greatly influenced by spatial local heterogeneity in habitat conditions. However, density dependence is often evaluated at the population level, leading to difficulties or even controversies in detecting such a process. Bayesian individual-based models such as spatial capture-recapture (SCR) models provide opportunities to study density dependence at the individual level, but such an approach remains to be developed and evaluated. In this study, we developed a SCR model that links habitat use to apparent survival and recruitment through density dependent processes at the individual level. Using simulations, we found that the model can properly inform habitat use, but tends to underestimate the effect of density dependence on apparent survival and recruitment. The reason for such underestimations is likely due to the difficulties of the current model in identifying the locations of unobserved individuals without using environmental covariates to inform these locations. How to accurately estimate the locations of unobserved individuals, and thus density dependence, remains a challenging topic in spatial statistics and statistical ecology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09431v2</guid>
      <category>q-bio.PE</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qing Zhao, Yunyi Shen</dc:creator>
    </item>
    <item>
      <title>DEViaN-LM: An R Package for Detecting Abnormal Values in the Gaussian Linear Model</title>
      <link>https://arxiv.org/abs/2509.02202</link>
      <description>arXiv:2509.02202v2 Announce Type: replace-cross 
Abstract: The DEViaN-LM is a R package that allows to detect the values poorly explained by a Gaussian linear model. The procedure is based on the maximum of the absolute value of the studentized residuals, which is a free statistic of the parameters of the model. This approach makes it possible to generalize several procedures used to detect abnormal values during longitudinal monitoring of certain biological markers. In this article, we describe the method used, and we show how to implement it on different real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02202v2</guid>
      <category>cs.MS</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geoffroy Berthelot (IRMES - URP\_7329, RELAIS), Guillaume Sauli\`ere (IRMES - URP\_7329), J\'er\^ome Dedecker (MAP5 - UMR 8145)</dc:creator>
    </item>
  </channel>
</rss>
