<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Jan 2025 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Semiparametrics via parametrics and contiguity</title>
      <link>https://arxiv.org/abs/2501.09483</link>
      <description>arXiv:2501.09483v1 Announce Type: new 
Abstract: Inference on the parametric part of a semiparametric model is no trivial task. On the other hand, if one approximates the infinite dimensional part of the semiparametric model by a parametric function, one obtains a parametric model that is in some sense close to the semiparametric model; and inference may proceed by the method of maximum likelihood. Under regularity conditions, and assuming that the approximating parametric model in fact generated the data, the ensuing maximum likelihood estimator is asymptotically normal and efficient (in the approximating parametric model). Thus one obtains a sequence of asymptotically normal and efficient estimators in a sequence of growing parametric models that approximate the semiparametric model and, intuitively, the limiting {`}semiparametric{'} estimator should be asymptotically normal and efficient as well. In this paper we make this intuition rigorous. Consequently, we are able to move much of the semiparametric analysis back into classical parametric terrain, and then translate our parametric results back to the semiparametric world by way of contiguity. Our approach departs from the sieve literature by being more specific about the approximating parametric models, by working under these when treating the parametric models, and by taking advantage of the mutual contiguity between the parametric and semiparametric models to lift conclusions about the former to conclusions about the latter. We illustrate our theory with two canonical examples of semiparametric models, namely the partially linear regression model and the Cox regression model. An upshot of our theory is a new, relatively simple, and rather parametric proof of the efficiency of the Cox partial likelihood estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09483v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Lee, Emil A. Stoltenberg, Per A. Mykland</dc:creator>
    </item>
    <item>
      <title>Generative Models with ELBOs Converging to Entropy Sums</title>
      <link>https://arxiv.org/abs/2501.09022</link>
      <description>arXiv:2501.09022v1 Announce Type: cross 
Abstract: The evidence lower bound (ELBO) is one of the most central objectives for probabilistic unsupervised learning. For the ELBOs of several generative models and model classes, we here prove convergence to entropy sums. As one result, we provide a list of generative models for which entropy convergence has been shown, so far, along with the corresponding expressions for entropy sums. Our considerations include very prominent generative models such as probabilistic PCA, sigmoid belief nets or Gaussian mixture models. However, we treat more models and entire model classes such as general mixtures of exponential family distributions. Our main contributions are the proofs for the individual models. For each given model we show that the conditions stated in Theorem 1 or Theorem 2 of [arXiv:2209.03077] are fulfilled such that by virtue of the theorems the given model's ELBO is equal to an entropy sum at all stationary points. The equality of the ELBO at stationary points applies under realistic conditions: for finite numbers of data points, for model/data mismatches, at any stationary point including saddle points etc, and it applies for any well behaved family of variational distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09022v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Warnken, Dmytro Velychko, Simon Damm, Asja Fischer, J\"org L\"ucke</dc:creator>
    </item>
    <item>
      <title>Estimating shared subspace with AJIVE: the power and limitation of multiple data matrices</title>
      <link>https://arxiv.org/abs/2501.09336</link>
      <description>arXiv:2501.09336v1 Announce Type: cross 
Abstract: Integrative data analysis often requires disentangling joint and individual variations across multiple datasets, a challenge commonly addressed by the Joint and Individual Variation Explained (JIVE) model. While numerous methods have been developed to estimate the shared subspace under JIVE, the theoretical understanding of their performance remains limited, particularly in the context of multiple matrices and varying levels of subspace misalignment. This paper bridges this gap by providing a systematic analysis of shared subspace estimation in multi-matrix settings.
  We focus on the Angle-based Joint and Individual Variation Explained (AJIVE) method, a two-stage spectral approach, and establish new performance guarantees that uncover its strengths and limitations. Specifically, we show that in high signal-to-noise ratio (SNR) regimes, AJIVE's estimation error decreases with the number of matrices, demonstrating the power of multi-matrix integration. Conversely, in low-SNR settings, AJIVE exhibits a non-diminishing error, highlighting fundamental limitations. To complement these results, we derive minimax lower bounds, showing that AJIVE achieves optimal rates in high-SNR regimes. Furthermore, we analyze an oracle-aided spectral estimator to demonstrate that the non-diminishing error in low-SNR scenarios is a fundamental barrier. Extensive numerical experiments corroborate our theoretical findings, providing insights into the interplay between SNR, matrix count, and subspace misalignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09336v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuepeng Yang, Cong Ma</dc:creator>
    </item>
    <item>
      <title>Lattice Rules Meet Kernel Cubature</title>
      <link>https://arxiv.org/abs/2501.09500</link>
      <description>arXiv:2501.09500v1 Announce Type: cross 
Abstract: Rank-1 lattice rules are a class of equally weighted quasi-Monte Carlo methods that achieve essentially linear convergence rates for functions in a reproducing kernel Hilbert space (RKHS) characterized by square-integrable first-order mixed partial derivatives. In this work, we explore the impact of replacing the equal weights in lattice rules with optimized cubature weights derived using the reproducing kernel. We establish a theoretical result demonstrating a doubled convergence rate in the one-dimensional case and provide numerical investigations of convergence rates in higher dimensions. We also present numerical results for an uncertainty quantification problem involving an elliptic partial differential equation with a random coefficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09500v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vesa Kaarnioja, Ilja Klebanov, Claudia Schillings, Yuya Suzuki</dc:creator>
    </item>
    <item>
      <title>Statistical inference for interacting innovation processes and related general results</title>
      <link>https://arxiv.org/abs/2501.09648</link>
      <description>arXiv:2501.09648v1 Announce Type: cross 
Abstract: Given the importance of understanding how different innovation processes affect each other, we have introduced a model for a finite system of interacting innovation processes. The present work focuses on the second-order asymptotic properties of the model and illustrates how to leverage the theoretical results in order to make statistical inference on the intensity of the interaction. We apply the proposed tools to two real data sets (from Reddit and Gutenberg).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09648v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giacomo Aletti, Irene Crimaldi, Andrea Ghiglietti</dc:creator>
    </item>
    <item>
      <title>A Near-optimal Algorithm for Learning Margin Halfspaces with Massart Noise</title>
      <link>https://arxiv.org/abs/2501.09691</link>
      <description>arXiv:2501.09691v1 Announce Type: cross 
Abstract: We study the problem of PAC learning $\gamma$-margin halfspaces in the presence of Massart noise. Without computational considerations, the sample complexity of this learning problem is known to be $\widetilde{\Theta}(1/(\gamma^2 \epsilon))$. Prior computationally efficient algorithms for the problem incur sample complexity $\tilde{O}(1/(\gamma^4 \epsilon^3))$ and achieve 0-1 error of $\eta+\epsilon$, where $\eta&lt;1/2$ is the upper bound on the noise rate. Recent work gave evidence of an information-computation tradeoff, suggesting that a quadratic dependence on $1/\epsilon$ is required for computationally efficient algorithms. Our main result is a computationally efficient learner with sample complexity $\widetilde{\Theta}(1/(\gamma^2 \epsilon^2))$, nearly matching this lower bound. In addition, our algorithm is simple and practical, relying on online SGD on a carefully selected sequence of convex losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09691v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Nikos Zarifis</dc:creator>
    </item>
    <item>
      <title>Double Data Piling for Heterogeneous Covariance Models</title>
      <link>https://arxiv.org/abs/2211.15562</link>
      <description>arXiv:2211.15562v2 Announce Type: replace 
Abstract: In this work, we characterize two data piling phenomena for a high-dimensional binary classification problem with general heterogeneous covariance models, which were previously characterized only for restricted homogeneous covariances. The data piling refers to the phenomenon where projections of the training data onto a direction vector have exactly two distinct values, one for each class. This first data piling phenomenon occurs for any data when the dimension $p$ is larger than the sample size $n$. We show that the second data piling phenomenon, which refers to a data piling of independent test data, can occur in an asymptotic context where $p$ grows while $n$ is fixed. We further show that a second maximal data piling direction, which gives an asymptotic maximal distance between the two piles of independent test data, can be obtained by projecting the first maximal data piling direction onto the nullspace of the common leading eigenspace. Based on the second data piling phenomenon, we propose various linear classification rules which ensure perfect classification of high-dimension low-sample-size data under generalized heterogeneous spiked covariance models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.15562v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taehyun Kim, Jeongyoun Ahn, Sungkyu Jung</dc:creator>
    </item>
    <item>
      <title>On the optimal prediction of extreme events in heavy-tailed time series with applications to solar flare forecasting</title>
      <link>https://arxiv.org/abs/2407.11887</link>
      <description>arXiv:2407.11887v2 Announce Type: replace 
Abstract: The prediction of extreme events in time series is a fundamental problem arising in many financial, scientific, engineering, and other applications. We begin by establishing a general Neyman-Pearson-type characterization of optimal extreme event predictors in terms of density ratios. This yields new insights and several closed-form optimal extreme event predictors for additive models. These results naturally extend to time series, where we study optimal extreme event prediction for both light- and heavy-tailed autoregressive and moving average models. Using a uniform law of large numbers for ergodic time series, we establish the asymptotic optimality of an empirical version of the optimal predictor for autoregressive models. Using multivariate regular variation, we obtain an expression for the optimal extremal precision in heavy-tailed infinite moving averages, which provides theoretical bounds on the ability to predict extremes in this general class of models. We address the important problem of predicting solar flares by applying our theory and methodology to a state-of-the-art time series consisting of solar soft X-ray flux measurements. Our results demonstrate the success and limitations in solar flare forecasting of long-memory autoregressive models and long-range-dependent, heavy-tailed FARIMA models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11887v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Victor Verma, Stilian Stoev, Yang Chen</dc:creator>
    </item>
    <item>
      <title>Asymptotic Equivalence of Locally Stationary Processes and Bivariate White Noise</title>
      <link>https://arxiv.org/abs/2410.05751</link>
      <description>arXiv:2410.05751v2 Announce Type: replace 
Abstract: We consider a general class of statistical experiments, in which an $n$-dimensional centered Gaussian random variable is observed and its covariance matrix is the parameter of interest. The covariance matrix is assumed to be well-approximable in a linear space of lower dimension $K_n$ with eigenvalues uniformly bounded away from zero and infinity. We prove asymptotic equivalence of this experiment and a class of $K_n$-dimensional Gaussian models with informative expectation in Le Cam's sense when $n$ tends to infinity and $K_n$ is allowed to increase moderately in $n$ at a polynomial rate. For this purpose we derive a new localization technique for non-i.i.d. data and a novel high-dimensional Central Limit Law in total variation distance. These results are key ingredients to show asymptotic equivalence between the experiments of locally stationary Gaussian time series and a bivariate Wiener process with the log spectral density as its drift. Therein a novel class of matrices is introduced which generalizes circulant Toeplitz matrices traditionally used for strictly stationary time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05751v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristina Butucea, Alexander Meister, Angelika Rohde</dc:creator>
    </item>
    <item>
      <title>Rapid Bayesian Computation and Estimation for Neural Networks via Log-Concave Coupling</title>
      <link>https://arxiv.org/abs/2411.17667</link>
      <description>arXiv:2411.17667v2 Announce Type: replace 
Abstract: This paper studies a Bayesian estimation procedure for single hidden-layer neural networks using $\ell_{1}$ controlled neuron weight vectors. We study the structure of the posterior density that makes it amenable to rapid sampling via Markov Chain Monte Carlo (MCMC), and statistical risk guarantees. Let the neural network have $K$ neurons with internal weights of dimension $d$ and fix the outer weights. With $N$ data observations, use a gain parameter or inverse temperature of $\beta$ in the posterior density.
  The posterior is multimodal and not naturally suited to the rapid mixing of MCMC algorithms. For a continuous uniform prior over the $\ell_{1}$ ball, we demonstrate that the posterior density can be written as a mixture density where the mixture components are log-concave. Furthermore, when the total number of model parameters $Kd$ exceeds $(Kd)/\sqrt{\log(Kd)+C_{1}} \geq C_{2}(\beta N)^{2}$, the mixing distribution is also log-concave. Thus, neuron parameters can be sampled from the posterior by only sampling log-concave densities. The authors refer to such a mixture expression as a log-concave coupling.
  For a discrete uniform prior restricted to a grid, we study the statistical risk (generalization error) of procedures based on the posterior. We demonstrate that notions of squared error are on the 4th root order $O(\left[(\log d)/N\right]^{1/4})$. If one further assumes independent Gaussian data with a variance $\sigma^{2} $ that matches the inverse temperature, $\beta = 1/\sigma^{2}$, we show Kullback divergence decays as an improved cube root power $O(\left[(\log d)/N\right]^{1/3})$.
  Future work aims to bridge the sampling ability of the continuous uniform prior with the risk control of the discrete uniform prior, resulting in a polynomial time Bayesian training algorithm for neural networks with statistical risk control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17667v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Curtis McDonald, Andrew R. Barron</dc:creator>
    </item>
    <item>
      <title>Necessary and sufficient conditions for posterior propriety for generalized linear mixed models</title>
      <link>https://arxiv.org/abs/2302.00665</link>
      <description>arXiv:2302.00665v2 Announce Type: replace-cross 
Abstract: Generalized linear mixed models (GLMMs) are commonly used to analyze correlated discrete or continuous response data. In Bayesian GLMMs, the often-used improper priors may yield undesirable improper posterior distributions. Thus, verifying posterior propriety is crucial for valid applications of Bayesian GLMMs with improper priors. Here, we consider the popular improper uniform prior on the regression coefficients and several proper or improper priors, including the widely used gamma and power priors on the variance components of the random effects. We also construct an approximate Jeffreys' prior for objective Bayesian analysis of GLMMs. For the two most widely used GLMMs, namely, the binomial and Poisson GLMMs, we provide easily verifiable sufficient conditions compared to the currently available results. We also derive the necessary conditions for posterior propriety for the general exponential family GLMMs. Finally, we use examples involving one-way and two-way random effects models to demonstrate the theoretical results derived here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.00665v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yalin Rao, Vivekananda Roy</dc:creator>
    </item>
    <item>
      <title>Variance Inequalities for Transformed Fr\'echet Means in Hadamard Spaces</title>
      <link>https://arxiv.org/abs/2310.13668</link>
      <description>arXiv:2310.13668v3 Announce Type: replace-cross 
Abstract: The Fr\'echet mean (or barycenter) generalizes the expectation of a random variable to metric spaces by minimizing the expected squared distance to the random variable. Similarly, the median can be generalized by its property of minimizing the expected absolute distance. We consider the class of transformed Fr\'echet means with nondecreasing, convex transformations that have a concave derivative. This class includes the Fr\'echet median, the Fr\'echet mean, the Huber loss-induced Fr\'echet mean, and other statistics related to robust statistics in metric spaces. We study variance inequalities for these transformed Fr\'echet means. These inequalities describe how the expected transformed distance grows when moving away from a minimizer, i.e., from a transformed Fr\'echet mean. Variance inequalities are useful in the theory of estimation and numerical approximation of transformed Fr\'echet means. Our focus is on variance inequalities in Hadamard spaces - metric spaces with globally nonpositive curvature. Notably, some results are new also for Euclidean spaces. Additionally, we are able to characterize uniqueness of transformed Fr\'echet means, in particular of the Fr\'echet median.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13668v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christof Sch\"otz</dc:creator>
    </item>
    <item>
      <title>Functional Singular Value Decomposition</title>
      <link>https://arxiv.org/abs/2410.03619</link>
      <description>arXiv:2410.03619v3 Announce Type: replace-cross 
Abstract: Heterogeneous functional data commonly arise in time series and longitudinal studies. To uncover the statistical structures of such data, we propose Functional Singular Value Decomposition (FSVD), a unified framework encompassing various tasks for the analysis of functional data with potential heterogeneity. We establish the mathematical foundation of FSVD by proving its existence and providing its fundamental properties. We then develop an implementation approach for noisy and irregularly observed functional data based on a novel alternating minimization scheme and provide theoretical guarantees for its convergence and estimation accuracy. The FSVD framework also introduces the concepts of intrinsic basis functions and intrinsic basis vectors, representing two fundamental structural aspects of random functions. These concepts enable FSVD to provide new and improved solutions to tasks including functional principal component analysis, factor models, functional clustering, functional linear regression, and functional completion, while effectively handling heterogeneity and irregular temporal sampling. Through extensive simulations, we demonstrate that FSVD-based methods consistently outperform existing methods across these tasks. To showcase the value of FSVD in real-world datasets, we apply it to extract temporal patterns from a COVID-19 case count dataset and perform data completion on an electronic health record dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03619v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Pixu Shi, Anru R. Zhang</dc:creator>
    </item>
  </channel>
</rss>
