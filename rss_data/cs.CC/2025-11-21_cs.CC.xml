<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Nov 2025 05:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Indefiniteness makes lattice reduction easier</title>
      <link>https://arxiv.org/abs/2511.16151</link>
      <description>arXiv:2511.16151v1 Announce Type: new 
Abstract: Since the invention of the famous LLL algorithm, lattice reduction has been an extremely useful tool in computational number theory. By construction, the LLL algorithm deals with lattices living in a vector space endowed with a positive definite scalar product. However, it seems quite nature to ask about the indefinite case, where the scalar product is replaced by an arbitrary quadratic form, possibily indefinite. This question was considered independently in two lines of work. One by G{\'a}bor Ivanyos and {\'A}gnes Sz{\'a}nt{\'o} and one by Denis Simon. Both lead to an algorithm that generalizes LLL and whose performance is very similar to LLL, i.e. a polynomial-time algorithm that approximates the shortest vector within an approximation factor exponential in the dimension. Denis Simon achieves an approximation factor close to that of LLL under the assumption that no isotropic vectors arise during reduction. G{\'a}bor Ivanyos and {\'A}gnes Sz{\'a}nt{\'o} show that it is possible to avoid isotropic vectors altogether, at the cost of a somewhat worse approximation factor.  In this paper, we revisit the reduction of indefinite lattices and conclude that it can lead to much better reduced representations that previously thought. We also conclude that the approximation factor depends on the signature of the indefinite lattice rather than on its dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16151v1</guid>
      <category>cs.CC</category>
      <category>math.NT</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Joux (CISPA, IMJ-PRG)</dc:creator>
    </item>
    <item>
      <title>Debordering Closure Results in Determinantal and Pfaffian Ideals</title>
      <link>https://arxiv.org/abs/2511.16492</link>
      <description>arXiv:2511.16492v1 Announce Type: new 
Abstract: One important question in algebraic complexity is understanding the complexity of polynomial ideals (Grochow, Bulletin of EATCS 131, 2020). Andrews and Forbes (STOC 2022) studied the determinantal ideals $I^{\det}_{n,m,r}$ generated by the $r\times r$ minors of $n\times m$ matrices. Over fields of characteristic zero or of sufficiently large characteristic, they showed that for any nonzero $f \in I^{\det}_{n,m,r}$, the determinant of a $t \times t$ matrix of variables with $t = \Theta(r^{1/3})$ is approximately computed by a constant-depth, polynomial-size $f$-oracle algebraic circuit, in the sense that the determinant lies in the border of such circuits. An analogous result was also obtained for Pfaffians in the same paper.
  In this work, we deborder the result of Andrews and Forbes by showing that when $f$ has polynomial degree, the determinant is in fact exactly computed by a constant-depth, polynomial-size $f$-oracle algebraic circuit. We further establish an analogous result for Pfaffian ideals.
  Our results are established using the isolation lemma, combined with a careful analysis of straightening-law expansions of polynomials in determinantal and Pfaffian ideals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16492v1</guid>
      <category>cs.CC</category>
      <category>math.AC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anakin Dey, Zeyu Guo</dc:creator>
    </item>
    <item>
      <title>Connectivity-Preserving Important Separators: Enumeration and an Improved FPT Algorithm for Node Multiway Cut-Uncut</title>
      <link>https://arxiv.org/abs/2511.15849</link>
      <description>arXiv:2511.15849v1 Announce Type: cross 
Abstract: We develop a framework for handling graph separation problems with connectivity constraints. Extending the classical concept of important separators, we introduce and analyze connectivity-preserving important separators, which are important separators that not only disconnect designated terminal sets $A$ and $B$ but also satisfy an arbitrary set of connectivity constraints over the terminals. These constraints can express requirements such as preserving the internal connectivity of each terminal set, enforcing pairwise connections defined by an equivalence relation, or maintaining reachability from a specified subset of vertices. We prove that for any graph $G=(V,E)$, terminal sets $A,B\subseteq V$, and integer $k$, the number of important $A,B$-separators of size at most $k$ satisfying a set of connectivity constraints is bounded by $2^{O(k\log k)}$, and that all such separators can be enumerated within $O(2^{O(k\log k)} \cdot n \cdot T(n,m))$ time, where $T(n,m)$ is the time required to compute a minimum $s,t$-separator. As an application, we obtain a new fixed-parameter-tractable algorithm for the Node Multiway Cut-Uncut (N-MWCU) problem, parameterized by $k$, the size of the separator set. The algorithm runs in $O(2^{O(k\log k)} \cdot n \cdot m^{1+o(1)})$ time for graphs with polynomially-bounded integer weights. This significantly improves the dependence on $k$ from the previous $2^{O(k^2\log k)}$ to $2^{O(k\log k)}$, thereby breaking a long-standing barrier, and simultaneously improves the polynomial factors. Our framework generalises the important-separator paradigm to separation problems in which the deletion set must satisfy both cut and uncut constraints on terminal subsets, thus offering a refined combinatorial foundation for designing fixed-parameter algorithms for cut-uncut problems in graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15849v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Batya Kenig</dc:creator>
    </item>
    <item>
      <title>Rate-optimal community detection near the KS threshold via node-robust algorithms</title>
      <link>https://arxiv.org/abs/2511.16613</link>
      <description>arXiv:2511.16613v1 Announce Type: cross 
Abstract: We study community detection in the \emph{symmetric $k$-stochastic block model}, where $n$ nodes are evenly partitioned into $k$ clusters with intra- and inter-cluster connection probabilities $p$ and $q$, respectively.
  Our main result is a polynomial-time algorithm that achieves the minimax-optimal misclassification rate
  \begin{equation*}
  \exp \Bigl(-\bigl(1 \pm o(1)\bigr) \tfrac{C}{k}\Bigr),
  \quad \text{where } C = (\sqrt{pn} - \sqrt{qn})^2,
  \end{equation*}
  whenever $C \ge K\,k^2\,\log k$ for some universal constant $K$, matching the Kesten--Stigum (KS) threshold up to a $\log k$ factor.
  Notably, this rate holds even when an adversary corrupts an $\eta \le \exp\bigl(- (1 \pm o(1)) \tfrac{C}{k}\bigr)$ fraction of the nodes.
  To the best of our knowledge, the minimax rate was previously only attainable either via computationally inefficient procedures [ZZ15] or via polynomial-time algorithms that require strictly stronger assumptions such as $C \ge K k^3$ [GMZZ17].
  In the node-robust setting, the best known algorithm requires the substantially stronger condition $C \ge K k^{102}$ [LM22].
  Our results close this gap by providing the first polynomial-time algorithm that achieves the minimax rate near the KS threshold in both settings.
  Our work has two key technical contributions:
  (1) we robustify majority voting via the Sum-of-Squares framework,
  (2) we develop a novel graph bisection algorithm via robust majority voting, which allows us to significantly improve the misclassification rate to $1/\mathrm{poly}(k)$ for the initial estimation near the KS threshold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16613v1</guid>
      <category>stat.ML</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingqiu Ding, Yiding Hua, Kasper Lindberg, David Steurer, Aleksandr Storozhenko</dc:creator>
    </item>
    <item>
      <title>#P-hardness proofs of matrix immanants evaluated on restricted matrices</title>
      <link>https://arxiv.org/abs/2103.04934</link>
      <description>arXiv:2103.04934v3 Announce Type: replace 
Abstract: We establish the $\#P$-hardness of computing a broad class of immanants, even when restricted to specific categories of matrices. Concretely, we prove that computing $\lambda$-immanants of $0$-$1$ matrices is $\#P$-hard whenever the partition~$\lambda$ contains a sufficiently large domino-tileable region, subject to certain technical conditions.
  We also give hardness proofs for some $\lambda$-immanants of weighted adjacency matrices of planar directed graphs, such that the shape $\lambda = (\mathbf{1} + \lambda_d)$ has size $n$ such that $|\lambda_d| = n^\varepsilon$ for some $0 &lt; \varepsilon &lt; \frac{1}{2}$, and such that for some $w$, the shape $\lambda_d/(w)$ is tileable with $1 \times 2$ dominos.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.04934v3</guid>
      <category>cs.CC</category>
      <category>math.RT</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Istvan Miklos, Cordian Riener</dc:creator>
    </item>
    <item>
      <title>Improved Bounds for Coin Flipping, Leader Election, and Random Selection</title>
      <link>https://arxiv.org/abs/2504.01856</link>
      <description>arXiv:2504.01856v2 Announce Type: replace 
Abstract: Random selection, leader election, and collective coin flipping are fundamental tasks in fault-tolerant distributed computing. We study these problems in the full-information model where despite decades of study, key gaps remain in our understanding of the trade-offs between round complexity, communication per player in each round, and adversarial resilience. We make progress by proving improved bounds for these problems.
  We first show that any $k$-round coin flipping protocol over $\ell$ players, each player sending one bit per round, can be biased by $O(\ell/\log^{(k)}(\ell))$ bad players. We obtain a similar lower bound for leader election. This strengthens prior best bounds [RSZ, SICOMP 2002] of $O(\ell/\log^{(2k-1)}(\ell))$ for coin flipping protocols and $O(\ell/\log^{(2k+1)}(\ell))$ for leader election protocols. Our result implies that any (1-bit per player) protocol tolerating linear fraction of bad players requires at least $\log^* \ell$ rounds, showing existing protocols [RZ, JCSS 2001; F, FOCS 1999] are near-optimal.
  We next initiate the study of one-round, (1-bit per player) random selection. We construct a protocol resilient to $\ell / (\log \ell)^2$ bad players that outputs $(\log \ell)^2 / (\log \log \ell)^2$ uniform random bits. This implies a one-round leader election protocol resilient to $\ell / (\log \ell)^2$ bad players, improving the prior best protocol [RZ, JCSS 2001] which was resilient to $\ell / (\log \ell)^3$ bad players. Our resilience matches that of the best one-round coin flipping protocol by Ajtai &amp; Linial. We also obtain an almost matching lower bound: any protocol outputting $(\log \ell)^2 / (\log \log \ell)^2$ bits can be corrupted by $\ell (\log \log \ell)^2 / (\log \ell)^2$ bad players. To obtain our lower bound, we introduce multi-output influence, an extension of influence of boolean functions to the multi-output setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01856v2</guid>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eshan Chattopadhyay, Mohit Gurumukhani, Noam Ringach, Rocco Servedio</dc:creator>
    </item>
    <item>
      <title>Geometry Of The Subset Sum Problem -- Part I</title>
      <link>https://arxiv.org/abs/2510.24806</link>
      <description>arXiv:2510.24806v2 Announce Type: replace 
Abstract: We announce two breakthrough results concerning important questions in the Theory of Computational Complexity. In this expository paper, a systematic and comprehensive geometric characterization of the Subset Sum Problem is presented. We show the existence of a universal geometric structure, comprised of a family of non-decreasing paths in the Cartesian plane, that captures any instance of the problem of size $n$. Inspired by the geometric structure, we provide an unconditional, deterministic and polynomial time algorithm, albeit with fairly high complexity, thereby showing that $\mathcal{P} = \mathcal{NP}$. Furthermore, our algorithm also outputs the number of solutions to the problem in polynomial time, thus leading to $\mathcal{FP} = \mathcal{\#P}$. As a bonus, one important consequence of our results, out of many, is that the quantum-polynomial class $\mathcal{BQP} \subseteq \mathcal{P}$.
  Not only this, but we show that when multiple solutions exist, they can be placed in certain equivalence classes based on geometric attributes, and be compactly represented by a polynomial sized directed acyclic graph. We show that the Subset Sum Problem has two aspects, namely a combinatorial aspect and a relational aspect, and that it is the latter which is the primary determiner of complexity. We reveal a surprising connection between the size of the elements and their number, and the precise way in which they affect the complexity. In particular, we show that for all instances of the Subset Sum Problem, the complexity is independent of the size of elements, once the difference between consecutive elements exceeds $\lceil{7\log{}n}\rceil$ bits in size.
  We provide some numerical examples to illustrate the algorithm, and also show how it can be used to estimate some difficult combinatorial quantities such as the number of restricted partitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24806v2</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Srinivas Balaji Bollepalli</dc:creator>
    </item>
    <item>
      <title>On the consistency of stronger lower bounds for NEXP</title>
      <link>https://arxiv.org/abs/2504.03320</link>
      <description>arXiv:2504.03320v4 Announce Type: replace-cross 
Abstract: It was recently shown by Atserias, Buss and Mueller that the standard complexity-theoretic conjecture NEXP not in P / poly is consistent with the relatively strong bounded arithmetic theory V^0_2, which can prove a substantial part of complexity theory. We observe that their approach can be extended to show that the stronger conjectures NEXP not in EXP / poly and NEXP not in coNEXP are consistent with a stronger theory, which includes every true universal number-sort sentence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03320v4</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <category>math.LO</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neil Thapen</dc:creator>
    </item>
    <item>
      <title>New Hardness Results for the LOCAL Model via a Simple Self-Reduction</title>
      <link>https://arxiv.org/abs/2510.19972</link>
      <description>arXiv:2510.19972v2 Announce Type: replace-cross 
Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta, \log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and $\Delta$ is the maximum degree. This result is shown through a new technique, called round elimination via self-reduction. The lower bound proof is beautiful and presents very nice ideas. However, it spans more than 25 pages of technical details, and hence it is hard to digest and generalize to other problems. Historically, the simplification of proofs and techniques has marked an important turning point in our understanding of the complexity of graph problems. Our paper makes a step forward towards this direction, and provides the following contributions.
  1. We present a short and simplified version of the round elimination via self-reduction technique. The simplification of this technique enables us to obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal $b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching problem is a generalization of the matching problem where each vertex can have up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log \Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le \Delta^{1-\varepsilon}$ and any constant $\varepsilon &gt; 0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19972v2</guid>
      <category>cs.DC</category>
      <category>cs.CC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alkida Balliu, Filippo Casagrande, Francesco d'Amore, Dennis Olivetti</dc:creator>
    </item>
  </channel>
</rss>
