<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Sep 2025 04:01:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Comprehensive Analysis of Churn Prediction in Telecommunications Using Machine Learning</title>
      <link>https://arxiv.org/abs/2509.22654</link>
      <description>arXiv:2509.22654v1 Announce Type: new 
Abstract: Customer churn prediction in the telecommunications sector represents a critical business intelligence task that has evolved from subjective human assessment to sophisticated algorithmic approaches. In this work, we present a comprehensive framework for telecommunications churn prediction leveraging deep neural networks. Through systematic problem formulation, rigorous dataset analysis, and careful feature engineering, we develop a model that captures complex patterns in customer behavior indicative of potential churn. We conduct extensive empirical evaluations across multiple performance metrics, demonstrating that our proposed neural architecture achieves significant improvements over existing baseline methods. Our approach not only advances the state-of-the-art in churn prediction accuracy but also provides interpretable insights into the key factors driving customer attrition in telecommunications services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22654v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuhang Chen, Bo Lv, Mengqian Wang, Xunwen Xiang, Shiting Wu, Shenghong Luo, Wenjun Zhang</dc:creator>
    </item>
    <item>
      <title>U.S. Port Disruptions under Tropical Cyclones: Resilience Analysis by Harnessing Multiple-Source Dataset</title>
      <link>https://arxiv.org/abs/2509.22656</link>
      <description>arXiv:2509.22656v1 Announce Type: new 
Abstract: This study introduces the CyPort Dataset, recording disruptions to 145 U.S. principal ports and freight network from 90 tropical cyclones (2015-2023). It addresses limitations of event specific resilience studies and provides a comprehensive dataset for broader analysis. To account for excess zeros and unobserved heterogeneity in disruption outcomes, the Random Parameter Negative Binomial Lindley (RPNB Lindley) model is employed to produce more reliable resilience insights. The model demonstrates improved fit over traditional methods and uncovers variation in how features such as wind speed, storm surge height, rainfall, and distance to cyclone influence disruption outcomes across ports. This analysis reveals a tipping point at Saffir Simpson Hurricane Category 4, where disruptions escalate sharply, causing greater impacts and prolonged recovery. Regionally, ports along the Gulf of America show greatest vulnerability. Within the freight network, ports with high betweenness centrality are more resilient, while transshipment and local hubs are more fragile.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22656v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenchen Kuai, Zihao Li, Yunlong Zhang, Xiubin Bruce Wang, Dominique Lord, Yang Zhou</dc:creator>
    </item>
    <item>
      <title>Forecasting West Nile virus with deep graph encoders</title>
      <link>https://arxiv.org/abs/2509.22657</link>
      <description>arXiv:2509.22657v1 Announce Type: new 
Abstract: West Nile virus is a significant, and growing, public health issue in the United States. With no human vaccine, mosquito control programs rely on accurate forecasting to determine when and where WNV will emerge. Recently, spatial Graph neural networks (GNNs) were shown to be a powerful tool for WNV forecasting, significantly improving over traditional methods. Building on this work, we introduce a new GNN variant that linearly connects graph attention layers, allowing us to train much larger models than previously used for WNV forecasting. This architecture specializes general densely connected GNNs so that the model focuses more heavily on local information to prevent over smoothing. To support training large GNNs we compiled a massive new dataset of weather data, land use information, and mosquito trap results across Illinois. Experiments show that our approach significantly outperforms both GNN and classical baselines in both out-of-sample and out-of-graph WNV prediction skill across a variety of scenarios and over all prediction horizons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22657v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ethan Greiffenstein, Trevor Harris, Rebecca Smith</dc:creator>
    </item>
    <item>
      <title>Dissecting Multi-Level Pricing Schemes in the Context of eCW Client Engagement</title>
      <link>https://arxiv.org/abs/2509.22669</link>
      <description>arXiv:2509.22669v1 Announce Type: new 
Abstract: This paper presents a usage-based pricing framework for the Intelligent Medical Objects ProblemIT Portal utilized by eClinicalWorks (eCW) clients. The approach begins by determining a stable monthly unit price per request, estimated as the median from semi-parametric Bayesian cubic smoothing spline analyses covering the period November 2015 to December 2016. Clients are subsequently segmented into eight volume-based tiers, with total charges computed by multiplying the derived median unit price by each client's total request count. Examination of the dataset reveals that 806 accounts with a single registered user and 470 accounts with two registered users both exhibit disproportionately high request volumes. The proposed model incorporates adjustments to account for these anomalies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22669v1</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paramahansa Pramanik, Joel Graff, Mike Decaro</dc:creator>
    </item>
    <item>
      <title>Modeling Tennis In-Match Momentum Using Probability Method</title>
      <link>https://arxiv.org/abs/2509.22670</link>
      <description>arXiv:2509.22670v1 Announce Type: new 
Abstract: This paper investigates the Tennis Momentum Model (TMM), which aims to enhance the understanding of match dynamics by integrating key factors such as efficiency, historical scoring probabilities, and real-time scoring data. The model is designed to explore how momentum affects player performance throughout a match and how it might influence overall match outcomes. By leveraging this model, players and coaches could gain valuable insights that may help them adjust their strategies in response to shifting momentum during a match.
  To validate the model, it was tested on two tennis matches, revealing its effectiveness in capturing shifts in momentum and correlating these shifts with scoring events. The results showed that the TMM accurately depicted the flow of momentum during matches, highlighting how shifts in momentum are directly linked to changes in scoring as the match progresses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22670v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jackson Graves, Daniel X. Guo, Ridge Shepherd, Alexander Young</dc:creator>
    </item>
    <item>
      <title>PISA: An AI Pipeline for Interpretable-by-design Survival Analysis Providing Multiple Complexity-Accuracy Trade-off Models</title>
      <link>https://arxiv.org/abs/2509.22673</link>
      <description>arXiv:2509.22673v1 Announce Type: new 
Abstract: Survival analysis is central to clinical research, informing patient prognoses, guiding treatment decisions, and optimising resource allocation. Accurate time-to-event predictions not only improve quality of life but also reveal risk factors that shape clinical practice. For these models to be relevant in healthcare, interpretability is critical: predictions must be traceable to patient-specific characteristics, and risk factors should be identifiable to generate actionable insights for both clinicians and researchers. Traditional survival models often fail to capture non-linear interactions, while modern deep learning approaches, though powerful, are limited by poor interpretability.
  We propose a Pipeline for Interpretable Survival Analysis (PISA) - a pipeline that provides multiple survival analysis models that trade off complexity and performance. Using multiple-feature, multi-objective feature engineering, PISA transforms patient characteristics and time-to-event data into multiple survival analysis models, providing valuable insights into the survival prediction task. Crucially, every model is converted into simple patient stratification flowcharts supported by Kaplan-Meier curves, whilst not compromising on performance. While PISA is model-agnostic, we illustrate its flexibility through applications of Cox regression and shallow survival trees, the latter avoiding proportional hazards assumptions.
  Applied to two clinical benchmark datasets, PISA produced interpretable survival models and intuitive stratification flowcharts whilst achieving state-of-the-art performances. Revisiting a prior departmental study further demonstrated its capacity to automate survival analysis workflows in real-world clinical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22673v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thalea Schlender, Catharina J. A. Romme, Yvette M. van der Linden, Luc R. C. W. van Lonkhuijzen, Peter A. N. Bosman, Tanja Alderliesten</dc:creator>
    </item>
    <item>
      <title>Profit over Proxies: A Scalable Bayesian Decision Framework for Optimizing Multi-Variant Online Experiments</title>
      <link>https://arxiv.org/abs/2509.22677</link>
      <description>arXiv:2509.22677v1 Announce Type: new 
Abstract: Online controlled experiments (A/B tests) are fundamental to data-driven decision-making in the digital economy. However, their real-world application is frequently compromised by two critical shortcomings: the use of statistically flawed heuristics like "p-value peeking", which inflates false positive rates, and an over-reliance on proxy metrics like conversion rates, which can lead to decisions that inadvertently harm core business profitability. This paper addresses these challenges by introducing a comprehensive and scalable Bayesian decision framework designed for profit optimization in multi-variant (A/B/n) experiments.
  We propose a hierarchical Bayesian model that simultaneously estimates the probability of conversion (using a Beta-Bernoulli model) and the monetary value of that conversion (using a robust Bayesian model for the mean transaction value). Building on this, we employ a decision-theoretic stopping rule based on Expected Loss, enabling experiments to be concluded not only when a superior variant is identified but also when it becomes clear that no variant offers a practically significant improvement (stopping for futility). The framework successfully navigates "revenue traps" where a variant with a higher conversion rate would have resulted in a net financial loss, correctly terminates futile experiments early to conserve resources, and maintains strict statistical integrity throughout the monitoring process.
  Ultimately, this work provides a practical and principled methodology for organizations to move beyond simple A/B testing towards a mature, profit-driven experimentation culture, ensuring that statistical conclusions translate directly to strategic business value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22677v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Srijesh Pillai, Rajesh Kumar Chandrawat</dc:creator>
    </item>
    <item>
      <title>Strategic Play and Home Advantage: Coaches' Tactical Impact in Serie A</title>
      <link>https://arxiv.org/abs/2509.22683</link>
      <description>arXiv:2509.22683v1 Announce Type: new 
Abstract: We analyze how coaching strategies affect goal difference and home win probabilities using hand-coded Serie A match commentary (2011/12--2013/14). Our dataset captures in-game dynamics, referee actions, and team behavior. Applying generalized linear, logit, and proportional-odds models with robust and bootstrap standard errors, we uncover stable effects across model averaging. Aggressive opening tactics consistently boost performance, while technical actions like crosses and goal-kicks show distinct patterns. Home advantage remains significant after full control. Our approach reveals the economic logic of real-time coaching, offering a novel, data-driven method to study decision-making under uncertainty in competitive environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22683v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Angelini, Massimiliano Castellani, Gery Andr\'es D\'iaz Rubio, Simone Giannerini, Greta Goracci</dc:creator>
    </item>
    <item>
      <title>Tracking the Spatiotemporal Spread of the Ohio Overdose Epidemic with Topological Data Analysis</title>
      <link>https://arxiv.org/abs/2509.22705</link>
      <description>arXiv:2509.22705v1 Announce Type: new 
Abstract: In recent years, techniques from Topological Data Analysis (TDA) have proven effective at capturing spatial features of multidimensional data. However, applying TDA to spatiotemporal data remains relatively underexplored. In this work, we extend previous studies of disease spread by using the Mapper algorithm to analyze the Ohio drug overdose epidemic from 2007 to 2024. We introduce a novel method for constructing covers in Mapper graphs of spatiotemporal data that respects geographic structure and highlights the time-dependent variables. Finally, we generate a Mapper visualization of regional demographics to examine how these factors relate to overdose deaths. Our approach effectively reveals temporal trends, overdose hotspots, and time-lagged patterns in relation to both geography and community demographics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22705v1</guid>
      <category>stat.AP</category>
      <category>math.AT</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Bermingham, David White, Nathan Willey</dc:creator>
    </item>
    <item>
      <title>An Econometric Analysis of the Impact of Telecare on the Length of Stay in Hospital</title>
      <link>https://arxiv.org/abs/2509.22706</link>
      <description>arXiv:2509.22706v1 Announce Type: new 
Abstract: In this paper, we develop a theoretical model that links the demand for telecare to the length of stay in hospital and formulate three models that can be used to derive the treatment effect by making various assumptions about the probability distribution of the outcome measure. We then fit the models to data and estimate them using a strategy that controls for the effects of confounding variables and unobservable factors, and compare the treatment effects with that of the Propensity Score Matching (PSM) technique which adopts a quasi-experimental study design. To ensure comparability, the covariates are kept identical in all cases. An important finding that emerges from our analysis is that the treatment effects derived from our econometric models of interest are better than that obtained from an experimental study design as the latter does not account for all the relevant unobservable factors. In particular, the results show that estimating the treatment effect of telecare in the way that an experimental study design entails fails to account for the systematic variations in individuals' health production functions within each experimental arm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22706v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Momanyi</dc:creator>
    </item>
    <item>
      <title>Vaccinating Now or Vaccinating Later: Separating Pull-Forward and Net Effects Using a Dynamic Regression Discontinuity Design</title>
      <link>https://arxiv.org/abs/2509.22714</link>
      <description>arXiv:2509.22714v1 Announce Type: new 
Abstract: We study the impact of a novel COVID-19 vaccine mandate, targeting graduating high-school students, on first vaccine uptake. In 2021, the State Government of Western Australia (WA) required attendees at "Leavers" -- a large-scale state-supported graduation party held annually in November in a WA regional town -- to be vaccinated. Using administrative data that link date-of-birth (at the month level), school attendance, and first-dose vaccination records, we exploit the strict school-age laws in WA to run regression discontinuity designs (RDDs). In other words, we use the date-of-birth cutoff for starting compulsory schooling in WA to build the counterfactual vaccination outcomes for Year-12 (i.e. graduating) students. We run both static and dynamic RDDs, the latter consisting of daily RDD estimations in a one-year window centred around the policy deadline in November 2021. We find that the "Leavers mandate" -- which excluded unvaccinated Year-12 students from popular post-graduation events -- raised vaccination rates by 9.3 percentage points at the mandate deadline. The dynamic RDD estimates show that this effect is entirely due to pulling forward future vaccinations by 46-80 days, with no net increase in ultimate uptake. Our paper is first to disentangle "pull-forward" (intensive margin) versus "net" (extensive margin) effects of a vaccine mandate in a pandemic context -- meaning that we identify how much the mandate made eventually-vaccinated people anticipate their vaccination, and how much it induced vaccinations that would not have happened absent the mandate. We also bring new evidence on the efficacy of time-limited non-monetary incentives for accelerating vaccination campaigns. Keywords: mandate; vaccination; incentives; uptake; adolescents; timing; coverage. JEL: I12; I18.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22714v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fabio I. Martinenghi, Mesfin Genie, Katie Attwell, Huong Le, Hannah Moore, Aregawi G. Gebremariam, Bette Liu, Francesco Paolucci, Christopher C. Blyth</dc:creator>
    </item>
    <item>
      <title>A Penalized Distributed Lag Non-Linear Lee-Carter Framework for Regional Weekly Mortality Forecasting</title>
      <link>https://arxiv.org/abs/2509.24087</link>
      <description>arXiv:2509.24087v1 Announce Type: new 
Abstract: Accurate forecasts of weekly mortality are essential for public health and the insurance industry. We develop a forecasting framework that extends the Lee-Carter model with age- and region-specific seasonal effects and penalized distributed lag non-linear components that capture the delayed and non-linear effects of heat, cold, and influenza on mortality. The model accommodates overdispersed mortality rates via a negative binomial distribution. We model the temporal dynamics of the latent factors in the model using SARIMAX processes and capture cross-regional dependencies through a copula-based approach. Using regional French mortality data (1990-2019), we demonstrate that the proposed framework yields well-calibrated forecast distributions and improves predictive accuracy relative to benchmark models. The results further show substantial heterogeneity in temperature- and influenza-related relative risks between ages and regions. These findings underscore the importance of incorporating exogenous drivers and dependence structures into a weekly mortality forecasting framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24087v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jens Robben, Karim Barigou</dc:creator>
    </item>
    <item>
      <title>Assessing Roundabout Safety Perceptions under Heterogeneous Traffic: Socio-Demographic and Geometric Influences in Indian Urban Contexts</title>
      <link>https://arxiv.org/abs/2509.24397</link>
      <description>arXiv:2509.24397v1 Announce Type: new 
Abstract: Evaluation of the safety perceptions of roundabout users is crucial for improving road safety in mixed-traffic environments. The crash- and conflict-based analyses do not incorporate the socio-demographic characteristics of the roundabout users, which can only be captured through questionnaire surveys on a larger scale. This research evaluated the relationship of roundabout safety perception with demographic factors, driving characteristics, and varying roundabout geometries using multiple correspondence analysis, cluster analysis, factor analysis, and multinomial logistic regression. The study analyzed data from 1,530 respondents across two Indian cities. The study identified three roundabout user clusters. Single-lane roundabouts were perceived as safer during entry and circulation, with a significant prominence among middle-aged users. In contrast, double- and multi-lane roundabouts presented higher perceived risks during exit maneuvers, especially among young, inexperienced, unemployed/self-employed users. Vulnerable road users reported significantly higher perceived risks, especially under suboptimal lighting conditions. Respondents with 10-20 years of driving experience, especially car users, perceived lower risk at single-lane roundabouts but acknowledged the higher risk linked to speed variations and complex maneuvers at multi-lane roundabouts. Driving experience, vehicle type, and geometric configurations were crucial in roundabout safety perception. The study highlighted the need to improve the built environment of roundabouts for vulnerable road users. The roundabout merging area was perceived as the most dangerous spot; however, exits were also perceived as dangerous for double- and multi-lane roundabouts. The findings can benefit policymakers, engineers, and urban planners by enabling them to deploy targeted safety interventions based on issues highlighted in the study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24397v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhijnan Maji, Indrajit Ghosh</dc:creator>
    </item>
    <item>
      <title>A Bayesian Update Method for Exponential Family Projection Filters with Non-Conjugate Likelihoods</title>
      <link>https://arxiv.org/abs/2504.16867</link>
      <description>arXiv:2504.16867v2 Announce Type: cross 
Abstract: The projection filter is one of the approximations to the solution of the optimal filtering problem. It approximates the filtering density by projecting the dynamics of the square-root filtering density onto the tangent space of the square-root parametric density manifold. While the projection filters for exponential and mixture families with continuous measurement processes have been well studied, the continuous-discrete projection filtering algorithm for non-conjugate priors has received less attention.
  In this paper, we introduce a simple Riemannian optimization method to be used for the Bayesian update step in the continuous-discrete projection filter for exponential families. Specifically, we show that the Bayesian update can be formulated as an optimization problem of $\alpha$-R\'enyi divergence, where the corresponding Riemannian gradient can be easily computed. We demonstrate the effectiveness of the proposed method via two highly non-Gaussian Bayesian update problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16867v2</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Fuady Emzir</dc:creator>
    </item>
    <item>
      <title>Global-Local Dirichlet Processes for Identifying Pan-Cancer Subpopulations Using Both Shared and Cancer-Specific Data</title>
      <link>https://arxiv.org/abs/2509.22884</link>
      <description>arXiv:2509.22884v1 Announce Type: cross 
Abstract: We consider the problem of clustering grouped data for which the observations may include group-specific variables in addition to the variables that are shared across groups. This type of data is common in cancer genomics where the molecular information is usually accompanied by cancer-specific clinical information. Existing grouped clustering methods only consider the shared variables, thereby ignoring valuable information from the cancer-specific variables. To allow for these cancer-specific variables to aid in the clustering, we propose a novel Bayesian nonparametric approach, termed global-local (GLocal) Dirichlet process, that models the ``global-local'' structure of the observations across groups. We characterize the GLocal Dirichlet process using the stick-breaking representation and the representation as a limit of a finite mixture model, which leads to an efficient posterior inference algorithm. We illustrate our model with extensive simulations and a real pan-gastrointestinal cancer dataset. The cancer-specific clinical variables included carcinoembryonic antigen level, patients' body mass index, and the number of cigarettes smoked per day. These important clinical variables refine the clusters of gene expression data and allow us to identify finer sub-clusters, which is not possible in their absence. This refinement aids in the better understanding of tumor progression and heterogeneity. Moreover, our proposed method is applicable beyond the field of cancer genomics to a general grouped clustering framework in the presence of group-specific idiosyncratic variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22884v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/25-AOAS2056</arxiv:DOI>
      <arxiv:journal_reference>The Annals of Applied Statistics 19(3), 2254-2278, (September 2025)</arxiv:journal_reference>
      <dc:creator>Arhit Chakrabarti, Yang Ni, Debdeep Pati, Bani K. Mallick</dc:creator>
    </item>
    <item>
      <title>A Modality-Tailored Graph Modeling Framework for Urban Region Representation via Contrastive Learning</title>
      <link>https://arxiv.org/abs/2509.23772</link>
      <description>arXiv:2509.23772v1 Announce Type: cross 
Abstract: Graph-based models have emerged as a powerful paradigm for modeling multimodal urban data and learning region representations for various downstream tasks. However, existing approaches face two major limitations. (1) They typically employ identical graph neural network architectures across all modalities, failing to capture modality-specific structures and characteristics. (2) During the fusion stage, they often neglect spatial heterogeneity by assuming that the aggregation weights of different modalities remain invariant across regions, resulting in suboptimal representations. To address these issues, we propose MTGRR, a modality-tailored graph modeling framework for urban region representation, built upon a multimodal dataset comprising point of interest (POI), taxi mobility, land use, road element, remote sensing, and street view images. (1) MTGRR categorizes modalities into two groups based on spatial density and data characteristics: aggregated-level and point-level modalities. For aggregated-level modalities, MTGRR employs a mixture-of-experts (MoE) graph architecture, where each modality is processed by a dedicated expert GNN to capture distinct modality-specific characteristics. For the point-level modality, a dual-level GNN is constructed to extract fine-grained visual semantic features. (2) To obtain effective region representations under spatial heterogeneity, a spatially-aware multimodal fusion mechanism is designed to dynamically infer region-specific modality fusion weights. Building on this graph modeling framework, MTGRR further employs a joint contrastive learning strategy that integrates region aggregated-level, point-level, and fusion-level objectives to optimize region representations. Experiments on two real-world datasets across six modalities and three tasks demonstrate that MTGRR consistently outperforms state-of-the-art baselines, validating its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23772v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaya Zhao, Kaiqi Zhao, Zixuan Tang, Zhiyuan Liu, Xiaoling Lu, Yalei Du</dc:creator>
    </item>
    <item>
      <title>AQUAIR: A High-Resolution Indoor Environmental Quality Dataset for Smart Aquaculture Monitoring</title>
      <link>https://arxiv.org/abs/2509.24069</link>
      <description>arXiv:2509.24069v1 Announce Type: cross 
Abstract: Smart aquaculture systems depend on rich environmental data streams to protect fish welfare, optimize feeding, and reduce energy use. Yet public datasets that describe the air surrounding indoor tanks remain scarce, limiting the development of forecasting and anomaly-detection tools that couple head-space conditions with water-quality dynamics. We therefore introduce AQUAIR, an open-access public dataset that logs six Indoor Environmental Quality (IEQ) variables--air temperature, relative humidity, carbon dioxide, total volatile organic compounds, PM2.5 and PM10--inside a fish aquaculture facility in Amghass, Azrou, Morocco. A single Awair HOME monitor sampled every five minutes from 14 October 2024 to 9 January 2025, producing more than 23,000 time-stamped observations that are fully quality-controlled and publicly archived on Figshare. We describe the sensor placement, ISO-compliant mounting height, calibration checks against reference instruments, and an open-source processing pipeline that normalizes timestamps, interpolates short gaps, and exports analysis-ready tables. Exploratory statistics show stable conditions (median CO2 = 758 ppm; PM2.5 = 12 micrograms/m3) with pronounced feeding-time peaks, offering rich structure for short-horizon forecasting, event detection, and sensor drift studies. AQUAIR thus fills a critical gap in smart aquaculture informatics and provides a reproducible benchmark for data-centric machine learning curricula and environmental sensing research focused on head-space dynamics in recirculating aquaculture systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24069v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youssef Sabiri, Walid Houmaidi, Ouail El Maadi, Yousra Chtouki</dc:creator>
    </item>
    <item>
      <title>Closing the Evidence Gap: reddemcee, a Fast Adaptive Parallel Tempering Sampler</title>
      <link>https://arxiv.org/abs/2509.24870</link>
      <description>arXiv:2509.24870v1 Announce Type: cross 
Abstract: Markov Chain Monte Carlo (MCMC) excels at sampling complex posteriors but traditionally lags behind nested sampling in accurate evidence estimation, which is crucial for model comparison in astrophysical problems. We introduce reddemcee, an Adaptive Parallel Tempering Ensemble Sampler, aiming to close this gap by simultaneously presenting next-generation automated temperature-ladder adaptation techniques and robust, low-bias evidence estimators. reddemcee couples an affine-invariant stretch move with five interchangeable ladder-adaptation objectives, Uniform Swap Acceptance Rate, Swap Mean Distance, Gaussian-Area Overlap, Small Gaussian Gap, and Equalised Thermodynamic Length, implemented through a common differential update rule. Three evidence estimators are provided: Curvature-aware Thermodynamic Integration (TI+), Geometric-Bridge Stepping Stones (SS+), and a novel Hybrid algorithm that blends both approaches (H+). Performance and accuracy are benchmarked on n-dimensional Gaussian Shells, Gaussian Egg-box, Rosenbrock Functions, and exoplanet radial-velocity time-series of HD 20794. Across Shells up to 15 dimensions, reddemcee presents roughly 7 times the effective sampling speed of the best dynamic nested sampling configuration. The TI+, SS+ and H+ estimators recover estimates under 3 percent error and supply realistic uncertainties with as few as six temperatures. In the HD 20794 case study, reddemcee reproduces literature model rankings and yields tighter yet consistent planetary parameters compared with dynesty, with evidence errors that track run-to-run dispersion. By unifying fast ladder adaptation with reliable evidence estimators, reddemcee delivers strong throughput and accurate evidence estimates, often matching, and occasionally surpassing, dynamic nested sampling, while preserving the rich posterior information which makes MCMC indispensable for modern Bayesian inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24870v1</guid>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo A. Pe\~na R., James S. Jenkins</dc:creator>
    </item>
    <item>
      <title>The Limits of Inference in Complex Systems: When Stochastic Models Become Indistinguishable</title>
      <link>https://arxiv.org/abs/2509.24977</link>
      <description>arXiv:2509.24977v1 Announce Type: cross 
Abstract: Robust inference methods are essential for parameter estimation and model selection in stochastic modeling approaches, which provide interpretable frameworks for describing time series of complex phenomena. However, applying such methods is often challenging, as they typically demand either high-frequency observations or access to the model's analytical solution, resources that are rarely available in practice. Here, we address these limitations by designing a novel Monte Carlo method based on full-path statistics and bridge processes, which optimize sampling efforts and performance even under coarse sampling. We systematically investigate how experimental design -- particularly sampling frequency and dataset size -- shapes inference accuracy, revealing optimal sampling regimes and the fundamental limits of model distinguishability. We validate our approach on four datasets -- optical tweezers, human microbiome, topic mentions in social media, and forest population dynamics -- where resolution-dependent limits to inference emerge, offering fresh insight into ongoing debates about the dominant sources of noise in these systems. Overall, this work shows how combining minimal stochastic models with path-inference tools and model selection can guide the experimental design of optimized strategies in data collection and clarify the boundaries of model-based understanding in complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24977v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Javier Aguilar, Miguel A. Mu\~noz, Sandro Azaele</dc:creator>
    </item>
    <item>
      <title>Markov Processes for Enhanced Deepfake Generation and Detection</title>
      <link>https://arxiv.org/abs/2411.07993</link>
      <description>arXiv:2411.07993v3 Announce Type: replace 
Abstract: New and existing methods for generating, and especially detecting, deepfakes are investigated and compared on the simple problem of authenticating coin flip data. Importantly, an alternative approach to deepfake generation and detection, which uses a Markov Observation Model (MOM) is introduced and compared on detection ability to the traditional Generative Adversarial Network (GAN) approach as well as Support Vector Machine (SVM), Branching Particle Filtering (BPF) and human alternatives. MOM was also compared on generative and discrimination ability to GAN, filtering and humans (as SVM does not have generative ability). Humans are shown to perform the worst, followed in order by GAN, SVM, BPF and MOM, which was the best at the detection of deepfakes. Unsurprisingly, the order was maintained on the generation problem with removal of SVM as it does not have generation ability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07993v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael A. Kouritzin, Ian Zhang, Jyoti Bhadana, Seoyeon Park</dc:creator>
    </item>
    <item>
      <title>From group stage to league format: The impact of the draw in European football's Champions League</title>
      <link>https://arxiv.org/abs/2507.15320</link>
      <description>arXiv:2507.15320v3 Announce Type: replace 
Abstract: A fundamental reform has been introduced in the 2024/25 season of club competitions organised by the Union of European Football Associations (UEFA): the well-established group stage has been replaced by an incomplete round-robin format. In this format, the 36 teams are ranked in a single league table, but play against only a subset of the competitors. While this innovative change has highlighted that the incomplete round-robin tournament is a reasonable alternative to the standard design of allocating the teams into round-robin groups, the characteristics of the new format remain unexplored. Our paper contributes to this topic by using simulations to compare the uncertainty generated by the draw in the old format with that in the new format of the UEFA Champions League. We develop a method to break down the impact of the 2024/25 reform into various components for each team. The new format is found to decrease the overall effect of the draw. However, this reduction can mainly be attributed to the inaccurate seeding system used by UEFA. If the teams are seeded based on their actual strengths, the impact of the draw is about the same in a tournament with an incomplete round-robin league or a group stage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15320v3</guid>
      <category>stat.AP</category>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, Andr\'as Gyimesi, Dries Goossens, Karel Devriesere, Roel Lambers, Frits Spieksma</dc:creator>
    </item>
    <item>
      <title>Sampling effects on Lasso estimation of drift functions in high-dimensional diffusion processes</title>
      <link>https://arxiv.org/abs/2408.08638</link>
      <description>arXiv:2408.08638v3 Announce Type: replace-cross 
Abstract: In this paper, we address high-dimensional parametric estimation of the drift function in diffusion models, specifically focusing on a $d$-dimensional ergodic diffusion process observed at discrete time points. We consider both a general linear form for the drift function and the particular case of the Ornstein-Uhlenbeck (OU) process. Assuming sparsity of the parameter vector, we examine the statistical behavior of the Lasso estimator for the unknown parameter. Our primary contribution is the proof of an oracle inequality for the Lasso estimator, which holds on the intersection of three specific sets defined for our analysis. We carefully control the probability of these sets, tackling the central challenge of our study. This approach allows us to derive error bounds for the $l_1$ and $l_2$ norms, assessing the performance of the proposed Lasso estimator. Our results demonstrate that, under certain conditions, the discretization error becomes negligible, enabling us to achieve the same optimal rate of convergence as if the continuous trajectory of the process were observed. We validate our theoretical findings through numerical experiments, which show that the Lasso estimator significantly outperforms the maximum likelihood estimator (MLE) in terms of support recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08638v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Amorino, Francisco Pina, Mark Podolskij</dc:creator>
    </item>
    <item>
      <title>changepointGA: An R package for Fast Changepoint Detection via Genetic Algorithm</title>
      <link>https://arxiv.org/abs/2410.15571</link>
      <description>arXiv:2410.15571v2 Announce Type: replace-cross 
Abstract: Detecting changepoints in a time series of length $N$ entails evaluating up to $2^{N-1}$ possible changepoint models, making exhaustive enumeration computationally infeasible. Genetic algorithms (GAs) provide a stochastic way to identify the structural changes: a population of candidate models evolves via selection, crossover, and mutation operators until it converges on one changepoint model that balances the goodness-of-fit with parsimony. The R package changepointGA encodes each candidate model as an integer chromosome vector and supports both the basic single-population model GA and the island model GA. Parallel computing is implemented on multi-core hardware to further accelerate computation. Users may supply custom fitness functions or genetic operators, while a user-friendly wrapper streamlines routine analyses. Extensive simulations demonstrate that our package runs significantly faster than binary-encoded GA alternatives. Additionally, this package can simultaneously locate changepoints and estimate their effects, as well as other model parameters and any integer-valued hyperparameters. Applications to array-based comparative genomic hybridization data and a century-long temperature series further highlight the package's value in biological and climate research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15571v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mo Li, QiQi Lu</dc:creator>
    </item>
    <item>
      <title>Conformal prediction of future insurance claims in the regression problem</title>
      <link>https://arxiv.org/abs/2503.03659</link>
      <description>arXiv:2503.03659v3 Announce Type: replace-cross 
Abstract: In the current insurance literature, prediction of insurance claims in the regression problem is often performed with a statistical model. This model-based approach may potentially suffer from several drawbacks: (i) model misspecification, (ii) selection effect, and (iii) lack of finite-sample validity. This article addresses these three issues simultaneously by employing conformal prediction -- a general machine learning strategy for valid predictions. The proposed method is both model-free and tuning-parameter-free. It also guarantees finite-sample validity at a pre-assigned coverage probability level. Examples, based on both simulated and real data, are provided to demonstrate the excellent performance of the proposed method and its applications in insurance, especially regarding meeting the solvency capital requirement of European insurance regulation, Solvency II.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03659v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liang Hong</dc:creator>
    </item>
    <item>
      <title>A Nonparametric Bayesian Local-Global Model for Enhanced Adverse Event Signal Detection in Spontaneous Reporting System Data</title>
      <link>https://arxiv.org/abs/2504.10881</link>
      <description>arXiv:2504.10881v2 Announce Type: replace-cross 
Abstract: Spontaneous reporting system databases are key resources for post-marketing surveillance, providing real-world evidence (RWE) on the adverse events (AEs) of regulated drugs or other medical products. Various statistical methods have been proposed for AE signal detection in these databases, flagging drug-specific AEs with disproportionately high observed counts compared to expected counts under independence. However, signal detection remains challenging for rare AEs or newer drugs, which receive small observed and expected counts and thus suffer from reduced statistical power. Principled information sharing on signal strengths across drugs/AEs is crucial in such cases to enhance signal detection. However, existing methods typically ignore complex between-drug associations on AE signal strengths, limiting their ability to detect signals. We propose novel local-global mixture Dirichlet process (DP) prior-based nonparametric Bayesian models to capture these associations, enabling principled information sharing between drugs while balancing flexibility and shrinkage for each drug, thereby enhancing statistical power. We develop efficient Markov chain Monte Carlo algorithms for implementation and employ a false discovery rate (FDR)-controlled, false negative rate (FNR)-optimized hypothesis testing framework for AE signal detection. Extensive simulations demonstrate our methods' superior sensitivity -- often surpassing existing approaches by a twofold or greater margin -- while strictly controlling the FDR. An application to FDA FAERS data on statin drugs further highlights our methods' effectiveness in real-world AE signal detection. Software implementing our methods is provided as supplementary material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10881v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xin-Wei Huang, Saptarshi Chakraborty</dc:creator>
    </item>
    <item>
      <title>Flexible and Efficient Drift Detection without Labels</title>
      <link>https://arxiv.org/abs/2506.08734</link>
      <description>arXiv:2506.08734v2 Announce Type: replace-cross 
Abstract: Machine learning models are being increasingly used to automate decisions in almost every domain, and ensuring the performance of these models is crucial for ensuring high quality machine learning enabled services. Ensuring concept drift is detected early is thus of the highest importance. A lot of research on concept drift has focused on the supervised case that assumes the true labels of supervised tasks are available immediately after making predictions. Controlling for false positives while monitoring the performance of predictive models used to make inference from extremely large datasets periodically, where the true labels are not instantly available, becomes extremely challenging. We propose a flexible and efficient concept drift detection algorithm that uses classical statistical process control in a label-less setting to accurately detect concept drifts. We show empirically that under computational constraints, our approach has better statistical power than previous known methods. Furthermore, we introduce a new semi-supervised drift detection framework to model the scenario of detecting drift (without labels) given prior detections, and show how our drift detection algorithm can be incorporated effectively into this framework. We demonstrate promising performance via numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08734v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nelvin Tan, Yu-Ching Shih, Dong Yang, Amol Salunkhe</dc:creator>
    </item>
    <item>
      <title>Diffusion models for multivariate subsurface generation and efficient probabilistic inversion</title>
      <link>https://arxiv.org/abs/2507.15809</link>
      <description>arXiv:2507.15809v3 Announce Type: replace-cross 
Abstract: Diffusion models offer stable training and state-of-the-art performance for deep generative modeling tasks. Here, we consider their use in the context of multivariate subsurface modeling and probabilistic inversion. We first demonstrate that diffusion models enhance multivariate modeling capabilities compared to variational autoencoders and generative adversarial networks. In diffusion modeling, the generative process involves a comparatively large number of time steps with update rules that can be modified to account for conditioning data. We propose different corrections to the popular Diffusion Posterior Sampling approach by Chung et al. (2023). In particular, we introduce a likelihood approximation accounting for the noise-contamination that is inherent in diffusion modeling. We assess performance in a multivariate geological scenario involving facies and correlated acoustic impedance. Conditional modeling is demonstrated using both local hard data (well logs) and nonlinear geophysics (fullstack seismic data). Our tests show significantly improved statistical robustness, enhanced sampling of the posterior probability density function and reduced computational costs, compared to the original approach. The method can be used with both hard and indirect conditioning data, individually or simultaneously. As the inversion is included within the diffusion process, it is faster than other methods requiring an outer-loop around the generative model, such as Markov chain Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15809v3</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Roberto Miele, Niklas Linde</dc:creator>
    </item>
    <item>
      <title>Power new generalized class of Kavya-Manoharan distributions with an application to exponential distribution</title>
      <link>https://arxiv.org/abs/2508.18930</link>
      <description>arXiv:2508.18930v2 Announce Type: replace-cross 
Abstract: Recently, Verma et al. (2025) introduced a novel generalized class of Kavya-Manoharan distributions, which have demonstrated significant utility in reliability analysis and the modeling of lifetime data. This paper proposes an extension of this class by applying the power generalization technique, thereby enhancing more flexibility and applicability. We take the exponential distribution as the baseline distribution to introduce a new model capable of accommodating both monotonic and non-monotonic hazard rate functions. Our model includes eleven submodels. We present several statistical properties of the introduced model, including moments, generating and characteristic functions, mean deviations, quantile function, mean residual life function, R\'enyi entropy, order statistics, and reliability. To estimate the unknown model parameters, we use the maximum likelihood approach. A simulation study is conducted to assess the validity of the maximum likelihood estimator. The superiority of the new distribution is demonstrated through the use of a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18930v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lazhar Benkhelifa</dc:creator>
    </item>
  </channel>
</rss>
