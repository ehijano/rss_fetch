<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Feb 2026 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Nationwide Hourly Population Estimating at the Neighborhood Scale in the United States Using Stable-Attendance Anchor Calibration</title>
      <link>https://arxiv.org/abs/2602.12291</link>
      <description>arXiv:2602.12291v1 Announce Type: new 
Abstract: Traditional population datasets are largely static and therefore unable to capture the strong temporal dynamics of human presence driven by daily mobility. Recent smartphone-based mobility data offer unprecedented spatiotemporal coverage, yet translating these opportunistic observations into accurate population estimates remains challenging due to incomplete sensing, spatially heterogeneous device penetration, and unstable observation processes. We propose a Stable-Attendance Anchor Calibration (SAAC) framework to reconstruct hourly population presence at the Census block group level across the United States. SAAC formulates population estimation as a balance-based population accounting problem, combining residential population with time-varying inbound and outbound mobility inferred from device-event observations. To address observation bias and identifiability limitations, the framework leverages locations with highly regular attendance as calibration anchors, using high schools in this study. These anchors enable estimation of observation scaling factors that correct for under-recorded mobility events. By integrating anchor-based calibration with an explicit sampling model, SAAC enables consistent conversion from observed device events to population presence at fine temporal resolution. The inferred population patterns are consistent with established empirical findings in prior mobility and urban population studies. SAAC provides a generalizable framework for transforming large-scale, biased digital trace data into interpretable dynamic population products, with implications for urban science, public health, and human mobility research. The hourly population estimates can be accessed at: https://gladcolor.github.io/hourly_population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12291v1</guid>
      <category>stat.AP</category>
      <category>cs.IR</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huan Ning, Zhenlong Li, Manzhu Yu, Xiao Huang, Shiyan Zhang, Shan Qiao</dc:creator>
    </item>
    <item>
      <title>Statistical Opportunities in Neuroimaging</title>
      <link>https://arxiv.org/abs/2602.12974</link>
      <description>arXiv:2602.12974v1 Announce Type: new 
Abstract: Neuroimaging has profoundly enhanced our understanding of the human brain by characterizing its structure, function, and connectivity through modalities like MRI, fMRI, EEG, and PET. These technologies have enabled major breakthroughs across the lifespan, from early brain development to neurodegenerative and neuropsychiatric disorders. Despite these advances, the brain is a complex, multiscale system, and neuroimaging measurements are correspondingly high-dimensional. This creates major statistical challenges, including measurement noise, motion-related artifacts, substantial inter-subject and site/scanner variability, and the sheer scale of modern studies. This paper explores statistical opportunities and challenges in neuroimaging across four key areas: (i) brain development from birth to age 20, (ii) the adult and aging brain, (iii) neurodegeneration and neuropsychiatric disorders, and (iv) brain encoding and decoding. After a quick tutorial on major imaging technologies, we review cutting-edge studies, underscore data and modeling challenges, and highlight research opportunities for statisticians. We conclude by emphasizing that close collaboration among statisticians, neuroscientists, and clinicians is essential for translating neuroimaging advances into improved diagnostics, deeper mechanistic insight, and more personalized treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12974v1</guid>
      <category>stat.AP</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jian Kang, Thomas Nichols, Lexin Li, Martin A. Lindquist, Hongtu Zhu</dc:creator>
    </item>
    <item>
      <title>Conjugate Variational Inference for Large Mixed Multinomial Logit Models and Consumer Choice</title>
      <link>https://arxiv.org/abs/2602.12577</link>
      <description>arXiv:2602.12577v1 Announce Type: cross 
Abstract: Heterogeneity in multinomial choice data is often accounted for using logit models with random coefficients. Such models are called "mixed", but they can be difficult to estimate for large datasets. We review current Bayesian variational inference (VI) methods that can do so, and propose a new VI method that scales more effectively. The key innovation is a step that updates efficiently a Gaussian approximation to the conditional posterior of the random coefficients, addressing a bottleneck within the variational optimization. The approach is used to estimate three types of mixed logit models: standard, nested and bundle variants. We first demonstrate the improvement of our new approach over existing VI methods using simulations. Our method is then applied to a large scanner panel dataset of pasta choice. We find consumer response to price and promotion variables exhibits substantial heterogeneity at the grocery store and product levels. Store size, premium and geography are found to be drivers of store level estimates of price elasticities. Extension to bundle choice with pasta sauce improves model accuracy further. Predictions from the mixed models are more accurate than those from fixed coefficients equivalents, and our VI method provides insights in circumstances which other methods find challenging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12577v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weiben Zhang, Ruben Loaiza-Maya, Michael Stanley Smith, Worapree Maneesoonthorn</dc:creator>
    </item>
    <item>
      <title>On the relation between Global VAR Models and Matrix Time Series Models with Multiple Terms</title>
      <link>https://arxiv.org/abs/2602.12710</link>
      <description>arXiv:2602.12710v1 Announce Type: cross 
Abstract: Matrix valued time series (MaTS) and global vector autoregressive (GVAR) models both impose restrictions on the general VAR for multidimensional data sets, in order to bring down the number of parameters. Both models are motivated from a different viewpoint such that on first sight they do not have much in common. When investigating the models more closely, however, one notices many connections between the two model sets. This paper investigates the relations between the restrictions imposed by the two models. We show that under appropriate restrictions in both models we obtain a joint framework allowing to gain insight into the nature of GVARs from the viewpoint of MaTS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12710v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dietmar Bauer Kurtulus Kidik</dc:creator>
    </item>
    <item>
      <title>Simulating the Power of Statistical Tests: A Collection of R Examples</title>
      <link>https://arxiv.org/abs/2110.09836</link>
      <description>arXiv:2110.09836v3 Announce Type: replace 
Abstract: This paper illustrates how to calculate the power of a statistical test by computer simulation. It provides R code for power simulations of several classical inference procedures including one- and two-sample t tests, chi-squared tests, regression, and analysis of variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.09836v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Wickelmaier</dc:creator>
    </item>
    <item>
      <title>Excess risk of heat-related hospitalization associated with temperature and PM2.5 among older adults</title>
      <link>https://arxiv.org/abs/2505.07662</link>
      <description>arXiv:2505.07662v2 Announce Type: replace 
Abstract: Background: With rising temperatures and an aging population, understanding how to prevent heat-related illness among older adults will be increasingly crucial. Despite biological plausibility, no study to date has investigated whether fine particulate matter air pollution (PM2.5) contributes to the risk of hospitalization with a diagnosis code indicating heat-related illness, referred to as heat-related hospitalization. This study aims to fill this gap by investigating the independent and combined effects of temperature and PM2.5 on heat-related hospitalization risk.
  Methods: We identified Medicare fee-for-service beneficiaries in the contiguous United States who experienced a heat-related hospitalization between 2008 and 2016. Using a case-crossover design and Bayesian conditional logistic regression, we characterized the associations of temperature and PM2.5 with heat-related hospitalization. We then estimated the relative excess risk due to interaction to quantify the additive interaction of simultaneous exposure to heat and PM2.5.
  Results: We observed 112,969 heat-related hospitalizations. Fixing PM2.5 at the case day median, the odds ratio for increasing temperature from its case day median to the 95th percentile was 1.05 (95% CI: 1.03, 1.06). Fixing temperature at the case day median, the odds ratio for increasing PM2.5 from its median to the 95th percentile was 1.01 (95% CI: 0.99, 1.04). The relative excess risk due to interaction for simultaneous median-to-95th percentile increases in temperature and PM2.5 was 0.03 (95% CI: 0.01, 0.06).
  Conclusions: Our study is the first to observe synergism between temperature and PM2.5 associated with the risk of heat-related hospitalization. These findings highlight the importance of considering air pollution in effective public health and clinical interventions to prevent heat-related illness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07662v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1097/EE9.0000000000000451</arxiv:DOI>
      <arxiv:journal_reference>Environmental Epidemiology 10(1):p e451, February 2026</arxiv:journal_reference>
      <dc:creator>Lauren Mock, Rachel C. Nethery, Poonam Gandhi, Ashwaghosha Parthasarathi, Melanie Rua, David Robinson, Soko Setoguchi, Kevin Josey</dc:creator>
    </item>
    <item>
      <title>Variational phylogenetic inference with products over bipartitions</title>
      <link>https://arxiv.org/abs/2502.15110</link>
      <description>arXiv:2502.15110v2 Announce Type: replace-cross 
Abstract: Bayesian phylogenetics is vital for understanding evolutionary dynamics, and requires accurate and efficient approximation of posterior distributions over trees. In this work, we develop a variational Bayesian approach for ultrametric phylogenetic trees. We present a novel variational family based on coalescent times of a single-linkage clustering and derive a closed-form density for the resulting distribution over trees. Unlike existing methods for ultrametric trees, our method performs inference over all of tree space, it does not require any Markov chain Monte Carlo subroutines, and our variational family is differentiable. Through experiments on benchmark genomic datasets and an application to the viral RNA of SARS-CoV-2, we demonstrate that our method achieves competitive accuracy while requiring significantly fewer gradient evaluations than existing state-of-the-art techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15110v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evan Sidrow, Alexandre Bouchard-C\^ot\'e, Lloyd T. Elliott</dc:creator>
    </item>
    <item>
      <title>SDE-based Monte Carlo dose calculation for proton therapy validated against Geant4</title>
      <link>https://arxiv.org/abs/2511.03115</link>
      <description>arXiv:2511.03115v2 Announce Type: replace-cross 
Abstract: Objective: To assess the accuracy and computational performance of a stochastic differential equation (SDE)--based model for proton beam dose calculation by benchmarking against Geant4 in simplified phantom geometries. Approach: Building on Crossley et al. (2025), we implemented the SDE model using standard approximations to interaction cross sections and mean excitation energies, enabling straightforward adaptation to new materials and configurations. The model was benchmarked against Geant4 in homogeneous, longitudinally heterogeneous and laterally heterogeneous phantoms to assess depth--dose behaviour, lateral transport and material heterogeneities. Main results: Across all phantoms and beam energies, the SDE model reproduced the main depth--dose characteristics predicted by Geant4, with proton range agreement within 0.2 mm for 100 MeV beams and 0.6 mm for 150 MeV beams. Voxel--wise comparisons yielded gamma pass rates exceeding 95% under 2%/0.5 mm criteria with a 1% dose threshold. Differences were localised to steep dose gradients or material interfaces, while overall lateral beam dispersion was well reproduced. The SDE model achieved speed-up factors of about 2.5--3 relative to single-threaded Geant4. Significance: The SDE approach reproduces key dosimetric features with good accuracy at lower computational cost and is amenable to parallel and GPU implementations, supporting fast proton therapy dose calculations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03115v2</guid>
      <category>physics.med-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher B. C. Dean, Maria L. P\'erez-Lara, Emma Horton, Matthew Southerby, Jere Koskela, Andreas E. Kyprianou</dc:creator>
    </item>
    <item>
      <title>Boundary Discontinuity Designs: Theory and Practice</title>
      <link>https://arxiv.org/abs/2511.06474</link>
      <description>arXiv:2511.06474v3 Announce Type: replace-cross 
Abstract: The boundary discontinuity (BD) design is a non-experimental method for identifying causal effects that exploits a thresholding rule based on a bivariate score and a boundary curve. This widely used method generalizes the univariate regression discontinuity design but introduces unique challenges arising from its multidimensional nature. We synthesize over 80 empirical papers that use the BD design, tracing the method's application from its formative stages to its implementation in modern research. We also overview ongoing theoretical and methodological research on identification, estimation, and inference for BD designs employing local polynomial regression, and offer recommendations for practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06474v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>Extreme-Path Benchmarks for Sequential Probability Forecasts</title>
      <link>https://arxiv.org/abs/2601.18774</link>
      <description>arXiv:2601.18774v2 Announce Type: replace-cross 
Abstract: Real-time probability forecasts for binary outcomes are routine in sports, online experimentation, medicine, and finance. Retrospective narratives, however, often hinge on pathwise extremes: for example, a forecast that reaches $90\%$ for an event that ultimately does not occur. Standard pointwise calibration tools (e.g. reliability diagrams) do not quantify how frequently such extremes should occur under correct sequential calibration. Under this ideal, the forecast path $p_k=\Pr(Y=1\mid F_k)$ is a bounded martingale with terminal value $p_N=Y\in\{0,1\}$. We derive benchmark distributions for extreme-path functionals conditional on the terminal outcome, emphasizing the peak-on-loss statistic $M_N=\max_{k\le N} p_k$ given $Y=0$. For continuous-time martingales with continuous sample paths, we obtain an exact identity for $\Pr(\sup_{t\in[0,1]}p_t\ge x\mid Y=0)$. In discrete time, we prove sharp finite-sample bounds and an explicit correction decomposition that isolates terminal-step crossings (non-attainment) and overshoots. These formulas provide model-agnostic null targets and one-sided tail probabilities (exact in the continuous-path setting; conservative in discrete time) for diagnosing sequential miscalibration from extreme-path behavior. We also develop competitive extensions tailored to win-probability feeds, including the eventual loser's peak win probability in two-outcome contests and the eventual winner's trough in $n$-outcome contests. An empirical illustration using ESPN win-probability series for NFL and NBA regular-season games (2018-2024) finds broad agreement with the benchmark in the NFL and systematic departures in the NBA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18774v2</guid>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Pipping-Gam\'on, Abraham J. Wyner</dc:creator>
    </item>
  </channel>
</rss>
