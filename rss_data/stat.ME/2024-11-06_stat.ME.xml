<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Nov 2024 02:44:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Comment on 'Sparse Bayesian Factor Analysis when the Number of Factors is Unknown' by S. Fr\"uhwirth-Schnatter, D. Hosszejni, and H. Freitas Lopes written by Roberto Casarin and Antonio Peruzzi (Ca' Foscari University of Venice)</title>
      <link>https://arxiv.org/abs/2411.02531</link>
      <description>arXiv:2411.02531v1 Announce Type: new 
Abstract: The techniques suggested in Fr\"uhwirth-Schnatter et al. (2024) concern sparsity and factor selection and have enormous potential beyond standard factor analysis applications. We show how these techniques can be applied to Latent Space (LS) models for network data. These models suffer from well-known identification issues of the latent factors due to likelihood invariance to factor translation, reflection, and rotation (see Hoff et al., 2002). A set of observables can be instrumental in identifying the latent factors via auxiliary equations (see Liu et al., 2021). These, in turn, share many analogies with the equations used in factor modeling, and we argue that the factor loading restrictions may be beneficial for achieving identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02531v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Casarin, Antonio Peruzzi</dc:creator>
    </item>
    <item>
      <title>Restricted Win Probability with Bayesian Estimation for Implementing the Estimand Framework in Clinical Trials With a Time-to-Event Outcome</title>
      <link>https://arxiv.org/abs/2411.02755</link>
      <description>arXiv:2411.02755v1 Announce Type: new 
Abstract: We propose a restricted win probability estimand for comparing treatments in a randomized trial with a time-to-event outcome. We also propose Bayesian estimators for this summary measure as well as the unrestricted win probability. Bayesian estimation is scalable and facilitates seamless handling of censoring mechanisms as compared to related non-parametric pairwise approaches like win ratios. Unlike the log-rank test, these measures effectuate the estimand framework as they reflect a clearly defined population quantity related to the probability of a later event time with the potential restriction that event times exceeding a pre-specified time are deemed equivalent. We compare efficacy with established methods using computer simulation and apply the proposed approach to 304 reconstructed datasets from oncology trials. We show that the proposed approach has more power than the log-rank test in early treatment difference scenarios, and at least as much power as the win ratio in all scenarios considered. We also find that the proposed approach's statistical significance is concordant with the log-rank test for the vast majority of the oncology datasets examined. The proposed approach offers an interpretable, efficient alternative for trials with time-to-event outcomes that aligns with the estimand framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02755v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michelle Leeberg, Xianghua Luo, Thomas A. Murray</dc:creator>
    </item>
    <item>
      <title>Identifying nonlinear relations among random variables: A network analytic approach</title>
      <link>https://arxiv.org/abs/2411.02763</link>
      <description>arXiv:2411.02763v1 Announce Type: new 
Abstract: Nonlinear relations between variables, such as the curvilinear relationship between childhood trauma and resilience in patients with schizophrenia and the moderation relationship between mentalizing, and internalizing and externalizing symptoms and quality of life in youths, are more prevalent than our current methods have been able to detect. Although there has been a rise in network models, network construction for the standard Gaussian graphical model depends solely upon linearity. While nonlinear models are an active field of study in psychological methodology, many of these models require the analyst to specify the functional form of the relation. When performing more exploratory modeling, such as with cross-sectional network psychometrics, specifying the functional form a nonlinear relation might take becomes infeasible given the number of possible relations modeled. Here, we apply a nonparametric approach to identifying nonlinear relations using partial distance correlations. We found that partial distance correlations excel overall at identifying nonlinear relations regardless of functional form when compared with Pearson's and Spearman's partial correlations. Through simulation studies and an empirical example, we show that partial distance correlations can be used to identify possible nonlinear relations in psychometric networks, enabling researchers to then explore the shape of these relations with more confirmatory models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02763v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lindley R. Slipetz, Jiaxing Qiu, Siqi Sun, Teague R. Henry</dc:creator>
    </item>
    <item>
      <title>Assessment of Misspecification in CDMs Using a Generalized Information Matrix Test</title>
      <link>https://arxiv.org/abs/2411.02769</link>
      <description>arXiv:2411.02769v1 Announce Type: new 
Abstract: If the probability model is correctly specified, then we can estimate the covariance matrix of the asymptotic maximum likelihood estimate distribution using either the first or second derivatives of the likelihood function. Therefore, if the determinants of these two different covariance matrix estimation formulas differ this indicates model misspecification. This misspecification detection strategy is the basis of the Determinant Information Matrix Test ($GIMT_{Det}$). To investigate the performance of the $GIMT_{Det}$, a Deterministic Input Noisy And gate (DINA) Cognitive Diagnostic Model (CDM) was fit to the Fraction-Subtraction dataset. Next, various misspecified versions of the original DINA CDM were fit to bootstrap data sets generated by sampling from the original fitted DINA CDM. The $GIMT_{Det}$ showed good discrimination performance for larger levels of misspecification. In addition, the $GIMT_{Det}$ did not detect model misspecification when model misspecification was not present and additionally did not detect model misspecification when the level of misspecification was very low. However, the $GIMT_{Det}$ discrimation performance was highly variable across different misspecification strategies when the misspecification level was moderately sized. The proposed new misspecification detection methodology is promising but additional empirical studies are required to further characterize its strengths and limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02769v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-55548-0_33</arxiv:DOI>
      <arxiv:journal_reference>Quantitative Psychology. IMPS 2023, vol 452. Springer, Cham (2024)</arxiv:journal_reference>
      <dc:creator>Reyhaneh Hosseinpourkhoshkbari, Richard M. Golden</dc:creator>
    </item>
    <item>
      <title>Automatic doubly robust inference for linear functionals via calibrated debiased machine learning</title>
      <link>https://arxiv.org/abs/2411.02771</link>
      <description>arXiv:2411.02771v1 Announce Type: new 
Abstract: In causal inference, many estimands of interest can be expressed as a linear functional of the outcome regression function; this includes, for example, average causal effects of static, dynamic and stochastic interventions. For learning such estimands, in this work, we propose novel debiased machine learning estimators that are doubly robust asymptotically linear, thus providing not only doubly robust consistency but also facilitating doubly robust inference (e.g., confidence intervals and hypothesis tests). To do so, we first establish a key link between calibration, a machine learning technique typically used in prediction and classification tasks, and the conditions needed to achieve doubly robust asymptotic linearity. We then introduce calibrated debiased machine learning (C-DML), a unified framework for doubly robust inference, and propose a specific C-DML estimator that integrates cross-fitting, isotonic calibration, and debiased machine learning estimation. A C-DML estimator maintains asymptotic linearity when either the outcome regression or the Riesz representer of the linear functional is estimated sufficiently well, allowing the other to be estimated at arbitrarily slow rates or even inconsistently. We propose a simple bootstrap-assisted approach for constructing doubly robust confidence intervals. Our theoretical and empirical results support the use of C-DML to mitigate bias arising from the inconsistent or slow estimation of nuisance functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02771v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Alex Luedtke, Marco Carone</dc:creator>
    </item>
    <item>
      <title>Temporal Wasserstein Imputation: Versatile Missing Data Imputation for Time Series</title>
      <link>https://arxiv.org/abs/2411.02811</link>
      <description>arXiv:2411.02811v1 Announce Type: new 
Abstract: Missing data often significantly hamper standard time series analysis, yet in practice they are frequently encountered. In this paper, we introduce temporal Wasserstein imputation, a novel method for imputing missing data in time series. Unlike existing techniques, our approach is fully nonparametric, circumventing the need for model specification prior to imputation, making it suitable for potential nonlinear dynamics. Its principled algorithmic implementation can seamlessly handle univariate or multivariate time series with any missing pattern. In addition, the plausible range and side information of the missing entries (such as box constraints) can easily be incorporated. As a key advantage, our method mitigates the distributional bias typical of many existing approaches, ensuring more reliable downstream statistical analysis using the imputed series. Leveraging the benign landscape of the optimization formulation, we establish the convergence of an alternating minimization algorithm to critical points. Furthermore, we provide conditions under which the marginal distributions of the underlying time series can be identified. Our numerical experiments, including extensive simulations covering linear and nonlinear time series models and an application to a real-world groundwater dataset laden with missing data, corroborate the practical usefulness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02811v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuo-Chieh Huang, Tengyuan Liang, Ruey S. Tsay</dc:creator>
    </item>
    <item>
      <title>A joint model of correlated ordinal and continuous variables</title>
      <link>https://arxiv.org/abs/2411.02924</link>
      <description>arXiv:2411.02924v1 Announce Type: new 
Abstract: In this paper we build a joint model which can accommodate for binary, ordinal and continuous responses, by assuming that the errors of the continuous variables and the errors underlying the ordinal and binary outcomes follow a multivariate normal distribution. We employ composite likelihood methods to estimate the model parameters and use composite likelihood inference for model comparison and uncertainty quantification. The complimentary R package mvordnorm implements estimation of this model using composite likelihood methods and is available for download from Github. We present two use-cases in the area of risk management to illustrate our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02924v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Vana-G\"ur, Rainer Hirk</dc:creator>
    </item>
    <item>
      <title>On Distributional Discrepancy for Experimental Design with General Assignment Probabilities</title>
      <link>https://arxiv.org/abs/2411.02956</link>
      <description>arXiv:2411.02956v1 Announce Type: new 
Abstract: We investigate experimental design for randomized controlled trials (RCTs) with both equal and unequal treatment-control assignment probabilities. Our work makes progress on the connection between the distributional discrepancy minimization (DDM) problem introduced by Harshaw et al. (2024) and the design of RCTs. We make two main contributions: First, we prove that approximating the optimal solution of the DDM problem within even a certain constant error is NP-hard. Second, we introduce a new Multiplicative Weights Update (MWU) algorithm for the DDM problem, which improves the Gram-Schmidt walk algorithm used by Harshaw et al. (2024) when assignment probabilities are unequal. Building on the framework of Harshaw et al. (2024) and our MWU algorithm, we then develop the MWU design, which reduces the worst-case mean-squared error in estimating the average treatment effect. Finally, we present a comprehensive simulation study comparing our design with commonly used designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02956v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anup B. Rao, Peng Zhang</dc:creator>
    </item>
    <item>
      <title>Your copula is a classifier in disguise: classification-based copula density estimation</title>
      <link>https://arxiv.org/abs/2411.03014</link>
      <description>arXiv:2411.03014v1 Announce Type: new 
Abstract: We propose reinterpreting copula density estimation as a discriminative task. Under this novel estimation scheme, we train a classifier to distinguish samples from the joint density from those of the product of independent marginals, recovering the copula density in the process. We derive equivalences between well-known copula classes and classification problems naturally arising in our interpretation. Furthermore, we show our estimator achieves theoretical guarantees akin to maximum likelihood estimation. By identifying a connection with density ratio estimation, we benefit from the rich literature and models available for such problems. Empirically, we demonstrate the applicability of our approach by estimating copulas of real and high-dimensional datasets, outperforming competing copula estimators in density evaluation as well as sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03014v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Huk, Mark Steel, Ritabrata Dutta</dc:creator>
    </item>
    <item>
      <title>Modeling sparsity in count-weighted networks</title>
      <link>https://arxiv.org/abs/2411.03100</link>
      <description>arXiv:2411.03100v1 Announce Type: new 
Abstract: Community detection methods have been extensively studied to recover communities structures in network data. While many models and methods focus on binary data, real-world networks also present the strength of connections, which could be considered in the network analysis. We propose a probabilistic model for generating weighted networks that allows us to control network sparsity and incorporates degree corrections for each node. We propose a community detection method based on the Variational Expectation-Maximization (VEM) algorithm. We show that the proposed method works well in practice for simulated networks. We analyze the Brazilian airport network to compare the community structures before and during the COVID-19 pandemic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03100v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andressa Cerqueira, Laila L. S. Costa</dc:creator>
    </item>
    <item>
      <title>Bayesian Controlled FDR Variable Selection via Knockoffs</title>
      <link>https://arxiv.org/abs/2411.03304</link>
      <description>arXiv:2411.03304v1 Announce Type: new 
Abstract: In many research fields, researchers aim to identify significant associations between a set of explanatory variables and a response while controlling the false discovery rate (FDR). To this aim, we develop a fully Bayesian generalization of the classical model-X knockoff filter. Knockoff filter introduces controlled noise in the model in the form of cleverly constructed copies of the predictors as auxiliary variables. In our approach we consider the joint model of the covariates and the response and incorporate the conditional independence structure of the covariates into the prior distribution of the auxiliary knockoff variables. We further incorporate the estimation of a graphical model among the covariates, which in turn aids knockoffs generation and improves the estimation of the covariate effects on the response. We use a modified spike-and-slab prior on the regression coefficients, which avoids the increase of the model dimension as typical in the classical knockoff filter. Our model performs variable selection using an upper bound on the posterior probability of non-inclusion. We show how our model construction leads to valid model-X knockoffs and demonstrate that the proposed characterization is sufficient for controlling the BFDR at an arbitrary level, in finite samples. We also show that the model selection is robust to the estimation of the precision matrix. We use simulated data to demonstrate that our proposal increases the stability of the selection with respect to classical knockoff methods, as it relies on the entire posterior distribution of the knockoff variables instead of a single sample. With respect to Bayesian variable selection methods, we show that our selection procedure achieves comparable or better performances, while maintaining control over the FDR. Finally, we show the usefulness of the proposed model with an application to real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03304v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lorenzo Focardi-Olmi, Anna Gottard, Michele Guindani, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Does Regression Produce Representative Causal Rankings?</title>
      <link>https://arxiv.org/abs/2411.02675</link>
      <description>arXiv:2411.02675v1 Announce Type: cross 
Abstract: We examine the challenges in ranking multiple treatments based on their estimated effects when using linear regression or its popular double-machine-learning variant, the Partially Linear Model (PLM), in the presence of treatment effect heterogeneity. We demonstrate by example that overlap-weighting performed by linear models like PLM can produce Weighted Average Treatment Effects (WATE) that have rankings that are inconsistent with the rankings of the underlying Average Treatment Effects (ATE). We define this as ranking reversals and derive a necessary and sufficient condition for ranking reversals under the PLM. We conclude with several simulation studies conditions under which ranking reversals occur.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02675v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Apoorva Lal</dc:creator>
    </item>
    <item>
      <title>Elliptical Wishart distributions: information geometry, maximum likelihood estimator, performance analysis and statistical learning</title>
      <link>https://arxiv.org/abs/2411.02726</link>
      <description>arXiv:2411.02726v1 Announce Type: cross 
Abstract: This paper deals with Elliptical Wishart distributions - which generalize the Wishart distribution - in the context of signal processing and machine learning. Two algorithms to compute the maximum likelihood estimator (MLE) are proposed: a fixed point algorithm and a Riemannian optimization method based on the derived information geometry of Elliptical Wishart distributions. The existence and uniqueness of the MLE are characterized as well as the convergence of both estimation algorithms. Statistical properties of the MLE are also investigated such as consistency, asymptotic normality and an intrinsic version of Fisher efficiency. On the statistical learning side, novel classification and clustering methods are designed. For the $t$-Wishart distribution, the performance of the MLE and statistical learning algorithms are evaluated on both simulated and real EEG and hyperspectral data, showcasing the interest of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02726v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Imen Ayadi, Florent Bouchard, Fr\'ed\'eric Pascal</dc:creator>
    </item>
    <item>
      <title>When is it worthwhile to jackknife? Breaking the quadratic barrier for Z-estimators</title>
      <link>https://arxiv.org/abs/2411.02909</link>
      <description>arXiv:2411.02909v1 Announce Type: cross 
Abstract: Resampling methods are especially well-suited to inference with estimators that provide only "black-box'' access. Jackknife is a form of resampling, widely used for bias correction and variance estimation, that is well-understood under classical scaling where the sample size $n$ grows for a fixed problem. We study its behavior in application to estimating functionals using high-dimensional $Z$-estimators, allowing both the sample size $n$ and problem dimension $d$ to diverge. We begin showing that the plug-in estimator based on the $Z$-estimate suffers from a quadratic breakdown: while it is $\sqrt{n}$-consistent and asymptotically normal whenever $n \gtrsim d^2$, it fails for a broad class of problems whenever $n \lesssim d^2$. We then show that under suitable regularity conditions, applying a jackknife correction yields an estimate that is $\sqrt{n}$-consistent and asymptotically normal whenever $n\gtrsim d^{3/2}$. This provides strong motivation for the use of jackknife in high-dimensional problems where the dimension is moderate relative to sample size. We illustrate consequences of our general theory for various specific $Z$-estimators, including non-linear functionals in linear models; generalized linear models; and the inverse propensity score weighting (IPW) estimate for the average treatment effect, among others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02909v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Licong Lin, Fangzhou Su, Wenlong Mou, Peng Ding, Martin Wainwright</dc:creator>
    </item>
    <item>
      <title>Testing Generalizability in Causal Inference</title>
      <link>https://arxiv.org/abs/2411.03021</link>
      <description>arXiv:2411.03021v1 Announce Type: cross 
Abstract: Ensuring robust model performance across diverse real-world scenarios requires addressing both transportability across domains with covariate shifts and extrapolation beyond observed data ranges. However, there is no formal procedure for statistically evaluating generalizability in machine learning algorithms, particularly in causal inference. Existing methods often rely on arbitrary metrics like AUC or MSE and focus predominantly on toy datasets, providing limited insights into real-world applicability. To address this gap, we propose a systematic and quantitative framework for evaluating model generalizability under covariate distribution shifts, specifically within causal inference settings. Our approach leverages the frugal parameterization, allowing for flexible simulations from fully and semi-synthetic benchmarks, offering comprehensive evaluations for both mean and distributional regression methods. By basing simulations on real data, our method ensures more realistic evaluations, which is often missing in current work relying on simplified datasets. Furthermore, using simulations and statistical testing, our framework is robust and avoids over-reliance on conventional metrics. Grounded in real-world data, it provides realistic insights into model performance, bridging the gap between synthetic evaluations and practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03021v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel de Vassimon Manela, Linying Yang, Robin J. Evans</dc:creator>
    </item>
    <item>
      <title>Scale Reliant Inference</title>
      <link>https://arxiv.org/abs/2201.03616</link>
      <description>arXiv:2201.03616v5 Announce Type: replace 
Abstract: Many scientific fields, including human gut microbiome science, collect multivariate count data where the sum of the counts is unrelated to the scale of the underlying system being measured (e.g., total microbial load in a subject's colon). This disconnect complicates downstream analyses such as differential analysis in case-control studies. This article is motivated by a novel study of in vitro human gut microbiome models. Popular tools for analyzing these data led to dramatically elevated rates of both false positives and false negatives. To understand those failures, we provide a formal problem statement that frames these challenges of scale in terms of the classical theory of identifiability. We call this the problem of Scale Reliant Inference (SRI). We use this formulation to prove fundamental limits on SRI in terms of criteria such as consistency and type-I error control. We show that the failures of existing methods stem from a fundamental failure to properly quantify uncertainty in the system scale. We demonstrate that a particular type of Bayesian model called a Bayesian Partially Identified Model (PIMs) can correctly quantify uncertainty in SRI. We introduce Scale Simulation Random Variables (SSRVs) as a flexible and efficient approach to specifying and inferring Bayesian PIMs. In the context of both real and simulated data, we find SSRVs drastically decrease type-I and type-II error rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.03616v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle Pistner Nixon, Kyle C. McGovern, Jeffrey Letourneau, Lawrence A. David, Nicole A. Lazar, Sayan Mukherjee, Justin D. Silverman</dc:creator>
    </item>
    <item>
      <title>Efficient Case-Cohort Design using Balanced Sampling</title>
      <link>https://arxiv.org/abs/2311.05914</link>
      <description>arXiv:2311.05914v2 Announce Type: replace 
Abstract: A case-cohort design is a two-phase sampling design frequently used to analyze censored survival data in a cost-effective way, where a subcohort is usually selected using simple random sampling or stratified simple random sampling. In this paper, we propose an efficient sampling procedure based on balanced sampling when selecting a subcohort in a case-cohort design. A sample selected via a balanced sampling procedure automatically calibrates auxiliary variables. When fitting a Cox model, calibrating sampling weights has been shown to lead to more efficient estimators of the regression coefficients (Breslow et al., 2009a, b). The reduced variabilities over its counterpart with a simple random sampling are shown via extensive simulation experiments. The proposed design and estimation procedure are also illustrated with the well-known National Wilms Tumor Study dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05914v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaeum Choi, Sangwook Kang</dc:creator>
    </item>
    <item>
      <title>A Graph-based Approach to Estimating the Number of Clusters</title>
      <link>https://arxiv.org/abs/2402.15600</link>
      <description>arXiv:2402.15600v2 Announce Type: replace 
Abstract: We consider the problem of estimating the number of clusters (k) in a dataset. We propose a non-parametric approach to the problem that utilizes similarity graphs to construct a robust statistic that effectively captures similarity information among observations. This graph-based statistic is applicable to datasets of any dimension, is computationally efficient to obtain, and can be paired with any kind of clustering technique. Asymptotic theory is developed to establish the selection consistency of the proposed approach. Simulation studies demonstrate that the graph-based statistic outperforms existing methods for estimating k, especially in the high-dimensional setting. We illustrate its utility on an imaging dataset and an RNA-seq dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15600v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yichuan Bai, Lynna Chu</dc:creator>
    </item>
    <item>
      <title>Measuring Dependence between Events</title>
      <link>https://arxiv.org/abs/2403.17580</link>
      <description>arXiv:2403.17580v2 Announce Type: replace 
Abstract: Measuring dependence between two events, or equivalently between two binary random variables, amounts to expressing the dependence structure inherent in a $2\times 2$ contingency table in a real number between $-1$ and $1$. Countless such dependence measures exist, but there is little theoretical guidance on how they compare and on their advantages and shortcomings. Thus, practitioners might be overwhelmed by the problem of choosing a suitable measure. We provide a set of natural desirable properties that a proper dependence measure should fulfill. We show that Yule's Q and the little-known Cole coefficient are proper, while the most widely-used measures, the phi coefficient and all contingency coefficients, are improper. They have a severe attainability problem, that is, even under perfect dependence they can be very far away from $-1$ and $1$, and often differ substantially from the proper measures in that they understate strength of dependence. The structural reason is that these are measures for equality of events rather than of dependence. We derive the (in some instances non-standard) limiting distributions of the measures and illustrate how asymptotically valid confidence intervals can be constructed. In a case study on drug consumption we demonstrate how misleading conclusions may arise from the use of improper dependence measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17580v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc-Oliver Pohle, Timo Dimitriadis, Jan-Lukas Wermuth</dc:creator>
    </item>
    <item>
      <title>Double Robust Variance Estimation with Parametric Working Models</title>
      <link>https://arxiv.org/abs/2404.16166</link>
      <description>arXiv:2404.16166v2 Announce Type: replace 
Abstract: Doubly robust estimators have gained popularity in the field of causal inference due to their ability to provide consistent point estimates when either an outcome or exposure model is correctly specified. However, for nonrandomized exposures the influence function based variance estimator frequently used with doubly robust estimators of the average causal effect is only consistent when both working models (i.e., outcome and exposure models) are correctly specified. Here, the empirical sandwich variance estimator and the nonparametric bootstrap are demonstrated to be doubly robust variance estimators. That is, they are expected to provide valid estimates of the variance leading to nominal confidence interval coverage when only one working model is correctly specified. Simulation studies illustrate the properties of the influence function based, empirical sandwich, and nonparametric bootstrap variance estimators in the setting where parametric working models are assumed. Estimators are applied to data from the Improving Pregnancy Outcomes with Progesterone (IPOP) study to estimate the effect of maternal anemia on birth weight among women with HIV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16166v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bonnie E. Shook-Sa, Paul N. Zivich, Chanhwa Lee, Keyi Xue, Rachael K. Ross, Jessie K. Edwards, Jeffrey S. A. Stringer, Stephen R. Cole</dc:creator>
    </item>
    <item>
      <title>TrendLSW: Trend and Spectral Estimation of Nonstationary Time Series in R</title>
      <link>https://arxiv.org/abs/2406.05012</link>
      <description>arXiv:2406.05012v2 Announce Type: replace 
Abstract: The TrendLSW R package has been developed to provide users with a suite of wavelet-based techniques to analyse the statistical properties of nonstationary time series. The key components of the package are (a) two approaches for the estimation of the evolutionary wavelet spectrum in the presence of trend; and (b) wavelet-based trend estimation in the presence of locally stationary wavelet errors via both linear and nonlinear wavelet thresholding; and (c) the calculation of associated pointwise confidence intervals. Lastly, the package directly implements boundary handling methods that enable the methods to be performed on data of arbitrary length, not just dyadic length as is common for wavelet-based methods, ensuring no pre-processing of data is necessary. The key functionality of the package is demonstrated through two data examples, arising from biology and activity monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05012v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Euan T. McGonigle, Rebecca Killick, Matthew A. Nunes</dc:creator>
    </item>
    <item>
      <title>Using Platt's scaling for calibration after undersampling -- limitations and how to address them</title>
      <link>https://arxiv.org/abs/2410.18144</link>
      <description>arXiv:2410.18144v2 Announce Type: replace 
Abstract: When modelling data where the response is dichotomous and highly imbalanced, response-based sampling where a subset of the majority class is retained (i.e., undersampling) is often used to create more balanced training datasets prior to modelling. However, the models fit to this undersampled data, which we refer to as base models, generate predictions that are severely biased. There are several calibration methods that can be used to combat this bias, one of which is Platt's scaling. Here, a logistic regression model is used to model the relationship between the base model's original predictions and the response. Despite its popularity for calibrating models after undersampling, Platt's scaling was not designed for this purpose. Our work presents what we believe is the first detailed study focused on the validity of using Platt's scaling to calibrate models after undersampling. We show analytically, as well as via a simulation study and a case study, that Platt's scaling should not be used for calibration after undersampling without critical thought. If Platt's scaling would have been able to successfully calibrate the base model had it been trained on the entire dataset (i.e., without undersampling), then Platt's scaling might be appropriate for calibration after undersampling. If this is not the case, we recommend a modified version of Platt's scaling that fits a logistic generalized additive model to the logit of the base model's predictions, as it is both theoretically motivated and performed well across the settings considered in our study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18144v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Phelps, Daniel J. Lizotte, Douglas G. Woolford</dc:creator>
    </item>
    <item>
      <title>FRODO: A novel approach to micro-macro multilevel regression</title>
      <link>https://arxiv.org/abs/2411.01686</link>
      <description>arXiv:2411.01686v2 Announce Type: replace 
Abstract: Within the field of hierarchical modelling, little attention is paid to micro-macro models: those in which group-level outcomes are dependent on covariates measured at the level of individuals within groups. Although such models are perhaps underrepresented in the literature, they have applications in economics, epidemiology, and the social sciences. Despite the strong mathematical similarities between micro-macro and measurement error models, few efforts have been made to apply the much better-developed methodology of the latter to the former. Here, we present a new empirical Bayesian technique for micro-macro data, called FRODO (Functional Regression On Densities of Observations). The method jointly infers group-specific densities for multilevel covariates and uses them as functional predictors in a functional linear regression, resulting in a model that is analogous to a generalized additive model (GAM). In doing so, it achieves a level of generality comparable to more sophisticated methods developed for errors-in-variables models, while further leveraging the larger group sizes characteristic of multilevel data to provide richer information about the within-group covariate distributions. After explaining the hierarchical structure of FRODO, its power and versatility are demonstrated on several simulated datasets, showcasing its ability to accommodate a wide variety of covariate distributions and regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01686v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaun McDonald, Alexandre Leblanc, Saman Muthukumarana, David Campbell</dc:creator>
    </item>
    <item>
      <title>Using Multiple Outcomes to Improve the Synthetic Control Method</title>
      <link>https://arxiv.org/abs/2311.16260</link>
      <description>arXiv:2311.16260v2 Announce Type: replace-cross 
Abstract: When there are multiple outcome series of interest, Synthetic Control analyses typically proceed by estimating separate weights for each outcome. In this paper, we instead propose estimating a common set of weights across outcomes, by balancing either a vector of all outcomes or an index or average of them. Under a low-rank factor model, we show that these approaches lead to lower bias bounds than separate weights, and that averaging leads to further gains when the number of outcomes grows. We illustrate this via a re-analysis of the impact of the Flint water crisis on educational outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16260v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liyang Sun, Eli Ben-Michael, Avi Feller</dc:creator>
    </item>
  </channel>
</rss>
