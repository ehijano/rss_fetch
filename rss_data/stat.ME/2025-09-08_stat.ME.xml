<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Sep 2025 02:23:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Improving prediction in M-estimation by integrating external information from heterogeneous populations</title>
      <link>https://arxiv.org/abs/2509.04609</link>
      <description>arXiv:2509.04609v1 Announce Type: new 
Abstract: A novel approach to improve prediction and inference in M-estimation by integrating external information from heterogeneous populations is proposed. Our method leverages joint asymptotics to combine estimates from external and internal datasets, where the external dataset provides auxiliary information about a subset of parameters of interest. We introduce a shrinkage estimator that combines internal and external estimates under a general class of transformations that ensure consistency across populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04609v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Walter Dempsey, Jeremy M. G. Taylor</dc:creator>
    </item>
    <item>
      <title>Composite method for fast computation of individual level spatial epidemic models</title>
      <link>https://arxiv.org/abs/2509.04660</link>
      <description>arXiv:2509.04660v1 Announce Type: new 
Abstract: Individual-level models, also known as ILMs, are commonly used in epidemics modelling, as they can flexibly incorporate individual-level covariates that influence susceptibility and transmissibility upon infection. However, inference for ILMs is computationally intensive, especially as the total population size increases and additional covariates are incorporated. We propose a composite method, the composite ILM (C-ILM), that clusters the population into minimally-interfered subpopulations, with between-cluster infections enabled through a ``spark function.'' This approach allows for parallel computation of subsets before aggregation. Focusing on C-ILM, we consider four ``spark functions'', and introduce a Dirichlet process mixture modelling (DPMM) algorithm for clustering. Simulation results indicate that, in addition to faster computation, C-ILM performs well in parameter estimation and posterior predictions. Furthermore, within C-ILM framework, DPMM algorithm demonstrates superior performance compared to the conventional $K$-means algorithm. We apply the methods to data from the 2001 UK foot-and-mouth disease outbreak. The results provide evidence that C-ILM is not only computationally efficient but also achieves a better model fit compared to the basic spatial ILM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04660v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yirao Zhang, Rob Deardon, Lorna Deeth</dc:creator>
    </item>
    <item>
      <title>When correcting for regression to the mean is worse than no correction at all</title>
      <link>https://arxiv.org/abs/2509.04718</link>
      <description>arXiv:2509.04718v1 Announce Type: new 
Abstract: The ubiquitous regression to the mean (RTM) effect complicates statistical inference in biological studies of change. We demonstrate that common RTM correction methods are flawed: the Berry et al. method popularized by Kelly &amp; Price in The American Naturalist is unreliable for hypothesis testing, leading to both false positives and negatives, while the theoretically unbiased Blomqvist method has poor efficiency in limited sample sizes. Our findings show that the most robust approach to handling RTM is not to correct the data but to use the crude slope in conjunction with an assessment of the experiment's repeatability. Ultimately, we argue that any conclusion about a differential treatment effect is statistically unfounded without a clear understanding of the experiment's repeatability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04718v1</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e F. Fontanari, Mauro Santos</dc:creator>
    </item>
    <item>
      <title>A Latent Class Bayesian Model for Multivariate Longitudinal Outcomes with Excess Zeros</title>
      <link>https://arxiv.org/abs/2509.04804</link>
      <description>arXiv:2509.04804v1 Announce Type: new 
Abstract: Latent class models have been successfully used to handle complex datasets in different disciplines. For longitudinal outcomes, we often get a trajectory of the outcome for each individual, and on that basis, we cluster them for a powerful statistical inference. Latent class models have been used to handle multivariate longitudinal outcomes coming from biology, health sciences, and economics. In this paper, we propose a Bayesian latent class model for multivariate outcomes with excess zeros. We consider a Tobit model for zero-inflated continuous outcomes such as out-of-pocket medical expenses (OOPME), a two-part model for financial debt, and a ZIP model for counting outcomes with excess zeros. We develop a Bayesian mixture model and employ an adaptive Lasso-type shrinkage method for variable selection. We analyze data from the Health and Retirement Study conducted by the University of Michigan and consider modeling four important outcomes measuring the physical and financial health of the aged individuals. Our analysis detects several latent clusters for different outcomes. Practical usefulness of the proposed model is validated through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04804v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chitradipa Chakraborty, Kiranmoy Das</dc:creator>
    </item>
    <item>
      <title>An information metric for comparing and assessing informative interim decisions in sequential clinical trials</title>
      <link>https://arxiv.org/abs/2509.04904</link>
      <description>arXiv:2509.04904v2 Announce Type: new 
Abstract: Group sequential designs enable interim analyses and potential early stopping for efficacy or futility. While these adaptations improve trial efficiency and ethical considerations, they also introduce bias into the adapted analyses. We demonstrate how failing to account for informative interim decisions in the analysis can substantially affect posterior estimates of the treatment effect, often resulting in overly optimistic credible intervals aligned with the stopping decision. Drawing on information theory, we use the Kullback-Leibler divergence to quantify this distortion and highlight its use for post-hoc evaluation of informative interim decisions, with a focus on end-of-study inference. Unlike pointwise comparisons, this measure provides an integrated summary of this distortion on the whole parameter space. By comparing alternative decision boundaries and prior specifications, we illustrate how this measure can improve the understanding of trial results and inform the planning of future adaptive studies. We also introduce an expected version of this metric to support clinicians in choosing decision boundaries. This guidance complements traditional strategies based on type-I error rate control by offering insights into the distortion introduced to the treatment effect at each interim phase. The use of this pre-experimental measure is finally illustrated in a group sequential trial for evaluating a treatment for central nervous system disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04904v2</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G. Caruso, W. F. Rosenberger, P. Mozgunov, N. Flournoy</dc:creator>
    </item>
    <item>
      <title>Semi-supervised inference for treatment heterogeneity</title>
      <link>https://arxiv.org/abs/2509.05048</link>
      <description>arXiv:2509.05048v1 Announce Type: new 
Abstract: In causal inference, measuring treatment heterogeneity is crucial as it provides scientific insights into how treatments influence outcomes and guides personalized decision-making. In this work, we study semi-supervised settings where a labeled dataset is accompanied by a large unlabeled dataset, and develop semi-supervised estimators for two measures of treatment heterogeneity: the total treatment heterogeneity (TTH) and the explained treatment heterogeneity (ETH) of a simplified working model. We propose semi-supervised estimators for both quantities and demonstrate their improved robustness and efficiency compared with supervised methods. For ETH estimation, we show that direct semi-supervised approaches may result in efficiency loss relative to supervised counterparts. To address this, we introduce a re-weighting strategy that assigns data-dependent weights to labeled and unlabeled samples to optimize efficiency. The proposed approach guarantees an asymptotic variance no larger than that of the supervised method, ensuring its safe use. We evaluate the performance of the proposed estimators through simulation studies and a real-data application based on an AIDS clinical trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05048v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yilizhati Anniwaer, Yuqian Zhang</dc:creator>
    </item>
    <item>
      <title>A functional tensor model for dynamic multilayer networks with common invariant subspaces and the RKHS estimation</title>
      <link>https://arxiv.org/abs/2509.05221</link>
      <description>arXiv:2509.05221v1 Announce Type: new 
Abstract: Dynamic multilayer networks are frequently used to describe the structure and temporal evolution of multiple relationships among common entities, with applications in fields such as sociology, economics, and neuroscience. However, exploration of analytical methods for these complex data structures remains limited. We propose a functional tensor-based model for dynamic multilayer networks, with the key feature of capturing the shared structure among common vertices across all layers, while simultaneously accommodating smoothly varying temporal dynamics and layer-specific heterogeneity. The proposed model and its embeddings can be applied to various downstream network inference tasks, including dimensionality reduction, vertex community detection, analysis of network evolution periodicity, visualization of dynamic network evolution patterns, and evaluation of inter-layer similarity. We provide an estimation algorithm based on functional tensor Tucker decomposition and the reproducing kernel Hilbert space framework, with an effective initialization strategy to improve computational efficiency. The estimation procedure can be extended to address more generalized functional tensor problems, as well as to handle missing data or unaligned observations. We validate our method on simulated data and two real-world cases: the dynamic Citi Bike trip network and an international food trade dynamic multilayer network, with each layer corresponding to a different product.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05221v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runshi Tang, Runbing Zheng, Anru R. Zhang, Carey E. Priebe</dc:creator>
    </item>
    <item>
      <title>Beyond Linearity and Time-homogeneity: Relational Hyper Event Models with Time-Varying Non-Linear Effects</title>
      <link>https://arxiv.org/abs/2509.05289</link>
      <description>arXiv:2509.05289v2 Announce Type: new 
Abstract: Recent technological advances have made it easier to collect large and complex networks of time-stamped relational events connecting two or more entities. Relational hyper-event models (RHEMs) aim to explain the dynamics of these events by modeling the event rate as a function of statistics based on past history and external information.
  However, despite the complexity of the data, most current RHEM approaches still rely on a linearity assumption to model this relationship. In this work, we address this limitation by introducing a more flexible model that allows the effects of statistics to vary non-linearly and over time. While time-varying and non-linear effects have been used in relational event modeling, we take this further by modeling joint time-varying and non-linear effects using tensor product smooths.
  We validate our methodology on both synthetic and empirical data. In particular, we use RHEMs to study how patterns of scientific collaboration and impact evolve over time. Our approach provides deeper insights into the dynamic factors driving relational hyper-events, allowing us to evaluate potential non-monotonic patterns that cannot be identified using linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05289v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Boschi, J\"urgen Lerner, Ernst C. Wit</dc:creator>
    </item>
    <item>
      <title>Indifference-Zone Relaxation Procedures for Finding Feasible Systems</title>
      <link>https://arxiv.org/abs/2509.04514</link>
      <description>arXiv:2509.04514v1 Announce Type: cross 
Abstract: We consider the problem of finding feasible systems with respect to stochastic constraints when system performance is evaluated through simulation. Our objective is to solve this problem with high computational efficiency and statistical validity. Existing indifference-zone (IZ) procedures introduce a fixed tolerance level, which denotes how much deviation the decision-maker is willing to accept from the threshold in the constraint. These procedures are developed under the assumption that all systems' performance measures are exactly the tolerance level away from the threshold, leading to unnecessary simulations. In contrast, IZ-free procedures, which eliminate the tolerance level, perform well when systems' performance measures are far from the threshold. However, they may significantly underperform compared to IZ procedures when systems' performance measures are close to the threshold. To address these challenges, we propose the Indifference-Zone Relaxation (IZR) procedure, IZR introduces a set of relaxed tolerance levels and utilizes two subroutines for each level: one to identify systems that are clearly feasible and the other to exclude those that are clearly infeasible. We also develop the IZR procedure with estimation (IZE), which introduces two relaxed tolerance levels for each system and constraint: one matching the original tolerance level and the other based on an estimate of the system's performance measure. By employing different tolerance levels, these procedures facilitate early feasibility determination with statistical validity. We prove that IZR and IZE determine system feasibility with the desired probability and show through experiments that they significantly reduce the number of observations required compared to an existing procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04514v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuwei Zhou, Sigr\'un Andrad\'ottir, Seong-Hee Kim, Chuljin Park</dc:creator>
    </item>
    <item>
      <title>Precision Mental Health: Predicting Heterogeneous Treatment Effects for Depression through Data Integration</title>
      <link>https://arxiv.org/abs/2509.04604</link>
      <description>arXiv:2509.04604v1 Announce Type: cross 
Abstract: When treating depression, clinicians are interested in determining the optimal treatment for a given patient, which is challenging given the amount of treatments available. To advance individualized treatment allocation, integrating data across multiple randomized controlled trials (RCTs) can enhance our understanding of treatment effect heterogeneity by increasing available information. However, extending these inferences to individuals outside of the original RCTs remains crucial for clinical decision-making. We introduce a two-stage meta-analytic method that predicts conditional average treatment effects (CATEs) in target patient populations by leveraging the distribution of CATEs across RCTs. Our approach generates 95\% prediction intervals for CATEs in target settings using first-stage models that can incorporate parametric regression or non-parametric methods such as causal forests or Bayesian additive regression trees (BART). We validate our method through simulation studies and operationalize it to integrate multiple RCTs comparing depression treatments, duloxetine and vortioxetine, to generate prediction intervals for target patient profiles. Our analysis reveals no strong evidence of effect heterogeneity across trials, with the exception of potential age-related variability. Importantly, we show that CATE prediction intervals capture broader uncertainty than study-specific confidence intervals when warranted, reflecting both within-study and between-study variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04604v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carly L. Brantner, Trang Quynh Nguyen, Harsh Parikh, Congwen Zhao, Hwanhee Hong, Elizabeth A. Stuart</dc:creator>
    </item>
    <item>
      <title>Precision Dose-Finding Design for Phase I Oncology Trials by Integrating Pharmacology Data</title>
      <link>https://arxiv.org/abs/2509.05120</link>
      <description>arXiv:2509.05120v1 Announce Type: cross 
Abstract: Phase I oncology trials aim to identify a safe yet effective dose - often the maximum tolerated dose (MTD) - for subsequent studies. Conventional designs focus on population-level toxicity modeling, with recent attention on leveraging pharmacokinetic (PK) data to improve dose selection. We propose the Precision Dose-Finding (PDF) design, a novel Bayesian phase I framework that integrates individual patient PK profiles into the dose-finding process. By incorporating patient-specific PK parameters (such as volume of distribution and elimination rate), PDF models toxicity risk at the individual level, in contrast to traditional methods that ignore inter-patient variability. The trial is structured in two stages: an initial training stage to update model parameters using cohort-based dose escalation, and a subsequent test stage in which doses for new patients are chosen based on each patient's own PK-predicted toxicity probability. This two-stage approach enables truly personalized dose assignment while maintaining rigorous safety oversight. Extensive simulation studies demonstrate the feasibility of PDF and suggest that it provides improved safety and dosing precision relative to the continual reassessment method (CRM). The PDF design thus offers a refined dose-finding strategy that tailors the MTD to individual patients, aligning phase I trials with the ideals of precision medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05120v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyong Ju Lee, Yuan Ji</dc:creator>
    </item>
    <item>
      <title>Maximum Agreement Linear Predictors</title>
      <link>https://arxiv.org/abs/2304.04221</link>
      <description>arXiv:2304.04221v3 Announce Type: replace 
Abstract: This paper studies predictor functions motivated by maximizing a measure of agreement with the predictand. Specifically, it examines distributional properties and predictive performance of the estimated maximum agreement linear predictor (MALP), the linear predictor maximizing Lin's concordance correlation coefficient (CCC) between the predictor and the predictand. It is compared and contrasted, theoretically and through computer experiments, with the estimated least-squares linear predictor (LSLP), with respect to some performance measures. Finite-sample and asymptotic properties are obtained, and confidence intervals and prediction intervals are also presented. Predictors are illustrated using two real data sets: an eye data set and a body fat data set. Results indicate that the estimated MALP is a viable alternative to the estimated LSLP if one desires a predictor whose predicted values possesses higher agreement with the predictand values, as measured by the CCC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.04221v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taeho Kim, Pierre Chausse, Matteo Bottai, Gheorghe Doros, Mihai Giurcanu, George Luta, Edsel A. Pena</dc:creator>
    </item>
    <item>
      <title>Tail-robust factor modelling of vector and tensor time series in high dimensions</title>
      <link>https://arxiv.org/abs/2407.09390</link>
      <description>arXiv:2407.09390v4 Announce Type: replace 
Abstract: We study the problem of factor modelling vector- and tensor-valued time series in the presence of heavy tails in the data, which produce extreme observations with non-negligible probability. We propose to combine a two-step procedure for tensor decomposition with data truncation, which is easy to implement and does not require an iterative search for a numerical solution. Departing away from the light-tail assumptions often adopted in the time series factor modelling literature, we derive the consistency and asymptotic normality of the proposed estimators while assuming the existence of the $(2 + 2\epsilon)$-th moment only for some $\epsilon \in (0, 1)$. Our rates explicitly depend on $\eps$ characterising the effect of heavy tails, and on the chosen level of truncation. We also propose a consistent criterion for determining the number of factors. Simulation studies and applications to two macroeconomic datasets demonstrate the good performance of the proposed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09390v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Haeran Cho, Hyeyoung Maeng</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Post-Integrated Inference with Surrogate Control Outcomes</title>
      <link>https://arxiv.org/abs/2410.04996</link>
      <description>arXiv:2410.04996v3 Announce Type: replace 
Abstract: Data integration methods aim to extract low-dimensional embeddings from high-dimensional outcomes to remove unwanted variations, such as batch effects and unmeasured covariates, across heterogeneous datasets. However, multiple hypothesis testing after integration can be biased due to data-dependent processes. We introduce a robust post-integrated inference (PII) method that adjusts for latent heterogeneity using control outcomes. Leveraging causal interpretations, we derive nonparametric identifiability of the direct effects using negative control outcomes. By utilizing surrogate control outcomes as an extension of negative control outcomes, we develop semiparametric inference on projected direct effect estimands, accounting for hidden mediators, confounders, and moderators. These estimands remain statistically meaningful under model misspecifications and with error-prone embeddings. We provide bias quantifications and finite-sample linear expansions with uniform concentration bounds. The proposed doubly robust estimators are consistent and efficient under minimal assumptions and potential misspecification, facilitating data-adaptive estimation with machine learning algorithms. Our proposal is evaluated with random forests through simulations and analysis of single-cell CRISPR perturbed datasets with potential unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04996v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Kathryn Roeder, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of the Patient Weighted While-Alive Estimand</title>
      <link>https://arxiv.org/abs/2412.03246</link>
      <description>arXiv:2412.03246v2 Announce Type: replace 
Abstract: In clinical trials with recurrent events, such as repeated hospitalizations terminating with death, it is important to consider the patient events overall history for a thorough assessment of treatment effects. The occurrence of fewer events due to early deaths can lead to misinterpretation, emphasizing the importance of a while-alive strategy as suggested in Schmidli et al. (2023). In this study, we focus on the patient weighted while-alive estimand, represented as the expected number of events divided by the time alive within a target window, and develop efficient estimation for this estimand. Specifically, we derive the corresponding efficient influence function and develop a one-step estimator initially applied to the simpler irreversible illness-death model. For the broader context of recurrent events, due to the increased complexity, this one-step estimator is practically intractable due to likely misspecification of the needed conditional transition intensities that depend on a patient's unique history. Therefore, we suggest an alternative estimator that is expected to have high efficiency, focusing on the randomized treatment setting. Additionally, we apply our proposed estimator to two real-world case studies, demonstrating the practical applicability of this second estimator and benefits of this while-alive approach over currently available alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03246v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alessandra Ragni, Torben Martinussen, Thomas Scheike</dc:creator>
    </item>
    <item>
      <title>moonboot: An R Package Implementing m-out-of-n Bootstrap Methods</title>
      <link>https://arxiv.org/abs/2412.05032</link>
      <description>arXiv:2412.05032v3 Announce Type: replace 
Abstract: The m-out-of-n bootstrap is a possible workaround to compute confidence intervals for bootstrap inconsistent estimators, because it works under weaker conditions than the n-out-of-n bootstrap. It has the disadvantage, however, that it requires knowledge of an appropriate scaling factor tau(n) and that the coverage probability for finite n depends on the choice of m. This article presents an R package moonboot which implements the computation of m-out-of-n bootstrap confidence intervals and provides functions for estimating the parameters tau(n) and m. By means of Monte Carlo simulations, we evaluate the different methods and compare them for different estimators</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05032v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoph Dalitz, Felix L\"ogler</dc:creator>
    </item>
    <item>
      <title>Applying non-negative matrix factorization with covariates to multivariate time series data as a vector autoregression model</title>
      <link>https://arxiv.org/abs/2501.17446</link>
      <description>arXiv:2501.17446v3 Announce Type: replace 
Abstract: We propose a novel framework for analyzing multivariate time series (MTS) data by integrating non-negative matrix factorization (NMF) with vector autoregression (VAR). Termed NMF-VAR, this method models the coefficient matrix of NMF as a VAR process, enabling simultaneous extraction of latent components and temporal dependencies. Unlike standard VAR, which struggles with high dimensionality and lacks clarity, our method introduces a low-rank latent structure that reduces the number of parameters while retaining explanatory power. The proposed framework generalizes the standard VAR model to high-dimensional non-negative data, including the standard VAR as a special case. We formulate the estimation as a constrained optimization problem and present multiplicative update rules for NMF based on existing tri-factorization techniques. We evaluate the method on three real-world datasets: quarterly first-differenced macroeconomic indicators of Canada, monthly international airline passenger volumes, and daily COVID-19 infection counts across Japanese prefectures. The results demonstrate that NMF-VAR effectively captures meaningful patterns such as economic cycles, seasonal travel behavior, and regional epidemic trends. Moreover, the method yields a significant reduction in regression parameters, improving both scalability and model transparency. Overall, NMF-VAR provides an efficient and insightful tool for analyzing high-dimensional and large-scale time series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17446v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s42081-025-00314-0</arxiv:DOI>
      <arxiv:journal_reference>Applying non-negative matrix factorization with covariates to multivariate time series data as a vector autoregression model, Japanese Journal of Statistics and Data Science, 2025</arxiv:journal_reference>
      <dc:creator>Kenichi Satoh</dc:creator>
    </item>
    <item>
      <title>Adaptive Designs in Fast-Track Registration Processes</title>
      <link>https://arxiv.org/abs/2507.04092</link>
      <description>arXiv:2507.04092v2 Announce Type: replace 
Abstract: Fast-track procedures play an important role in the context of conditional registration of health products, such as conditional approval processes and listing processes for digital health applications. Fast-track procedures offer the potential for earlier patient access to innovative products. They involve two registration steps. The applicants can apply first for conditional registration. A successful conditional registration provides a limited funding or approval period and time to prepare the application for permanent registration, which is the second step of the registration process. For conditional registration, typically, products have to fulfil only a part of the requirements necessary for permanent registration. There is interest in valid and efficient study designs for fast-track procedures. This will be addressed in this paper. A motivating example of the paper is the German fast-track registration process of digital health applications (DiGA) for reimbursement by statutory health insurances. The main focus of the paper is the systematic investigation of the utility of adaptive designs in the context of fast-track registration processes. The paper also covers a careful discussion of the different requirements found in the guidelines and their consequences. We demonstrate that the use of adaptive designs in the context of fast-track processes like the DiGA registration process is, in most cases, much more efficient than the current standard of two separate studies. The results presented in this paper are based on numerical calculations supported by mathematical arguments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04092v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Liane Kluge, Werner Brannath</dc:creator>
    </item>
    <item>
      <title>Doubly robust outlier resistant inference on causal treatment effect</title>
      <link>https://arxiv.org/abs/2507.17439</link>
      <description>arXiv:2507.17439v2 Announce Type: replace 
Abstract: Outliers can severely distort causal effect estimation in observational studies, especially in small samples. We develop a doubly robust estimator of the ATE under a contaminated-data model that explicitly accommodates outliers. Robustness to outliers is delivered via a bounded-influence estimating equation for the outcome model and covariate balancing propensity scores (CBPS) for treatment assignment. To mitigate overfitting in high dimensions, we incorporate variable selection and unify all components within a penalized empirical likelihood framework. For further inference, we derive an optimal finite-sample confidence interval (CI) whose endpoints are invariant to outliers under the contaminated model. Across extensive simulations and two gene-expression applications (Golub; Khan pediatric tumor), the proposed ATE estimator and finite-sample CI outperform state-of-the-art competitors in bias, mean squared error, empirical coverage, and interval length over a wide range of contamination levels and sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17439v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Byeonghee Lee, Juhyun Park, Saebom Jeon, Joonsung Kang</dc:creator>
    </item>
    <item>
      <title>Double robust estimation of functional outcomes with data missing at random</title>
      <link>https://arxiv.org/abs/2411.17224</link>
      <description>arXiv:2411.17224v2 Announce Type: replace-cross 
Abstract: We present and study semi-parametric estimators for the mean of functional outcomes in situations where some of these outcomes are missing and covariate information is available on all units. Assuming that the missingness mechanism depends only on the covariates (missing at random assumption), we present two estimators for the functional mean parameter, using working models for the functional outcome given the covariates, and the probability of missingness given the covariates. We contribute by establishing that both these estimators have Gaussian processes as limiting distributions and explicitly give their covariance functions. One of the estimators is double robust in the sense that the limiting distribution holds whenever at least one of the nuisance models is correctly specified. These results allow us to present simultaneous confidence bands for the mean function with asymptotically guaranteed coverage. A Monte Carlo study shows the finite sample properties of the proposed functional estimators and their associated simultaneous inference. The use of the method is illustrated in an application where the mean of counterfactual outcomes is targeted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17224v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xijia Liu, Kreske Felix Ecker, Lina Schelin, Xavier de Luna</dc:creator>
    </item>
  </channel>
</rss>
