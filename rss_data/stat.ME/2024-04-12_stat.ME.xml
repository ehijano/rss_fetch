<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Apr 2024 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 12 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Parametric estimation of conditional Archimedean copula generators for censored data</title>
      <link>https://arxiv.org/abs/2404.07248</link>
      <description>arXiv:2404.07248v1 Announce Type: new 
Abstract: In this paper, we propose a novel approach for estimating Archimedean copula generators in a conditional setting, incorporating endogenous variables. Our method allows for the evaluation of the impact of the different levels of covariates on both the strength and shape of dependence by directly estimating the generator function rather than the copula itself. As such, we contribute to relaxing the simplifying assumption inherent in traditional copula modeling. We demonstrate the effectiveness of our methodology through applications in two diverse settings: a diabetic retinopathy study and a claims reserving analysis. In both cases, we show how considering the influence of covariates enables a more accurate capture of the underlying dependence structure in the data, thus enhancing the applicability of copula models, particularly in actuarial contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07248v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marie Michaelides, H\'el\`ene Cossette, Mathieu Pigeon</dc:creator>
    </item>
    <item>
      <title>Surrogate modeling for probability distribution estimation:uniform or adaptive design?</title>
      <link>https://arxiv.org/abs/2404.07323</link>
      <description>arXiv:2404.07323v1 Announce Type: new 
Abstract: The active learning (AL) technique, one of the state-of-the-art methods for constructing surrogate models, has shown high accuracy and efficiency in forward uncertainty quantification (UQ) analysis. This paper provides a comprehensive study on AL-based global surrogates for computing the full distribution function, i.e., the cumulative distribution function (CDF) and the complementary CDF (CCDF). To this end, we investigate the three essential components for building surrogates, i.e., types of surrogate models, enrichment methods for experimental designs, and stopping criteria. For each component, we choose several representative methods and study their desirable configurations. In addition, we devise a uniform design (i.e., space-filling design) as a baseline for measuring the improvement of using AL. Combining all the representative methods, a total of 1,920 UQ analyses are carried out to solve 16 benchmark examples. The performance of the selected strategies is evaluated based on accuracy and efficiency. In the context of full distribution estimation, this study concludes that (i) AL techniques cannot provide a systematic improvement compared with uniform designs, (ii) the recommended surrogate modeling methods depend on the features of the problems (especially the local nonlinearity), target accuracy, and computational budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07323v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maijia Su, Ziqi Wang, Oreste Salvatore Bursi, Marco Broccardo</dc:creator>
    </item>
    <item>
      <title>Mediated probabilities of causation</title>
      <link>https://arxiv.org/abs/2404.07397</link>
      <description>arXiv:2404.07397v1 Announce Type: new 
Abstract: We propose a set of causal estimands that we call ``the mediated probabilities of causation.'' These estimands quantify the probabilities that an observed negative outcome was induced via a mediating pathway versus a direct pathway in a stylized setting involving a binary exposure or intervention, a single binary mediator, and a binary outcome. We outline a set of conditions sufficient to identify these effects given observed data, and propose a doubly-robust projection based estimation strategy that allows for the use of flexible non-parametric and machine learning methods for estimation. We argue that these effects may be more relevant than the probability of causation, particularly in settings where we observe both some negative outcome and negative mediating event, and we wish to distinguish between settings where the outcome was induced via the exposure inducing the mediator versus the exposure inducing the outcome directly. We motivate our quantities of interest by discussing applications to legal and medical questions of causal attribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07397v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Rubinstein, Maria Cuellar, Daniel Malinsky</dc:creator>
    </item>
    <item>
      <title>Joint mixed-effects models for causal inference in clustered network-based observational studies</title>
      <link>https://arxiv.org/abs/2404.07411</link>
      <description>arXiv:2404.07411v1 Announce Type: new 
Abstract: Causal inference on populations embedded in social networks poses technical challenges, since the typical no interference assumption frequently does not hold. Existing methods developed in the context of network interference rely upon the assumption of no unmeasured confounding. However, when faced with multilevel network data, there may be a latent factor influencing both the exposure and the outcome at the cluster level. We propose a Bayesian inference approach that combines a joint mixed-effects model for the outcome and the exposure with direct standardization to identify and estimate causal effects in the presence of network interference and unmeasured cluster confounding. In simulations, we compare our proposed method with linear mixed and fixed effects models and show that unbiased estimation is achieved using the joint model. Having derived valid tools for estimation, we examine the effect of maternal college education on adolescent school performance using data from the National Longitudinal Study of Adolescent Health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07411v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vanessa McNealis, Erica E. M. Moodie, Nema Dean</dc:creator>
    </item>
    <item>
      <title>Bayesian Penalized Transformation Models: Structured Additive Location-Scale Regression for Arbitrary Conditional Distributions</title>
      <link>https://arxiv.org/abs/2404.07440</link>
      <description>arXiv:2404.07440v1 Announce Type: new 
Abstract: Penalized transformation models (PTMs) are a novel form of location-scale regression. In PTMs, the shape of the response's conditional distribution is estimated directly from the data, and structured additive predictors are placed on its location and scale. The core of the model is a monotonically increasing transformation function that relates the response distribution to a reference distribution. The transformation function is equipped with a smoothness prior that regularizes how much the estimated distribution diverges from the reference distribution. These models can be seen as a bridge between conditional transformation models and generalized additive models for location, scale and shape. Markov chain Monte Carlo inference for PTMs can be conducted with the No-U-Turn sampler and offers straightforward uncertainty quantification for the conditional distribution as well as for the covariate effects. A simulation study demonstrates the effectiveness of the approach. We apply the model to data from the Fourth Dutch Growth Study and the Framingham Heart Study. A full-featured implementation is available as a Python library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07440v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Brachem, Paul F. V. Wiemann, Thomas Kneib</dc:creator>
    </item>
    <item>
      <title>Safe subspace screening for the adaptive nuclear norm regularized trace regression</title>
      <link>https://arxiv.org/abs/2404.07459</link>
      <description>arXiv:2404.07459v1 Announce Type: new 
Abstract: Matrix form data sets arise in many areas, so there are lots of works about the matrix regression models. One special model of these models is the adaptive nuclear norm regularized trace regression, which has been proven have good statistical performances. In order to accelerate the computation of this model, we consider the technique called screening rule. According to matrix decomposition and optimal condition of the model, we develop a safe subspace screening rule that can be used to identify inactive subspace of the solution decomposition and reduce the dimension of the solution. To evaluate the efficiency of the safe subspace screening rule, we embed this result into the alternating direction method of multipliers algorithm under a sequence of the tuning parameters. Under this process, each solution under the tuning parameter provides a matrix decomposition space. Then, the safe subspace screening rule is applied to eliminate inactive subspace, reduce the solution dimension and accelerate the computation process. Some numerical experiments are implemented on simulation data sets and real data sets, which illustrate the efficiency of our screening rule.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07459v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pan Shang, Lingchen Kong</dc:creator>
    </item>
    <item>
      <title>Consistent Distribution Free Affine Invariant Tests for the Validity of Independent Component Models</title>
      <link>https://arxiv.org/abs/2404.07632</link>
      <description>arXiv:2404.07632v1 Announce Type: new 
Abstract: We propose a family of tests of the validity of the assumptions underlying independent component analysis methods. The tests are formulated as L2-type procedures based on characteristic functions and involve weights; a proper choice of these weights and the estimation method for the mixing matrix yields consistent and affine-invariant tests. Due to the complexity of the asymptotic null distribution of the resulting test statistics, implementation is based on permutational and resampling strategies. This leads to distribution-free procedures regardless of whether these procedures are performed on the estimated independent components themselves or the componentwise ranks of their components. A Monte Carlo study involving various estimation methods for the mixing matrix, various weights, and a competing test based on distance covariance is conducted under the null hypothesis as well as under alternatives. A real-data application demonstrates the practical utility and effectiveness of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07632v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Hallin, Simos G. Meintanis, Klaus Nordhausen</dc:creator>
    </item>
    <item>
      <title>WiNNbeta: Batch and drift correction method by white noise normalization for metabolomic studies</title>
      <link>https://arxiv.org/abs/2404.07906</link>
      <description>arXiv:2404.07906v1 Announce Type: new 
Abstract: We developed a method called batch and drift correction method by White Noise Normalization (WiNNbeta) to correct individual metabolites for batch effects and drifts. This method tests for white noise properties to identify metabolites in need of correction and corrects them by using fine-tuned splines. To test the method performance we applied WiNNbeta to LC-MS data from our metabolomic studies and computed CVs before and after WiNNbeta correction in quality control samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07906v1</guid>
      <category>stat.ME</category>
      <category>q-bio.BM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olga Demler, Franco Giulianini, Yanyan Liu, Malte Londschien, Anja Sj\"ostr\"om, Tanmay Tanna, Heike Luttmann-Gibson, Antoine Jeanrenaud</dc:creator>
    </item>
    <item>
      <title>A Bayesian Estimator of Sample Size</title>
      <link>https://arxiv.org/abs/2404.07923</link>
      <description>arXiv:2404.07923v1 Announce Type: new 
Abstract: We consider a Bayesian estimator of sample size (BESS) and an application to oncology dose optimization clinical trials. BESS is built upon balancing a trio of Sample size, Evidence from observed data, and Confidence in posterior inference. It uses a simple logic of "given the evidence from data, a specific sample size can achieve a degree of confidence in the posterior inference." The key distinction between BESS and standard sample size estimation (SSE) is that SSE, typically based on Frequentist inference, specifies the true parameters values in its calculation while BESS assumes a possible outcome from the observed data. As a result, the calibration of the sample size is not based on Type I or Type II error rates, but on posterior probabilities. We argue that BESS leads to a more interpretable statement for investigators, and can easily accommodates prior information as well as sample size re-estimation. We explore its performance in comparison to SSE and demonstrate its usage through a case study of oncology optimization trial. BESS can be applied to general hypothesis tests. R functions are available at https://ccte.uchicago.edu/bess.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07923v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dehua Bi, Yuan Ji</dc:creator>
    </item>
    <item>
      <title>Application of the chemical master equation and its analytical solution to the illness-death model</title>
      <link>https://arxiv.org/abs/2404.07238</link>
      <description>arXiv:2404.07238v1 Announce Type: cross 
Abstract: The aim of this article is relating the chemical master equation (CME) to the illness-death model for chronic diseases. We show that a recently developed differential equation for the prevalence directly follows from the CME. As an application, we use the theory of the CME in a simulation study about diabetes in Germany from a previous publication. We find a good agreement between the theory and the simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07238v1</guid>
      <category>physics.bio-ph</category>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ralph Brinks</dc:creator>
    </item>
    <item>
      <title>State-Space Modeling of Shape-constrained Functional Time Series</title>
      <link>https://arxiv.org/abs/2404.07586</link>
      <description>arXiv:2404.07586v1 Announce Type: cross 
Abstract: Functional time series data frequently appears in economic applications, where the functions of interest are subject to some shape constraints, including monotonicity and convexity, as typical of the estimation of the Lorenz curve. This paper proposes a state-space model for time-varying functions to extract trends and serial dependence from functional time series while imposing the shape constraints on the estimated functions. The function of interest is modeled by a convex combination of selected basis functions to satisfy the shape constraints, where the time-varying convex weights on simplex follow the dynamic multi-logit models. For the complicated likelihood of this model, a novel data augmentation technique is devised to enable posterior computation by an efficient Markov chain Monte Carlo method. The proposed method is applied to the estimation of time-varying Lorenz curves, and its utility is illustrated through numerical experiments and analysis of panel data of household incomes in Japan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07586v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daichi Hiraki, Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Diffusion posterior sampling for simulation-based inference in tall data settings</title>
      <link>https://arxiv.org/abs/2404.07593</link>
      <description>arXiv:2404.07593v1 Announce Type: cross 
Abstract: Determining which parameters of a non-linear model could best describe a set of experimental data is a fundamental problem in science and it has gained much traction lately with the rise of complex large-scale simulators (a.k.a. black-box simulators). The likelihood of such models is typically intractable, which is why classical MCMC methods can not be used. Simulation-based inference (SBI) stands out in this context by only requiring a dataset of simulations to train deep generative models capable of approximating the posterior distribution that relates input parameters to a given observation. In this work, we consider a tall data extension in which multiple observations are available and one wishes to leverage their shared information to better infer the parameters of the model. The method we propose is built upon recent developments from the flourishing score-based diffusion literature and allows us to estimate the tall data posterior distribution simply using information from the score network trained on individual observations. We compare our method to recently proposed competing approaches on various numerical experiments and demonstrate its superiority in terms of numerical stability and computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07593v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Linhart, Gabriel Victorino Cardoso, Alexandre Gramfort, Sylvain Le Corff, Pedro L. C. Rodrigues</dc:creator>
    </item>
    <item>
      <title>Robust performance metrics for imbalanced classification problems</title>
      <link>https://arxiv.org/abs/2404.07661</link>
      <description>arXiv:2404.07661v1 Announce Type: cross 
Abstract: We show that established performance metrics in binary classification, such as the F-score, the Jaccard similarity coefficient or Matthews' correlation coefficient (MCC), are not robust to class imbalance in the sense that if the proportion of the minority class tends to $0$, the true positive rate (TPR) of the Bayes classifier under these metrics tends to $0$ as well. Thus, in imbalanced classification problems, these metrics favour classifiers which ignore the minority class. To alleviate this issue we introduce robust modifications of the F-score and the MCC for which, even in strongly imbalanced settings, the TPR is bounded away from $0$. We numerically illustrate the behaviour of the various performance metrics in simulations as well as on a credit default data set. We also discuss connections to the ROC and precision-recall curves and give recommendations on how to combine their usage with performance metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07661v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hajo Holzmann, Bernhard Klar</dc:creator>
    </item>
    <item>
      <title>Debiased Inverse Propensity Score Weighting for Estimation of Average Treatment Effects with High-Dimensional Confounders</title>
      <link>https://arxiv.org/abs/2011.08661</link>
      <description>arXiv:2011.08661v3 Announce Type: replace 
Abstract: We consider estimation of average treatment effects given observational data with high-dimensional pretreatment variables. Existing methods for this problem typically assume some form of sparsity for the regression functions. In this work, we introduce a debiased inverse propensity score weighting (DIPW) scheme for average treatment effect estimation that delivers $\sqrt{n}$-consistent estimates when the propensity score follows a sparse logistic regression model; the outcome regression functions are permitted to be arbitrarily complex. We further demonstrate how confidence intervals centred on our estimates may be constructed. Our theoretical results quantify the price to pay for permitting the regression functions to be unestimable, which shows up as an inflation of the variance of the estimator compared to the semiparametric efficient variance by a constant factor, under mild conditions. We also show that when outcome regressions can be estimated faster than a slow $1/\sqrt{ \log n}$ rate, our estimator achieves semiparametric efficiency. As our results accommodate arbitrary outcome regression functions, averages of transformed responses under each treatment may also be estimated at the $\sqrt{n}$ rate. Thus, for example, the variances of the potential outcomes may be estimated. We discuss extensions to estimating linear projections of the heterogeneous treatment effect function and explain how propensity score models with more general link functions may be handled within our framework. An R package \texttt{dipw} implementing our methodology is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.08661v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Wang, Rajen D. Shah</dc:creator>
    </item>
    <item>
      <title>Selection and Aggregation of Conformal Prediction Sets</title>
      <link>https://arxiv.org/abs/2104.13871</link>
      <description>arXiv:2104.13871v3 Announce Type: replace 
Abstract: Conformal prediction is a generic methodology for finite-sample valid distribution-free prediction. This technique has garnered a lot of attention in the literature partly because it can be applied with any machine learning algorithm that provides point predictions to yield valid prediction regions. Of course, the efficiency (width/volume) of the resulting prediction region depends on the performance of the machine learning algorithm. In the context of point prediction, several techniques (such as cross-validation) exist to select one of many machine learning algorithms for better performance. In contrast, such selection techniques are seldom discussed in the context of set prediction (or prediction regions). In this paper, we consider the problem of obtaining the smallest conformal prediction region given a family of machine learning algorithms. We provide two general-purpose selection algorithms and consider coverage as well as width properties of the final prediction region. The first selection method yields the smallest width prediction region among the family of conformal prediction regions for all sample sizes but only has an approximate coverage guarantee. The second selection method has a finite sample coverage guarantee but only attains close to the smallest width. The approximate optimal width property of the second method is quantified via an oracle inequality. As an illustration, we consider the use of aggregation of non-parametric regression estimators in the split conformal method with the absolute residual conformal score.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.13871v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yachong Yang, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Functional Regression Models with Functional Response: New Approaches and a Comparative Study</title>
      <link>https://arxiv.org/abs/2207.04773</link>
      <description>arXiv:2207.04773v4 Announce Type: replace 
Abstract: This paper proposes a new nonlinear approach for additive functional regression with functional response based on kernel methods along with some slight reformulation and implementation of the linear regression and the spectral additive model. The latter methods have in common that the covariates and the response are represented in a basis and so, can only be applied when the response and the covariates belong to a Hilbert space, while the proposed method only uses the distances among data and thus can be applied to those situations where any of the covariates or the response is not Hilbertian typically normed or even metric spaces with a real vector structure. A comparison of these methods with other procedures readily available in R is performed in a simulation study and in real datasets showing the results the advantages of the nonlinear proposals and the small loss of efficiency when the simulation scenario is truly linear. The comparison is done in the Hilbertian case as it is the only scenario where all the procedures can be compared. Finally, the supplementary material provides a visualization tool for checking the linearity of the relationship between a single covariate and the response and a link to a GitHub repository where the code and data are available.} %and an example considering that the response is not Hilbertian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.04773v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Manuel Febrero-Bande, Manuel Oviedo-de la Fuente, Mohammad Darbalaei, Morteza Amini</dc:creator>
    </item>
    <item>
      <title>Assessing quality of selection procedures: Lower bound of false positive rate as a function of inter-rater reliability</title>
      <link>https://arxiv.org/abs/2207.09101</link>
      <description>arXiv:2207.09101v3 Announce Type: replace 
Abstract: Inter-rater reliability (IRR) is one of the commonly used tools for assessing the quality of ratings from multiple raters. However, applicant selection procedures based on ratings from multiple raters usually result in a binary outcome; the applicant is either selected or not. This final outcome is not considered in IRR, which instead focuses on the ratings of the individual subjects or objects. We outline the connection between the ratings' measurement model (used for IRR) and a binary classification framework. We develop a simple way of approximating the probability of correctly selecting the best applicants which allows us to compute error probabilities of the selection procedure (i.e., false positive and false negative rate) or their lower bounds. We draw connections between the inter-rater reliability and the binary classification metrics, showing that binary classification metrics depend solely on the IRR coefficient and proportion of selected applicants. We assess the performance of the approximation in a simulation study and apply it in an example comparing the reliability of multiple grant peer review selection procedures. We also discuss possible other uses of the explored connections in other contexts, such as educational testing, psychological assessment, and health-related measurement and implement the computations in IRR2FPR R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.09101v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franti\v{s}ek Barto\v{s}, Patr\'icia Martinkov\'a</dc:creator>
    </item>
    <item>
      <title>Differentially Private Confidence Intervals for Proportions under Stratified Random Sampling</title>
      <link>https://arxiv.org/abs/2301.08324</link>
      <description>arXiv:2301.08324v2 Announce Type: replace 
Abstract: Confidence intervals are a fundamental tool for quantifying the uncertainty of parameters of interest. With the increase of data privacy awareness, developing a private version of confidence intervals has gained growing attention from both statisticians and computer scientists. Differential privacy is a state-of-the-art framework for analyzing privacy loss when releasing statistics computed from sensitive data. Recent work has been done around differentially private confidence intervals, yet to the best of our knowledge, rigorous methodologies on differentially private confidence intervals in the context of survey sampling have not been studied. In this paper, we propose three differentially private algorithms for constructing confidence intervals for proportions under stratified random sampling. We articulate two variants of differential privacy that make sense for data from stratified sampling designs, analyzing each of our algorithms within one of these two variants. We establish analytical privacy guarantees and asymptotic properties of the estimators. In addition, we conduct simulation studies to evaluate the proposed private confidence intervals, and two applications to the 1940 Census data are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08324v2</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/24-EJS2234</arxiv:DOI>
      <arxiv:journal_reference>Electronic Journal of Statistics, Electron. J. Statist. 18(1), 1455-1494, (2024)</arxiv:journal_reference>
      <dc:creator>Shurong Lin, Mark Bun, Marco Gaboardi, Eric D. Kolaczyk, Adam Smith</dc:creator>
    </item>
    <item>
      <title>Efficiently transporting average treatment effects using a sufficient subset of effect modifiers</title>
      <link>https://arxiv.org/abs/2304.00117</link>
      <description>arXiv:2304.00117v2 Announce Type: replace 
Abstract: We develop flexible and nonparametric estimators of the average treatment effect (ATE) transported to a new population that offer potential efficiency gains by incorporating only a sufficient subset of effect modifiers that are differentially distributed between the source and target populations into the transport step. We develop both a one-step estimator when this sufficient subset of effect modifiers is known and a collaborative one-step estimator when it is unknown. We discuss when we would expect our estimators to be more efficient than those that assume all covariates may be relevant effect modifiers and the exceptions when we would expect worse efficiency. We use simulation to compare finite sample performance across our proposed estimators and existing estimators of the transported ATE, including in the presence of practical violations of the positivity assumption. Lastly, we apply our proposed estimators to a large-scale housing trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00117v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kara E. Rudolph, Nicholas T. Williams, Elizabeth A. Stuart, Ivan Diaz</dc:creator>
    </item>
    <item>
      <title>Debiasing Welch's Method for Spectral Density Estimation</title>
      <link>https://arxiv.org/abs/2312.13643</link>
      <description>arXiv:2312.13643v2 Announce Type: replace 
Abstract: Welch's method provides an estimator of the power spectral density that is statistically consistent. This is achieved by averaging over periodograms calculated from overlapping segments of a time series. For a finite length time series, while the variance of the estimator decreases as the number of segments increase, the magnitude of the estimator's bias increases: a bias-variance trade-off ensues when setting the segment number. We address this issue by providing a novel method for debiasing Welch's method which maintains the computational complexity and asymptotic consistency, and leads to improved finite-sample performance. Theoretical results are given for fourth-order stationary processes with finite fourth-order moments and absolutely convergent fourth-order cumulant function. The significant bias reduction is demonstrated with numerical simulation and an application to real-world data. Our estimator also permits irregular spacing over frequency and we demonstrate how this may be employed for signal compression and further variance reduction. Code accompanying this work is available in R and python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13643v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lachlan C. Astfalck, Adam M. Sykulski, Edward J. Cripps</dc:creator>
    </item>
    <item>
      <title>Extract Mechanisms from Heterogeneous Effects: Identification Strategy for Mediation Analysis</title>
      <link>https://arxiv.org/abs/2403.04131</link>
      <description>arXiv:2403.04131v2 Announce Type: replace 
Abstract: Understanding causal mechanisms is essential for explaining and generalizing empirical phenomena. Causal mediation analysis offers statistical techniques to quantify mediation effects. However, existing methods typically require strong identification assumptions or sophisticated research designs. We develop a new identification strategy that simplifies these assumptions, enabling the simultaneous estimation of causal and mediation effects. The strategy is based on a novel decomposition of total treatment effects, which transforms the challenging mediation problem into a simple linear regression problem. The new method establishes a new link between causal mediation and causal moderation. We discuss several research designs and estimators to increase the usability of our identification strategy for a variety of empirical studies. We demonstrate the application of our method by estimating the causal mediation effect in experiments concerning common pool resource governance and voting information. Additionally, we have created statistical software to facilitate the implementation of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04131v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu</dc:creator>
    </item>
    <item>
      <title>Lorenz map, inequality ordering and curves based on multidimensional rearrangements</title>
      <link>https://arxiv.org/abs/2203.09000</link>
      <description>arXiv:2203.09000v3 Announce Type: replace-cross 
Abstract: We propose a multivariate extension of the Lorenz curve based on multivariate rearrangements of optimal transport theory. We define a vector Lorenz map as the integral of the vector quantile map associated with a multivariate resource allocation. Each component of the Lorenz map is the cumulative share of each resource, as in the traditional univariate case. The pointwise ordering of such Lorenz maps defines a new multivariate majorization order, which is equivalent to preference by any social planner with inequality averse multivariate rank dependent social evaluation functional. We define a family of multi-attribute Gini index and complete ordering based on the Lorenz map. We propose the level sets of an Inverse Lorenz Function as a practical tool to visualize and compare inequality in two dimensions, and apply it to income-wealth inequality in the United States between 1989 and 2022.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.09000v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanqin Fan, Marc Henry, Brendan Pass, Jorge A. Rivero</dc:creator>
    </item>
  </channel>
</rss>
