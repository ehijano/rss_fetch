<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 May 2025 04:01:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Improving Travel Time Reliability with Variable Speed Limits</title>
      <link>https://arxiv.org/abs/2505.08059</link>
      <description>arXiv:2505.08059v1 Announce Type: new 
Abstract: This paper analyzes the use of variable speed limits to optimize travel time reliability for commuters. The investigation focuses on a traffic corridor with a bottleneck subject to the capacity drop phenomenon. The optimization criterion is a linear combination of the expected value and standard deviation of average travel time, with traffic flow dynamics following the kinematic wave model (Lighthill, 1955; Richards, 1956). We develop two complementary models to explain unreliable travel times at different temporal levels: In the first model, daily peak traffic demand is conceptualized as a stochastic variable, and the resulting model is solved through a three-stage optimization algorithm. The second model is based on deterministic demand, instead modeling bottleneck capacity as a stochastic process using a stochastic differential equation (SDE). The practical applicability of both approaches is demonstrated through numerical examples with empirically calibrated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08059v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Hammerl, Ravi Seshadri, Thomas Kj{\ae}r Rasmussen, Otto Anker Nielsen</dc:creator>
    </item>
    <item>
      <title>A Dantzig-Wolfe Decomposition Method for Quasi-Variational Inequalities</title>
      <link>https://arxiv.org/abs/2505.08108</link>
      <description>arXiv:2505.08108v1 Announce Type: new 
Abstract: We propose an algorithm to solve quasi-variational inequality problems, based on the Dantzig-Wolfe decomposition paradigm. Our approach solves in the subproblems variational inequalities, which is a simpler problem, while restricting quasi-variational inequalities in the master subproblems, making them generally (much) smaller in size when the original problem is large-scale. We prove global convergence of our algorithm, assuming that the mapping of the quasi-variational inequality is either single-valued and continuous or it is set-valued maximally monotone. Quasi-variational inequalities serve as a framework for several equilibrium problems, and we apply our algorithm to an important example in the field of economics, namely the Walrasian equilibrium problem formulated as a generalized Nash equilibrium problem. Our numerical assessment demonstrates good performance and usefullness of the approach for the large-scale cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08108v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manoel Jardim, Claudia Sagastiz\'abal, Mikhail Solodov</dc:creator>
    </item>
    <item>
      <title>A graph-based approach to customer segmentation using the RFM model</title>
      <link>https://arxiv.org/abs/2505.08136</link>
      <description>arXiv:2505.08136v1 Announce Type: new 
Abstract: The present article proposes a graph-based approach to customer segmentation, combining the RFM analysis with the classical optimization max-$k$-cut problem. We consider each customer as a vertex of a weighted graph, and the edge weights are given by the distances between the vectors corresponding to the $(R,F,M)$-scores of the customers. We design a procedure to build a reduced graph with fewer vertices and edges, and the customer segmentation is obtained by solving the max-$k$-cut for this reduced graph. We prove that the optimal objective function values of the original and the reduced problems are equal. Additionally, we show that an optimal solution to the original problem can be easily obtained from an optimal solution to the reduced problem, which provides an advantage in dealing with computational complexity in large instances. Applying our methodology to a real customer dataset allowed us to identify distinct behaviors between groups and analyze their meaning and value from a business perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08136v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andr\'e Luiz Corr\^ea Vianna Filho, Leonardo de Lima, Mariana Kleina</dc:creator>
    </item>
    <item>
      <title>Utilization of noise for the control of a class of non-linear systems</title>
      <link>https://arxiv.org/abs/2505.08257</link>
      <description>arXiv:2505.08257v1 Announce Type: new 
Abstract: Utilization of noise for the control of a class of non-linear systems is presented. The application of state-multiplicative noise as a mean of control is far more limited then the use of standard determinis?tic gains. Nevertheless, so called Stochastic Anti Resonance (SAR) with state-multiplicative noise based control, do arise in a variety of situations such as in engineering applications, physics modelling, bi?ology, and models of visuo-motor tasks. Linear Matrix Inequalities based conditions from recent publications are reviewed, that character?ize stochastic stability of such nonlinear systems applying SAR. While those results dealt with systems that are, apriori, modelled using sec?tor bounded nonlinearities, we demonstrate that more general systems that can be approximated as such, can be also controlled using SAR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08257v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrian-Mihail Stoica, Isaac Yaesh</dc:creator>
    </item>
    <item>
      <title>Stationary Mean-Field Games of Singular Control under Knightian Uncertainty</title>
      <link>https://arxiv.org/abs/2505.08317</link>
      <description>arXiv:2505.08317v1 Announce Type: new 
Abstract: In this work, we study a class of stationary mean-field games of singular stochastic control under model uncertainty. The representative agent adjusts the dynamics of an It\^o diffusion via one-sided singular stochastic control, aiming to maximize a long-term average expected profit criterion. The mean-field interaction is of scalar type through the stationary distribution of the population. Due to the presence of uncertainty, the problem involves the study of a stochastic (zero-sum) game, where the decision maker chooses the "best" singular control policy, while the adversarial player selects the "worst" probability measure. Using a constructive approach, we prove existence and uniqueness of a stationary mean-field equilibrium. Finally, we present an example of mean-field optimal extraction of natural resources under uncertainty and we analyze the impact of uncertainty on the mean-field equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08317v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giorgio Ferrari, Ioannis Tzouanas</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust LQG with Kullback-Leibler Ambiguity Sets</title>
      <link>https://arxiv.org/abs/2505.08370</link>
      <description>arXiv:2505.08370v1 Announce Type: new 
Abstract: The Linear Quadratic Gaussian (LQG) controller is known to be inherently fragile to model misspecifications often occurring in real-world situations. We consider discretetime partially observable stochastic linear systems and provide a robustification of the standard LQG against distributional uncertainties on the process and measurement noise. Our distributionally robust formulation specifies the admissible perturbations by defining a relative entropy based ambiguity set individually for each time step along a finite-horizon trajectory, and minimizes the worst-case cost across all admissible distributions. Notably, we prove that the optimal control policy is still linear, as in standard LQG, and we derive a computational scheme grounded on iterative best response that provably converges to the set of saddle points. Finally, we consider the case of endogenous uncertainty captured via decision-dependent ambiguity sets and we propose an approximation scheme based on dynamic programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08370v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marta Fochesato, Lucia Falconi, Mattia Zorzi, Augusto Ferrante, John Lygeros</dc:creator>
    </item>
    <item>
      <title>A three-term Polak-Ribi\`{e}re-Polyak conjugate gradient method for vector optimization</title>
      <link>https://arxiv.org/abs/2505.08408</link>
      <description>arXiv:2505.08408v1 Announce Type: new 
Abstract: A three-term Polak-Ribi\`{e}re-Polyak conjugate gradient method is first proposed in this paper for solving vector optimization problems. This method can autonomously generate descent directions independent of line search procedures, while retaining the conjugate property. The global convergence of the proposed scheme is established by the generalized Wolfe line search procedure without self-adjusting strategies, regular restarts and convexity assumptions. Finally, numerical experiments illustrating the practical performance of this method are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08408v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guangxuan Lin, Shouqiang Du</dc:creator>
    </item>
    <item>
      <title>SPP-SBL: Space-Power Prior Sparse Bayesian Learning for Block Sparse Recovery</title>
      <link>https://arxiv.org/abs/2505.08518</link>
      <description>arXiv:2505.08518v1 Announce Type: new 
Abstract: The recovery of block-sparse signals with unknown structural patterns remains a fundamental challenge in structured sparse signal reconstruction. By proposing a variance transformation framework, this paper unifies existing pattern-based block sparse Bayesian learning methods, and introduces a novel space power prior based on undirected graph models to adaptively capture the unknown patterns of block-sparse signals. By combining the EM algorithm with high-order equation root-solving, we develop a new structured sparse Bayesian learning method, SPP-SBL, which effectively addresses the open problem of space coupling parameter estimation in pattern-based methods. We further demonstrate that learning the relative values of space coupling parameters is key to capturing unknown block-sparse patterns and improving recovery accuracy. Experiments validate that SPP-SBL successfully recovers various challenging structured sparse signals (e.g., chain-structured signals and multi-pattern sparse signals) and real-world multi-modal structured sparse signals (images, audio), showing significant advantages in recovery accuracy across multiple metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08518v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanhao Zhang, Zhihan Zhu, Yong Xia</dc:creator>
    </item>
    <item>
      <title>Synthesis of safety certificates for discrete-time uncertain systems via convex optimization</title>
      <link>https://arxiv.org/abs/2505.08559</link>
      <description>arXiv:2505.08559v1 Announce Type: new 
Abstract: We study the problem of co-designing control barrier functions and linear state feedback controllers for discrete-time linear systems affected by additive disturbances. For disturbances of bounded magnitude, we provide a semi-definite program whose feasibility implies the existence of a control law and a certificate ensuring safety in the infinite horizon with respect to the worst-case disturbance realization in the uncertainty set. For disturbances with unbounded support, we rely on martingale theory to derive a second semi-definite program whose feasibility provides probabilistic safety guarantees holding joint-in-time over a finite time horizon. We examine several extensions, including (i) encoding of different types of input constraints, (ii) robustification against distributional ambiguity around the true distribution, (iii) design of safety filters, and (iv) extension to general safety specifications such as obstacle avoidance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08559v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marta Fochesato, Han Wang, Antonis Papachristodoulou, Paul Goulart</dc:creator>
    </item>
    <item>
      <title>Communication-Efficient Distributed Online Nonconvex Optimization with Time-Varying Constraints</title>
      <link>https://arxiv.org/abs/2505.08592</link>
      <description>arXiv:2505.08592v1 Announce Type: new 
Abstract: This paper considers distributed online nonconvex optimization with time-varying inequality constraints over a network of agents, where the nonconvex local loss and convex local constraint functions can vary arbitrarily across iterations, and the information of them is privately revealed to each agent at each iteration. For a uniformly jointly strongly connected time-varying directed graph, we propose two distributed bandit online primal--dual algorithm with compressed communication to efficiently utilize communication resources in the one-point and two-point bandit feedback settings, respectively. In nonconvex optimization, finding a globally optimal decision is often NP-hard. As a result, the standard regret metric used in online convex optimization becomes inapplicable. To measure the performance of the proposed algorithms, we use a network regret metric grounded in the first-order optimality condition associated with the variational inequality. We show that the compressed algorithms establish sublinear network regret and cumulative constraint violation bounds. Finally, a simulation example is presented to validate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08592v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kunpeng Zhang, Lei Xu, Xinlei Yi, Guanghui Wen, Ming Cao, Karl H. Johansson, Tianyou Chai, Tao Yang</dc:creator>
    </item>
    <item>
      <title>Iteratively reweighted kernel machines efficiently learn sparse functions</title>
      <link>https://arxiv.org/abs/2505.08277</link>
      <description>arXiv:2505.08277v1 Announce Type: cross 
Abstract: The impressive practical performance of neural networks is often attributed to their ability to learn low-dimensional data representations and hierarchical structure directly from data. In this work, we argue that these two phenomena are not unique to neural networks, and can be elicited from classical kernel methods. Namely, we show that the derivative of the kernel predictor can detect the influential coordinates with low sample complexity. Moreover, by iteratively using the derivatives to reweight the data and retrain kernel machines, one is able to efficiently learn hierarchical polynomials with finite leap complexity. Numerical experiments illustrate the developed theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08277v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Libin Zhu, Damek Davis, Dmitriy Drusvyatskiy, Maryam Fazel</dc:creator>
    </item>
    <item>
      <title>A spherical amplitude-phase formulation for 3-D adaptive line-of-sight (ALOS) guidance with USGES stability guarantees</title>
      <link>https://arxiv.org/abs/2505.08344</link>
      <description>arXiv:2505.08344v1 Announce Type: cross 
Abstract: A recently proposed 3-D adaptive line-of-sight (ALOS) path-following algorithm addressed coupled motion dynamics of marine craft, aircraft, and uncrewed vehicles under environmental disturbances such as wind, waves, and ocean currents. Stability analysis established uniform semiglobal exponential stability (USGES) of the cross- and vertical-track errors using a body-velocity-based amplitude-phase representation of the North-East-Down (NED) kinematic differential equations. In this brief paper, we revisit the ALOS framework and introduce a novel spherical amplitude-phase representation. This formulation yields a more geometrically intuitive and physically observable description of the guidance errors and enables a significantly simplified stability proof. Unlike the previous model, which relied on a vertical crab angle derived from body-frame velocities, the new representation uses an alternative vertical crab angle and retains the USGES property. It also removes restrictive assumptions such as constant altitude/depth or zero horizontal crab angle, and remains valid for general 3-D maneuvers with nonzero roll, pitch, and flight-path angles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08344v1</guid>
      <category>eess.SY</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erlend M. Coates, Thor I. Fossen</dc:creator>
    </item>
    <item>
      <title>Input-to-state type Stability for Simplified Fluid-Particle Interaction System</title>
      <link>https://arxiv.org/abs/2505.08393</link>
      <description>arXiv:2505.08393v1 Announce Type: cross 
Abstract: In this paper, we study the well-posedness and the input-to-state type stability of a one-dimensional fluid-particle interaction system. A distinctive feature, not yet considered in the ISS literature, is that our system involves a free boundary. More precisely, the fluid is described by the viscous Burgers equation, and the motion of the particle obeys Newton second law. The point mass is subject to both a feedback control and an open-loop control. We first establish the well-posedness of the system for any open-loop input in the L2(0, infinity) space. Assuming the input also belongs to the L1(0,infinity) space, we prove that the particle's position remains uniformly bounded and that the system is input-to-state type stable. The proof is based on the construction of a Lyapunov functional derived from a special test function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08393v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zhuo Xu</dc:creator>
    </item>
    <item>
      <title>Evaluating the Sharpness and Limitations of Bounds on the Frobenius Number</title>
      <link>https://arxiv.org/abs/2505.08560</link>
      <description>arXiv:2505.08560v1 Announce Type: cross 
Abstract: In this paper we study the (classical) Frobenius problem, namely the problem of finding the largest integer that cannot be represented as a nonnegative integral combination of given relatively prime (strictly) positive integers (known as the Frobenius number). We firstly compare several upper bounds on the Frobenius number, assessing their relative tightness through both theoretical arguments and Monte Carlo simulations. We then explore whether a general upper bound with a worst-case exponent strictly less than quadratic can exist, and formally demonstrate that such an improvement is impossible. These findings offer new insights into the structural properties of established bounds and underscore inherent constraints for future refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08560v1</guid>
      <category>math.NT</category>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aled Williams</dc:creator>
    </item>
    <item>
      <title>Isoperimetric inequality with zero magnetic field in doubly connected domains</title>
      <link>https://arxiv.org/abs/2505.08595</link>
      <description>arXiv:2505.08595v1 Announce Type: cross 
Abstract: We investigate how the lowest eigenvalue of a magnetic Laplacian depends on the geometry of a planar domain with a disk shaped hole, where the magnetic field is generated by a singular flux. Under Dirichlet boundary conditions on the inner boundary and Neumann boundary conditions on the outer boundary, we show that this eigenvalue is maximized when the domain is an annulus, for a fixed area and magnetic flux. As consequences, we establish geometric inequalities for eigenvalues in settings with both singular and localized magnetic fields. We also propose a conjecture for a general optimality result and establish its validity for large magnetic fluxes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08595v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <category>math.SP</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mrityunjoy Ghosh, Ayman Kachmar</dc:creator>
    </item>
    <item>
      <title>Uniform-in-time propagation of chaos for Consensus-Based Optimization</title>
      <link>https://arxiv.org/abs/2505.08669</link>
      <description>arXiv:2505.08669v1 Announce Type: cross 
Abstract: We study the derivative-free global optimization algorithm Consensus-Based Optimization (CBO), establishing uniform-in-time propagation of chaos as well as an almost uniform-in-time stability result for the microscopic particle system. The proof of these results is based on a novel stability estimate for the weighted mean and on a quantitative concentration inequality for the microscopic particle system around the empirical mean. Our propagation of chaos result recovers the classical Monte Carlo rate, with a prefactor that depends explicitly on the parameters of the problem. Notably, in the case of CBO with anisotropic noise, this prefactor is independent of the problem dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08669v1</guid>
      <category>math.PR</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolai Gerber, Franca Hoffmann, Dohyeon Kim, Urbain Vaes</dc:creator>
    </item>
    <item>
      <title>A Smooth, Recurrent, Non-Periodic Viscosity Solution of the Hamilton-Jacobi Equation</title>
      <link>https://arxiv.org/abs/2505.08700</link>
      <description>arXiv:2505.08700v1 Announce Type: cross 
Abstract: Viscosity solutions of the Hamilton-Jacobi equation were introduced by Lions and Crandall. For Tonelli Hamiltonians, these solutions are generated by the Lax-Oleinik operator. It is known that this operator converges in the autonomous framework, but this convergence fails in the general cases. In this paper, we introduce a method to construct smooth, recurrent, non-periodic viscosity solutions on fixed compact manifolds $M$ of dimension 2 or higher. Additionally, we provide a detailed description of the non-wandering set of the Lax-Oleinik operator and identify its action on various omega-limit sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08700v1</guid>
      <category>math.DS</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Skander Charfi</dc:creator>
    </item>
    <item>
      <title>Morrey's conjecture: rank-one convexity implies quasi-convexity for two-dimensional, two-component maps</title>
      <link>https://arxiv.org/abs/1905.06571</link>
      <description>arXiv:1905.06571v4 Announce Type: replace 
Abstract: We prove that for two-component maps in dimension two, rank-one convexity is equivalent to quasiconvexity. The essential tool for the proof is a fixed-point argument for a suitable set-valued map going from one component to the other that preserves decomposition directions within the $(H_n)$-condition formalism. The existence of a fixed point ensures that, in addition to keeping decomposition directions, joint volume fractions are respected as well, leading to the fundamental fact that every two-dimensional, two-component gradient can be reached by lamination. When maps have more than two components, fixed points exist for every combination of two components, but they do not match in general. Higher dimension would require further insight on how to organize and deal with triangulations for piece-wise affine maps.</description>
      <guid isPermaLink="false">oai:arXiv.org:1905.06571v4</guid>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pablo Pedregal</dc:creator>
    </item>
    <item>
      <title>On Discrete-Time Approximations to Infinite Horizon Differential Games</title>
      <link>https://arxiv.org/abs/2112.03153</link>
      <description>arXiv:2112.03153v2 Announce Type: replace 
Abstract: In this paper we study a discrete-time semidiscretization and a fully discretization (discrete-time, discrete-state) of an infinite time horizon noncooperative $N$-player differential game. We prove that as either the discretization time step or both time step and mesh size parameters approach zero the discrete value function approximates the value function of the differential game. Furthermore, the discrete Nash equilibrium is an $\epsilon$-Nash equilibrium for the continuous-time differential game both in the discrete-time and fully discrete cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.03153v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javier de Frutos, V\'ictor Gat\'on, Julia Novo</dc:creator>
    </item>
    <item>
      <title>Riemannian Adaptive Regularized Newton Methods with H\"older Continuous Hessians</title>
      <link>https://arxiv.org/abs/2309.04052</link>
      <description>arXiv:2309.04052v3 Announce Type: replace 
Abstract: This paper presents strong worst-case iteration and operation complexity guarantees for Riemannian adaptive regularized Newton methods, a unified framework encompassing both Riemannian adaptive regularization (RAR) methods and Riemannian trust region (RTR) methods. We comprehensively characterize the sources of approximation in second-order manifold optimization methods: the objective function's smoothness, retraction's smoothness, and subproblem solver's inexactness. Specifically, for a function with a $\mu$-H\"older continuous Hessian, when equipped with a retraction featuring a $\nu$-H\"older continuous differential and a $\theta$-inexact subproblem solver, both RTR and RAR with $2+\alpha$ regularization (where $\alpha=\min\{\mu,\nu,\theta\}$) locate an $(\epsilon,\epsilon^{\alpha/(1+\alpha)})$-approximate second-order stationary point within at most $O(\epsilon^{-(2+\alpha)/(1+\alpha)})$ iterations and at most $\tilde{O}(\epsilon^{-(4+3\alpha)/(2(1+\alpha))})$ Hessian-vector products. These complexity results are novel and sharp, and reduce to an iteration complexity of $O(\epsilon^{-3/2})$ and an operation complexity of $\tilde{O}(\epsilon^{-7/4})$ when $\alpha=1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04052v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyu Zhang, Rujun Jiang</dc:creator>
    </item>
    <item>
      <title>Inexact subgradient methods for semialgebraic functions</title>
      <link>https://arxiv.org/abs/2404.19517</link>
      <description>arXiv:2404.19517v2 Announce Type: replace 
Abstract: Motivated by the extensive application of approximate gradients in machine learning and optimization, we investigate inexact subgradient methods subject to persistent additive errors. Within a nonconvex semialgebraic framework, assuming boundedness or coercivity, we establish that the method yields iterates that eventually fluctuate near the critical set at a proximity characterized by an $O(\epsilon^\rho)$ distance, where $\epsilon$ denotes the magnitude of subgradient evaluation errors, and $\rho$ encapsulates geometric characteristics of the underlying problem. Our analysis comprehensively addresses both vanishing and constant step-size regimes. Notably, the latter regime inherently enlarges the fluctuation region, yet this enlargement remains on the order of $\epsilon^\rho$. In the convex scenario, employing a universal error bound applicable to coercive semialgebraic functions, we derive novel complexity results concerning averaged iterates. Additionally, our study produces auxiliary results of independent interest, including descent-type lemmas for nonsmooth nonconvex functions and an invariance principle governing the behavior of algorithmic sequences under small-step limits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19517v2</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\'er\^ome Bolte (TSE-R), Tam Le (DAO), \'Eric Moulines (CMAP, MBZUAI), Edouard Pauwels (TSE-R, IUF)</dc:creator>
    </item>
    <item>
      <title>Nesterov acceleration in benignly non-convex landscapes</title>
      <link>https://arxiv.org/abs/2410.08395</link>
      <description>arXiv:2410.08395v3 Announce Type: replace 
Abstract: While momentum-based optimization algorithms are commonly used in the notoriously non-convex optimization problems of deep learning, their analysis has historically been restricted to the convex and strongly convex setting. In this article, we partially close this gap between theory and practice and demonstrate that virtually identical guarantees can be obtained in optimization problems with a `benign' non-convexity. We show that these weaker geometric assumptions are well justified in overparametrized deep learning, at least locally. Variations of this result are obtained for a continuous time model of Nesterov's accelerated gradient descent algorithm (NAG), the classical discrete time version of NAG, and versions of NAG with stochastic gradient estimates with purely additive noise and with noise that exhibits both additive and multiplicative scaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08395v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kanan Gupta, Stephan Wojtowytsch</dc:creator>
    </item>
    <item>
      <title>A distributed proximal splitting method with linesearch for locally Lipschitz gradients</title>
      <link>https://arxiv.org/abs/2410.15583</link>
      <description>arXiv:2410.15583v2 Announce Type: replace 
Abstract: In this paper, we propose a distributed first-order algorithm with backtracking linesearch for solving multi-agent minimisation problems, where each agent handles a local objective involving nonsmooth and smooth components. Unlike existing methods that require global Lipschitz continuity and predefined stepsizes, our algorithm adjusts stepsizes using distributed linesearch procedures, making it suitable for problems where global constants are unavailable or difficult to compute. The proposed algorithm is designed within an abstract linesearch framework for a primal-dual proximal-gradient method to solve min-max convex-concave problems, enabling the consensus constraint to be decoupled from the optimisation task. Our theoretical analysis allows for gradients of functions to be locally Lipschitz continuous, relaxing the prevalent assumption of globally Lipschitz continuous gradients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15583v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felipe Atenas, Minh N. Dao, Matthew K. Tam</dc:creator>
    </item>
    <item>
      <title>Zeroth-order Stochastic Cubic Newton Method Revisited</title>
      <link>https://arxiv.org/abs/2410.22357</link>
      <description>arXiv:2410.22357v4 Announce Type: replace 
Abstract: This paper studies stochastic minimization of a finite-sum loss $ F (\mathbf{x}) = \frac{1}{N} \sum_{\xi=1}^N f(\mathbf{x};\xi) $. In many real-world scenarios, the Hessian matrix of such objectives exhibits a low-rank structure on a batch of data. At the same time, zeroth-order optimization has gained prominence in important applications such as fine-tuning large language models. Drawing on these observations, we propose a novel stochastic zeroth-order cubic Newton method that leverages the low-rank Hessian structure via a matrix recovery-based estimation technique. Our method circumvents restrictive incoherence assumptions, enabling accurate Hessian approximation through finite-difference queries. Theoretically, we establish that for most real-world problems in $\mathbb{R}^n$, $\mathcal{O}\left(\frac{n}{\eta^{\frac{7}{2}}}\right)+\widetilde{\mathcal{O}}\left(\frac{n^2 }{\eta^{\frac{5}{2}}}\right)$ function evaluations suffice to attain a second-order $\eta$-stationary point with high probability. This represents a significant improvement in dimensional dependence over existing methods. This improvement is mostly due to a new Hessian estimator that achieves superior sample complexity; This new Hessian estimation method might be of separate interest. Numerical experiments on matrix recovery and machine learning tasks validate the efficacy and scalability of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22357v4</guid>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Liu, Weibin Peng, Tianyu Wang, Jiajia Yu</dc:creator>
    </item>
    <item>
      <title>Optimization-free Smooth Control Barrier Function for Polygonal Collision Avoidance</title>
      <link>https://arxiv.org/abs/2502.16293</link>
      <description>arXiv:2502.16293v2 Announce Type: replace 
Abstract: Polygonal collision avoidance (PCA) is short for the problem of collision avoidance between two polygons (i.e., polytopes in planar) that own their dynamic equations. This problem suffers the inherent difficulty in dealing with non-smooth boundaries and recently optimization-defined metrics, such as signed distance field (SDF) and its variants, have been proposed as control barrier functions (CBFs) to tackle PCA problems. In contrast, we propose an optimization-free smooth CBF method in this paper, which is computationally efficient and proved to be nonconservative. It is achieved by three main steps: a lower bound of SDF is expressed as a nested Boolean logic composition first, then its smooth approximation is established by applying the latest log-sum-exp method, after which a specified CBF-based safety filter is proposed to address this class of problems. To illustrate its wide applications, the optimization-free smooth CBF method is extended to solve distributed collision avoidance of two underactuated nonholonomic vehicles and drive an underactuated container crane to avoid a moving obstacle respectively, for which numerical simulations are also performed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16293v2</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shizhen Wu, Yongchun Fang, Ning Sun, Biao Lu, Xiao Liang, Yiming Zhao</dc:creator>
    </item>
    <item>
      <title>Lower Complexity Bounds of First-order Methods for Affinely Constrained Composite Non-convex Problems</title>
      <link>https://arxiv.org/abs/2502.17770</link>
      <description>arXiv:2502.17770v2 Announce Type: replace 
Abstract: Many recent studies on first-order methods (FOMs) focus on \emph{composite non-convex non-smooth} optimization with linear and/or nonlinear function constraints. Upper (or worst-case) complexity bounds have been established for these methods. However, little can be claimed about their optimality as no lower bound is known, except for a few special \emph{smooth non-convex} cases. In this paper, we make the first attempt to establish lower complexity bounds of FOMs for solving a class of composite non-convex non-smooth optimization with linear constraints. Assuming two different first-order oracles, we establish lower complexity bounds of FOMs to produce a (near) $\epsilon$-stationary point of a problem (and its reformulation) in the considered problem class, for any given tolerance $\epsilon&gt;0$. Our lower bounds indicate that the existence of a non-smooth convex regularizer can evidently increase the difficulty of an affinely constrained regularized problem over its nonregularized counterpart. In addition, we show that our lower bound of FOMs with the second oracle is tight, with a difference of up to a logarithmic factor from an upper complexity bound established in the extended arXiv version of this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17770v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Liu, Qihang Lin, Yangyang Xu</dc:creator>
    </item>
    <item>
      <title>La M\'ethode du Gradient Proxim\'e</title>
      <link>https://arxiv.org/abs/2503.14479</link>
      <description>arXiv:2503.14479v2 Announce Type: replace 
Abstract: The proximal gradient method is a splitting algorithm for the minimization of the sum of two convex functions, one of which is smooth. It has applications in areas such as mechanics, inverse problems, machine learning, image reconstruction, variational inequalities, statistics, operations research, and optimal transportation. Its formalism encompasses a wide variety of numerical methods in optimization such as gradient descent, projected gradient, iterative thresholding, alternating projections, the constrained Landweber method, as well as various algorithms in statistics and sparse data analysis. This paper aims at providing an account of the main properties of the proximal gradient method and to discuss some of its applications.
  ---
  La m\'ethode du gradient proxim\'e est un algorithme d'\'eclatement pour la minimisation de la somme de deux fonctions convexes, dont l'une est lisse. Elle trouve des applications dans des domaines tels que la m\'ecanique, le traitement du signal, les probl\`emes inverses, l'apprentissage automatique, la reconstruction d'images, les in\'equations variationnelles, les statistiques, la recherche op\'erationnelle et le transport optimal. Son formalisme englobe une grande vari\'et\'e de m\'ethodes num\'eriques en optimisation, telles que la descente de gradient, le gradient projet\'e, la m\'ethode de seuillage it\'eratif, la m\'ethode des projections altern\'ees, la m\'ethode de Landweber contrainte, ainsi que divers algorithmes en statistique et en analyse parcimonieuse de donn\'ees. Cette synth\`ese vise \`a donner un aper\c{c}u des principales propri\'et\'es de la m\'ethode du gradient proxim\'e et d'aborder certaines de ses applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14479v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick L. Combettes</dc:creator>
    </item>
    <item>
      <title>A constructive approach to strengthen algebraic descriptions of function and operator classes</title>
      <link>https://arxiv.org/abs/2504.14377</link>
      <description>arXiv:2504.14377v3 Announce Type: replace 
Abstract: It is well known that functions (resp. operators) satisfying a property~$p$ on a subset $Q\subset \mathbb{R}^d$ cannot necessarily be extended to a function (resp. operator) satisfying~$p$ on the whole of~$\mathbb{R}^d$. Given $Q \subseteq \mathbb{R}^d$, this work considers the problem of obtaining necessary and ideally sufficient conditions to be satisfied by a function (resp. operator) on $Q$, ensuring the existence of an extension of this function (resp. operator) satisfying $p$ on $\mathbb{R}^d$.
  More precisely, given some property $p$, we present a refinement procedure to obtain stronger necessary conditions to be imposed on $Q$. This procedure can be applied iteratively until the stronger conditions are also sufficient. We illustrate the procedure on a few examples, including the strengthening of existing descriptions for the classes of smooth functions satisfying a \L{}ojasiewicz condition, convex blockwise smooth functions, Lipschitz monotone operators, strongly monotone cocoercive operators, and uniformly convex functions.
  In most cases, these strengthened descriptions can be represented, or relaxed, to semi-definite constraints, which can be used to formulate tractable optimization problems on functions (resp. operators) within those classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14377v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anne Rubbens, Julien M. Hendrickx, Adrien Taylor</dc:creator>
    </item>
    <item>
      <title>A primal-dual perspective for distributed TD-learning</title>
      <link>https://arxiv.org/abs/2310.00638</link>
      <description>arXiv:2310.00638v3 Announce Type: replace-cross 
Abstract: The goal of this paper is to investigate distributed temporal difference (TD) learning for a networked multi-agent Markov decision process. The proposed approach is based on distributed optimization algorithms, which can be interpreted as primal-dual Ordinary differential equation (ODE) dynamics subject to null-space constraints. Based on the exponential convergence behavior of the primal-dual ODE dynamics subject to null-space constraints, we examine the behavior of the final iterate in various distributed TD-learning scenarios, considering both constant and diminishing step-sizes and incorporating both i.i.d. and Markovian observation models. Unlike existing methods, the proposed algorithm does not require the assumption that the underlying communication network structure is characterized by a doubly stochastic matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00638v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han-Dong Lim, Donghwan Lee</dc:creator>
    </item>
    <item>
      <title>Learning Optimal Classification Trees Robust to Distribution Shifts</title>
      <link>https://arxiv.org/abs/2310.17772</link>
      <description>arXiv:2310.17772v2 Announce Type: replace-cross 
Abstract: We consider the problem of learning classification trees that are robust to distribution shifts between training and testing/deployment data. This problem arises frequently in high stakes settings such as public health and social work where data is often collected using self-reported surveys which are highly sensitive to e.g., the framing of the questions, the time when and place where the survey is conducted, and the level of comfort the interviewee has in sharing information with the interviewer. We propose a method for learning optimal robust classification trees based on mixed-integer robust optimization technology. In particular, we demonstrate that the problem of learning an optimal robust tree can be cast as a single-stage mixed-integer robust optimization problem with a highly nonlinear and discontinuous objective. We reformulate this problem equivalently as a two-stage linear robust optimization problem for which we devise a tailored solution procedure based on constraint generation. We evaluate the performance of our approach on numerous publicly available datasets, and compare the performance to a regularized, non-robust optimal tree. We show an increase of up to 12.48% in worst-case accuracy and of up to 4.85% in average-case accuracy across several datasets and distribution shifts from using our robust solution in comparison to the non-robust one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17772v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Justin, Sina Aghaei, Andr\'es G\'omez, Phebe Vayanos</dc:creator>
    </item>
    <item>
      <title>A quantum approach for optimal control</title>
      <link>https://arxiv.org/abs/2407.02864</link>
      <description>arXiv:2407.02864v3 Announce Type: replace-cross 
Abstract: In this work, we propose a novel variational quantum approach for solving a class of nonlinear optimal control problems. Our approach integrates Dirac's canonical quantization of dynamical systems with the solution of the ground state of the resulting non-Hermitian Hamiltonian via a variational quantum eigensolver (VQE). We introduce a new perspective on the Dirac bracket formulation for generalized Hamiltonian dynamics in the presence of constraints, providing a clear motivation and illustrative examples. Additionally, we explore the structural properties of Dirac brackets within the context of multidimensional constrained optimization problems.
  Our approach for solving a class of nonlinear optimal control problems employs a VQE-based approach to determine the eigenstate and corresponding eigenvalue associated with the ground state energy of a non-Hermitian Hamiltonian. Assuming access to an ideal VQE, our formulation demonstrates excellent results, as evidenced by selected computational examples. Furthermore, our method performs well when combined with a VQE-based approach for non-Hermitian Hamiltonian systems. Our VQE-based formulation effectively addresses challenges associated with a wide range of optimal control problems, particularly in high-dimensional scenarios. Compared to standard classical approaches, our quantum-based method shows significant promise and offers a compelling alternative for tackling complex, high-dimensional optimization challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02864v3</guid>
      <category>quant-ph</category>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11128-025-04710-z</arxiv:DOI>
      <arxiv:journal_reference>Quantum Inf Process 24, 95 (2025)</arxiv:journal_reference>
      <dc:creator>Hirmay Sandesara, Alok Shukla, Prakash Vedula</dc:creator>
    </item>
    <item>
      <title>Efficient nonlocal linear image denoising: Bilevel optimization with Nonequispaced Fast Fourier Transform and matrix-free preconditioning</title>
      <link>https://arxiv.org/abs/2407.06834</link>
      <description>arXiv:2407.06834v2 Announce Type: replace-cross 
Abstract: We present a new approach for nonlocal image denoising, based around the application of an unnormalized extended Gaussian ANOVA kernel within a bilevel optimization algorithm. A critical bottleneck when solving such problems for finely-resolved images is the solution of huge-scale, dense linear systems arising from the minimization of an energy term. We tackle this using a Krylov subspace approach, with a Nonequispaced Fast Fourier Transform utilized to approximate matrix-vector products in a matrix-free manner. We accelerate the algorithm using a novel change of basis approach to account for the (known) smallest eigenvalue-eigenvector pair of the matrices involved, coupled with a simple but frequently very effective diagonal preconditioning approach. We present a number of theoretical results concerning the eigenvalues and predicted convergence behavior, and a range of numerical experiments which validate our solvers and use them to tackle parameter learning problems. These demonstrate that very large problems may be effectively and rapidly denoised with very low storage requirements on a computer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06834v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'es Miniguano-Trujillo, John W. Pearson, Benjamin D. Goddard</dc:creator>
    </item>
    <item>
      <title>Predictive control for nonlinear stochastic systems: Closed-loop guarantees with unbounded noise</title>
      <link>https://arxiv.org/abs/2407.13257</link>
      <description>arXiv:2407.13257v4 Announce Type: replace-cross 
Abstract: We present a stochastic model predictive control framework for nonlinear systems subject to unbounded process noise with closed-loop guarantees. First, we provide a conceptual shrinking-horizon framework that utilizes general probabilistic reachable sets and minimizes the expected cost. Then, we provide a tractable receding-horizon formulation that uses a nominal state to minimize a deterministic quadratic cost and satisfy tightened constraints. Our theoretical analysis demonstrates recursive feasibility, satisfaction of chance constraints, and bounds on the expected cost for the resulting closed-loop system. We provide a constructive design for probabilistic reachable sets of nonlinear continuously differentiable systems using stochastic contraction metrics and an assumed bound on the covariance matrices. Numerical simulations highlight the computational efficiency and theoretical guarantees of the proposed method. Overall, this paper provides a framework for computationally tractable stochastic predictive control with closed-loop guarantees for nonlinear systems with unbounded noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13257v4</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes K\"ohler, Melanie N. Zeilinger</dc:creator>
    </item>
    <item>
      <title>An Efficient On-Policy Deep Learning Framework for Stochastic Optimal Control</title>
      <link>https://arxiv.org/abs/2410.05163</link>
      <description>arXiv:2410.05163v3 Announce Type: replace-cross 
Abstract: We present a novel on-policy algorithm for solving stochastic optimal control (SOC) problems. By leveraging the Girsanov theorem, our method directly computes on-policy gradients of the SOC objective without expensive backpropagation through stochastic differential equations or adjoint problem solutions. This approach significantly accelerates the optimization of neural network control policies while scaling efficiently to high-dimensional problems and long time horizons. We evaluate our method on classical SOC benchmarks as well as applications to sampling from unnormalized distributions via Schr\"odinger-F\"ollmer processes and fine-tuning pre-trained diffusion models. Experimental results demonstrate substantial improvements in both computational speed and memory efficiency compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05163v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengjian Hua, Mathieu Lauri\`ere, Eric Vanden-Eijnden</dc:creator>
    </item>
    <item>
      <title>Sample-Efficient Reinforcement Learning of Koopman eNMPC</title>
      <link>https://arxiv.org/abs/2503.18787</link>
      <description>arXiv:2503.18787v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) can be used to tune data-driven (economic) nonlinear model predictive controllers ((e)NMPCs) for optimal performance in a specific control task by optimizing the dynamic model or parameters in the policy's objective function or constraints, such as state bounds. However, the sample efficiency of RL is crucial, and to improve it, we combine a model-based RL algorithm with our published method that turns Koopman (e)NMPCs into automatically differentiable policies. We apply our approach to an eNMPC case study of a continuous stirred-tank reactor (CSTR) model from the literature. The approach outperforms benchmark methods, i.e., data-driven eNMPCs using models based on system identification without further RL tuning of the resulting policy, and neural network controllers trained with model-based RL, by achieving superior control performance and higher sample efficiency. Furthermore, utilizing partial prior knowledge about the system dynamics via physics-informed learning further increases sample efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18787v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Mayfrank, Mehmet Velioglu, Alexander Mitsos, Manuel Dahmen</dc:creator>
    </item>
  </channel>
</rss>
