<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Jul 2025 01:41:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Complex Dynamics in Psychological Data: Mapping Individual Symptom Trajectories to Group-Level Patterns</title>
      <link>https://arxiv.org/abs/2507.14161</link>
      <description>arXiv:2507.14161v1 Announce Type: new 
Abstract: This study integrates causal inference, graph analysis, temporal complexity measures, and machine learning to examine whether individual symptom trajectories can reveal meaningful diagnostic patterns. Testing on a longitudinal dataset of N=45 individuals affected by General Anxiety Disorder (GAD) and/or Major Depressive Disorder (MDD) derived from Fisher et al. 2017, we propose a novel pipeline for the analysis of the temporal dynamics of psychopathological symptoms. First, we employ the PCMCI+ algorithm with nonparametric independence test to determine the causal network of nonlinear dependencies between symptoms in individuals with different mental disorders. We found that the PCMCI+ effectively highlights the individual peculiarities of each symptom network, which could be leveraged towards personalized therapies. At the same time, aggregating the networks by diagnosis sheds light to disorder-specific causal mechanisms, in agreement with previous psychopathological literature. Then, we enrich the dataset by computing complexity-based measures (e.g. entropy, fractal dimension, recurrence) from the symptom time series, and feed it to a suitably selected machine learning algorithm to aid the diagnosis of each individual. The new dataset yields 91% accuracy in the classification of the symptom dynamics, proving to be an effective diagnostic support tool. Overall, these findings highlight how integrating causal modeling and temporal complexity can enhance diagnostic differentiation, offering a principled, data-driven foundation for both personalized assessment in clinical psychology and structural advances in psychological research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14161v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eleonora Vitanza, Pietro DeLellis, Chiara Mocenni, Manuel Ruiz Marin</dc:creator>
    </item>
    <item>
      <title>Regional compositional trajectories and structural change: A spatiotemporal multivariate autoregressive framework</title>
      <link>https://arxiv.org/abs/2507.14389</link>
      <description>arXiv:2507.14389v1 Announce Type: new 
Abstract: Compositional data, such as regional shares of economic sectors or property transactions, are central to understanding structural change in economic systems across space and time. This paper introduces a spatiotemporal multivariate autoregressive model tailored for panel data with composition-valued responses at each areal unit and time point. The proposed framework enables the joint modelling of temporal dynamics and spatial dependence under compositional constraints and is estimated via a quasi maximum likelihood approach. We build on recent theoretical advances to establish identifiability and asymptotic properties of the estimator when both the number of regions and time points grow. The utility and flexibility of the model are demonstrated through two applications: analysing property transaction compositions in an intra-city housing market (Berlin), and regional sectoral compositions in Spain's economy. These case studies highlight how the proposed framework captures key features of spatiotemporal economic processes that are often missed by conventional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14389v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Eckardt, Philipp Otto</dc:creator>
    </item>
    <item>
      <title>What Quality Engineers Need to Know about Degradation Models?</title>
      <link>https://arxiv.org/abs/2507.14666</link>
      <description>arXiv:2507.14666v1 Announce Type: new 
Abstract: Degradation models play a critical role in quality engineering by enabling the assessment and prediction of system reliability based on data. The objective of this paper is to provide an accessible introduction to degradation models. We explore commonly used degradation data types, including repeated measures degradation data and accelerated destructive degradation test data, and review modeling approaches such as general path models and stochastic process models. Key inference problems, including reliability estimation and prediction, are addressed. Applications across diverse fields, including material science, renewable energy, civil engineering, aerospace, and pharmaceuticals, illustrate the broad impact of degradation models in industry. We also discuss best practices for quality engineers, software implementations, and challenges in applying these models. This paper aims to provide quality engineers with a foundational understanding of degradation models, equipping them with the knowledge necessary to apply these techniques effectively in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14666v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jared M. Clark, Jie Min, Mingyang Li, Richard L. Warr, Stephanie P. DeHart, Caleb B. King, Lu Lu, Yili Hong</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach to Estimating Effect Sizes in Educational Research</title>
      <link>https://arxiv.org/abs/2507.14848</link>
      <description>arXiv:2507.14848v1 Announce Type: new 
Abstract: In educational research, a first step in the descriptive analysis of data from any psychometric measurement of the performance of subjects in tests at different time points and between groups is to compute relative measures of learning gains and learning achievements. For these effect sizes, there are classical approaches coming from frequentistic statistics like Student's or Welch's $t$-test with their own strengths and weaknesses. In this paper, we propose a purely Bayesian approach for analysing within-group and between-group differences in learning outcomes, taking naturally into account the multilevel structure of the data, as well as heterogeneous variances among time points and groups. We provide a detailed implementation using the brms package in R serving as a wrapper for the probabilistic programming language Stan, facilitating the implementation of these methods in future research by including online supplementary material. We recommend that for a pooled design, one computes an effect size $d_s$, and for a paired design, one should compute two possibly different quantities $d_s$ and $d_z$ to correct for correlations in within-group designs and allowing for comparability across different studies. All these effect sizes are based on ideas coming from Hedge's total effect size $\delta_t$ introduced in 2007.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14848v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yannis B\"ahni</dc:creator>
    </item>
    <item>
      <title>Change point detection in ERA5 ground temperature time series</title>
      <link>https://arxiv.org/abs/2507.15045</link>
      <description>arXiv:2507.15045v1 Announce Type: new 
Abstract: We analyze the ERA5 reanalysis 2-meter temperature time series
  on all land
  grid points using change point analysis. We fit two linear
  slopes to the data with the constraint that they merge at the point in time
  where the slope changes. We compare such fits to a standard
  linear regression in two ways: We use Akaike's and the
  Bayesian
  information
  criteria for model selection, and we test against the null hypothesis of
  no change of the trend value. For those grid points where the
  dual linear fit is superior, we construct maps of the time when the trend
  changes, and of the warming trends in both time intervals. In doing so, we
  indentify areas where warming speeds up, but find as well areas where
  warming slows down.
  We thereby contribute to the characterization of local effects of climate
  change. We find that many grid points exhibit a
  change to a much stronger warming trend around the 1980s. This raises
  the question of whether the climate system has already passed some
  tipping point.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15045v1</guid>
      <category>stat.AP</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatemeh Aghaei A., Ewan T. Phillips, Holger Kantz</dc:creator>
    </item>
    <item>
      <title>Space time modeling for drought classification and prediction</title>
      <link>https://arxiv.org/abs/2507.15099</link>
      <description>arXiv:2507.15099v1 Announce Type: new 
Abstract: The Standardized Precipitation Index (SPI) is a critical tool for monitoring drought conditions, typically relying on normalized accumulated precipitation. While longer historical records of precipitation yield more accurate parameter estimates of marginal distribution, they often reflect nonstationary influences such as anthropogenic climate change and multidecadal natural variability. Traditional approaches either overlook this nonstationarity or address it with quasi-stationary reference periods. This study introduces a novel nonstationary SPI framework that utilizes generalized additive models (GAMs) to flexibly model the spatiotemporal variability inherent in drought processes. GAMs are employed to estimate parameters of the Gamma distribution, while dual extreme tails flexible models are integrated to robustly capture the probabilistic risk of extreme drought events. Future drought and wet extremes events in terms of return levels are calculated using an extended generalized Pareto distribution, which offers flexibility in modeling the entire distribution of the data while bypassing the threshold selection step. Results demonstrate that the proposed nonstationary SPI model is both stable and capable of reproducing known nonstationary drought patterns, while also providing new insights into the evolving dynamics of drought. This approach represents a significant advancement in drought modeling under changing climatic conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15099v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Touqeer Ahmad, Taha Hasan</dc:creator>
    </item>
    <item>
      <title>Does the draw matter in an incomplete round-robin tournament? The case of the UEFA Champions League</title>
      <link>https://arxiv.org/abs/2507.15320</link>
      <description>arXiv:2507.15320v1 Announce Type: new 
Abstract: A fundamental reform has been introduced in the 2024/25 season of club competitions organised by the Union of European Football Associations (UEFA): the well-established group stage has been replaced by an incomplete round-robin format. In this format, the 36 teams are ranked in a single league table, but play against only a subset of the competitors. While this innovative change has highlighted that the incomplete round-robin tournament is a reasonable alternative to the standard design of allocating the teams into round-robin groups, the characteristics of the new format remain unexplored. Our paper contributes to this topic by using simulations to compare the uncertainty generated by the draw in the old format with that in the new format of the UEFA Champions League. We develop a method to break down the impact of the 2024/25 reform into various components for each team. The new format is found to decrease the overall effect of the draw. However, this reduction can mainly be attributed to the inaccurate seeding system used by UEFA. When teams are seeded based on their actual strength, the impact of the draw is about the same in a tournament with an incomplete round-robin league or a group stage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15320v1</guid>
      <category>stat.AP</category>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, Andr\'as Gyimesi, Dries Goossens, Karel Devriesere, Roel Lambers, Frits Spieksma</dc:creator>
    </item>
    <item>
      <title>Log-Euclidean Frameworks for Smooth Brain Connectivity Trajectories</title>
      <link>https://arxiv.org/abs/2507.15374</link>
      <description>arXiv:2507.15374v1 Announce Type: new 
Abstract: The brain is often studied from a network perspective, where functional activity is assessed using functional Magnetic Resonance Imaging (fMRI) to estimate connectivity between predefined neuronal regions. Functional connectivity can be represented by correlation matrices computed over time, where each matrix captures the Pearson correlation between the mean fMRI signals of different regions within a sliding window. We introduce several Log-Euclidean Riemannian framework for constructing smooth approximations of functional brain connectivity trajectories. Representing dynamic functional connectivity as time series of full-rank correlation matrices, we leverage recent theoretical Log-Euclidean diffeomorphisms to map these trajectories in practice into Euclidean spaces where polynomial regression becomes feasible. Pulling back the regressed curve ensures that each estimated point remains a valid correlation matrix, enabling a smooth, interpretable, and geometrically consistent approximation of the original brain connectivity dynamics. Experiments on fMRI-derived connectivity trajectories demonstrate the geometric consistency and computational efficiency of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15374v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>GSI'25 - International Conference on Geometric Science of Information, Oct 2025, Saint-Malo (France), France</arxiv:journal_reference>
      <dc:creator>Olivier Bisson (EPIONE, UniCA), Yanis Aeschlimann (CRONOS, UniCA), Samuel Deslauriers-Gauthier (CRONOS, UniCA), Xavier Pennec (EPIONE, UniCA)</dc:creator>
    </item>
    <item>
      <title>Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data</title>
      <link>https://arxiv.org/abs/2507.14175</link>
      <description>arXiv:2507.14175v1 Announce Type: cross 
Abstract: Background: Mental illnesses such as depression and anxiety require improved methods for early detection and personalized intervention. Traditional predictive models often rely on unimodal data or early fusion strategies that fail to capture the complex, multimodal nature of psychiatric data. Advanced integration techniques, such as intermediate (latent space) fusion, may offer better accuracy and clinical utility. Methods: Using data from the BRIGHTEN clinical trial, we evaluated intermediate (latent space) fusion for predicting daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented with a Random Forest (RF) model and intermediate fusion implemented via a Combined Model (CM) using autoencoders and a neural network. The dataset included behavioral (smartphone-based), demographic, and clinical features. Experiments were conducted across multiple temporal splits and data stream combinations. Performance was evaluated using mean squared error (MSE) and coefficient of determination (R2). Results: The CM outperformed both RF and Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985 vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed signs of overfitting, with a large gap between training and test performance, while the CM maintained consistent generalization. Performance was best when integrating all data modalities in the CM (in contradistinction to RF), underscoring the value of latent space fusion for capturing non-linear interactions in complex psychiatric datasets. Conclusion: Latent space fusion offers a robust alternative to traditional fusion methods for prediction with multimodal mental health data. Future work should explore model interpretability and individual-level prediction for clinical deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14175v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youcef Barkat, Dylan Hamitouche, Deven Parekh, Ivy Guo, David Benrimoh</dc:creator>
    </item>
    <item>
      <title>On the Testing of complete causal mediation and its applications</title>
      <link>https://arxiv.org/abs/2507.14246</link>
      <description>arXiv:2507.14246v1 Announce Type: cross 
Abstract: The Complete Mediation Test (CMT) serves as a specialized approach of mediation analysis to assess whether an independent variable A, influences an outcome variable Y exclusively through a mediator M, without any direct effect. An application of CMT lies in Mendelian Randomization (MR) studies, where it can be used to investigate non-pleiotropy, that is, to test whether genetic variants impact a disease outcome solely through their effect on a target exposure variable. Traditionally, CMT has relied on two significance-based criteria and a proportion-based criterion with a heuristic threshold that has not been rigorously evaluated. In this paper, we explored the theoretical properties of conventional CMT, and proposed using standardized absolute proportion of mediation (SAPM) as a criterion for CMT. We, systematically assess the performance of various CMT criteria via simulation, and demonstrate their practical utility in the context of MR studies. Our results indicate that the offers the best performance. We also propose using different optimal thresholds depending on whether the mediator and outcome are continuous or binary. The SAPM with proper thresholds ensures that the indirect pathway meaningfully accounts for the effect of the exposure on the outcome, thereby strengthening the case for complete mediation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14246v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichin Tsai, Wan-Tzu Chang, Jia Jyun Sie, Cathy SJ Fann, Iebin Lian</dc:creator>
    </item>
    <item>
      <title>Leveraging Covariates in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2507.14311</link>
      <description>arXiv:2507.14311v1 Announce Type: cross 
Abstract: It is common practice to incorporate additional covariates in empirical economics. In the context of Regression Discontinuity (RD) designs, covariate adjustment plays multiple roles, making it essential to understand its impact on analysis and conclusions. Typically implemented via local least squares regressions, covariate adjustment can serve three main distinct purposes: (i) improving the efficiency of RD average causal effect estimators, (ii) learning about heterogeneous RD policy effects, and (iii) changing the RD parameter of interest. This article discusses and illustrates empirically how to leverage covariates effectively in RD designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14311v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Filippo Palomba</dc:creator>
    </item>
    <item>
      <title>A Hybrid Mixture Approach for Clustering and Characterizing Cancer Data</title>
      <link>https://arxiv.org/abs/2507.14380</link>
      <description>arXiv:2507.14380v1 Announce Type: cross 
Abstract: Model-based clustering is widely used for identifying and distinguishing types of diseases. However, modern biomedical data coming with high dimensions make it challenging to perform the model estimation in traditional cluster analysis. The incorporation of factor analyzer into the mixture model provides a way to characterize the large set of data features, but the current estimation method is computationally impractical for massive data due to the intrinsic slow convergence of the embedded algorithms, and the incapability to vary the size of the factor analyzers, preventing the implementation of a generalized mixture of factor analyzers and further characterization of the data clusters. We propose a hybrid matrix-free computational scheme to efficiently estimate the clusters and model parameters based on a Gaussian mixture along with generalized factor analyzers to summarize the large number of variables using a small set of underlying factors. Our approach outperforms the existing method with faster convergence while maintaining high clustering accuracy. Our algorithms are applied to accurately identify and distinguish types of breast cancer based on large tumor samples, and to provide a generalized characterization for subtypes of lymphoma using massive gene records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14380v1</guid>
      <category>stat.ME</category>
      <category>q-bio.TO</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazeem Kareem, Fan Dai</dc:creator>
    </item>
    <item>
      <title>Bivariate generalized autoregressive models for forecasting bivariate non-Gaussian times series</title>
      <link>https://arxiv.org/abs/2507.14442</link>
      <description>arXiv:2507.14442v1 Announce Type: cross 
Abstract: This paper introduces a novel approach, the bivariate generalized autoregressive (BGAR) model, for modeling and forecasting bivariate time series data. The BGAR model generalizes the bivariate vector autoregressive (VAR) models by allowing data that does not necessarily follow a normal distribution. We consider a random vector of two time series and assume each belongs to the canonical exponential family, similarly to the univariate generalized autoregressive moving average (GARMA) model. We include autoregressive terms of one series into the dynamical structure of the other and vice versa. The model parameters are estimated using the conditional maximum likelihood (CML) method. We provide general closed-form expressions for the conditional score vector and conditional Fisher information matrix, encompassing all canonical exponential family distributions. We develop asymptotic confidence intervals and hypothesis tests. We discuss techniques for model selection, residual diagnostic analysis, and forecasting. We carry out Monte Carlo simulation studies to evaluate the performance of the finite sample CML inferences, including point and interval estimation. An application to real data analyzes the number of leptospirosis cases on hospitalizations due to leptospirosis in S\~ao Paulo state, Brazil. Competing models such as GARMA, autoregressive integrated moving average (ARIMA), and VAR models are considered for comparison purposes. The new model outperforms the competing models by providing more accurate out-of-sample forecasting and allowing quantification of the lagged effect of the case count series on hospitalizations due to leptospirosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14442v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatiane Fontana Ribeiro, Airlane P. Alencar, F\'abio M. Bayer</dc:creator>
    </item>
    <item>
      <title>The Rest is Silence: Leveraging Unseen Species Models for Computational Musicology</title>
      <link>https://arxiv.org/abs/2507.14638</link>
      <description>arXiv:2507.14638v1 Announce Type: cross 
Abstract: For many decades, musicologists have engaged in creating large databases serving different purposes for musicological research and scholarship. With the rise of fields like music information retrieval and digital musicology, there is now a constant and growing influx of musicologically relevant datasets and corpora. In historical or observational settings, however, these datasets are necessarily incomplete, and the true extent of a collection of interest remains unknown -- silent. Here, we apply, for the first time, so-called Unseen Species models (USMs) from ecology to areas of musicological activity. After introducing the models formally, we show in four case studies how USMs can be applied to musicological data to address quantitative questions like: How many composers are we missing in RISM? What percentage of medieval sources of Gregorian chant have we already cataloged? How many differences in music prints do we expect to find between editions? How large is the coverage of songs from genres of a folk music tradition? And, finally, how close are we in estimating the size of the harmonic vocabulary of a large number of composers?</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14638v1</guid>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabian C. Moss, Jan Haji\v{c} jr., Adrian Nachtwey, Laurent Pugin</dc:creator>
    </item>
    <item>
      <title>Powerful Foldover Designs</title>
      <link>https://arxiv.org/abs/2507.14648</link>
      <description>arXiv:2507.14648v1 Announce Type: cross 
Abstract: The foldover technique for screening designs is well known to guarantee zero aliasing of the main effect estimators with respect to two factor interactions and quadratic effects. It is a key feature of many popular response surface designs, including central composite designs, definitive screening designs, and most orthogonal, minimally-aliased response surface designs. In this paper, we show the foldover technique is even more powerful, because it produces degrees of freedom for a variance estimator that is independent of model selection. These degrees of freedom are characterized as either pure error or fake factor degrees of freedom. A fast design construction algorithm is presented that minimizes the expected confidence interval criterion to maximize the power of screening main effects. An augmented design and analysis method is also presented to avoid having too many degrees of freedom for estimating variance and to improve model selection performance for second order models. Simulation studies show our new designs are at least as good as traditional designs when effect sparsity and hierarchy hold, but do significantly better when these effect principles do not hold. A real data example is given for a 20-run experiment where optimization of ethylene concentration is performed by manipulating eight process parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14648v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan W. Stallrich, Rakhi Singh, Kyle Vogt-Lowell, Fanxing Li</dc:creator>
    </item>
    <item>
      <title>An Information-Theoretic Intersectional Data Valuation Theory</title>
      <link>https://arxiv.org/abs/2507.14742</link>
      <description>arXiv:2507.14742v1 Announce Type: cross 
Abstract: In contemporary digital markets, personal data often reveals not just isolated traits, but complex, intersectional identities based on combinations of race, gender, disability, and other protected characteristics. This exposure generates a privacy externality: firms benefit economically from profiling, prediction, and personalization, while users face hidden costs in the form of social risk and discrimination. We introduce a formal pricing rule that quantifies and internalizes this intersectional privacy loss using mutual information, assigning monetary value to the entropy reduction induced by each datum. The result is a Pigouvian-style surcharge that discourages harmful data trades and rewards transparency. Our formulation has the advantage that it operates independently of the underlying statistical model of the intersectional variables, be it parametric, nonparametric, or learned, and can be approximated in practice by discretizing the intersectional joint probability distributions. We illustrate how regulators can calibrate this surcharge to reflect different societal values, and argue that it provides not just a technical fix to market failures, but also a redistributive shield that empowers vulnerable groups in the face of asymmetric digital power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14742v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduardo C. Garrido-Merch\'an</dc:creator>
    </item>
    <item>
      <title>Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization</title>
      <link>https://arxiv.org/abs/2507.14746</link>
      <description>arXiv:2507.14746v1 Announce Type: cross 
Abstract: High-fidelity simulations and physical experiments are essential for engineering analysis and design. However, their high cost often limits their applications in two critical tasks: global sensitivity analysis (GSA) and optimization. This limitation motivates the common use of Gaussian processes (GPs) as proxy regression models to provide uncertainty-aware predictions based on a limited number of high-quality observations. GPs naturally enable efficient sampling strategies that support informed decision-making under uncertainty by extracting information from a subset of possible functions for the model of interest. Despite their popularity in machine learning and statistics communities, sampling from GPs has received little attention in the community of engineering optimization. In this paper, we present the formulation and detailed implementation of two notable sampling methods -- random Fourier features and pathwise conditioning -- for generating posterior samples from GPs. Alternative approaches are briefly described. Importantly, we detail how the generated samples can be applied in GSA, single-objective optimization, and multi-objective optimization. We show successful applications of these sampling methods through a series of numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14746v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bach Do, Nafeezat A. Ajenifuja, Taiwo A. Adebiyi, Ruda Zhang</dc:creator>
    </item>
    <item>
      <title>A Stability-Driven Framework for Long-Term Hourly Electricity Demand Forecasting</title>
      <link>https://arxiv.org/abs/2507.15001</link>
      <description>arXiv:2507.15001v1 Announce Type: cross 
Abstract: Long-term electricity demand forecasting is essential for grid and operations planning, as well as for the analysis and planning of energy transition strategies. However, accurate long-term load forecasting with high temporal resolution remains challenging, as most existing approaches focus on aggregated forecasts, which require accurate prediction of numerous variables for bottom-up sectoral forecasts. In this study, we propose a parsimonious methodology that employs t-tests to verify load stability and the correlation of load with gross domestic product (GDP) to produce a long-term hourly load forecast. Applying this method to Singapore's electricity demand, analysis of multi-year historical data (2004-2022) reveals that its relative hourly load has remained statistically stable, with an overall percentage deviation of 4.24% across seasonality indices. Utilizing these stability findings, five-year-ahead total yearly forecasts were generated using GDP as a predictor, and hourly loads were forecasted using hourly seasonality index fractions. The maximum Mean Absolute Percentage Error (MAPE) across multiple experiments for six-year-ahead forecasts was 6.87%. The methodology was further applied to Belgium (an OECD country) and Bulgaria (a non-OECD country), yielding MAPE values of 6.81% and 5.64%, respectively. Additionally, stability results were incorporated into a short-term forecasting model based on exponential smoothing, demonstrating comparable or improved accuracy relative to existing machine learning-based methods. These findings indicate that parsimonious approaches can effectively produce long-term, high-resolution forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15001v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumyadeep Dhar, Ayushkumar Parmar, Haifeng Qiu, Juan Ramon L. Senga, S. Viswanathan</dc:creator>
    </item>
    <item>
      <title>Time-Dependent Pseudo $\boldsymbol{R^2}$ for Assessing Predictive Performance in Competing Risks Data</title>
      <link>https://arxiv.org/abs/2507.15040</link>
      <description>arXiv:2507.15040v1 Announce Type: cross 
Abstract: Evaluating and validating the performance of prediction models is a fundamental task in statistics, machine learning, and their diverse applications. However, developing robust performance metrics for competing risks time-to-event data poses unique challenges. We first highlight how certain conventional predictive performance metrics, such as the C-index, Brier score, and time-dependent AUC, can yield undesirable results when comparing predictive performance between different prediction models. To address this research gap, we introduce a novel time-dependent pseudo $R^2$ measure to evaluate the predictive performance of a predictive cumulative incidence function over a restricted time domain under right-censored competing risks time-to-event data. Specifically, we first propose a population-level time-dependent pseudo $R^2$ measures for the competing risk event of interest and then define their corresponding sample versions based on right-censored competing risks time-to-event data. We investigate the asymptotic properties of the proposed measure and demonstrate its advantages over conventional metrics through comprehensive simulation studies and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15040v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zian Zhuang, Wen Su, Eric Kawaguchi, Gang Li</dc:creator>
    </item>
    <item>
      <title>Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts</title>
      <link>https://arxiv.org/abs/2507.15079</link>
      <description>arXiv:2507.15079v1 Announce Type: cross 
Abstract: Quantifying the uncertainty of forecasting models is essential to assess and mitigate the risks associated with data-driven decisions, especially in volatile domains such as electricity markets. Machine learning methods can provide highly accurate electricity price forecasts, critical for informing the decisions of market participants. However, these models often lack uncertainty estimates, which limits the ability of decision makers to avoid unnecessary risks. In this paper, we propose a novel method for generating probabilistic forecasts from ensembles of point forecasts, called Isotonic Quantile Regression Averaging (iQRA). Building on the established framework of Quantile Regression Averaging (QRA), we introduce stochastic order constraints to improve forecast accuracy, reliability, and computational costs. In an extensive forecasting study of the German day-ahead electricity market, we show that iQRA consistently outperforms state-of-the-art postprocessing methods in terms of both reliability and sharpness. It produces well-calibrated prediction intervals across multiple confidence levels, providing superior reliability to all benchmark methods, particularly coverage-based conformal prediction. In addition, isotonic regularization decreases the complexity of the quantile regression problem and offers a hyperparameter-free approach to variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15079v1</guid>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkadiusz Lipiecki, Bartosz Uniejewski</dc:creator>
    </item>
    <item>
      <title>Prediction of linear fractional stable motions using codifference</title>
      <link>https://arxiv.org/abs/2507.15437</link>
      <description>arXiv:2507.15437v1 Announce Type: cross 
Abstract: The linear fractional stable motion (LFSM) extends the fractional Brownian motion (fBm) by considering $\alpha$-stable increments. We propose a method to forecast future increments of the LFSM from past discrete-time observations, using the conditional expectation when $\alpha&gt;1$ or a semimetric projection otherwise. It relies on the codifference, which describes the serial dependence of the process, instead of the covariance. Indeed, covariance is commonly used for predicting an fBm but it is infinite when $\alpha&lt;2$. Some theoretical properties of the method and of its accuracy are studied and both a simulation study and an application to real data confirm the relevance of the approach. The LFSM-based method outperforms the fBm, when forecasting high-frequency FX rates. It also shows a promising performance in the forecast of time series of volatilities, decomposing properly, in the fractal dynamic of rough volatilities, the contribution of the kurtosis of the increments and the contribution of their serial dependence. Moreover, the analysis of hit ratios suggests that, beside independence, persistence, and antipersistence, a fourth regime of serial dependence exists for fractional processes, characterized by a selective memory controlled by a few large increments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15437v1</guid>
      <category>stat.ME</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matthieu Garcin, Karl Sawaya, Thomas Valade</dc:creator>
    </item>
    <item>
      <title>Approaches for modelling the term-structure of default risk under IFRS 9: A tutorial using discrete-time survival analysis</title>
      <link>https://arxiv.org/abs/2507.15441</link>
      <description>arXiv:2507.15441v1 Announce Type: cross 
Abstract: Under the International Financial Reporting Standards (IFRS) 9, credit losses ought to be recognised timeously and accurately. This requirement belies a certain degree of dynamicity when estimating the constituent parts of a credit loss event, most notably the probability of default (PD). It is notoriously difficult to produce such PD-estimates at every point of loan life that are adequately dynamic and accurate, especially when considering the ever-changing macroeconomic background. In rendering these lifetime PD-estimates, the choice of modelling technique plays an important role, which is why we first review a few classes of techniques, including the merits and limitations of each. Our main contribution however is the development of an in-depth and data-driven tutorial using a particular class of techniques called discrete-time survival analysis. This tutorial is accompanied by a diverse set of reusable diagnostic measures for evaluating various aspects of a survival model and the underlying data. A comprehensive R-based codebase is further contributed. We believe that our work can help cultivate common modelling practices under IFRS 9, and should be valuable to practitioners, model validators, and regulators alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15441v1</guid>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Botha, Tanja Verster</dc:creator>
    </item>
    <item>
      <title>On the Distribution of a Two-Dimensional Random Walk with Restricted Angles</title>
      <link>https://arxiv.org/abs/2507.15475</link>
      <description>arXiv:2507.15475v1 Announce Type: cross 
Abstract: In this paper, we derive the distribution of a two-dimensional (complex) random walk in which the angle of each step is restricted to a subset of the circle. This setting appears in various domains, such as in over-the-air computation in signal processing. In particular, we derive the exact joint and marginal distributions for two steps, numerical solutions for a general number of steps, and approximations for a large number of steps. Furthermore, we provide an exact characterization of the support for an arbitrary number of steps. The results in this work provide a reference for future work involving such problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15475v1</guid>
      <category>eess.SP</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karl-Ludwig Besser</dc:creator>
    </item>
    <item>
      <title>The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information</title>
      <link>https://arxiv.org/abs/2507.15548</link>
      <description>arXiv:2507.15548v1 Announce Type: cross 
Abstract: Background: Radiomics shows promise in characterizing glioblastoma, but its added value over clinical and molecular predictors has yet to be proven. This study assessed the added value of conventional radiomics (CR) and deep learning (DL) MRI radiomics for glioblastoma prognosis (&lt;= 6 vs &gt; 6 months survival) on a large multi-center dataset.
  Methods: After patient selection, our curated dataset gathers 1152 glioblastoma (WHO 2016) patients from five Swiss centers and one public source. It included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI data (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were developed using standard methods and evaluated on internal and external cohorts. Sub-analyses assessed models with different feature sets (imaging-only, clinical/molecular-only, combined-features) and patient subsets (S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).
  Results: The best performance was observed in the full cohort (S-1). In external validation, the combined-feature CR model achieved an AUC of 0.75, slightly, but significantly outperforming clinical-only (0.74) and imaging-only (0.68) models. DL models showed similar trends, though without statistical significance. In S-2 and S-3, combined models did not outperform clinical-only models. Exploratory analysis of CR models for overall survival prediction suggested greater relevance of imaging data: across all subsets, combined-feature models significantly outperformed clinical-only models, though with a modest advantage of 2-4 C-index points.
  Conclusions: While confirming the predictive value of anatomical MRI sequences for glioblastoma prognosis, this multi-center study found standard CR and DL radiomics approaches offer minimal added value over demographic predictors such as age and gender.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15548v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D. Abler, O. Pusterla, A. Joye-K\"uhnis, N. Andratschke, M. Bach, A. Bink, S. M. Christ, P. Hagmann, B. Pouymayou, E. Pravat\`a, P. Radojewski, M. Reyes, L. Ruinelli, R. Schaer, B. Stieltjes, G. Treglia, W. Valenzuela, R. Wiest, S. Zoergiebel, M. Guckenberger, S. Tanadini-Lang, A. Depeursinge</dc:creator>
    </item>
    <item>
      <title>Diffusion models for multivariate subsurface generation and efficient probabilistic inversion</title>
      <link>https://arxiv.org/abs/2507.15809</link>
      <description>arXiv:2507.15809v1 Announce Type: cross 
Abstract: Diffusion models offer stable training and state-of-the-art performance for deep generative modeling tasks. Here, we consider their use in the context of multivariate subsurface modeling and probabilistic inversion. We first demonstrate that diffusion models enhance multivariate modeling capabilities compared to variational autoencoders and generative adversarial networks. In diffusion modeling, the generative process involves a comparatively large number of time steps with update rules that can be modified to account for conditioning data. We propose different corrections to the popular Diffusion Posterior Sampling approach by Chung et al. (2023). In particular, we introduce a likelihood approximation accounting for the noise-contamination that is inherent in diffusion modeling. We assess performance in a multivariate geological scenario involving facies and correlated acoustic impedance. Conditional modeling is demonstrated using both local hard data (well logs) and nonlinear geophysics (fullstack seismic data). Our tests show significantly improved statistical robustness, enhanced sampling of the posterior probability density function and reduced computational costs, compared to the original approach. The method can be used with both hard and indirect conditioning data, individually or simultaneously. As the inversion is included within the diffusion process, it is faster than other methods requiring an outer-loop around the generative model, such as Markov chain Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15809v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Roberto Miele, Niklas Linde</dc:creator>
    </item>
    <item>
      <title>Dynamic Interconnections between Corruption and Economic Growth</title>
      <link>https://arxiv.org/abs/2410.08132</link>
      <description>arXiv:2410.08132v2 Announce Type: replace 
Abstract: This study explores the dynamic relationship between corruption and economic growth through an approach based on a system of stochastic equations. In the context of globalization and economic interdependencies, corruption not only affects investment and distorts markets, but it can also, under certain conditions, temporarily boost economic activity. Using data from the Gross Domestic Product (GDP) and the Corruption Perception Index (CPI), we implement a time-series-based model to capture the interactions between these two variables. Through a coupled vector autoregressive equations system, our model identifies patterns of interdependence between economic fluctuations and perceptions of corruption at a global level. Employing graph theory and Granger causality, we build a network of interconnections that illustrates how corruption dynamics in one country can influence economic growth and corruption perception in others. The results provide a robust tool for analyzing international political-economic relationships and can serve as a basis for designing policies that promote transparency and sustainable development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08132v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Macavilca Tello Bartolome, Kevin Fernandez, Oscar Cutipa-Luque, Yhon Tiahuallpa, Helder Rojas</dc:creator>
    </item>
    <item>
      <title>Regional consistency evaluation and sample size calculation under two MRCTs</title>
      <link>https://arxiv.org/abs/2411.15567</link>
      <description>arXiv:2411.15567v2 Announce Type: replace 
Abstract: Multi-regional clinical trial (MRCT) has been common practice for drug development and global registration. The FDA guidance `Demonstrating Substantial Evidence of Effectiveness for Human Drug and Biological Products Guidance for Industry' (FDA, 2019) requires that substantial evidence of effectiveness of a drug/biologic product to be demonstrated for market approval. In the situations where two pivotal MRCTs are needed to establish effectiveness of a specific indication for a drug or biological product, a systematic approach of consistency evaluation for regional effect is crucial. In this paper, we first present some existing regional consistency evaluations in a unified way that facilitates regional sample size calculation under the simple fixed effects model. Second, we extend the two commonly used consistency assessment criteria of MHLW (2007) in the context of two MRCTs and provide their evaluation and regional sample size calculation. Numerical studies demonstrate the proposed regional sample size attains the desired probability of showing regional consistency. A hypothetical example is presented to illustrate the application. We provide an R package for implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15567v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kunhai Qing, Xinru Ren, Shuping Jiang, Ping Yang, Menggang Yu, Jin Xu</dc:creator>
    </item>
    <item>
      <title>Spatial Dependencies in Item Response Theory: Gaussian Process Priors for Geographic and Cognitive Measurement</title>
      <link>https://arxiv.org/abs/2507.09824</link>
      <description>arXiv:2507.09824v2 Announce Type: replace 
Abstract: Measurement validity in Item Response Theory depends on appropriately modeling dependencies between items when these reflect meaningful theoretical structures rather than random measurement error. In ecological assessment, citizen scientists identifying species across geographic regions exhibit systematic spatial patterns in task difficulty due to environmental factors. Similarly, in Author Recognition Tests, literary knowledge organizes by genre, where familiarity with science fiction authors systematically predicts recognition of other science fiction authors. Current spatial Item Response Theory methods, represented by the 1PLUS, 2PLUS, and 3PLUS model family, address these dependencies but remain limited by (1) binary response restrictions, and (2) conditional autoregressive priors that impose rigid local correlation assumptions, preventing effective modeling of complex spatial relationships. Our proposed method, Spatial Gaussian Process Item Response Theory (SGP-IRT), addresses these limitations by replacing conditional autoregressive priors with flexible Gaussian process priors that adapt to complex dependency structures while maintaining principled uncertainty quantification. SGP-IRT accommodates polytomous responses and models spatial dependencies in both geographic and abstract cognitive spaces, where items cluster by theoretical constructs rather than physical proximity. Simulation studies demonstrate improved parameter recovery, particularly for item difficulty estimation. Empirical applications show enhanced recovery of meaningful difficulty surfaces and improved measurement precision across psychological, educational, and ecological research applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09824v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingya Huang, Soham Ghosh</dc:creator>
    </item>
    <item>
      <title>Modeling Firm-Level ESG-Sentiment Interactions in Stock Returns: Evidence from 16 Companies Using Retrofitted Word Embeddings</title>
      <link>https://arxiv.org/abs/2507.11485</link>
      <description>arXiv:2507.11485v4 Announce Type: replace 
Abstract: This study investigates how emotion-specific sentiment embedded in financial news headlines interacts with firm-level Environmental, Social, and Governance (ESG) ratings to influence stock return behavior. Addressing key methodological gaps in existing literature, the analysis leverages Retrofitted Word Embeddings to encode discrete emotional cues tailored to ESG-relevant narratives. Unlike prior studies that rely on lexicon-based or transformer-based models, this approach explicitly incorporates domain-specific emotional semantics while accounting for firm-level heterogeneity and temporal sentiment fluctuations. Using a dataset of 16 multinational firms and sentiment data extracted from Seeking Alpha headlines, the study tests three hypotheses: (1) emotion-specific sentiment independently predicts stock returns; (2) the moderating effect of sentiment varies across ESG dimensions; and (3) positive (negative) sentiment amplifies (dampens) ESG performance effects. The analysis implements a dual sentiment aggregation strategy and introduces a triple-significance filtering criterion to identify robust interactions. Results support Hypotheses 1 and 2, with emotions such as anticipation and trust showing consistent associations with return variation across firms and ESG categories. However, findings for Hypothesis 3 are mixed: while some sentiment-ESG combinations align with theoretical expectations, many contradictory interactions exhibit stronger effects. In this study, retrofitted embeddings outperform the NRC Emotion Lexicon in explaining stock return variation within ESG-sentiment interaction models, underscoring the value of emotional nuance in ESG-finance modeling. These results underscore the importance of emotion-sensitive sentiment modeling in understanding investor behavior and ESG-related stock price movements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11485v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sangdeok Lee</dc:creator>
    </item>
    <item>
      <title>Hierarchical Temporal Point Process Modeling of Aggressive Behavior Onset in Psychiatric Inpatient Youth with Autism for Branching Factor Estimation</title>
      <link>https://arxiv.org/abs/2507.12424</link>
      <description>arXiv:2507.12424v2 Announce Type: replace 
Abstract: Aggressive behavior in autistic inpatient youth often arises in temporally clustered bursts complicating efforts to distinguish external triggers from internal escalation. The sample population branching factor-the expected number of new onsets triggered by a given event-is a key summary of self-excitation in behavior dynamics. Prior pooled models overestimate this quantity by ignoring patient-specific variability. We addressed this using a hierarchical Hawkes process with an exponential kernel and edge-effect correction allowing partial pooling across patients. This approach reduces bias from high-frequency individuals and stabilizes estimates for those with sparse data. Bayesian inference was performed using the No U-Turn Sampler with model evaluation via convergence diagnostics, power-scaling sensitivity analysis, and multiple Goodness-of-Fit (GOF) metrics: PSIS-LOO the Lewis test with Durbin's modification and residual analysis based on the Random Time Change Theorem (RTCT). The hierarchical model yielded a significantly lower and more precise branching factor estimate mean (0.742 +- 0.026) than the pooled model (0.899 +- 0.015) and narrower intervals than the unpooled model (0.717 +- 0.139). This led to a threefold smaller cascade of events per onset under the hierarchical model. Sensitivity analyses confirmed robustness to prior and likelihood perturbations while the unpooled model showed instability for sparse individuals. GOF measures consistently favored or on par to the hierarchical model. Hierarchical Hawkes modeling with edge-effect correction provides robust estimation of branching dynamics by capturing both within- and between-patient variability. This enables clearer separation of endogenous from exogenous events supports linkage to physiological signals and enhances early warning systems individualized treatment and resource allocation in inpatient care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12424v2</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <category>stat.OT</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Potter, Michael Everett, Deniz Erdogmus, Yuna Watanabe, Tales Imbiriba, Matthew S. Goodwin</dc:creator>
    </item>
    <item>
      <title>A network and machine learning approach to detect Value Added Tax fraud</title>
      <link>https://arxiv.org/abs/2106.14005</link>
      <description>arXiv:2106.14005v3 Announce Type: replace-cross 
Abstract: Value Added Tax (VAT) fraud erodes public revenue and puts legitimate businesses at a disadvantaged position thereby impacting inequality. Identifying and combating VAT fraud before it occurs is therefore important for welfare. This paper proposes flexible machine learning algorithms which detect fraudulent transactions, utilising the information provided by the complex VAT network structure of a large dimension. VAT fraud detection is implemented through a combination of a suitably constructed Laplacian matrix with classification algorithms that rely on scalable machine learning techniques. The method is implemented on the universe of Bulgarian VAT data and detects around 50 percent of the VAT fraud, outperforming well-known techniques that ignore the information provided by the network of VAT transactions. Importantly, the proposed methods are automated, and can be implemented following the taxpayers submission of their VAT returns. This allows tax revenue authorities to prevent large losses of tax revenues through performing early identification of fraud between business-to-business transactions within the VAT system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.14005v3</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angelos Alexopoulos, Petros Dellaportas, Stanley Gyoshev, Christos Kotsogiannis, Sofia C. Olhede, Trifon Pavkov</dc:creator>
    </item>
    <item>
      <title>Detecting and Understanding the Difference between Natural Mediation Effects and Their Randomized Interventional Analogues</title>
      <link>https://arxiv.org/abs/2407.02671</link>
      <description>arXiv:2407.02671v2 Announce Type: replace-cross 
Abstract: In causal mediation analysis, the natural direct and indirect effects (natural effects) are nonparametrically unidentifiable in the presence of treatment-induced confounding, which motivated the development of randomized interventional analogues (RIAs) of the natural effects. Being easier to identify, the RIAs are becoming widely used in practice. However, applied researchers often interpret RIA estimates as if they were the natural effects, even though the RIAs can be poor proxies for the natural effects. This calls for practical and theoretical guidance on when the RIAs differ from or coincide with the natural effects. We develop the first empirical test to detect the divergence between the natural effects and their RIAs under the weak assumptions sufficient for identifying the RIAs and illustrate the test using the Moving to Opportunity Study. We also provide new theoretical insights on the relationship between the natural effects and the RIAs both using a covariance formulation and from a structural equation perspective. This analysis also reveals previously undocumented connections between the natural effects, the RIAs, and estimands in instrumental variable analysis and Wilcoxon-Mann-Whitney tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02671v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ang Yu, Li Ge, Felix Elwert</dc:creator>
    </item>
    <item>
      <title>Survey Data Integration for Distribution Function Estimation</title>
      <link>https://arxiv.org/abs/2409.14284</link>
      <description>arXiv:2409.14284v2 Announce Type: replace-cross 
Abstract: Integration of probabilistic and non-probabilistic samples for the estimation of finite population totals (or means) has recently received considerable attention in the field of survey sampling; yet, to the best of our knowledge, this framework has not been extended to cumulative distribution function (CDF) estimation. To address this gap, we propose a novel CDF estimator that integrates data from probability samples with data from, potentially big, nonprobability samples. Assuming that a set of shared covariates are observed in both, while the response variable is observed only in the latter, the proposed estimator uses a survey-weighted empirical CDF of regression residuals trained on the convenience sample to estimate the CDF of the response variable. Under some assumptions, we derive the asymptotic bias and variance of our CDF estimator and show that it is asymptotically unbiased for the finite population CDF if ignorability holds. Our empirical results imply that the proposed CDF estimator is robust to model misspecification under ignorability, and robust to ignorability under model misspecification; when both assumptions are violated, our residual-based CDF estimator still outperforms its `plug-in' mass imputation and naive siblings, albeit with noted decreases in efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14284v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Flood, Sayed Mostafa</dc:creator>
    </item>
    <item>
      <title>Assessing the Optimistic Bias in the Natural Inflow Forecasts: A Call for Model Monitoring in Brazil</title>
      <link>https://arxiv.org/abs/2410.13763</link>
      <description>arXiv:2410.13763v4 Announce Type: replace-cross 
Abstract: Hydroelectricity accounted for roughly 61.4% of Brazil's total generation in 2024 and addressed most of the intermittency of wind and solar generation. Thus, inflow forecasting plays a critical role in the operation, planning, and market in this country, as well as in any other hydro-dependent power system. These forecasts influence generation schedules, reservoir management, and market pricing, shaping the dynamics of the entire electricity sector. The objective of this paper is to measure and present empirical evidence of a systematic optimistic bias in the official inflow forecast methodology, which is based on the PAR(p)-A model. Additionally, we discuss possible sources of this bias and recommend ways to mitigate it. By analyzing 14 years of historical data from the Brazilian system through rolling-window multistep (out-of-sample) forecasts, results indicate that the official forecast model exhibits statistically significant biases of 1.28, 3.83, 5.39, and 6.73 average GW for 1-, 6-, 12-, and 24-step-ahead forecasts in the Southeast subsystem, and 0.54, 1.66, 2.32, and 3.17 average GW in the Northeast subsystem. These findings uncover the limitations of current inflow forecasting methodologies used in Brazil and call for new governance and monitoring policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13763v4</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arthur Brigatto, Alexandre Street, Cristiano Fernandes, Davi Valladao, Guilherme Bodin, Joaquim Dias Garcia</dc:creator>
    </item>
    <item>
      <title>Overcoming data challenges through enriched validation and targeted sampling to measure whole-person health in electronic health records</title>
      <link>https://arxiv.org/abs/2502.05380</link>
      <description>arXiv:2502.05380v4 Announce Type: replace-cross 
Abstract: The allostatic load index (ALI) is a 10-component measure of whole-person health. Data from electronic health records (EHR) present a huge opportunity to operationalize the ALI in learning health systems; however, these data are prone to missingness and errors. Validation (e.g., through chart reviews) provides better-quality data, but realistically, only a subset of patients' data can be validated, and most protocols do not recover missing data. Using a representative sample of 1000 patients from the EHR at an extensive learning health system (100 of whom could be validated), we propose methods to design, conduct, and analyze statistically efficient and robust studies of ALI and healthcare utilization. Employing semiparametric maximum likelihood estimation, we robustly incorporate all available patient information into statistical models. Using targeted design strategies, we examine ways to select the most informative patients for validation. Incorporating clinical expertise, we devise a novel validation protocol to promote EHR data quality and completeness. Chart reviews uncovered few errors (99% matched source documents) and recovered some missing data through auxiliary information in patients' charts. On average, validation increased the number of non-missing ALI components per patient from 6 to 7. Through simulations based on preliminary data, residual sampling was identified as the most informative strategy for completing our validation study. Incorporating validation data, statistical models indicated that worse whole-person health (higher ALI) was associated with higher odds of engaging in the healthcare system, adjusting for age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05380v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Sheetal Kedar, Rabeya Tahir, Aidan D. Keleghan, Amelia Miranda, Stephany N. Duda, Michael P. Bancks, Brian J. Wells, Ashish K. Khanna, Joseph Rigdon</dc:creator>
    </item>
    <item>
      <title>Assessing the Impact of Covariate Distribution and Positivity Violation on Weighting-Based Indirect Comparisons: a Simulation Study</title>
      <link>https://arxiv.org/abs/2507.12241</link>
      <description>arXiv:2507.12241v2 Announce Type: replace-cross 
Abstract: Population-Adjusted Indirect Comparisons (PAICs) are used to estimate treatment effects when direct comparisons are infeasible and individual patient data (IPD) are only available for one trial. Among PAIC methods, Matching-Adjusted Indirect Comparison (MAIC) is the most widely used. However, little is known about how MAIC performs under challenging conditions such as limited covariate overlap or markedly non-normal covariate distributions.
  We conducted a Monte Carlo simulation study comparing three estimators: (i) MAIC matching first moment (MAIC-1), (ii) MAIC matching first and second moments (MAIC-2), and (iii) a benchmark method leveraging full IPD -- Propensity Score Weighting (PSW). We examined eight scenarios ranging from ideal conditions to situations with positivity violations and non-normal (including bimodal) covariate distributions. We assessed both anchored and unanchored estimators and examined the impact of adjustment model misspecification. We also applied these estimators to real-world data from the AKIKI and AKIKI-2 trials, comparing renal replacement therapy strategies in critically ill patients.
  MAIC-1 demonstrated robust performance, remaining unbiased in the presence of moderate positivity violations and non-normal covariates, while MAIC-2 and PSW appeared more sensitive to positivity violations. All methods showed substantial bias when key confounders were omitted, emphasizing the importance of correct model specification. In real-world data, a consistent trend was found with MAIC-1 showing narrower confidence intervals with positivity violation.
  Our findings support the cautious use of unanchored MAICs and highlight MAIC-1's resilience across moderate violations of assumptions. However, the method's limited flexibility underscores the need for careful use in real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12241v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnaud Serret-Larmande, J\'er\^ome Lambert, St\'ephane Gaudry, David Hajage</dc:creator>
    </item>
  </channel>
</rss>
