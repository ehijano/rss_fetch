<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Aug 2025 04:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Mass conservation analysis of extrusion-based 3D printing simulations based on the level-set method</title>
      <link>https://arxiv.org/abs/2508.20617</link>
      <description>arXiv:2508.20617v1 Announce Type: new 
Abstract: Numerical simulations of extrusion-based printing require tracking evolving material bound- aries, a challenging task due to possible topological changes and mass conservation issues. Inaccurate conservation of mass can lead to a mismatch between the extruded and simulated shapes, and generally to unreliable predictions of the actual ink behavior. This work investigates the mass conservation properties of the conservative level-set method in extrusion-based 3D printing applications. We analyze the effects of the level set parameters on the accuracy of mass conservation using the cross-sectional area of the deposited strand. We compare the cross- sectional areas obtained in the simulation with the ideal areas obtained from a mass balance when the system reaches a steady-state condition. The numerical results indicate that reducing the reinitialization and the interface thickness parameters decreases the errors in the cross-sectional area obtained. However, the reductions in error tend to decline and could lead to excessive computational cost. Furthermore, we also found that the typical strong mesh requirements can be lessened by selecting an adequate interface thickness. Finally, we obtained the cross-sectional areas from simulations with different printing settings and found that they show good agreement with the simulated and experimental data published in previous work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20617v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlos J. G. Rojas, C. A. G\'omez-P\'erez, Leyla \"Ozkan</dc:creator>
    </item>
    <item>
      <title>Can News Predict the Direction of Oil Price Volatility? A Language Model Approach with SHAP Explanations</title>
      <link>https://arxiv.org/abs/2508.20707</link>
      <description>arXiv:2508.20707v1 Announce Type: new 
Abstract: Financial markets can be highly sensitive to news, investor sentiment, and economic indicators, leading to important asset price fluctuations. In this study we focus on crude oil, due to its crucial role in commodity markets and the global economy. Specifically, we are interested in understanding the directional changes of oil price volatility, and for this purpose we investigate whether news alone -- without incorporating traditional market data -- can effectively predict the direction of oil price movements. Using a decade-long dataset from Eikon (2014-2024), we develop an ensemble learning framework to extract predictive signals from financial news. Our approach leverages diverse sentiment analysis techniques and modern language models, including FastText, FinBERT, Gemini, and LLaMA, to capture market sentiment and textual patterns. We benchmark our model against the Heterogeneous Autoregressive (HAR) model and assess statistical significance using the McNemar test. While most sentiment-based indicators do not consistently outperform HAR, the raw news count emerges as a robust predictor. Among embedding techniques, FastText proves most effective for forecasting directional movements. Furthermore, SHAP-based interpretation at the word level reveals evolving predictive drivers across market regimes: pre-pandemic emphasis on supply-demand and economic terms; early pandemic focus on uncertainty and macroeconomic instability; post-shock attention to long-term recovery indicators; and war-period sensitivity to geopolitical and regional oil market disruptions. These findings highlight the predictive power of news-driven features and the value of explainable NLP in financial forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20707v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romina Hashamia, Felipe Maldonado</dc:creator>
    </item>
    <item>
      <title>Self-consistent clustering analysis for homogenisation of heterogeneous plates</title>
      <link>https://arxiv.org/abs/2508.20446</link>
      <description>arXiv:2508.20446v1 Announce Type: cross 
Abstract: This work introduces a reduced-order model for plate structures with periodic micro-structures by coupling self-consistent clustering analysis (SCA) with the Lippmann-Schwinger equation, enabling rapid multiscale homogenisation of heterogeneous plates. A plate-specific SCA scheme is derived for the first time and features two key elements: (i) an offline-online strategy that combines Green's functions with k-means data compression, and (ii) an online self-consistent update that exploits the weak sensitivity of the reference medium. The framework handles both linear and nonlinear problems in classical plate theory and first-order shear deformation theory, and its performance is verified on linear isotropic perforated plates and woven composites, as well as on non-linear elasto-plastic perforated plates and woven composites with damage. Across all cases the proposed model matches the accuracy of FFT-based direct numerical simulation while reducing computational cost by over an order of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20446v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.CE</category>
      <category>physics.app-ph</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Menglei Li, Haolin Li, Bing Wang, Bing Wang</dc:creator>
    </item>
    <item>
      <title>The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic Framework for Ordinal State Estimation</title>
      <link>https://arxiv.org/abs/2508.20806</link>
      <description>arXiv:2508.20806v1 Announce Type: cross 
Abstract: Traditional state estimation methods rely on probabilistic assumptions that often collapse epistemic uncertainty into scalar beliefs, risking overconfidence in sparse or adversarial sensing environments. We introduce the Epistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework fully grounded in possibility theory and epistemic humility. ESPF redefines the evolution of belief over state space using compatibility-weighted support updates, surprisalaware pruning, and adaptive dispersion via sparse grid quadrature. Unlike conventional filters, ESPF does not seek a posterior distribution, but rather maintains a structured region of plausibility or non-rejection, updated using ordinal logic rather than integration. For multi-model inference, we employ the Choquet integral to fuse competing hypotheses based on a dynamic epistemic capacity function, generalizing classical winner-take-all strategies. The result is an inference engine capable of dynamically contracting or expanding belief support in direct response to information structure, without requiring prior statistical calibration. This work presents a foundational shift in how inference, evidence, and ignorance are reconciled, supporting robust estimation where priors are unavailable, misleading, or epistemically unjustified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20806v1</guid>
      <category>cs.IT</category>
      <category>cs.CE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.DS</category>
      <category>math.IT</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moriba Jah, Van Haslett</dc:creator>
    </item>
    <item>
      <title>LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources</title>
      <link>https://arxiv.org/abs/2508.00654</link>
      <description>arXiv:2508.00654v2 Announce Type: replace 
Abstract: In the interdisciplinary field of microscopy research, managing and integrating large volumes of data stored across disparate platforms remains a major challenge. Data types such as bioimages, experimental records, and spectral information are often maintained in separate repositories, each following different management standards. However, linking these data sources across the research lifecycle is essential to align with the FAIR principles of data management: Findability, Accessibility, Interoperability, and Reusability. Despite this need, there is a notable lack of tools capable of effectively integrating and linking data from heterogeneous sources. To address this gap, we present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based platform designed to create and manage links between distributed data systems. LEO was initially developed to link objects between Electronic Lab Notebooks (ELNs) and OMERO, but its functionality has since been extended through a plugin-based architecture, allowing the integration of additional data sources. This extensibility makes LEO a scalable and flexible solution for a wide range of microscopy research workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00654v2</guid>
      <category>cs.CE</category>
      <category>cs.SE</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rodrigo Escobar D\'iaz Guerrero, Jamile Mohammad Jafari, Tobias Meyer-Zedler, Michael Schmitt, Juergen Popp, Thomas Bocklitz</dc:creator>
    </item>
    <item>
      <title>Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha Mining in Quantitative Trading</title>
      <link>https://arxiv.org/abs/2508.06312</link>
      <description>arXiv:2508.06312v2 Announce Type: replace 
Abstract: Alpha factor mining is a fundamental task in quantitative trading, aimed at discovering interpretable signals that can predict asset returns beyond systematic market risk. While traditional methods rely on manual formula design or heuristic search with machine learning, recent advances have leveraged Large Language Models (LLMs) for automated factor discovery. However, existing LLM-based alpha mining approaches remain limited in terms of automation, generality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel, simple, yet effective and efficient LLM-based framework for fully automated formulaic alpha mining. Our method features a dual-chain architecture, consisting of a Factor Generation Chain and a Factor Optimization Chain, which iteratively generate, evaluate, and refine candidate alpha factors using only market data, while leveraging backtest feedback and prior optimization knowledge. The two chains work synergistically to enable high-quality alpha discovery without human intervention and offer strong scalability. Extensive experiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha outperforms existing baselines across multiple metrics, presenting a promising direction for LLM-driven quantitative research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06312v2</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lang Cao, Zekun Xi, Long Liao, Ziwei Yang, Zheng Cao</dc:creator>
    </item>
    <item>
      <title>Scale-invariant Monte Carlo and multilevel Monte Carlo estimation of mean and variance: An application to simulation of linear elastic bone tissue</title>
      <link>https://arxiv.org/abs/2106.13723</link>
      <description>arXiv:2106.13723v3 Announce Type: replace-cross 
Abstract: We propose novel scale-invariant error estimators for the Monte Carlo and multilevel Monte Carlo estimation of mean and variance. For any linear transformation of the distribution of the quantity of interest, the computation cost across grid levels is optimized using a normalized error estimate, which is not only fully dimensionless but also remains robust to variation in characteristics of the distribution. We demonstrate the effectiveness of the algorithms through application to a mechanical simulation of linear elastic bone tissue, where material uncertainty incorporating both heterogeneity and random anisotropy is considered in the constitutive law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.13723v3</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sharana Kumar Shivanand, Bojana Rosi\'c</dc:creator>
    </item>
    <item>
      <title>Faster Random Walk-based Capacitance Extraction with Generalized Antithetic Sampling</title>
      <link>https://arxiv.org/abs/2504.20586</link>
      <description>arXiv:2504.20586v2 Announce Type: replace-cross 
Abstract: Floating random walk-based capacitance extraction has emerged in recent years as a tried and true approach for extracting parasitic capacitance in very large scale integrated circuits. Being a Monte Carlo method, its performance is dependent on the variance of sampled quantities and variance reduction methods are crucial for the challenges posed by ever denser process technologies and layout-dependent effects. In this work, we present a novel, universal variance reduction method for floating random walk-based capacitance extraction, which is conceptually simple, highly efficient and provably reduces variance in all extractions, especially when layout-dependent effects are present. It is complementary to existing mathematical formulations for variance reduction and its performance gains are experienced on top of theirs. Numerical experiments demonstrate substantial such gains of up to 30% in number of walks necessary and even more in actual extraction times compared to the best previously proposed variance reduction approaches for the floating random-walk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20586v2</guid>
      <category>physics.comp-ph</category>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Periklis Liaskovitis, Marios Visvardis, Efthymios Efstathiou</dc:creator>
    </item>
  </channel>
</rss>
