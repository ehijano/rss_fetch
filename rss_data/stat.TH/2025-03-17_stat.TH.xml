<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Mar 2025 04:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Kolmogorov-Smirnov Statistic Revisited</title>
      <link>https://arxiv.org/abs/2503.11673</link>
      <description>arXiv:2503.11673v1 Announce Type: new 
Abstract: The Kolmogorov-Smirnov (KS) statistic is a classical nonparametric test widely used for comparing an empirical distribution function with a reference distribution or for comparing two empirical distributions. Despite its broad applicability in statistical hypothesis testing and model validation, certain aspects of the KS statistic remain under-explored among the young generation, particularly under finite sample conditions. This paper revisits the KS statistic in both one-sample and two-sample scenarios, considering one-sided and two-sided variants. We derive exact probabilities for the supremum of the empirical process and present a unified treatment of the KS statistic under diverse settings. Additionally, we explore the discrete nature of the hitting times of the normalized empirical process, providing practical insights into the computation of KS test p-values. The study also discusses the Dvoretzky-Kiefer-Wolfowitz-Massart (DKWM) inequality, highlighting its role in constructing confidence bands for distribution functions. Using empirical process theory, we establish the limit distribution of the KS statistic when the true distribution includes unknown parameters. Our findings extend existing results, offering improved methodologies for statistical analysis and hypothesis testing using the KS statistic, particularly in finite sample scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11673v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elvis Han Cui, Yihao Li, Zhuang Liu</dc:creator>
    </item>
    <item>
      <title>Excess Mean Squared Error of Empirical Bayes Estimators</title>
      <link>https://arxiv.org/abs/2503.11863</link>
      <description>arXiv:2503.11863v1 Announce Type: new 
Abstract: Empirical Bayes estimators are based on minimizing the average risk with the hyper-parameters in the weighting function being estimated from observed data. The performance of an empirical Bayes estimator is typically evaluated by its mean squared error (MSE). However, the explicit expression for its MSE is generally unavailable for finite sample sizes. To address this issue, we define a high-order analytical criterion: the excess MSE. It quantifies the performance difference between the maximum likelihood and empirical Bayes estimators. An explicit expression for the excess MSE of an empirical Bayes estimator employing a general data-dependent hyper-parameter estimator is derived. As specific instances, we provide excess MSE expressions for kernel-based regularized estimators using the scaled empirical Bayes, Stein unbiased risk estimation, and generalized cross-validation hyper-parameter estimators. Moreover, we propose a modification to the excess MSE expressions for regularized estimators for moderate sample sizes and show its improvement on accuracy in numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11863v1</guid>
      <category>math.ST</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Ju, Bo Wahlberg, H\r{a}kan Hjalmarsson</dc:creator>
    </item>
    <item>
      <title>Two statistical problems for multivariate mixture distributions</title>
      <link>https://arxiv.org/abs/2503.12147</link>
      <description>arXiv:2503.12147v1 Announce Type: new 
Abstract: In an earlier work arXiv:2410.22038, it was shown that mixtures of multivariate Gaussian or $t$-distributions can be distinguished by projecting them onto a certain predetermined finite set of lines, the number of lines depending only on the total number of distributions involved and on the ambient dimension. Using this work, we address the following two important statistical problems: that of testing and measuring the agreement between two different random partitions, and that of estimating for mixtures of multivariate normal distributions and mixtures of $t$-distributions based of univariate projections. We also compare our proposal with robust versions of the expectation-maximization method EM. In each case, we present algorithms for effecting the task, and compare them with existing methods by carrying out some simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12147v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Fraiman, Leonardo Moreno, Thomas Ransford</dc:creator>
    </item>
    <item>
      <title>Optimal ANOVA-based emulators of models with(out) derivatives</title>
      <link>https://arxiv.org/abs/2503.12151</link>
      <description>arXiv:2503.12151v1 Announce Type: new 
Abstract: This paper proposes new ANOVA-based approximations of functions and emulators of high-dimensional models using either available derivatives or local stochastic evaluations of such models. Our approach makes use of sensitivity indices to design adequate structures of emulators. For high-dimensional models with available derivatives, our derivative-based emulators reach dimension-free mean squared errors (MSEs) and parametric rate of convergence (i.e., $\mathsf{O}(N^{-1})$). This approach is extended to cope with every model (without available derivatives) by deriving global emulators that account for the local properties of models or simulators. Such generic emulators enjoy dimension-free biases, parametric rates of convergence and MSEs that depend on the dimensionality. Dimension-free MSEs are obtained for high-dimensional models with particular inputs' distributions. Our emulators are also competitive in dealing with different distributions of the input variables and for selecting inputs and interactions. Simulations show the efficiency of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12151v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matieyendou Lamboni</dc:creator>
    </item>
    <item>
      <title>On a conjecture of Roverato regarding G-Wishart normalising constants</title>
      <link>https://arxiv.org/abs/2503.13046</link>
      <description>arXiv:2503.13046v1 Announce Type: new 
Abstract: The evaluation of G-Wishart normalising constants is a core component for Bayesian analyses for Gaussian graphical models, but remains a computationally intensive task in general. Based on empirical evidence, Roverato [Scandinavian Journal of Statistics, 29:391--411 (2002)] observed and conjectured that such constants can be simplified and rewritten in terms of constants with an identity scale matrix. In this note, we disprove this conjecture for general graphs by showing that the conjecture instead implies an independently-derived approximation for certain ratios of normalising constants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13046v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ching Wong, Giusi Moffa, Jack Kuipers</dc:creator>
    </item>
    <item>
      <title>Stein's method of moment estimators for local dependency exponential random graph models</title>
      <link>https://arxiv.org/abs/2503.13191</link>
      <description>arXiv:2503.13191v1 Announce Type: new 
Abstract: Providing theoretical guarantees for parameter estimation in exponential random graph models is a largely open problem. While maximum likelihood estimation has theoretical guarantees in principle, verifying the assumptions for these guarantees to hold can be very difficult. Moreover, in complex networks, numerical maximum likelihood estimation is computer-intensive and may not converge in reasonable time. To ameliorate this issue, local dependency exponential random graph models have been introduced, which assume that the network consists of many independent exponential random graphs. In this setting, progress towards maximum likelihood estimation has been made. However the estimation is still computer-intensive. Instead, we propose to use so-called Stein estimators: we use the Stein characterizations to obtain new estimators for local dependency exponential random graph models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13191v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrian Fischer, Gesine Reinert, Wenkai Xu</dc:creator>
    </item>
    <item>
      <title>Proposal for the Application of Fractional Operators in Polynomial Regression Models to Enhance the Determination Coefficient $R^2$ on Unseen Data</title>
      <link>https://arxiv.org/abs/2503.11749</link>
      <description>arXiv:2503.11749v1 Announce Type: cross 
Abstract: Since polynomial regression models are generally quite reliable for data with a linear trend, it is important to note that, in some cases, they may encounter overfitting issues during the training phase, which could result in negative values of the coefficient of determination $R^2$ for unseen data. For this reason, this work proposes the partial implementation of fractional operators in polynomial regression models to generate a fractional regression model. The goal of this proposal is to attempt to mitigate overfitting, which could improve the value of the coefficient of determination for unseen data, compared to the polynomial model, under the assumption that this would contribute to generating predictive models with better performance. The methodology for constructing these fractional regression models is detailed, and examples applicable to both Riemann-Liouville and Caputo fractional operators are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11749v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anthony Torres-Hernandez</dc:creator>
    </item>
    <item>
      <title>Training Diagonal Linear Networks with Stochastic Sharpness-Aware Minimization</title>
      <link>https://arxiv.org/abs/2503.11891</link>
      <description>arXiv:2503.11891v1 Announce Type: cross 
Abstract: We analyze the landscape and training dynamics of diagonal linear networks in a linear regression task, with the network parameters being perturbed by small isotropic normal noise. The addition of such noise may be interpreted as a stochastic form of sharpness-aware minimization (SAM) and we prove several results that relate its action on the underlying landscape and training dynamics to the sharpness of the loss. In particular, the noise changes the expected gradient to force balancing of the weight matrices at a fast rate along the descent trajectory. In the diagonal linear model, we show that this equates to minimizing the average sharpness, as well as the trace of the Hessian matrix, among all possible factorizations of the same matrix. Further, the noise forces the gradient descent iterates towards a shrinkage-thresholding of the underlying true parameter, with the noise level explicitly regulating both the shrinkage factor and the threshold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11891v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gabriel Clara, Sophie Langer, Johannes Schmidt-Hieber</dc:creator>
    </item>
    <item>
      <title>On self-training of summary data with genetic applications</title>
      <link>https://arxiv.org/abs/2503.12155</link>
      <description>arXiv:2503.12155v1 Announce Type: cross 
Abstract: Prediction model training is often hindered by limited access to individual-level data due to privacy concerns and logistical challenges, particularly in biomedical research. Resampling-based self-training presents a promising approach for building prediction models using only summary-level data. These methods leverage summary statistics to sample pseudo datasets for model training and parameter optimization, allowing for model development without individual-level data. Although increasingly used in precision medicine, the general behaviors of self-training remain unexplored. In this paper, we leverage a random matrix theory framework to establish the statistical properties of self-training algorithms for high-dimensional sparsity-free summary data. We demonstrate that, within a class of linear estimators, resampling-based self-training achieves the same asymptotic predictive accuracy as conventional training methods that require individual-level datasets. These results suggest that self-training with only summary data incurs no additional cost in prediction accuracy, while offering significant practical convenience. Our analysis provides several valuable insights and counterintuitive findings. For example, while pseudo-training and validation datasets are inherently dependent, their interdependence unexpectedly cancels out when calculating prediction accuracy measures, preventing overfitting in self-training algorithms. Furthermore, we extend our analysis to show that the self-training framework maintains this no-cost advantage when combining multiple methods or when jointly training on data from different distributions. We numerically validate our findings through simulations and real data analyses using the UK Biobank. Our study highlights the potential of resampling-based self-training to advance genetic risk prediction and other fields that make summary data publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12155v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Buxin Su, Jiaoyang Huang, Jin Jin, Bingxin Zhao</dc:creator>
    </item>
    <item>
      <title>Asymptotic Expansions of Gaussian and Laguerre Ensembles at the Soft Edge II: Level Densities</title>
      <link>https://arxiv.org/abs/2503.12644</link>
      <description>arXiv:2503.12644v1 Announce Type: cross 
Abstract: We continue our work [arXiv:2403.07628] on asymptotic expansions at the soft edge for the classical $n$-dimensional Gaussian and Laguerre random matrix ensembles. By revisiting the construction of the associated skew-orthogonal polynomials in terms of wave functions, we obtain concise expressions for the level densities that are well suited for proving asymptotic expansions in powers of a certain parameter $h \asymp n^{-2/3}$. In the unitary case, the expansion for the level density can be used to reconstruct the first correction term in an established asymptotic expansion of the associated generating function. In the orthogonal and symplectic cases, we can even reconstruct the conjectured first and second correction terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12644v1</guid>
      <category>math.PR</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Folkmar Bornemann</dc:creator>
    </item>
    <item>
      <title>Estimating stationary mass, frequency by frequency</title>
      <link>https://arxiv.org/abs/2503.12808</link>
      <description>arXiv:2503.12808v1 Announce Type: cross 
Abstract: Suppose we observe a trajectory of length $n$ from an $\alpha$-mixing stochastic process over a finite but potentially large state space. We consider the problem of estimating the probability mass placed by the stationary distribution of any such process on elements that occur with a certain frequency in the observed sequence. We estimate this vector of probabilities in total variation distance, showing universal consistency in $n$ and recovering known results for i.i.d. sequences as special cases. Our proposed methodology carefully combines the plug-in (or empirical) estimator with a recently-proposed modification of the Good--Turing estimator called \textsc{WingIt}, which was originally developed for Markovian sequences. En route to controlling the error of our estimator, we develop new performance bounds on \textsc{WingIt} and the plug-in estimator for $\alpha$-mixing stochastic processes. Importantly, the extensively used method of Poissonization can no longer be applied in our non i.i.d. setting, and so we develop complementary tools -- including concentration inequalities for a natural self-normalized statistic of mixing sequences -- that may prove independently useful in the design and analysis of estimators for related problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12808v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milind Nakul, Vidya Muthukumar, Ashwin Pananjady</dc:creator>
    </item>
    <item>
      <title>Parameter estimation for generalized mixed fractional stochastic heat equation</title>
      <link>https://arxiv.org/abs/2503.12863</link>
      <description>arXiv:2503.12863v1 Announce Type: cross 
Abstract: We study the properties of a stochastic heat equation with a generalized mixed fractional Brownian noise. We obtain the covariance structure, stationarity and obtain bounds for the asymptotic behaviour of the solution. We suggest estimators for the unknown parameters based on discrete time observations and study their asymptotic properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12863v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>B. L. S. Prakasa Rao</dc:creator>
    </item>
    <item>
      <title>Spearman's rho for bivariate zero-inflated data</title>
      <link>https://arxiv.org/abs/2503.13148</link>
      <description>arXiv:2503.13148v1 Announce Type: cross 
Abstract: Quantifying the association between two random variables is crucial in applications. Traditional estimation techniques for common association measures, such as Spearman's rank correlation coefficient, $\rho_S$, often fail when data contain ties. This is particularly problematic in zero-inflated contexts and fields like insurance, healthcare, and weather forecasting, where zeros are more frequent and require an extra probability mass. In this paper, we provide a new formulation of Spearman's rho specifically designed for zero-inflated data and propose a novel estimator of Spearman's rho based on our derived expression. Besides, we make our proposed estimator useful in practice by deriving its achievable bounds and suggest how to estimate them. We analyze our method in a comprehensive simulation study and show that our approach overcomes state-of-the-art methods in all the simulated scenarios. Additionally, we illustrate how the proposed theory can be used in practice for a more accurate quantification of association by considering two real-life applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13148v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jasper Arends, Elisa Perrone</dc:creator>
    </item>
    <item>
      <title>Multi-category Angle-based Classifier Refit</title>
      <link>https://arxiv.org/abs/1607.05709</link>
      <description>arXiv:1607.05709v2 Announce Type: replace 
Abstract: Classification is an important statistical learning tool. In real application, besides high prediction accuracy, it is often desirable to estimate class conditional probabilities for new observations. For traditional problems where the number of observations is large, there exist many well developed approaches. Recently, high dimensional low sample size problems are becoming increasingly popular. Margin-based classifiers, such as logistic regression, are well established methods in the literature. On the other hand, in terms of probability estimation, it is known that for binary classifiers, the commonly used methods tend to under-estimate the norm of the classification function. This can lead to biased probability estimation. Remedy approaches have been proposed in the literature. However, for the simultaneous multicategory classification framework, much less work has been done. We fill the gap in this paper. In particular, we give theoretical insights on why heavy regularization terms are often needed in high dimensional applications, and how this can lead to bias in probability estimation. To overcome this difficulty, we propose a new refit strategy for multicategory angle-based classifiers. Our new method only adds a small computation cost to the problem, and is able to attain prediction accuracy that is as good as the regular margin-based classifiers. On the other hand, the improvement of probability estimation can be very significant. Numerical results suggest that the new refit approach is highly competitive.</description>
      <guid isPermaLink="false">oai:arXiv.org:1607.05709v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guo Xian Yau, Chong Zhang</dc:creator>
    </item>
    <item>
      <title>Exponential Inequalities for Some Mixing Processes and Dynamic Systems</title>
      <link>https://arxiv.org/abs/2208.11481</link>
      <description>arXiv:2208.11481v4 Announce Type: replace 
Abstract: Many important dynamic systems, time series models or even algorithms exhibit non-strong mixing properties. In this paper, we introduce the general concept of $\mathcal{C}_{p,\mathcal{F}}$-mixing to cover such cases, where assumptions on the dependence structure become stronger with increasing $p\in [1, \infty].$ We derive a series of sharp exponential-type (or Bernstein-type) inequalities under this dependence concept for $p=1$ and $p=\infty$.
  More specifically, $\mathcal{C}_{\infty,\mathcal{F}}$-mixing is equal to the widely discussed $\mathcal{C}$-mixing \citep{maume2006exponential}, and we prove a refinement of an Berntsein-type inequality in \cite{hang2017bernstein} for $\mathcal{C}$-mixing processes under more general assumptions. As there exist many stochastic processes and dynamic systems, which are not $\mathcal{C}$ (or $\mathcal{C}_{\infty,\mathcal{F}}$)-mixing, we derive Bernstein-type inequalities for $\mathcal{C}_{1,\mathcal{F}}$-mixing processes as well and we use this result to investigate the convergence rates of plug-in-type estimators of the local conditional mode set for vector-valued output, in particular in situations where the density is less smooth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.11481v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Yuan, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Regularization Using Synthetic Data in High-Dimensional Models</title>
      <link>https://arxiv.org/abs/2407.04194</link>
      <description>arXiv:2407.04194v4 Announce Type: replace 
Abstract: To address the challenges of reliable statistical inference in high-dimensional models, we introduce the Synthetic-data Regularized Estimator (SRE). Unlike traditional regularization methods, the SRE regularizes the complex target model via a weighted likelihood based on synthetic data generated from a simpler, more stable model. This method provides a theoretically sound and practically effective alternative to parameter penalization. We establish key theoretical properties of the SRE in generalized linear models, including existence, stability, consistency, and minimax rate optimality. Applying the Convex Gaussian Min-Max Theorem, we derive a precise asymptotic characterization in the high-dimensional linear regime. To deal with the non-separable regularization, we introduce a novel decomposition in our analysis. Building upon these results, we develop practical methodologies for tuning parameter selection, confidence interval construction, and calibrated variable selection in high-dimensional inference. The effectiveness of the SRE is demonstrated through simulation studies and real-data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04194v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weihao Li, Dongming Huang</dc:creator>
    </item>
    <item>
      <title>Sampling from the Random Linear Model via Stochastic Localization Up to the AMP Threshold</title>
      <link>https://arxiv.org/abs/2407.10763</link>
      <description>arXiv:2407.10763v2 Announce Type: replace 
Abstract: Recently, Approximate Message Passing (AMP) has been integrated with stochastic localization (diffusion model) by providing a computationally efficient estimator of the posterior mean. Existing (rigorous) analysis typically proves the success of sampling for sufficiently small noise, but determining the exact threshold involves several challenges. In this paper, we focus on sampling from the posterior in the linear inverse problem, with an i.i.d. random design matrix, and show that the threshold for sampling coincides with that of posterior mean estimation. We give a proof for the convergence in smoothed KL divergence whenever the noise variance $\Delta$ is below $\Delta_{\rm AMP}$, which is the computation threshold for mean estimation introduced in (Barbier et al., 2020). We also show convergence in the Wasserstein distance under the same threshold assuming a dimension-free bound on the operator norm of the posterior covariance matrix, a condition strongly suggested by recent breakthroughs on operator norm bounds in similar replica symmetric systems. A key observation in our analysis is that phase transition does not occur along the sampling and interpolation paths assuming $\Delta&lt;\Delta_{\rm AMP}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10763v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Cui, Zhiyuan Yu, Jingbo Liu</dc:creator>
    </item>
    <item>
      <title>Optimal Recovery Meets Minimax Estimation</title>
      <link>https://arxiv.org/abs/2502.17671</link>
      <description>arXiv:2502.17671v2 Announce Type: replace 
Abstract: A fundamental problem in statistics and machine learning is to estimate a function $f$ from possibly noisy observations of its point samples. The goal is to design a numerical algorithm to construct an approximation $\hat f$ to $f$ in a prescribed norm that asymptotically achieves the best possible error (as a function of the number $m$ of observations and the variance $\sigma^2$ of the noise). This problem has received considerable attention in both nonparametric statistics (noisy observations) and optimal recovery (noiseless observations). Quantitative bounds require assumptions on $f$, known as model class assumptions. Classical results assume that $f$ is in the unit ball of a Besov space. In nonparametric statistics, the best possible performance of an algorithm for finding $\hat f$ is known as the minimax rate and has been studied in this setting under the assumption that the noise is Gaussian. In optimal recovery, the best possible performance of an algorithm is known as the optimal recovery rate and has also been determined in this setting. While one would expect that the minimax rate recovers the optimal recovery rate when the noise level $\sigma$ tends to zero, it turns out that the current results on minimax rates do not carefully determine the dependence on $\sigma$ and the limit cannot be taken. This paper handles this issue and determines the noise-level-aware (NLA) minimax rates for Besov classes when error is measured in an $L_q$-norm with matching upper and lower bounds. The end result is a reconciliation between minimax rates and optimal recovery rates. The NLA minimax rate continuously depends on the noise level and recovers the optimal recovery rate when $\sigma$ tends to zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17671v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ronald DeVore, Robert D. Nowak, Rahul Parhi, Guergana Petrova, Jonathan W. Siegel</dc:creator>
    </item>
    <item>
      <title>On the Glivenko-Cantelli theorem for real-valued empirical functions of stationary $\alpha$-mixing and $\beta$-mixing sequences</title>
      <link>https://arxiv.org/abs/2502.20206</link>
      <description>arXiv:2502.20206v5 Announce Type: replace 
Abstract: In this paper we extend the classical Glivenko-Cantelli theorem to real-valued empirical functions under dependence structures characterised by $\alpha$-mixing and $\beta$-mixing conditions. We investigate sufficient conditions ensuring that families of real-valued functions exhibit the Glivenko-Cantelli (GC) property in these dependence settings. Our analysis focuses on function classes satisfying uniform entropy conditions and establishes deviation bounds under mixing coefficients that decay at appropriate rates. Our results refine the existing literature by relaxing the independence assumptions and highlighting the role of dependence in empirical process convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20206v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ousmane Coulibaly, Harouna Sangar\'e</dc:creator>
    </item>
    <item>
      <title>Pointwise Minimax Vector Field Reconstruction from Noisy ODE</title>
      <link>https://arxiv.org/abs/2503.08355</link>
      <description>arXiv:2503.08355v2 Announce Type: replace 
Abstract: This work addresses the problem of estimating a vector field from a noisy Ordinary Differential Equation (ODE) in a non-parametric regression setting with a random design for initial values. More specifically, given a vector field $ f:\mathbb{R}^{D}\rightarrow \mathbb{R}^{D}$ governing a dynamical system defined by the autonomous ODE: $y' = f(y)$, we assume that the observations are $\tilde{y}_{X_{i}}(t_{j}) = y_{X_{i}}(t_{j}) + \varepsilon_{i,j}$ where $y_{X_{i}}(t_{j})$ is the solution of the ODE at time $t_{j}$ with initial condition $y(0) = X_{i}$, $X_{i}$ is sampled from a probability distribution $\mu$, and $\varepsilon_{i,j}$ some noise. In this context, we investigate, from a minimax perspective, the pointwise reconstruction of $f$ within the envelope of trajectories originating from the support of $\mu$. We propose an estimation strategy based on preliminary flow reconstruction and techniques from derivative estimation in non-parametric regression. Under mild assumptions on $f$, we establish convergence rates that depend on the temporal resolution, the number of sampled initial values and the mass concentration of $\mu$. Importantly, we show that these rates are minimax optimal. Furthermore, we discuss the implications of our results in a manifold learning setting, providing insights into how our approach can mitigate the curse of dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08355v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Henneuse</dc:creator>
    </item>
    <item>
      <title>Gaussian and Non-Gaussian Universality of Data Augmentation</title>
      <link>https://arxiv.org/abs/2202.09134</link>
      <description>arXiv:2202.09134v4 Announce Type: replace-cross 
Abstract: We provide universality results that quantify how data augmentation affects the variance and limiting distribution of estimates through simple surrogates, and analyze several specific models in detail. The results confirm some observations made in machine learning practice, but also lead to unexpected findings: Data augmentation may increase rather than decrease the uncertainty of estimates, such as the empirical prediction risk. It can act as a regularizer, but fails to do so in certain high-dimensional problems, and it may shift the double-descent peak of an empirical risk. Overall, the analysis shows that several properties data augmentation has been attributed with are not either true or false, but rather depend on a combination of factors -- notably the data distribution, the properties of the estimator, and the interplay of sample size, number of augmentations, and dimension. As our main theoretical tool, we develop an adaptation of Lindeberg's technique for block dependence. The resulting universality regime may be Gaussian or non-Gaussian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.09134v4</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Han Huang, Peter Orbanz, Morgane Austern</dc:creator>
    </item>
    <item>
      <title>Unified Fourier bases for signals on random graphs with group symmetries</title>
      <link>https://arxiv.org/abs/2406.06306</link>
      <description>arXiv:2406.06306v2 Announce Type: replace-cross 
Abstract: We consider a recently proposed approach to graph signal processing (GSP) based on graphons. We show how the graphon-based approach to GSP applies to graphs sampled from a stochastic block model derived from a weighted Cayley graph. When SBM block sizes are equal, a nice Fourier basis can be derived from the representation theory of the underlying group. We explore how the SBM Fourier basis is affected when block sizes are not uniform. When block sizes are nearly uniform, we demonstrate that the group Fourier basis closely approximates the SBM Fourier basis. More specifically, we quantify the approximation error using matrix perturbation theory. When block sizes are highly non-uniform, the group-based Fourier basis can no longer be used. However, we show that partial information regarding the SBM Fourier basis can still be obtained from the underlying group.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06306v2</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahya Ghandehari, Jeannette Janssen, Silo Murphy</dc:creator>
    </item>
    <item>
      <title>BudgetIV: Optimal Partial Identification of Causal Effects with Mostly Invalid Instruments</title>
      <link>https://arxiv.org/abs/2411.06913</link>
      <description>arXiv:2411.06913v2 Announce Type: replace-cross 
Abstract: Instrumental variables (IVs) are widely used to estimate causal effects in the presence of unobserved confounding between exposure and outcome. An IV must affect the outcome exclusively through the exposure and be unconfounded with the outcome. We present a framework for relaxing either or both of these strong assumptions with tuneable and interpretable budget constraints. Our algorithm returns a feasible set of causal effects that can be identified exactly given relevant covariance parameters. The feasible set may be disconnected but is a finite union of convex subsets. We discuss conditions under which this set is sharp, i.e., contains all and only effects consistent with the background assumptions and the joint distribution of observable variables. Our method applies to a wide class of semiparametric models, and we demonstrate how its ability to select specific subsets of instruments confers an advantage over convex relaxations in both linear and nonlinear settings. We also adapt our algorithm to form confidence sets that are asymptotically valid under a common statistical assumption from the Mendelian randomization literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06913v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan Penn, Lee M. Gunderson, Gecia Bravo-Hermsdorff, Ricardo Silva, David S. Watson</dc:creator>
    </item>
    <item>
      <title>Rethinking the Win Ratio: A Causal Framework for Hierarchical Outcome Analysis</title>
      <link>https://arxiv.org/abs/2501.16933</link>
      <description>arXiv:2501.16933v2 Announce Type: replace-cross 
Abstract: Quantifying causal effects in the presence of complex and multivariate outcomes is a key challenge to evaluate treatment effects. For hierarchical multivarariates outcomes, the FDA recommends the Win Ratio and Generalized Pairwise Comparisons approaches. However, as far as we know, these empirical methods lack causal or statistical foundations to justify their broader use in recent studies. To address this gap, we establish causal foundations for hierarchical comparison methods. We define related causal effect measures, and highlight that depending on the methodology used to compute Win Ratios or Net Benefits of treatments, the causal estimand targeted can be different, as proved by our consistency results. Quite dramatically, it appears that the causal estimand related to the historical estimation approach can yield reversed and incorrect treatment recommendations in heterogeneous populations, as we illustrate through striking examples. In order to compensate for this fallacy, we introduce a novel, individual-level yet identifiable causal effect measure that better approximates the ideal, non-identifiable individual-level estimand. We prove that computing Win Ratio or Net Benefits using a Nearest Neighbor pairing approach between treated and controlled patients, an approach that can be seen as an extreme form of stratification, leads to estimating this new causal estimand measure. We extend our methods to observational settings via propensity weighting, distributional regression to address the curse of dimensionality, and a doubly robust framework. We prove the consistency of our methods, and the double robustness of our augmented estimator. Finally, we validate our approach using synthetic data and on CRASH-3, a major clinical trial focused on assessing the effects of tranexamic acid in patients with traumatic brain injury.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16933v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathieu Even, Julie Josse</dc:creator>
    </item>
  </channel>
</rss>
