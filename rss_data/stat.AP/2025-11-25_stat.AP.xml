<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Nov 2025 05:02:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Inferring Transmission Dynamics of Respiratory Syncytial Virus from Houston Wastewater</title>
      <link>https://arxiv.org/abs/2511.17816</link>
      <description>arXiv:2511.17816v1 Announce Type: new 
Abstract: Wastewater-based epidemiology (WBE) is an effective tool for tracking community circulation of respiratory viruses. We address estimating the effective reproduction number ($R_t$) and the relative number of infections from wastewater viral load. Using weekly Houston data on respiratory syncytial virus (RSV), we implement a parsimonious Bayesian renewal model that links latent infections to measured viral load through biologically motivated generation and shedding kernels. The framework yields estimates of $R_t$ and relative infections, enabling a coherent interpretation of transmission timing and phase. We compare two input strategies-(i) raw viral-load measurements with a log-scale standard deviation, and (ii) state-space-filtered load estimates with time-varying variances-and find no practically meaningful differences in inferred trajectories or peak timing. Given this equivalence, we report the filtered input as a pragmatic default because it embeds week-specific variances while leaving epidemiological conclusions unchanged.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17816v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jose R. Palacio, Katherine B. Ensor, Sallie A. Keller, Rebecca Schneider, Kaavya Domakonda, Loren Hopkins, Lauren B. Stadler</dc:creator>
    </item>
    <item>
      <title>Fractional cumulative Residual Inaccuracy in the Quantile Framework and its Appications</title>
      <link>https://arxiv.org/abs/2511.18844</link>
      <description>arXiv:2511.18844v1 Announce Type: new 
Abstract: Fractional cumulative residual inaccuracy (FCRI) measure allows to determine regions of discrepancy between systems, depending on their respective fractional and chaotic map parameters. Most of the theoretical results and applications related to the FCRI of the lifetime random variable are based on the distribution function approach. However, there are situations in which the distribution function may not be available in explicit form but has a closed-form quantile function (QF), an alternative method of representing a probability distribution. Motivated by these, the present study is devoted to introduce a quantile-based FCRI and study its various properties. We also deal with non-parametric estimation of quantile-based FCRI and examine its validity using simulation studies and illustrate its usefulness in measuring the discrepancy between chaotic systems and in measuring the discrepancy in two different time regimes using Nifty 50 dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18844v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iona Ann Sebastian, S. M. Sunoj</dc:creator>
    </item>
    <item>
      <title>Validity in machine learning for extreme event attribution</title>
      <link>https://arxiv.org/abs/2511.19039</link>
      <description>arXiv:2511.19039v1 Announce Type: new 
Abstract: Extreme event attribution (EEA), an approach for assessing the extent to which disasters are caused by climate change, is crucial for informing climate policy and legal proceedings. Machine learning is increasingly used for EEA by modeling rare weather events otherwise too complex or computationally intensive to model using traditional simulation methods. However, the validity of using machine learning in this context remains unclear, particularly as high-stakes machine learning applications in general are criticized for inherent bias and lack of robustness. Here we use machine learning and simulation analyses to evaluate EEA in the context of California wildfire data from 2003-2020. We identify three major threats to validity: (1) individual event attribution estimates are highly sensitive to algorithmic design choices; (2) common performance metrics like area under the ROC curve or Brier score are not strongly correlated with attribution error, facilitating suboptimal model selection; and (3) distribution shift -- changes in temperature across climate scenarios -- substantially degrades predictive performance. To address these challenges, we propose a more valid and robust attribution analysis based on aggregate machine learning estimates, using an additional metric -- mean calibration error -- to assess model performance, and using subgroup and propensity diagnostics to assess distribution shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19039v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cassandra C. Chou, Scott L. Zeger, Benjamin Q. Huynh</dc:creator>
    </item>
    <item>
      <title>Modeling smooth and localized mortality patterns across age, time, and space to uncover small-area inequalities</title>
      <link>https://arxiv.org/abs/2511.19151</link>
      <description>arXiv:2511.19151v1 Announce Type: new 
Abstract: Small-area mortality estimation is inherently difficult, as random fluctuations from low death counts can obscure real geographic differences. We introduce a flexible model that borrows strength across age, space, and time to estimate mortality schedules and trends in very small populations. The approach ensures smooth patterns across these dimensions while allowing localized breaks from the spatial structure, capturing broad trajectories as well as sharp local contrasts. We implement our model within a Penalized Spline framework and estimate it using Generalized Linear Array Model techniques, resulting in a computationally fast, interpretable, and parsimonious method. Crucially, it can readily incorporate sudden mortality shocks, such as the Covid-19 pandemic, making it highly versatile for real-world demographic and epidemiological challenges. We demonstrate its application by estimating life expectancy and age-specific mortality inequalities in over 4,800 small areas across the Greater London Authority from 2002 to 2024.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19151v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jacob Martin, Carlo Giovanni Camarda</dc:creator>
    </item>
    <item>
      <title>Seasonality in the U.S. Housing Market: Post-Pandemic Shifts and Regional Dynamics</title>
      <link>https://arxiv.org/abs/2511.10808</link>
      <description>arXiv:2511.10808v1 Announce Type: cross 
Abstract: Seasonality has traditionally shaped the U.S. housing market, with activity peaking in spring-summer and declining in autumn-winter. However, recent disruptions, particularly post-COVID-19, raise questions about shift in these patterns. This study analyzes housing market date (1991-2024) to examine evolving seasonality and regional heterogeneity. Using Housing Price Index (HPI), inventory and sales data from the Federal Housing Finance Agency and U.S. Census Bureau, seasonal components are extracted via the X-13-ARIMA procedure, and statistical tests assess variations across regions. The results confirm seasonal fluctuations in prices and volumes, with recent shifts toward earlier annual peak (March-April) and amplified seasonal effects. Regional variations align with differences in climate and market structure, while prices and sales volumes exhibit in-phase movement, suggesting thick-market momentum behaviour. These findings highlight key implications for policymakers, realtors and investors navigating post-pandemic market dynamics, offering insights into the timing and interpretation of housing market activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10808v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihan Hu, Yifei Huang, Weizhao Wang</dc:creator>
    </item>
    <item>
      <title>Single Changepoint Procedures</title>
      <link>https://arxiv.org/abs/2511.17870</link>
      <description>arXiv:2511.17870v1 Announce Type: cross 
Abstract: Single changepoint tests have become a staple check for homogeneity of a climate time series, suggesting how climate has changed should non-homogeneity be declared. This paper summarizes the most prominent single changepoint tests used in today's climate literature, relating them to one and other and unifying their presentations. Asymptotic quantiles for the individual tests are presented. Derivations of the quantiles are given, enabling the reader to tackle cases not considered within. Our work here studies both mean and trend shifts, covering the most common settings arising in climatology. SOI and global temperature series are analyzed within to illustrate the techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17870v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Robert Lund, Xueheng Shi</dc:creator>
    </item>
    <item>
      <title>A sensitivity analysis for non-inferiority studies with non-randomised data</title>
      <link>https://arxiv.org/abs/2511.18094</link>
      <description>arXiv:2511.18094v1 Announce Type: cross 
Abstract: Background: Non-inferiority studies based on non-randomised data are increasingly used in clinical research but remain prone to unmeasured confounding. The classical E-value offers a simple way to quantify such bias but has been applied almost exclusively with respect to the statistical null. We reformulated the E-value framework to make explicit its applicability to predefined clinical margins, thereby extending its utility to non-inferiority analyses.
  Development: Using the bias-factor formulation by Ding and VanderWeele, we defined the non-inferiority E-value as the minimum strength of association that an unmeasured confounder would need with both treatment and outcome, on the risk-ratio scale, to move the 95% confidence-limit estimate to the prespecified non-inferiority margin.
  Application: This approach was applied to three observational studies and one single-arm trial with external controls to illustrate interpretation and range. The resulting non-inferiority E-values for the confidence limits varied from about one to three, depending on design and findings. In the single-arm trial, a large gap between the confidence-limit and point-estimate NIEs reflected small sample size and wide confidence intervals, highlighting that both should be reported for a balanced assessment of robustness.
  Conclusion: This study reformulates the E-value to focus on clinically meaningful margins rather than the statistical null, enabling its application to non-inferiority analyses. Although the non-inferiority E-value inherits the limitations of the original method and cannot address all bias sources, it offers a transparent framework for interpreting non-randomised evidence and for generating insights that inform the design of future, more definitive randomised controlled trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18094v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daijiro Kabata, Takumi Imai</dc:creator>
    </item>
    <item>
      <title>Optimal Meal Schedule for a Local Nonprofit Using LLM-Aided Data Extraction</title>
      <link>https://arxiv.org/abs/2511.18483</link>
      <description>arXiv:2511.18483v1 Announce Type: cross 
Abstract: We present a data-driven pipeline developed in collaboration with the Power Packs Project, a nonprofit addressing food insecurity in local communities. The system integrates data extraction from PDFs, large language models for ingredient standardization, and binary integer programming to generate a 15-week recipe schedule that minimizes projected wholesale costs while meeting nutritional constraints. All 157 recipes were mapped to a nutritional database and assigned estimated and predicted costs using historical invoice data and category-specific inflation adjustments. The model effectively handles real-world price volatility and is structured for easy updates as new recipes or cost data become available. Optimization results show that constraint-based selection yields nutritionally balanced and cost-efficient plans under uncertainty. To facilitate real-time decision-making, we deployed a searchable web platform that integrates analytical models into daily operations by enabling staff to explore recipes by ingredient, category, or through an optimized meal plan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18483v1</guid>
      <category>cs.CY</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sergio Marin (Bohong), Nhu Nguyen (Bohong),  Max (Bohong),  Zheng, Christina M. Weaver</dc:creator>
    </item>
    <item>
      <title>On the role of fractional Brownian motion in models of chemotaxis and stochastic gradient ascent</title>
      <link>https://arxiv.org/abs/2511.18745</link>
      <description>arXiv:2511.18745v1 Announce Type: cross 
Abstract: Cell migration often exhibits long-range temporal correlations and anomalous diffusion, even in the absence of external guidance cues such as chemical gradients or topographical constraints. These observations raise a fundamental question: do such correlations simply reflect internal cellular processes, or do they enhance a cell's ability to navigate complex environments? In this work, we explore how temporally correlated noise (modeled using fractional Brownian motion) influences chemotactic search dynamics. Through computational experiments, we show that superdiffusive motion, when combined with gradient-driven migration, enables robust exploration of the chemoattractant landscape. Cells reliably reach the global maximum of the concentration field, even in the presence of spatial noise, secondary cues, or irregular signal geometry. We quantify this behavior by analyzing the distribution of first hitting times under varying degrees of temporal correlation. Notably, our results are consistent across diverse conditions, including flat and curved substrates, and scenarios involving both primary and self-generated chemotactic signals. Beyond biological implications, these findings also offer insight into the design of optimization and sampling algorithms that benefit from structured stochasticity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18745v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gustavo Cornejo-Olea, Lucas Buvinic, Jerome Darbon, Radek Erban, Andrea Ravasio, Anastasios Matzavinos</dc:creator>
    </item>
    <item>
      <title>X-chromosome Multilocus Association Studies for Common and Rare Variants</title>
      <link>https://arxiv.org/abs/2511.18948</link>
      <description>arXiv:2511.18948v1 Announce Type: cross 
Abstract: X-chromosome association study has specific model uncertainty challenges, such as unknown X-chromosome inactivation status and baseline allele, and considering nonadditive and gene-sex interaction effects in the analysis or not. Although these challenges have been answered for single-locus X-chromosome variants, it remains unclear how to properly perform multilocus association studies when above uncertainties are present. We first carefully investigate the inferential consequences of these uncertainties on existing multilocus association analysis methods, and then propose a theoretically justified framework to analyze multilocus X-chromosome variants while all the uncertainty issues are addressed. We provide separate solutions for common and rare variants, and simulation results show that our solutions are overall more powerful than existing multilocus methods which were proposed to analyze autosomal variants. We finally provide supporting evidences of our approach by revisiting some published X-chromosome association studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18948v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruilin Bai, Bo Chen</dc:creator>
    </item>
    <item>
      <title>Structured Matching via Cost-Regularized Unbalanced Optimal Transport</title>
      <link>https://arxiv.org/abs/2511.19075</link>
      <description>arXiv:2511.19075v1 Announce Type: cross 
Abstract: Unbalanced optimal transport (UOT) provides a flexible way to match or compare nonnegative finite Radon measures. However, UOT requires a predefined ground transport cost, which may misrepresent the data's underlying geometry. Choosing such a cost is particularly challenging when datasets live in heterogeneous spaces, often motivating practitioners to adopt Gromov-Wasserstein formulations. To address this challenge, we introduce cost-regularized unbalanced optimal transport (CR-UOT), a framework that allows the ground cost to vary while allowing mass creation and removal. We show that CR-UOT incorporates unbalanced Gromov-Wasserstein type problems through families of inner-product costs parameterized by linear transformations, enabling the matching of measures or point clouds across Euclidean spaces. We develop algorithms for such CR-UOT problems using entropic regularization and demonstrate that this approach improves the alignment of heterogeneous single-cell omics profiles, especially when many cells lack direct matches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19075v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuele Pardini, Katerina Papagiannouli</dc:creator>
    </item>
    <item>
      <title>Integrating Complex Covariate Transformations in Generalized Additive Models</title>
      <link>https://arxiv.org/abs/2511.19234</link>
      <description>arXiv:2511.19234v1 Announce Type: cross 
Abstract: Transformations of covariates are widely used in applied statistics to improve interpretability and to satisfy assumptions required for valid inference. More broadly, feature engineering encompasses a wider set of practices aimed at enhancing predictive performance, and is typically performed as part of a data pre-processing step. In contrast, this paper integrates a substantial component of the feature engineering process directly into the modelling stage. This is achieved by introducing a novel general framework for embedding interpretable covariate transformations within multi-parameter Generalised Additive Models (GAMs). Our framework accommodates any sufficiently differentiable scalar-valued transformation of potentially high-dimensional and complex covariates. These transformations are treated as integral model components, with their parameters estimated jointly with regression coefficients via maximum a posteriori (MAP) methods, and joint uncertainty quantified via approximate Bayesian techniques. Smoothing parameters are selected in an empirical Bayes framework using a Laplace approximation to the marginal likelihood, supported by efficient computation based on implicit differentiation methods. We demonstrate the flexibility and practical value of the proposed methodology through applications to forecasting electricity net-demand in Great Britain and to modelling house prices in London. The proposed methods are implemented by the gamFactory R package, available at https://github.com/mfasiolo/gamFactory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19234v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudia Collarin, Matteo Fasiolo, Yannig Goude, Simon Wood</dc:creator>
    </item>
    <item>
      <title>Community-level core-periphery structures in collaboration networks</title>
      <link>https://arxiv.org/abs/2511.19305</link>
      <description>arXiv:2511.19305v1 Announce Type: cross 
Abstract: Uncovering structural patterns in collaboration networks is key for understanding how knowledge flows and innovation emerges. These networks often exhibit a rich interplay of meso-scale structures, such as communities, core-periphery organization, and influential hubs, which shape the complexity of scientific collaboration. The coexistence of such structures challenges traditional approaches, which typically isolate specific network patterns at the node level. We introduce a novel framework for detecting core-periphery structures at the community level. Given a reference grouping of the nodes, the method optimizes an objective function that assigns core or peripheral roles to communities by accounting for the density and strength of their inter-community connections. The node-level partition may correspond to either inferred communities or to a node-attribute classification, such as discipline or location, enabling direct interpretation of how different social or organizational groups occupy central positions in the network. The method is motivated by an application to a co-authorship network of Italian academics in three different disciplines, where it reveals a hierarchical core-periphery structure associated with institutional role, regional location, and research topics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19305v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Geremia, Domenico De Stefano, Michael Fop</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian spectral analysis of multiple stationary time series</title>
      <link>https://arxiv.org/abs/2511.19406</link>
      <description>arXiv:2511.19406v1 Announce Type: cross 
Abstract: The power spectrum of biomedical time series provides important indirect measurements of physiological processes underlying health and biological functions. However, simultaneously characterizing power spectra for multiple time series remains challenging due to extra spectral variability and varying time series lengths. We propose a method for hierarchical Bayesian estimation of stationary time series (HBEST) that provides an interpretable framework for efficiently modeling multiple power spectra. HBEST models log power spectra using a truncated cosine basis expansion with a novel global-local coefficient decomposition, enabling simultaneous estimation of population-level and individual-level power spectra and accommodating time series of varying lengths. The fully Bayesian framework provides shrinkage priors for regularized estimation and efficient information sharing. Simulations demonstrate HBEST's advantages over competing methods in computational efficiency and estimation accuracy. An application to heart rate variability time series demonstrates HBEST's ability to accurately characterize power spectra and capture associations with traditional cardiovascular risk factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19406v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rebecca Lee, Alexander Coulter, Greg J. Siegle, Scott A. Bruce, Anirban Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Protocol For An Observational Study On The Effects Of Combinations Of Adverse Childhood Experiences On Adult Depression</title>
      <link>https://arxiv.org/abs/2502.17679</link>
      <description>arXiv:2502.17679v2 Announce Type: replace 
Abstract: Adverse childhood experiences (ACEs) have been linked to a wide range of negative health outcomes in adulthood. However, few studies have investigated what specific combinations of ACEs most substantially impact mental health. In this article, we provide the protocol for our observational study of the effects of combinations of ACEs on adult depression. We use data from the 2023 Behavioral Risk Factor Surveillance System (BRFSS) to assess these effects. We will evaluate the replicability of our findings by splitting the sample into two discrete subpopulations of individuals. We employ data turnover for this analysis, enabling a single team of statisticians and domain experts to collaboratively evaluate the strength of evidence, and also integrating both qualitative and quantitative insights from exploratory data analysis. We outline our analysis plan using this method and conclude with a brief discussion of several specifics for our study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17679v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruizhe Zhang, Jooyoung Kong, Dylan S. Small, William Bekerman</dc:creator>
    </item>
    <item>
      <title>Optimized Supergeo Design: A Scalable Framework for Geographic Marketing Experiments</title>
      <link>https://arxiv.org/abs/2506.20499</link>
      <description>arXiv:2506.20499v3 Announce Type: replace 
Abstract: Geographic experiments are a widely-used methodology for measuring incremental return on ad spend (iROAS) at scale, yet their design presents significant challenges. The unit count is small, heterogeneity is large, and the optimal Supergeo partitioning problem is NP-hard. We introduce Optimized Supergeo Design (OSD), a two-stage framework that renders Supergeo designs practical for large-scale markets. Principal Component Analysis (PCA) first reduces the covariate space to create interpretable geo-embeddings. A Mixed-Integer Linear Programming (MILP) solver then selects a partition that balances both baseline outcomes and pre-treatment covariates. We provide theoretical arguments that OSD's objective value is within (1+{\epsilon}) of the global optimum under community-structure assumptions. Rigorous ablation analysis demonstrates that PCA-based clustering achieves statistical parity with unit-level randomization (7% higher RMSE, not statistically significant) while enabling operational benefits through coarser granularity. Crucially, OSD solves the scalability bottleneck: for N=210 markets, OSD completes in 0.22 seconds compared to weeks for exact methods -- a speedup of over 5 million times. In extensive simulations with up to 1,000 units, OSD achieves excellent covariate balance (SMD &lt; 1%) and retains every media dollar, establishing a scalable framework that matches the statistical efficiency of randomization with the operational practicality of Supergeos.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20499v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Shaw</dc:creator>
    </item>
    <item>
      <title>The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective</title>
      <link>https://arxiv.org/abs/2312.15524</link>
      <description>arXiv:2312.15524v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown impressive potential to simulate human behavior. We identify a fundamental challenge in using them to simulate experiments: when LLM-simulated subjects are blind to the experimental design (as is standard practice with human subjects), variations in treatment systematically affect unspecified variables that should remain constant, violating the unconfoundedness assumption. Using demand estimation as a context and an actual experiment with 40 different products as a benchmark, we show this can lead to implausible results. While confounding may in principle be addressed by controlling for covariates, this can compromise ecological validity in the context of LLM simulations: controlled covariates become artificially salient in the simulated decision process. We show formally that confoundness stems from ambiguous prompting strategies. Therefore, it can be addressed by developing unambiguous prompting strategies through unblinding, i.e., revealing the experiment design in LLM simulations. Our empirical results show that this strategy consistently enhances model performance across all tested models, including both out-of-box reasoning and non-reasoning models. We also show that it is a technique that complements fine-tuning: while fine-tuning can improve simulation performance, an unambiguous prompting strategy makes the predictions robust to the inclusion of irrelevant data in the fine-tuning process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15524v3</guid>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2139/ssrn.4650172</arxiv:DOI>
      <dc:creator>George Gui, Olivier Toubia</dc:creator>
    </item>
    <item>
      <title>Filtered Markovian Projection: Dimensionality Reduction in Filtering for Stochastic Reaction Networks</title>
      <link>https://arxiv.org/abs/2502.07918</link>
      <description>arXiv:2502.07918v3 Announce Type: replace-cross 
Abstract: Stochastic reaction networks (SRNs) model stochastic effects for various applications, including intracellular chemical or biological processes and epidemiology. A typical challenge in practical problems modeled by SRNs is that only a few state variables can be dynamically observed. Given the measurement trajectories, one can estimate the conditional probability distribution of unobserved (hidden) state variables by solving a stochastic filtering problem. In this setting, the conditional distribution evolves over time according to an extensive or potentially infinite-dimensional system of coupled ordinary differential equations with jumps, known as the filtering equation. The current numerical filtering techniques, such as the filtered finite state projection (D'Ambrosio et al., 2022), are hindered by the curse of dimensionality, significantly affecting their computational performance. To address these limitations, we propose to use a dimensionality reduction technique based on the Markovian projection (MP), initially introduced for forward problems (Ben Hammouda et al., 2024). In this work, we explore how to adapt the existing MP approach to the filtering problem and introduce a novel version of the MP, the Filtered MP, that guarantees the consistency of the resulting estimator. The novel consistent MP filter employs a reduced-variance particle filter for estimating the jump intensities of the projected model and solves the filtering equations in a low-dimensional space. The analysis and empirical results highlight the superior computational efficiency of projection methods compared to the existing filtered finite state projection in the large dimensional setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07918v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiheb Ben Hammouda, Maksim Chupin, Sophia M\"unker, Ra\'ul Tempone</dc:creator>
    </item>
    <item>
      <title>Comparison of Bayesian methods for extrapolation of treatment effects: a large scale simulation study</title>
      <link>https://arxiv.org/abs/2504.01949</link>
      <description>arXiv:2504.01949v3 Announce Type: replace-cross 
Abstract: Extrapolating treatment effects from related studies is a promising strategy for designing and analyzing clinical trials in situations where achieving an adequate sample size is challenging. Bayesian methods are well-suited for this purpose, as they enable the synthesis of prior information through the use of prior distributions. While the operating characteristics of Bayesian approaches for borrowing data from control arms have been extensively studied, methods that borrow treatment effects -- quantities derived from the comparison between two arms -- remain less well understood. In this paper, we present the findings of an extensive simulation study designed to address this gap. We evaluate the frequentist operating characteristics of these methods, including the probability of success, mean squared error, bias, precision, and credible interval coverage. Our results provide insights into the strengths and limitations of existing methods in the context of confirmatory trials. In particular, we show that the Conditional Power Prior and the Robust Mixture Prior perform better overall, while the test-then-pool variants and the p-value-based power prior display suboptimal performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01949v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tristan Fauvel, Julien Tanniou, Pascal Godbillot, Marie G\'enin, Billy Amzal</dc:creator>
    </item>
    <item>
      <title>BioDisco: Multi-agent hypothesis generation with dual-mode evidence, iterative feedback and temporal evaluation</title>
      <link>https://arxiv.org/abs/2508.01285</link>
      <description>arXiv:2508.01285v2 Announce Type: replace-cross 
Abstract: Identifying novel hypotheses is essential to scientific research, yet this process risks being overwhelmed by the sheer volume and complexity of available information. Existing automated methods often struggle to generate novel and evidence-grounded hypotheses, lack robust iterative refinement and rarely undergo rigorous temporal evaluation for future discovery potential. To address this, we propose BioDisco, a multi-agent framework that draws upon language model-based reasoning and a dual-mode evidence system (biomedical knowledge graphs and automated literature retrieval) for grounded novelty, integrates an internal scoring and feedback loop for iterative refinement, and validates performance through pioneering temporal and human evaluations and a Bradley-Terry paired comparison model to provide statistically-grounded assessment. Our evaluations demonstrate superior novelty and significance over ablated configurations and generalist biomedical agents. Designed for flexibility and modularity, BioDisco allows seamless integration of custom language models or knowledge graphs, and can be run with just a few lines of code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01285v2</guid>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujing Ke, Kevin George, Kathan Pandya, David Blumenthal, Maximilian Sprang, Gerrit Gro{\ss}mann, Sebastian Vollmer, David Antony Selby</dc:creator>
    </item>
    <item>
      <title>A Bayesian Model for Multi-stage Censoring</title>
      <link>https://arxiv.org/abs/2511.11684</link>
      <description>arXiv:2511.11684v3 Announce Type: replace-cross 
Abstract: Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11684v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuvom Sadhuka, Sophia Lin, Bonnie Berger, Emma Pierson</dc:creator>
    </item>
  </channel>
</rss>
