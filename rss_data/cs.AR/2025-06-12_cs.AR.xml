<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Jun 2025 01:41:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Exploiting Control-flow Enforcement Technology for Sound and Precise Static Binary Disassembly</title>
      <link>https://arxiv.org/abs/2506.09426</link>
      <description>arXiv:2506.09426v1 Announce Type: new 
Abstract: Rewriting x86_64 binaries-whether for security hardening, dynamic instrumentation, or performance profiling is notoriously difficult due to variable-length instructions, interleaved code and data, and indirect jumps to arbitrary byte offsets. Existing solutions (e.g., "superset disassembly") ensure soundness but incur significant overhead and produce large rewritten binaries, especially for on-the-fly instrumentation. This paper addresses these challenges by introducing the Time Variance Authority (TVA), which leverages Intel's Control-Flow Enforcement Technology (CET). By recognizing endbr64 as the only valid indirect jump target, TVA prunes spurious disassembly paths while preserving soundness and emulates CET constraints on processors lacking native CET support, effectively mitigating ROP/JOP exploits without new hardware. We implement TVA by modernizing the Multiverse rewriter for 64-bit Linux. Our evaluation on SPEC CPU2017 and real-world applications shows that TVA-guided rewriting achieves up to 1.3x faster instrumentation time. These results underscore TVA's feasibility as a high-performance, uprobes-free alternative for robust x86_64 binary analysis and rewriting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09426v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian Zhao, Yiwei Yang, Yusheng Zheng, Andi Quinn</dc:creator>
    </item>
    <item>
      <title>FPGA-Based Multiplier with a New Approximate Full Adder for Error-Resilient Applications</title>
      <link>https://arxiv.org/abs/2506.09596</link>
      <description>arXiv:2506.09596v1 Announce Type: new 
Abstract: Electronic devices primarily aim to offer low power consumption, high speed, and a compact area. The performance of very large-scale integration (VLSI) devices is influenced by arithmetic operations, where multiplication is a crucial operation. Therefore, a high-speed multiplier is essential for developing any signal-processing module. Numerous multipliers have been reviewed in existing literature, and their speed is largely determined by how partial products (PPs) are accumulated. To enhance the speed of multiplication beyond current methods, an approximate adder-based multiplier is introduced. This approach allows for the simultaneous addition of PPs from two consecutive bits using a novel approximate adder. The proposed multiplier is utilized in a mean filter structure and implemented in ISE Design Suite 14.7 using VHDL and synthesized on the Xilinx Spartan3-XC3S400 FPGA board. Compared to the literature, the proposed multiplier achieves power and power-delay product (PDP) improvements of 56.09% and 73.02%, respectively. The validity of the expressed multiplier is demonstrated through the mean filter system. Results show that it achieves power savings of 33.33%. Additionally, the proposed multiplier provides more accurate results than other approximate multipliers by expressing higher values of peak signal-to-noise ratio (PSNR), (30.58%), and structural similarity index metric (SSIM), (22.22%), while power consumption is in a low range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09596v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Ranjbar, Elham Esmaeili, Roghayeh Rafieisangari, Nabiollah Shiri</dc:creator>
    </item>
    <item>
      <title>Low-Level and NUMA-Aware Optimization for High-Performance Quantum Simulation</title>
      <link>https://arxiv.org/abs/2506.09198</link>
      <description>arXiv:2506.09198v1 Announce Type: cross 
Abstract: Scalable classical simulation of quantum circuits is crucial for advancing both quantum algorithm development and hardware validation. In this work, we focus on performance enhancements through meticulous low-level tuning on a single-node system, thereby not only advancing the performance of classical quantum simulations but also laying the groundwork for scalable, heterogeneous implementations that may eventually bridge the gap toward noiseless quantum computing. Although similar efforts in low-level tuning have been reported in the literature, such implementations have not been released as open-source software, thereby impeding independent evaluation and further development. We introduce an open-source, high-performance extension to the QuEST simulator that brings state-of-the-art low-level and NUMA optimizations to modern computers. Our approach emphasizes locality-aware computation and incorporates hardware-specific optimizations such as NUMA-aware memory allocation, thread pinning, AVX-512 vectorization, aggressive loop unrolling, and explicit memory prefetching. Experiments demonstrate significant speedups - 5.5-6.5x for single-qubit gate operations, 4.5x for two-qubit gates, 4x for Random Quantum Circuits (RQC), and 1.8x for Quantum Fourier Transform (QFT), demonstrating that rigorous performance tuning can substantially extend the practical simulation capacity of classical quantum simulators on current hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09198v1</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Rezaei, Luc Jaulmes, Maria Bahna, Oliver Thomson Brown, Antonio Barbalace</dc:creator>
    </item>
    <item>
      <title>Efficient Modular Multiplier over GF (2^m) for ECPM</title>
      <link>https://arxiv.org/abs/2506.09464</link>
      <description>arXiv:2506.09464v1 Announce Type: cross 
Abstract: Elliptic curve cryptography (ECC) has emerged as the dominant public-key protocol, with NIST standardizing parameters for binary field GF(2^m) ECC systems. This work presents a hardware implementation of a Hybrid Multiplication technique for modular multiplication over binary field GF(2m), targeting NIST B-163, 233, 283, and 571 parameters. The design optimizes the combination of conventional multiplication (CM) and Karatsuba multiplication (KM) to enhance elliptic curve point multiplication (ECPM). The key innovation uses CM for smaller operands (up to 41 bits for m=163) and KM for larger ones, reducing computational complexity and enhancing efficiency. The design is evaluated in three areas: Resource Utilization For m=163, the hybrid design uses 6,812 LUTs, a 39.82% reduction compared to conventional methods. For m=233, LUT usage reduces by 45.53% and 70.70% compared to overlap-free and bit-parallel implementations. Delay Performance For m=163, achieves 13.31ns delay, improving by 37.60% over bit-parallel implementations. For m=233, maintains 13.39ns delay. Area-Delay Product For m=163, achieves ADP of 90,860, outperforming bit-parallel (75,337) and digit-serial (43,179) implementations. For m=233, demonstrates 16.86% improvement over overlap-free and 96.10% over bit-parallel designs. Results show the hybrid technique significantly improves speed, hardware efficiency, and resource utilization for ECC cryptographic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09464v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruby Kumari, Gaurav Purohit, Abhijit Karmakar</dc:creator>
    </item>
    <item>
      <title>Mainframe-style channel controllers for modern disaggregated memory systems</title>
      <link>https://arxiv.org/abs/2506.09758</link>
      <description>arXiv:2506.09758v1 Announce Type: cross 
Abstract: Despite the promise of alleviating the main memory bottleneck, and the existence of commercial hardware implementations, techniques for Near-Data Processing have seen relatively little real-world deployment. The idea has received renewed interest with the appearance of disaggregated or "far" memory, for example in the use of CXL memory pools.
  However, we argue that the lack of a clear OS-centric abstraction of Near-Data Processing is a major barrier to adoption of the technology. Inspired by the channel controllers which interface the CPU to disk drives in mainframe systems, we propose memory channel controllers as a convenient, portable, and virtualizable abstraction of Near-Data Processing for modern disaggregated memory systems.
  In addition to providing a clean abstraction that enables OS integration while requiring no changes to CPU architecture, memory channel controllers incorporate another key innovation: they exploit the cache coherence provided by emerging interconnects to provide a much richer programming model, with more fine-grained interaction, than has been possible with existing designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09758v1</guid>
      <category>cs.OS</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zikai Liu, Jasmin Schult, Pengcheng Xu, Timothy Roscoe</dc:creator>
    </item>
    <item>
      <title>On the Impossibility of a Perfect Hypervisor</title>
      <link>https://arxiv.org/abs/2506.09825</link>
      <description>arXiv:2506.09825v1 Announce Type: cross 
Abstract: We establish a fundamental impossibility result for a `perfect hypervisor', one that (1) preserves every observable behavior of any program exactly as on bare metal and (2) adds zero timing or resource overhead.
  Within this model we prove two theorems. (1) Indetectability Theorem. If such a hypervisor existed, no guest-level program, measurement, or timing test could distinguish it from native execution; all traces, outputs, and timings would be identical.
  (2) Impossibility Theorem. Despite that theoretical indetectability, a perfect hypervisor cannot exist on any machine with finite computational resources.
  These results are architecture-agnostic and extend beyond hypervisors to any virtualization layer emulators, sandboxes, containers, or runtime-instrumentation frameworks. Together they provide a formal foundation for future work on the principles and limits of virtualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09825v1</guid>
      <category>cs.OS</category>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mordechai Guri</dc:creator>
    </item>
    <item>
      <title>YOCO: A Hybrid In-Memory Computing Architecture with 8-bit Sub-PetaOps/W In-Situ Multiply Arithmetic for Large-Scale AI</title>
      <link>https://arxiv.org/abs/2312.11836</link>
      <description>arXiv:2312.11836v3 Announce Type: replace 
Abstract: In this paper, we further explore the potential of analog in-memory computing (AiMC) and introduce an innovative artificial intelligence (AI) accelerator architecture named YOCO, featuring three key proposals: (1) YOCO proposes a novel 8-bit in-situ multiply arithmetic (IMA) achieving 123.8 TOPS/W energy-efficiency and 34.9 TOPS throughput through efficient charge-domain computation and timedomain accumulation mechanism. (2) YOCO employs a hybrid ReRAM-SRAM memory structure to balance computational efficiency and storage density. (3) YOCO tailors an IMC-friendly attention computing flow with an efficient pipeline to accelerate the inference of transformer-based AI models. Compared to three SOTA baselines, YOCO on average improves energy efficiency by up to 3.9x-19.9x and throughput by up to 6.8x-33.6x across 10 CNN/transformer models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11836v3</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihao Xuan, Yuxuan Yang, Wei Xuan, Zijia Su, Song Chen, Yi Kang</dc:creator>
    </item>
    <item>
      <title>An FPGA Compiler for On-the-Fly Adaptive CNN Deployment and Reconfiguration</title>
      <link>https://arxiv.org/abs/2504.08534</link>
      <description>arXiv:2504.08534v3 Announce Type: replace 
Abstract: We introduce ForgeMorph, a full-stack compiler for adaptive CNN deployment on FPGAs, combining design-time optimization with runtime reconfigurability. At compile time, the NeuroForge engine performs constraint-driven design space exploration, generating RTL mappings that are Pareto-optimal with respect to user-defined latency and resource budgets. Unlike existing FPGA compilers, which rely on static scheduling and manual tuning, NeuroForge leverages analytical performance models and multi-objective genetic algorithms to efficiently search large configuration spaces and propose highly optimized hardware implementations. At runtime, the NeuroMorph module enables dynamic reconfiguration of network width and depth without requiring redeployment. This is made possible by a novel training strategy, DistillCycle, which jointly trains the full model and its subnetworks using hierarchical knowledge distillation. As a result, each execution path maintains accuracy even under aggressive resource and power constraints. We demonstrate Forge-Morph on the Zynq-7100 using custom and benchmark models including MobileNetV2, ResNet-50, SqueezeNet, and YOLOv5. The system achieves up to 50x latency reduction and 32% lower power consumption at runtime, while matching or exceeding the efficiency of state-of-the-art compilers. ForgeMorph offers a unified solution for deployment scenarios that demand flexibility, performance, and hardware efficiency</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08534v3</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alaa Mazouz, Duc Han Le, Van-Tam Nguyen</dc:creator>
    </item>
    <item>
      <title>LaZagna: An Open-Source Framework for Flexible 3D FPGA Architectural Exploration</title>
      <link>https://arxiv.org/abs/2505.05579</link>
      <description>arXiv:2505.05579v2 Announce Type: replace 
Abstract: While 3D IC technology has been extensively explored for ASICs, their application to FPGAs remains limited. Existing studies on 3D FPGAs are often constrained to fixed prototypes, narrow architectural templates, and simulation-only evaluations. In this work, we present LaZagna, the first open-source framework for automated, end-to-end 3D FPGA architecture generation and evaluation. LaZagna supports high-level architectural specification, synthesizable RTL generation, and bitstream production, enabling comprehensive validation of 3D FPGA designs beyond simulation. It significantly broadens the design space compared to prior work by introducing customizable vertical interconnect patterns, novel 3D switch block designs, and support for heterogeneous logic layers. The framework also incorporates practical design constraints such as inter-layer via density and vertical interconnect delay. We demonstrate the capabilities of LaZagna by generating synthesizable RTL that can be taken through full physical design flows for fabric generation, along with functionally correct bitstreams. Furthermore, we conduct five case studies that explore various architectural parameters and evaluate their impact on wirelength, critical path delay, and routing runtime. These studies showcase the framework's scalability, flexibility, and effectiveness in guiding future 3D FPGA architectural and packaging decisions. LaZagna is fully open-source and available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05579v2</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ismael Youssef, Hang Yang, Cong Hao</dc:creator>
    </item>
    <item>
      <title>CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference</title>
      <link>https://arxiv.org/abs/2407.12736</link>
      <description>arXiv:2407.12736v4 Announce Type: replace-cross 
Abstract: Vision Transformers (ViTs) represent a groundbreaking shift in machine learning approaches to computer vision. Unlike traditional approaches, ViTs employ the self-attention mechanism, which has been widely used in natural language processing, to analyze image patches. Despite their advantages in modeling visual tasks, deploying ViTs on hardware platforms, notably Field-Programmable Gate Arrays (FPGAs), introduces considerable challenges. These challenges stem primarily from the non-linear calculations and high computational and memory demands of ViTs. This paper introduces CHOSEN, a software-hardware co-design framework to address these challenges and offer an automated framework for ViT deployment on the FPGAs in order to maximize performance. Our framework is built upon three fundamental contributions: multi-kernel design to maximize the bandwidth, mainly targeting benefits of multi DDR memory banks, approximate non-linear functions that exhibit minimal accuracy degradation, and efficient use of available logic blocks on the FPGA, and efficient compiler to maximize the performance and memory-efficiency of the computing kernels by presenting a novel algorithm for design space exploration to find optimal hardware configuration that achieves optimal throughput and latency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a 1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12736v4</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Erfan Sadeghi, Arash Fayyazi, Suhas Somashekar, Armin Abdollahi, Massoud Pedram</dc:creator>
    </item>
  </channel>
</rss>
