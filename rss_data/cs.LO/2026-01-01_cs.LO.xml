<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Jan 2026 05:00:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A precise proof of the n-variable Bekic principle</title>
      <link>https://arxiv.org/abs/2512.24038</link>
      <description>arXiv:2512.24038v1 Announce Type: new 
Abstract: We provide a proof of the $n$-ary Beki\v{c} principle, which states that a vectorial fixpoint of size $n$ can be written in terms of nested fixpoints in each coordinate according to lexicographic order. The proof is inductive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24038v1</guid>
      <category>cs.LO</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jun Xu</dc:creator>
    </item>
    <item>
      <title>Proof-Carrying PWL Verification for ReLU Networks: Convex-Hull Semantics, Exact \SMT/\MILP Encodings, and Symbolic Certificate Checking</title>
      <link>https://arxiv.org/abs/2512.24339</link>
      <description>arXiv:2512.24339v1 Announce Type: new 
Abstract: ReLU networks are piecewise-linear (PWL), enabling exact symbolic verification via \SMT(\LRA) or \MILP. However, safety claims in certification pipelines require not only correctness but also \emph{checkable evidence}. We develop a proof-carrying verification core for PWL neural constraints: (i) we formalize ReLU networks as unions of polyhedra indexed by activation patterns; (ii) we present exact \SMT/\MILP encodings and the canonical convex-hull relaxation for each bounded ReLU; and (iii) we introduce a certificate calculus in which bound tightening, stabilization, strengthening, and pruning steps emit explicit algebraic witnesses (LP dual multipliers and Farkas infeasibility certificates). Crucially, these witnesses are \emph{symbolic objects} that admit independent verification in exact arithmetic over $\Q$. We provide a symbolic certificate checker, normalization rules that preserve validity, and a compositional view of region-wise certificates as a global proof artifact for universal safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24339v1</guid>
      <category>cs.LO</category>
      <category>math.RA</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chandrasekhar Gokavarapu (Department of Mathematics, Government College)</dc:creator>
    </item>
    <item>
      <title>Open Horn Type Theory</title>
      <link>https://arxiv.org/abs/2512.24498</link>
      <description>arXiv:2512.24498v1 Announce Type: new 
Abstract: We introduce Open Horn Type Theory (OHTT), an extension of dependent type theory with two primitive judgment forms: coherence and gap, subject to a mutual exclusion law. Unlike classical or intuitionistic negation, gap is not defined via implication but is a primitive witness of non-coherence. Judgments may also be open -- neither coherent nor gapped -- yielding a trichotomy that generalizes the binary derivable/underivable distinction. The central construction is the transport horn: a configuration where a term and a path both cohere, but transport along the path is witnessed as gapped. This captures obstructions that Homotopy Type Theory (HoTT) cannot express, since HoTT's Kan condition guarantees all transport succeeds. We develop the semantics via ruptured simplicial sets -- simplicial sets equipped with coherence and gap structure -- and ruptured Kan complexes, which model types where some horns fill, some are gap-witnessed, and some remain open. We show that HoTT embeds as the coherent fragment of OHTT, recovered by imposing totality. Three classes of obstructions are developed in detail: topological (monodromy, holonomy, characteristic classes), semantic (polysemy, meaning fibrations), and logical (resource-sensitive derivability, substructural failure). In each case, the gap witness is positive structure -- not absence of proof, but certified obstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24498v1</guid>
      <category>cs.LO</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Poernomo</dc:creator>
    </item>
    <item>
      <title>LeanCat: A Benchmark Suite for Formal Category Theory in Lean (Part I: 1-Categories)</title>
      <link>https://arxiv.org/abs/2512.24796</link>
      <description>arXiv:2512.24796v1 Announce Type: new 
Abstract: Large language models (LLMs) have made rapid progress in formal theorem proving, yet current benchmarks under-measure the kind of abstraction and library-mediated reasoning that organizes modern mathematics. In parallel with FATE's emphasis on frontier algebra, we introduce LeanCat, a Lean benchmark for category-theoretic formalization -- a unifying language for mathematical structure and a core layer of modern proof engineering -- serving as a stress test of structural, interface-level reasoning. Part I: 1-Categories contains 100 fully formalized statement-level tasks, curated into topic families and three difficulty tiers via an LLM-assisted + human grading process. The best model solves 8.25% of tasks at pass@1 (32.50%/4.17%/0.00% by Easy/Medium/High) and 12.00% at pass@4 (50.00%/4.76%/0.00%). We also evaluate LeanBridge which use LeanExplore to search Mathlib, and observe consistent gains over single-model baselines. LeanCat is intended as a compact, reusable checkpoint for tracking both AI and human progress toward reliable, research-level formalization in Lean.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24796v1</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <category>cs.FL</category>
      <category>cs.LG</category>
      <category>math.CT</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rongge Xu, Hui Dai, Yiming Fu, Jiedong Jiang, Tianjiao Nie, Hongwei Wang, Junkai Wang, Holiverse Yang, Jiatong Yang, Zhi-Hao Zhang</dc:creator>
    </item>
    <item>
      <title>A Modal Logic for Possibilistic Reasoning with Fuzzy Formal Contexts</title>
      <link>https://arxiv.org/abs/2512.24980</link>
      <description>arXiv:2512.24980v1 Announce Type: new 
Abstract: We introduce a two-sort weighted modal logic for possibilistic reasoning with fuzzy formal contexts. The syntax of the logic includes two types of weighted modal operators corresponding to classical necessity ($\Box$) and sufficiency ($\boxminus$) modalities and its formulas are interpreted in fuzzy formal contexts based on possibility theory. We present its axiomatization that is \emph{sound} with respect to the class of all fuzzy context models. In addition, both the necessity and sufficiency fragments of the logic are also individually complete with respect to the class of all fuzzy context models. We highlight the expressive power of the logic with some illustrative examples. As a formal context is the basic construct of formal concept analysis (FCA), we generalize three main notions in FCA, i.e., formal concepts, object oriented concepts, and property oriented concepts, to their corresponding $c$-cut concepts in fuzzy formal contexts. Then, we show that our logical language can represent all three of these generalized notions. Finally, we demonstrate the possibility of extending our logic to reasoning with multi-relational fuzzy contexts, in which the Boolean combinations of different fuzzy relations are allowed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24980v1</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prosenjit Howlader, Churn-Jung Liau</dc:creator>
    </item>
    <item>
      <title>On Good-for-MDPs Automata</title>
      <link>https://arxiv.org/abs/2202.07629</link>
      <description>arXiv:2202.07629v4 Announce Type: cross 
Abstract: Nondeterministic good-for-MDPs (GFM) automata are for MDP model checking and reinforcement learning what good-for-games (GFG) automata are for reactive synthesis: a more compact alternative to deterministic automata that displays nondeterminism, but only so much that it can be resolved locally, such that a syntactic product can be analysed.
  GFM has recently been introduced as a property for reinforcement learning, where the simpler B\"uchi acceptance conditions it allows to use is key. However, while there are classic and novel techniques to obtain automata that are GFM, there has not been a decision procedure for checking whether or not an automaton is GFM. We show that GFM-ness is decidable and provide an EXPTIME decision procedure as well as a PSPACE-hardness proof.
  We also compare the succinctness of GFM automata with other types of automata with restricted nondeterminism. The first natural comparison point are GFG automata. Deterministic automata are GFG, and GFG automata are GFM, but not vice versa. This raises the question of how these classes relate in terms of succinctness. GFG automata are known to be exponentially more succinct than deterministic automata, but the gap between GFM and GFG automata as well as the gap between ordinary nondeterministic automata and those that are GFM have been open. We establish that these gaps are exponential, and sharpen this result by showing that the latter gap remains exponential when restricting the nondeterministic automata to separating safety or unambiguous reachability automata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.07629v4</guid>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sven Schewe, Qiyi Tang, Tansholpan Zhanabekova</dc:creator>
    </item>
    <item>
      <title>Biochemical Computing Mode for Sequential Logic</title>
      <link>https://arxiv.org/abs/2512.23734</link>
      <description>arXiv:2512.23734v1 Announce Type: cross 
Abstract: Recent years have witnessed the growing scholarly interest in the next-generation general-purpose computers. Various innovative computing modes have been proposed, such as optical, quantum phenomena, and DNA-based modes. Sequential logic circuits are a critical factor that enables these modes to function as general-purpose computers, given their essential role in facilitating continuous computation and memory storage through their ability to store states. However, compared to computability, it is often overlooked due to the difficulty of its implementation. In this paper, we first demonstrate sequential mapping, a crucial necessary condition for electronic computers to realize sequential logic circuits, and highlight this distinctive property of general-purpose computers in the context of logic gate circuits. To achieve computational functionalities comparable to those of electronic computers, we utilize the control effect of enzymes on enzymatic reactions to design a logic gate model that is composed of small molecules and driven by enzymes, subsequently propose a biochemical computing mode. Furthermore, we mathematically analyze the static and dynamic input-output properties of biochemical logic gate components and prove that the biochemical computing mode satisfies sequential mapping similar to electronic computers. When combined with the storage characteristics of NOT-AND gates, it can realize sequential logic circuits. The findings can serve as a theoretical foundation for developing general-purpose biochemical computers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23734v1</guid>
      <category>cs.ET</category>
      <category>cs.LO</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Huang, Chengzhi Ma, Yuxin Zhao, Qingyao Wang, Xinglong Xiao, Xiulin Shu, Zhifeng Hao</dc:creator>
    </item>
    <item>
      <title>Enforcing Temporal Constraints for LLM Agents</title>
      <link>https://arxiv.org/abs/2512.23738</link>
      <description>arXiv:2512.23738v1 Announce Type: cross 
Abstract: LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23738v1</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adharsh Kamath, Sishen Zhang, Calvin Xu, Shubham Ugare, Gagandeep Singh, Sasa Misailovic</dc:creator>
    </item>
    <item>
      <title>A Tale of 1001 LoC: Potential Runtime Error-Guided Specification Synthesis for Verifying Large-Scale Programs</title>
      <link>https://arxiv.org/abs/2512.24594</link>
      <description>arXiv:2512.24594v1 Announce Type: cross 
Abstract: Fully automated verification of large-scale software and hardware systems is arguably the holy grail of formal methods. Large language models (LLMs) have recently demonstrated their potential for enhancing the degree of automation in formal verification by, e.g., generating formal specifications as essential to deductive verification, yet exhibit poor scalability due to long-context reasoning limitations and, more importantly, the difficulty of inferring complex, interprocedural specifications. This paper presents Preguss -- a modular, fine-grained framework for automating the generation and refinement of formal specifications. Preguss synergizes between static analysis and deductive verification by steering two components in a divide-and-conquer fashion: (i) potential runtime error-guided construction and prioritization of verification units, and (ii) LLM-aided synthesis of interprocedural specifications at the unit level. We show that Preguss substantially outperforms state-of-the-art LLM-based approaches and, in particular, it enables highly automated RTE-freeness verification for real-world programs with over a thousand LoC, with a reduction of 80.6%~88.9% human verification effort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24594v1</guid>
      <category>cs.SE</category>
      <category>cs.LO</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhongyi Wang, Tengjie Lin, Mingshuai Chen, Haokun Li, Mingqi Yang, Xiao Yi, Shengchao Qin, Yixing Luo, Xiaofeng Li, Bin Gu, Liqiang Lu, Jianwei Yin</dc:creator>
    </item>
    <item>
      <title>Enumeration and updates for conjunctive linear algebra queries through expressibility</title>
      <link>https://arxiv.org/abs/2310.04118</link>
      <description>arXiv:2310.04118v5 Announce Type: replace-cross 
Abstract: Due to the importance of linear algebra and matrix operations in data analytics, there is significant interest in using relational query optimization and processing techniques for evaluating (sparse) linear algebra programs. In particular, in recent years close connections have been established between linear algebra programs and relational algebra that allow transferring optimization techniques of the latter to the former. In this paper, we ask ourselves which linear algebra programs in MATLANG correspond to the free-connex and q-hierarchical fragments of conjunctive first-order logic. Both fragments have desirable query processing properties: free-connex conjunctive queries support constant-delay enumeration after a linear-time preprocessing phase, and q-hierarchical conjunctive queries further allow constant-time updates. By characterizing the corresponding fragments of MATLANG, we hence identify the fragments of linear algebra programs that one can evaluate with constant-delay enumeration after linear-time preprocessing and with constant-time updates. To derive our results, we improve and generalize previous correspondences between MATLANG and relational algebra evaluated over semiring-annotated relations. In addition, we identify properties on semirings that allow to generalize the complexity bounds for free-connex and q-hierarchical conjunctive queries from Boolean annotations to general semirings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04118v5</guid>
      <category>cs.CC</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Mu\~noz, Cristian Riveros, Stijn Vansummeren</dc:creator>
    </item>
    <item>
      <title>Decomposing graphs into stable and ordered parts</title>
      <link>https://arxiv.org/abs/2505.00594</link>
      <description>arXiv:2505.00594v2 Announce Type: replace-cross 
Abstract: Connections between structural graph theory and finite model theory recently gained a lot of attention. In this setting, many interesting questions remain on the properties of dependent (NIP) hereditary classes of graphs, in particular related to first-order transductions. In this paper, we study modelizations (which are strong forms of transduction pairings) of classes of graphs by classes of structures. In particular, we consider models obtained by coupling a partial order and a colored graph (thus forming a partially ordered colored graph). Motivated by Simon's decomposition theorem of dependent types into a stable part and a distal (order-like) part, we conjecture that every dependent hereditary class of graphs admits a modelization in a monadically dependent coupling of a class of posets with bounded treewidth cover graphs and a monadically stable class of colored graphs. In this paper, we consider the first non-trivial case (classes with bounded linear cliquewidth) and prove that the conjecture holds in a strong form, the model class being a monadically dependent coupling of a class of disjoint unions of chains and a class of colored graphs with bounded pathwidth. We extend our study to classes that admit bounded-size bounded linear cliquewidth decompositions and prove that they have a modelization in a monadically dependent coupling of a class of disjoint unions of chains and a class of colored graphs with bounded expansion, the model class also admitting bounded-size bounded linear cliquewidth decompositions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00594v2</guid>
      <category>math.CO</category>
      <category>cs.LO</category>
      <category>math.LO</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hector Buffi\`ere, Patrice Ossona de Mendez</dc:creator>
    </item>
    <item>
      <title>Toward Robust Legal Text Formalization into Defeasible Deontic Logic using LLMs</title>
      <link>https://arxiv.org/abs/2506.08899</link>
      <description>arXiv:2506.08899v3 Announce Type: replace-cross 
Abstract: We present a comprehensive approach to the automated formalization of legal texts using large language models (LLMs), targeting their transformation into Defeasible Deontic Logic (DDL). Our method employs a structured pipeline that segments complex normative language into atomic snippets, extracts deontic rules, and evaluates them for syntactic and semantic coherence. We introduce a refined success metric that more precisely captures the completeness of formalizations, and a novel two-stage pipeline with a dedicated refinement step to improve logical consistency and coverage. The evaluation procedure has been strengthened with stricter error assessment, and we provide comparative results across multiple LLM configurations, including newly released models and various prompting and fine-tuning strategies. Experiments on legal norms from the Australian Telecommunications Consumer Protections Code demonstrate that, when guided effectively, LLMs can produce formalizations that align closely with expert-crafted representations, underscoring their potential for scalable legal informatics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08899v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LO</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elias Horner, Cristinel Mateis, Guido Governatori, Agata Ciabattoni</dc:creator>
    </item>
    <item>
      <title>Cubic Incompleteness: Hilbert's Tenth Problem at Degree Three</title>
      <link>https://arxiv.org/abs/2510.00759</link>
      <description>arXiv:2510.00759v4 Announce Type: replace-cross 
Abstract: We analyze the cubic fragment $\mathcal D_3$ over $\mathbb N$ by isolating the uniform closure principle any total correct cubic solver would have to realize. In $\mathsf{HA}$ we give a fully constructive, additive and degree-controlled encoding of bounded verification: for each externally fixed bound, we effectively produce a finite system of degree-3 Diophantine equations whose solutions represent the existence of the corresponding finite proof or computation trace. The encoding is purely syntactic, using "gadgets" and "Carryless Pairing". In a classical metatheory (e.g. $\mathsf{PA}$) we show that the global solver hypothesis implies a uniform operator eliminating the bound inside $\mathcal D_3$, which is incompatible with standard non-uniformity/realizability constraints. Hence no uniform cubic can exist clasically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00759v4</guid>
      <category>math.LO</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milan Rosko</dc:creator>
    </item>
    <item>
      <title>Information Physics of Intelligence: Unifying Logical Depth and Entropy under Thermodynamic Constraints</title>
      <link>https://arxiv.org/abs/2511.19156</link>
      <description>arXiv:2511.19156v4 Announce Type: replace-cross 
Abstract: The rapid scaling of artificial intelligence models has revealed a fundamental tension between model capacity (storage) and inference efficiency (computation). While classical information theory focuses on transmission and storage limits, it lacks a unified physical framework to quantify the thermodynamic costs of generating information from compressed laws versus retrieving it from memory. In this paper, we propose a theoretical framework that treats information processing as an enabling mapping from ontological states to carrier states. We introduce a novel metric, Derivation Entropy, which quantifies the effective work required to compute a target state from a given logical depth. By analyzing the interplay between Shannon entropy (storage) and computational complexity (time/energy), we demonstrate the existence of a critical phase transition point. Below this threshold, memory retrieval is thermodynamically favorable; above it, generative computation becomes the optimal strategy. This "Energy-Time-Space" conservation law provides a physical explanation for the efficiency of generative models and offers a rigorous mathematical bound for designing next-generation, energy-efficient AI architectures. Our findings suggest that the minimization of Derivation Entropy is a governing principle for the evolution of both biological and artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19156v4</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <category>math.IT</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianfeng Xu, Zeyan Li</dc:creator>
    </item>
  </channel>
</rss>
