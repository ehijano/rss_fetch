<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Oct 2024 04:01:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A C++ implementation of the discrete adjoint sensitivity analysis method for explicit adaptive Runge-Kutta methods enabled by automatic adjoint differentiation and SIMD vectorization</title>
      <link>https://arxiv.org/abs/2410.01911</link>
      <description>arXiv:2410.01911v1 Announce Type: new 
Abstract: A C++ library for sensitivity analysis of optimisation problems involving ordinary differential equations (ODEs) enabled by automatic differentiation (AD) and SIMD (Single Instruction, Multiple data) vectorization is presented. The discrete adjoint sensitivity analysis method is implemented for adaptive explicit Runge-Kutta (ERK) methods. Automatic adjoint differentiation (AAD) is employed for efficient evaluations of products of vectors and the Jacobian matrix of the right hand side of the ODE system. This approach avoids the low-level drawbacks of the black box approach of employing AAD on the entire ODE solver and opens the possibility to leverage parallelization. SIMD vectorization is employed to compute the vector-Jacobian products concurrently. We study the performance of other methods and implementations of sensitivity analysis and we find that our algorithm presents a small advantage compared to equivalent existing software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01911v1</guid>
      <category>math.NA</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Martins, Evgeny Lakshtanov</dc:creator>
    </item>
    <item>
      <title>Barycentric rational approximation for learning the index of a dynamical system from limited data</title>
      <link>https://arxiv.org/abs/2410.02000</link>
      <description>arXiv:2410.02000v1 Announce Type: new 
Abstract: We consider the task of data-driven identification of dynamical systems, specifically for systems whose behavior at large frequencies is non-standard, as encoded by a non-trivial relative degree of the transfer function or, alternatively, a non-trivial index of a corresponding realization as a descriptor system. We develop novel surrogate modeling strategies that allow state-of-the-art rational approximation algorithms (e.g., AAA and vector fitting) to better handle data coming from such systems with non-trivial relative degree. Our contribution is twofold. On one hand, we describe a strategy to build rational surrogate models with prescribed relative degree, with the objective of mirroring the high-frequency behavior of the high-fidelity problem, when known. The surrogate model's desired degree is achieved through constraints on its barycentric coefficients, rather than through ad-hoc modifications of the rational form. On the other hand, we present a degree-identification routine that allows one to estimate the unknown relative degree of a system from low-frequency data. By identifying the degree of the system that generated the data, we can build a surrogate model that, in addition to matching the data well (at low frequencies), has enhanced extrapolation capabilities (at high frequencies). We showcase the effectiveness and robustness of the newly proposed method through a suite of numerical tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02000v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Pradovera, Ion Victor Gosea, Jan Heiland</dc:creator>
    </item>
    <item>
      <title>First-order empirical interpolation method for real-time solution of parametric time-dependent nonlinear PDEs</title>
      <link>https://arxiv.org/abs/2410.02093</link>
      <description>arXiv:2410.02093v1 Announce Type: new 
Abstract: We present a model reduction approach for the real-time solution of time-dependent nonlinear partial differential equations (PDEs) with parametric dependencies. The approach integrates several ingredients to develop efficient and accurate reduced-order models. Proper orthogonal decomposition is used to construct a reduced-basis (RB) space which provides a rapidly convergent approximation of the parametric solution manifold. The Galerkin projection is employed to reduce the dimensionality of the problem by projecting the weak formulation of the governing PDEs onto the RB space. A major challenge in model reduction for nonlinear PDEs is the efficient treatment of nonlinear terms, which we address by unifying the implementation of several hyperreduction methods. We introduce a first-order empirical interpolation method to approximate the nonlinear terms and recover the computational efficiency. We demonstrate the effectiveness of our methodology through its application to the Allen-Cahn equation, which models phase separation processes, and the Buckley-Leverett equation, which describes two-phase fluid flow in porous media. Numerical results highlight the accuracy, efficiency, and stability of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02093v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ngoc Cuong Nguyen</dc:creator>
    </item>
    <item>
      <title>High-order empirical interpolation methods for real time solution of parametrized nonlinear PDEs</title>
      <link>https://arxiv.org/abs/2410.02100</link>
      <description>arXiv:2410.02100v1 Announce Type: new 
Abstract: We present novel model reduction methods for rapid solution of parametrized nonlinear partial differential equations (PDEs) in real-time or many-query contexts. Our approach combines reduced basis (RB) space for rapidly convergent approximation of the parametric solution manifold, Galerkin projection of the underlying PDEs onto the RB space for dimensionality reduction, and high-order empirical interpolation for efficient treatment of the nonlinear terms. We propose a class of high-order empirical interpolation methods to derive basis functions and interpolation points by using high-order partial derivatives of the nonlinear terms. As these methods can generate high-quality basis functions and interpolation points from a snapshot set of full-order model (FOM) solutions, they significantly improve the approximation accuracy. We develop effective a posteriori estimator to quantify the interpolation errors and construct a parameter sample via greedy sampling. Furthermore, we implement two hyperreduction schemes to construct efficient reduced-order models: one that applies the empirical interpolation before Newton's method and another after. The latter scheme shows flexibility in controlling hyperreduction errors. Numerical results are presented to demonstrate the accuracy and efficiency of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02100v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ngoc Cuong Nguyen</dc:creator>
    </item>
    <item>
      <title>An Efficient Scaled spectral preconditioner for sequences of symmetric positive definite linear systems</title>
      <link>https://arxiv.org/abs/2410.02204</link>
      <description>arXiv:2410.02204v1 Announce Type: new 
Abstract: We explore a scaled spectral preconditioner for the efficient solution of sequences of symmetric and positive-definite linear systems. We design the scaled preconditioner not only as an approximation of the inverse of the linear system but also with consideration of its use within the conjugate gradient (CG) method. We propose three different strategies for selecting a scaling parameter, which aims to position the eigenvalues of the preconditioned matrix in a way that reduces the energy norm of the error, the quantity that CG monotonically decreases at each iteration. Our focus is on accelerating convergence especially in the early iterations, which is particularly important when CG is truncated due to computational cost constraints. Numerical experiments provide in data assimilation confirm that the scaled spectral preconditioner can significantly improve early CG convergence with negligible computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02204v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.28678.38725</arxiv:DOI>
      <dc:creator>Youssef Diouane, Selime G\"urol, Oussama Mouhtal, Dominique Orban</dc:creator>
    </item>
    <item>
      <title>Polynomial approximation of noisy functions</title>
      <link>https://arxiv.org/abs/2410.02317</link>
      <description>arXiv:2410.02317v1 Announce Type: new 
Abstract: Approximating a univariate function on the interval $[-1,1]$ with a polynomial is among the most classical problems in numerical analysis. When the function evaluations come with noise, a least-squares fit is known to reduce the effect of noise as more samples are taken. The generic algorithm for the least-squares problem requires $O(Nn^2)$ operations, where $N+1$ is the number of sample points and $n$ is the degree of the polynomial approximant. This algorithm is unstable when $n$ is large, for example $n\gg \sqrt{N}$ for equispaced sample points. In this study, we blend numerical analysis and statistics to introduce a stable and fast $O(N\log N)$ algorithm called NoisyChebtrunc based on the Chebyshev interpolation. It has the same error reduction effect as least-squares and the convergence is spectral until the error reaches $O(\sigma \sqrt{{n}/{N}})$, where $\sigma$ is the noise level, after which the error continues to decrease at the Monte-Carlo $O(1/\sqrt{N})$ rate. To determine the polynomial degree, NoisyChebtrunc employs a statistical criterion, namely Mallows' $C_p$. We analyze NoisyChebtrunc in terms of the variance and concentration in the infinity norm to the underlying noiseless function. These results show that with high probability the infinity-norm error is bounded by a small constant times $\sigma \sqrt{{n}/{N}}$, when the noise {is} independent and follows a subgaussian or subexponential distribution. We illustrate the performance of NoisyChebtrunc with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02317v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takeru Matsuda, Yuji Nakatsukasa</dc:creator>
    </item>
    <item>
      <title>$hp$-error analysis of mixed-order hybrid high-order methods for elliptic problems on simplicial meshes</title>
      <link>https://arxiv.org/abs/2410.02540</link>
      <description>arXiv:2410.02540v1 Announce Type: new 
Abstract: We present both $hp$-a priori and $hp$-a posteriori error analysis of a mixed-order hybrid high-order (HHO) method to approximate second-order elliptic problems on simplicial meshes. Our main result on the $hp$-a priori error analysis is a $\frac12$-order $p$-suboptimal error estimate. This result is, to our knowledge, the first of this kind for hybrid nonconforming methods and matches the state-of-the-art for other nonconforming methods as discontinuous Galerkin methods. Our second main result is a residual-based $hp$-a posteriori upper error bound, comprising residual, normal flux jump, tangential jump, and stabilization estimators (plus data oscillation terms). The first three terms are $p$-optimal and only the latter is $\frac12$-order $p$-suboptimal. This result is, to our knowledge, the first $hp$-a posteriori error estimate for HHO methods. A novel approach based on the partition-of-unity provided by hat basis functions and on local Helmholtz decompositions on vertex stars is devised to estimate the nonconformity error. Finally, we establish local lower error bounds. Remarkably, the normal flux jump estimator is only $\frac12$-order $p$-suboptimal, as it can be bounded by the stabilization owing to the local conservation property of HHO methods. Numerical examples illustrate the theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02540v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhaonan Dong, Alexandre Ern</dc:creator>
    </item>
    <item>
      <title>A Priori Error Bounds for the Approximate Deconvolution Leray Reduced Order Model</title>
      <link>https://arxiv.org/abs/2410.02673</link>
      <description>arXiv:2410.02673v1 Announce Type: new 
Abstract: The approximate deconvolution Leray reduced order model (ADL-ROM) uses spatial filtering to increase the ROM stability, and approximate deconvolution to increase the ROM accuracy. In the under-resolved numerical simulation of convection-dominated flows, ADL-ROM was shown to be significantly more stable than the standard ROM, and more accurate than the Leray ROM. In this paper, we prove a priori error bounds for the approximate deconvolution operator and ADL-ROM. To our knowledge, these are the first numerical analysis results for approximate deconvolution in a ROM context. We illustrate these numerical analysis results in the numerical simulation of convection-dominated flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02673v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian Moore, Anna Sanfilippo, Francesco Ballarin, Traian Iliescu</dc:creator>
    </item>
    <item>
      <title>Mamba Neural Operator: Who Wins? Transformers vs. State-Space Models for PDEs</title>
      <link>https://arxiv.org/abs/2410.02113</link>
      <description>arXiv:2410.02113v1 Announce Type: cross 
Abstract: Partial differential equations (PDEs) are widely used to model complex physical systems, but solving them efficiently remains a significant challenge. Recently, Transformers have emerged as the preferred architecture for PDEs due to their ability to capture intricate dependencies. However, they struggle with representing continuous dynamics and long-range interactions. To overcome these limitations, we introduce the Mamba Neural Operator (MNO), a novel framework that enhances neural operator-based techniques for solving PDEs. MNO establishes a formal theoretical connection between structured state-space models (SSMs) and neural operators, offering a unified structure that can adapt to diverse architectures, including Transformer-based models. By leveraging the structured design of SSMs, MNO captures long-range dependencies and continuous dynamics more effectively than traditional Transformers. Through extensive analysis, we show that MNO significantly boosts the expressive power and accuracy of neural operators, making it not just a complement but a superior framework for PDE-related tasks, bridging the gap between efficient representation and accurate solution approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02113v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chun-Wun Cheng, Jiahao Huang, Yi Zhang, Guang Yang, Carola-Bibiane Sch\"onlieb, Angelica I Aviles-Rivero</dc:creator>
    </item>
    <item>
      <title>Nonuniform random feature models using derivative information</title>
      <link>https://arxiv.org/abs/2410.02132</link>
      <description>arXiv:2410.02132v1 Announce Type: cross 
Abstract: We propose nonuniform data-driven parameter distributions for neural network initialization based on derivative data of the function to be approximated. These parameter distributions are developed in the context of non-parametric regression models based on shallow neural networks, and compare favorably to well-established uniform random feature models based on conventional weight initialization. We address the cases of Heaviside and ReLU activation functions, and their smooth approximations (sigmoid and softplus), and use recent results on the harmonic analysis and sparse representation of neural networks resulting from fully trained optimal networks. Extending analytic results that give exact representation, we obtain densities that concentrate in regions of the parameter space corresponding to neurons that are well suited to model the local derivatives of the unknown function. Based on these results, we suggest simplifications of these exact densities based on approximate derivative data in the input points that allow for very efficient sampling and lead to performance of random feature models close to optimal networks in several scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02132v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konstantin Pieper, Zezhong Zhang, Guannan Zhang</dc:creator>
    </item>
    <item>
      <title>Quantitative Approximation for Neural Operators in Nonlinear Parabolic Equations</title>
      <link>https://arxiv.org/abs/2410.02151</link>
      <description>arXiv:2410.02151v1 Announce Type: cross 
Abstract: Neural operators serve as universal approximators for general continuous operators. In this paper, we derive the approximation rate of solution operators for the nonlinear parabolic partial differential equations (PDEs), contributing to the quantitative approximation theorem for solution operators of nonlinear PDEs. Our results show that neural operators can efficiently approximate these solution operators without the exponential growth in model complexity, thus strengthening the theoretical foundation of neural operators. A key insight in our proof is to transfer PDEs into the corresponding integral equations via Duahamel's principle, and to leverage the similarity between neural operators and Picard's iteration, a classical algorithm for solving PDEs. This approach is potentially generalizable beyond parabolic PDEs to a range of other equations, including the Navier-Stokes equation, nonlinear Schr\"odinger equations and nonlinear wave equations, which can be solved by Picard's iteration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02151v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takashi Furuya, Koichi Taniguchi, Satoshi Okuda</dc:creator>
    </item>
    <item>
      <title>Deep Learning-Based Prediction of Suspension Dynamics Performance in Multi-Axle Vehicles</title>
      <link>https://arxiv.org/abs/2410.02566</link>
      <description>arXiv:2410.02566v1 Announce Type: cross 
Abstract: This paper presents a deep learning-based framework for predicting the dynamic performance of suspension systems in multi-axle vehicles, emphasizing the integration of machine learning with traditional vehicle dynamics modeling. A Multi-Task Deep Belief Network Deep Neural Network (MTL-DBN-DNN) was developed to capture the relationships between key vehicle parameters and suspension performance metrics. The model was trained on data generated from numerical simulations and demonstrated superior prediction accuracy compared to conventional DNN models. A comprehensive sensitivity analysis was conducted to assess the impact of various vehicle and suspension parameters on dynamic suspension performance. Additionally, the Suspension Dynamic Performance Index (SDPI) was introduced as a holistic measure to quantify overall suspension performance, accounting for the combined effects of multiple parameters. The findings highlight the effectiveness of multitask learning in improving predictive models for complex vehicle systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02566v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Chun Lin, Bo-Yi Lin</dc:creator>
    </item>
    <item>
      <title>An Improved Variational Method for Image Denoising</title>
      <link>https://arxiv.org/abs/2410.02587</link>
      <description>arXiv:2410.02587v1 Announce Type: cross 
Abstract: The total variation (TV) method is an image denoising technique that aims to reduce noise by minimizing the total variation of the image, which measures the variation in pixel intensities. The TV method has been widely applied in image processing and computer vision for its ability to preserve edges and enhance image quality. In this paper, we propose an improved TV model for image denoising and the associated numerical algorithm to carry out the procedure, which is particularly effective in removing several types of noises and their combinations. Our improved model admits a unique solution and the associated numerical algorithm guarantees the convergence. Numerical experiments are demonstrated to show improved effectiveness and denoising quality compared to other TV models. Such encouraging results further enhance the utility of the TV method in image processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02587v1</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing-En Huang, Jia-Wei Liao, Ku-Te Lin, Yu-Ju Tsai, Mei-Heng Yueh</dc:creator>
    </item>
    <item>
      <title>Ranking Perspective for Tree-based Methods with Applications to Symbolic Feature Selection</title>
      <link>https://arxiv.org/abs/2410.02623</link>
      <description>arXiv:2410.02623v1 Announce Type: cross 
Abstract: Tree-based methods are powerful nonparametric techniques in statistics and machine learning. However, their effectiveness, particularly in finite-sample settings, is not fully understood. Recent applications have revealed their surprising ability to distinguish transformations (which we call symbolic feature selection) that remain obscure under current theoretical understanding. This work provides a finite-sample analysis of tree-based methods from a ranking perspective. We link oracle partitions in tree methods to response rankings at local splits, offering new insights into their finite-sample behavior in regression and feature selection tasks. Building on this local ranking perspective, we extend our analysis in two ways: (i) We examine the global ranking performance of individual trees and ensembles, including Classification and Regression Trees (CART) and Bayesian Additive Regression Trees (BART), providing finite-sample oracle bounds, ranking consistency, and posterior contraction results. (ii) Inspired by the ranking perspective, we propose concordant divergence statistics $\mathcal{T}_0$ to evaluate symbolic feature mappings and establish their properties. Numerical experiments demonstrate the competitive performance of these statistics in symbolic feature selection tasks compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02623v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengrui Luo, Meng Li</dc:creator>
    </item>
    <item>
      <title>Ion-Acoustic Wave Dynamics in a Two-Fluid Plasma</title>
      <link>https://arxiv.org/abs/2410.02659</link>
      <description>arXiv:2410.02659v1 Announce Type: cross 
Abstract: Plasma is a medium containing free electrons and cations, where each particle group behaves as a conducting fluid with a single velocity and temperature in the presence of electromagnetic fields. The difference in roles electrons and ions play define the two-fluid description of plasma. This paper examines ion-acoustic waves generated by the particles in both hot and cold plasma using a collisionless "Euler-Poisson" (EP) system. Employing phase-space asymptotic analysis, we establish that for specific wave speeds, EP acquires homoclinic orbits at the steady-state equilibrium and consequently, traveling waves. Combining python and Wolfram Mathematica, we captured visualizations of such behavior in one spatial dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02659v1</guid>
      <category>physics.plasm-ph</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.DS</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emily Kelting, J. Douglas Wright</dc:creator>
    </item>
    <item>
      <title>Lie Algebra Canonicalization: Equivariant Neural Operators under arbitrary Lie Groups</title>
      <link>https://arxiv.org/abs/2410.02698</link>
      <description>arXiv:2410.02698v1 Announce Type: cross 
Abstract: The quest for robust and generalizable machine learning models has driven recent interest in exploiting symmetries through equivariant neural networks. In the context of PDE solvers, recent works have shown that Lie point symmetries can be a useful inductive bias for Physics-Informed Neural Networks (PINNs) through data and loss augmentation. Despite this, directly enforcing equivariance within the model architecture for these problems remains elusive. This is because many PDEs admit non-compact symmetry groups, oftentimes not studied beyond their infinitesimal generators, making them incompatible with most existing equivariant architectures. In this work, we propose Lie aLgebrA Canonicalization (LieLAC), a novel approach that exploits only the action of infinitesimal generators of the symmetry group, circumventing the need for knowledge of the full group structure. To achieve this, we address existing theoretical issues in the canonicalization literature, establishing connections with frame averaging in the case of continuous non-compact groups. Operating within the framework of canonicalization, LieLAC can easily be integrated with unconstrained pre-trained models, transforming inputs to a canonical form before feeding them into the existing model, effectively aligning the input for model inference according to allowed symmetries. LieLAC utilizes standard Lie group descent schemes, achieving equivariance in pre-trained models. Finally, we showcase LieLAC's efficacy on tasks of invariant image classification and Lie point symmetry equivariant neural PDE solvers using pre-trained models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02698v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zakhar Shumaylov, Peter Zaika, James Rowbottom, Ferdia Sherry, Melanie Weber, Carola-Bibiane Sch\"onlieb</dc:creator>
    </item>
    <item>
      <title>Kernel Multi-Grid on Manifolds</title>
      <link>https://arxiv.org/abs/2302.13039</link>
      <description>arXiv:2302.13039v2 Announce Type: replace 
Abstract: Kernel methods for solving partial differential equations on surfaces have the advantage that those methods work intrinsically on the surface and yield high approximation rates if the solution to the partial differential equation is smooth enough. Localized Lagrange bases have proven to alleviate the computational complexity of usual kernel methods to some extent, although the efficient numerical solution of the ill-conditioned linear systems of equations arising from kernel-based Galerkin solutions to PDEs has not been addressed in the literature so far. In this article we apply the framework of the geometric multigrid method with a $\tau\ge 2$-cycle to scattered, quasi-uniform point clouds on the surface. We show that the resulting linear algebra can be accelerated by using the Lagrange function decay, with convergence rates which are obtained by a rigorous analysis. In particular, we can show that the computational cost to solve the linear system scales log-linear in the degrees of freedom.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13039v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Hangelbroek, Christian Rieger</dc:creator>
    </item>
    <item>
      <title>Uniqueness of an inverse electromagnetic coefficient problem with partial boundary data and its numerical resolution through an iterated sensitivity equation</title>
      <link>https://arxiv.org/abs/2309.11931</link>
      <description>arXiv:2309.11931v2 Announce Type: replace 
Abstract: In this paper we study an inverse boundary value problem for Maxwell's equations. The goal is to reconstruct perturbations in the refractive index of the medium inside an object from the knowledge of the tangential trace of an electric field on a part of the boundary of the domain. We first provide a uniqueness result for this inverse problem. Then, we propose a complete procedure to reconstruct numerically the perturbations, based on the minimization of a cost functional involving an iterated sensitivity equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.11931v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\'er\'emy Heleine (IMT)</dc:creator>
    </item>
    <item>
      <title>Quasi-Monte Carlo sparse grid Galerkin finite element methods for linear elasticity equations with uncertainties</title>
      <link>https://arxiv.org/abs/2310.06187</link>
      <description>arXiv:2310.06187v3 Announce Type: replace 
Abstract: We explore a linear inhomogeneous elasticity equation with random Lam\'e parameters. The latter are parameterized by a countably infinite number of terms in separated expansions. The main aim of this work is to estimate expected values (considered as an infinite dimensional integral on the parametric space corresponding to the random coefficients) of linear functionals acting on the solution of the elasticity equation. To achieve this, the expansions of the random parameters are truncated, a high-order quasi-Monte Carlo (QMC) is combined with a sparse grid approach to approximate the high dimensional integral, and a Galerkin finite element method (FEM) is introduced to approximate the solution of the elasticity equation over the physical domain. The error estimates from (1) truncating the infinite expansion, (2) the Galerkin FEM, and (3) the QMC sparse grid quadrature rule are all studied. For this purpose, we show certain required regularity properties of the continuous solution with respect to both the parametric and physical variables. To achieve our theoretical regularity and convergence results, some reasonable assumptions on the expansions of the random coefficients are imposed. Finally, some numerical results are delivered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06187v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Clarke, J. Dick, Q. T. Le Gia, K. Mustapha, T. Tran</dc:creator>
    </item>
    <item>
      <title>Randomized Runge-Kutta-Nystr\"om Methods for Unadjusted Hamiltonian and Kinetic Langevin Monte Carlo</title>
      <link>https://arxiv.org/abs/2310.07399</link>
      <description>arXiv:2310.07399v2 Announce Type: replace 
Abstract: We introduce $5/2$- and $7/2$-order $L^2$-accurate randomized Runge-Kutta-Nystr\"{o}m methods, tailored for approximating Hamiltonian flows within non-reversible Markov chain Monte Carlo samplers, such as unadjusted Hamiltonian Monte Carlo and unadjusted kinetic Langevin Monte Carlo. We establish quantitative $5/2$-order $L^2$-accuracy upper bounds under gradient and Hessian Lipschitz assumptions on the potential energy function. The numerical experiments demonstrate the superior efficiency of the proposed unadjusted samplers on a variety of well-behaved, high-dimensional target distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07399v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nawaf Bou-Rabee, Tore Selland Kleppe</dc:creator>
    </item>
    <item>
      <title>Minimal rank factorizations of polynomial matrices</title>
      <link>https://arxiv.org/abs/2312.00676</link>
      <description>arXiv:2312.00676v2 Announce Type: replace 
Abstract: We investigate rank revealing factorizations of $m \times n$ polynomial matrices $P(\lambda)$ into products of three, $P(\lambda) = L(\lambda) E(\lambda) R(\lambda)$, or two, $P(\lambda) = L(\lambda) R(\lambda)$, polynomial matrices. Among all possible factorizations of these types, we focus on those for which $L(\lambda)$ and/or $R(\lambda)$ is a minimal basis, since they have favorable properties from the point of view of data compression and allow us to relate easily the degree of $P(\lambda)$ with some degree properties of the factors. We call these factorizations minimal rank factorizations. Motivated by the well-known fact that, generically, rank deficient polynomial matrices over the complex field do not have eigenvalues, we pay particular attention to the properties of the minimal rank factorizations of polynomial matrices without eigenvalues. We carefully analyze the degree properties of generic minimal rank factorizations in the set of complex $m \times n$ polynomial matrices with normal rank at most $r&lt; \min \{m,n\}$ and degree at most $d$, and we prove that there are only $rd+1$ different classes of generic factorizations according to the degree properties of the factors and that all of them are of the form $L(\lambda) R(\lambda)$, where the degrees of the $r$ columns of $L(\lambda)$ differ at most by one, the degrees of the $r$ rows of $R(\lambda)$ differ at most by one, and, for each $i=1, \ldots, r$, the sum of the degrees of the $i$th column of $L(\lambda)$ and of the $i$th row of $R(\lambda)$ is equal to $d$. Finally, we show how these sets of polynomial matrices with generic factorizations are related to the sets of polynomial matrices with generic eigenstructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00676v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrii Dmytryshyn, Froil\'an Dopico, Paul Van Dooren</dc:creator>
    </item>
    <item>
      <title>Gaussian Process Regression under Computational and Epistemic Misspecification</title>
      <link>https://arxiv.org/abs/2312.09225</link>
      <description>arXiv:2312.09225v2 Announce Type: replace 
Abstract: Gaussian process regression is a classical kernel method for function estimation and data interpolation. In large data applications, computational costs can be reduced using low-rank or sparse approximations of the kernel. This paper investigates the effect of such kernel approximations on the interpolation error. We introduce a unified framework to analyze Gaussian process regression under important classes of computational misspecification: Karhunen-Lo\`eve expansions that result in low-rank kernel approximations, multiscale wavelet expansions that induce sparsity in the covariance matrix, and finite element representations that induce sparsity in the precision matrix. Our theory also accounts for epistemic misspecification in the choice of kernel parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09225v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Sanz-Alonso, Ruiyi Yang</dc:creator>
    </item>
    <item>
      <title>Improving performance of contour integral-based nonlinear eigensolvers with infinite GMRES</title>
      <link>https://arxiv.org/abs/2403.19309</link>
      <description>arXiv:2403.19309v2 Announce Type: replace 
Abstract: In this work, the infinite GMRES algorithm, recently proposed by Correnty et al., is employed in contour integral-based nonlinear eigensolvers, avoiding the computation of costly factorizations at each quadrature node to solve the linear systems efficiently. Several techniques are applied to make the infinite GMRES memory-friendly, computationally efficient, and numerically stable in practice. More specifically, we analyze the relationship between polynomial eigenvalue problems and their scaled linearizations, and provide a novel weighting strategy which can significantly accelerate the convergence of infinite GMRES in this particular context. We also adopt the technique of TOAR to infinite GMRES to reduce the memory footprint. Theoretical analysis and numerical experiments are provided to illustrate the efficiency of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19309v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuqi Liu, Jose E. Roman, Meiyue Shao</dc:creator>
    </item>
    <item>
      <title>Graph Neural Preconditioners for Iterative Solutions of Sparse Linear Systems</title>
      <link>https://arxiv.org/abs/2406.00809</link>
      <description>arXiv:2406.00809v2 Announce Type: replace 
Abstract: Preconditioning is at the heart of iterative solutions of large, sparse linear systems of equations in scientific disciplines. Several algebraic approaches, which access no information beyond the matrix itself, are widely studied and used, but ill-conditioned matrices remain very challenging. We take a machine learning approach and propose using graph neural networks as a general-purpose preconditioner. They show attractive performance for many problems and can be used when the mainstream preconditioners perform poorly. Empirical evaluation on over 800 matrices suggests that the construction time of these graph neural preconditioners (GNPs) is more predictable and can be much shorter than that of other widely used ones, such as ILU and AMG, while the execution time is faster than using a Krylov method as the preconditioner, such as in inner-outer GMRES. GNPs have a strong potential for solving large-scale, challenging algebraic problems arising from not only partial differential equations, but also economics, statistics, graph, and optimization, to name a few.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00809v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Chen</dc:creator>
    </item>
    <item>
      <title>A Class of Generalized Shift-Splitting Preconditioners for Double Saddle Point Problems</title>
      <link>https://arxiv.org/abs/2408.11750</link>
      <description>arXiv:2408.11750v2 Announce Type: replace 
Abstract: In this paper, we propose a generalized shift-splitting (GSS) preconditioner, along with its two relaxed variants to solve the double saddle point problem (DSPP). The convergence of the associated GSS iterative method is analyzed, and sufficient conditions for its convergence are established. Spectral analyses are performed to derive sharp bounds for the eigenvalues of the preconditioned matrices. Numerical experiments based on examples arising from the PDE-constrained optimization problems demonstrate the effectiveness and robustness of the proposed preconditioners compared with existing state-of-the-art preconditioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11750v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sk. Safique Ahmad, Pinki Khatun</dc:creator>
    </item>
    <item>
      <title>Multilevel Picard approximations overcome the curse of dimensionality when approximating semilinear heat equations with gradient-dependent nonlinearities in $L^p$-sense</title>
      <link>https://arxiv.org/abs/2410.00203</link>
      <description>arXiv:2410.00203v2 Announce Type: replace 
Abstract: We prove that multilevel Picard approximations are capable of approximating solutions of semilinear heat equations in $L^{p}$-sense, ${p}\in [2,\infty)$, in the case of gradient-dependent, Lipschitz-continuous nonlinearities, in the sense that the computational effort of the multilevel Picard approximations grow at most polynomially in both the dimension $d$ and the reciprocal $1/\epsilon$ of the prescribed accuracy $\epsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00203v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuan Anh Nguyen</dc:creator>
    </item>
    <item>
      <title>Convergence analysis of a primal-dual optimization-by-continuation algorithm</title>
      <link>https://arxiv.org/abs/2311.09123</link>
      <description>arXiv:2311.09123v2 Announce Type: replace-cross 
Abstract: We present a numerical iterative optimization algorithm for the minimization of a cost function consisting of a linear combination of three convex terms, one of which is differentiable, a second one is prox-simple and the third one is the composition of a linear map and a prox-simple function. The algorithm's special feature lies in its ability to approximate, in a single iteration run, the minimizers of the cost function for many different values of the parameters determining the relative weight of the three terms in the cost function. A proof of convergence of the algorithm, based on an inexact variable metric approach, is also provided. As a special case, one recovers a generalization of the primal-dual algorithm of Chambolle and Pock, and also of the proximal-gradient algorithm. Finally, we show how it is related to a primal-dual iterative algorithm based on inexact proximal evaluations of the non-smooth terms of the cost function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09123v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cam.2024.116299</arxiv:DOI>
      <dc:creator>Ignace Loris, Simone Rebegoldi</dc:creator>
    </item>
    <item>
      <title>Design Guidelines for Noise-Tolerant Optimization with Applications in Robust Design</title>
      <link>https://arxiv.org/abs/2401.15007</link>
      <description>arXiv:2401.15007v2 Announce Type: replace-cross 
Abstract: The development of nonlinear optimization algorithms capable of performing reliably in the presence of noise has garnered considerable attention lately. This paper advocates for strategies to create noise-tolerant nonlinear optimization algorithms by adapting classical deterministic methods. These adaptations follow certain design guidelines described here, which make use of estimates of the noise level in the problem. The application of our methodology is illustrated by the development of a line search gradient projection method, which is tested on an engineering design problem. It is shown that a new self-calibrated line search and noise-aware finite-difference techniques are effective even in the high noise regime. Numerical experiments investigate the resiliency of key algorithmic components. A convergence analysis of the line search gradient projection method establishes convergence to a neighborhood of stationarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15007v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchen Lou, Shigeng Sun, Jorge Nocedal</dc:creator>
    </item>
  </channel>
</rss>
