<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Dec 2024 05:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>SMART-MC: Sparse Matrix Estimation with Covariate-Based Transitions in Markov Chain Modeling of Multiple Sclerosis Disease Modifying Therapies</title>
      <link>https://arxiv.org/abs/2412.03596</link>
      <description>arXiv:2412.03596v1 Announce Type: new 
Abstract: A Markov model is a widely used tool for modeling sequences of events from a finite state-space and hence can be employed to identify the transition probabilities across treatments based on treatment sequence data. To understand how patient-level covariates impact these treatment transitions, the transition probabilities are modeled as a function of patient covariates. This approach enables the visualization of the effect of patient-level covariates on the treatment transitions across patient visits. The proposed method automatically estimates the entries of the transition matrix with smaller numbers of empirical transitions as constant; the user can set desired cutoff of the number of empirical transition counts required for a particular transition probability to be estimated as a function of covariates. Firstly, this strategy automatically enforces the final estimated transition matrix to contain zeros at the locations corresponding to zero empirical transition counts, avoiding further complicated model constructs to handle sparsity, in an efficient manner. Secondly, it restricts estimation of transition probabilities as a function of covariates, when the number of empirical transitions is particularly small, thus avoiding the identifiability issue which might arise due to the p&gt;n scenario when estimating each transition probability as a function of patient covariates. To optimize the multi-modal likelihood, a parallelized scalable global optimization routine is also developed. The proposed method is applied to understand how the transitions across disease modifying therapies (DMTs) in Multiple Sclerosis (MS) patients are influenced by patient-level demographic and clinical phenotypes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03596v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beomchang Kim, Zongqi Xia, Priyam Das</dc:creator>
    </item>
    <item>
      <title>Hidden Markov graphical models with state-dependent generalized hyperbolic distributions</title>
      <link>https://arxiv.org/abs/2412.03668</link>
      <description>arXiv:2412.03668v1 Announce Type: new 
Abstract: In this paper we develop a novel hidden Markov graphical model to investigate time-varying interconnectedness between different financial markets. To identify conditional correlation structures under varying market conditions and accommodate stylized facts embedded in financial time series, we rely upon the generalized hyperbolic family of distributions with time-dependent parameters evolving according to a latent Markov chain. We exploit its location-scale mixture representation to build a penalized EM algorithm for estimating the state-specific sparse precision matrices by means of an $L_1$ penalty. The proposed approach leads to regime-specific conditional correlation graphs that allow us to identify different degrees of network connectivity of returns over time. The methodology's effectiveness is validated through simulation exercises under different scenarios. In the empirical analysis we apply our model to daily returns of a large set of market indexes, cryptocurrencies and commodity futures over the period 2017-2023.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03668v1</guid>
      <category>stat.ME</category>
      <category>q-fin.ST</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beatrice Foroni, Luca Merlo, Lea Petrella</dc:creator>
    </item>
    <item>
      <title>Using a Two-Parameter Sensitivity Analysis Framework to Efficiently Combine Randomized and Non-randomized Studies</title>
      <link>https://arxiv.org/abs/2412.03731</link>
      <description>arXiv:2412.03731v1 Announce Type: new 
Abstract: Causal inference is vital for informed decision-making across fields such as biomedical research and social sciences. Randomized controlled trials (RCTs) are considered the gold standard for the internal validity of inferences, whereas observational studies (OSs) often provide the opportunity for greater external validity. However, both data sources have inherent limitations preventing their use for broadly valid statistical inferences: RCTs may lack generalizability due to their selective eligibility criterion, and OSs are vulnerable to unobserved confounding. This paper proposes an innovative approach to integrate RCT and OS that borrows the other study's strengths to remedy each study's limitations. The method uses a novel triplet matching algorithm to align RCT and OS samples and a new two-parameter sensitivity analysis framework to quantify internal and external biases. This combined approach yields causal estimates that are more robust to hidden biases than OSs alone and provides reliable inferences about the treatment effect in the general population. We apply this method to investigate the effects of lactation on maternal health using a small RCT and a long-term observational health records dataset from the California National Primate Research Center. This application demonstrates the practical utility of our approach in generating scientifically sound and actionable causal estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03731v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoqi Yu, Bikram Karmakar, Jessica Vandeleest, Eleanor Bimla Schwarz</dc:creator>
    </item>
    <item>
      <title>A Two-stage Approach for Variable Selection in Joint Modeling of Multiple Longitudinal Markers and Competing Risk Outcomes</title>
      <link>https://arxiv.org/abs/2412.03797</link>
      <description>arXiv:2412.03797v1 Announce Type: new 
Abstract: Background: In clinical and epidemiological research, the integration of longitudinal measurements and time-to-event outcomes is vital for understanding relationships and improving risk prediction. However, as the number of longitudinal markers increases, joint model estimation becomes more complex, leading to long computation times and convergence issues. This study introduces a novel two-stage Bayesian approach for variable selection in joint models, illustrated through a practical application.
  Methods: Our approach conceptualizes the analysis in two stages. In the first stage, we estimate one-marker joint models for each longitudinal marker related to the event, allowing for bias reduction from informative dropouts through individual marker trajectory predictions. The second stage employs a proportional hazard model that incorporates expected current values of all markers as time-dependent covariates. We explore continuous and Dirac spike-and-slab priors for variable selection, utilizing Markov chain Monte Carlo (MCMC) techniques.
  Results: The proposed method addresses the challenges of parameter estimation and risk prediction with numerous longitudinal markers, demonstrating robust performance through simulation studies. We further validate our approach by predicting dementia risk using the Three-City (3C) dataset, a longitudinal cohort study from France.
  Conclusions: This two-stage Bayesian method offers an efficient process for variable selection in joint modeling, enhancing risk prediction capabilities in longitudinal studies. The accompanying R package VSJM, which is freely available at https://github.com/tbaghfalaki/VSJM, facilitates implementation, making this approach accessible for diverse clinical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03797v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taban Baghfalaki, Reza Hashemi, Christophe Tzourio, Catherine Helmer, Helene Jacqmin-Gadda</dc:creator>
    </item>
    <item>
      <title>Optimal Correlation for Bernoulli Trials with Covariates</title>
      <link>https://arxiv.org/abs/2412.03827</link>
      <description>arXiv:2412.03827v1 Announce Type: new 
Abstract: Given covariates for $n$ units, each of which is to receive a treatment with probability $1/2$, we study the question of how best to correlate their treatment assignments to minimize the variance of the IPW estimator of the average treatment effect. Past work by \cite{bai2022} found that the optimal stratified experiment is a matched-pair design, where the matching depends on oracle knowledge of the distributions of potential outcomes given covariates. We show that, in the strictly broader class of all admissible correlation structures, the optimal design is to divide the units into two clusters and uniformly assign treatment to exactly one of the two clusters. This design can be computed by solving a 0-1 knapsack problem that uses the same oracle information and can result in an arbitrarily large variance improvement. A shift-invariant version can be constructed by ensuring that exactly half of the units are treated. A method with just two clusters is not robust to a bad proxy for the oracle, and we mitigate this with a hybrid that uses $O(n^\alpha)$ clusters for $0&lt;\alpha&lt;1$. Under certain assumptions, we also derive a CLT for the IPW estimator under our design and a consistent estimator of the variance. We compare our proposed designs to the optimal stratified design in simulated examples and find improved performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03827v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Morrison, Art B. Owen</dc:creator>
    </item>
    <item>
      <title>A Note on the Identifiability of the Degree-Corrected Stochastic Block Model</title>
      <link>https://arxiv.org/abs/2412.03833</link>
      <description>arXiv:2412.03833v1 Announce Type: new 
Abstract: In this short note, we address the identifiability issues inherent in the Degree-Corrected Stochastic Block Model (DCSBM). We provide a rigorous proof demonstrating that the parameters of the DCSBM are identifiable up to a scaling factor and a permutation of the community labels, under a mild condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03833v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Park, Yunpeng Zhao, Ning Hao</dc:creator>
    </item>
    <item>
      <title>Selection of Ultrahigh-Dimensional Interactions Using $L_0$ Penalty</title>
      <link>https://arxiv.org/abs/2412.03918</link>
      <description>arXiv:2412.03918v1 Announce Type: new 
Abstract: Selecting interactions from an ultrahigh-dimensional statistical model with $n$ observations and $p$ variables when $p\gg n$ is difficult because the number of candidates for interactions is $p(p-1)/2$ and a selected model should satisfy the strong hierarchical (SH) restriction. A new method called the SHL0 is proposed to overcome the difficulty. The objective function of the SHL0 method is composed of a loglikelihood function and an $L_0$ penalty. A well-known approach in theoretical computer science called local combinatorial optimization is used to optimize the objective function. We show that any local solution of the SHL0 is consistent and enjoys the oracle properties, implying that it is unnecessary to use a global solution in practice. Three additional advantages are: a tuning parameter is used to penalize the main effects and interactions; a closed-form expression can derive the tuning parameter; and the idea can be extended to arbitrary ultrahigh-dimensional statistical models. The proposed method is more flexible than the previous methods for selecting interactions. A simulation study of the research shows that the proposed SHL0 outperforms its competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03918v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tonglin Zhang</dc:creator>
    </item>
    <item>
      <title>A shiny app for modeling the lifetime in primary breast cancer patients through phase-type distributions</title>
      <link>https://arxiv.org/abs/2412.03975</link>
      <description>arXiv:2412.03975v1 Announce Type: new 
Abstract: Phase-type distributions (PHDs), which are defined as the distribution of the lifetime up to the absorption in an absorbent Markov chain, are an appropriate candidate to model the lifetime of any system, since any non-negative probability distribution can be approximated by a PHD with sufficient precision. Despite PHD potential, friendly statistical programs do not have a module implemented in their interfaces to handle PHD. Thus, researchers must consider others statistical software such as R, Matlab or Python that work with the compilation of code chunks and functions. This fact might be an important handicap for those researchers who do not have sufficient knowledge in programming environments. In this paper, a new interactive web application developed with shiny is introduced in order to adjust PHD to an experimental dataset. This open access app does not require any kind of knowledge about programming or major mathematical concepts. Users can easily compare the graphic fit of several PHDs while estimating their parameters and assess the goodness of fit with just several clicks. All these functionalities are exhibited by means of a numerical simulation and modeling the time to live since the diagnostic in primary breast cancer patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03975v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3934/mbe.2024065 10.3934/mbe.2024065</arxiv:DOI>
      <arxiv:journal_reference>Mathematical Biosciences and Engineering. 2024, Volume 21, Issue 1: 1508-1526</arxiv:journal_reference>
      <dc:creator>Christian Acal, Elena Contreras, Ismael Montero, Juan Eloy Ruiz-Castro</dc:creator>
    </item>
    <item>
      <title>Pseudo-Observations for Bivariate Survival Data</title>
      <link>https://arxiv.org/abs/2412.04109</link>
      <description>arXiv:2412.04109v1 Announce Type: new 
Abstract: The pseudo-observations approach has been gaining popularity as a method to estimate covariate effects on censored survival data. It is used regularly to estimate covariate effects on quantities such as survival probabilities, restricted mean life, cumulative incidence, and others. In this work, we propose to generalize the pseudo-observations approach to situations where a bivariate failure-time variable is observed, subject to right censoring. The idea is to first estimate the joint survival function of both failure times and then use it to define the relevant pseudo-observations. Once the pseudo-observations are calculated, they are used as the response in a generalized linear model. We consider two common nonparametric estimators of the joint survival function: the estimator of Lin and Ying (1993) and the Dabrowska estimator (Dabrowska, 1988). For both estimators, we show that our bivariate pseudo-observations approach produces regression estimates that are consistent and asymptotically normal. Our proposed method enables estimation of covariate effects on quantities such as the joint survival probability at a fixed bivariate time point, or simultaneously at several time points, and consequentially can estimate covariate-adjusted conditional survival probabilities. We demonstrate the method using simulations and an analysis of two real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04109v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yael Travis-Lumer, Micha Mandel, Rebecca A. Betensky</dc:creator>
    </item>
    <item>
      <title>Scoping review of methodology for aiding generalisability and transportability of clinical prediction models</title>
      <link>https://arxiv.org/abs/2412.04275</link>
      <description>arXiv:2412.04275v1 Announce Type: new 
Abstract: Generalisability and transportability of clinical prediction models (CPMs) refer to their ability to maintain predictive performance when applied to new populations. While CPMs may show good generalisability or transportability to a specific new population, it is rare for a CPM to be developed using methods that prioritise good generalisability or transportability. There is an emerging literature of such techniques; therefore, this scoping review aims to summarise the main methodological approaches, assumptions, advantages, disadvantages and future development of methodology aiding the generalisability/transportability. Relevant articles were systematically searched from MEDLINE, Embase, medRxiv, arxiv databases until September 2023 using a predefined set of search terms. Extracted information included methodology description, assumptions, applied examples, advantages and disadvantages. The searches found 1,761 articles; 172 were retained for full text screening; 18 were finally included. We categorised the methodologies according to whether they were data-driven or knowledge-driven, and whether are specifically tailored for target population. Data-driven approaches range from data augmentation to ensemble methods and density ratio weighting, while knowledge-driven strategies rely on causal methodology. Future research could focus on comparison of such methodologies on simulated and real datasets to identify their strengths specific applicability, as well as synthesising these approaches for enhancing their practical usefulness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04275v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kritchavat Ploddi, Matthew Sperrin, Glen P. Martin, Maurice M. O'Connell</dc:creator>
    </item>
    <item>
      <title>Bayesian Perspective for Orientation Estimation in Cryo-EM and Cryo-ET</title>
      <link>https://arxiv.org/abs/2412.03723</link>
      <description>arXiv:2412.03723v1 Announce Type: cross 
Abstract: Accurate orientation estimation is a crucial component of 3D molecular structure reconstruction, both in single-particle cryo-electron microscopy (cryo-EM) and in the increasingly popular field of cryo-electron tomography (cryo-ET). The dominant method, which involves searching for an orientation with maximum cross-correlation relative to given templates, falls short, particularly in low signal-to-noise environments. In this work, we propose a Bayesian framework to develop a more accurate and flexible orientation estimation approach, with the minimum mean square error (MMSE) estimator as a key example. This method effectively accommodates varying structural conformations and arbitrary rotational distributions. Through simulations, we demonstrate that our estimator consistently outperforms the cross-correlation-based method, especially in challenging conditions with low signal-to-noise ratios, and offer a theoretical framework to support these improvements. We further show that integrating our estimator into the iterative refinement in the 3D reconstruction pipeline markedly enhances overall accuracy, revealing substantial benefits across the algorithmic workflow. Finally, we show empirically that the proposed Bayesian approach enhances robustness against the ``Einstein from Noise'' phenomenon, reducing model bias and improving reconstruction reliability. These findings indicate that the proposed Bayesian framework could substantially advance cryo-EM and cryo-ET by enhancing the accuracy, robustness, and reliability of 3D molecular structure reconstruction, thereby facilitating deeper insights into complex biological systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03723v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sheng Xu, Amnon Balanov, Tamir Bendory</dc:creator>
    </item>
    <item>
      <title>Modeling climate extremes using the four-parameter kappa distribution for $r$-largest order statistics</title>
      <link>https://arxiv.org/abs/2007.12031</link>
      <description>arXiv:2007.12031v3 Announce Type: replace 
Abstract: Accurate estimation of the T-year return levels of climate extremes using statistical distribution is a critical step in the projection of future climate and in engineering design for disaster response. We show how the estimation of such quantities can be improved by fitting {the four-parameter kappa distribution for $r$-largest order statistics} (rK4D), which was developed in this study. The rK4D is an extension of {the generalized extreme value distribution for $r$-largest order statistics} (rGEVD), similar to the four-parameter kappa distribution (K4D), which is an extension of the generalized extreme value distribution (GEVD). This new distribution (rK4D) can be useful not only for fitting data when three parameters in the GEVD are not sufficient to capture the variability of the extreme observations, but also in reducing the estimation uncertainty by making use of the r-largest extreme observations instead of only the block maxima. We derive a joint probability density function (PDF) of rK4D and the marginal and conditional cumulative distribution functions and PDFs. To estimate the parameters, the maximum likelihood estimation and the maximum penalized likelihood estimation methods were considered. The usefulness and practical effectiveness of the rK4D are illustrated by the Monte Carlo simulation and by an application to the Bangkok extreme rainfall data. A few new distributions for $r$-largest order statistics are also derived as special cases of the rK4D, such as the $r$-largest logistic, the $r$-largest generalized logistic, and the $r$-largest generalized Gumbel distributions. These distributions for $r$-largest order statistics would be useful in modeling extreme values for many research areas, including hydrology and climatology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.12031v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.wace.2022.100533</arxiv:DOI>
      <arxiv:journal_reference>Weather and Climate Extremes, 39, 100533 (2023)</arxiv:journal_reference>
      <dc:creator>Yire Shin, Jeong-Soo Park</dc:creator>
    </item>
    <item>
      <title>Moment-based Random-effects Meta-analysis Equipped with Huber's M-Estimation</title>
      <link>https://arxiv.org/abs/2407.04446</link>
      <description>arXiv:2407.04446v2 Announce Type: replace 
Abstract: Meta-analyses are commonly used to provide solid evidence across numerous studies. Traditional moment methods, such as the DerSimonian-Laird method, remain popular in spite of the availability of more accurate alternatives. While moment estimators are simple and intuitive, they are known to underestimate the variance of the overall treatment effect, particularly when the number of studies is small. This underestimation can lead to excessively narrow confidence intervals that do not meet the nominal confidence level, potentially resulting in misleading conclusions. In this study, we improve traditional moment-based meta-analysis methods by incorporating Huber's M-estimation to more accurately capture the distributional characteristics of between-study variance. Our approach enables conservative parameter estimation, even when almost all existing methods lead to underestimation of between-study variance under a small number of studies. Additionally, by deriving the simultaneous distribution of overall treatment effect and between-study variance, we propose facilitating a visual exploration of the relationship between these two quantities. Our method provides more reliable estimators for the overall treatment effect and between-study variance, particularly in situations with few studies. Using simulations and real data analysis, we demonstrate that our approach always yields more conservative results compared to traditional moment methods, and ensures more accurate confidence intervals in meta-analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04446v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keisuke Hanada, Tomoyuki Sugimoto</dc:creator>
    </item>
    <item>
      <title>Using Platt's scaling for calibration after undersampling -- limitations and how to address them</title>
      <link>https://arxiv.org/abs/2410.18144</link>
      <description>arXiv:2410.18144v3 Announce Type: replace 
Abstract: When modelling data where the response is dichotomous and highly imbalanced, response-based sampling where a subset of the majority class is retained (i.e., undersampling) is often used to create more balanced training datasets prior to modelling. However, the models fit to this undersampled data, which we refer to as base models, generate predictions that are severely biased. There are several calibration methods that can be used to combat this bias, one of which is Platt's scaling. Here, a logistic regression model is used to model the relationship between the base model's original predictions and the response. Despite its popularity for calibrating models after undersampling, Platt's scaling was not designed for this purpose. Our work presents what we believe is the first detailed study focused on the validity of using Platt's scaling to calibrate models after undersampling. We show analytically, as well as via a simulation study and a case study, that Platt's scaling should not be used for calibration after undersampling without critical thought. If Platt's scaling would have been able to successfully calibrate the base model had it been trained on the entire dataset (i.e., without undersampling), then Platt's scaling might be appropriate for calibration after undersampling. If this is not the case, we recommend a modified version of Platt's scaling that fits a logistic generalized additive model to the logit of the base model's predictions, as it is both theoretically motivated and performed well across the settings considered in our study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18144v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Phelps, Daniel J. Lizotte, Douglas G. Woolford</dc:creator>
    </item>
    <item>
      <title>Finite population inference for skewness measures</title>
      <link>https://arxiv.org/abs/2411.18549</link>
      <description>arXiv:2411.18549v2 Announce Type: replace 
Abstract: In this article we consider Bowley's skewness measure and the Groeneveld-Meeden $b_{3}$ index in the context of finite population sampling. We employ the functional delta method to obtain asymptotic variance formulae for plug-in estimators and propose corresponding variance estimators. We then consider plug-in estimators based on the H\'{a}jek cdf-estimator and on a Deville-S\"arndal type calibration estimator and test the performance of normal confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18549v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leo Pasquazzi</dc:creator>
    </item>
    <item>
      <title>Characterizing the Effects of Environmental Exposures on Social Mobility: Bayesian Semi-parametrics for Principal Stratification</title>
      <link>https://arxiv.org/abs/2412.00311</link>
      <description>arXiv:2412.00311v2 Announce Type: replace 
Abstract: Principal stratification provides a robust causal inference framework for the adjustment of post-treatment variables when comparing the effects of a treatment in health and social sciences. In this paper, we introduce a novel Bayesian nonparametric model for principal stratification, leveraging the dependent Dirichlet process to flexibly model the distribution of potential outcomes. By incorporating confounders and potential outcomes for the post-treatment variable in the Bayesian mixture model for the final outcome, our approach improves the accuracy of missing data imputation and allows for the characterization of treatment effects across strata defined based on the values of the post-treatment variable. We assess the performance of our method through a Monte Carlo simulation study where we compare the proposed method with state-of-the-art Bayesian method in principal stratification. Finally, we leverage the proposed method to evaluate the principal causal effects of exposure to air pollution on social mobility in the US on strata defined by educational attainment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00311v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dafne Zorzetto, Paolo Dalla Torre, Sonia Petrone, Francesca Dominici, Falco J. Bargagli-Stoffi</dc:creator>
    </item>
    <item>
      <title>Marginal Causal Flows for Validation and Inference</title>
      <link>https://arxiv.org/abs/2411.01295</link>
      <description>arXiv:2411.01295v2 Announce Type: replace-cross 
Abstract: Investigating the marginal causal effect of an intervention on an outcome from complex data remains challenging due to the inflexibility of employed models and the lack of complexity in causal benchmark datasets, which often fail to reproduce intricate real-world data patterns. In this paper we introduce Frugal Flows, a novel likelihood-based machine learning model that uses normalising flows to flexibly learn the data-generating process, while also directly inferring the marginal causal quantities from observational data. We propose that these models are exceptionally well suited for generating synthetic data to validate causal methods. They can create synthetic datasets that closely resemble the empirical dataset, while automatically and exactly satisfying a user-defined average treatment effect. To our knowledge, Frugal Flows are the first generative model to both learn flexible data representations and also exactly parameterise quantities such as the average treatment effect and the degree of unobserved confounding. We demonstrate the above with experiments on both simulated and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01295v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel de Vassimon Manela, Laura Battaglia, Robin J. Evans</dc:creator>
    </item>
  </channel>
</rss>
