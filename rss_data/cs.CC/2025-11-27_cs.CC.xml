<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Nov 2025 05:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The communication complexity of distributed estimation</title>
      <link>https://arxiv.org/abs/2511.21015</link>
      <description>arXiv:2511.21015v1 Announce Type: new 
Abstract: We study an extension of the standard two-party communication model in which Alice and Bob hold probability distributions $p$ and $q$ over domains $X$ and $Y$, respectively. Their goal is to estimate \[ \mathbb{E}_{x \sim p,\, y \sim q}[f(x, y)] \] to within additive error $\varepsilon$ for a bounded function $f$, known to both parties. We refer to this as the distributed estimation problem. Special cases of this problem arise in a variety of areas including sketching, databases and learning. Our goal is to understand how the required communication scales with the communication complexity of $f$ and the error parameter $\varepsilon$.
  The random sampling approach -- estimating the mean by averaging $f$ over $O(1/\varepsilon^2)$ random samples -- requires $O(R(f)/\varepsilon^2)$ total communication, where $R(f)$ is the randomized communication complexity of $f$. We design a new debiasing protocol which improves the dependence on $1/\varepsilon$ to be linear instead of quadratic. Additionally we show better upper bounds for several special classes of functions, including the Equality and Greater-than functions. We introduce lower bound techniques based on spectral methods and discrepancy, and show the optimality of many of our protocols: the debiasing protocol is tight for general functions, and that our protocols for the equality and greater-than functions are also optimal. Furthermore, we show that among full-rank Boolean functions, Equality is essentially the easiest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21015v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parikshit Gopalan, Raghu Meka, Prasad Raghavendra, Mihir Singhal, Avi Wigderson</dc:creator>
    </item>
    <item>
      <title>Identifying Codes Kernelization Limitations</title>
      <link>https://arxiv.org/abs/2511.21067</link>
      <description>arXiv:2511.21067v1 Announce Type: new 
Abstract: The Identifying Code (IC) problem seeks a vertex subset whose intersection with every vertex's closed neighborhood is unique, enabling fault detection in multiprocessor systems and practical uses in identity verification, environmental monitoring, and dynamic localization. A closely related problem is the Locating-Dominating Set (LD), which requires each non-dominating vertex to be uniquely identified by its intersection with the set. Cappelle, Gomes, and Santos (2021) proved that LD is W-hard for minimum clique cover and lacks polynomial kernels for parameters such as vertex cover, but their methods did not apply to IC. This paper answers their question by showing that IC does not admit a polynomial kernel parameterized by solution size plus vertex cover unless NP is a subset of coNP/poly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21067v1</guid>
      <category>cs.CC</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aritra Banik, Praneet Kumar Patra, Adele Anna Rescigno, Abhishek Sahu</dc:creator>
    </item>
    <item>
      <title>Efficient Isolation of Perfect Matching in O(log n) Genus Bipartite Graphs</title>
      <link>https://arxiv.org/abs/2511.21217</link>
      <description>arXiv:2511.21217v1 Announce Type: new 
Abstract: We show that given an embedding of an $O(\log n)$ genus bipartite graph, one can construct an edge weight function in logarithmic space, with respect to which the minimum weight perfect matching in the graph is unique, if one exists.
  As a consequence, we obtain that deciding whether such a graph has a perfect matching or not is in SPL. In 1999, Reinhardt, Allender and Zhou proved that if one can construct a polynomially bounded weight function for a graph in logspace such that it isolates a minimum weight perfect matching in the graph, then the perfect matching problem can be solved in SPL. In this paper, we give a deterministic logspace construction of such a weight function for $O(\log n)$ genus bipartite graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21217v1</guid>
      <category>cs.CC</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chetan Gupta, Raghunath Tewari, Vimal Raj Sharma</dc:creator>
    </item>
    <item>
      <title>On the Usefulness of Promises</title>
      <link>https://arxiv.org/abs/2511.21450</link>
      <description>arXiv:2511.21450v1 Announce Type: new 
Abstract: A Boolean predicate $A$ is defined to be promise-useful if $\operatorname{PCSP}(A,B)$ is tractable for some non-trivial $B$ and otherwise it is promise-useless. We initiate investigations of this notion and derive sufficient conditions for both promise-usefulness and promise-uselessness (assuming $\text{P} \ne \text{NP}$). While we do not obtain a complete characterization, our conditions are sufficient to classify all predicates of arity at most $4$ and almost all predicates of arity $5$. We also derive asymptotic results to show that for large arities a vast majority of all predicates are promise-useless.
  Our results are primarily obtained by a thorough study of the "Promise-SAT" problem, in which we are given a $k$-SAT instance with the promise that there is a satisfying assignment for which the literal values of each clause satisfy some additional constraint.
  The algorithmic results are based on the basic LP + affine IP algorithm of Brakensiek et al. (SICOMP, 2020) while we use a number of novel criteria to establish NP-hardness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21450v1</guid>
      <category>cs.CC</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Per Austrin, Johan H{\aa}stad, Bj\"orn Martinsson</dc:creator>
    </item>
    <item>
      <title>Nearly Tight Lower Bounds for Relaxed Locally Decodable Codes via Robust Daisies</title>
      <link>https://arxiv.org/abs/2511.21659</link>
      <description>arXiv:2511.21659v1 Announce Type: new 
Abstract: We show a nearly optimal lower bound on the length of linear relaxed locally decodable codes (RLDCs). Specifically, we prove that any $q$-query linear RLDC $C\colon \{0,1\}^k \to \{0,1\}^n$ must satisfy $n = k^{1+\Omega(1/q)}$. This bound closely matches the known upper bound of $n = k^{1+O(1/q)}$ by Ben-Sasson, Goldreich, Harsha, Sudan, and Vadhan (STOC 2004).
  Our proof introduces the notion of robust daisies, which are relaxed sunflowers with pseudorandom structure, and leverages a new spread lemma to extract dense robust daisies from arbitrary distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21659v1</guid>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Goldberg, Tom Gur, Sidhant Saraogi</dc:creator>
    </item>
    <item>
      <title>Deep Learning as a Convex Paradigm of Computation: Minimizing Circuit Size with ResNets</title>
      <link>https://arxiv.org/abs/2511.20888</link>
      <description>arXiv:2511.20888v1 Announce Type: cross 
Abstract: This paper argues that DNNs implement a computational Occam's razor -- finding the `simplest' algorithm that fits the data -- and that this could explain their incredible and wide-ranging success over more traditional statistical methods. We start with the discovery that the set of real-valued function $f$ that can be $\epsilon$-approximated with a binary circuit of size at most $c\epsilon^{-\gamma}$ becomes convex in the `Harder than Monte Carlo' (HTMC) regime, when $\gamma&gt;2$, allowing for the definition of a HTMC norm on functions. In parallel one can define a complexity measure on the parameters of a ResNets (a weighted $\ell_1$ norm of the parameters), which induce a `ResNet norm' on functions. The HTMC and ResNet norms can then be related by an almost matching sandwich bound. Thus minimizing this ResNet norm is equivalent to finding a circuit that fits the data with an almost minimal number of nodes (within a power of 2 of being optimal). ResNets thus appear as an alternative model for computation of real functions, better adapted to the HTMC regime and its convexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20888v1</guid>
      <category>stat.ML</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Arthur Jacot</dc:creator>
    </item>
    <item>
      <title>Choosing What Game to Play without Selecting Equilibria: Inferring Safe (Pareto) Improvements in Binary Constraint Structures</title>
      <link>https://arxiv.org/abs/2511.21262</link>
      <description>arXiv:2511.21262v1 Announce Type: cross 
Abstract: We consider a setting in which a principal gets to choose which game from some given set is played by a group of agents. The principal would like to choose a game that favors one of the players, the social preferences of the players, or the principal's own preferences. Unfortunately, given the potential multiplicity of equilibria, it is conceptually unclear how to tell which of even any two games is better. Oesterheld et al. (2022) propose that we use assumptions about outcome correspondence -- i.e., about how the outcomes of different games relate -- to allow comparisons in some cases. For example, it seems reasonable to assume that isomorphic games are played isomorphically. From such assumptions we can sometimes deduce that the outcome of one game G' is guaranteed to be better than the outcome of another game G, even if we do not have beliefs about how each of G and G' will be played individually. Following Oesterheld et al., we then call G' a safe improvement on G.
  In this paper, we study how to derive safe improvement relations. We first show that if we are given a set of games and arbitrary assumptions about outcome correspondence between these games, deriving safe improvement relations is co-NP-complete. We then study the (in)completeness of a natural set of inference rules for outcome correspondence. We show that in general the inference rules are incomplete. However, we also show that under natural, generally applicable assumptions about outcome correspondence the rules are complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21262v1</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <category>cs.MA</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.437.22</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 437, 2025, pp. 251-270</arxiv:journal_reference>
      <dc:creator>Caspar Oesterheld (Carnegie Mellon University), Vincent Conitzer (Carnegie Mellon University)</dc:creator>
    </item>
    <item>
      <title>Reductions Between Code Equivalence Problems</title>
      <link>https://arxiv.org/abs/2502.07916</link>
      <description>arXiv:2502.07916v2 Announce Type: replace 
Abstract: In this paper we present two reductions between variants of the Code Equivalence problem. We give polynomial-time Karp reductions from Permutation Code Equivalence (PCE) to both Linear Code Equivalence (LCE) and Signed Permutation Code Equivalence (SPCE). Along with a Karp reduction from SPCE to the Lattice Isomorphism Problem (LIP) proved in a paper by Bennett and Win (2024), our second result implies a reduction from PCE to LIP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07916v2</guid>
      <category>cs.CC</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Cheraghchi, Nikhil Shagrithaya, Alexandra Veliche</dc:creator>
    </item>
    <item>
      <title>Relaxed vs. Full Local Decodability with Few Queries: Equivalence and Separations for Linear Codes</title>
      <link>https://arxiv.org/abs/2511.02633</link>
      <description>arXiv:2511.02633v2 Announce Type: replace 
Abstract: A locally decodable code (LDC) $C \colon \{0,1\}^k \to \{0,1\}^n$ is an error-correcting code that allows one to recover any bit of the original message with good probability while only reading a small number of bits from a corrupted codeword. A relaxed locally decodable code (RLDC) is a weaker notion where the decoder is additionally allowed to abort and output a special symbol $\bot$ if it detects an error. For a large constant number of queries $q$, there is a large gap between the blocklength $n$ of the best $q$-query LDC and the best $q$-query RLDC. Existing constructions of RLDCs achieve polynomial length $n = k^{1 + O(1/q)}$, while the best-known $q$-LDCs only achieve subexponential length $n = 2^{k^{o(1)}}$. On the other hand, for $q = 2$, it is known that RLDCs and LDCs are equivalent. We thus ask the question: what is the smallest $q$ such that there exists a $q$-RLDC that is not a $q$-LDC?
  In this work, we show that any linear $3$-query RLDC is in fact a $3$-LDC, i.e., linear RLDCs and LDCs are equivalent at $3$ queries. More generally, we show for any constant $q$, there is a soundness error threshold $s(q)$ such that any linear $q$-RLDC with soundness error below this threshold must be a $q$-LDC. This implies that linear RLDCs cannot have "strong soundness" -- a stricter condition satisfied by linear LDCs that says the soundness error is proportional to the fraction of errors in the corrupted codeword -- unless they are simply LDCs.
  In addition, we give simple constructions of linear $15$-query RLDCs that are not $q$-LDCs for any constant $q$, showing that for $q = 15$, linear RLDCs and LDCs are not equivalent.
  We also prove nearly identical results for locally correctable codes and their corresponding relaxed counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02633v2</guid>
      <category>cs.CC</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elena Grigorescu, Vinayak M. Kumar, Peter Manohar, Geoffrey Mon</dc:creator>
    </item>
    <item>
      <title>Stiefel optimization is NP-hard</title>
      <link>https://arxiv.org/abs/2507.02839</link>
      <description>arXiv:2507.02839v2 Announce Type: replace-cross 
Abstract: We show that linearly constrained linear optimization over a Stiefel or Grassmann manifold is NP-hard in general. We show that the same is true for unconstrained quadratic optimization over a Stiefel manifold. We will show that unless $\mathrm{P}=\mathrm{NP}$, these optimization problems over a Stiefel manifold do not have $\mathrm{FPTAS}$. As an aside we extend our results to flag manifolds. Combined with earlier findings, this shows that manifold optimization is a difficult endeavor -- even the simplest problems like LP and unconstrained QP are already NP-hard on the most common manifolds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02839v2</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zehua Lai, Lek-Heng Lim, Tianyun Tang</dc:creator>
    </item>
  </channel>
</rss>
