<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Dec 2024 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Massimult: A Novel Parallel CPU Architecture Based on Combinator Reduction</title>
      <link>https://arxiv.org/abs/2412.02765</link>
      <description>arXiv:2412.02765v1 Announce Type: new 
Abstract: The Massimult project aims to design and implement an innovative CPU architecture based on combinator reduction with a novel combinator base and a new abstract machine. The evaluation of programs within this architecture is inherently highly parallel and localized, allowing for faster computation, reduced energy consumption, improved scalability, enhanced reliability, and increased resistance to attacks. In this paper, we introduce the machine language LambdaM, detail its compilation into KVY assembler code, and describe the abstract machine Matrima. The best part of Matrima is its ability to exploit inherent parallelism and locality in combinator reduction, leading to significantly faster computations with lower energy consumption, scalability across multiple processors, and enhanced security against various types of attacks. Matrima can be simulated as a software virtual machine and is intended for future hardware implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02765v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <category>cs.PL</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jurgen Nicklisch-Franken, Ruslan Feizerakhmanov</dc:creator>
    </item>
    <item>
      <title>GoldFish: Serverless Actors with Short-Term Memory State for the Edge-Cloud Continuum</title>
      <link>https://arxiv.org/abs/2412.02867</link>
      <description>arXiv:2412.02867v1 Announce Type: new 
Abstract: Serverless Computing is a computing paradigm that provides efficient infrastructure management and elastic scalability. Serverless functions scale up or down based on demand, which means that functions are not directly addressable and rely on platform-managed invocation. Serverless stateless nature requires functions to leverage external services, such as object storage and KVS, to exchange data. Serverless actors have emerged as a solution to these issues. However, the state-of-the-art serverless lifecycle and event-trigger invocation force actors to leverage remote services to manage their state and exchange data, which impacts the performance and incurs additional costs and dependency on third-party services.
  To address these issues, in this paper, we introduce a novel serverless lifecycle model that allows short-term stateful actors, enabling actors to maintain their state between executions. Additionally, we propose a novel serverless Invocation Model that enables serverless actors to influence the processing of future messages. We present GoldFish, a lightweight WebAssembly short-term stateful serverless actor platform that provides a novel serverless actor lifecycle and invocation model. GoldFish leverages WebAssembly to provide the actors with lightweight sandbox isolation, making them suitable for the Edge-Cloud Continuum, where computational resources are limited. Experimental results show that GoldFish optimizes the data exchange latency by up to 92% and increases the throughput by up to 10x compared to OpenFaaS and Spin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02867v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3703790.3703797</arxiv:DOI>
      <dc:creator>Cynthia Marcelino, Jack Shahhoud, Stefan Nastic</dc:creator>
    </item>
    <item>
      <title>Exploring the Viability of Unikernels for ARM-powered Edge Computing</title>
      <link>https://arxiv.org/abs/2412.03030</link>
      <description>arXiv:2412.03030v1 Announce Type: new 
Abstract: The rapid expansion of IoT devices and their real-time applications have driven a growing need for edge computing. To meet this need, efficient and secure solutions are required for running such applications on resource-constrained devices with limited power, CPU, and memory. Unikernel, with its minimalistic design and application-specific approach, offers a promising alternative to traditional virtualization and container technologies in these environments. The existing research does not thoroughly examine the feasibility of using unikernel for edge computing. This paper investigates the potential of unikernel for ARM-powered edge computing by evaluating the performance and efficiency of three prominent unikernel systems such as OSv, Nanos, and Unikraft against Docker container. We experiment with real-world edge computing applications and utilize key metrics such as boot time, execution time, memory usage, CPU overhead, and network performance to determine how unikernel performs under the constraints of edge devices. Our findings reveal the potential advantages of unikernel in terms of reduced resource consumption and faster startup times while highlighting areas where they may need further optimization for edge deployment. This study provides valuable insights for researchers and practitioners considering unikernel as a lightweight, efficient solution for edge computing on ARM architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03030v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahidullah Kaiser, Ali Saman Tosun, Turgay Korkmaz</dc:creator>
    </item>
    <item>
      <title>Edge System Design Using Containers and Unikernels for IoT Applications</title>
      <link>https://arxiv.org/abs/2412.03032</link>
      <description>arXiv:2412.03032v1 Announce Type: new 
Abstract: Edge computing is emerging as a key enabler of low-latency, high-efficiency processing for the Internet of Things (IoT) and other real-time applications. To support these demands, containerization has gained traction in edge computing due to its lightweight virtualization and efficient resource management. However, there is currently no established framework to leverage both containers and unikernels on edge devices for optimized IoT deployments. This paper proposes a hybrid edge system design that leverages container and unikernel technologies to optimize resource utilization based on application complexity. Containers are employed for resource-intensive applications, e.g., computer vision, providing faster processing, flexibility, and ease of deployment. In contrast, unikernels are used for lightweight applications, offering enhanced resource performance with minimal overhead. Our system design also incorporates container orchestration to efficiently manage multiple instances across the edge efficiently, ensuring scalability and reliability. We demonstrate our hybrid approach's performance and efficiency advantages through real-world computer vision and data science applications on ARM-powered edge device. Our results demonstrate that this hybrid approach improves resource utilization and reduces latency compared to traditional virtualized solutions. This work provides insights into optimizing edge infrastructures, enabling more efficient and specialized deployment strategies for diverse application workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03032v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahidullah Kaiser, Ali Saman Tosun, Turgay Korkmaz</dc:creator>
    </item>
    <item>
      <title>Cost-Performance Evaluation of General Compute Instances: AWS, Azure, GCP, and OCI</title>
      <link>https://arxiv.org/abs/2412.03037</link>
      <description>arXiv:2412.03037v1 Announce Type: new 
Abstract: Cloud computing has become the cornerstone of modern IT infrastructure, offering a wide range of general-purpose instances optimized for diverse workloads. This paper provides a comparative analysis of cost and performance for general-purpose compute instances across four major cloud providers: AWS, Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI). Using standardized configurations of 4 vCPUs and 16 GiB of RAM, the study evaluates instances based on processor architecture (Intel, AMD, ARM), pricing models, and performance benchmarks. Key findings reveal that ARM-based instances deliver superior price-performance ratios for cost-sensitive workloads, while Intel-based instances excel in enterprise-grade applications requiring versatility and reliability. The results aim to guide organizations in selecting the most cost-effective and performance-efficient cloud resources for their specific needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03037v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jay Tharwani, Arnab A Purkayastha</dc:creator>
    </item>
    <item>
      <title>Resource Slicing through Intelligent Orchestration of Energy-aware IoT services in Edge-Cloud Continuum</title>
      <link>https://arxiv.org/abs/2412.03181</link>
      <description>arXiv:2412.03181v1 Announce Type: new 
Abstract: The rapid growth of the Internet of Things (IoT) applications inflicts high requirements for computing resources and network bandwidth. A growing number of service providers are applying edge-cloud computing to improve the quality of their services. Deploying IoT applications to optimal computing nodes to minimize energy consumption and enhance system performance remains an open challenge. In this paper, we present an intelligent orchestration concept for breaking down IoT applications into granular microservices, called nanoservices, and deploying them in an energy-aware manner to optimal computing nodes in the edge-cloud continuum by applying resource and network slicing methods. With this consolidated slicing scheme, we can efficiently allocate network and compute resources to meet the needs of these nanoservices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03181v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3703790.3703831</arxiv:DOI>
      <dc:creator>Hafiz Faheem Shahid, Erkki Harjula</dc:creator>
    </item>
    <item>
      <title>Reactive Orchestration for Hierarchical Federated Learning Under a Communication Cost Budget</title>
      <link>https://arxiv.org/abs/2412.03385</link>
      <description>arXiv:2412.03385v1 Announce Type: new 
Abstract: Deploying a Hierarchical Federated Learning (HFL) pipeline across the computing continuum (CC) requires careful organization of participants into a hierarchical structure with intermediate aggregation nodes between FL clients and the global FL server. This is challenging to achieve due to (i) cost constraints, (ii) varying data distributions, and (iii) the volatile operating environment of the CC. In response to these challenges, we present a framework for the adaptive orchestration of HFL pipelines, designed to be reactive to client churn and infrastructure-level events, while balancing communication cost and ML model accuracy. Our mechanisms identify and react to events that cause HFL reconfiguration actions at runtime, building on multi-level monitoring information (model accuracy, resource availability, resource cost). Moreover, our framework introduces a generic methodology for estimating reconfiguration costs to continuously re-evaluate the quality of adaptation actions, while being extensible to optimize for various HFL performance criteria. By extending the Kubernetes ecosystem, our framework demonstrates the ability to react promptly and effectively to changes in the operating environment, making the best of the available communication cost budget and effectively balancing costs and ML performance at runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03385v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivan \v{C}ili\'c, Anna Lackinger, Pantelis Frangoudis, Ivana Podnar \v{Z}arko, Alireza Furutanpey, Ilir Murturi, Schahram Dustdar</dc:creator>
    </item>
    <item>
      <title>Resource-Adaptive Successive Doubling for Hyperparameter Optimization with Large Datasets on High-Performance Computing Systems</title>
      <link>https://arxiv.org/abs/2412.02729</link>
      <description>arXiv:2412.02729v1 Announce Type: cross 
Abstract: On High-Performance Computing (HPC) systems, several hyperparameter configurations can be evaluated in parallel to speed up the Hyperparameter Optimization (HPO) process. State-of-the-art HPO methods follow a bandit-based approach and build on top of successive halving, where the final performance of a combination is estimated based on a lower than fully trained fidelity performance metric and more promising combinations are assigned more resources over time. Frequently, the number of epochs is treated as a resource, letting more promising combinations train longer. Another option is to use the number of workers as a resource and directly allocate more workers to more promising configurations via data-parallel training. This article proposes a novel Resource-Adaptive Successive Doubling Algorithm (RASDA), which combines a resource-adaptive successive doubling scheme with the plain Asynchronous Successive Halving Algorithm (ASHA). Scalability of this approach is shown on up to 1,024 Graphics Processing Units (GPUs) on modern HPC systems. It is applied to different types of Neural Networks (NNs) and trained on large datasets from the Computer Vision (CV), Computational Fluid Dynamics (CFD), and Additive Manufacturing (AM) domains, where performing more than one full training run is usually infeasible. Empirical results show that RASDA outperforms ASHA by a factor of up to 1.9 with respect to the runtime. At the same time, the solution quality of final ASHA models is maintained or even surpassed by the implicit batch size scheduling of RASDA. With RASDA, systematic HPO is applied to a terabyte-scale scientific dataset for the first time in the literature, enabling efficient optimization of complex models on massive scientific data. The implementation of RASDA is available on https://github.com/olympiquemarcel/rasda</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02729v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marcel Aach, Rakesh Sarma, Helmut Neukirchen, Morris Riedel, Andreas Lintermann</dc:creator>
    </item>
    <item>
      <title>Taurus Database: How to be Fast, Available, and Frugal in the Cloud</title>
      <link>https://arxiv.org/abs/2412.02792</link>
      <description>arXiv:2412.02792v1 Announce Type: cross 
Abstract: Using cloud Database as a Service (DBaaS) offerings instead of on-premise deployments is increasingly common. Key advantages include improved availability and scalability at a lower cost than on-premise alternatives. In this paper, we describe the design of Taurus, a new multi-tenant cloud database system. Taurus separates the compute and storage layers in a similar manner to Amazon Aurora and Microsoft Socrates and provides similar benefits, such as read replica support, low network utilization, hardware sharing and scalability. However, the Taurus architecture has several unique advantages. Taurus offers novel replication and recovery algorithms providing better availability than existing approaches using the same or fewer replicas. Also, Taurus is highly optimized for performance, using no more than one network hop on critical paths and exclusively using append-only storage, delivering faster writes, reduced device wear, and constant-time snapshots. This paper describes Taurus and provides a detailed description and analysis of the storage node architecture, which has not been previously available from the published literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02792v1</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3318464.3386129</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data</arxiv:journal_reference>
      <dc:creator>Alex Depoutovitch, Chong Chen, Jin Chen, Paul Larson, Shu Lin, Jack Ng, Wenlin Cui, Qiang Liu, Wei Huang, Yong Xiao, Yongjun He</dc:creator>
    </item>
    <item>
      <title>QPET: A Versatile and Portable Quantity-of-Interest-preservation Framework for Error-Bounded Lossy Compression</title>
      <link>https://arxiv.org/abs/2412.02799</link>
      <description>arXiv:2412.02799v1 Announce Type: cross 
Abstract: Error-bounded lossy compression has been widely adopted in many scientific domains because it can address the challenges in storing, transferring, and analyzing the unprecedented amount of scientific data. Although error-bounded lossy compression offers general data distortion control by enforcing strict error bounds on raw data, they may fail to meet the quality requirements on the results of downstream analysis derived from raw data, a.k.a Quantities of Interest (QoIs). This may lead to uncertainties and even misinterpretations in scientific discoveries, significantly limiting the use of lossy compression in practice. In this paper, we propose QPET, a novel, versatile, and portable framework for QoI-preserving error-bounded lossy compression, which overcomes the challenges of modeling diverse QoIs by leveraging numerical strategies. QPET features (1) high portability to multiple existing lossy compressors, (2) versatile preservation to most differentiable univariate and multivariate QoIs, and (3) significant compression improvements in QoI-preservation tasks. Experiments with six real-world datasets demonstrate that QPET outperformed existing QoI-preserving compression framework in terms of speed, and integrating QPET into state-of-the-art error-bounded lossy compressors can gain up to 250% compression ratio improvements to original compressors and up to 75% compression ratio improvements to existing QoI-integrated scientific compressors. Under the same level of peak signal-to-noise ratios in the QoIs, QPET can improve the compression ratio by up to 102%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02799v1</guid>
      <category>cs.DB</category>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyang Liu, Pu Jiao, Kai Zhao, Xin Liang, Sheng Di, Franck Cappello</dc:creator>
    </item>
    <item>
      <title>BGTplanner: Maximizing Training Accuracy for Differentially Private Federated Recommenders via Strategic Privacy Budget Allocation</title>
      <link>https://arxiv.org/abs/2412.02934</link>
      <description>arXiv:2412.02934v1 Announce Type: cross 
Abstract: To mitigate the rising concern about privacy leakage, the federated recommender (FR) paradigm emerges, in which decentralized clients co-train the recommendation model without exposing their raw user-item rating data. The differentially private federated recommender (DPFR) further enhances FR by injecting differentially private (DP) noises into clients. Yet, current DPFRs, suffering from noise distortion, cannot achieve satisfactory accuracy. Various efforts have been dedicated to improving DPFRs by adaptively allocating the privacy budget over the learning process. However, due to the intricate relation between privacy budget allocation and model accuracy, existing works are still far from maximizing DPFR accuracy. To address this challenge, we develop BGTplanner (Budget Planner) to strategically allocate the privacy budget for each round of DPFR training, improving overall training performance. Specifically, we leverage the Gaussian process regression and historical information to predict the change in recommendation accuracy with a certain allocated privacy budget. Additionally, Contextual Multi-Armed Bandit (CMAB) is harnessed to make privacy budget allocation decisions by reconciling the current improvement and long-term privacy constraints. Our extensive experimental results on real datasets demonstrate that \emph{BGTplanner} achieves an average improvement of 6.76\% in training performance compared to state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02934v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xianzhi Zhang, Yipeng Zhou, Miao Hu, Di Wu, Pengshan Liao, Mohsen Guizani, Michael Sheng</dc:creator>
    </item>
    <item>
      <title>Partially Conditioned Patch Parallelism for Accelerated Diffusion Model Inference</title>
      <link>https://arxiv.org/abs/2412.02962</link>
      <description>arXiv:2412.02962v1 Announce Type: cross 
Abstract: Diffusion models have exhibited exciting capabilities in generating images and are also very promising for video creation. However, the inference speed of diffusion models is limited by the slow sampling process, restricting its use cases. The sequential denoising steps required for generating a single sample could take tens or hundreds of iterations and thus have become a significant bottleneck. This limitation is more salient for applications that are interactive in nature or require small latency. To address this challenge, we propose Partially Conditioned Patch Parallelism (PCPP) to accelerate the inference of high-resolution diffusion models. Using the fact that the difference between the images in adjacent diffusion steps is nearly zero, Patch Parallelism (PP) leverages multiple GPUs communicating asynchronously to compute patches of an image in multiple computing devices based on the entire image (all patches) in the previous diffusion step. PCPP develops PP to reduce computation in inference by conditioning only on parts of the neighboring patches in each diffusion step, which also decreases communication among computing devices. As a result, PCPP decreases the communication cost by around $70\%$ compared to DistriFusion (the state of the art implementation of PP) and achieves $2.36\sim 8.02\times$ inference speed-up using $4\sim 8$ GPUs compared to $2.32\sim 6.71\times$ achieved by DistriFusion depending on the computing device configuration and resolution of generation at the cost of a possible decrease in image quality. PCPP demonstrates the potential to strike a favorable trade-off, enabling high-quality image generation with substantially reduced latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02962v1</guid>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>XiuYu Zhang, Zening Luo, Michelle E. Lu</dc:creator>
    </item>
    <item>
      <title>Unifying KV Cache Compression for Large Language Models with LeanKV</title>
      <link>https://arxiv.org/abs/2412.03131</link>
      <description>arXiv:2412.03131v1 Announce Type: cross 
Abstract: Large language models (LLMs) demonstrate exceptional performance but incur high serving costs due to substantial memory demands, with the key-value (KV) cache being a primary bottleneck. Existing KV cache compression methods, including quantization and pruning, struggle with limitations such as uniform treatment of keys and values and static memory allocation across attention heads. To address these challenges, we introduce LeanKV, a unified KV cache compression framework that enhances LLM serving efficiency without compromising accuracy through three innovations: (1) Hetero-KV quantization, which stores keys at a higher precision than values to reflect their greater impact on attention computations; (2) per-head dynamic sparsity, which allocates memory based on token importance per head and per request; and (3) unified KV compression, integrating mixed-precision quantization and selective pruning to enable a smooth tradeoff between model accuracy and memory efficiency. To efficiently support these techniques, LeanKV introduces systems optimizations including unified paging and on-GPU parallel memory management. Implemented on vLLM, LeanKV compresses the KV cache by $3.0\times$ to $5.0\times$ without accuracy loss and up to $11.0\times$ with under 5% accuracy loss, enhancing throughput by $1.9\times$ to $2.5\times$, and up to $6.9\times$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03131v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanqi Zhang, Yuwei Hu, Runyuan Zhao, John C. S. Lui, Haibo Chen</dc:creator>
    </item>
    <item>
      <title>Semi-decentralized Training of Spatio-Temporal Graph Neural Networks for Traffic Prediction</title>
      <link>https://arxiv.org/abs/2412.03188</link>
      <description>arXiv:2412.03188v1 Announce Type: cross 
Abstract: In smart mobility, large networks of geographically distributed sensors produce vast amounts of high-frequency spatio-temporal data that must be processed in real time to avoid major disruptions. Traditional centralized approaches are increasingly unsuitable to this task, as they struggle to scale with expanding sensor networks, and reliability issues in central components can easily affect the whole deployment. To address these challenges, we explore and adapt semi-decentralized training techniques for Spatio-Temporal Graph Neural Networks (ST-GNNs) in smart mobility domain. We implement a simulation framework where sensors are grouped by proximity into multiple cloudlets, each handling a subgraph of the traffic graph, fetching node features from other cloudlets to train its own local ST-GNN model, and exchanging model updates with other cloudlets to ensure consistency, enhancing scalability and removing reliance on a centralized aggregator. We perform extensive comparative evaluation of four different ST-GNN training setups -- centralized, traditional FL, server-free FL, and Gossip Learning -- on large-scale traffic datasets, the METR-LA and PeMS-BAY datasets, for short-, mid-, and long-term vehicle speed predictions. Experimental results show that semi-decentralized setups are comparable to centralized approaches in performance metrics, while offering advantages in terms of scalability and fault tolerance. In addition, we highlight often overlooked issues in existing literature for distributed ST-GNNs, such as the variation in model performance across different geographical areas due to region-specific traffic patterns, and the significant communication overhead and computational costs that arise from the large receptive field of GNNs, leading to substantial data transfers and increased computation of partial embeddings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03188v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivan Kralj, Lodovico Giaretta, Gordan Je\v{z}i\'c, Ivana Podnar \v{Z}arko, \v{S}ar\=unas Girdzijauskas</dc:creator>
    </item>
    <item>
      <title>DaVE -- A Curated Database of Visualization Examples</title>
      <link>https://arxiv.org/abs/2408.03188</link>
      <description>arXiv:2408.03188v2 Announce Type: replace 
Abstract: Visualization, from simple line plots to complex high-dimensional visual analysis systems, has established itself throughout numerous domains to explore, analyze, and evaluate data. Applying such visualizations in the context of simulation science where High-Performance Computing (HPC) produces ever-growing amounts of data that is more complex, potentially multidimensional, and multimodal, takes up resources and a high level of technological experience often not available to domain experts. In this work, we present DaVE -- a curated database of visualization examples, which aims to provide state-of-the-art and advanced visualization methods that arise in the context of HPC applications. Based on domain- or data-specific descriptors entered by the user, DaVE provides a list of appropriate visualization techniques, each accompanied by descriptions, examples, references, and resources. Sample code, adaptable container templates, and recipes for easy integration in HPC applications can be downloaded for easy access to high-fidelity visualizations. While the database is currently filled with a limited number of entries based on a broad evaluation of needs and challenges of current HPC users, DaVE is designed to be easily extended by experts from both the visualization and HPC communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03188v2</guid>
      <category>cs.DC</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/VIS55277.2024.00010</arxiv:DOI>
      <dc:creator>Jens Koenen, Marvin Petersen, Christoph Garth, Tim Gerrits</dc:creator>
    </item>
    <item>
      <title>Marconi: Prefix Caching for the Era of Hybrid LLMs</title>
      <link>https://arxiv.org/abs/2411.19379</link>
      <description>arXiv:2411.19379v2 Announce Type: replace 
Abstract: Hybrid models that combine the language modeling capabilities of Attention layers with the efficiency of Recurrent layers (e.g., State Space Models) have gained traction in practically supporting long contexts in Large Language Model serving. Yet, the unique properties of these models complicate the usage of complementary efficiency optimizations such as prefix caching that skip redundant computations across requests. Most notably, their use of in-place state updates for recurrent layers precludes rolling back cache entries for partial sequence overlaps, and instead mandates only exact-match cache hits; the effect is a deluge of (large) cache entries per sequence, most of which yield minimal reuse opportunities. We present Marconi, the first system that supports efficient prefix caching with Hybrid LLMs. Key to Marconi are its novel admission and eviction policies that more judiciously assess potential cache entries based not only on recency, but also on (1) forecasts of their reuse likelihood across a taxonomy of different hit scenarios, and (2) the compute savings that hits deliver relative to memory footprints. Across diverse workloads and Hybrid models, Marconi achieves up to 34.4$\times$ higher token hit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix caching systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19379v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rui Pan, Zhuang Wang, Zhen Jia, Can Karakus, Luca Zancato, Tri Dao, Yida Wang, Ravi Netravali</dc:creator>
    </item>
    <item>
      <title>ML-QLS: Multilevel Quantum Layout Synthesis</title>
      <link>https://arxiv.org/abs/2405.18371</link>
      <description>arXiv:2405.18371v3 Announce Type: replace-cross 
Abstract: Quantum Layout Synthesis (QLS) plays a crucial role in optimizing quantum circuit execution on physical quantum devices. As we enter the era where quantum computers have hundreds of qubits, we are faced with scalability issues using optimal approaches and degrading heuristic methods' performance due to the lack of global optimization. To this end, we introduce a hybrid design that obtains the much improved solution for the heuristic method utilizing the multilevel framework, which is an effective methodology to solve large-scale problems in VLSI design. In this paper, we present ML-QLS, the first multilevel quantum layout tool with a scalable refinement operation integrated with novel cost functions and clustering strategies. Our clustering provides valuable insights into generating a proper problem approximation for quantum circuits and devices. Our experimental results demonstrate that ML-QLS can scale up to problems involving hundreds of qubits and achieve a remarkable 52% performance improvement over leading heuristic QLS tools for large circuits, which underscores the effectiveness of multilevel frameworks in quantum applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18371v3</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wan-Hsuan Lin, Jason Cong</dc:creator>
    </item>
    <item>
      <title>MQFL-FHE: Multimodal Quantum Federated Learning Framework with Fully Homomorphic Encryption</title>
      <link>https://arxiv.org/abs/2412.01858</link>
      <description>arXiv:2412.01858v2 Announce Type: replace-cross 
Abstract: The integration of fully homomorphic encryption (FHE) in federated learning (FL) has led to significant advances in data privacy. However, during the aggregation phase, it often results in performance degradation of the aggregated model, hindering the development of robust representational generalization. In this work, we propose a novel multimodal quantum federated learning framework that utilizes quantum computing to counteract the performance drop resulting from FHE. For the first time in FL, our framework combines a multimodal quantum mixture of experts (MQMoE) model with FHE, incorporating multimodal datasets for enriched representation and task-specific learning. Our MQMoE framework enhances performance on multimodal datasets and combined genomics and brain MRI scans, especially for underrepresented categories. Our results also demonstrate that the quantum-enhanced approach mitigates the performance degradation associated with FHE and improves classification accuracy across diverse datasets, validating the potential of quantum interventions in enhancing privacy in FL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01858v2</guid>
      <category>quant-ph</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddhant Dutta, Nouhaila Innan, Sadok Ben Yahia, Muhammad Shafique, David Esteban Bernal Neira</dc:creator>
    </item>
  </channel>
</rss>
