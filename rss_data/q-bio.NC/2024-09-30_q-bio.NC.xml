<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Oct 2024 03:15:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Heterogeneous populations of quadratic integrate-and-fire neurons: on the generality of Lorentzian distributions</title>
      <link>https://arxiv.org/abs/2409.18278</link>
      <description>arXiv:2409.18278v1 Announce Type: new 
Abstract: Over the last decade, next-generation neural mass models have become increasingly prominent in mathematical neuroscience. These models link microscopic dynamics with low-dimensional systems of so-called firing rate equations that exactly capture the collective dynamics of large populations of heterogeneous quadratic integrate-and-fire (QIF) neurons. A particularly tractable type of heterogeneity is the distribution of the QIF neurons' excitability parameters, or inputs, according to a Lorentzian. While other distributions -- such as those approximating Gaussian or uniform distributions -- admit to exact mean-field reductions, they result in more complex firing rate equations that are challenging to analyze, and it remains unclear whether they produce comparable collective dynamics. Here, we first demonstrate why Lorentzian heterogeneity is analytically favorable and, second, identify when it leads to qualitatively different collective dynamics compared to other types of heterogeneity. A stationary mean-field approach enables us to derive explicit expressions for the distributions of the neurons' firing rates and voltages in macroscopic stationary states with arbitrary heterogeneities. We also explicate the exclusive relationship between Lorentzian distributed inputs and Lorentzian distributed voltages, whose width happens to coincide with the population firing rate. A dynamic mean-field approach for unimodal heterogeneities further allows us to comprehensively analyze and compare collective dynamics. We find that different types of heterogeneity typically yield qualitatively similar dynamics. However, when gap junction coupling is present, Lorentzian heterogeneity induces nonuniversal behavior, obscuring a diversity-induced transition to synchrony.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18278v1</guid>
      <category>q-bio.NC</category>
      <category>nlin.AO</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bastian Pietras, Ernest Montbri\'o</dc:creator>
    </item>
    <item>
      <title>Harnessing and modulating chaos to sample from neural generative models</title>
      <link>https://arxiv.org/abs/2409.18329</link>
      <description>arXiv:2409.18329v1 Announce Type: new 
Abstract: Chaos is generic in strongly-coupled recurrent networks of model neurons, and thought to be an easily accessible dynamical regime in the brain. While neural chaos is typically seen as an impediment to robust computation, we show how such chaos might play a functional role in allowing the brain to learn and sample from generative models. We construct architectures that combine a classic model of neural chaos either with a canonical generative modeling architecture or with energy-based models of neural memory. We show that these architectures have appealing properties for sampling, including easy biologically-plausible control of sampling rates via overall gain modulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18329v1</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <category>nlin.CD</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rishidev Chaudhuri, Vivek Handebagh</dc:creator>
    </item>
    <item>
      <title>A Framework for Standardizing Similarity Measures in a Rapidly Evolving Field</title>
      <link>https://arxiv.org/abs/2409.18333</link>
      <description>arXiv:2409.18333v1 Announce Type: new 
Abstract: Similarity measures are fundamental tools for quantifying the alignment between artificial and biological systems. However, the diversity of similarity measures and their varied naming and implementation conventions makes it challenging to compare across studies. To facilitate comparisons and make explicit the implementation choices underlying a given code package, we have created and are continuing to develop a Python repository that benchmarks and standardizes similarity measures. The goal of creating a consistent naming convention that uniquely and efficiently specifies a similarity measure is not trivial as, for example, even commonly used methods like Centered Kernel Alignment (CKA) have at least 12 different variations, and this number will likely continue to grow as the field evolves. For this reason, we do not advocate for a fixed, definitive naming convention. The landscape of similarity measures and best practices will continue to change and so we see our current repository, which incorporates approximately 100 different similarity measures from 14 packages, as providing a useful tool at this snapshot in time. To accommodate the evolution of the field we present a framework for developing, validating, and refining naming conventions with the goal of uniquely and efficiently specifying similarity measures, ultimately making it easier for the community to make comparisons across studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18333v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Cloos, Guangyu Robert Yang, Christopher J. Cueva</dc:creator>
    </item>
    <item>
      <title>Heterogeneous quantization regularizes spiking neural network activity</title>
      <link>https://arxiv.org/abs/2409.18396</link>
      <description>arXiv:2409.18396v1 Announce Type: new 
Abstract: The learning and recognition of object features from unregulated input has been a longstanding challenge for artificial intelligence systems. Brains are adept at learning stable representations given small samples of noisy observations; across sensory modalities, this capacity is aided by a cascade of signal conditioning steps informed by domain knowledge. The olfactory system, in particular, solves a source separation and denoising problem compounded by concentration variability, environmental interference, and unpredictably correlated sensor affinities. To function optimally, its plastic network requires statistically well-behaved input. We present a data-blind neuromorphic signal conditioning strategy whereby analog data are normalized and quantized into spike phase representations. Input is delivered to a column of duplicated spiking principal neurons via heterogeneous synaptic weights; this regularizes layer utilization, yoking total activity to the network's operating range and rendering internal representations robust to uncontrolled open-set stimulus variance. We extend this mechanism by adding a data-aware calibration step whereby the range and density of the quantization weights adapt to accumulated input statistics, optimizing resource utilization by balancing activity regularization and information retention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18396v1</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roy Moyal, Kyrus R. Mama, Matthew Einhorn, Ayon Borthakur, Thomas A. Cleland</dc:creator>
    </item>
    <item>
      <title>Unconditional stability of a recurrent neural circuit implementing divisive normalization</title>
      <link>https://arxiv.org/abs/2409.18946</link>
      <description>arXiv:2409.18946v1 Announce Type: new 
Abstract: Stability in recurrent neural models poses a significant challenge, particularly in developing biologically plausible neurodynamical models that can be seamlessly trained. Traditional cortical circuit models are notoriously difficult to train due to expansive nonlinearities in the dynamical system, leading to an optimization problem with nonlinear stability constraints that are difficult to impose. Conversely, recurrent neural networks (RNNs) excel in tasks involving sequential data but lack biological plausibility and interpretability. In this work, we address these challenges by linking dynamic divisive normalization (DN) to the stability of ORGaNICs, a biologically plausible recurrent cortical circuit model that dynamically achieves DN and has been shown to simulate a wide range of neurophysiological phenomena. By using the indirect method of Lyapunov, we prove the remarkable property of unconditional local stability for an arbitrary-dimensional ORGaNICs circuit when the recurrent weight matrix is the identity. We thus connect ORGaNICs to a system of coupled damped harmonic oscillators, which enables us to derive the circuit's energy function, providing a normative principle of what the circuit, and individual neurons, aim to accomplish. Further, for a generic recurrent weight matrix, we prove the stability of the 2D model and demonstrate empirically that stability holds in higher dimensions. Finally, we show that ORGaNICs can be trained by backpropagation through time without gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which address the problems of exploding, vanishing, and oscillating gradients. By evaluating the model's performance on RNN benchmarks, we find that ORGaNICs outperform alternative neurodynamical models on static image classification tasks and perform comparably to LSTMs on sequential tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18946v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shivang Rawat, David J. Heeger, Stefano Martiniani</dc:creator>
    </item>
    <item>
      <title>AM-MTEEG: Multi-task EEG classification based on impulsive associative memory</title>
      <link>https://arxiv.org/abs/2409.18375</link>
      <description>arXiv:2409.18375v1 Announce Type: cross 
Abstract: Electroencephalogram-based brain-computer interface (BCI) has potential applications in various fields, but their development is hindered by limited data and significant cross-individual variability. Inspired by the principles of learning and memory in the human hippocampus, we propose a multi-task (MT) classification model, called AM-MTEEG, which combines learning-based impulsive neural representations with bidirectional associative memory (AM) for cross-individual BCI classification tasks. The model treats the EEG classification of each individual as an independent task and facilitates feature sharing across individuals. Our model consists of an impulsive neural population coupled with a convolutional encoder-decoder to extract shared features and a bidirectional associative memory matrix to map features to class. Experimental results in two BCI competition datasets show that our model improves average accuracy compared to state-of-the-art models and reduces performance variance across individuals, and the waveforms reconstructed by the bidirectional associative memory provide interpretability for the model's classification results. The neuronal firing patterns in our model are highly coordinated, similarly to the neural coding of hippocampal neurons, indicating that our model has biological similarities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18375v1</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyan Li, Bin Hu, Zhi-Hong Guan</dc:creator>
    </item>
    <item>
      <title>Latent Representation Learning for Multimodal Brain Activity Translation</title>
      <link>https://arxiv.org/abs/2409.18462</link>
      <description>arXiv:2409.18462v1 Announce Type: cross 
Abstract: Neuroscience employs diverse neuroimaging techniques, each offering distinct insights into brain activity, from electrophysiological recordings such as EEG, which have high temporal resolution, to hemodynamic modalities such as fMRI, which have increased spatial precision. However, integrating these heterogeneous data sources remains a challenge, which limits a comprehensive understanding of brain function. We present the Spatiotemporal Alignment of Multimodal Brain Activity (SAMBA) framework, which bridges the spatial and temporal resolution gaps across modalities by learning a unified latent space free of modality-specific biases. SAMBA introduces a novel attention-based wavelet decomposition for spectral filtering of electrophysiological recordings, graph attention networks to model functional connectivity between functional brain units, and recurrent layers to capture temporal autocorrelations in brain signal. We show that the training of SAMBA, aside from achieving translation, also learns a rich representation of brain information processing. We showcase this classify external stimuli driving brain activity from the representation learned in hidden layers of SAMBA, paving the way for broad downstream applications in neuroscience research and clinical contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18462v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arman Afrasiyabi, Dhananjay Bhaskar, Erica L. Busch, Laurent Caplette, Rahul Singh, Guillaume Lajoie, Nicholas B. Turk-Browne, Smita Krishnaswamy</dc:creator>
    </item>
    <item>
      <title>Toward Universal and Interpretable World Models for Open-ended Learning Agents</title>
      <link>https://arxiv.org/abs/2409.18676</link>
      <description>arXiv:2409.18676v1 Announce Type: cross 
Abstract: We introduce a generic, compositional and interpretable class of generative world models that supports open-ended learning agents. This is a sparse class of Bayesian networks capable of approximating a broad range of stochastic processes, which provide agents with the ability to learn world models in a manner that may be both interpretable and computationally scalable. This approach integrating Bayesian structure learning and intrinsically motivated (model-based) planning enables agents to actively develop and refine their world models, which may lead to open-ended learning and more robust, adaptive behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18676v1</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lancelot Da Costa</dc:creator>
    </item>
    <item>
      <title>Mapping effective connectivity by virtually perturbing a surrogate brain</title>
      <link>https://arxiv.org/abs/2301.00148</link>
      <description>arXiv:2301.00148v4 Announce Type: replace 
Abstract: Effective connectivity (EC), indicative of the causal interactions between brain regions, is fundamental to understanding information processing in the brain. Traditional approaches, which infer EC from neural responses to stimulations, are not suited for mapping whole-brain EC in humans due to being invasive and having limited spatial coverage of stimulations. To address this gap, we present Neural Perturbational Inference (NPI), a data-driven framework designed to map EC across the entire brain. NPI employs an artificial neural network trained to learn large-scale neural dynamics as a computational surrogate of the brain. NPI maps EC by perturbing each region of the surrogate brain and observing the resulting responses in all other regions. NPI captures the directionality, strength, and excitatory/inhibitory properties of brain-wide EC. Our validation of NPI, using models having ground-truth EC, shows its superiority over Granger causality and dynamic causal modeling. Applying NPI to resting-state fMRI data from diverse datasets reveals consistent and structurally supported EC. Further validation using a cortico-cortical evoked potentials dataset reveals a significant correlation between NPI-inferred EC and real stimulation propagation pathways. By transitioning from correlational to causal understandings of brain functionality, NPI marks a stride in decoding the brain's functional architecture and facilitating both neuroscience research and clinical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.00148v4</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zixiang Luo, Kaining Peng, Zhichao Liang, Shengyuan Cai, Chenyu Xu, Dan Li, Yu Hu, Changsong Zhou, Quanying Liu</dc:creator>
    </item>
    <item>
      <title>ELiSe: Efficient Learning of Sequences in Structured Recurrent Networks</title>
      <link>https://arxiv.org/abs/2402.16763</link>
      <description>arXiv:2402.16763v2 Announce Type: replace 
Abstract: Behavior can be described as a temporal sequence of actions driven by neural activity. To learn complex sequential patterns in neural networks, memories of past activities need to persist on significantly longer timescales than the relaxation times of single-neuron activity. While recurrent networks can produce such long transients, training these networks is a challenge. Learning via error propagation confers models such as FORCE, RTRL or BPTT a significant functional advantage, but at the expense of biological plausibility. While reservoir computing circumvents this issue by learning only the readout weights, it does not scale well with problem complexity. We propose that two prominent structural features of cortical networks can alleviate these issues: the presence of a certain network scaffold at the onset of learning and the existence of dendritic compartments for enhancing neuronal information storage and computation. Our resulting model for Efficient Learning of Sequences (ELiSe) builds on these features to acquire and replay complex non-Markovian spatio-temporal patterns using only local, always-on and phase-free synaptic plasticity. We showcase the capabilities of ELiSe in a mock-up of birdsong learning, and demonstrate its flexibility with respect to parametrization, as well as its robustness to external disturbances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16763v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Kriener, Kristin V\"olk, Ben von H\"unerbein, Federico Benitez, Walter Senn, Mihai A. Petrovici</dc:creator>
    </item>
    <item>
      <title>Molecular Quantum (MolQ) Communication Channel in the Gut-Brain Axis Synapse</title>
      <link>https://arxiv.org/abs/2407.07106</link>
      <description>arXiv:2407.07106v2 Announce Type: replace 
Abstract: The gut-brain axis is the communication link between the gut and the brain. Although it is known that the gut-brain axis plays a pivotal role in homeostasis, its overall mechanism is still not known. However, for neural synapses, classical molecular communication is described by the formation of ligand-receptor complexes, which leads to the opening of ion channels. Moreover, there are some conditions that need to be fulfilled before the opening of the ion channel. In this study, we consider the gut-brain axis, where neurotransmitters diffuse through the synaptic cleft, considering molecular communication. On the vagus nerve (VN) membrane, i.e., the post-synaptic membrane of the synapse, it undergoes a quantum communication (QC), which initiates the opening of the ion channel, thus initiating the communication signal from the gut to the brain. It evolves a new paradigm of communication approach, Molecular Quantum (MolQ) communication. Based on the QC model, we theoretically analyze the output states, and QC is simulated considering the incoming neurotransmitter's concentration and validated by analyzing the entropy and the mutual information of the input, i.e., neurotransmitter's concentration, and output, i.e., ion channel opening.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07106v2</guid>
      <category>q-bio.NC</category>
      <category>quant-ph</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TMBMC.2024.3462727</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Molecular, Biological, and Multi-Scale Communications (2024)</arxiv:journal_reference>
      <dc:creator>Bitop Maitra, Ozgur B. Akan</dc:creator>
    </item>
    <item>
      <title>Generalisation to unseen topologies: Towards control of biological neural network activity</title>
      <link>https://arxiv.org/abs/2407.12789</link>
      <description>arXiv:2407.12789v2 Announce Type: replace 
Abstract: Novel imaging and neurostimulation techniques open doors for advancements in closed-loop control of activity in biological neural networks. This would allow for applications in the investigation of activity propagation, and for diagnosis and treatment of pathological behaviour. Due to the partially observable characteristics of activity propagation, through networks in which edges can not be observed, and the dynamic nature of neuronal systems, there is a need for adaptive, generalisable control. In this paper, we introduce an environment that procedurally generates neuronal networks with different topologies to investigate this generalisation problem. Additionally, an existing transformer-based architecture is adjusted to evaluate the generalisation performance of a deep RL agent in the presented partially observable environment. The agent demonstrates the capability to generalise control from a limited number of training networks to unseen test networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12789v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laurens Engwegen, Daan Brinks, Wendelin B\"ohmer</dc:creator>
    </item>
    <item>
      <title>Evolutionary emergence of biological intelligence</title>
      <link>https://arxiv.org/abs/2409.04928</link>
      <description>arXiv:2409.04928v2 Announce Type: replace 
Abstract: Characterising the intelligence of biological organisms is challenging. This work considers intelligent algorithms developed evolutionarily within neural systems. Mathematical analyses unveil a natural equivalence between canonical neural networks, variational Bayesian inference under a class of partially observable Markov decision processes, and differentiable Turing machines, by showing that they minimise the shared Helmholtz energy. Consequently, canonical neural networks can biologically plausibly equip Turing machines and conduct variational Bayesian inferences of external Turing machines in the environment. Applying Helmholtz energy minimisation at the species level facilitates deriving active Bayesian model selection inherent in natural selection, resulting in the emergence of adaptive algorithms. In particular, canonical neural networks with two mental actions can separately memorise transition mappings of multiple external Turing machines to form a universal machine. These propositions were corroborated by numerical simulations of algorithm implementation and neural network evolution. These notions offer a universal characterisation of biological intelligence emerging from evolution in terms of Bayesian model selection and belief updating.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04928v2</guid>
      <category>q-bio.NC</category>
      <category>q-bio.PE</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Isomura</dc:creator>
    </item>
  </channel>
</rss>
