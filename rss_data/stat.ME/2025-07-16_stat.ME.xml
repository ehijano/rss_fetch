<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Jul 2025 01:31:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Enhancing Wildlife Density Estimation: A New Two-Parameter Detection Function for Line Transect Sampling</title>
      <link>https://arxiv.org/abs/2507.10572</link>
      <description>arXiv:2507.10572v1 Announce Type: new 
Abstract: Accurate estimation of wildlife density is vital for effective ecological monitoring, conservation, and management. Line transect sampling, a central technique in distance sampling, relies on selecting an appropriate detection function to model the probability of detecting individuals as a function of their distance from the transect line. In this study, we propose a novel two-parameter detection function that extends the flexibility of traditional models such as the half-normal and exponential, while retaining interpretability and computational tractability. Notably, one of the parameters is assumed to take a known integer value, allowing us to explore a range of detection curve shapes by varying this parameter across different settings in our computational analysis. This structure enables the model to capture a broader spectrum of detection patterns, especially in cases where classical models fall short. The proposed method is evaluated through extensive simulation studies and applied to real ecological survey data. The results show that the new model consistently yields improved fit and more accurate estimates of animal density, offering ecologists a practical and robust alternative for use in diverse field conditions</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10572v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Midhat M. Edous, Omar M. Eidous</dc:creator>
    </item>
    <item>
      <title>Estimation de la tendance-cycle avec des m\'ethodes robustes aux points atypiques</title>
      <link>https://arxiv.org/abs/2507.10704</link>
      <description>arXiv:2507.10704v1 Announce Type: new 
Abstract: Seasonally adjusted series are usually used to analyse the business cycle and turning points. When the irregular is too high, it is preferable to smooth the series in order to analyse the trend-cycle component directly. This study focuses on the real-time estimation of the trend-cycle component around shocks and turning points. The linear moving averages classically used for estimating the trend-cycle, which are sensitive to the presence of atypical points, are compared with robust non-linear methods. We also propose a methodology for extending the Henderson and Musgrave moving averages to take account of external information and thus construct moving averages that are robust to the presence of certain shocks. We describe how to estimate confidence intervals for estimates derived from moving averages, thereby validating the use of these new moving averages. By comparing the methods on simulated and real series, we show that: building robust moving averages makes it possible to reduce revisions and better model turning points around shocks, without degrading the estimates when no shock is observed; robust non-linear methods do not make it possible to extract a trend-cycle component that is satisfactory for economic analysis, with sometimes significant revisions. This study is fully reproducible and all the codes used are available under https://github.com/AQLT/robustMA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10704v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alain Quartier-la-Tente</dc:creator>
    </item>
    <item>
      <title>Optimal Debiased Inference on Privatized Data via Indirect Estimation and Parametric Bootstrap</title>
      <link>https://arxiv.org/abs/2507.10746</link>
      <description>arXiv:2507.10746v1 Announce Type: new 
Abstract: We design a debiased parametric bootstrap framework for statistical inference from differentially private data. Existing usage of the parametric bootstrap on privatized data ignored or avoided handling the effect of clamping, a technique employed by the majority of privacy mechanisms. Ignoring the impact of clamping often leads to under-coverage of confidence intervals and miscalibrated type I errors of hypothesis tests. The main reason for the failure of the existing methods is the inconsistency of the parameter estimate based on the privatized data. We propose using the indirect inference method to estimate the parameter values consistently, and we use the improved estimator in parametric bootstrap for inference. To implement the indirect estimator, we present a novel simulation-based, adaptive approach along with the theory that establishes the consistency of the corresponding parametric bootstrap estimates, confidence intervals, and hypothesis tests. In particular, we prove that our adaptive indirect estimator achieves the minimum asymptotic variance among all "well-behaved" consistent estimators based on the released summary statistic. Our simulation studies show that our framework produces confidence intervals with well-calibrated coverage and performs hypothesis testing with the correct type I error, giving state-of-the-art performance for inference on location-scale normals, simple linear regression, and logistic regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10746v1</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhanyu Wang, Arin Chang, Jordan Awan</dc:creator>
    </item>
    <item>
      <title>Propensity score weighting across counterfactual worlds: longitudinal effects under positivity violations</title>
      <link>https://arxiv.org/abs/2507.10774</link>
      <description>arXiv:2507.10774v1 Announce Type: new 
Abstract: When examining a contrast between two interventions, longitudinal causal inference studies frequently encounter positivity violations when one or both regimes are impossible to observe for some subjects. Existing weighting methods either assume positivity holds or produce effects that conflate interventions' impacts on ultimate outcomes with their effects on intermediate treatments and covariates. We propose a novel class of estimands -- cumulative cross-world weighted effects -- that weights potential outcome differences using propensity scores adapting to positivity violations cumulatively across timepoints and simultaneously across both counterfactual treatment histories. This new estimand isolates mechanistic differences between treatment regimes, is identifiable without positivity assumptions, and circumvents the limitations of existing longitudinal methods. Further, our analysis reveals two fundamental insights about longitudinal causal inference under positivity violations. First, while mechanistically meaningful, these effects correspond to non-implementable interventions, exposing a core interpretability-implementability tradeoff. Second, the identified effects faithfully capture mechanistic differences only under a partial common support assumption; violations cause the identified functional to collapse to zero, even when the causal effect is non-zero. We develop doubly robust-style estimators that achieve asymptotic normality and parametric convergence under nonparametric assumptions on the nuisance estimators. To this end, we reformulate challenging density ratio estimation as regression function estimation, which is achievable with standard machine learning methods. We illustrate our methods through analysis of union membership's effect on earnings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10774v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>Wavelet shrinkage based on the raised cosine prior</title>
      <link>https://arxiv.org/abs/2507.10794</link>
      <description>arXiv:2507.10794v1 Announce Type: new 
Abstract: We propose a Bayesian shrinkage rule to estimate the wavelet coefficients in a nonparametric regression model with Gaussian errors, based on a mixture of a point mass function at zero and a symmetric, zero-centered raised cosine distribution prior. The proposed rule outperformed established shrinkage and thresholding methods in specific scenarios of signal-to-noise ratio and sample size values in conducted simulation studies involving the so-called Donoho and Johnstone test functions. Statistical properties of the rule, such as squared bias, variance, and risks, are analyzed, and two illustrations in real datasets are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10794v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juliana Marchesi Reina, Alex Rodrigo dos Santos Sousa</dc:creator>
    </item>
    <item>
      <title>Scalable Variational Inference for Multinomial Probit Models under Large Choice Sets and Sample Sizes</title>
      <link>https://arxiv.org/abs/2507.10945</link>
      <description>arXiv:2507.10945v1 Announce Type: new 
Abstract: The multinomial probit (MNP) model is widely used to analyze categorical outcomes due to its ability to capture flexible substitution patterns among alternatives. Conventional likelihood based and Markov chain Monte Carlo (MCMC) estimators become computationally prohibitive in high dimensional choice settings. This study introduces a fast and accurate conditional variational inference (CVI) approach to calibrate MNP model parameters, which is scalable to large samples and large choice sets. A flexible variational distribution on correlated latent utilities is defined using neural embeddings, and a reparameterization trick is used to ensure the positive definiteness of the resulting covariance matrix. The resulting CVI estimator is similar to a variational autoencoder, with the variational model being the encoder and the MNP's data generating process being the decoder. Straight through estimation and Gumbel SoftMax approximation are adopted for the argmax operation to select an alternative with the highest latent utility. This eliminates the need to sample from high dimensional truncated Gaussian distributions, significantly reducing computational costs as the number of alternatives grows. The proposed method achieves parameter recovery comparable to MCMC. It can calibrate MNP parameters with 20 alternatives and one million observations in approximately 28 minutes roughly 36 times faster and more accurate than the existing benchmarks in recovering model parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10945v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gyeongjun Kim, Yeseul Kang, Lucas Kock, Prateek Bansal, Keemin Sohn</dc:creator>
    </item>
    <item>
      <title>Active Learning via Heteroskedastic Rational Kriging</title>
      <link>https://arxiv.org/abs/2507.10952</link>
      <description>arXiv:2507.10952v1 Announce Type: new 
Abstract: Active learning methods for emulating complex computer models that rely on stationary Gaussian processes tend to produce design points that uniformly fill the entire experimental region, which can be wasteful for functions which vary only in small regions. In this article, we propose a new Gaussian process model that captures the heteroskedasticity of the function. Active learning using this new model can place design points in the more interesting regions of the response surface, and thus obtain surrogate models with better accuracy. The proposed active learning method is compared with the state-of-the-art methods using simulations and two real datasets. It is found to have comparable or better performance relative to other non-stationary Gaussian process-based methods, but faster by orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10952v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shangkun Wang, V. Roshan Joseph</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian high-dimensional variable selection and inference with the horseshoe family of priors</title>
      <link>https://arxiv.org/abs/2507.10975</link>
      <description>arXiv:2507.10975v1 Announce Type: new 
Abstract: Frequentist robust variable selection has been extensively investigated in high-dimensional regression. Despite success, developing the corresponding statistical inference procedures remains a challenging task. Recently, tackling this challenge from a Bayesian perspective has received much attention, as fully Bayesian analysis yields Bayesian credible intervals whose validity can be directly assessed on finite samples. In literature, the two-group spike-and-slab priors that can induce exact sparsity have been demonstrated to yield valid inference in robust sparse linear models. Nevertheless, another important category of sparse priors, the horseshoe family of priors, including horseshoe, horseshoe+, and regularized horseshoe priors, has not yet been examined in robust high-dimensional regression by far. Their performance in variable selection and especially statistical inference in the presence of heavy-tailed model errors is not well understood. In this paper, we address the question by developing robust Bayesian hierarchical models utilizing the horseshoe family of priors along with an efficient Gibbs sampling scheme. We show that compared with competing methods with alternative sampling strategies such as slice sampling, our proposals lead to superior performance in variable selection, Bayesian estimation and statistical inference. In particular, our numeric studies indicate that even without imposing exact sparsity, the one-group horseshoe priors can still yield valid Bayesian credible intervals under robust high-dimensional linear regression models. Applications of the proposed and alternative methods on real data further illustrates the advantage of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10975v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Fan, Srijana Subedi, Vishmi Ridmika Dissanayake Pathiranage, Cen Wu</dc:creator>
    </item>
    <item>
      <title>Context-stratified Mendelian randomization: exploiting regional exposure variation to explore causal effect heterogeneity and non-linearity</title>
      <link>https://arxiv.org/abs/2507.11088</link>
      <description>arXiv:2507.11088v1 Announce Type: new 
Abstract: Mendelian randomization (MR) uses genetic variants as instrumental variables to make causal claims. Standard MR approaches typically report a single population-averaged estimate, limiting their ability to explore effect heterogeneity or non-linear dose-response relationships. Existing stratification methods, such as residual-based and doubly-ranked stratified MR, attempt to overcome this but rely on strong and unverifiable assumptions. We propose an alternative, context-stratified Mendelian randomization, which exploits exogenous variation in the exposure across subgroups -- such as recruitment centres, geographic regions, or time periods -- to investigate effect heterogeneity and non-linearity. Separate MR analyses are performed within each context, and heterogeneity in the resulting estimates is assessed using Cochran's Q statistic and meta-regression.
  We demonstrate through simulations that the approach detects heterogeneity when present while maintaining nominal false positive rates under homogeneity when appropriate methods are used. In an applied example using UK Biobank data, we assess the effect of vitamin D levels on coronary artery disease risk across 20 recruitment centres. Despite some regional variation in vitamin D distributions, there is no evidence for a causal effect or heterogeneity in estimates. Compared to stratification methods requiring model-based assumptions, the context-stratified approach is simple to implement and robust to collider bias, provided the context variable is exogenous. However, the method's power and interpretability depend critically on meaningful exogenous variation in exposure distributions between contexts. In the example of vitamin D, subgroups from other stratification methods explored a much wider range of the exposure distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11088v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Burgess, Benjamin A R Woolf, Amy M Mason</dc:creator>
    </item>
    <item>
      <title>Joint semi-parametric INAR bootstrap inference for model coefficients and innovation distribution</title>
      <link>https://arxiv.org/abs/2507.11124</link>
      <description>arXiv:2507.11124v1 Announce Type: new 
Abstract: For modeling the serial dependence in time series of counts, various approaches have been proposed in the literature. In particular, models based on a recursive, autoregressive-type structure such as the well-known integer-valued autoregressive (INAR) models are very popular in practice. The distribution of such INAR models is fully determined by a vector of autoregressive binomial thinning coefficients and the discrete innovation distribution. While fully parametric estimation techniques for these models are mostly covered in the literature, a semi-parametric approach allows for consistent and efficient joint estimation of the model coefficients and the innovation distribution without imposing any parametric assumptions. Although the limiting distribution of this estimator is known, which, in principle, enables asymptotic inference and INAR model diagnostics on the innovations, it is cumbersome to apply in practice.
  In this paper, we consider a corresponding semi-parametric INAR bootstrap procedure and show its joint consistency for the estimation of the INAR coefficients and for the estimation of the innovation distribution. We discuss different application scenarios that include goodness-of-fit testing, predictive inference and joint dispersion index analysis for count time series. In simulations, we illustrate the finite sample performance of the semi-parametric INAR bootstrap using several innovation distributions and provide real-data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11124v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxime Faymonville, Carsten Jentsch</dc:creator>
    </item>
    <item>
      <title>Near-perfect Clustering Based on Recursive Binary Splitting Using Max-MMD</title>
      <link>https://arxiv.org/abs/2507.11158</link>
      <description>arXiv:2507.11158v1 Announce Type: new 
Abstract: We develop novel clustering algorithms for functional data when the number of clusters $K$ is unknown and also when it is prefixed. These algorithms are developed based on the Maximum Mean Discrepancy (MMD) measure between two sets of observations. The algorithms recursively use a binary splitting strategy to partition the dataset into two subgroups such that they are maximally separated in terms of an appropriate weighted MMD measure. When $K$ is unknown, the proposed clustering algorithm has an additional step to check whether a group of observations obtained by the binary splitting technique consists of observations from a single population. We also obtain a bonafide estimator of $K$ using this algorithm. When $K$ is prefixed, a modification of the previous algorithm is proposed which consists of an additional step of merging subgroups which are similar in terms of the weighted MMD distance. The theoretical properties of the proposed algorithms are investigated in an oracle scenario that requires the knowledge of the empirical distributions of the observations from different populations involved. In this setting, we prove that the algorithm proposed when $K$ is unknown achieves perfect clustering while the algorithm proposed when $K$ is prefixed has the perfect order preserving (POP) property. Extensive real and simulated data analyses using a variety of models having location difference as well as scale difference show near-perfect clustering performance of both the algorithms which improve upon the state-of-the-art clustering methods for functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11158v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourav Chakrabarty, Anirvan Chakraborty, Shyamal K. De</dc:creator>
    </item>
    <item>
      <title>Efficient Canonical Correlation Analysis with Sparsity</title>
      <link>https://arxiv.org/abs/2507.11160</link>
      <description>arXiv:2507.11160v1 Announce Type: new 
Abstract: In high-dimensional settings, Canonical Correlation Analysis (CCA) often fails, and existing sparse methods force an untenable choice between computational speed and statistical rigor. This work introduces a fast and provably consistent sparse CCA algorithm (ECCAR) that resolves this trade-off. We formulate CCA as a high-dimensional reduced-rank regression problem, which allows us to derive consistent estimators with high-probability error bounds without relying on computationally expensive techniques like Fantope projections. The resulting algorithm is scalable, projection-free, and significantly faster than its competitors. We validate our method through extensive simulations and demonstrate its power to uncover reliable and interpretable associations in two complex biological datasets, as well as in an ML interpretability task. Our work makes sparse CCA a practical and trustworthy tool for large-scale multimodal data analysis. A companion R package has been made available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11160v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zixuan Wu, Elena Tuzhilina, Claire Donnat</dc:creator>
    </item>
    <item>
      <title>A sequential classification learning for estimating quantile optimal treatment regimes</title>
      <link>https://arxiv.org/abs/2507.11255</link>
      <description>arXiv:2507.11255v1 Announce Type: new 
Abstract: Quantile optimal treatment regimes (OTRs) aim to assign treatments that maximize a specified quantile of patients' outcomes. Compared to treatment regimes that target the mean outcomes, quantile OTRs offer fairer regimes when a lower quantile is selected, as it focuses on improving outcomes for individuals who would otherwise experience relatively poor results. In this paper, we propose a novel method for estimating quantile OTRs by reformulating the problem as a sequential classification task. This reformulation enables us to leverage the powerful machine learning technique to enhance computational efficiency and handle complex decision boundaries. We also investigate the estimation of quantile OTRs when outcomes are discrete, a setting that has received limited attention in the literature. A key challenge is that direct extensions of existing methods to discrete outcomes often lead to inconsistency and ineffectiveness issues. To overcome this, we introduce a smoothing technique that maps discrete outcomes to continuous surrogates, enabling consistent and effective estimation. We provide theoretical guarantees to support our methodology, and demonstrate its superior performance through comprehensive simulation studies and real-data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11255v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junwen Xia, Jingxiao Zhang, Dehan Kong</dc:creator>
    </item>
    <item>
      <title>How to rank imputation methods?</title>
      <link>https://arxiv.org/abs/2507.11297</link>
      <description>arXiv:2507.11297v1 Announce Type: new 
Abstract: Imputation is an attractive tool for dealing with the widespread issue of missing values. Consequently, studying and developing imputation methods has been an active field of research over the last decade. Faced with an imputation task and a large number of methods, how does one find the most suitable imputation? Although model selection in different contexts, such as prediction, has been well studied, this question appears not to have received much attention. In this paper, we follow the concept of Imputation Scores (I-Scores) and develop a new, reliable, and easy-to-implement score to rank missing value imputations for a given data set without access to the complete data. In practice, this is usually done by artificially masking observations to compare imputed to observed values using measures such as the Root Mean Squared Error (RMSE). We discuss how this approach of additionally masking observations can be misleading if not done carefully and that it is generally not valid under MAR. We then identify a new missingness assumption and develop a score that combines a sensible masking of observations with proper scoring rules. As such the ranking is geared towards the imputation that best replicates the distribution of the data, allowing to find imputations that are suitable for a range of downstream tasks. We show the propriety of the score and discuss an estimation algorithm involving energy scores. Finally, we show the efficacy of the new score in simulated data examples, as well as a downstream task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11297v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey N\"af, Krystyna Grzesiak, Erwan Scornet</dc:creator>
    </item>
    <item>
      <title>Tree inference with varifold distances</title>
      <link>https://arxiv.org/abs/2507.11313</link>
      <description>arXiv:2507.11313v1 Announce Type: new 
Abstract: In this paper, we consider a tree inference problem motivated by the critical problem in single-cell genomics of reconstructing dynamic cellular processes from sequencing data. In particular, given a population of cells sampled from such a process, we are interested in the problem of ordering the cells according to their progression in the process. This is known as trajectory inference. If the process is differentiation, this amounts to reconstructing the corresponding differentiation tree. One way of doing this in practice is to estimate the shortest-path distance between nodes based on cell similarities observed in sequencing data. Recent sequencing techniques make it possible to measure two types of data: gene expression levels, and RNA velocity, a vector that predicts changes in gene expression. The data then consist of a discrete vector field on a (subset of a) Euclidean space of dimension equal to the number of genes under consideration. By integrating this velocity field, we trace the evolution of gene expression levels in each single cell from some initial stage to its current stage. Eventually, we assume that we have a faithful embedding of the differentiation tree in a Euclidean space, but which we only observe through the curves representing the paths from the root to the nodes. Using varifold distances between such curves, we define a similarity measure between nodes which we prove approximates the shortest-path distance in a tree that is isomorphic to the target tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11313v1</guid>
      <category>stat.ME</category>
      <category>math.DG</category>
      <category>q-bio.GN</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elodie Maignant, Tim Conrad, Christoph von Tycowicz</dc:creator>
    </item>
    <item>
      <title>Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis</title>
      <link>https://arxiv.org/abs/2507.10582</link>
      <description>arXiv:2507.10582v1 Announce Type: cross 
Abstract: Unstructured text from legal, medical, and administrative sources offers a rich but underutilized resource for research in public health and the social sciences. However, large-scale analysis is hampered by two key challenges: the presence of sensitive, personally identifiable information, and significant heterogeneity in structure and language. We present a modular toolchain that prepares such text data for embedding-based analysis, relying entirely on open-weight models that run on local hardware, requiring only a workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize, summarize, and, when needed, translate texts to English for greater comparability. Anonymization is achieved via LLM-based redaction, supplemented with named entity recognition and rule-based methods to minimize the risk of disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages. Each document is processed into an anonymized, standardized summary and transformed into a document-level embedding. Validation, including manual review, automated scanning, and predictive evaluation shows the toolchain effectively removes identifying information while retaining semantic content. As an illustrative application, we train a predictive model using embedding vectors derived from a small set of manually labeled summaries, demonstrating the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents, our toolchain opens new possibilities for large-scale research in domains where textual data was previously inaccessible due to privacy and heterogeneity constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10582v1</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anders Ledberg, Anna Thal\'en</dc:creator>
    </item>
    <item>
      <title>AGFS-Tractometry: A Novel Atlas-Guided Fine-Scale Tractometry Approach for Enhanced Along-Tract Group Statistical Comparison Using Diffusion MRI Tractography</title>
      <link>https://arxiv.org/abs/2507.10601</link>
      <description>arXiv:2507.10601v1 Announce Type: cross 
Abstract: Diffusion MRI (dMRI) tractography is currently the only method for in vivo mapping of the brain's white matter (WM) connections. Tractometry is an advanced tractography analysis technique for along-tract profiling to investigate the morphology and microstructural properties along the fiber tracts. Tractometry has become an essential tool for studying local along-tract differences between different populations (e.g., health vs disease). In this study, we propose a novel atlas-guided fine-scale tractometry method, namely AGFS-Tractometry, that leverages tract spatial information and permutation testing to enhance the along-tract statistical analysis between populations. There are two major contributions in AGFS-Tractometry. First, we create a novel atlas-guided tract profiling template that enables consistent, fine-scale, along-tract parcellation of subject-specific fiber tracts. Second, we propose a novel nonparametric permutation testing group comparison method to enable simultaneous analysis across all along-tract parcels while correcting for multiple comparisons. We perform experimental evaluations on synthetic datasets with known group differences and in vivo real data. We compare AGFS-Tractometry with two state-of-the-art tractometry methods, including Automated Fiber-tract Quantification (AFQ) and BUndle ANalytics (BUAN). Our results show that the proposed AGFS-Tractometry obtains enhanced sensitivity and specificity in detecting local WM differences. In the real data analysis experiments, AGFS-Tractometry can identify more regions with significant differences, which are anatomically consistent with the existing literature. Overall, these demonstrate the ability of AGFS-Tractometry to detect subtle or spatially localized WM group-level differences. The created tract profiling template and related code are available at: https://github.com/ZhengRuixi/AGFS-Tractometry.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10601v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruixi Zheng, Wei Zhang, Yijie Li, Xi Zhu, Zhou Lan, Jarrett Rushmore, Yogesh Rathi, Nikos Makris, Lauren J. O'Donnell, Fan Zhang</dc:creator>
    </item>
    <item>
      <title>FARS: Factor Augmented Regression Scenarios in R</title>
      <link>https://arxiv.org/abs/2507.10679</link>
      <description>arXiv:2507.10679v1 Announce Type: cross 
Abstract: Obtaining realistic scenarios for the distribution of key economic variables is crucial for econometricians, policy-makers, and financial analysts. The FARS package provides a comprehensive framework in R for modeling and designing economic scenarios based on distributions derived from multi-level dynamic factor models (ML-DFMs) and factor-augmented quantile regressions (FA-QRs). The package enables users to: (i) extract global and block-specific factors using a flexible multi-level factor structure; (ii) compute asymptotically valid confidence regions for the estimated factors, accounting for uncertainty in the factor loadings; (iii) estimate FA-QRs; (iv) recover full predictive conditional densities from quantile forecasts; and (v) estimate the conditional density when the factors are stressed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10679v1</guid>
      <category>stat.CO</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gian Pietro Bellocca, Ignacio Garr\'on, Vladimir Rodr\'iguez-Caballero, Esther Ruiz</dc:creator>
    </item>
    <item>
      <title>Causal Discovery for Linear Non-Gaussian Models with Disjoint Cycles</title>
      <link>https://arxiv.org/abs/2507.10767</link>
      <description>arXiv:2507.10767v1 Announce Type: cross 
Abstract: The paradigm of linear structural equation modeling readily allows one to incorporate causal feedback loops in the model specification. These appear as directed cycles in the common graphical representation of the models. However, the presence of cycles entails difficulties such as the fact that models need no longer be characterized by conditional independence relations. As a result, learning cyclic causal structures remains a challenging problem. In this paper, we offer new insights on this problem in the context of linear non-Gaussian models. First, we precisely characterize when two directed graphs determine the same linear non-Gaussian model. Next, we take up a setting of cycle-disjoint graphs, for which we are able to show that simple quadratic and cubic polynomial relations among low-order moments of a non-Gaussian distribution allow one to locate source cycles. Complementing this with a strategy of decorrelating cycles and multivariate regression allows one to infer a block-topological order among the directed cycles, which leads to a {consistent and computationally efficient algorithm} for learning causal structures with disjoint cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10767v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias Drton, Marina Garrote-L\'opez, Niko Nikov, Elina Robeva, Y. Samuel Wang</dc:creator>
    </item>
    <item>
      <title>Probabilistic Modeling of Antibody Kinetics Post Infection and Vaccination: A Markov Chain Approach</title>
      <link>https://arxiv.org/abs/2507.10793</link>
      <description>arXiv:2507.10793v1 Announce Type: cross 
Abstract: Understanding the dynamics of antibody levels is crucial for characterizing the time-dependent response to immune events: either infections or vaccinations. The sequence and timing of these events significantly influence antibody level changes. Despite extensive interest in the topic in the recent years and many experimental studies, the effect of immune event sequences on antibody levels is not well understood. Moreover, disease or vaccination prevalence in the population are time-dependent. This, alongside the complexities of personal antibody kinetics, makes it difficult to analyze a sample immune measurement from a population. As a solution, we design a rigorous mathematical characterization in terms of a time-inhomogeneous Markov chain model for event-to-event transitions coupled with a probabilistic framework for the post-event antibody kinetics of multiple immune events. We demonstrate that this is an ideal model for immune event sequences, referred to as personal trajectories. This novel modeling framework surpasses the susceptible-infected-recovered (SIR) characterizations by rigorously tracking the probability distribution of population antibody response across time. To illustrate our ideas, we apply our mathematical framework to longitudinal severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) data from individuals with multiple documented infection and vaccination events. Our work is an important step towards a comprehensive understanding of antibody kinetics that could lead to an effective way to analyze the protective power of natural immunity or vaccination, predict missed immune events at an individual level, and inform booster timing recommendations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10793v1</guid>
      <category>q-bio.PE</category>
      <category>math.PR</category>
      <category>physics.bio-ph</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rayanne A. Luke, Prajakta Bedekar, Lyndsey M. Muehling, Glenda Canderan, Yesun Lee, Wesley A. Cheng, Judith A. Woodfolk, Jeffrey M. Wilson, Pia S. Pannaraj, Anthony J. Kearsley</dc:creator>
    </item>
    <item>
      <title>Multiscale patterns of migration flows in Austria: regionalization, administrative barriers, and urban-rural divides</title>
      <link>https://arxiv.org/abs/2507.11503</link>
      <description>arXiv:2507.11503v1 Announce Type: cross 
Abstract: Migration is central in various societal problems related to socioeconomic development. While much of the existing research has focused on international migration, migration patterns within a single country remain relatively unexplored. In this work we study internal migration patterns in Austria for a period of over 20 years, obtained from open and high-granularity administrative records. We employ inferential network methods to characterize the flows between municipalities and extract their clustering according to similar target and destination rates. Our methodology reveals significant deviations from commonly assumed relocation patterns modeled by the gravity law. At the same time, we observe unexpected biases of internal migrations that leads to less frequent movements across boundaries at both district and state levels than predictions suggest. This leads to significant regionalization of migration at multiple geographical scales and augmented division between urban and rural areas. These patterns appear to be remarkably persistent across decades of migration data, demonstrating systematic limitations of conventionally used gravity models in migration studies. Our approach presents a robust methodology that can be used to improve such evaluations, and can reveal new phenomena in migration networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11503v1</guid>
      <category>physics.soc-ph</category>
      <category>physics.comp-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Robiglio, Martina Contisciani, M\'arton Karsai, Tiago P. Peixoto</dc:creator>
    </item>
    <item>
      <title>Bayesian Joint Modeling for Longitudinal Magnitude Data with Informative Dropout: an Application to Critical Care Data</title>
      <link>https://arxiv.org/abs/2405.19666</link>
      <description>arXiv:2405.19666v2 Announce Type: replace 
Abstract: In various biomedical studies, analysis often focuses on data magnitudes, particularly when algebraic signs are irrelevant or lost. For repeated measures studies involving magnitude outcomes, incorporating random effects is essential as they account for individual heterogeneity, thereby enhancing parameter estimation precision. However, established regression methods specifically designed for magnitude outcomes that incorporate random effects are currently lacking. This article bridges this gap by introducing Bayesian regression modeling approaches for analyzing magnitude data, with a key focus on incorporating random effects. The proposed method is further extended to address multiple causes of informative dropout, a common challenge in repeated measures studies. To tackle this missing data challenge, a joint modeling strategy is developed, building upon the introduced regression techniques. Two numerical simulation studies assess the validity of our method. The chosen simulation scenarios are designed to resemble the conditions of our motivating study. Results demonstrate that the proposed method for magnitude data performs well in terms of estimation accuracy, and the joint models effectively mitigate bias due to missing data. Finally, we apply these models to analyze magnitude data from the motivating study, investigating whether sex impacts the magnitude change in diaphragm thickness over time for ICU patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19666v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wen Teng, Niall D. Ferguson, Ewan C. Goligher, Anna Heath</dc:creator>
    </item>
    <item>
      <title>Non-parametric estimation of transition intensities in interval censored Markov multi-state models without loops</title>
      <link>https://arxiv.org/abs/2409.07176</link>
      <description>arXiv:2409.07176v3 Announce Type: replace 
Abstract: Interval-censored multi state data is collected when the state of a subject is observed periodically. The analysis of such data using non-parametric multi-state models was not possible until recently, but is very desirable as it allows for more flexibility than its parametric counterparts. The single available result to date has some unique drawbacks. We propose a non-parametric estimator of the transition intensities for interval-censored multi state data using an Expectation Maximisation algorithm. The method allows for a mix of interval-censored and right-censored (exactly observed) transitions. A condition to check for the convergence of the algorithm is given. A simulation study comparing the proposed estimator to a consistent estimator is performed, and shown to yield near identical estimates at smaller computational cost. A data set on the emergence of teeth in children is analysed. Software to perform the analyses is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07176v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Gomon, Hein Putter</dc:creator>
    </item>
    <item>
      <title>Spatial scale-aware tail dependence modeling for high-dimensional spatial extremes</title>
      <link>https://arxiv.org/abs/2412.07957</link>
      <description>arXiv:2412.07957v2 Announce Type: replace 
Abstract: Extreme events over large spatial domains may exhibit highly heterogeneous tail dependence characteristics, yet most existing spatial extremes models yield only one dependence class over the entire spatial domain. To accurately characterize "data-level dependence'' in analysis of extreme events, we propose a mixture model that achieves flexible dependence properties and allows high-dimensional inference for extremes of spatial processes. We modify the popular random scale construction that multiplies a Gaussian random field by a single radial variable; we allow the radial variable to vary smoothly across space and add non-stationarity to the Gaussian process. As the level of extremeness increases, this single model exhibits both asymptotic independence at long ranges and either asymptotic dependence or independence at short ranges. We make joint inference on the dependence model and a marginal model using a copula approach within a Bayesian hierarchical model. Three different simulation scenarios show close to nominal frequentist coverage rates. Lastly, we apply the model to a dataset of extreme summertime precipitation over the central United States. We find that the joint tail of precipitation exhibits non-stationary dependence structure that cannot be captured by limiting extreme value models or current state-of-the-art sub-asymptotic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07957v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muyang Shi, Likun Zhang, Mark D. Risser, Benjamin A. Shaby</dc:creator>
    </item>
    <item>
      <title>Nullstrap: A Simple, High-Power, and Fast Framework for FDR Control in Variable Selection for Diverse High-Dimensional Models</title>
      <link>https://arxiv.org/abs/2501.05012</link>
      <description>arXiv:2501.05012v2 Announce Type: replace 
Abstract: Balancing false discovery rate (FDR) control with high statistical power remains a central challenge in high-dimensional variable selection. While several FDR-controlling methods have been proposed, many degrade the original data -- by adding knockoff variables or splitting the data -- which often leads to substantial power loss and hampers detection of true signals. We introduce Nullstrap, a novel framework that controls FDR without altering the original data. Nullstrap generates synthetic null data by fitting a null model under the global null hypothesis that no variables are important. It then applies the same estimation procedure in parallel to both the original and synthetic data. This parallel approach mirrors that of the classical likelihood ratio test, making Nullstrap its numerical analog. By adjusting the synthetic null coefficient estimates through a data-driven correction procedure, Nullstrap identifies important variables while controlling the FDR. We provide theoretical guarantees for asymptotic FDR control at any desired level and show that power converges to one in probability. Nullstrap is simple to implement and broadly applicable to high-dimensional linear models, generalized linear models, Cox models, and Gaussian graphical models. Simulations and real-data applications show that Nullstrap achieves robust FDR control and consistently outperforms leading methods in both power and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05012v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Changhu Wang, Ziheng Zhang, Jingyi Jessica Li</dc:creator>
    </item>
    <item>
      <title>Interpretational challenges of the Win Ratio in analyzing Hierarchical Composite Endpoints in Chronic Kidney Disease</title>
      <link>https://arxiv.org/abs/2504.05909</link>
      <description>arXiv:2504.05909v3 Announce Type: replace 
Abstract: Win statistics based methods have gained traction as a method for analyzing Hierarchical Composite Endpoints (HCEs) in randomized clinical trials, particularly in cardiovascular and kidney disease research. HCEs offer several key advantages, including increased statistical power, mitigation of competing risks, and hierarchical ranking of clinical outcomes. While, as summary measures, the win ratio (WR) along with the Net Benefit (NB) and the Win Odds (WO) provide a structured approach to analyzing HCEs, several concerns regarding their interpretability remain. In this paper, we present known issues with the WR using simple examples designed to explore the implications for the clinical interpretability of the treatment effect measure in the chronic kidney disease setting. Specifically, we discuss the challenge of defining an appropriate estimand in the context of HCEs using the WR, the difficulties in formulating a relevant causal question underlying the WR, and the dependency of the WR on the variance of its components, which complicates its role as an effect measure. Additionally, we highlight the non-collapsibility and non-transitivity of the WR, further complicating its interpretation. While the WR remains a valuable tool in clinical trials, its inherent limitations must be acknowledged to ensure its proper use in regulatory and clinical decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05909v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henrik F. Thomsen, Samvel B. Gasparyan, Julie F. Furberg, Christoph Tasto, Nicole Rethemeier, Patrick Schloemer, Tuo Wang, Niels Jongs, Yu Du, Tom Greene</dc:creator>
    </item>
    <item>
      <title>Counterfactual Q Learning via the Linear Buckley James Method for Longitudinal Survival Data</title>
      <link>https://arxiv.org/abs/2505.12159</link>
      <description>arXiv:2505.12159v2 Announce Type: replace 
Abstract: Treatment strategies are critical in healthcare, particularly when outcomes are subject to censoring. This study introduces the Counterfactual Buckley-James Q-Learning framework, which integrates the Buckley-James method with reinforcement learning to address challenges posed by censored survival data. The Buckley-James method imputes censored survival times via conditional expectations based on observed data, offering a robust mechanism for handling incomplete outcomes. By incorporating these imputed values into a counterfactual Q-learning framework, the proposed method enables the estimation and comparison of potential outcomes under different treatment strategies. This facilitates the identification of optimal dynamic treatment regimes that maximize expected survival time. Through extensive simulation studies, the method demonstrates robust performance across various sample sizes and censoring scenarios, including right censoring and missing at random (MAR). Application to real-world clinical trial data further highlights the utility of this approach in informing personalized treatment decisions, providing an interpretable and reliable tool for optimizing survival outcomes in complex clinical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12159v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeongjin Lee, Jong-Min Kim</dc:creator>
    </item>
    <item>
      <title>Constructing Confidence Intervals for Infinite-Dimensional Functional Prameters by Highly Adaptive Lasso</title>
      <link>https://arxiv.org/abs/2507.10511</link>
      <description>arXiv:2507.10511v2 Announce Type: replace 
Abstract: Estimating the conditional mean function is a central task in statistical learning. In this paper, we consider estimation and inference for a nonparametric class of real-valued c\`adl\`ag functions with bounded sectional variation (Gill et al., 1995), using the Highly Adaptive Lasso (HAL) (van der Laan, 2015; Benkeser and van der Laan, 2016; van der Laan, 2023), a flexible empirical risk minimizer over linear combinations of tensor products of zero- or higher-order spline basis functions under an L1 norm constraint. Building on recent theoretical advances in asymptotic normality and uniform convergence rates for higher-order spline HAL estimators (van der Laan, 2023), this work focuses on constructing robust confidence intervals for HAL-based conditional mean estimators. To address regularization bias, we propose a targeted HAL with a debiasing step to remove bias for the conditional mean, and also consider a relaxed HAL estimator to reduce bias. We also introduce both global and local undersmoothing strategies to adaptively select the working model, reducing bias relative to variance. Combined with delta-method-based variance estimation, we construct confidence intervals for conditional means based on HAL. Through simulations, we evaluate combinations of estimation and model selection strategies, showing that our methods substantially reduce bias and yield confidence intervals with coverage rates close to nominal levels across scenarios. We also provide recommendations for different estimation objectives and illustrate the generality of our framework by applying it to estimate conditional average treatment effect (CATE) functions, highlighting how HAL-based inference extends to other infinite-dimensional, non-pathwise differentiable parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10511v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxin Zhang, Junming Shi, Alan Hubbard, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Uniform Confidence Bands for Group-Time Conditional Average Treatment Effects in Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2305.02185</link>
      <description>arXiv:2305.02185v4 Announce Type: replace-cross 
Abstract: We consider a panel data analysis to examine the heterogeneity in treatment effects with respect to groups, periods, and a pre-treatment covariate of interest in the staggered difference-in-differences setting of Callaway and Sant'Anna (2021). Under standard identification conditions, a doubly robust estimand conditional on the covariate identifies the group-time conditional average treatment effect given the covariate. Focusing on the case of a continuous covariate, we propose a three-step estimation procedure based on nonparametric local polynomial regressions and parametric estimation methods. Using uniformly valid distributional approximation results for empirical processes and weighted/multiplier bootstrapping, we develop doubly robust inference methods to construct uniform confidence bands for the group-time conditional average treatment effect function and a variety of useful summary parameters. The accompanying R package didhetero allows for easy implementation of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.02185v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunsuke Imai, Lei Qin, Takahide Yanagi</dc:creator>
    </item>
    <item>
      <title>A multivariate spatial regression model using signatures</title>
      <link>https://arxiv.org/abs/2410.07899</link>
      <description>arXiv:2410.07899v2 Announce Type: replace-cross 
Abstract: We propose a spatial autoregressive model for a multivariate response variable and functional covariates. The approach is based on the notion of signature, which represents a function as an infinite series of its iterated integrals and presents the advantage of being applicable to a wide range of processes. We have provided theoretical guarantees for the choice of the signature truncation order, and we have shown in a simulation study and an application to pollution data that this approach outperforms existing approaches in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07899v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Camille Fr\'event, Issa-Mbenard Dabo</dc:creator>
    </item>
    <item>
      <title>Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift</title>
      <link>https://arxiv.org/abs/2507.05412</link>
      <description>arXiv:2507.05412v2 Announce Type: replace-cross 
Abstract: We consider the problem of learning robust discriminative representations of causally-related latent variables. In addition to observational data, the training dataset also includes interventional data obtained through targeted interventions on some of these latent variables to learn representations robust against the resulting interventional distribution shifts. Existing approaches treat interventional data like observational data, even when the underlying causal model is known, and ignore the independence relations that arise from these interventions. Since these approaches do not fully exploit the causal relational information resulting from interventions, they learn representations that produce large disparities in predictive performance on observational and interventional data, which worsens when the number of interventional training samples is limited. In this paper, (1) we first identify a strong correlation between this performance disparity and adherence of the representations to the independence conditions induced by the interventional causal model. (2) For linear models, we derive sufficient conditions on the proportion of interventional data in the training dataset, for which enforcing interventional independence between representations corresponding to the intervened node and its non-descendants lowers the error on interventional data. Combining these insights, (3) we propose RepLIn, a training algorithm to explicitly enforce this statistical independence during interventions. We demonstrate the utility of RepLIn on a synthetic dataset and on real image and text datasets on facial attribute classification and toxicity detection, respectively. Our experiments show that RepLIn is scalable with the number of nodes in the causal graph and is suitable to improve the robust representations against interventional distribution shifts of both continuous and discrete latent variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05412v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gautam Sreekumar, Vishnu Naresh Boddeti</dc:creator>
    </item>
  </channel>
</rss>
