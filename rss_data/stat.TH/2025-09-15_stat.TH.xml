<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Sep 2025 04:00:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Smooth Computational Transition in Tensor PCA</title>
      <link>https://arxiv.org/abs/2509.09904</link>
      <description>arXiv:2509.09904v1 Announce Type: new 
Abstract: We propose an efficient algorithm for tensor PCA based on counting a specific family of weighted hypergraphs. For the order-$p$ tensor PCA problem where $p \geq 3$ is a fixed integer, we show that when the signal-to-noise ratio is $\lambda n^{-\frac{p}{4}}$ where $\lambda=\Omega(1)$, our algorithm succeeds and runs in time $n^{C+o(1)}$ where $C=C(\lambda)$ is a constant depending on $\lambda$. This algorithm improves a poly-logarithmic factor compared to previous algorithms based on the Sum-of-Squares hierarchy \cite{HSS15} or based on the Kikuchi hierarchy in statistical physics \cite{WEM19}. Furthermore, our result shows a smooth tradeoff between the signal-to-noise ratio and the computational cost in this problem, thereby confirming a conjecture posed in \cite{KWB22}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09904v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>Quantifying and testing dependence to categorical variables</title>
      <link>https://arxiv.org/abs/2509.10268</link>
      <description>arXiv:2509.10268v1 Announce Type: new 
Abstract: We suggest a dependence coefficient between a categorical variable and some general variable taking values in a metric space. We derive important theoretical properties and study the large sample behaviour of our suggested estimator. Moreover, we develop an independence test which has an asymptotic $\chi^2$-distribution if the variables are independent and prove that this test is consistent against any violation of independence. We discuss some extensions, including a variant of the coefficient for measuring conditional dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10268v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siegfried H\"ormann, Daniel Strenger-Galvis</dc:creator>
    </item>
    <item>
      <title>Optimal Inference of the Mean Outcome under Optimal Treatment Regime</title>
      <link>https://arxiv.org/abs/2509.09773</link>
      <description>arXiv:2509.09773v1 Announce Type: cross 
Abstract: When an optimal treatment regime (OTR) is considered, we need to evaluate the OTR in a valid and efficient way. The classical inference applied to the mean outcome under OTR, assuming the OTR is the same as the estimated OTR, might be biased when the regularity assumption that OTR is unique is violated. Although several methods have been proposed to allow nonregularity in such inference, its optimality is unclear due to challenges in deriving semiparametric efficiency bounds under potential nonregularity. In this paper, we address the bias issue via adaptive smoothing over the estimated OTR and develop a valid inference procedure on the mean outcome under OTR regardless of whether regularity is satisfied. We establish the optimality of the proposed method by deriving a lower bound of the asymptotic variance for the robust asymptotically linear unbiased estimator to the mean outcome under OTR and showing that our proposed estimator achieves the variance lower bound. The considered estimator class is general and the derived variance lower bound paves a novel way to establish efficiency optimality theories for OTR in a more general scenario allowing nonregularity. The merit of the proposed method is demonstrated by re-analyzing the ACTG 175 trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09773v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuoxun Xu (Department of Mathematics, The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong, China, Division of Biostatistics, School of Public Health, University of California, Berkeley, Berkeley, CA, USA), Xinzhou Guo (Department of Mathematics, The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong, China)</dc:creator>
    </item>
    <item>
      <title>Confidence Intervals for Extinction Risk: Validating Population Viability Analysis with Limited Data</title>
      <link>https://arxiv.org/abs/2509.09965</link>
      <description>arXiv:2509.09965v1 Announce Type: cross 
Abstract: Quantitative assessment of extinction risk requires not only point estimates but also confidence intervals (CIs) that remain informative with limited data. Their reliability has been debated, as short observation spans can inflate uncertainty and reduce usefulness. I derive new CIs for extinction probability $G$ under the Wiener process with drift, a canonical model of population viability analysis. The method uses correlated noncentral-$t$ distributions for the transformed statistics $\widehat{w}$ and $\widehat{z}$, derived from drift and variance estimators, and constructs CIs of the extinction probability by exploiting the geometric properties of $G(\widehat{w},\widehat{z})$ in parameter space. Monte Carlo experiments show that the proposed intervals attain nominal coverage with narrower widths than common approximate methods, including the delta method, moment-based approaches, and bootstrap. A key result is that even with short time series, extinction probabilities that are very small or very large can be estimated reliably. This resolves a long-standing concern that population viability analysis fails under data scarcity. Applied to three 64-year catch series for Japanese eel (Anguilla japonica), the analysis indicates extinction risk well below the IUCN Criterion E thresholds for Critically Endangered and Endangered, with narrow CIs. These findings demonstrate that extinction-risk CIs can be both statistically rigorous and practical for Red List evaluations, even when data are limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09965v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroshi Hakoyama</dc:creator>
    </item>
    <item>
      <title>Improving discrepancy by moving a few points</title>
      <link>https://arxiv.org/abs/2503.04100</link>
      <description>arXiv:2503.04100v2 Announce Type: replace 
Abstract: We show how to improve the discrepancy of an iid sample by moving only a few points. Specifically, modifying \( O(m) \) sample points on average reduces the Kolmogorov-Smirnov distance to the population distribution to \(1/m\).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04100v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gleb Smirnov, Roman Vershynin</dc:creator>
    </item>
    <item>
      <title>The e-Partitioning Principle of False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2504.15946</link>
      <description>arXiv:2504.15946v2 Announce Type: replace 
Abstract: We present a novel necessary and sufficient principle for False Discovery Rate (FDR) control. This e-Partitioning Principle says that a procedure controls FDR if and only if it is a special case of a general e-Partitioning procedure. By writing existing methods as special cases of this procedure, we can achieve uniform improvements of these methods, and we show this in particular for the eBH, BY and Su methods. We also show that methods developed using the $e$-Partitioning Principle have several valuable properties. They generally control FDR not just for one rejected set, but simultaneously over many, allowing post hoc flexibility for the researcher in the final choice of the rejected hypotheses. Under some conditions, they also allow for post hoc adjustment of the error rate, choosing the FDR level $\alpha$ post hoc, or switching to familywise error control after seeing the data. In addition, e-Partitioning allows FDR control methods to exploit logical relationships between hypotheses to gain power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15946v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jelle Goeman, Rianne de Heide, Aldo Solari</dc:creator>
    </item>
    <item>
      <title>A Modern Theory of Cross-Validation through the Lens of Stability</title>
      <link>https://arxiv.org/abs/2505.23592</link>
      <description>arXiv:2505.23592v2 Announce Type: replace 
Abstract: Modern data analysis and statistical learning are marked by complex data structures and black-box algorithms. Data complexity stems from technologies like imaging, remote sensing, wearables, and genomic sequencing. Simultaneously, black-box models -- especially deep neural networks -- have achieved impressive results. This combination raises new challenges for uncertainty quantification and statistical inference, which we term "black-box inference."
  Black-box inference is difficult due to the lack of traditional modeling assumptions and the opaque behavior of modern estimators. These make it hard to characterize the distribution of estimation errors. A popular solution is post-hoc randomization, which, under mild assumptions like exchangeability, can yield valid uncertainty quantification. Such methods range from classical techniques like permutation tests, jackknife, and bootstrap, to recent innovations like conformal inference. These approaches typically need little knowledge of data distributions or the internal working of estimators. Many rely on the idea that estimators behave similarly under small data changes -- a concept formalized as stability. Over time, stability has become a key principle in data science, influencing generalization error, privacy, and adaptive inference.
  This article investigates cross-validation (CV) -- a widely used resampling method -- through the lens of stability. We first review recent theoretical results on CV for estimating generalization error and model selection under stability. We then examine uncertainty quantification for CV-based risk estimates. Together, these insights yield new theory and tools, which we apply to topics like model selection, selective inference, and conformal prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23592v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing Lei</dc:creator>
    </item>
    <item>
      <title>On Regression in Extreme Regions</title>
      <link>https://arxiv.org/abs/2303.03084</link>
      <description>arXiv:2303.03084v3 Announce Type: replace-cross 
Abstract: We establish a statistical learning theoretical framework aimed at extrapolation, or out-of-domain generalization, on the unobserved tails of covariates in continuous regression problems. Our strategy involves performing statistical regression on a subsample of observations with continuous labels that are the furthest away from the origin, focusing specifically on their angular components. The underlying assumptions of our approach are grounded in the theory of multivariate regular variation, a cornerstone of extreme value theory. We address the stylized problem of nonparametric least squares regression with predictors chosen from a Vapnik-Chervonenkis class.
  This work contributes to a broader initiative to develop statistical learning theoretical foundations for supervised learning strategies that enhance performance on the supposedly heavy tails of covariates. Previous efforts in this area have focused exclusively on binary classification on extreme covariates. Although the continuous target setting necessitates different techniques and regularity assumptions, our main results echo findings from earlier studies. We quantify the predictive performance on tail regions in terms of excess risk, presenting it as a finite sample risk bound with a clear bias-variance decomposition. Numerical experiments with simulated and real data illustrate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.03084v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephan Cl\'emen\c{c}on, Nathan Huet, Anne Sabourin</dc:creator>
    </item>
    <item>
      <title>Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures</title>
      <link>https://arxiv.org/abs/2509.08926</link>
      <description>arXiv:2509.08926v2 Announce Type: replace-cross 
Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise, which typically leads to significant performance degradation. We address this challenge by reframing Re-ID as a supervised image similarity task and adopting a Siamese network architecture trained to capture discriminative pairwise relationships. Central to our approach is a novel statistical outlier detection (OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier Detection), which models the distribution of cosine similarities between embedding pairs using a two-component Beta distribution mixture model. We establish a novel identifiability result for mixtures of two Beta distributions, ensuring that our learning task is well-posed. The proposed OD step complements the Re-ID architecture combining binary cross-entropy, contrastive, and cosine embedding losses that jointly optimize feature-level similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance compared to the state-of-the-art methods across various noise levels (10-30\%), demonstrating both robustness and broad applicability in noisy Re-ID scenarios. The implementation of Beta-SOD is available at: github.com/waqar3411/Beta-SOD</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08926v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Waqar Ahmad, Evan Murphy, Vladimir A. Krylov</dc:creator>
    </item>
  </channel>
</rss>
