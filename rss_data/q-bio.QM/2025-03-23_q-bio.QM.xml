<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Mar 2025 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Early Prediction of Alzheimer's and Related Dementias: A Machine Learning Approach Utilizing Social Determinants of Health Data</title>
      <link>https://arxiv.org/abs/2503.16560</link>
      <description>arXiv:2503.16560v1 Announce Type: new 
Abstract: Alzheimer's disease and related dementias (AD/ADRD) represent a growing healthcare crisis affecting over 6 million Americans. While genetic factors play a crucial role, emerging research reveals that social determinants of health (SDOH) significantly influence both the risk and progression of cognitive functioning, such as cognitive scores and cognitive decline. This report examines how these social, environmental, and structural factors impact cognitive health trajectories, with a particular focus on Hispanic populations, who face disproportionate risk for AD/ADRD. Using data from the Mexican Health and Aging Study (MHAS) and its cognitive assessment sub study (Mex-Cog), we employed ensemble of regression trees models to predict 4-year and 9-year cognitive scores and cognitive decline based on SDOH. This approach identified key predictive SDOH factors to inform potential multilevel interventions to address cognitive health disparities in this population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16560v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bereket Kindo, Arjee Restar, Anh Tran</dc:creator>
    </item>
    <item>
      <title>Structural and Practical Identifiability of Phenomenological Growth Models for Epidemic Forecasting</title>
      <link>https://arxiv.org/abs/2503.17135</link>
      <description>arXiv:2503.17135v1 Announce Type: new 
Abstract: Phenomenological models are highly effective tools for forecasting disease dynamics using real world data, particularly in scenarios where detailed knowledge of disease mechanisms is limited. However, their reliability depends on the model parameters' structural and practical identifiability. In this study, we systematically analyze the identifiability of six commonly used growth models in epidemiology: the generalized growth model (GGM), the generalized logistic model (GLM), the Richards model, the generalized Richards model (GRM), the Gompertz model, and a modified SEIR model with inhomogeneous mixing. To address challenges posed by non integer power exponents in these models, we reformulate them by introducing additional state variables. This enables rigorous structural identifiability analysis using the StructuralIdentifiability.jl package in JULIA. We validate the structural identifiability results by performing parameter estimation and forecasting using the GrowthPredict MATLAB toolbox. This toolbox is designed to fit and forecast time series trajectories based on phenomenological growth models. We applied it to three epidemiological datasets: weekly incidence data for monkeypox, COVID 19, and Ebola. Additionally, we assess practical identifiability through Monte Carlo simulations to evaluate parameter estimation robustness under varying levels of observational noise. Our results demonstrate the structural and practical identifiability of the models, emphasizing how noise affects parameter estimation accuracy. These findings provide valuable insights into the utility and limitations of phenomenological models for epidemic data analysis, highlighting their adaptability to real world challenges and their role in guiding public health decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17135v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuganthi R. Liyanage, Gerardo Chowell, Gleb Pogudin, Necibe Tuncer</dc:creator>
    </item>
    <item>
      <title>Leveraging statistical models to improve pre-season forecasting and in-season management of a recreational fishery</title>
      <link>https://arxiv.org/abs/2503.17293</link>
      <description>arXiv:2503.17293v1 Announce Type: new 
Abstract: Effective management of recreational fisheries requires accurate forecasting of future harvests and real-time monitoring of ongoing harvests. Traditional methods that rely on historical catch data to predict short-term harvests can be unreliable, particularly if changes in management regulations alter angler behavior. In contrast, statistical modeling approaches can provide faster, more flexible, and potentially more accurate predictions, enhancing management outcomes. In this study, we developed and tested models to improve predictions of Gulf of Mexico gag harvests for both pre-season planning and in-season monitoring. Our best-fitting model outperformed traditional methods (i.e., estimates derived from historical average harvest) for both cumulative pre-season projections and in-season monitoring. Notably, our modeling framework appeared to be more accurate in more recent, shorter seasons due to its ability to account for effort compression. A key advantage of our framework is its ability to explicitly quantify the probability of exceeding harvest quotas for any given season duration. This feature enables managers to evaluate trade-offs between season duration and conservation goals. This is especially critical for vulnerable, highly targeted stocks. Our findings also underscore the value of statistical models to complement and advance traditional fisheries management approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17293v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Challen Hyman, Chloe Ramsay, Tiffanie A. Cross, Beverly Sauls, Thomas K. Frazer</dc:creator>
    </item>
    <item>
      <title>Ex vivo experiment on vertebral body with defect representing bone metastasis</title>
      <link>https://arxiv.org/abs/2503.17047</link>
      <description>arXiv:2503.17047v1 Announce Type: cross 
Abstract: Osteolytic metastases located in the vertebrae reduce strength and enhance the risk of vertebral fractures. This risk can be predicted by means of validated finite element models, but their reproducibility needs to be assessed. For that purpose, experimental data are requested. The aim of this study was to conduct open-access experiments on vertebrae, with artificial defect representing lytic metastasis and using well-defined boundary conditions. Twelve lumbar vertebral bodies (L1) were prepared by removing the cortical endplates and creating defects that represent lytic metastases, by drilling the cancellous bone. Vertebral bodies were scanned using clinical High Resolution peripherical Quantitative Computed Tomography before and after defect creation for 3D reconstruction. The specimens were then tested under compression loading until failure. Surface Digital Image Correlation was used to assess strain fields on the anterior wall of the vertebral body. These data (biomechanics data and the tomographic images needed to build subject-specific models) are shared with the scientific community in order to assess different vertebral models on the same dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17047v1</guid>
      <category>q-bio.TO</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>W. Lokbani, V. Allard, T. Broussolle, CY. Barrey, C. B. Confavreux, K. Bruyere, JP. Roux, F. Bermond, H. Follet, D. Mitton</dc:creator>
    </item>
    <item>
      <title>Karyotype AI for Precision Oncology</title>
      <link>https://arxiv.org/abs/2211.14312</link>
      <description>arXiv:2211.14312v5 Announce Type: replace 
Abstract: We present a machine learning method capable of accurately detecting chromosome abnormalities that cause blood cancers directly from microscope images of the metaphase stage of cell division. The pipeline is built on a series of fine-tuned Vision Transformers. Current state of the art (and standard clinical practice) requires expensive, manual expert analysis, whereas our pipeline takes only 15 seconds per metaphase image. Using a novel pretraining-finetuning strategy to mitigate the challenge of data scarcity, we achieve a high precision-recall score of 94% AUC for the clinically significant del(5q) and t(9;22) anomalies. Our method also unlocks zero-shot detection of rare aberrations based on model latent embeddings. The ability to quickly, accurately, and scalably diagnose genetic abnormalities directly from metaphase images could transform karyotyping practice and improve patient outcomes. We will make code publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.14312v5</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zahra Shamsi, Isaac Reid, Drew Bryant, Jacob Wilson, Xiaoyu Qu, Avinava Dubey, Konik Kothari, Mostafa Dehghani, Mariya Chavarha, Valerii Likhosherstov, Brian Williams, Michael Frumkin, Fred Appelbaum, Krzysztof Choromanski, Ali Bashir, Min Fang</dc:creator>
    </item>
    <item>
      <title>UQSA -- An R-Package for Uncertainty Quantification and Sensitivity Analysis for Biochemical Reaction Network Models</title>
      <link>https://arxiv.org/abs/2308.05527</link>
      <description>arXiv:2308.05527v2 Announce Type: replace 
Abstract: Biochemical reaction models describing subcellular processes generally come with a large uncertainty. To be able to account for this during the modeling process, we have developed the R-package UQSA, performing uncertainty quantification and sensitivity analysis in an integrated fashion. UQSA is designed for fast sampling of complicated multi-dimensional parameter distributions, using efficient Markov chain Monte Carlo (MCMC) sampling techniques and Vine-copulas to model complicated joint distributions. We perform MCMC sampling both from stochastic and deterministic models, in either likelihood-free or likelihood-based settings. In the likelihood-free case, we use Approximate Bayesian Computation (ABC), while for likelihood-based sampling we provide different algorithms, including the fast geometry-informed algorithm SMMALA (Simplified Manifold Metropolis-Adjusted Langevin Algorithm). The uncertainty quantification can be followed by a variance decomposition-based global sensitivity analysis. We are aiming for biochemical models, but UQSA can be used for any type of reaction networks. The use of Vine-copulas allows us to describe, evaluate, and sample from complicated parameter distributions, as well as adding new datasets in a sequential manner without redoing the previous parameter fit. The code is written in R, with C as a backend to improve speed. We use the SBtab table format for Systems Biology projects for the model description as well as the experimental data. An event system allows the user to model complicated transient input, common within, e.g., neuroscience. UQSA has an extensive documentation with several examples describing different types of models and data. The code has been tested on up to 2000 cores on several nodes on a computing cluster, but we also include smaller examples that can be run on a laptop. Source code: https://github.com/icpm-kth/uqsa</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05527v2</guid>
      <category>q-bio.QM</category>
      <category>q-bio.SC</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andrei Kramer, Federica Milinanni, Jeanette Hellgren Kotaleski, Pierre Nyquist, Alexandra Jauhiainen, Olivia Eriksson</dc:creator>
    </item>
    <item>
      <title>Optimising image capture for low-light widefield quantitative fluorescence microscopy</title>
      <link>https://arxiv.org/abs/2410.19210</link>
      <description>arXiv:2410.19210v2 Announce Type: replace 
Abstract: Low-light optical imaging refers to the use of cameras to capture images with minimal photon flux. This area has broad application to diverse fields, including optical microscopy for biological studies. In such studies, it is important to reduce the intensity of illumination to reduce adverse effects such as photobleaching and phototoxicity that may perturb the biological system under study. The challenge when minimising illumination is to maintain image quality that reflects the underlying biology and can be used for quantitative measurements. An example is the optical redox ratio which is computed from autofluorescence intensity to measure metabolism. In all such cases, it is critical for researchers to optimise selection and application of scientific cameras to their microscopes, but few resources discuss performance in the low-light regime. In this tutorial, we address the challenges in optical fluorescence imaging at low-light levels for quantitative microscopy, with an emphasis on live biological samples. We analyse the performance of specialised low-light scientific cameras such as the EMCCD, qCMOS, and sCMOS, while considering the differences in platform architecture and the contribution of various sources of noise. The tutorial covers a detailed discussion of user-controllable parameters, as well as the application of post-processing algorithms for denoising. We illustrate these concepts using autofluorescence images of live mammalian embryos captured with a two-photon light sheet fluorescence microscope.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19210v2</guid>
      <category>q-bio.QM</category>
      <category>eess.IV</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0245239</arxiv:DOI>
      <arxiv:journal_reference>APL Photon. 10, 031102 (2025)</arxiv:journal_reference>
      <dc:creator>Zane Peterkovic, Avinash Upadhya, Christopher Perrella, Admir Bajraktarevic, Ramses Bautista Gonzalez, Megan Lim, Kylie R Dunning, Kishan Dholakia</dc:creator>
    </item>
  </channel>
</rss>
