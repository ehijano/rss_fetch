<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Feb 2026 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>"Everyone's using it, but no one is allowed to talk about it": College Students' Experiences Navigating the Higher Education Environment in a Generative AI World</title>
      <link>https://arxiv.org/abs/2602.17720</link>
      <description>arXiv:2602.17720v1 Announce Type: new 
Abstract: Higher education students are increasingly using generative AI in their academic work. However, existing institutional practices have not yet adapted to this shift. Through semi-structured interviews with 23 college students, our study examines the environmental and social factors that influence students' use of AI. Findings show that institutional pressure factors like deadlines, exam cycles, and grading lead students to engage with AI even when they think it undermines their learning. Social influences, particularly peer micro-communities, establish de-facto AI norms regardless of official AI policies. Campus-wide ``AI shame'' is prevalent, often pushing AI use underground. Current institutional AI policies are perceived as generic, inconsistent, and confusing, resulting in routine noncompliance. Additionally, students develop value-based self-regulation strategies, but environmental pressures create a gap between students' intentions and their behaviors. Our findings show student AI use to be a situated practice, and we discuss implications for institutions, instructors, and system tool designers to effectively support student learning with AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17720v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yue Fu, Yifan Lin, Yessica Wang, Sarah Tran, Alexis Hiniker</dc:creator>
    </item>
    <item>
      <title>Gender and Digital Platform Work During Turbulent Times</title>
      <link>https://arxiv.org/abs/2602.17721</link>
      <description>arXiv:2602.17721v1 Announce Type: new 
Abstract: This commentary explores how the platform economy shapes labour market responses during times of crisis, with a focus on gendered experiences. Drawing on cases of economic crisis, natural disasters, and refugee displacement, it examines how digital labour platforms offer flexible work opportunities while also reinforcing existing inequalities. Women face distinct constraints (such as caregiving responsibilities, limited mobility, and economic insecurity) that hinder their employment opportunities and earnings potential. These constraints are more pronounced during crises, when access to stable income and safe working conditions becomes more difficult. While platform work can serve as a lifeline, it is not a guaranteed solution, and its benefits are unevenly distributed. The commentary calls for gender-responsive policies and new research to understand how digital infrastructures mediate labour experiences across different crisis contexts. Such research can inform inclusive strategies that promote resilience and equity in platform-based work, particularly for marginalized and displaced populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17721v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1111/gwao.70074</arxiv:DOI>
      <arxiv:journal_reference>33 (2), March 2026, 295-299</arxiv:journal_reference>
      <dc:creator>Melissa Langworthy, Yana Rodgers</dc:creator>
    </item>
    <item>
      <title>Closing Africa's Early Warning Gap: AI Weather Forecasting for Disaster Prevention</title>
      <link>https://arxiv.org/abs/2602.17726</link>
      <description>arXiv:2602.17726v1 Announce Type: new 
Abstract: In January 2026, torrential rains killed 200-300 people across Southern Africa, exposing a critical reality: 60% of the continent lacks effective early warning systems due to infrastructure costs. Traditional radar stations exceed USD 1 million each, leaving Africa with an 18x coverage deficit compared to the US and EU. We present a production-grade architecture for deploying NVIDIA Earth-2 AI weather models at USD 1,430-1,730/month for national-scale deployment - enabling coverage at 2,000-4,545x lower cost than radar. The system generates 15-day global atmospheric forecasts, cached in PostgreSQL to enable user queries under 200 milliseconds without real-time inference.
  Deployed in South Africa in February 2026, our system demonstrates three technical contributions: (1) a ProcessPoolExecutor-based event loop isolation pattern that resolves aiobotocore session lifecycle conflicts in async Python applications; (2) a database-backed serving architecture where the GPU writes global forecasts directly to PostgreSQL, eliminating HTTP transfer bottlenecks for high-resolution tensors; and (3) an automated coordinate management pattern for multi-step inference across 61 timesteps. Forecasts are delivered via WhatsApp, leveraging 80%+ market penetration. This architecture makes continent-scale early warning systems economically viable, supporting UNDRR findings that such systems reduce disaster death rates by 6x. All architectural details are documented inline for full reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17726v1</guid>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qness Ndlovu</dc:creator>
    </item>
    <item>
      <title>Stop Saying "AI"</title>
      <link>https://arxiv.org/abs/2602.17729</link>
      <description>arXiv:2602.17729v1 Announce Type: new 
Abstract: Across academia, industry, and government, ``AI'' has become central in research and development, regulatory debates, and promises of ever faster and more capable decision-making and action. In numerous domains, especially safety-critical ones, there are significant concerns over how ``AI'' may affect decision-making, responsibility, or the likelihood of mistakes (to name only a few categories of critique). However, for most critiques, the target is generally ``AI'', a broad term admitting many (types of) systems used for a variety of tasks and each coming with its own set of limitations, challenges, and potential use cases. In this article, we focus on the military domain as a case study and present both a loose enumerative taxonomy of systems captured under the umbrella term ``military AI'', as well as discussion of the challenges of each. In doing so, we highlight that critiques of one (type of) system will not always transfer to other (types of) systems. Building on this, we argue that in order for debates to move forward fruitfully, it is imperative that the discussions be made more precise and that ``AI'' be excised from debates to the extent possible. Researchers, developers, and policy-makers should make clear exactly what systems they have in mind and what possible benefits and risks attend the deployment of those particular systems. While we focus on AI in the military as an exemplar for the overall trends in discussions of ``AI'', the argument's conclusions are broad and have import for discussions of AI across a host of domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17729v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan G. Wood (Institute of Air Transportation Systems, Hamburg University of Technology, Ethics + Emerging Sciences Group, California Polytechnic State University San Luis Obispo, Center for Environmental and Technology Ethics - Prague), Scott Robbins (Academy for Responsible Research, Teaching, and Innovation, Karlsruhe Institute of Technology), Eduardo Zegarra Berodt (Institute of Air Transportation Systems, Hamburg University of Technology), Anton Graf von Westerholt (Institute of Air Transportation Systems, Hamburg University of Technology), Michelle Behrndt (Institute of Air Transportation Systems, Hamburg University of Technology, Department of Philosophy, University of Hamburg), Daniel Kloock-Schreiber (Institute of Air Transportation Systems, Hamburg University of Technology)</dc:creator>
    </item>
    <item>
      <title>The 2025 AI Agent Index: Documenting Technical and Safety Features of Deployed Agentic AI Systems</title>
      <link>https://arxiv.org/abs/2602.17753</link>
      <description>arXiv:2602.17753v1 Announce Type: new 
Abstract: Agentic AI systems are increasingly capable of performing professional and personal tasks with limited human involvement. However, tracking these developments is difficult because the AI agent ecosystem is complex, rapidly evolving, and inconsistently documented, posing obstacles to both researchers and policymakers. To address these challenges, this paper presents the 2025 AI Agent Index. The Index documents information regarding the origins, design, capabilities, ecosystem, and safety features of 30 state-of-the-art AI agents based on publicly available information and email correspondence with developers. In addition to documenting information about individual agents, the Index illuminates broader trends in the development of agents, their capabilities, and the level of transparency of developers. Notably, we find different transparency levels among agent developers and observe that most developers share little information about safety, evaluations, and societal impacts. The 2025 AI Agent Index is available online at https://aiagentindex.mit.edu</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17753v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leon Staufer, Kevin Feng, Kevin Wei, Luke Bailey, Yawen Duan, Mick Yang, A. Pinar Ozisik, Stephen Casper, Noam Kolt</dc:creator>
    </item>
    <item>
      <title>The Digital Divide in Generative AI: Evidence from Large Language Model Use in College Admissions Essays</title>
      <link>https://arxiv.org/abs/2602.17791</link>
      <description>arXiv:2602.17791v1 Announce Type: new 
Abstract: Large language models (LLMs) have become popular writing tools among students and may expand access to high-quality feedback for students with less access to traditional writing support. At the same time, LLMs may standardize student voice or invite overreliance. This study examines how adoption of LLM-assisted writing varies across socioeconomic groups and how it relates to outcomes in a high-stakes context: U.S. college admissions. We analyze a de-identified longitudinal dataset of applications to a selective university from 2020 to 2024 (N = 81,663). Estimating LLM use using a distribution-based detector trained on synthetic and historical essays, we tracked how student writing changed as LLM use proliferated, how adoption differed by socioeconomic status (SES), and whether potential benefits translated equitably into admissions outcomes. Using fee-waiver status as a proxy for SES, we observe post-2023 convergence in surface-level linguistic features, with the largest changes in fee-waived and rejected applicants. Estimated LLM use rose sharply in 2024 across all groups, with disproportionately larger increases among lower SES applicants, consistent with an access hypothesis in which LLMs substitute for scarce writing support. However, increased estimated LLM use was more strongly associated with declines in predicted admission probability for lower SES applicants than for higher SES applicants, even after controlling for academic credentials and stylometric features. These findings raise concerns about equity and the validity of essay-based evaluation in an era of AI-assisted writing and provide the first large-scale longitudinal evidence linking LLM adoption, linguistic change, and evaluative outcomes in college admissions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17791v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jinsook Lee, Conrad Borchers, AJ Alvero, Thorsten Joachims, Rene F. Kizilcec</dc:creator>
    </item>
    <item>
      <title>Strange Undercurrents: A Critical Outlook on AI's Cultural Influence</title>
      <link>https://arxiv.org/abs/2602.17841</link>
      <description>arXiv:2602.17841v1 Announce Type: new 
Abstract: While generative artificial intelligence (generative AI) is being examined extensively, some issues it epitomizes call for more refined scrutiny and deeper contextualization. Besides the lack of nuanced understanding of art's continuously changing character in discussions about generative AI's cultural impact, one of the notably underexplored aspects is the conceptual and ideological substrate of AI science and industry whose attributes generative AI propagates by fostering the integration of diverse modes of AI-powered artmaking into the mainstream culture and economy. Taking the current turmoil around the generative AI as a pretext, this paper summarizes a broader study of AI's influence on art notions focusing on the confluence of certain foundational concepts in computer science and ideological vectors of the AI industry that transfer into art, culture, and society. This influence merges diverse and sometimes inconsistent but somehow coalescing philosophical premises, technical ideas, and political views, many of which have unfavorable overtones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17841v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.34626/2024_xcoax_011</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the Twelfth Conference on Computation, Communication, Aesthetics &amp; X, 2024</arxiv:journal_reference>
      <dc:creator>Dejan Grba</dc:creator>
    </item>
    <item>
      <title>Visual Anthropomorphism Shifts Evaluations of Gendered AI Managers</title>
      <link>https://arxiv.org/abs/2602.17919</link>
      <description>arXiv:2602.17919v1 Announce Type: new 
Abstract: This research examines whether competence cues can reduce gender bias in evaluations of AI managers and whether these effects depend on how the AI is represented. Across two preregistered experiments (N = 2,505), each employing a 2 x 2 x 3 design manipulating AI gender, competence, and decision outcome, we compared text-based descriptions of AI managers with visually generated AI faces created using a reverse-correlation paradigm. In the text condition, evaluations were driven by competence rather than gender. When participants received unfavourable decisions, high-competence AI managers were judged as fairer, more competent, and better leaders than low-competence managers, regardless of AI gender. In contrast, when the AI manager was visually represented, competence cues had attenuated influence once facial information was present. Instead, participants showed systematic gender-differentiated responses to AI faces, with feminine-appearing managers evaluated as more competent and more trustworthy than masculine-appearing managers, particularly when delivering favourable outcomes. These gender effects were largely absent when outcomes were unfavourable, suggesting that negative feedback attenuates the influence of both competence information and facial cues. Taken together, these findings show that competence information can mitigate negative reactions to AI managers in text-based interactions, whereas facial anthropomorphism elicits gendered perceptual biases not observed in text-only settings. The results highlight that representational modality plays a critical role in determining when gender stereotypes are activated in evaluations of AI systems and underscore that design choices are consequential for AI governance in evaluative contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17919v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiqing Han, Hao Cui, Taha Yasseri</dc:creator>
    </item>
    <item>
      <title>Operational Agency: A Permeable Legal Fiction for Tracing Culpability in AI Systems</title>
      <link>https://arxiv.org/abs/2602.17932</link>
      <description>arXiv:2602.17932v1 Announce Type: new 
Abstract: Modern artificial intelligence (AI) systems act with a high degree of independence yet lack legal personhood-a paradox that fractures doctrines grounded in human-centric notions of mens rea and actus reus. This Article introduces Operational Agency (OA)-a permeable legal fiction structured as an ex post evidentiary framework-and Operational Agency Graph (OAG), a tool for mapping causal interactions among human actors, organizations, and AI systems. OA evaluates an AI's observable operational characteristics: its goal-directedness (as a proxy for intent), predictive processing (as a proxy for foresight), and safety architecture (as a proxy for a standard of care). OAG operationalizes that analysis by embedding these characteristics in a causal graph to trace and apportion culpability among developers, fine-tuners, deployers, and users. Drawing on corporate criminal liability, the innocent-agent doctrine, and secondary and vicarious liability frameworks, the Article shows how OA and OAG strengthen existing doctrines. Across five real-world case studies spanning tort, civil rights, constitutional law, and antitrust, it demonstrates how the framework addresses challenges ranging from autonomous vehicle collisions to algorithmic price-fixing, offering courts a principled evidentiary method-and legislatures and industry a conceptual foundation-to ensure human accountability keeps pace with technological autonomy, without conferring personhood on AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17932v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anirban Mukherjee, Hannah Hanwen Chang</dc:creator>
    </item>
    <item>
      <title>Atrial Fibrillation Detection Using Machine Learning</title>
      <link>https://arxiv.org/abs/2602.18036</link>
      <description>arXiv:2602.18036v1 Announce Type: new 
Abstract: Atrial fibrillation (AF) is a common cardiac arrhythmia and a major risk factor for ischemic stroke. Early detection of AF using non-invasive signals can enable timely intervention. In this work, we present a comprehensive machine learning framework for AF detection from simultaneous photoplethysmogram (PPG) and electrocardiogram (ECG) signals. We partitioned continuous recordings from 35 subjects into 525 segments (15 segments of 10,000 samples each at 125Hz per subject). After data cleaning to remove segments with missing samples, 481 segments remained (263 AF, 218 normal).
  We extracted 22 features per segment, including time-domain statistics (mean, standard deviation, skewness, etc.), bandpower, and heart-rate variability metrics from both PPG and ECG signals. Three classifiers -- ensemble of bagged decision trees, cubic-kernel support vector machine (SVM), and subspace k-nearest neighbors (KNN) -- were trained and evaluated using 10-fold cross-validation and hold-out testing. The subspace KNN achieved the highest test accuracy (98.7\%), slightly outperforming bagged trees (97.9\%) and cubic SVM (97.1\%). Sensitivity (AF detection) and specificity (normal rhythm detection) were all above 95\% for the top-performing models.
  The results indicate that ensemble-based machine learning models using combined PPG and ECG features can effectively detect atrial fibrillation. A comparative analysis of model performance along with strengths and limitations of the proposed framework is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18036v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ankit Singh, Vidhi Thakur, Nachiket Tapas</dc:creator>
    </item>
    <item>
      <title>Demonstrating Restraint</title>
      <link>https://arxiv.org/abs/2602.18139</link>
      <description>arXiv:2602.18139v1 Announce Type: new 
Abstract: Some have claimed that the future development of powerful AI systems would enable the United States to shift the international balance of power dramatically in its favor. Such a feat may not be technically possible; even so, if American AI development is perceived as a sufficiently severe threat by its nation-state adversaries, then the risk that they take extreme preventive action against the United States may rise. To bolster its security against preventive action, the United States could aim to pursue a strategy of restraint by demonstrating that it would not use powerful AI to threaten the survival of other nations. Drawing from the international relations literature that explores how states can make credible commitments, we sketch a set of options that the United States could employ to implement this strategy. In the most challenging setting, where it is certain that the US will unilaterally obtain powerful new capabilities, it is difficult to credibly commit to restraint, though an approach that layers significant policy effort with technical breakthroughs may make credibility achievable. If an adversary has realistic levels of uncertainty about the capabilities and intentions of the United States, a strategy of restraint becomes more feasible. Though restraint faces difficulties, it deserves to be weighed against alternative strategies that have been proposed for avoiding conflict during the transition to a world with advanced AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18139v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L. C. R. Patell, O. E. Guest</dc:creator>
    </item>
    <item>
      <title>Computer Vision in Tactical AI Art</title>
      <link>https://arxiv.org/abs/2602.18189</link>
      <description>arXiv:2602.18189v1 Announce Type: new 
Abstract: AI art comprises a spectrum of creative endeavors that emerge from and respond to the development of artificial intelligence (AI), the expansion of AI-powered economies, and their influence on culture and society. Within this repertoire, the relationship between the cognitive value of human vision and the wide application range of computer vision (CV) technologies opens a sizeable space for exploring the problematic sociopolitical aspects of automated inference and decision-making in modern AI. In this paper, I examine the art practices critically engaged with the notions and protocols of CV. After identifying and contextualizing the CV-related tactical AI art, I discuss the features of exemplar artworks in four interrelated subject areas. Their topical imbrications, common critical points, and shared pitfalls plot a wider landscape of tactical AI art, allowing me to detect factors that affect its poetic cogency, social responsibility, and political impact, some of which exist in the theoretical premises of digital art activism. Along these lines, I outline the routes for addressing the challenges and advancing the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18189v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.36922/ac.2282</arxiv:DOI>
      <arxiv:journal_reference>Arts &amp; Communication Journal, 2024</arxiv:journal_reference>
      <dc:creator>Dejan Grba</dc:creator>
    </item>
    <item>
      <title>Art Notions in the Age of (Mis)anthropic AI</title>
      <link>https://arxiv.org/abs/2602.18202</link>
      <description>arXiv:2602.18202v1 Announce Type: new 
Abstract: In this paper, I take the cultural effects of generative artificial intelligence (generative AI) as a context for examining a broader perspective of AI's impact on contemporary art notions. After the introductory overview of generative AI, I summarize the distinct but often confused aspects of art notions and review the principal lines in which AI influences them: the strategic normalization of AI through art, the representation of AI art in the artworld, academia, and AI research, and the mutual permeability of art and kitsch in the digital culture. I connect these notional factors with the conceptual and ideological substrate of the computer science and AI industry, which blends the machinic agency fetishism, the equalization of computers and humans, the sociotechnical blindness, and cyberlibertarianism. The overtones of alienation, sociopathy, and misanthropy in the disparate but somehow coalescing philosophical premises, technical ideas, and political views in this substrate remain underexposed in AI studies so, in the closing discussion, I outline their manifestations in generative AI and introduce several viewpoints for a further critique of AI's cultural zeitgeist. They add a touch of skepticism to pondering how technological trends change our understanding of art and in which directions they stir its social, economic, and political roles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18202v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.3390/arts13050137</arxiv:DOI>
      <arxiv:journal_reference>JDMI Arts, 13, (5), 2024</arxiv:journal_reference>
      <dc:creator>Dejan Grba</dc:creator>
    </item>
    <item>
      <title>SMaRT: Online Reusable Resource Assignment and an Application to Mediation in the Kenyan Judiciary</title>
      <link>https://arxiv.org/abs/2602.18431</link>
      <description>arXiv:2602.18431v1 Announce Type: new 
Abstract: Motivated by the problem of assigning mediators to cases in the Kenyan judicial, we study an online resource allocation problem where incoming tasks (cases) must be immediately assigned to available, capacity-constrained resources (mediators). The resources differ in their quality, which may need to be learned. In addition, resources can only be assigned to a subset of tasks that overlaps to varying degrees with the subset of tasks other resources can be assigned to. The objective is to maximize task completion while satisfying soft capacity constraints across all the resources. The scale of the real-world problem poses substantial challenges, since there are over 2000 mediators and a multitude of combinations of geographic locations (87) and case types (12) that each mediator is qualified to work on. Together, these features, unknown quality of new resources, soft capacity constraints, and a high-dimensional state space, make existing scheduling and resource allocation algorithms either inapplicable or inefficient. We formalize the problem in a tractable manner using a quadratic program formulation for assignment and a multi-agent bandit-style framework for learning. We demonstrate the key properties and advantages of our new algorithm, SMaRT (Selecting Mediators that are Right for the Task), compared with baselines on stylized instances of the mediator allocation problem. We then consider its application to real-world data on cases and mediators from the Kenyan judiciary. SMaRT outperforms baselines and allows control over the tradeoff between the strictness of capacity constraints and overall case resolution rates, both in settings where mediator quality is known beforehand and in bandit-like settings where learning is part of the problem definition. On the strength of these results, we plan to run a randomized controlled trial with SMaRT in the judiciary in the near future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18431v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shafkat Farabi, Didac Marti Pinto, Wei Lu, Manuel Ramos-Maqueda, Sanmay Das, Antoine Deeb, Anja Sautmann</dc:creator>
    </item>
    <item>
      <title>Assessing LLM Response Quality in the Context of Technology-Facilitated Abuse</title>
      <link>https://arxiv.org/abs/2602.17672</link>
      <description>arXiv:2602.17672v1 Announce Type: cross 
Abstract: Technology-facilitated abuse (TFA) is a pervasive form of intimate partner violence (IPV) that leverages digital tools to control, surveil, or harm survivors. While tech clinics are one of the reliable sources of support for TFA survivors, they face limitations due to staffing constraints and logistical barriers. As a result, many survivors turn to online resources for assistance. With the growing accessibility and popularity of large language models (LLMs), and increasing interest from IPV organizations, survivors may begin to consult LLM-based chatbots before seeking help from tech clinics.
  In this work, we present the first expert-led manual evaluation of four LLMs - two widely used general-purpose non-reasoning models and two domain-specific models designed for IPV contexts - focused on their effectiveness in responding to TFA-related questions. Using real-world questions collected from literature and online forums, we assess the quality of zero-shot single-turn LLM responses generated with a survivor safety-centered prompt on criteria tailored to the TFA domain. Additionally, we conducted a user study to evaluate the perceived actionability of these responses from the perspective of individuals who have experienced TFA.
  Our findings, grounded in both expert assessment and user feedback, provide insights into the current capabilities and limitations of LLMs in the TFA context and may inform the design, development, and fine-tuning of future models for this domain. We conclude with concrete recommendations to improve LLM performance for survivor support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17672v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Vijay Prakash, Majed Almansoori, Donghan Hu, Rahul Chatterjee, Danny Yuxing Huang</dc:creator>
    </item>
    <item>
      <title>Digital self-Efficacy as a foundation for a generative AI usage framework in faculty's professional practices</title>
      <link>https://arxiv.org/abs/2602.17673</link>
      <description>arXiv:2602.17673v1 Announce Type: cross 
Abstract: This research explores the role of digital self-efficacy in the appropriation of generative artificial intelligence (GAI) by higher education faculty. Drawing on Bandura's sociocognitive theory and Flichy's concept of usage framework, our study examines the relationships between levels of digital self-efficacy and GAI usage profiles. A survey of 265 faculty members identified three user profiles (Engaged, Reflective Reserved, Critical Resisters) and validated a three-dimensional digital self-efficacy scale. Results reveal a significant association between self-efficacy profiles and GAI appropriation patterns. Based on these findings, we propose a differentiated usage framework integrating four sociotechnical configurations, appropriation trajectories adapted to self-efficacy profiles, and personalized institutional support mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17673v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fatiha Tali (EFTS, LINE, Grhapes)</dc:creator>
    </item>
    <item>
      <title>Mind the Style: Impact of Communication Style on Human-Chatbot Interaction</title>
      <link>https://arxiv.org/abs/2602.17850</link>
      <description>arXiv:2602.17850v1 Announce Type: cross 
Abstract: Conversational agents increasingly mediate everyday digital interactions, yet the effects of their communication style on user experience and task success remain unclear. Addressing this gap, we describe the results of a between-subject user study where participants interact with one of two versions of a chatbot called NAVI which assists users in an interactive map-based 2D navigation task. The two chatbot versions differ only in communication style: one is friendly and supportive, while the other is direct and task-focused. Our results show that the friendly style increases subjective satisfaction and significantly improves task completion rates among female participants only, while no baseline differences between female and male participants were observed in a control condition without the chatbot. Furthermore, we find little evidence of users mimicking the chatbot's style, suggesting limited linguistic accommodation. These findings highlight the importance of user- and task-sensitive conversational agents and support that communication style personalization can meaningfully enhance interaction quality and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17850v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erik Derner, Dalibor Ku\v{c}era, Aditya Gulati, Ayoub Bagheri, Nuria Oliver</dc:creator>
    </item>
    <item>
      <title>Perceived Political Bias in LLMs Reduces Persuasive Abilities</title>
      <link>https://arxiv.org/abs/2602.18092</link>
      <description>arXiv:2602.18092v1 Announce Type: cross 
Abstract: Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18092v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew DiGiuseppe, Joshua Robison</dc:creator>
    </item>
    <item>
      <title>The Statistical Signature of LLMs</title>
      <link>https://arxiv.org/abs/2602.18152</link>
      <description>arXiv:2602.18152v1 Announce Type: cross 
Abstract: Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18152v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ortal Hadad, Edoardo Loru, Jacopo Nudo, Niccol\`o Di Marco, Matteo Cinelli, Walter Quattrociocchi</dc:creator>
    </item>
    <item>
      <title>Role and Identity Work of Software Engineering Professionals in the Generative AI Era</title>
      <link>https://arxiv.org/abs/2602.18190</link>
      <description>arXiv:2602.18190v1 Announce Type: cross 
Abstract: The adoption of Generative AI (GenAI) suggests major changes for software engineering, including technical aspects but also human aspects of the professionals involved. One of these aspects is how individuals perceive themselves regarding their work, i.e., their work identity, and the processes they perform to form, adapt and reject these identities, i.e., identity work. Existent studies provide evidence of such identity work of software professionals triggered by the adoption of GenAI, however they do not consider differences among diverse roles, such as developers and testers. In this paper, we argue the need for considering the role as a factor defining the identity work of software professionals. To support our claim, we review some studies regarding different roles and also recent studies on how to adopt GenAI in software engineering. Then, we propose a research agenda to better understand how the role influences identity work of software professionals triggered by the adoption of GenAI, and, based on that, to propose new artifacts to support this adoption. We also discuss the potential implications for practice of the results to be obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18190v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3794860.3794913</arxiv:DOI>
      <dc:creator>Jorge Melegati</dc:creator>
    </item>
    <item>
      <title>Understanding and Mitigating the Impacts of Differentially Private Census Data on State Level Redistricting</title>
      <link>https://arxiv.org/abs/2409.06801</link>
      <description>arXiv:2409.06801v2 Announce Type: replace 
Abstract: Data from the Decennial Census is published only after applying a disclosure avoidance system (DAS). Data users were shaken by the adoption of differential privacy in the 2020 DAS, a radical departure from past methods. The goal of this paper is to better understand how the perturbations from the 2020 DAS combine with sharp legal thresholds to impact redistricting. We consider two redistricting settings in which a data user might be concerned about the impacts of privacy preserving noise: drawing equal population districts and litigating voting rights cases. What discrepancies arise if the user does nothing to account for disclosure avoidance? How can the discrepancies be understood and accounted for? We study these questions by comparing the official 2010 Redistricting Data to the 2010 Demonstration Data--created using the 2020 DAS--in an analysis of millions of algorithmically generated state legislative redistricting plans. We find that thresholding can amplify the impact of the noise from disclosure avoidance. Large discrepancies do occur, but in ways that are well-captured by simple models and appear to be possible to account for. We demonstrate the utility of these models by proposing an approach to mitigate discrepancies when balancing district populations. At least for state legislatures, Alabama's claim that differential privacy "inhibits a State's right to draw fair lines" lacks support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06801v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Cianfarani, Aloni Cohen</dc:creator>
    </item>
    <item>
      <title>Imitating AI agents increase diversity in homogeneous information environments but can reduce it in heterogeneous ones</title>
      <link>https://arxiv.org/abs/2503.16021</link>
      <description>arXiv:2503.16021v4 Announce Type: replace 
Abstract: Recent developments in large language models (LLMs) have facilitated autonomous AI agents capable of imitating human-generated content, raising fundamental questions about how AI may reshape democratic information environments such as news. We develop a large-scale simulation framework to examine the system-level effects of AI-based imitation, using the full population of Danish digital news articles published in 2022. Varying imitation strategies and AI prevalence across information environments with different baseline structures, we show that the effects of AI-driven imitation are strongly context-dependent: imitating AI agents increase semantic diversity in initially homogeneous environments but can reduce diversity in heterogeneous ones. This pattern is qualitatively consistent across multiple LLMs. However, this diversity arises primarily through stylistic differentiation and variance compression rather than factual enrichment, as AI-generated articles tend to omit information while remaining semantically distinct. These findings indicate that AI-driven imitation produces ambivalent transformations of information environments that may shape collective intelligence in democratic societies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16021v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emil Bakkensen Johansen, Oliver Baumann</dc:creator>
    </item>
    <item>
      <title>Fluid Agency in AI Systems: A Case for Functional Equivalence in Copyright, Patent, and Tort</title>
      <link>https://arxiv.org/abs/2601.02633</link>
      <description>arXiv:2601.02633v2 Announce Type: replace 
Abstract: Modern Artificial Intelligence (AI) systems lack human-like consciousness or culpability, yet they exhibit fluid agency: behavior that is (i) stochastic (probabilistic and path-dependent), (ii) dynamic (co-evolving with user interaction), and (iii) adaptive (able to reorient across contexts). Fluid agency generates valuable outputs but collapses attribution, irreducibly entangling human and machine inputs. This fundamental unmappability fractures doctrines that assume traceable provenance -- authorship, inventorship, and liability -- yielding ownership gaps and moral "crumple zones." This Article argues that only functional equivalence stabilizes doctrine. Where provenance is indeterminate, legal frameworks must treat human and AI contributions as equivalent for allocating rights and responsibility -- not as a claim of moral or economic parity but as a pragmatic default. This principle stabilizes doctrine across domains, offering administrable rules: in copyright, vesting ownership in human orchestrators without parsing inseparable contributions; in patent, tying inventor-of-record status to human orchestration and reduction to practice, even when AI supplies the pivotal insight; and in tort, replacing intractable causation inquiries with enterprise-level and sector-specific strict or no-fault schemes. The contribution is both descriptive and normative: fluid agency explains why origin-based tests fail, while functional equivalence supplies an outcome-focused framework to allocate rights and responsibility when attribution collapses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02633v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Wash. J. L. Tech. &amp; Arts (2026). Available at: https://digitalcommons.law.uw.edu/wjlta/vol21/iss1/3</arxiv:journal_reference>
      <dc:creator>Anirban Mukherjee, Hannah Hanwen Chang</dc:creator>
    </item>
    <item>
      <title>Peaceful Anarcho-Accelerationism: Decentralized Full Automation for a Society of Universal Care</title>
      <link>https://arxiv.org/abs/2602.13154</link>
      <description>arXiv:2602.13154v5 Announce Type: replace 
Abstract: Foundational results in machine learning establish that all human labor may in principle be automatable. Consequently, this paper introduces peaceful anarcho-accelerationism, a rigorously defined sociotechnical framework grounded in the 200-year anarchist tradition from Godwin through Kropotkin to Bookchin, and in the methodological categories of Eltzbacher, Nettlau, and Correa, for ensuring that full automation is decentralized, commons-governed, and oriented toward universal care. We state five formal hypotheses and six research objectives, present a formal definition through analytical categories of interdependent spheres, and propose the Liberation Stack as a layered technical architecture with explicit preconditions and gate conditions for each layer. Moreover, we introduce Universal Desired Resources as a post-monetary design principle that eliminates the material basis of intersectional oppression, and develop a framework for progressive state dissolution through incremental, reversible commons-building compatible with existing democratic institutions. A nonviolent social mobilization strategy maps concrete peaceful methods to each stage of transition. We show that accelerationism and degrowth share anarchist pacifism as substrate and differ only along a Pareto-optimal technological frontier. Empirical evidence from Linux, Wikipedia, Mondragon, Rojava, guifi.net, the Fediverse, and contemporary commons initiatives confirms that commons-based systems already operate at scale. We conclude with a phased roadmap specifying explicit assumptions, hard constraints, gate conditions between phases, and detailed limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13154v5</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduardo C. Garrido-Merch\'an</dc:creator>
    </item>
    <item>
      <title>Governance of Generative Artificial Intelligence for Companies</title>
      <link>https://arxiv.org/abs/2403.08802</link>
      <description>arXiv:2403.08802v5 Announce Type: replace-cross 
Abstract: Generative Artificial Intelligence (GenAI), specifically large language models (LLMs) like ChatGPT, has swiftly entered organizations without adequate governance, posing both opportunities and risks. Despite extensive debate on GenAI's transformative potential and emerging regulatory measures, limited research addresses organizational governance from both technical and business perspectives. While frameworks for AI governance exist, it remains unclear to what extent they apply to GenAI. This review paper fills this gap by surveying recent literature to better understand the fundamental characteristics of GenAI and to adapt existing governance frameworks specifically to GenAI within organizations. To this end, it extends Nickerson's framework development process by incorporating prior conceptualizations. The resulting framework delineates scope, objectives, and governance mechanisms designed to both harness business opportunities and mitigate risks associated with GenAI integration. Overall, this research advances a focused approach to GenAI governance, offering practical guidance for companies navigating the challenges of GenAI adoption and highlighting research gaps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08802v5</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Schneider, Pauline Kuss, Rene Abraham, Christian Meske</dc:creator>
    </item>
    <item>
      <title>I, Robot? Exploring Ultra-Personalized AI-Powered AAC; an Autoethnographic Account</title>
      <link>https://arxiv.org/abs/2509.13671</link>
      <description>arXiv:2509.13671v3 Announce Type: replace-cross 
Abstract: Generic AI auto-complete for message composition often fails to capture the nuance of personal identity, requiring editing. While harmless in low-stakes settings, for users of Augmentative and Alternative Communication (AAC) devices, who rely on such systems to communicate, this burden is severe. Intuitively, the need for edits would be lower if language models were personalized to the specific user's communication. While personalization is technically feasible, it raises questions about how such systems affect AAC users' agency, identity, and privacy. We conducted an autoethnographic study in three phases: (1) seven months of collecting all the lead author's AAC communication data, (2) fine-tuning a model on this dataset, and (3) three months of daily use of personalized AI suggestions. We observed that: logging everyday conversations reshaped the author's sense of agency, model training selectively amplified or muted aspects of his identity, and suggestions occasionally resurfaced private details outside their original context. We find that ultra-personalized AAC reshapes communication by continually renegotiating agency, identity, and privacy between user and model. We highlight design directions for building personalized AAC technology that supports expressive, authentic communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13671v3</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790310</arxiv:DOI>
      <dc:creator>Tobias M. Weinberg, Ricardo E. Gonzalez Penuela, Stephanie Valencia, Thijs Roumen</dc:creator>
    </item>
    <item>
      <title>The AI Pyramid A Conceptual Framework for Workforce Capability in the Age of AI</title>
      <link>https://arxiv.org/abs/2601.06500</link>
      <description>arXiv:2601.06500v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) represents a qualitative shift in technological change by extending cognitive labor itself rather than merely automating routine tasks. Recent evidence shows that generative AI disproportionately affects highly educated, white collar work, challenging existing assumptions about workforce vulnerability and rendering traditional approaches to digital or AI literacy insufficient. This paper introduces the concept of AI Nativity, the capacity to integrate AI fluidly into everyday reasoning, problem solving, and decision making, and proposes the AI Pyramid, a conceptual framework for organizing human capability in an AI mediated economy. The framework distinguishes three interdependent capability layers: AI Native capability as a universal baseline for participation in AI augmented environments; AI Foundation capability for building, integrating, and sustaining AI enabled systems; and AI Deep capability for advancing frontier AI knowledge and applications. Crucially, the pyramid is not a career ladder but a system level distribution of capabilities required at scale. Building on this structure, the paper argues that effective AI workforce development requires treating capability formation as infrastructure rather than episodic training, centered on problem based learning embedded in work contexts and supported by dynamic skill ontologies and competency based measurement. The framework has implications for organizations, education systems, and governments seeking to align learning, measurement, and policy with the evolving demands of AI mediated work, while addressing productivity, resilience, and inequality at societal scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06500v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alok Khatri (NAAMII, Nepal, Tangible Careers), Bishesh Khanal (NAAMII, Nepal, Tangible Careers)</dc:creator>
    </item>
    <item>
      <title>Framing Responsible Design of AI for Mental Well-Being: AI as Primary Care, Nutritional Supplement, or Yoga Instructor?</title>
      <link>https://arxiv.org/abs/2602.02740</link>
      <description>arXiv:2602.02740v2 Announce Type: replace-cross 
Abstract: Millions of people now use non-clinical Large Language Model (LLM) tools like ChatGPT for mental well-being support. This paper investigates what it means to design such tools responsibly, and how to operationalize that responsibility in their design and evaluation. By interviewing experts and analyzing related regulations, we found that designing an LLM tool responsibly involves: (1) Articulating the specific benefits it guarantees and for whom. Does it guarantee specific, proven relief, like an over-the-counter drug, or offer minimal guarantees, like a nutritional supplement? (2) Specifying the LLM tool's "active ingredients" for improving well-being and whether it guarantees their effective delivery (like a primary care provider) or not (like a yoga instructor). These specifications outline an LLM tool's pertinent risks, appropriate evaluation metrics, and the respective responsibilities of LLM developers, tool designers, and users. These analogies - LLM tools as supplements, drugs, yoga instructors, and primary care providers - can scaffold further conversations about their responsible design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02740v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ned Cooper, Jose A. Guridi, Angel Hsing-Chi Hwang, Beth Kolko, Emma Elizabeth McGinty, Qian Yang</dc:creator>
    </item>
    <item>
      <title>VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration</title>
      <link>https://arxiv.org/abs/2602.04587</link>
      <description>arXiv:2602.04587v2 Announce Type: replace-cross 
Abstract: This paper describes VILLAIN, a multimodal fact-checking system that verifies image-text claims through prompt-based multi-agent collaboration. For the AVerImaTeC shared task, VILLAIN employs vision-language model agents across multiple stages of fact-checking. Textual and visual evidence is retrieved from the knowledge store enriched through additional web collection. To identify key information and address inconsistencies among evidence items, modality-specific and cross-modal agents generate analysis reports. In the subsequent stage, question-answer pairs are produced based on these reports. Finally, the Verdict Prediction agent produces the verification outcome based on the image-text claim and the generated question-answer pairs. Our system ranked first on the leaderboard across all evaluation metrics. The source code is publicly available at https://github.com/ssu-humane/VILLAIN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04587v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jaeyoon Jung, Yejun Yoon, Kunwoo Park</dc:creator>
    </item>
    <item>
      <title>The Global Representativeness Index: A Total Variation Distance Framework for Measuring Demographic Fidelity in Survey Research</title>
      <link>https://arxiv.org/abs/2602.14835</link>
      <description>arXiv:2602.14835v2 Announce Type: replace-cross 
Abstract: Global survey research increasingly informs high-stakes decisions in AI governance and cross-cultural policy, yet no standardized metric quantifies how well a sample's demographic composition matches its target population. Response rates and demographic quotas -- the prevailing proxies for sample quality -- measure effort and coverage but not distributional fidelity. This paper introduces the Global Representativeness Index (GRI), a framework grounded in Total Variation Distance that scores any survey sample against population benchmarks across multiple demographic dimensions on a [0, 1] scale. Validation on seven waves of the Global Dialogues survey (N = 7,500 across 60+ countries) finds fine-grained demographic GRI scores of only 0.33--0.36 -- roughly 43% of the theoretical maximum at that sample size. Cross-validation on the World Values Survey (seven waves, N = 403,000), Afrobarometer Round 9 (N = 53,000), and Latinobarometro (N = 19,000) reveals that even large probability surveys score below 0.22 on fine-grained global demographics when country coverage is limited. The GRI connects to classical survey statistics through the design effect; both metrics are recommended as a minimum summary of sample quality, since GRI quantifies demographic distance symmetrically while effective N captures the asymmetric inferential cost of underrepresentation. The framework is released as an open-source Python library with UN and Pew Research Center population benchmarks, applicable to survey research, machine learning dataset auditing, and AI evaluation benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14835v2</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evan Hadfield, Andrew Konya</dc:creator>
    </item>
  </channel>
</rss>
