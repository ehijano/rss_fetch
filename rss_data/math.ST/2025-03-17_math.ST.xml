<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Mar 2025 03:26:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Nonparametric Exponential Family Regression Under Star-Shaped Constraints</title>
      <link>https://arxiv.org/abs/2503.10794</link>
      <description>arXiv:2503.10794v1 Announce Type: new 
Abstract: We study the minimax rate of estimation in nonparametric exponential family regression under star-shaped constraints. Specifically, the parameter space $K$ is a star-shaped set contained within a bounded box $[-M, M]^n$, where $M$ is a known positive constant. Moreover, we assume that the exponential family is nonsingular and that its cumulant function is twice continuously differentiable. Our main result shows that the minimax rate for this problem is $\varepsilon^{*2} \wedge \operatorname{diam}(K)^2$, up to absolute constants, where $\varepsilon^*$ is defined as
  \[ \varepsilon^* = \sup \{\varepsilon: \varepsilon^2 \kappa(M) \leq \log N^{\operatorname{loc}}(\varepsilon)\}, \]
  with $N^{\operatorname{loc}}(\varepsilon)$ denoting the local entropy and $\kappa(M)$ is an absolute constant allowed to depend on $M$. We also provide an example and derive its corresponding minimax optimal rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10794v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanghong Yi, Matey Neykov</dc:creator>
    </item>
    <item>
      <title>The pushed beta distribution and contaminated binary sampling</title>
      <link>https://arxiv.org/abs/2503.11128</link>
      <description>arXiv:2503.11128v1 Announce Type: new 
Abstract: We examine a generalisation of the beta distribution that we call the pushed beta distribution. This is a continuous univariate distribution on the unit interval which generalises the beta distribution by "pushing" the density in a particular direction using an additional multiplicative term in the density kernel. We examine the properties of this distribution and compare it to the beta distribution. We also examine the use of this distribution in contaminated binary sampling using Bayesian inference. We find that this distribution arises as the appropriate posterior distribution for inference in certain kinds of contaminated binary models. We derive a broad range of properties of the distribution and we also establish some computational methods to compute various functions for the distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11128v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ben O'Neill</dc:creator>
    </item>
    <item>
      <title>On continuity of Chatterjee's rank correlation and related dependence measures</title>
      <link>https://arxiv.org/abs/2503.11390</link>
      <description>arXiv:2503.11390v1 Announce Type: new 
Abstract: While measures of concordance -- such as Spearman's rho, Kendall's tau, and Blomqvist's beta -- are continuous with respect to weak convergence, Chatterjee's rank correlation xi recently introduced in Azadkia and Chatterjee [5] does not share this property, causing drawbacks in statistical inference as pointed out in B\"ucher and Dette [7]. As we study in this paper, xi is instead weakly continuous with respect to conditionally independent copies -- the Markov products. To establish weak continuity of Markov products, we provide several sufficient conditions, including copula-based criteria and conditions relying on the concept of conditional weak convergence in Sweeting [36]. As a consequence, we also obtain continuity results for xi and related dependence measures and verify their continuity in the parameters of standard models such as multivariate elliptical and l1-norm symmetric distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11390v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Ansari, Sebastian Fuchs</dc:creator>
    </item>
    <item>
      <title>A New Design-Based Variance Estimator for Finely Stratified Experiments</title>
      <link>https://arxiv.org/abs/2503.10851</link>
      <description>arXiv:2503.10851v1 Announce Type: cross 
Abstract: This paper considers the problem of design-based inference for the average treatment effect in finely stratified experiments. Here, by "design-based'' we mean that the only source of uncertainty stems from the randomness in treatment assignment; by "finely stratified'' we mean units are first stratified into groups of size k according to baseline covariates and then, within each group, a fixed number l &lt; k are assigned uniformly at random to treatment and the remainder to control. In this setting, we first show under mild conditions that inference using the difference-in-means estimator requires an estimator of its variance that is at least asymptotically upward-biased. We then present a novel estimator of the variance and show that it is upward-biased; furthermore, the magnitude of the bias depends in a natural way on the quality of the stratification. Importantly, this estimator remains well-defined even in the setting in which l = 1 or k - l = 1. We then compare our estimator with some well-known estimators that have been proposed previously for this case. We first show that, while these estimators are also upward-biased, the magnitude of their bias does not change in the natural way with the quality of stratification. To further discriminate among these estimators, we introduce a framework motivated by a thought experiment in which the finite population can be modeled as having been drawn once in an i.i.d. fashion from a well-behaved probability distribution. In this framework, we argue that our estimator dominates the others in terms of limiting bias, and that these improvements are strict except under exceptionally strong restrictions on the treatment effects. Finally, we illustrate our theoretical results through a simulation study, which reveals that our estimator can lead to substantially more precise inferences, especially when the quality of stratification is high.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10851v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Xun Huang, Joseph P. Romano, Azeem M. Shaikh, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>Statistical Impossibility and Possibility of Aligning LLMs with Human Preferences: From Condorcet Paradox to Nash Equilibrium</title>
      <link>https://arxiv.org/abs/2503.10990</link>
      <description>arXiv:2503.10990v1 Announce Type: cross 
Abstract: Aligning large language models (LLMs) with diverse human preferences is critical for ensuring fairness and informed outcomes when deploying these models for decision-making. In this paper, we seek to uncover fundamental statistical limits concerning aligning LLMs with human preferences, with a focus on the probabilistic representation of human preferences and the preservation of diverse preferences in aligned LLMs. We first show that human preferences can be represented by a reward model if and only if the preference among LLM-generated responses is free of any Condorcet cycle. Moreover, we prove that Condorcet cycles exist with probability converging to one exponentially fast under a probabilistic preference model, thereby demonstrating the impossibility of fully aligning human preferences using reward-based approaches such as reinforcement learning from human feedback. Next, we explore the conditions under which LLMs would employ mixed strategies -- meaning they do not collapse to a single response -- when aligned in the limit using a non-reward-based approach, such as Nash learning from human feedback (NLHF). We identify a necessary and sufficient condition for mixed strategies: the absence of a response that is preferred over all others by a majority. As a blessing, we prove that this condition holds with high probability under the probabilistic preference model, thereby highlighting the statistical possibility of preserving minority preferences without explicit regularization in aligning LLMs. Finally, we leverage insights from our statistical results to design a novel, computationally efficient algorithm for finding Nash equilibria in aligning LLMs with NLHF. Our experiments show that Llama-3.2-1B, aligned with our algorithm, achieves a win rate of 60.55\% against the base model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10990v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaizhao Liu, Qi Long, Zhekun Shi, Weijie J. Su, Jiancong Xiao</dc:creator>
    </item>
    <item>
      <title>Towards practical PDMP sampling: Metropolis adjustments, locally adaptive step-sizes, and NUTS-based time lengths</title>
      <link>https://arxiv.org/abs/2503.11479</link>
      <description>arXiv:2503.11479v1 Announce Type: cross 
Abstract: Piecewise-Deterministic Markov Processes (PDMPs) hold significant promise for sampling from complex probability distributions. However, their practical implementation is hindered by the need to compute model-specific bounds. Conversely, while Hamiltonian Monte Carlo (HMC) offers a generally efficient approach to sampling, its inability to adaptively tune step sizes impedes its performance when sampling complex distributions like funnels.
  To address these limitations, we introduce three innovative concepts: (a) a Metropolis-adjusted approximation for PDMP simulation that eliminates the need for explicit bounds without compromising the invariant measure, (b) an adaptive step size mechanism compatible with the Metropolis correction, and (c) a No U-Turn Sampler (NUTS)-inspired scheme for dynamically selecting path lengths in PDMPs. These three ideas can be seamlessly integrated into a single, `doubly-adaptive' PDMP sampler with favourable robustness and efficiency properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11479v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Augustin Chevallier, Sam Power, Matthew Sutton</dc:creator>
    </item>
    <item>
      <title>The only admissible way of merging arbitrary e-values</title>
      <link>https://arxiv.org/abs/2409.19888</link>
      <description>arXiv:2409.19888v2 Announce Type: replace 
Abstract: We prove that the only admissible way of merging arbitrary e-values is to use a weighted arithmetic average. This result completes the picture of merging methods for e-values, and generalizes the result of Vovk and Wang (2021, Annals of Statistics, 49(3), 1736--1754) that the only admissible way of symmetrically merging e-values is to use the arithmetic average combined with a constant. Although the proved statement is naturally anticipated, its proof relies on a sophisticated application of optimal transport duality and a minimax theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19888v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing with e-values</title>
      <link>https://arxiv.org/abs/2410.23614</link>
      <description>arXiv:2410.23614v3 Announce Type: replace 
Abstract: This book is written to offer a humble, but unified, treatment of e-values in hypothesis testing. The book is organized into three parts: Fundamental Concepts, Core Ideas, and Advanced Topics. The first part includes three chapters that introduce the basic concepts. The second part includes five chapters of core ideas such as universal inference, log-optimality, e-processes, operations on e-values, and e-values in multiple testing. The third part contains five chapters of advanced topics. We hope that, by putting the materials together in this book, the concept of e-values becomes more accessible for educational, research, and practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23614v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaditya Ramdas, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Parameter estimation for partially observed second-order diffusion processes</title>
      <link>https://arxiv.org/abs/2406.14738</link>
      <description>arXiv:2406.14738v2 Announce Type: replace-cross 
Abstract: Estimating parameters of a diffusion process given continuous-time observations of the process via maximum likelihood approaches or, online, via stochastic gradient descent or Kalman filter formulations constitutes a well-established research area. It has also been established previously that these techniques are, in general, not robust to perturbations in the data in the form of temporal correlations of the driving noise. While the subject is relatively well understood and appropriate modifications have been suggested in the context of multi-scale diffusion processes and their reduced model equations, we consider here an alternative but related setting where a diffusion process in positions and velocities is only observed via its positions. In this note, we propose a simple modification to standard stochastic gradient descent and Kalman filter formulations, which eliminates the arising systematic estimation biases. The modification can be extended to standard maximum likelihood approaches and avoids computation of previously proposed correction terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14738v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Albrecht, Sebastian Reich</dc:creator>
    </item>
    <item>
      <title>Theoretical Insights into CycleGAN: Analyzing Approximation and Estimation Errors in Unpaired Data Generation</title>
      <link>https://arxiv.org/abs/2407.11678</link>
      <description>arXiv:2407.11678v2 Announce Type: replace-cross 
Abstract: In this paper, we focus on analyzing the excess risk of the unpaired data generation model, called CycleGAN. Unlike classical GANs, CycleGAN not only transforms data between two unpaired distributions but also ensures the mappings are consistent, which is encouraged by the cycle-consistency term unique to CycleGAN. The increasing complexity of model structure and the addition of the cycle-consistency term in CycleGAN present new challenges for error analysis. By considering the impact of both the model architecture and training procedure, the risk is decomposed into two terms: approximation error and estimation error. These two error terms are analyzed separately and ultimately combined by considering the trade-off between them. Each component is rigorously analyzed; the approximation error through constructing approximations of the optimal transport maps, and the estimation error through establishing an upper bound using Rademacher complexity. Our analysis not only isolates these errors but also explores the trade-offs between them, which provides a theoretical insights of how CycleGAN's architecture and training procedures influence its performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11678v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luwei Sun, Dongrui Shen, Han Feng</dc:creator>
    </item>
    <item>
      <title>Is a Good Foundation Necessary for Efficient Reinforcement Learning? The Computational Role of the Base Model in Exploration</title>
      <link>https://arxiv.org/abs/2503.07453</link>
      <description>arXiv:2503.07453v2 Announce Type: replace-cross 
Abstract: Language model alignment (or, reinforcement learning) techniques that leverage active exploration -- deliberately encouraging the model to produce diverse, informative responses -- offer the promise of super-human capabilities. However, current understanding of algorithm design primitives for computationally efficient exploration with language models is limited. To better understand how to leverage access to powerful pre-trained generative models to improve the efficiency of exploration, we introduce a new computational framework for RL with language models, in which the learner interacts with the model through a sampling oracle. Focusing on the linear softmax model parameterization, we provide new results that reveal the computational-statistical tradeoffs of efficient exploration:
  1. Necessity of coverage: Coverage refers to the extent to which the pre-trained model covers near-optimal responses -- a form of hidden knowledge. We show that coverage, while not necessary for data efficiency, lower bounds the runtime of any algorithm in our framework.
  2. Inference-time exploration: We introduce a new algorithm, SpannerSampling, which obtains optimal data efficiency and is computationally efficient whenever the pre-trained model enjoys sufficient coverage, matching our lower bound. SpannerSampling leverages inference-time computation with the pre-trained model to reduce the effective search space for exploration.
  3. Insufficiency of training-time interventions: We contrast the result above by showing that training-time interventions that produce proper policies cannot achieve similar guarantees in polynomial time.
  4. Computational benefits of multi-turn exploration: Finally, we show that under additional representational assumptions, one can achieve improved runtime (replacing sequence-level coverage with token-level coverage) through multi-turn exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07453v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dylan J. Foster, Zakaria Mhammedi, Dhruv Rohatgi</dc:creator>
    </item>
  </channel>
</rss>
