<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Aug 2024 04:00:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Weighted Brier Score -- an Overall Summary Measure for Risk Prediction Models with Clinical Utility Consideration</title>
      <link>https://arxiv.org/abs/2408.01626</link>
      <description>arXiv:2408.01626v1 Announce Type: new 
Abstract: As advancements in novel biomarker-based algorithms and models accelerate disease risk prediction and stratification in medicine, it is crucial to evaluate these models within the context of their intended clinical application. Prediction models output the absolute risk of disease; subsequently, patient counseling and shared decision-making are based on the estimated individual risk and cost-benefit assessment. The overall impact of the application is often referred to as clinical utility, which received significant attention in terms of model assessment lately. The classic Brier score is a popular measure of prediction accuracy; however, it is insufficient for effectively assessing clinical utility. To address this limitation, we propose a class of weighted Brier scores that aligns with the decision-theoretic framework of clinical utility. Additionally, we decompose the weighted Brier score into discrimination and calibration components, examining how weighting influences the overall score and its individual components. Through this decomposition, we link the weighted Brier score to the $H$ measure, which has been proposed as a coherent alternative to the area under the receiver operating characteristic curve. This theoretical link to the $H$ measure further supports our weighting method and underscores the essential elements of discrimination and calibration in risk prediction evaluation. The practical use of the weighted Brier score as an overall summary is demonstrated using data from the Prostate Cancer Active Surveillance Study (PASS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01626v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kehao Zhu, Yingye Zheng, Kwun Chuen Gary Chan</dc:creator>
    </item>
    <item>
      <title>On Nonparametric Estimation of Covariograms</title>
      <link>https://arxiv.org/abs/2408.01628</link>
      <description>arXiv:2408.01628v1 Announce Type: new 
Abstract: The paper overviews and investigates several nonparametric methods of estimating covariograms. It provides a unified approach and notation to compare the main approaches used in applied research. The primary focus is on methods that utilise the actual values of observations, rather than their ranks. We concentrate on such desirable properties of covariograms as bias, positive-definiteness and behaviour at large distances. The paper discusses several theoretical properties and demonstrates some surprising drawbacks of well-known estimators. Numerical studies provide a comparison of representatives from different methods using various metrics. The results provide important insight and guidance for practitioners who use estimated covariograms in various applications, including kriging, monitoring network optimisation, cross-validation, and other related tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01628v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Bilchouris, Andriy Olenko</dc:creator>
    </item>
    <item>
      <title>Principal component analysis balancing prediction and approximation accuracy for spatial data</title>
      <link>https://arxiv.org/abs/2408.01662</link>
      <description>arXiv:2408.01662v1 Announce Type: new 
Abstract: Dimension reduction is often the first step in statistical modeling or prediction of multivariate spatial data. However, most existing dimension reduction techniques do not account for the spatial correlation between observations and do not take the downstream modeling task into consideration when finding the lower-dimensional representation. We formalize the closeness of approximation to the original data and the utility of lower-dimensional scores for downstream modeling as two complementary, sometimes conflicting, metrics for dimension reduction. We illustrate how existing methodologies fall into this framework and propose a flexible dimension reduction algorithm that achieves the optimal trade-off. We derive a computationally simple form for our algorithm and illustrate its performance through simulation studies, as well as two applications in air pollution modeling and spatial transcriptomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01662v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Si Cheng, Magali N. Blanco, Timothy V. Larson, Lianne Sheppard, Adam Szpiro, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Minimum Gamma Divergence for Regression and Classification Problems</title>
      <link>https://arxiv.org/abs/2408.01893</link>
      <description>arXiv:2408.01893v1 Announce Type: new 
Abstract: The book is structured into four main chapters. Chapter 1 introduces the foundational concepts of divergence measures, including the well-known Kullback-Leibler divergence and its limitations. It then presents a detailed exploration of power divergences, such as the $\alpha$, $\beta$, and $\gamma$-divergences, highlighting their unique properties and advantages. Chapter 2 explores minimum divergence methods for regression models, demonstrating how these methods can improve robustness and efficiency in statistical estimation. Chapter 3 extends these methods to Poisson point processes, with a focus on ecological applications, providing a robust framework for modeling species distributions and other spatial phenomena. Finally, Chapter 4 explores the use of divergence measures in machine learning, including applications in Boltzmann machines, AdaBoost, and active learning. The chapter emphasizes the practical benefits of these measures in enhancing model robustness and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01893v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shinto Eguchi</dc:creator>
    </item>
    <item>
      <title>Multivariate Information Measures: A Copula-based Approach</title>
      <link>https://arxiv.org/abs/2408.02028</link>
      <description>arXiv:2408.02028v1 Announce Type: new 
Abstract: Multivariate datasets are common in various real-world applications. Recently, copulas have received significant attention for modeling dependencies among random variables. A copula-based information measure is required to quantify the uncertainty inherent in these dependencies. This paper introduces a multivariate variant of the cumulative copula entropy and explores its various properties, including bounds, stochastic orders, and convergence-related results. Additionally, we define a cumulative copula information generating function and derive it for several well-known families of multivariate copulas. A fractional generalization of the multivariate cumulative copula entropy is also introduced and examined. We present a non-parametric estimator of the cumulative copula entropy using empirical beta copula. Furthermore, we propose a new distance measure between two copulas based on the Kullback-Leibler divergence and discuss a goodness-of-fit test based on this measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02028v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohd. Arshad, Swaroop Georgy Zachariah, Ashok Kumar Pathak</dc:creator>
    </item>
    <item>
      <title>SPINEX-TimeSeries: Similarity-based Predictions with Explainable Neighbors Exploration for Time Series and Forecasting Problems</title>
      <link>https://arxiv.org/abs/2408.02159</link>
      <description>arXiv:2408.02159v1 Announce Type: new 
Abstract: This paper introduces a new addition to the SPINEX (Similarity-based Predictions with Explainable Neighbors Exploration) family, tailored specifically for time series and forecasting analysis. This new algorithm leverages the concept of similarity and higher-order temporal interactions across multiple time scales to enhance predictive accuracy and interpretability in forecasting. To evaluate the effectiveness of SPINEX, we present comprehensive benchmarking experiments comparing it against 18 algorithms and across 49 synthetic and real datasets characterized by varying trends, seasonality, and noise levels. Our performance assessment focused on forecasting accuracy and computational efficiency. Our findings reveal that SPINEX consistently ranks among the top 5 performers in forecasting precision and has a superior ability to handle complex temporal dynamics compared to commonly adopted algorithms. Moreover, the algorithm's explainability features, Pareto efficiency, and medium complexity (on the order of O(log n)) are demonstrated through detailed visualizations to enhance the prediction and decision-making process. We note that integrating similarity-based concepts opens new avenues for research in predictive analytics, promising more accurate and transparent decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02159v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmed Z Naser, MZ Naser</dc:creator>
    </item>
    <item>
      <title>Explaining and Connecting Kriging with Gaussian Process Regression</title>
      <link>https://arxiv.org/abs/2408.02331</link>
      <description>arXiv:2408.02331v1 Announce Type: new 
Abstract: Kriging and Gaussian Process Regression are statistical methods that allow predicting the outcome of a random process or a random field by using a sample of correlated observations. In other words, the random process or random field is partially observed, and by using a sample a prediction is made, pointwise or as a whole, where the latter can be thought as a reconstruction. In addition, the techniques permit to give a measure of uncertainty of the prediction. The methods have different origins. Kriging comes from geostatistics, a field which started to develop around 1950 oriented to mining valuation problems, whereas Gaussian Process Regression has gained popularity in the area of machine learning in the last decade of the previous century. In the literature, the methods are usually presented as being the same technique. However, beyond this affirmation, the techniques have yet not been compared on a thorough mathematical basis and neither explained why and under which conditions this affirmation holds. Furthermore, Kriging has many variants and this affirmation should be precised. In this paper, this gap is filled. It is shown, step by step how both methods are deduced from the first principles -- with a major focus on Kriging, the mathematical connection between them, and which Kriging variant corresponds to which Gaussian Process Regression set up. The three most widely used versions of Kriging are considered: Simple Kriging, Ordinary Kriging and Universal Kriging. It is found, that despite their closeness, the techniques are different in their approach and assumptions, in a similar way the Least Square method, the Best Linear Unbiased Estimator method, and the Likelihood method in regression do. I hope this work can serve for a deeper understanding of the relationship between Kriging and Gaussian Process Regression, as well as a cohesive introductory resource for researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02331v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Marinescu</dc:creator>
    </item>
    <item>
      <title>Unified Principal Components Analysis of Irregularly Observed Functional Time Series</title>
      <link>https://arxiv.org/abs/2408.02343</link>
      <description>arXiv:2408.02343v1 Announce Type: new 
Abstract: Irregularly observed functional time series (FTS) are increasingly available in many real-world applications. To analyze FTS, it is crucial to account for both serial dependencies and the irregularly observed nature of functional data. However, existing methods for FTS often rely on specific model assumptions in capturing serial dependencies, or cannot handle the irregular observational scheme of functional data. To solve these issues, one can perform dimension reduction on FTS via functional principal component analysis (FPCA) or dynamic FPCA. Nonetheless, these methods may either be not theoretically optimal or too redundant to represent serially dependent functional data. In this article, we introduce a novel dimension reduction method for FTS based on dynamic FPCA. Through a new concept called optimal functional filters, we unify the theories of FPCA and dynamic FPCA, providing a parsimonious and optimal representation for FTS adapting to its serial dependence structure. This framework is referred to as principal analysis via dependency-adaptivity (PADA). Under a hierarchical Bayesian model, we establish an estimation procedure for dimension reduction via PADA. Our method can be used for both sparsely and densely observed FTS, and is capable of predicting future functional data. We investigate the theoretical properties of PADA and demonstrate its effectiveness through extensive simulation studies. Finally, we illustrate our method via dimension reduction and prediction of daily PM2.5 data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02343v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zerui Guo, Jianbin Tan, Hui Huang</dc:creator>
    </item>
    <item>
      <title>Graphical Modelling without Independence Assumptions for Uncentered Data</title>
      <link>https://arxiv.org/abs/2408.02393</link>
      <description>arXiv:2408.02393v1 Announce Type: new 
Abstract: The independence assumption is a useful tool to increase the tractability of one's modelling framework. However, this assumption does not match reality; failing to take dependencies into account can cause models to fail dramatically. The field of multi-axis graphical modelling (also called multi-way modelling, Kronecker-separable modelling) has seen growth over the past decade, but these models require that the data have zero mean. In the multi-axis case, inference is typically done in the single sample scenario, making mean inference impossible.
  In this paper, we demonstrate how the zero-mean assumption can cause egregious modelling errors, as well as propose a relaxation to the zero-mean assumption that allows the avoidance of such errors. Specifically, we propose the "Kronecker-sum-structured mean" assumption, which leads to models with nonconvex-but-unimodal log-likelihoods that can be solved efficiently with coordinate descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02393v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bailey Andrew, David R. Westhead, Luisa Cutillo</dc:creator>
    </item>
    <item>
      <title>The appeal of the gamma family distribution to protect the confidentiality of contingency tables</title>
      <link>https://arxiv.org/abs/2408.02513</link>
      <description>arXiv:2408.02513v1 Announce Type: new 
Abstract: Administrative databases, such as the English School Census (ESC), are rich sources of information that are potentially useful for researchers. For such data sources to be made available, however, strict guarantees of privacy would be required. To achieve this, synthetic data methods can be used. Such methods, when protecting the confidentiality of tabular data (contingency tables), often utilise the Poisson or Poisson-mixture distributions, such as the negative binomial (NBI). These distributions, however, are either equidispersed (in the case of the Poisson) or overdispersed (e.g. in the case of the NBI), which results in excessive noise being applied to large low-risk counts. This paper proposes the use of the (discretized) gamma family (GAF) distribution, which allows noise to be applied in a more bespoke fashion. Specifically, it allows less noise to be applied as cell counts become larger, providing an optimal balance in relation to the risk-utility trade-off. We illustrate the suitability of the GAF distribution on an administrative-type data set that is reminiscent of the ESC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02513v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Jackson, Robin Mitra, Brian Francis, Iain Dove</dc:creator>
    </item>
    <item>
      <title>Time-series imputation using low-rank matrix completion</title>
      <link>https://arxiv.org/abs/2408.02594</link>
      <description>arXiv:2408.02594v1 Announce Type: new 
Abstract: We investigate the use of matrix completion methods for time-series imputation. Specifically we consider low-rank completion of the block-Hankel matrix representation of a time-series. Simulation experiments are used to compare the method with five recognised imputation techniques with varying levels of computational effort. The Hankel Imputation (HI) method is seen to perform competitively at interpolating missing time-series data, and shows particular potential for reproducing sharp peaks in the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02594v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Poudevigne, Owen Jones</dc:creator>
    </item>
    <item>
      <title>Evaluating and Utilizing Surrogate Outcomes in Covariate-Adjusted Response-Adaptive Designs</title>
      <link>https://arxiv.org/abs/2408.02667</link>
      <description>arXiv:2408.02667v1 Announce Type: new 
Abstract: This manuscript explores the intersection of surrogate outcomes and adaptive designs in statistical research. While surrogate outcomes have long been studied for their potential to substitute long-term primary outcomes, current surrogate evaluation methods do not directly account for the potential benefits of using surrogate outcomes to adapt randomization probabilities in adaptive randomized trials that aim to learn and respond to treatment effect heterogeneity. In this context, surrogate outcomes can benefit participants in the trial directly (i.e. improve expected outcome of newly-enrolled participants) by allowing for more rapid adaptation of randomization probabilities, particularly when surrogates enable earlier detection of heterogeneous treatment effects and/or indicate the optimal (individualized) treatment with stronger signals. Our study introduces a novel approach for surrogate evaluation that quantifies both of these benefits in the context of sequential adaptive experiment designs. We also propose a new Covariate-Adjusted Response-Adaptive (CARA) design that incorporates an Online Superlearner to assess and adaptively choose surrogate outcomes for updating treatment randomization probabilities. We introduce a Targeted Maximum Likelihood Estimator that addresses data dependency challenges in adaptively collected data and achieves asymptotic normality under reasonable assumptions without relying on parametric model assumptions. The robust performance of our adaptive design with Online Superlearner is presented via simulations. Our framework not only contributes a method to more comprehensively quantifying the benefits of candidate surrogate outcomes and choosing between them, but also offers an easily generalizable tool for evaluating various adaptive designs and making inferences, providing insights into alternative choices of designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02667v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxin Zhang, Aaron Hudson, Maya Petersen, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Conformal Diffusion Models for Individual Treatment Effect Estimation and Inference</title>
      <link>https://arxiv.org/abs/2408.01582</link>
      <description>arXiv:2408.01582v1 Announce Type: cross 
Abstract: Estimating treatment effects from observational data is of central interest across numerous application domains. Individual treatment effect offers the most granular measure of treatment effect on an individual level, and is the most useful to facilitate personalized care. However, its estimation and inference remain underdeveloped due to several challenges. In this article, we propose a novel conformal diffusion model-based approach that addresses those intricate challenges. We integrate the highly flexible diffusion modeling, the model-free statistical inference paradigm of conformal inference, along with propensity score and covariate local approximation that tackle distributional shifts. We unbiasedly estimate the distributions of potential outcomes for individual treatment effect, construct an informative confidence interval, and establish rigorous theoretical guarantees. We demonstrate the competitive performance of the proposed method over existing solutions through extensive numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01582v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengrui Cai, Huaqing Jin, Lexin Li</dc:creator>
    </item>
    <item>
      <title>Review and Demonstration of a Mixture Representation for Simulation from Densities Involving Sums of Powers</title>
      <link>https://arxiv.org/abs/2408.01617</link>
      <description>arXiv:2408.01617v1 Announce Type: cross 
Abstract: Penalized and robust regression, especially when approached from a Bayesian perspective, can involve the problem of simulating a random variable $\boldsymbol z$ from a posterior distribution that includes a term proportional to a sum of powers, $\|\boldsymbol z \|^q_q$, on the log scale. However, many popular gradient-based methods for Markov Chain Monte Carlo simulation from such posterior distributions use Hamiltonian Monte Carlo and accordingly require conditions on the differentiability of the unnormalized posterior distribution that do not hold when $q \leq 1$ (Plummer, 2023). This is limiting; the setting where $q \leq 1$ includes widely used sparsity inducing penalized regression models and heavy tailed robust regression models. In the special case where $q = 1$, a latent variable representation that facilitates simulation from such a posterior distribution is well known. However, the setting where $q &lt; 1$ has not been treated as thoroughly. In this note, we review the availability of a latent variable representation described in Devroye (2009), show how it can be used to simulate from such posterior distributions when $0 &lt; q &lt; 2$, and demonstrate its utility in the context of estimating the parameters of a Bayesian penalized regression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01617v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maryclare Griffin</dc:creator>
    </item>
    <item>
      <title>Efficient Decision Trees for Tensor Regressions</title>
      <link>https://arxiv.org/abs/2408.01926</link>
      <description>arXiv:2408.01926v1 Announce Type: cross 
Abstract: We proposed the tensor-input tree (TT) method for scalar-on-tensor and tensor-on-tensor regression problems. We first address scalar-on-tensor problem by proposing scalar-output regression tree models whose input variable are tensors (i.e., multi-way arrays). We devised and implemented fast randomized and deterministic algorithms for efficient fitting of scalar-on-tensor trees, making TT competitive against tensor-input GP models. Based on scalar-on-tensor tree models, we extend our method to tensor-on-tensor problems using additive tree ensemble approaches. Theoretical justification and extensive experiments on real and synthetic datasets are provided to illustrate the performance of TT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01926v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengrui Luo, Akira Horiguchi, Li Ma</dc:creator>
    </item>
    <item>
      <title>Winners with Confidence: Discrete Argmin Inference with an Application to Model Selection</title>
      <link>https://arxiv.org/abs/2408.02060</link>
      <description>arXiv:2408.02060v1 Announce Type: cross 
Abstract: We study the problem of finding the index of the minimum value of a vector from noisy observations. This problem is relevant in population/policy comparison, discrete maximum likelihood, and model selection. We develop a test statistic that is asymptotically normal, even in high-dimensional settings and with potentially many ties in the population mean vector, by integrating concepts and tools from cross-validation and differential privacy. The key technical ingredient is a central limit theorem for globally dependent data. We also propose practical ways to select the tuning parameter that adapts to the signal landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02060v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Hao Lee, Jing Lei</dc:creator>
    </item>
    <item>
      <title>A Functional Data Approach for Structural Health Monitoring</title>
      <link>https://arxiv.org/abs/2408.02106</link>
      <description>arXiv:2408.02106v1 Announce Type: cross 
Abstract: Structural Health Monitoring (SHM) is increasingly applied in civil engineering. One of its primary purposes is detecting and assessing changes in structure conditions to increase safety and reduce potential maintenance downtime. Recent advancements, especially in sensor technology, facilitate data measurements, collection, and process automation, leading to large data streams. We propose a function-on-function regression framework for (nonlinear) modeling the sensor data and adjusting for covariate-induced variation. Our approach is particularly suited for long-term monitoring when several months or years of training data are available. It combines highly flexible yet interpretable semi-parametric modeling with functional principal component analysis and uses the corresponding out-of-sample phase-II scores for monitoring. The method proposed can also be described as a combination of an ``input-output'' and an ``output-only'' method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02106v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Wittenberg, Lizzie Neumann, Alexander Mendler, Jan Gertheiss</dc:creator>
    </item>
    <item>
      <title>Graph-Enabled Fast MCMC Sampling with an Unknown High-Dimensional Prior Distribution</title>
      <link>https://arxiv.org/abs/2408.02122</link>
      <description>arXiv:2408.02122v1 Announce Type: cross 
Abstract: Posterior sampling is a task of central importance in Bayesian inference. For many applications in Bayesian meta-analysis and Bayesian transfer learning, the prior distribution is unknown and needs to be estimated from samples. In practice, the prior distribution can be high-dimensional, adding to the difficulty of efficient posterior inference. In this paper, we propose a novel Markov chain Monte Carlo algorithm, which we term graph-enabled MCMC, for posterior sampling with unknown and potentially high-dimensional prior distributions. The algorithm is based on constructing a geometric graph from prior samples and subsequently uses the graph structure to guide the transition of the Markov chain. Through extensive theoretical and numerical studies, we demonstrate that our graph-enabled MCMC algorithm provides reliable approximation to the posterior distribution and is highly computationally efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02122v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyang Zhong, Shouxuan Ji, Tian Zheng</dc:creator>
    </item>
    <item>
      <title>The uniform general signed rank test and its design sensitivity</title>
      <link>https://arxiv.org/abs/1904.08895</link>
      <description>arXiv:1904.08895v2 Announce Type: replace 
Abstract: A sensitivity analysis in an observational study tests whether the qualitative conclusions of an analysis would change if we were to allow for the possibility of limited bias due to confounding. The design sensitivity of a hypothesis test quantifies the asymptotic performance of the test in a sensitivity analysis against a particular alternative. We propose a new, non-asymptotic, distribution-free test, the uniform general signed rank test, for observational studies with paired data, and examine its performance under Rosenbaum's sensitivity analysis model. Our test can be viewed as adaptively choosing from among a large underlying family of signed rank tests, and we show that the uniform test achieves design sensitivity equal to the maximum design sensitivity over the underlying family of signed rank tests. Our test thus achieves superior, and sometimes infinite, design sensitivity, indicating it will perform well in sensitivity analyses on large samples. We support this conclusion with simulations and a data example, showing that the advantages of our test extend to moderate sample sizes as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:1904.08895v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Biometrika 108, 381-396 (2021)</arxiv:journal_reference>
      <dc:creator>Steven R. Howard, Samuel D. Pimentel</dc:creator>
    </item>
    <item>
      <title>Design-Based Inference for Spatial Experiments under Unknown Interference</title>
      <link>https://arxiv.org/abs/2010.13599</link>
      <description>arXiv:2010.13599v5 Announce Type: replace 
Abstract: We consider design-based causal inference for spatial experiments in which treatments may have effects that bleed out and feed back in complex ways. Such spatial spillover effects violate the standard ``no interference'' assumption for standard causal inference methods. The complexity of spatial spillover effects also raises the risk of misspecification and bias in model-based analyses. We offer an approach for robust inference in such settings without having to specify a parametric outcome model. We define a spatial ``average marginalized effect'' (AME) that characterizes how, in expectation, units of observation that are a specified distance from an intervention location are affected by treatment at that location, averaging over effects emanating from other intervention nodes. We show that randomization is sufficient for non-parametric identification of the AME even if the nature of interference is unknown. Under mild restrictions on the extent of interference, we establish asymptotic distributions of estimators and provide methods for both sample-theoretic and randomization-based inference. We show conditions under which the AME recovers a structural effect. We illustrate our approach with a simulation study. Then we re-analyze a randomized field experiment and a quasi-experiment on forest conservation, showing how our approach offers robust inference on policy-relevant spillover effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.13599v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Wang, Cyrus Samii, Haoge Chang, P. M. Aronow</dc:creator>
    </item>
    <item>
      <title>A stochastic network approach to clustering and visualising single-cell genomic count data</title>
      <link>https://arxiv.org/abs/2303.02498</link>
      <description>arXiv:2303.02498v4 Announce Type: replace 
Abstract: Important tasks in the study of genomic data include the identification of groups of similar cells (for example by clustering), and visualisation of data summaries (for example by dimensional reduction). In this paper, we propose a novel approach to studying single-cell genomic data, by modelling the observed genomic data count matrix $\mathbf{X}\in\mathbb{Z}_{\geq0}^{p\times n}$ as a bipartite network with multi-edges. Utilising this first-principles network representation of the raw data, we propose clustering single cells in a suitably identified $d$-dimensional Laplacian Eigenspace (LE) via a Gaussian mixture model (GMM-LE), and employing UMAP to non-linearly project the LE to two dimensions for visualisation (UMAP-LE). This LE representation of the data estimates transformed latent positions (of genes and cells), under a latent position model of nodes in a bipartite stochastic network. We demonstrate how these estimated latent positions can enable fine-grained clustering and visualisation of single-cell genomic data, by application to data from three recent genomics studies in different biological contexts. In each data application, clusters of cells independently learned by our proposed methodology are found to correspond to cells expressing specific marker genes that were independently defined by domain experts. In this validation setting, our proposed clustering methodology outperforms the industry-standard for these data. Furthermore, we validate components of the LE decomposition of the data by contrasting healthy cells from normal and at-risk groups in a machine-learning model, thereby identifying an LE cancer biomarker that significantly predicts long-term patient survival outcome in two independent validation cohorts with data from 1904 and 1091 individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.02498v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Thomas E. Bartlett, Swati Chandna, Sandipan Roy</dc:creator>
    </item>
    <item>
      <title>Priming bias versus post-treatment bias in experimental designs</title>
      <link>https://arxiv.org/abs/2306.01211</link>
      <description>arXiv:2306.01211v5 Announce Type: replace 
Abstract: Conditioning on variables affected by treatment can induce post-treatment bias when estimating causal effects. Although this suggests that researchers should measure potential moderators before administering the treatment in an experiment, doing so may also bias causal effect estimation if the covariate measurement primes respondents to react differently to the treatment. This paper formally analyzes this trade-off between post-treatment and priming biases in three experimental designs that vary when moderators are measured: pre-treatment, post-treatment, or a randomized choice between the two. We derive nonparametric bounds for interactions between the treatment and the moderator under each design and show how to use substantive assumptions to narrow these bounds. These bounds allow researchers to assess the sensitivity of their empirical findings to priming and post-treatment bias. We then apply the proposed methodology to a survey experiment on electoral messaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01211v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Blackwell, Jacob R. Brown, Sophie Hill, Kosuke Imai, Teppei Yamamoto</dc:creator>
    </item>
    <item>
      <title>Mode-wise Principal Subspace Pursuit and Matrix Spiked Covariance Model</title>
      <link>https://arxiv.org/abs/2307.00575</link>
      <description>arXiv:2307.00575v2 Announce Type: replace 
Abstract: This paper introduces a novel framework called Mode-wise Principal Subspace Pursuit (MOP-UP) to extract hidden variations in both the row and column dimensions for matrix data. To enhance the understanding of the framework, we introduce a class of matrix-variate spiked covariance models that serve as inspiration for the development of the MOP-UP algorithm. The MOP-UP algorithm consists of two steps: Average Subspace Capture (ASC) and Alternating Projection (AP). These steps are specifically designed to capture the row-wise and column-wise dimension-reduced subspaces which contain the most informative features of the data. ASC utilizes a novel average projection operator as initialization and achieves exact recovery in the noiseless setting. We analyze the convergence and non-asymptotic error bounds of MOP-UP, introducing a blockwise matrix eigenvalue perturbation bound that proves the desired bound, where classic perturbation bounds fail. The effectiveness and practical merits of the proposed framework are demonstrated through experiments on both simulated and real datasets. Lastly, we discuss generalizations of our approach to higher-order data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00575v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Runshi Tang, Ming Yuan, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>The projected dynamic linear model for time series on the sphere</title>
      <link>https://arxiv.org/abs/2308.14996</link>
      <description>arXiv:2308.14996v2 Announce Type: replace 
Abstract: Time series on the unit n-sphere arise in directional statistics, compositional data analysis, and many scientific fields. There are few models for such data, and the ones that exist suffer from several limitations: they are often computationally challenging to fit, many of them apply only to the circular case of n=2, and they are usually based on families of distributions that are not flexible enough to capture the complexities observed in real data. Furthermore, there is little work on Bayesian methods for spherical time series. To address these shortcomings, we propose a state space model based on the projected normal distribution that can be applied to spherical time series of arbitrary dimension. We describe how to perform fully Bayesian offline inference for this model using a simple and efficient Gibbs sampling algorithm, and we develop a Rao-Blackwellized particle filter to perform online inference for streaming data. In analyses of wind direction and energy market time series, we show that the proposed model outperforms competitors in terms of point, set, and density forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14996v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Zito, Daniel Kowal</dc:creator>
    </item>
    <item>
      <title>An efficient joint model for high dimensional longitudinal and survival data via generic association features</title>
      <link>https://arxiv.org/abs/2309.03714</link>
      <description>arXiv:2309.03714v2 Announce Type: replace 
Abstract: This paper introduces a prognostic method called FLASH that addresses the problem of joint modelling of longitudinal data and censored durations when a large number of both longitudinal and time-independent features are available. In the literature, standard joint models are either of the shared random effect or joint latent class type. Combining ideas from both worlds and using appropriate regularisation techniques, we define a new model with the ability to automatically identify significant prognostic longitudinal features in a high-dimensional context, which is of increasing importance in many areas such as personalised medicine or churn prediction. We develop an estimation methodology based on the EM algorithm and provide an efficient implementation. The statistical performance of the method is demonstrated both in extensive Monte Carlo simulation studies and on publicly available real-world datasets. Our method significantly outperforms the state-of-the-art joint models in predicting the latent class membership probability in terms of the C-index in a so-called ``real-time'' prediction setting, with a computational speed that is orders of magnitude faster than competing methods. In addition, our model automatically identifies significant features that are relevant from a practical perspective, making it interpretable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03714v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Van Tuan Nguyen, Adeline Fermanian, Agathe Guilloux, Antoine Barbieri, Sarah Zohar, Anne-Sophie Jannot, Simon Bussy</dc:creator>
    </item>
    <item>
      <title>Bayesian inference and cure rate modeling for event history data</title>
      <link>https://arxiv.org/abs/2310.06926</link>
      <description>arXiv:2310.06926v2 Announce Type: replace 
Abstract: Estimating model parameters of a general family of cure models is always a challenging task mainly due to flatness and multimodality of the likelihood function. In this work, we propose a fully Bayesian approach in order to overcome these issues. Posterior inference is carried out by constructing a Metropolis-coupled Markov chain Monte Carlo (MCMC) sampler, which combines Gibbs sampling for the latent cure indicators and Metropolis-Hastings steps with Langevin diffusion dynamics for parameter updates. The main MCMC algorithm is embedded within a parallel tempering scheme by considering heated versions of the target posterior distribution. It is demonstrated via simulations that the proposed algorithm freely explores the multimodal posterior distribution and produces robust point estimates, while it outperforms maximum likelihood estimation via the Expectation-Maximization algorithm. A by-product of our Bayesian implementation is to control the False Discovery Rate when classifying items as cured or not. Finally, the proposed method is illustrated in a real dataset which refers to recidivism for offenders released from prison; the event of interest is whether the offender was re-incarcerated after probation or not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06926v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panagiotis Papastamoulis, Fotios Milienos</dc:creator>
    </item>
    <item>
      <title>Conjugacy properties of multivariate unified skew-elliptical distributions</title>
      <link>https://arxiv.org/abs/2402.09837</link>
      <description>arXiv:2402.09837v3 Announce Type: replace 
Abstract: The broad class of multivariate unified skew-normal (SUN) distributions has been recently shown to possess important conjugacy properties. When used as priors for the coefficients vector in probit, tobit, and multinomial probit models, these distributions yield posteriors that still belong to the SUN family. Although this result has led to important advancements in Bayesian inference and computation, its applicability beyond likelihoods associated with fully-observed, discretized, or censored realizations from multivariate Gaussian models remains yet unexplored. This article covers such a gap by proving that the wider family of multivariate unified skew-elliptical (SUE) distributions, which extends SUNs to more general perturbations of elliptical densities, guarantees conjugacy for broader classes of models, beyond those relying on fully-observed, discretized or censored Gaussians. Such a result leverages the closure under linear combinations, conditioning and marginalization of SUE to prove that this family is conjugate to the likelihood induced by multivariate regression models for fully-observed, censored or dichotomized realizations from skew-elliptical distributions. This advancement enlarges the set of models that enable conjugate Bayesian inference to general formulations arising from elliptical and skew-elliptical families, including the multivariate Student's t and skew-t, among others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09837v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maicon J. Karling, Daniele Durante, Marc G. Genton</dc:creator>
    </item>
    <item>
      <title>Integer Programming for Learning Directed Acyclic Graphs from Non-identifiable Gaussian Models</title>
      <link>https://arxiv.org/abs/2404.12592</link>
      <description>arXiv:2404.12592v2 Announce Type: replace 
Abstract: We study the problem of learning directed acyclic graphs from continuous observational data, generated according to a linear Gaussian structural equation model. State-of-the-art structure learning methods for this setting have at least one of the following shortcomings: i) they cannot provide optimality guarantees and can suffer from learning sub-optimal models; ii) they rely on the stringent assumption that the noise is homoscedastic, and hence the underlying model is fully identifiable. We overcome these shortcomings and develop a computationally efficient mixed-integer programming framework for learning medium-sized problems that accounts for arbitrary heteroscedastic noise. We present an early stopping criterion under which we can terminate the branch-and-bound procedure to achieve an asymptotically optimal solution and establish the consistency of this approximate solution. In addition, we show via numerical experiments that our method outperforms state-of-the-art algorithms and is robust to noise heteroscedasticity, whereas the performance of some competing methods deteriorates under strong violations of the identifiability assumption. The software implementation of our method is available as the Python package \emph{micodag}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12592v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tong Xu, Armeen Taeb, Simge K\"u\c{c}\"ukyavuz, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Causal inference for N-of-1 trials</title>
      <link>https://arxiv.org/abs/2406.10360</link>
      <description>arXiv:2406.10360v2 Announce Type: replace 
Abstract: The aim of personalized medicine is to tailor treatment decisions to individuals' characteristics. N-of-1 trials are within-person crossover trials that hold the promise of targeting individual-specific effects. While the idea behind N-of-1 trials might seem simple, analyzing and interpreting N-of-1 trials is not straightforward. Here we ground N-of-1 trials in a formal causal inference framework and formalize intuitive claims from the N-of-1 trials literature. We focus on causal inference from a single N-of-1 trial and define a conditional average treatment effect (CATE) that represents a target in this setting, which we call the U-CATE. We discuss assumptions sufficient for identification and estimation of the U-CATE under different causal models where the treatment schedule is assigned at baseline. A simple mean difference is an unbiased, asymptotically normal estimator of the U-CATE in simple settings. We also consider settings where carryover effects, trends over time, time-varying common causes of the outcome, and outcome-outcome effects are present. In these more complex settings, we show that a time-varying g-formula identifies the U-CATE under explicit assumptions. Finally, we analyze data from N-of-1 trials about acne symptoms and show how different assumptions about the data generating process can lead to different analytical strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10360v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Piccininni, Mats J. Stensrud, Zachary Shahn, Stefan Konigorski</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Outcomes Truncated by Death and Missing Not at Random</title>
      <link>https://arxiv.org/abs/2406.10554</link>
      <description>arXiv:2406.10554v2 Announce Type: replace 
Abstract: In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10554v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Li, Yuan Liu, Shanshan Luo, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>The inverse Kalman filter</title>
      <link>https://arxiv.org/abs/2407.10089</link>
      <description>arXiv:2407.10089v2 Announce Type: replace 
Abstract: In this study, we introduce a new approach, the inverse Kalman filter (IKF), which enables accurate matrix-vector multiplication between a covariance matrix from a dynamic linear model and any real-valued vector with linear computational cost. We incorporate the IKF with the conjugate gradient algorithm, which substantially accelerates the computation of matrix inversion for a general form of covariance matrices, whereas other approximation approaches may not be directly applicable. We demonstrate the scalability and efficiency of the IKF approach through distinct applications, including nonparametric estimation of particle interaction functions and predicting incomplete lattices of correlated data, using both simulation and real-world observations, including cell trajectory and satellite radar interferogram.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10089v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Fang, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Randomized Controlled Trials of Service Interventions: The Impact of Capacity Constraints</title>
      <link>https://arxiv.org/abs/2407.21322</link>
      <description>arXiv:2407.21322v2 Announce Type: replace 
Abstract: Randomized controlled trials (RCTs), or experiments, are the gold standard for intervention evaluation. However, the main appeal of RCTs, the clean identification of causal effects, can be compromised by interference, when one subject's treatment assignment can influence another subject's behavior or outcomes. In this paper, we formalise and study a type of interference stemming from the operational implementation of a subclass of interventions we term Service Interventions (SIs): interventions that include an on-demand service component provided by a costly and limited resource (e.g., healthcare providers or teachers).
  We show that in such a system, the capacity constraints induce dependencies across experiment subjects, where an individual may need to wait before receiving the intervention. By modeling these dependencies using a queueing system, we show how increasing the number of subjects without increasing the capacity of the system can lead to a nonlinear decrease in the treatment effect size. This has implications for conventional power analysis and recruitment strategies: increasing the sample size of an RCT without appropriately expanding capacity can decrease the study's power. To address this issue, we propose a method to jointly select the system capacity and number of users using the square root staffing rule from queueing theory. We show how incorporating knowledge of the queueing structure can help an experimenter reduce the amount of capacity and number of subjects required while still maintaining high power. In addition, our analysis of congestion-driven interference provides one concrete mechanism to explain why similar protocols can result in different RCT outcomes and why promising interventions at the RCT stage may not perform well at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21322v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin Boutilier, Jonas Oddur Jonasson, Hannah Li, Erez Yoeli</dc:creator>
    </item>
    <item>
      <title>Unveiling land use dynamics: Insights from a hierarchical Bayesian spatio-temporal modelling of Compositional Data</title>
      <link>https://arxiv.org/abs/2407.21695</link>
      <description>arXiv:2407.21695v2 Announce Type: replace 
Abstract: Changes in land use patterns have significant environmental and socioeconomic impacts, making it crucial for policymakers to understand their causes and consequences. This study, part of the European LAMASUS (Land Management for Sustainability) project, aims to support the EU's climate neutrality target by developing a governance model through collaboration between policymakers, land users, and researchers. We present a methodological synthesis for treating land use data using a Bayesian approach within spatial and spatio-temporal modeling frameworks.
  The study tackles the challenges of analyzing land use changes, particularly the presence of zero values and computational issues with large datasets. It introduces joint model structures to address zeros and employs sequential inference and consensus methods for Big Data problems. Spatial downscaling models approximate smaller scales from aggregated data, circumventing high-resolution data complications.
  We explore Beta regression and Compositional Data Analysis (CoDa) for land use data, review relevant spatial and spatio-temporal models, and present strategies for handling zeros. The paper demonstrates the implementation of key models, downscaling techniques, and solutions to Big Data challenges with examples from simulated data and the LAMASUS project, providing a comprehensive framework for understanding and managing land use changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21695v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mario Figueira, Carmen Guarner, David Conesa, Antonio L\'opez-Qu\'ilez, Tam\'as Krisztin</dc:creator>
    </item>
    <item>
      <title>Robust Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models</title>
      <link>https://arxiv.org/abs/2209.15224</link>
      <description>arXiv:2209.15224v4 Announce Type: replace-cross 
Abstract: Unsupervised learning has been widely used in many real-world applications. One of the simplest and most important unsupervised learning models is the Gaussian mixture model (GMM). In this work, we study the multi-task learning problem on GMMs, which aims to leverage potentially similar GMM parameter structures among tasks to obtain improved learning performance compared to single-task learning. We propose a multi-task GMM learning procedure based on the EM algorithm that effectively utilizes unknown similarities between related tasks and is robust against a fraction of outlier tasks from arbitrary distributions. The proposed procedure is shown to achieve the minimax optimal rate of convergence for both parameter estimation error and the excess mis-clustering error, in a wide range of regimes. Moreover, we generalize our approach to tackle the problem of transfer learning for GMMs, where similar theoretical results are derived. Additionally, iterative unsupervised multi-task and transfer learning methods may suffer from an initialization alignment problem, and two alignment algorithms are proposed to resolve the issue. Finally, we demonstrate the effectiveness of our methods through simulations and real data examples. To the best of our knowledge, this is the first work studying multi-task and transfer learning on GMMs with theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.15224v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Tian, Haolei Weng, Lucy Xia, Yang Feng</dc:creator>
    </item>
    <item>
      <title>Inference in Cluster Randomized Trials with Matched Pairs</title>
      <link>https://arxiv.org/abs/2211.14903</link>
      <description>arXiv:2211.14903v4 Announce Type: replace-cross 
Abstract: This paper studies inference in cluster randomized trials where treatment status is determined according to a "matched pairs" design. Here, by a cluster randomized experiment, we mean one in which treatment is assigned at the level of the cluster; by a "matched pairs" design, we mean that a sample of clusters is paired according to baseline, cluster-level covariates and, within each pair, one cluster is selected at random for treatment. We study the large-sample behavior of a weighted difference-in-means estimator and derive two distinct sets of results depending on if the matching procedure does or does not match on cluster size. We then propose a single variance estimator which is consistent in either regime. Combining these results establishes the asymptotic exactness of tests based on these estimators. Next, we consider the properties of two common testing procedures based on t-tests constructed from linear regressions, and argue that both are generally conservative in our framework. We additionally study the behavior of a randomization test which permutes the treatment status for clusters within pairs, and establish its finite-sample and asymptotic validity for testing specific null hypotheses. Finally, we propose a covariate-adjusted estimator which adjusts for additional baseline covariates not used for treatment assignment, and establish conditions under which such an estimator leads to strict improvements in precision. A simulation study confirms the practical relevance of our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.14903v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Jizhou Liu, Azeem M. Shaikh, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>Adaptive Principal Component Regression with Applications to Panel Data</title>
      <link>https://arxiv.org/abs/2307.01357</link>
      <description>arXiv:2307.01357v3 Announce Type: replace-cross 
Abstract: Principal component regression (PCR) is a popular technique for fixed-design error-in-variables regression, a generalization of the linear regression setting in which the observed covariates are corrupted with random noise. We provide the first time-uniform finite sample guarantees for (regularized) PCR whenever data is collected adaptively. Since the proof techniques for analyzing PCR in the fixed design setting do not readily extend to the online setting, our results rely on adapting tools from modern martingale concentration to the error-in-variables setting. We demonstrate the usefulness of our bounds by applying them to the domain of panel data, a ubiquitous setting in econometrics and statistics. As our first application, we provide a framework for experiment design in panel data settings when interventions are assigned adaptively. Our framework may be thought of as a generalization of the synthetic control and synthetic interventions frameworks, where data is collected via an adaptive intervention assignment policy. Our second application is a procedure for learning such an intervention assignment policy in a setting where units arrive sequentially to be treated. In addition to providing theoretical performance guarantees (as measured by regret), we show that our method empirically outperforms a baseline which does not leverage error-in-variables regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01357v3</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anish Agarwal, Keegan Harris, Justin Whitehouse, Zhiwei Steven Wu</dc:creator>
    </item>
    <item>
      <title>Comparison of Probabilistic Structural Reliability Methods for Ultimate Limit State Assessment of Wind Turbines</title>
      <link>https://arxiv.org/abs/2312.04972</link>
      <description>arXiv:2312.04972v2 Announce Type: replace-cross 
Abstract: The probabilistic design of offshore wind turbines aims to ensure structural safety in a cost-effective way. This involves conducting structural reliability assessments for different design options and considering different structural responses. There are several structural reliability methods, and this paper will apply and compare different approaches in some simplified case studies. In particular, the well known environmental contour method will be compared to a more novel approach based on sequential sampling and Gaussian processes regression for an ultimate limit state case study. For one of the case studies, results will also be compared to results from a brute force simulation approach. Interestingly, the comparison is very different from the two case studies. In one of the cases the environmental contours method agrees well with the sequential sampling method but in the other, results vary considerably. Probably, this can be explained by the violation of some of the assumptions associated with the environmental contour approach, i.e. that the short-term variability of the response is large compared to the long-term variability of the environmental conditions. Results from this simple comparison study suggests that the sequential sampling method can be a robust and computationally effective approach for structural reliability assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04972v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.strusafe.2024.102502</arxiv:DOI>
      <dc:creator>Hong Wang, Odin Gramstad, Styfen Sch\"ar, Stefano Marelli, Erik Vanem</dc:creator>
    </item>
    <item>
      <title>Estimating Lagged (Cross-)Covariance Operators of $L^p$-$m$-approximable Processes in Cartesian Product Hilbert Spaces</title>
      <link>https://arxiv.org/abs/2402.08110</link>
      <description>arXiv:2402.08110v3 Announce Type: replace-cross 
Abstract: Estimating parameters of functional ARMA, GARCH and invertible processes requires estimating lagged covariance and cross-covariance operators of Cartesian product Hilbert space-valued processes. Asymptotic results have been derived in recent years, either less generally or under a strict condition. This article derives upper bounds of the estimation errors for such operators based on the mild condition Lp-m-approximability for each lag, Cartesian power(s) and sample size, where the two processes can take values in different spaces in the context of lagged cross-covariance operators. Implications of our results on eigenelements, parameters in functional AR(MA) models and other general situations are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08110v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian K\"uhnert</dc:creator>
    </item>
    <item>
      <title>Mean and Covariance Estimation for Discretely Observed High-Dimensional Functional Data: Rates of Convergence and Division of Observational Regimes</title>
      <link>https://arxiv.org/abs/2408.01326</link>
      <description>arXiv:2408.01326v2 Announce Type: replace-cross 
Abstract: Estimation of the mean and covariance parameters for functional data is a critical task, with local linear smoothing being a popular choice. In recent years, many scientific domains are producing multivariate functional data for which $p$, the number of curves per subject, is often much larger than the sample size $n$. In this setting of high-dimensional functional data, much of developed methodology relies on preliminary estimates of the unknown mean functions and the auto- and cross-covariance functions. This paper investigates the convergence rates of local linear estimators in terms of the maximal error across components and pairs of components for mean and covariance functions, respectively, in both $L^2$ and uniform metrics. The local linear estimators utilize a generic weighting scheme that can adjust for differing numbers of discrete observations $N_{ij}$ across curves $j$ and subjects $i$, where the $N_{ij}$ vary with $n$. Particular attention is given to the equal weight per observation (OBS) and equal weight per subject (SUBJ) weighting schemes. The theoretical results utilize novel applications of concentration inequalities for functional data and demonstrate that, similar to univariate functional data, the order of the $N_{ij}$ relative to $p$ and $n$ divides high-dimensional functional data into three regimes (sparse, dense, and ultra-dense), with the high-dimensional parametric convergence rate of $\left\{\log(p)/n\right\}^{1/2}$ being attainable in the latter two.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01326v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexander Petersen</dc:creator>
    </item>
  </channel>
</rss>
