<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Sep 2025 04:03:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Efficient High-Dimensional Conditional Independence Testing</title>
      <link>https://arxiv.org/abs/2509.10817</link>
      <description>arXiv:2509.10817v1 Announce Type: new 
Abstract: This article deals with the problem of testing conditional independence between two random vectors ${\bf X}$ and ${\bf Y}$ given a confounding random vector ${\bf Z}$. Several authors have considered this problem for multivariate data. However, most of the existing tests has poor performance against local contiguous alternatives beyond linear dependency. In this article, an Energy distance type measure of conditional dependence is developed, borrowing ideas from the model-X framework. A consistent estimator of the measure is proposed, and its theoretical properties are studied under general assumptions. Using the estimator as a test statistic a test of conditional independence is developed, and a suitable resampling algorithm is designed to calibrate the test. The test turns out to be not only large sample consistent, but also Pitman efficient against local contiguous alternatives, and is provably consistent when the dimension of the data diverges to infinity with the sample size. Several empirical studies are conducted to demonstrate the efficacy of the test against state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10817v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bilol Banerjee</dc:creator>
    </item>
    <item>
      <title>Testing for LLM response differences: the case of a composite null consisting of semantically irrelevant query perturbations</title>
      <link>https://arxiv.org/abs/2509.10963</link>
      <description>arXiv:2509.10963v1 Announce Type: new 
Abstract: Given an input query, generative models such as large language models produce a random response drawn from a response distribution. Given two input queries, it is natural to ask if their response distributions are the same. While traditional statistical hypothesis testing is designed to address this question, the response distribution induced by an input query is often sensitive to semantically irrelevant perturbations to the query, so much so that a traditional test of equality might indicate that two semantically equivalent queries induce statistically different response distributions. As a result, the outcome of the statistical test may not align with the user's requirements. In this paper, we address this misalignment by incorporating into the testing procedure consideration of a collection of semantically similar queries. In our setting, the mapping from the collection of user-defined semantically similar queries to the corresponding collection of response distributions is not known a priori and must be estimated, with a fixed budget. Although the problem we address is quite general, we focus our analysis on the setting where the responses are binary, show that the proposed test is asymptotically valid and consistent, and discuss important practical considerations with respect to power and computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10963v1</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aranyak Acharyya, Carey E. Priebe, Hayden S. Helm</dc:creator>
    </item>
    <item>
      <title>The Honest Truth About Causal Trees: Accuracy Limits for Heterogeneous Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2509.11381</link>
      <description>arXiv:2509.11381v1 Announce Type: new 
Abstract: Recursive decision trees have emerged as a leading methodology for heterogeneous causal treatment effect estimation and inference in experimental and observational settings. These procedures are fitted using the celebrated CART (Classification And Regression Tree) algorithm [Breiman et al., 1984], or custom variants thereof, and hence are believed to be "adaptive" to high-dimensional data, sparsity, or other specific features of the underlying data generating process. Athey and Imbens [2016] proposed several "honest" causal decision tree estimators, which have become the standard in both academia and industry. We study their estimators, and variants thereof, and establish lower bounds on their estimation error. We demonstrate that these popular heterogeneous treatment effect estimators cannot achieve a polynomial-in-$n$ convergence rate under basic conditions, where $n$ denotes the sample size. Contrary to common belief, honesty does not resolve these limitations and at best delivers negligible logarithmic improvements in sample size or dimension. As a result, these commonly used estimators can exhibit poor performance in practice, and even be inconsistent in some settings. Our theoretical insights are empirically validated through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11381v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Jason M. Klusowski, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>About the Multiplicative Inverse of a Non-Zero-Mean Gaussian Process</title>
      <link>https://arxiv.org/abs/2509.11650</link>
      <description>arXiv:2509.11650v1 Announce Type: new 
Abstract: We study the spectral properties of a stochastic process obtained by multiplicative inversion of a non-zero-mean Gaussian process. We show that its autocorrelation and power spectrum exist for most regular processes, and we find a convergent series expansion of the autocorrelation function in powers of the ratio between mean and standard deviation of the underlying Gaussian process. We apply the results to two sample processes, and we validate the theoretical results with simulations based on standard signal processing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11650v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Lanucara</dc:creator>
    </item>
    <item>
      <title>On the universal calibration of Pareto-type linear combination tests</title>
      <link>https://arxiv.org/abs/2509.12066</link>
      <description>arXiv:2509.12066v1 Announce Type: new 
Abstract: It is often of interest to test a global null hypothesis using multiple, possibly dependent, $p$-values by combining their strengths while controlling the Type I error. Recently, several heavy-tailed combinations tests, such as the harmonic mean test and the Cauchy combination test, have been proposed: they map $p$-values into heavy-tailed random variables before combining them in some fashion into a single test statistic. The resulting tests, which are calibrated under the assumption of independence of the $p$-values, have shown to be rather robust to dependence. The complete understanding of the calibration properties of the resulting combination tests of dependent and possibly tail-dependent $p$-values has remained an important open problem in the area. In this work, we show that the powerful framework of multivariate regular variation (MRV) offers a nearly complete solution to this problem.
  We first show that the precise asymptotic calibration properties of a large class of homogeneous combination tests can be expressed in terms of the angular measure -- a characteristic of the asymptotic tail-dependence under MRV. Consequently, we show that under MRV, the Pareto-type linear combination tests, which are equivalent to the harmonic mean test, are universally calibrated regardless of the tail-dependence structure of the underlying $p$-values. In contrast, the popular Cauchy combination test is shown to be universally honest but often conservative; the Tippet combination test, while being honest, is calibrated if and only if the underlying $p$-values are tail-independent.
  One of our major findings is that the Pareto-type linear combination tests are the only universally calibrated ones among the large family of possibly non-linear homogeneous heavy-tailed combination tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12066v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parijat Chakraborty, F. Richard Guo, Kerby Shedden, Stilian Stoev</dc:creator>
    </item>
    <item>
      <title>Kernel-based Stochastic Approximation Framework for Nonlinear Operator Learning</title>
      <link>https://arxiv.org/abs/2509.11070</link>
      <description>arXiv:2509.11070v1 Announce Type: cross 
Abstract: We develop a stochastic approximation framework for learning nonlinear operators between infinite-dimensional spaces utilizing general Mercer operator-valued kernels. Our framework encompasses two key classes: (i) compact kernels, which admit discrete spectral decompositions, and (ii) diagonal kernels of the form $K(x,x')=k(x,x')T$, where $k$ is a scalar-valued kernel and $T$ is a positive operator on the output space. This broad setting induces expressive vector-valued reproducing kernel Hilbert spaces (RKHSs) that generalize the classical $K=kI$ paradigm, thereby enabling rich structural modeling with rigorous theoretical guarantees. To address target operators lying outside the RKHS, we introduce vector-valued interpolation spaces to precisely quantify misspecification error. Within this framework, we establish dimension-free polynomial convergence rates, demonstrating that nonlinear operator learning can overcome the curse of dimensionality. The use of general operator-valued kernels further allows us to derive rates for intrinsically nonlinear operator learning, going beyond the linear-type behavior inherent in diagonal constructions of $K=kI$. Importantly, this framework accommodates a wide range of operator learning tasks, ranging from integral operators such as Fredholm operators to architectures based on encoder-decoder representations. Moreover, we validate its effectiveness through numerical experiments on the two-dimensional Navier-Stokes equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11070v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia-Qi Yang, Lei Shi</dc:creator>
    </item>
    <item>
      <title>Some Robustness Properties of Label Cleaning</title>
      <link>https://arxiv.org/abs/2509.11379</link>
      <description>arXiv:2509.11379v1 Announce Type: cross 
Abstract: We demonstrate that learning procedures that rely on aggregated labels, e.g., label information distilled from noisy responses, enjoy robustness properties impossible without data cleaning. This robustness appears in several ways. In the context of risk consistency -- when one takes the standard approach in machine learning of minimizing a surrogate (typically convex) loss in place of a desired task loss (such as the zero-one mis-classification error) -- procedures using label aggregation obtain stronger consistency guarantees than those even possible using raw labels. And while classical statistical scenarios of fitting perfectly-specified models suggest that incorporating all possible information -- modeling uncertainty in labels -- is statistically efficient, consistency fails for ``standard'' approaches as soon as a loss to be minimized is even slightly mis-specified. Yet procedures leveraging aggregated information still converge to optimal classifiers, highlighting how incorporating a fuller view of the data analysis pipeline, from collection to model-fitting to prediction time, can yield a more robust methodology by refining noisy signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11379v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Cheng, John Duchi</dc:creator>
    </item>
    <item>
      <title>Long-time dynamics and universality of nonconvex gradient descent</title>
      <link>https://arxiv.org/abs/2509.11426</link>
      <description>arXiv:2509.11426v1 Announce Type: cross 
Abstract: This paper develops a general approach to characterize the long-time trajectory behavior of nonconvex gradient descent in generalized single-index models in the large aspect ratio regime. In this regime, we show that for each iteration the gradient descent iterate concentrates around a deterministic vector called the `Gaussian theoretical gradient descent', whose dynamics can be tracked by a state evolution system of two recursive equations for two scalars. Our concentration guarantees hold universally for a broad class of design matrices and remain valid over long time horizons until algorithmic convergence or divergence occurs. Moreover, our approach reveals that gradient descent iterates are in general approximately independent of the data and strongly incoherent with the feature vectors, a phenomenon previously known as the `implicit regularization' effect of gradient descent in specific models under Gaussian data.
  As an illustration of the utility of our general theory, we present two applications of different natures in the regression setting. In the first, we prove global convergence of nonconvex gradient descent with general independent initialization for a broad class of structured link functions, and establish universality of randomly initialized gradient descent in phase retrieval for large aspect ratios. In the second, we develop a data-free iterative algorithm for estimating state evolution parameters along the entire gradient descent trajectory, thereby providing a low-cost yet statistically valid tool for practical tasks such as hyperparameter tuning and runtime determination.
  As a by-product of our analysis, we show that in the large aspect ratio regime, the Gaussian theoretical gradient descent coincides with a recent line of dynamical mean-field theory for gradient descent over the constant-time horizon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11426v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyang Han</dc:creator>
    </item>
    <item>
      <title>Wavelet-SARIMA-Transformer: A Hybrid Model for Rainfall Forecasting</title>
      <link>https://arxiv.org/abs/2509.11903</link>
      <description>arXiv:2509.11903v1 Announce Type: cross 
Abstract: This study develops and evaluates a novel hybridWavelet SARIMA Transformer, WST framework to forecast using monthly rainfall across five meteorological subdivisions of Northeast India over the 1971 to 2023 period. The approach employs the Maximal Overlap Discrete Wavelet Transform, MODWT with four wavelet families such as, Haar, Daubechies, Symlet, Coiflet etc. to achieve shift invariant, multiresolution decomposition of the rainfall series. Linear and seasonal components are modeled using Seasonal ARIMA, SARIMA, while nonlinear components are modeled by a Transformer network, and forecasts are reconstructed via inverse MODWT. Comprehensive validation using an 80 is to 20 train test split and multiple performance indices such as, RMSE, MAE, SMAPE, Willmotts d, Skill Score, Percent Bias, Explained Variance, and Legates McCabes E1 demonstrates the superiority of the Haar-based hybrid model, WHST. Across all subdivisions, WHST consistently achieved lower forecast errors, stronger agreement with observed rainfall, and unbiased predictions compared with stand alone SARIMA, stand-alone Transformer, and two-stage wavelet hybrids. Residual adequacy was confirmed through the Ljung Box test, while Taylor diagrams provided an integrated assessment of correlation, variance fidelity, and RMSE, further reinforcing the robustness of the proposed approach. The results highlight the effectiveness of integrating multiresolution signal decomposition with complementary linear and deep learning models for hydroclimatic forecasting. Beyond rainfall, the proposed WST framework offers a scalable methodology for forecasting complex environmental time series, with direct implications for flood risk management, water resources planning, and climate adaptation strategies in data-sparse and climate-sensitive regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11903v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Junmoni Saikia, Kuldeep Goswami, Sarat C. Kakaty</dc:creator>
    </item>
    <item>
      <title>Modeling Non-Uniform Hypergraphs Using Determinantal Point Processes</title>
      <link>https://arxiv.org/abs/2509.12028</link>
      <description>arXiv:2509.12028v1 Announce Type: cross 
Abstract: Most statistical models for networks focus on pairwise interactions between nodes. However, many real-world networks involve higher-order interactions among multiple nodes, such as co-authors collaborating on a paper. Hypergraphs provide a natural representation for these networks, with each hyperedge representing a set of nodes. The majority of existing hypergraph models assume uniform hyperedges (i.e., edges of the same size) or rely on diversity among nodes. In this work, we propose a new hypergraph model based on non-symmetric determinantal point processes. The proposed model naturally accommodates non-uniform hyperedges, has tractable probability mass functions, and accounts for both node similarity and diversity in hyperedges. For model estimation, we maximize the likelihood function under constraints using a computationally efficient projected adaptive gradient descent algorithm. We establish the consistency and asymptotic normality of the estimator. Simulation studies confirm the efficacy of the proposed model, and its utility is further demonstrated through edge predictions on several real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12028v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichao Chen, Jingfei Zhang, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Extrapolation of Tempered Posteriors</title>
      <link>https://arxiv.org/abs/2509.12173</link>
      <description>arXiv:2509.12173v1 Announce Type: cross 
Abstract: Tempering is a popular tool in Bayesian computation, being used to transform a posterior distribution $p_1$ into a reference distribution $p_0$ that is more easily approximated. Several algorithms exist that start by approximating $p_0$ and proceed through a sequence of intermediate distributions $p_t$ until an approximation to $p_1$ is obtained. Our contribution reveals that high-quality approximation of terms up to $p_1$ is not essential, as knowledge of the intermediate distributions enables posterior quantities of interest to be extrapolated. Specifically, we establish conditions under which posterior expectations are determined by their associated tempered expectations on any non-empty $t$ interval. Harnessing this result, we propose novel methodology for approximating posterior expectations based on extrapolation and smoothing of tempered expectations, which we implement as a post-processing variance-reduction tool for sequential Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12173v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengxin Xi, Zheyang Shen, Marina Riabiz, Nicolas Chopin, Chris J. Oates</dc:creator>
    </item>
    <item>
      <title>The Morgan-Pitman Test of Equality of Variances and its Application to Machine Learning Model Evaluation and Selection</title>
      <link>https://arxiv.org/abs/2509.12185</link>
      <description>arXiv:2509.12185v1 Announce Type: cross 
Abstract: Model selection in non-linear models often prioritizes performance metrics over statistical tests, limiting the ability to account for sampling variability. We propose the use of a statistical test to assess the equality of variances in forecasting errors. The test builds upon the classic Morgan-Pitman approach, incorporating enhancements to ensure robustness against data with heavy-tailed distributions or outliers with high variance, plus a strategy to make residuals from machine learning models statistically independent. Through a series of simulations and real-world data applications, we demonstrate the test's effectiveness and practical utility, offering a reliable tool for model evaluation and selection in diverse contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12185v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Argimiro Arratia, Alejandra Caba\~na, Ernesto Mordecki, Gerard Rovira-Parra</dc:creator>
    </item>
    <item>
      <title>Eigen-convergence of Gaussian kernelized graph Laplacian by manifold heat interpolation</title>
      <link>https://arxiv.org/abs/2101.09875</link>
      <description>arXiv:2101.09875v3 Announce Type: replace 
Abstract: This work studies the spectral convergence of graph Laplacian to the Laplace-Beltrami operator when the graph affinity matrix is constructed from $N$ random samples on a $d$-dimensional manifold embedded in a possibly high dimensional space. By analyzing Dirichlet form convergence and constructing candidate approximate eigenfunctions via convolution with manifold heat kernel, we prove that, with Gaussian kernel, one can set the kernel bandwidth parameter $\epsilon \sim (\log N/ N)^{1/(d/2+2)}$ such that the eigenvalue convergence rate is $N^{-1/(d/2+2)}$ and the eigenvector convergence in 2-norm has rate $N^{-1/(d+4)}$; When $\epsilon \sim (\log N/N)^{1/(d/2+3)}$, both eigenvalue and eigenvector rates are $N^{-1/(d/2+3)}$. These rates are up to a $\log N$ factor and proved for finitely many low-lying eigenvalues. The result holds for un-normalized and random-walk graph Laplacians when data are uniformly sampled on the manifold, as well as the density-corrected graph Laplacian (where the affinity matrix is normalized by the degree matrix from both sides) with non-uniformly sampled data. As an intermediate result, we prove new point-wise and Dirichlet form convergence rates for the density-corrected graph Laplacian. Numerical results are provided to verify the theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.09875v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.acha.2022.06.003</arxiv:DOI>
      <arxiv:journal_reference>Applied and Computational Harmonic Analysis, 61, 132-190 (2022)</arxiv:journal_reference>
      <dc:creator>Xiuyuan Cheng, Nan Wu</dc:creator>
    </item>
    <item>
      <title>Optimal empirical Bayes estimation for the Poisson model via minimum-distance methods</title>
      <link>https://arxiv.org/abs/2209.01328</link>
      <description>arXiv:2209.01328v3 Announce Type: replace 
Abstract: The Robbins estimator is the most iconic and widely used procedure in the empirical Bayes literature for the Poisson model. On one hand, this method has been recently shown to be minimax optimal in terms of the regret (excess risk over the Bayesian oracle that knows the true prior) for various nonparametric classes of priors. On the other hand, it has been long recognized in practice that the Robbins estimator lacks the desired smoothness and monotonicity of Bayes estimators and can be easily derailed by those data points that were rarely observed before. Based on the minimum-distance distance method, we propose a suite of empirical Bayes estimators, including the classical nonparametric maximum likelihood, that outperform the Robbins method in a variety of synthetic and real data sets and retain its optimality in terms of minimax regret.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.01328v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soham Jana, Yury Polyanskiy, Yihong Wu</dc:creator>
    </item>
    <item>
      <title>A Point on Discrete versus Continuous State-Space Markov Chains</title>
      <link>https://arxiv.org/abs/2407.12308</link>
      <description>arXiv:2407.12308v2 Announce Type: replace 
Abstract: This paper examines the impact of discrete marginal distributions on copula-based Markov chains. We present results on mixing and parameter estimation for a copula-based Markov chain model with Bernoulli($p$) marginal distribution and highlight the differences between continuous and discrete state-space Markov chains. We derive estimators for model parameters using the maximum likelihood approach and discuss other estimators of $p$ that are asymptotically equivalent to its maximum likelihood estimator. The asymptotic distributions of the parameter estimators are provided. A simulation study showcases the performance of the different estimators of $p$. Additionally, statistical tests for model parameters are included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12308v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathias N. Muia, Martial Longla</dc:creator>
    </item>
    <item>
      <title>Parameter estimation for fractional autoregressive process with periodic structure</title>
      <link>https://arxiv.org/abs/2503.20736</link>
      <description>arXiv:2503.20736v2 Announce Type: replace 
Abstract: This paper introduces a new periodic fractional autoregressive process (PFAR) driven by fractional Gaussian noise (fGn) to model time series of precipitation evapotranspiration. Compared with the similar model in [\emph{Water Resources Research}, \textbf{20} (1984) 1898--1908], the new model incorporates a periodic structure via specialized varying coefficients and captures long memory and rough voltality through fGn for $0&lt;H&lt;1$, rather than via fractional differencing. In this work, Generalized Least Squares Estimation (GLSE) and the GPH method are employed to construct an initial estimator for the joint estimation of model parameters. A One-Step procedure is then used to obtain a more asymptotically efficient estimator. The paper proves that both estimators are consistent and asymptotically normal, and their performance is demonstrated via Monte Carlo simulations with finite-size samples. Simulation studies suggest that, while both estimation methods can accurately estimate the model parameters, the One-Step estimator outperforms the initial estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20736v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunhao Cai, Yiwu Shang</dc:creator>
    </item>
    <item>
      <title>Asymptotic breakdown point analysis of the minimum density power divergence estimator under independent non-homogeneous setups</title>
      <link>https://arxiv.org/abs/2508.12426</link>
      <description>arXiv:2508.12426v2 Announce Type: replace 
Abstract: The minimum density power divergence estimator (MDPDE) has gained significant attention in the literature of robust inference due to its strong robustness properties and high asymptotic efficiency; it is relatively easy to compute and can be interpreted as a generalization of the classical maximum likelihood estimator. It has been successfully applied in various setups, including the case of independent and non-homogeneous (INH) observations that cover both classification and regression-type problems with a fixed design. While the local robustness of this estimator has been theoretically validated through the bounded influence function, no general result is known about the global reliability or the breakdown behavior of this estimator under the INH setup, except for the specific case of location-type models. In this paper, we extend the notion of asymptotic breakdown point from the case of independent and identically distributed data to the INH setup and derive a theoretical lower bound for the asymptotic breakdown point of the MDPDE, under some easily verifiable assumptions. These results are further illustrated with applications to some fixed design regression models and corroborated through extensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12426v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suryasis Jana, Subhrajyoty Roy, Ayanendranath Basu, Abhik Ghosh</dc:creator>
    </item>
    <item>
      <title>Extended UCB Policies for Multi-armed Bandit Problems</title>
      <link>https://arxiv.org/abs/1112.1768</link>
      <description>arXiv:1112.1768v5 Announce Type: replace-cross 
Abstract: The multi-armed bandit (MAB) problems are widely studied in fields of operations research, stochastic optimization, and reinforcement learning. In this paper, we consider the classical MAB model with heavy-tailed reward distributions and introduce the extended robust UCB policy, which is an extension of the results of Bubeck et al. [5] and Lattimore [22] that are further based on the pioneering idea of UCB policies [e.g. Auer et al. 3]. The previous UCB policies require some strict conditions on reward distributions, which can be difficult to guarantee in practical scenarios. Our extended robust UCB generalizes Lattimore's seminary work (for moments of orders $p=4$ and $q=2$) to arbitrarily chosen $p&gt;q&gt;1$ as long as the two moments have a known controlled relationship, while still achieving the optimal regret growth order $O(log T)$, thus providing a broadened application area of UCB policies for heavy-tailed reward distributions. Furthermore, we achieve a near-optimal regret order without any knowledge of the reward distributions as long as their $p$-th moments exist for some $p&gt;1$. Finally, we briefly present our earlier work on light-tailed reward distributions for a complete illustration of the amazing simplicity and power of UCB policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:1112.1768v5</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keqin Liu, Tianshuo Zheng, Zhi-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>A Permutation-free Kernel Two-Sample Test</title>
      <link>https://arxiv.org/abs/2211.14908</link>
      <description>arXiv:2211.14908v3 Announce Type: replace-cross 
Abstract: The kernel Maximum Mean Discrepancy~(MMD) is a popular multivariate distance metric between distributions that has found utility in two-sample testing. The usual kernel-MMD test statistic is a degenerate U-statistic under the null, and thus it has an intractable limiting distribution. Hence, to design a level-$\alpha$ test, one usually selects the rejection threshold as the $(1-\alpha)$-quantile of the permutation distribution. The resulting nonparametric test has finite-sample validity but suffers from large computational cost, since every permutation takes quadratic time. We propose the cross-MMD, a new quadratic-time MMD test statistic based on sample-splitting and studentization. We prove that under mild assumptions, the cross-MMD has a limiting standard Gaussian distribution under the null. Importantly, we also show that the resulting test is consistent against any fixed alternative, and when using the Gaussian kernel, it has minimax rate-optimal power against local alternatives. For large sample sizes, our new cross-MMD provides a significant speedup over the MMD, for only a slight loss in power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.14908v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubhanshu Shekhar, Ilmun Kim, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Efficient Pauli channel estimation with logarithmic quantum memory</title>
      <link>https://arxiv.org/abs/2309.14326</link>
      <description>arXiv:2309.14326v5 Announce Type: replace-cross 
Abstract: Here we revisit one of the prototypical tasks for characterizing the structure of noise in quantum devices: estimating every eigenvalue of an $n$-qubit Pauli noise channel to error $\epsilon$. Prior work [14] proved no-go theorems for this task in the practical regime where one has a limited amount of quantum memory, e.g. any protocol with $\le 0.99n$ ancilla qubits of quantum memory must make exponentially many measurements, provided it is non-concatenating. Such protocols can only interact with the channel by repeatedly preparing a state, passing it through the channel, and measuring immediately afterward.
  This left open a natural question: does the lower bound hold even for general protocols, i.e. ones which chain together many queries to the channel, interleaved with arbitrary data-processing channels, before measuring? Surprisingly, in this work we show the opposite: there is a protocol that can estimate the eigenvalues of a Pauli channel to error $\epsilon$ using only $O(\log n/\epsilon^2)$ ancilla and $\tilde{O}(n^2/\epsilon^2)$ measurements. In contrast, we show that any protocol with zero ancilla, even a concatenating one, must make $\Omega(2^n/\epsilon^2)$ measurements, which is tight.
  Our results imply, to our knowledge, the first quantum learning task where logarithmically many qubits of quantum memory suffice for an exponential statistical advantage. Our protocol can be naturally extended to a protocol that learns the eigenvalues of Pauli terms within any subset $A$ of a Pauli channel with $O(\log\log(|A|)/\epsilon^2)$ ancilla and $\tilde{O}(n^2/\epsilon^2)$ measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14326v5</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sitan Chen, Weiyuan Gong</dc:creator>
    </item>
    <item>
      <title>Convergence of the Adapted Smoothed Empirical Measures</title>
      <link>https://arxiv.org/abs/2401.14883</link>
      <description>arXiv:2401.14883v3 Announce Type: replace-cross 
Abstract: The adapted Wasserstein distance controls the calibration errors of optimal values in various stochastic optimization problems, pricing and hedging problems, optimal stopping problems, etc. However, statistical aspects of the adapted Wasserstein distance are bottlenecked by the failure of empirical measures to converge under this distance. Kernel smoothing and adapted projection have been introduced to construct converging substitutes of empirical measures, known respectively as smoothed empirical measures and adapted empirical measures. However, both approaches have limitations. Specifically, smoothed empirical measures lack comprehensive convergence results, whereas adapted empirical measures in practical applications lead to fewer distinct samples compared to standard empirical measures.
  In this work, we address both of the aforementioned issues. First, we develop comprehensive convergence results of smoothed empirical measures. We then introduce a smoothed version for adapted empirical measures, which provide as many distinct samples as desired. We refer them as adapted smoothed empirical measures and establish their convergence in mean, deviation, and almost sure convergence. The convergence estimation incorporates two results: the empirical analysis of the smoothed adapted Wasserstein distance and its bandwidth effects. Both results are novel and their proof techniques could be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14883v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songyan Hou</dc:creator>
    </item>
    <item>
      <title>Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models</title>
      <link>https://arxiv.org/abs/2404.19707</link>
      <description>arXiv:2404.19707v5 Announce Type: replace-cross 
Abstract: We show that structural smooth transition vector autoregressive models are statistically identified if the shocks are mutually independent and at most one of them is Gaussian. This extends a known identification result for linear structural vector autoregressions to a time-varying impact matrix. We also propose an estimation method, show how a blended identification strategy can be adopted to address weak identification, and establish a sufficient condition for ergodic stationarity. The introduced methods are implemented in the accompanying R package sstvars. Our empirical application finds that a positive climate policy uncertainty shock reduces production and raises inflation under both low and high economic policy uncertainty, but its effects, particularly on inflation, are stronger during the latter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19707v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Savi Virolainen</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian inference for heat kernel Gaussian processes on manifolds</title>
      <link>https://arxiv.org/abs/2405.13342</link>
      <description>arXiv:2405.13342v2 Announce Type: replace-cross 
Abstract: We establish a scalable manifold learning method and theory, motivated by the problem of estimating fMRI activation manifolds in the Human Connectome Project (HCP). Our primary contribution is the development of an efficient estimation technique for heat kernel Gaussian processes in the exponential family model. This approach handles large sample sizes $n$, preserves the intrinsic geometry of data, and significantly reduces computational complexity from $\mathcal{O}(n^3)$ to $\mathcal{O}(n)$ via a novel reduced-rank approximation of the graph Laplacian's transition matrix and a Truncated Singular Value Decomposition for the eigenpair computation. The numerical experiments demonstrate the scalability and improved accuracy of our method for manifold learning tasks involving complex large-scale data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13342v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssb/qkaf057</arxiv:DOI>
      <dc:creator>Junhui He, Guoxuan Ma, Jian Kang, Ying Yang</dc:creator>
    </item>
    <item>
      <title>On the Lower Confidence Band for the Optimal Welfare in Policy Learning</title>
      <link>https://arxiv.org/abs/2410.07443</link>
      <description>arXiv:2410.07443v3 Announce Type: replace-cross 
Abstract: We study inference on the optimal welfare in a policy learning problem and propose reporting a lower confidence band (LCB). A natural approach to constructing an LCB is to invert a one-sided t-test based on an efficient estimator for the optimal welfare. However, we show that for an empirically relevant class of DGPs, such an LCB can be first-order dominated by an LCB based on a welfare estimate for a suitable suboptimal treatment policy. We show that such first-order dominance is possible if and only if the optimal treatment policy is not ``well-separated'' from the rest, in the sense of the commonly imposed margin condition. When this condition fails, standard debiased inference methods are not applicable. We show that uniformly valid and easy-to-compute LCBs can be constructed analytically by inverting moment-inequality tests with the maximum and quasi-likelihood-ratio test statistics. As an empirical illustration, we revisit the National JTPA study and find that the proposed LCBs achieve reliable coverage and competitive length.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07443v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kirill Ponomarev, Vira Semenova</dc:creator>
    </item>
    <item>
      <title>Lean Formalization of Generalization Error Bound by Rademacher Complexity</title>
      <link>https://arxiv.org/abs/2503.19605</link>
      <description>arXiv:2503.19605v3 Announce Type: replace-cross 
Abstract: We formalize the generalization error bound using the Rademacher complexity for the Lean 4 theorem prover based on the probability theory in the Mathlib 4 library. Generalization error quantifies the gap between a learning machine's performance on given training data versus unseen test data, and the Rademacher complexity is a powerful tool to upper-bound the generalization error of a variety of modern learning problems. Previous studies have only formalized extremely simple cases such as bounds by parameter counts and analyses for very simple models (decision stumps). Formalizing the Rademacher complexity bound, also known as the uniform law of large numbers, requires substantial development and is achieved for the first time in this study. In the course of development, we formalize the Rademacher complexity and its unique arguments such as symmetrization, and clarify the topological assumptions on hypothesis classes under which the bound holds. As an application, we also present the formalization of generalization error bound for $L^2$-regularization models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19605v3</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sho Sonoda, Kazumi Kasaura, Yuma Mizuno, Kei Tsukamoto, Naoto Onda</dc:creator>
    </item>
    <item>
      <title>Kernel Embeddings and the Separation of Measure Phenomenon</title>
      <link>https://arxiv.org/abs/2505.04613</link>
      <description>arXiv:2505.04613v2 Announce Type: replace-cross 
Abstract: We prove that kernel covariance embeddings lead to information-theoretically perfect separation of distinct probability distributions. In statistical terms, we establish that testing for the equality of two probability measures on a compact and separable metric space is equivalent to testing for the singularity between two centered Gaussian measures on a reproducing kernel Hilbert Space. The corresponding Gaussians are defined via the notion of kernel covariance embedding of a probability measure, and the Hilbert space is that generated by the embedding kernel. Distinguishing singular Gaussians is fundamentally simpler from an information-theoretic perspective than non-parametric two-sample testing, particularly in complex or high-dimensional domains. This is because singular Gaussians are supported on essentially separate and affine subspaces. Our proof leverages the classical Feldman-Hajek dichotomy, and shows that even a small perturbation of a distribution will be maximally magnified through its Gaussian embedding. This ``separation of measure phenomenon'' appears to be a blessing of infinite dimensionality, by means of embedding, with the potential to inform the design of efficient inference tools in considerable generality. The elicitation of this phenomenon also appears to crystallize, in a precise and simple mathematical statement, the outstanding empirical effectiveness of the so-called ``kernel trick".</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04613v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo V. Santoro, Kartik G. Waghmare, Victor M. Panaretos</dc:creator>
    </item>
    <item>
      <title>Higher-order Gini indices: An axiomatic approach</title>
      <link>https://arxiv.org/abs/2508.10663</link>
      <description>arXiv:2508.10663v2 Announce Type: replace-cross 
Abstract: Via an axiomatic approach, we characterize the family of n-th order Gini deviation, defined as the expected range over n independent draws from a distribution, to quantify joint dispersion across multiple observations. This family extends the classical Gini deviation, which relies solely on pairwise comparisons. The normalized version is called a high-order Gini coefficient. The generalized indices grow increasingly sensitive to tail inequality as n increases, offering a more nuanced view of distributional extremes. The higher-order Gini deviations admit a Choquet integral representation, inheriting the desirable properties of coherent deviation measures. Furthermore, we show that both the n-th order Gini deviation and the n-th order Gini coefficient are statistically n-observation elicitable, allowing for direct computation through empirical risk minimization. Data analysis using World Inequality Database data reveals that higher-order Gini coefficients capture disparities that the classical Gini coefficient may fail to reflect, particularly in cases of extreme income or wealth concentration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10663v2</guid>
      <category>q-fin.MF</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xia Han, Ruodu Wang, Qinyu Wu</dc:creator>
    </item>
    <item>
      <title>A Case for a "Refutations and Critiques'' Track in Statistics Journals</title>
      <link>https://arxiv.org/abs/2509.03702</link>
      <description>arXiv:2509.03702v2 Announce Type: replace-cross 
Abstract: The statistics community, which has traditionally lacked a transparent and open peer-review system, faces a challenge of inconsistent paper quality, with some published work containing substantial errors. This problem resonates with concerns raised by Schaeffer et al. (2025) regarding the rapid growth of machine learning research. They argue that peer review has proven insufficient to prevent the publication of ``misleading, incorrect, flawed or perhaps even fraudulent studies'' and that a ``dynamic self-correcting research ecosystem'' is needed. This note provides a concrete illustration of this problem by examining two published papers, Wang, Zhou and Lin (2025) and Liu et al. (2023), and exposing striking and critical errors in their proofs. The presence of such errors in major journals raises a fundamental question about the importance and verification of mathematical proofs in our field. Echoing the proposal from Schaeffer et al. (2025), we argue that reforming the peer-review system itself is likely impractical. Instead, we propose a more viable path forward: the creation of a high-profile, reputable platform, such as a ``Refutations and Critiques'' track on arXiv, to provide visibility to vital research that critically challenges prior work. Such a mechanism would be crucial for enhancing the reliability and credibility of statistical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03702v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhen Li</dc:creator>
    </item>
    <item>
      <title>Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures</title>
      <link>https://arxiv.org/abs/2509.08926</link>
      <description>arXiv:2509.08926v3 Announce Type: replace-cross 
Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise, which typically leads to significant performance degradation. We address this challenge by reframing Re-ID as a supervised image similarity task and adopting a Siamese network architecture trained to capture discriminative pairwise relationships. Central to our approach is a novel statistical outlier detection (OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier Detection), which models the distribution of cosine similarities between embedding pairs using a two-component Beta distribution mixture model. We establish a novel identifiability result for mixtures of two Beta distributions, ensuring that our learning task is well-posed. The proposed OD step complements the Re-ID architecture combining binary cross-entropy, contrastive, and cosine embedding losses that jointly optimize feature-level similarity learning. We demonstrate the effectiveness of Beta-SOD in de-noising and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance compared to the state-of-the-art methods across various noise levels (10-30\%), demonstrating both robustness and broad applicability in noisy Re-ID scenarios. The implementation of Beta-SOD is available at: github.com/waqar3411/Beta-SOD</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08926v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Waqar Ahmad, Evan Murphy, Vladimir A. Krylov</dc:creator>
    </item>
  </channel>
</rss>
