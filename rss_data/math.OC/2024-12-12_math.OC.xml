<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Dec 2024 05:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Finite-time Non-overshooting Leader-following Consensus Control for Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2412.07855</link>
      <description>arXiv:2412.07855v1 Announce Type: new 
Abstract: This paper addresses the finite-time non-overshooting leader-following consensus problem for multi-agent systems, whose agents are modeled by a dynamical system topologically equivalent to the integrator chain. Based on the weighted homogeneity, a nonlinear consensus control protocol is designed. A tuning scheme ensures the finite-time stability of the consensus error such that the agents do not have overshoots in the first component of the state vector. Simulations are presented to demonstrate the effectiveness of the proposed design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07855v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Min Li, Andrey Polyakov, Siyuan Wang, Gang Zheng</dc:creator>
    </item>
    <item>
      <title>Indirect Optimization of Multi-Phase Trajectories Involving Arbitrary Discrete Logic</title>
      <link>https://arxiv.org/abs/2412.07960</link>
      <description>arXiv:2412.07960v1 Announce Type: new 
Abstract: Multi-phase trajectories of aerospace vehicle systems involve multiple flight segments whose transitions may be triggered by boolean logic in continuous state variables, control and time. When the boolean logic is represented using only states and/or time, such systems are termed autonomously switched hybrid systems. The relaxed autonomously switched hybrid system approach (RASHS) was previously introduced to simplify the trajectory optimization process of such systems in the indirect framework when the boolean logic is solely represented using AND operations. This investigation enables cases involving arbitrary discrete logic. The new approach is termed the Generalized Relaxed Autonomously Switched Hybrid System (GRASHS) approach. Similar to the RASHS approach, the outcome of the GRASHS approach is the transformation of the necessary conditions of optimality from a multi-point boundary value problem to a two-point boundary value problem, which is simpler to handle. This is accomplished by converting the arbitrary boolean logic to the disjunctive normal form and applying smoothing using sigmoid and hyperbolic tangent functions. The GRASHS approach is demonstrated by optimizing a Mars entry, descent, and landing trajectory, where the parachute descent segment is active when the velocity is below the parachute deployment velocity or the altitude is below the parachute deployment altitude, and the altitude is above the powered descent initiation altitude. This set of conditions represents a combination of AND and OR logic. The previously introduced RASHS approach is not designed to handle such problems. The proposed GRASHS approach aims to fill this gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07960v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Harish Saranathan</dc:creator>
    </item>
    <item>
      <title>Support matrix machine: exploring sample sparsity, low rank, and adaptive sieving in high-performance computing</title>
      <link>https://arxiv.org/abs/2412.08023</link>
      <description>arXiv:2412.08023v1 Announce Type: new 
Abstract: Support matrix machine (SMM) is a successful supervised classification model for matrix-type samples. Unlike support vector machines, it employs low-rank regularization on the regression matrix to effectively capture the intrinsic structure embedded in each input matrix. When solving a large-scale SMM, a major challenge arises from the potential increase in sample size, leading to substantial computational and storage burdens. To address these issues, we design a semismooth Newton-CG (SNCG) based augmented Lagrangian method (ALM) for solving the SMM. The ALM exhibits an asymptotic R-superlinear convergence if a strict complementarity condition is satisfied. The SNCG method is employed to solve the ALM subproblems, achieving at least a superlinear convergence rate under the nonemptiness of an index set. Furthermore, the sparsity of samples and the low-rank nature of solutions enable us to reduce the computational cost and storage demands for the Newton linear systems. Additionally, we develop an adaptive sieving strategy that generates a solution path for the SMM by exploiting sample sparsity. The finite convergence of this strategy is also demonstrated. Numerical experiments on both large-scale real and synthetic datasets validate the effectiveness of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08023v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Can Wu, Dong-Hui Li, Defeng Sun</dc:creator>
    </item>
    <item>
      <title>Criteria and Bias of Parameterized Linear Regression under Edge of Stability Regime</title>
      <link>https://arxiv.org/abs/2412.08025</link>
      <description>arXiv:2412.08025v1 Announce Type: new 
Abstract: Classical optimization theory requires a small step-size for gradient-based methods to converge. Nevertheless, recent findings challenge the traditional idea by empirically demonstrating Gradient Descent (GD) converges even when the step-size $\eta$ exceeds the threshold of $2/L$, where $L$ is the global smooth constant. This is usually known as the Edge of Stability (EoS) phenomenon. A widely held belief suggests that an objective function with subquadratic growth plays an important role in incurring EoS. In this paper, we provide a more comprehensive answer by considering the task of finding linear interpolator $\beta \in R^{d}$ for regression with loss function $l(\cdot)$, where $\beta$ admits parameterization as $\beta = w^2_{+} - w^2_{-}$. Contrary to the previous work that suggests a subquadratic $l$ is necessary for EoS, our novel finding reveals that EoS occurs even when $l$ is quadratic under proper conditions. This argument is made rigorous by both empirical and theoretical evidence, demonstrating the GD trajectory converges to a linear interpolator in a non-asymptotic way. Moreover, the model under quadratic $l$, also known as a depth-$2$ diagonal linear network, remains largely unexplored under the EoS regime. Our analysis then sheds some new light on the implicit bias of diagonal linear networks when a larger step-size is employed, enriching the understanding of EoS on more practical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08025v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiyuan Zhang, Amin Karbasi</dc:creator>
    </item>
    <item>
      <title>Optimal Reactive Operation of General Topology Supply Chain and Manufacturing Networks under Disruptions</title>
      <link>https://arxiv.org/abs/2412.08046</link>
      <description>arXiv:2412.08046v1 Announce Type: new 
Abstract: Supply and manufacturing networks in the chemical industry involve diverse processing steps across different locations, rendering their operation vulnerable to disruptions from unplanned events. Optimal responses should consider factors such as product allocation, delayed shipments, and price renegotiation , among other factors. In such context, we propose a multiperiod mixed-integer linear programming model that integrates production, scheduling, shipping, and order management to minimize the financial impact of such disruptions. The model accommodates arbitrary supply chain topologies and incorporates various disruption scenarios, offering adaptability to real-world complexities. A case study from the chemical industry demonstrates the scalability of the model under finer time discretization and explores the influence of disruption types and order management costs on optimal schedules. This approach provides a tractable, adaptable framework for developing responsive operational plans in supply chain and manufacturing networks under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08046v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Ovalle, Joshua L. Pulsipher, Yixin Ye, Kyle Harshbarger, Scott Bury, Carl D. Laird, Ignacio E. Grossmann</dc:creator>
    </item>
    <item>
      <title>Stochastic Kinematic Optimal Control on SO(3)</title>
      <link>https://arxiv.org/abs/2412.08124</link>
      <description>arXiv:2412.08124v1 Announce Type: new 
Abstract: In this paper, we develop a novel method for deriving a global optimal control strategy for stochastic attitude kinematics on the special orthogonal group SO(3). We first introduce a stochastic Lie-Hamilton-Jacobi-Bellman (SL-HJB) equation on SO(3), which theoretically provides an optimality condition for the global optimal control strategy of the stochastic attitude kinematics. Then we propose a novel numerical method, the Successive Wigner-Galerkin Approximation (SWGA) method, to solve the SL-HJB equation on SO(3). The SWGA method leverages the Wigner-D functions to represent the Galerkin solution of the SL-HJB equation in a policy iteration framework, providing a computationally efficient approach to derive a global optimal control strategy for systems on SO(3). We demonstrate the effectiveness of the SWGA method through numerical simulation on stochastic attitude stabilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08124v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xi Wang, Xiaoyi Wang, Victor Solo</dc:creator>
    </item>
    <item>
      <title>Mixed-Integer Linear Optimization via Learning-Based Two-Layer Large Neighborhood Search</title>
      <link>https://arxiv.org/abs/2412.08206</link>
      <description>arXiv:2412.08206v1 Announce Type: new 
Abstract: Mixed-integer linear programs (MILPs) are extensively used to model practical problems such as planning and scheduling. A prominent method for solving MILPs is large neighborhood search (LNS), which iteratively seeks improved solutions within specific neighborhoods. Recent advancements have integrated machine learning techniques into LNS to guide the construction of these neighborhoods effectively. However, for large-scale MILPs, the search step in LNS becomes a computational bottleneck, relying on off-the-shelf solvers to optimize auxiliary MILPs of substantial size. To address this challenge, we introduce a two-layer LNS (TLNS) approach that employs LNS to solve both the original MILP and its auxiliary MILPs, necessitating the optimization of only small-sized MILPs using off-the-shelf solvers. Additionally, we incorporate a lightweight graph transformer model to inform neighborhood design. We conduct extensive computational experiments using public benchmarks. The results indicate that our learning-based TLNS approach achieves remarkable performance gains--up to 66% and 96% over LNS and state-of-the-art MILP solvers, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08206v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenbo Liu, Akang Wang, Wenguo Yang, Qingjiang Shi</dc:creator>
    </item>
    <item>
      <title>Efficient gradient-based methods for bilevel learning via recycling Krylov subspaces</title>
      <link>https://arxiv.org/abs/2412.08264</link>
      <description>arXiv:2412.08264v1 Announce Type: new 
Abstract: Many optimization problems require hyperparameters, i.e., parameters that must be pre-specified in advance, such as regularization parameters and parametric regularizers in variational regularization methods for inverse problems, and dictionaries in compressed sensing. A data-driven approach to determine appropriate hyperparameter values is via a nested optimization framework known as bilevel learning. Even when it is possible to employ a gradient-based solver to the bilevel optimization problem, construction of the gradients, known as hypergradients, is computationally challenging, each one requiring both a solution of a minimization problem and a linear system solve. These systems do not change much during the iterations, which motivates us to apply recycling Krylov subspace methods, wherein information from one linear system solve is re-used to solve the next linear system. Existing recycling strategies often employ eigenvector approximations called Ritz vectors. In this work we propose a novel recycling strategy based on a new concept, Ritz generalized singular vectors, which acknowledge the bilevel setting. Additionally, while existing iterative methods primarily terminate according to the residual norm, this new concept allows us to define a new stopping criterion that directly approximates the error of the associated hypergradient. The proposed approach is validated through extensive numerical testing in the context of an inverse problem in imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08264v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias J. Ehrhardt, Silvia Gazzola, Sebastian J. Scott</dc:creator>
    </item>
    <item>
      <title>A general approach to optimal imperfect maintenance activities of a repairable equipment with imperfect maintenance and multiple failure modes</title>
      <link>https://arxiv.org/abs/2412.08380</link>
      <description>arXiv:2412.08380v1 Announce Type: new 
Abstract: In this paper we describe a general approach to optimal imperfect maintenance activities of a repairable equipment with independent components. Most of the existing works on optimal imperfect maintenance activities of a repairable equipment with independent components. In addition, it is assumed that all the components of the equipment share the same model and the same maintenance intervals and that effectiveness of maintenance is known. In this paper we take a different approach. In order to formalize the uncertainty on the occurrence of failures and on the effect of maintenance activities we consider, for each component, a class of candidate models obtained combining models for failure rate with models for imperfect maintenance and let the data select the best model (that might be different for the different components). All the parameters are assumed to be unknown and are jointly estimated via maximum likelihood. Model selection is performed, separately for each component, using standard selection criteria that take into account the problem of over-parametrization. The selected models are used to derive the cost per unit time and the average reliability of the equipment, the objective functions of a Multi-Objective Optimization Problem with maintenance intervals of each single component as decision variables. The proposed procedure is illustrated using a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08380v1</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cie.2018.12.032</arxiv:DOI>
      <arxiv:journal_reference>Computers &amp; Industrial Engineering 128 (2019) 24-31</arxiv:journal_reference>
      <dc:creator>Rub\'en Mullor, Julio Mulero, Mario Trottini</dc:creator>
    </item>
    <item>
      <title>Ill-Posed Configurations in Random and Experimental Data Points Collection</title>
      <link>https://arxiv.org/abs/2412.08420</link>
      <description>arXiv:2412.08420v1 Announce Type: new 
Abstract: Ill-posed configurations, such as collinear or coplanar point arrangements, are a persistent challenge in computational geometry, complicating tasks as in triangulation and convex hull construction. This paper discusses the probability of such configurations arising in two scenarios: (1) data sampled randomly from a uniform distribution, and (2) data collected from physical systems, such as reflective surfaces or structured environments. We present a probabilistic framework, analyze the geometric and sampling constraints, and provide some mathematical insights into how data acquisition processes influence the likelihood of degeneracies. Notably, our findings reveal that degeneracies occur more frequently in physical systems than in purely random simulations due to systematic biases introduced by instrumental setups and environmental structures, emphasizing the risks of drawing conclusions solely based on assumptions derived from random data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08420v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Netzer Moriya</dc:creator>
    </item>
    <item>
      <title>Approximate Solutions in Linear Fractional Vector Optimization</title>
      <link>https://arxiv.org/abs/2412.08454</link>
      <description>arXiv:2412.08454v1 Announce Type: new 
Abstract: This paper studies approximate solutions of a linear fractional vector optimization problem without requiring boundedness of the constraint set. We establish necessary and sufficient conditions for approximating weakly efficient points of such a problem via some properties of the objective function and a technical lemma related to the intersection of the topological closure of the cone generated by a subset of the Euclidean space and the interior of the negative orthant. As a consequence, we obtain necessary conditions and sufficient conditions for approximate efficient solutions to the considered problem. Applications of these results to linear vector optimization are considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08454v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nguyen Thi Thu Huong</dc:creator>
    </item>
    <item>
      <title>Local Identifiability of Networks with Nonlinear Node Dynamics</title>
      <link>https://arxiv.org/abs/2412.08472</link>
      <description>arXiv:2412.08472v1 Announce Type: new 
Abstract: We study the identifiability of nonlinear network systems with partial excitation and partial measurement when the network dynamics is linear on the edges and nonlinear on the nodes. We assume that the graph topology and the nonlinear functions at the node level are known, and we aim to identify the weight matrix of the graph. Our main result is to prove that fully-connected layered feed-forward networks are generically locally identifiable by exciting sources and measuring sinks in the class of analytic functions that cross the origin. This holds even when all other nodes remain unexcited and unmeasured and stands in sharp contrast to most findings on network identifiability requiring measurement and/or excitation of each node. The result applies in particular to feed-forward artificial neural networks with no offsets and generalizes previous literature by considering a broader class of functions and topologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08472v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martina Vanelli, Julien M. Hendrickx</dc:creator>
    </item>
    <item>
      <title>An Enhanced Levenberg--Marquardt Method via Gram Reduction</title>
      <link>https://arxiv.org/abs/2412.08561</link>
      <description>arXiv:2412.08561v1 Announce Type: new 
Abstract: This paper studied the problem of solving the system of nonlinear equations ${\bf F}({\bf x})={\bf 0}$, where ${\bf F}:{\mathbb R}^{d}\to{\mathbb R}^d$. We propose Gram-Reduced Levenberg--Marquardt method which updates the Gram matrix ${\bf J}(\cdot)^\top{\bf J}(\cdot)$ in every $m$ iterations, where ${\bf J}(\cdot)$ is the Jacobian of ${\bf F}(\cdot)$. Our method has a global convergence guarantee without relying on any step of line-search or solving sub-problems. We prove our method takes at most $\mathcal{O}(m^2+m^{-0.5}\epsilon^{-2.5})$ iterations to find an $\epsilon$-stationary point of $\frac{1}{2}\|{\bf F}(\cdot)\|^2$, which leads to overall computation cost of $\mathcal{O}(d^3\epsilon^{-1}+d^2\epsilon^{-2})$ by taking $m=\Theta(\epsilon^{-1})$. Our results are strictly better than the cost of $\mathcal{O}(d^3\epsilon^{-2})$ for existing Levenberg--Marquardt methods. We also show the proposed method enjoys local superlinear convergence rate under the non-degenerate assumption. We provide experiments on real-world applications in scientific computing and machine learning to validate the efficiency of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08561v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chengchang Liu, Luo Luo, John C. S. Lui</dc:creator>
    </item>
    <item>
      <title>A Bi-Level Optimization Approach to Joint Trajectory Optimization for Redundant Manipulators</title>
      <link>https://arxiv.org/abs/2412.07859</link>
      <description>arXiv:2412.07859v1 Announce Type: cross 
Abstract: In this work, we present an approach to minimizing the time necessary for the end-effector of a redundant robot manipulator to traverse a Cartesian path by optimizing the trajectory of its joints. Each joint has limits in the ranges of position, velocity and acceleration, the latter making jerks in joint space undesirable. The proposed approach takes this nonlinear optimization problem whose variables are path speed and joint trajectory and reformulates it into a bi-level problem. The lower-level formulation is a convex subproblem that considers a fixed joint trajectory and maximizes path speed while considering all joint velocity and acceleration constraints. Under particular conditions, this subproblem has a closed-form solution. Then, we solve a higher-level subproblem by leveraging the directional derivative of the lower-level value with respect to the joint trajectory parameters. In particular, we use this direction to implement a Primal-Dual method that considers the path accuracy and joint position constraints. We show the efficacy of our proposed approach with simulations and experimental results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07859v1</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Fried, Santiago Paternain</dc:creator>
    </item>
    <item>
      <title>An Optimistic Algorithm for Online Convex Optimization with Adversarial Constraints</title>
      <link>https://arxiv.org/abs/2412.08060</link>
      <description>arXiv:2412.08060v1 Announce Type: cross 
Abstract: We study Online Convex Optimization (OCO) with adversarial constraints, where an online algorithm must make repeated decisions to minimize both convex loss functions and cumulative constraint violations. We focus on a setting where the algorithm has access to predictions of the loss and constraint functions. Our results show that we can improve the current best bounds of $ O(\sqrt{T}) $ regret and $ \tilde{O}(\sqrt{T}) $ cumulative constraint violations to $ O(\sqrt{E_T(f)}) $ and $ \tilde{O}(\sqrt{E_T(g)}) $, respectively, where $ E_T(f) $ and $ E_T(g) $ represent the cumulative prediction errors of the loss and constraint functions. In the worst case, where $ E_T(f) = O(T) $ and $ E_T(g) = O(T) $ (assuming bounded loss and constraint functions), our rates match the prior $ O(\sqrt{T}) $ results. However, when the loss and constraint predictions are accurate, our approach yields significantly smaller regret and cumulative constraint violations. Notably, if the constraint function remains constant over time, we achieve $ \tilde{O}(1) $ cumulative constraint violation, aligning with prior results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08060v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Lekeufack, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Offset-free model predictive control: stability under plant-model mismatch</title>
      <link>https://arxiv.org/abs/2412.08104</link>
      <description>arXiv:2412.08104v1 Announce Type: cross 
Abstract: We present the first general stability results for nonlinear offset-free model predictive control (MPC). Despite over twenty years of active research, the offset-free MPC literature has not shaken the assumption of closed-loop stability for establishing offset-free performance. In this paper, we present a nonlinear offset-free MPC design that is robustly stable with respect to the tracking errors, and thus achieves offset-free performance, despite plant-model mismatch and persistent disturbances. Key features and assumptions of this design include quadratic costs, differentiability of the plant and model functions, constraint backoffs at steady state, and a robustly stable state and disturbance estimator. We first establish nominal stability and offset-free performance. Then, robustness to state and disturbance estimate errors and setpoint and disturbance changes is demonstrated. Finally, the results are extended to sufficiently small plant-model mismatch. The results are illustrated by numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08104v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Steven J. Kuntz, James B. Rawlings</dc:creator>
    </item>
    <item>
      <title>DistrictNet: Decision-aware learning for geographical districting</title>
      <link>https://arxiv.org/abs/2412.08287</link>
      <description>arXiv:2412.08287v1 Announce Type: cross 
Abstract: Districting is a complex combinatorial problem that consists in partitioning a geographical area into small districts. In logistics, it is a major strategic decision determining operating costs for several years. Solving districting problems using traditional methods is intractable even for small geographical areas and existing heuristics often provide sub-optimal results. We present a structured learning approach to find high-quality solutions to real-world districting problems in a few minutes. It is based on integrating a combinatorial optimization layer, the capacitated minimum spanning tree problem, into a graph neural network architecture. To train this pipeline in a decision-aware fashion, we show how to construct target solutions embedded in a suitable space and learn from target solutions. Experiments show that our approach outperforms existing methods as it can significantly reduce costs on real-world cities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08287v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheikh Ahmed, Alexandre Forel, Axel Parmentier, Thibaut Vidal</dc:creator>
    </item>
    <item>
      <title>Global Controllability of the Kawahara Equation at Any Time</title>
      <link>https://arxiv.org/abs/2412.08353</link>
      <description>arXiv:2412.08353v1 Announce Type: cross 
Abstract: In this article, we prove that the nonlinear Kawahara equation on the periodic domain \(\mathbb{T}\) (the unit circle in the plane) is globally approximately controllable in \(H^s(\mathbb{T})\) for \(s \in \mathbb{N}\), at any time \(T &gt; 0\), using a two-dimensional control force. The proof is based on the Agrachev-Sarychev approach in geometric control theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08353v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakil Ahamed, Debanjit Mondal</dc:creator>
    </item>
    <item>
      <title>On Inverse Problems for Mean Field Games with Common Noise via Carleman Estimate</title>
      <link>https://arxiv.org/abs/2412.08483</link>
      <description>arXiv:2412.08483v1 Announce Type: cross 
Abstract: In this paper, we study two kinds of inverse problems for Mean Field Games (MFGs) with common noise. Our focus is on MFGs described by a coupled system of stochastic Hamilton-Jacobi-Bellman and Fokker-Planck equations. Firstly, we establish the Lipschitz and H\"older stability for determining the solutions of a coupled system of stochastic Hamilton-Jacobi-Bellman and Fokker-Planck equations based on terminal observation of the density function. Secondly, we derive a uniqueness theorem for an inverse source problem related to the system under consideration. The main tools to establish those results are two new Carleman estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08483v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi L\"u, Zhonghua Liao</dc:creator>
    </item>
    <item>
      <title>ConvMesh: Reimagining Mesh Quality Through Convex Optimization</title>
      <link>https://arxiv.org/abs/2412.08484</link>
      <description>arXiv:2412.08484v1 Announce Type: cross 
Abstract: Mesh generation has become a critical topic in recent years, forming the foundation of all 3D objects used across various applications, such as virtual reality, gaming, and 3D printing. With advancements in computational resources and machine learning, neural networks have emerged as powerful tools for generating high-quality 3D object representations, enabling accurate scene and object reconstructions. Despite these advancements, many methods produce meshes that lack realism or exhibit geometric and textural flaws, necessitating additional processing to improve their quality. This research introduces a convex optimization programming called disciplined convex programming to enhance existing meshes by refining their texture and geometry with a conic solver. By focusing on a sparse set of point clouds from both the original and target meshes, this method demonstrates significant improvements in mesh quality with minimal data requirements. To evaluate the approach, the classical dolphin mesh dataset from Facebook AI was used as a case study, with optimization performed using the CVXPY library. The results reveal promising potential for streamlined and effective mesh refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08484v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Valverde</dc:creator>
    </item>
    <item>
      <title>Fair Primal Dual Splitting Method for Image Inverse Problems</title>
      <link>https://arxiv.org/abs/2412.08613</link>
      <description>arXiv:2412.08613v1 Announce Type: cross 
Abstract: Image inverse problems have numerous applications, including image processing, super-resolution, and computer vision, which are important areas in image science. These application models can be seen as a three-function composite optimization problem solvable by a variety of primal dual-type methods. We propose a fair primal dual algorithmic framework that incorporates the smooth term not only into the primal subproblem but also into the dual subproblem. We unify the global convergence and establish the convergence rates of our proposed fair primal dual method. Experiments on image denoising and super-resolution reconstruction demonstrate the superiority of the proposed method over the current state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08613v1</guid>
      <category>cs.CV</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunfei Qu, Deren Han</dc:creator>
    </item>
    <item>
      <title>On the Uniqueness of Kantorovich Potentials</title>
      <link>https://arxiv.org/abs/2201.08316</link>
      <description>arXiv:2201.08316v2 Announce Type: replace 
Abstract: Kantorovich potentials denote the dual solutions of the renowned optimal transportation problem. Uniqueness of these solutions is relevant from both a theoretical and an algorithmic point of view, and has recently emerged as a necessary condition for asymptotic results in the context of statistical and entropic optimal transport. In this work, we challenge the common perception that uniqueness in continuous settings is reliant on the connectedness of the support of at least one of the involved measures, and we provide mild sufficient conditions for uniqueness even when both measures have disconnected support. Since our main finding builds upon the uniqueness of Kantorovich potentials on connected components, we revisit the corresponding arguments and provide generalizations of well-known results. To this end, we introduce the notion of induced regularity and employ it to extend the regularity theory of Kantorovich potentials advanced by Gangbo and McCann (1996) to more general cost functions in $\mathbb{R}^d$ and to geodesic spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.08316v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Staudt, Shayan Hundrieser, Axel Munk</dc:creator>
    </item>
    <item>
      <title>Global and Preference-based Optimization with Mixed Variables using Piecewise Affine Surrogates</title>
      <link>https://arxiv.org/abs/2302.04686</link>
      <description>arXiv:2302.04686v4 Announce Type: replace 
Abstract: Optimization problems involving mixed variables (i.e., variables of numerical and categorical nature) can be challenging to solve, especially in the presence of mixed-variable constraints. Moreover, when the objective function is the result of a complicated simulation or experiment, it may be expensive-to-evaluate. This paper proposes a novel surrogate-based global optimization algorithm to solve linearly constrained mixed-variable problems up to medium size (around 100 variables after encoding). The proposed approach is based on constructing a piecewise affine surrogate of the objective function over feasible samples. We assume the objective function is black-box and expensive-to-evaluate, while the linear constraints are quantifiable, unrelaxable, a priori known, and are cheap to evaluate. We introduce two types of exploration functions to efficiently search the feasible domain via mixed-integer linear programming solvers. We also provide a preference-based version of the algorithm designed for situations where only pairwise comparisons between samples can be acquired, while the underlying objective function to minimize remains unquantified. The two algorithms are evaluated on several unconstrained and constrained mixed-variable benchmark problems. The results show that, within a small number of required experiments/simulations, the proposed algorithms can often achieve better or comparable results than other existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.04686v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengjia Zhu, Alberto Bemporad</dc:creator>
    </item>
    <item>
      <title>Coordinated Multi-Agent Patrolling with State-Dependent Cost Rates: Asymptotically Optimal Policies for Large-Scale Systems</title>
      <link>https://arxiv.org/abs/2309.13388</link>
      <description>arXiv:2309.13388v3 Announce Type: replace 
Abstract: We study a large-scale patrol problem with state-dependent costs and multi-agent coordination.We consider heterogeneous agents, rather general reward functions, and the capabilities of tracking agents' trajectories.Given the complexity and uncertainty of the practical situations for patrolling, we model the problem as a discrete-time Markov decision process (MDP) that consists of a large number of parallel stochastic processes.We aim to minimize the cumulative patrolling cost over a finite time horizon. The problem exhibits an excessively large size of state space, which increases exponentially in the number of agents and the size of geographical region for patrolling. To reach practical solutions, we relax the dependencies between these parallel stochastic processes by randomizing all the state and action variables. In this context, the entire problem can be decomposed into a number of sub-problems, each of which has a much smaller state space and can be solved independently. The solutions of these sub-problems can lead to efficient heuristics. Unlike the past systems assuming relatively simple structure of the underlying stochastic process, here, tracking the patrol trajectories involves strong dependencies between the stochastic processes, leading to entirely different state and action spaces, transition kernels, and behaviours of processes, rendering the existing methods inapplicable or impractical. Further more, we prove that the performance deviation between the proposed policies and the possible optimal solution diminishes exponentially in the problem size, which also establishes the fact that the policies converge asymptotically at an exponential rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13388v3</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Fu, Zengfu Wang, Jie Chen</dc:creator>
    </item>
    <item>
      <title>A hybrid deep learning method for finite-horizon mean-field game problems</title>
      <link>https://arxiv.org/abs/2310.18968</link>
      <description>arXiv:2310.18968v3 Announce Type: replace 
Abstract: This paper develops a new deep learning algorithm to solve a class of finite-horizon mean-field games. The proposed hybrid algorithm uses Markov chain approximation method combined with a stochastic approximation-based iterative deep learning algorithm. Under the framework of finite-horizon mean-field games, the induced measure and Monte-Carlo algorithm are adopted to establish the iterative mean-field interaction in Markov chain approximation method and deep learning, respectively. The Markov chain approximation method plays a key role in constructing the iterative algorithm and estimating an initial value of a neural network, whereas stochastic approximation is used to find accurate parameters in a bounded region. The convergence of the hybrid algorithm is proved; two numerical examples are provided to illustrate the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18968v3</guid>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Zhang, Zhuo Jin, Jiaqin Wei, George Yin</dc:creator>
    </item>
    <item>
      <title>Adaptive waveform inversion for transmitted wave data</title>
      <link>https://arxiv.org/abs/2402.17696</link>
      <description>arXiv:2402.17696v3 Announce Type: replace 
Abstract: Adaptive Waveform Inversion (AWI) applied to transient transmitted wave data can yield estimates of index of refraction (or wave velocity) similar to those obtained by travel time inversion. The AWI objective function measures normalized mean-square dispersion about zero time of a family of filters, one filter for each source-reeciver pair, designed to match predicted data to observed data. Provided that the data contain a single smooth wavefront, this function approaches the mean square traveltime error as data wavelength tends to zero. The scaling of each filter to have unit norm is responsible for the simple relation with travel time tomography. A similar approach, Matched Source Waveform Inversion (MSWI), does not normalize the filters and has a looser relation with mean-square travel time error. If substantial energy is spread amongst multiple arrivals, on the other hand, neither AWI nor MSWI objectives approximate the travel time mean square erorr, and attempts to minimize them by local (Newton-related) optimization are as likely to stagnate at models predicting erroneous travel times as is least-square Full Waveform Inversion (FWI). The matching filter at the heart of this approach must be ``pre-whitened'', that is, computed by solving a regularized least squares problem for filtered data misfit. In order for the relation to traveltime tomography to hold, and for the filtered data misfit to stay relatively small, the regularization weight must be coupled to data wavelength. These objective functions can also be used as penalty terms in a homotopy of objectives connecting AWI or MSWI with FWI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17696v3</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William W. Symes</dc:creator>
    </item>
    <item>
      <title>Variational Dynamic Programming for Stochastic Optimal Control</title>
      <link>https://arxiv.org/abs/2404.14806</link>
      <description>arXiv:2404.14806v3 Announce Type: replace 
Abstract: We consider the problem of stochastic optimal control, where the state-feedback control policies take the form of a probability distribution and where a penalty on the entropy is added. By viewing the cost function as a Kullback- Leibler (KL) divergence between two joint distributions, we bring the tools from variational inference to bear on our optimal control problem. This allows for deriving a dynamic programming principle, where the value function is defined as a KL divergence again. We then resort to Gaussian distributions to approximate the control policies and apply the theory to control affine nonlinear systems with quadratic costs. This results in closed-form recursive updates, which generalize LQR control and the backward Riccati equation. We illustrate this novel method on the simple problem of stabilizing an inverted pendulum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14806v3</guid>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>2024 Conference on Decision and Control, Dec 2024, Milano, Italy</arxiv:journal_reference>
      <dc:creator>Marc Lambert (SIERRA), Francis Bach (SIERRA), Silv\`ere Bonnabel (CAOR)</dc:creator>
    </item>
    <item>
      <title>Analysis and Synthesis Denoisers for Forward-Backward Plug-and-Play Algorithms</title>
      <link>https://arxiv.org/abs/2411.13276</link>
      <description>arXiv:2411.13276v2 Announce Type: replace 
Abstract: In this work we study the behavior of the forward-backward (FB) algorithm when the proximity operator is replaced by a sub-iterative procedure to approximate a Gaussian denoiser, in a Plug-and-Play (PnP) fashion. In particular, we consider both analysis and synthesis Gaussian denoisers within a dictionary framework, obtained by unrolling dual-FB iterations or FB iterations, respectively. We analyze the associated minimization problems as well as the asymptotic behavior of the resulting FB-PnP iterations. In particular, we show that the synthesis Gaussian denoising problem can be viewed as a proximity operator. For each case, analysis and synthesis, we show that the FB-PnP algorithms solve the same problem whether we use only one or an infinite number of sub-iteration to solve the denoising problem at each iteration. To this aim, we show that each "one sub-iteration" strategy within the FB-PnP can be interpreted as a primal-dual algorithm when a warm-restart strategy is used. We further present similar results when using a Moreau-Yosida smoothing of the global problem, for an arbitrary number of sub-iterations. Finally, we provide numerical simulations to illustrate our theoretical results. In particular we first consider a toy compressive sensing example, as well as an image restoration problem in a deep dictionary framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13276v2</guid>
      <category>math.OC</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthieu Kowalski, Beno\^it Mal\'ezieux, Thomas Moreau, Audrey Repetti</dc:creator>
    </item>
    <item>
      <title>Majority voting is not good for heaven or hell, with mirrored performance</title>
      <link>https://arxiv.org/abs/2401.00592</link>
      <description>arXiv:2401.00592v3 Announce Type: replace-cross 
Abstract: Within the ViSE (Voting in Stochastic Environment) model, we study the effectiveness of majority voting in various environments. By the pit of losses paradox, majority decisions in apparently hostile environments systematically reduce the capital of society. In such cases, the simple social decision rule of ``rejecting all proposals without voting'' outperforms majority. In this paper, we identify another pit of losses appearing in favorable environments. Here, the simple social decision rule of ``accepting all proposals without voting'' is superior to majority. We prove that under a version of simple majority called symmetrized majority and the antisymmetry of the voting body, the second pit of losses is a mirror image of the pit of losses in hostile environments and explain this phenomenon. Technically, we consider a voting society consisting of individualists whose strategy is supporting all proposals that increase their capital and a group (groups) whose members vote to increase the total group capital. According to the main result, the expected capital gain of each agent in the environment whose generator $X$ has mean $\mu&gt;0$ exceeds by $\mu$ their expected capital gain under generator $-X$. This result extends to location families of generators with distributions symmetric about their mean. The mentioned result determines the symmetry of the difference between the expected capital gain under the symmetrized majority and that under the ``basic'' social decision rule that rejects/accepts all proposals in unfavorable/favorable environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00592v3</guid>
      <category>physics.soc-ph</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pavel Chebotarev, Vadim Afonkin</dc:creator>
    </item>
    <item>
      <title>Bayesian inverse Navier-Stokes problems: joint flow field reconstruction and parameter learning</title>
      <link>https://arxiv.org/abs/2406.18464</link>
      <description>arXiv:2406.18464v3 Announce Type: replace-cross 
Abstract: We formulate and solve a Bayesian inverse Navier-Stokes (N-S) problem that assimilates velocimetry data in order to jointly reconstruct a 3D flow field and learn the unknown N-S parameters, including the boundary position. By hardwiring a generalised N-S problem, and regularising its unknown parameters using Gaussian prior distributions, we learn the most likely parameters in a collapsed search space. The most likely flow field reconstruction is then the N-S solution that corresponds to the learned parameters. We develop the method in the variational setting and use a stabilised Nitsche weak form of the N-S problem that permits the control of all N-S parameters. To regularise the inferred the geometry, we use a viscous signed distance field (vSDF) as an auxiliary variable, which is given as the solution of a viscous Eikonal boundary value problem. We devise an algorithm that solves this inverse problem, and numerically implement it using an adjoint-consistent stabilised cut-cell finite element method. We then use this method to reconstruct magnetic resonance velocimetry (flow-MRI) data of a 3D steady laminar flow through a physical model of an aortic arch for two different Reynolds numbers and signal-to-noise ratio (SNR) levels (low/high). We find that the method can accurately i) reconstruct the low SNR data by filtering out the noise/artefacts and recovering flow features that are obscured by noise, and ii) reproduce the high SNR data without overfitting. Although the framework that we develop applies to 3D steady laminar flows in complex geometries, it readily extends to time-dependent laminar and Reynolds-averaged turbulent flows, as well as non-Newtonian (e.g. viscoelastic) fluids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18464v3</guid>
      <category>physics.flu-dyn</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1361-6420/ad9cb7</arxiv:DOI>
      <dc:creator>Alexandros Kontogiannis, Scott V. Elgersma, Andrew J. Sederman, Matthew P. Juniper</dc:creator>
    </item>
    <item>
      <title>Singular perturbation in heavy ball dynamics</title>
      <link>https://arxiv.org/abs/2407.15044</link>
      <description>arXiv:2407.15044v2 Announce Type: replace-cross 
Abstract: Given a $C^{1,1}_\mathrm{loc}$ lower bounded function $f:\mathbb{R}^n\rightarrow \mathbb{R}$ definable in an o-minimal structure on the real field, we show that the singular perturbation $\epsilon \searrow 0$ in the heavy ball system \begin{equation} \label{eq:P_eps} \tag{$P_\epsilon$}
  \epsilon\ddot{x}_\epsilon(t) + \gamma\dot{x}_\epsilon(t) + \nabla f(x_\epsilon(t)) = 0, ~~~ \forall t \geqslant 0, ~~~ x_\epsilon(0) = x_0, ~~~ \dot{x}_\epsilon(0) = \dot{x}_0, \end{equation} preserves boundedness of solutions, where $\gamma&gt;0$ is the friction and $(x_0,\dot{x}_0) \in \mathbb{R}^n \times \mathbb{R}^n$ is the initial condition. This complements the work of Attouch, Goudou, and Redont which deals with finite time horizons. In other words, this work studies the asymptotic behavior of a ball rolling on a surface subject to gravitation and friction, without assuming convexity nor coercivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15044v2</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cedric Josz, Xiaopeng Li</dc:creator>
    </item>
    <item>
      <title>Learning incomplete factorization preconditioners for GMRES</title>
      <link>https://arxiv.org/abs/2409.08262</link>
      <description>arXiv:2409.08262v2 Announce Type: replace-cross 
Abstract: Incomplete LU factorizations of sparse matrices are widely used as preconditioners in Krylov subspace methods to speed up solving linear systems. Unfortunately, computing the preconditioner itself can be time-consuming and sensitive to hyper-parameters. Instead, we replace the hand-engineered algorithm with a graph neural network that is trained to approximate the matrix factorization directly. To apply the output of the neural network as a preconditioner, we propose an output activation function that guarantees that the predicted factorization is invertible. Further, applying a graph neural network architecture allows us to ensure that the output itself is sparse which is desirable from a computational standpoint. We theoretically analyze and empirically evaluate different loss functions to train the learned preconditioners and show their effectiveness in decreasing the number of GMRES iterations and improving the spectral properties on synthetic data. The code is available at https://github.com/paulhausner/neural-incomplete-factorization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08262v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul H\"ausner, Aleix Nieto Juscafresa, Jens Sj\"olund</dc:creator>
    </item>
    <item>
      <title>Learning Variational Inequalities from Data: Fast Generalization Rates under Strong Monotonicity</title>
      <link>https://arxiv.org/abs/2410.20649</link>
      <description>arXiv:2410.20649v2 Announce Type: replace-cross 
Abstract: Variational inequalities (VIs) are a broad class of optimization problems encompassing machine learning problems ranging from standard convex minimization to more complex scenarios like min-max optimization and computing the equilibria of multi-player games. In convex optimization, strong convexity allows for fast statistical learning rates requiring only $\Theta(1/\epsilon)$ stochastic first-order oracle calls to find an $\epsilon$-optimal solution, rather than the standard $\Theta(1/\epsilon^2)$ calls. In this paper, we explain how one can similarly obtain fast $\Theta(1/\epsilon)$ rates for learning VIs that satisfy strong monotonicity, a generalization of strong convexity. Specifically, we demonstrate that standard stability-based generalization arguments for convex minimization extend directly to VIs when the domain admits a small covering, or when the operator is integrable and suboptimality is measured by potential functions; such as when finding equilibria in multi-player games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20649v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Zhao, Tatjana Chavdarova, Michael Jordan</dc:creator>
    </item>
  </channel>
</rss>
