<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Jan 2025 02:27:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Inferring collective synchrony observing spiking of one or several neurons</title>
      <link>https://arxiv.org/abs/2501.07696</link>
      <description>arXiv:2501.07696v1 Announce Type: new 
Abstract: We tackle a quantification of synchrony in a large ensemble of interacting neurons from the observation of spiking events. In a simulation study, we efficiently infer the synchrony level in a neuronal population from a point process reflecting spiking of a small number of units and even from a single neuron. We introduce a synchrony measure (order parameter) based on the Bartlett covariance density; this quantity can be easily computed from the recorded point process. This measure is robust concerning missed spikes and, if computed from observing several neurons, does not require spike sorting. We illustrate the approach by modeling populations of spiking or bursting neurons, including the case of sparse synchrony.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07696v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arkady Pikovsky, Michael Rosenblum</dc:creator>
    </item>
    <item>
      <title>Operators in the mind: Jan Lukasiewicz and Polish notation</title>
      <link>https://arxiv.org/abs/2501.07660</link>
      <description>arXiv:2501.07660v1 Announce Type: cross 
Abstract: In 1929 Jan Lukasiewicz used, apparently for the first time, his Polish notation to represent the operations of formal logic. This is a parenthesis-free notation, which also implies that logical functions are operators preceding the variables on which they act. In the 1980s, within the framework of research into mathematical models on the parallel processing of neural systems, a group of operators emerged -- neurally inspired and based on matrix algebra -- which computed logical operations automatically. These matrix operators reproduce the order of operators and variables of Polish notation. These logical matrices can also generate a three-valued logic with broad similarities to Lukasiewicz's three-valued logic. In this paper, a parallel is drawn between relevant formulas represented in Polish notation, and their counterparts in terms of neurally based matrix operators. Lukasiewicz's three-valued logic, shown in Polish notation has several points of contact with what matrices produce when they process uncertain truth vectors. This formal parallelism opens up scientific and philosophical perspectives that deserve to be further explored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07660v1</guid>
      <category>math.HO</category>
      <category>math.LO</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Eduardo Mizraji</dc:creator>
    </item>
    <item>
      <title>CREIMBO: Cross-Regional Ensemble Interactions in Multi-view Brain Observations</title>
      <link>https://arxiv.org/abs/2405.17395</link>
      <description>arXiv:2405.17395v2 Announce Type: replace 
Abstract: Modern recordings of neural activity provide diverse observations of neurons across brain areas, conditions, and subjects; presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics. Current analysis methods often fail to harness the richness of such data, as they provide either uninterpretable representations or oversimplify models (e.g., by assuming stationary dynamics). Here, instead of regarding asynchronous neural recordings that lack alignment in neural identity or brain areas as a limitation, we leverage these diverse views into the brain to learn a unified model of neural dynamics. We assume that brain activity is driven by multiple hidden global sub-circuits. These sub-circuits represent global basis interactions between neural ensembles -- functional groups of neurons -- such that the time-varying decomposition of these circuits defines how the ensembles' interactions evolve over time non-stationarily. We discover the neural ensembles underlying non-simultaneous observations, along with their non-stationary evolving interactions, with our new model, CREIMBO. CREIMBO identifies the hidden composition of per-session neural ensembles through graph-driven dictionary learning and models the ensemble dynamics on a low-dimensional manifold spanned by a sparse time-varying composition of the global sub-circuits. Thus, CREIMBO disentangles overlapping temporal neural processes while preserving interpretability due to the use of a shared underlying sub-circuit basis. Moreover, CREIMBO distinguishes session-specific computations from global (session-invariant) ones by identifying session covariates and variations in sub-circuit activations. We demonstrate CREIMBO's ability to recover true components in synthetic data, and uncover meaningful brain dynamics including cross-subject neural mechanisms and inter- vs. intra-region dynamical motifs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17395v2</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noga Mudrik, Ryan Ly, Oliver Ruebel, Adam S. Charles</dc:creator>
    </item>
    <item>
      <title>Divergences between Language Models and Human Brains</title>
      <link>https://arxiv.org/abs/2311.09308</link>
      <description>arXiv:2311.09308v3 Announce Type: replace-cross 
Abstract: Do machines and humans process language in similar ways? Recent research has hinted at the affirmative, showing that human neural activity can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using an LLM-based data-driven approach, we identify two domains that LMs do not capture well: social/emotional intelligence and physical commonsense. We validate these findings with human behavioral experiments and hypothesize that the gap is due to insufficient representations of social/emotional and physical knowledge in LMs. Our results show that fine-tuning LMs on these domains can improve their alignment with human brain responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09308v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchen Zhou, Emmy Liu, Graham Neubig, Michael J. Tarr, Leila Wehbe</dc:creator>
    </item>
    <item>
      <title>Mode-conditioned music learning and composition: a spiking neural network inspired by neuroscience and psychology</title>
      <link>https://arxiv.org/abs/2411.14773</link>
      <description>arXiv:2411.14773v2 Announce Type: replace-cross 
Abstract: Musical mode is one of the most critical element that establishes the framework of pitch organization and determines the harmonic relationships. Previous works often use the simplistic and rigid alignment method, and overlook the diversity of modes. However, in contrast to AI models, humans possess cognitive mechanisms for perceiving the various modes and keys. In this paper, we propose a spiking neural network inspired by brain mechanisms and psychological theories to represent musical modes and keys, ultimately generating musical pieces that incorporate tonality features. Specifically, the contributions are detailed as follows: 1) The model is designed with multiple collaborated subsystems inspired by the structures and functions of corresponding brain regions; 2)We incorporate mechanisms for neural circuit evolutionary learning that enable the network to learn and generate mode-related features in music, reflecting the cognitive processes involved in human music perception. 3)The results demonstrate that the proposed model shows a connection framework closely similar to the Krumhansl-Schmuckler model, which is one of the most significant key perception models in the music psychology domain. 4) Experiments show that the model can generate music pieces with characteristics of the given modes and keys. Additionally, the quantitative assessments of generated pieces reveals that the generating music pieces have both tonality characteristics and the melodic adaptability needed to generate diverse and musical content. By combining insights from neuroscience, psychology, and music theory with advanced neural network architectures, our research aims to create a system that not only learns and generates music but also bridges the gap between human cognition and artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14773v2</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>eess.AS</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Liang, Yi Zeng, Menghaoran Tang</dc:creator>
    </item>
    <item>
      <title>A comparative study of sensory encoding models for human navigation in virtual reality</title>
      <link>https://arxiv.org/abs/2501.06698</link>
      <description>arXiv:2501.06698v2 Announce Type: replace-cross 
Abstract: In virtual reality applications, users often navigate through virtual environments, but the issue of physiological responses, such as cybersickness, fatigue, and cognitive workload, can disrupt or even halt these activities. Despite its impact, the underlying mechanisms of how the sensory system encodes information in VR remain unclear. In this study, we compare three sensory encoding models, Bayesian Efficient Coding, Fitness Maximizing Coding, and the Linear Nonlinear Poisson model, regarding their ability to simulate human navigation behavior in VR. By incorporating the factor of physiological responses into the models, we find that the Bayesian Efficient Coding model generally outperforms the others. Furthermore, the Fitness Maximizing Code framework provides more accurate estimates when the error penalty is small. Our results suggest that the Bayesian Efficient Coding framework offers superior predictions in most scenarios, providing a better understanding of human navigation behavior in VR environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06698v2</guid>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tangyao Li, Qiyuan Zhan, Yitong Zhu, Bojing Hou, Yuyang Wang</dc:creator>
    </item>
  </channel>
</rss>
