<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Apr 2025 03:06:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Trading off Relevance and Revenue in the Jobs Marketplace: Estimation, Optimization and Auction Design</title>
      <link>https://arxiv.org/abs/2504.03618</link>
      <description>arXiv:2504.03618v1 Announce Type: new 
Abstract: We study the problem of position allocation in job marketplaces, where the platform determines the ranking of the jobs for each seeker. The design of ranking mechanisms is critical to marketplace efficiency, as it influences both short-term revenue from promoted job placements and long-term health through sustained seeker engagement. Our analysis focuses on the tradeoff between revenue and relevance, as well as the innovations in job auction design. We demonstrated two ways to improve relevance with minimal impact on revenue: incorporating the seekers preferences and applying position-aware auctions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03618v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Farzad Pourbabaee, Sophie Yanying Sheng, Peter McCrory, Luke Simon, Di Mo</dc:creator>
    </item>
    <item>
      <title>Persuasive Calibration</title>
      <link>https://arxiv.org/abs/2504.03211</link>
      <description>arXiv:2504.03211v1 Announce Type: cross 
Abstract: We introduce and study the persuasive calibration problem, where a principal aims to provide trustworthy predictions about underlying events to a downstream agent to make desired decisions. We adopt the standard calibration framework that regulates predictions to be unbiased conditional on their own value, and thus, they can reliably be interpreted at the face value by the agent. Allowing a small calibration error budget, we aim to answer the following question: what is and how to compute the optimal predictor under this calibration error budget, especially when there exists incentive misalignment between the principal and the agent? We focus on standard Lt-norm Expected Calibration Error (ECE) metric.
  We develop a general framework by viewing predictors as post-processed versions of perfectly calibrated predictors. Using this framework, we first characterize the structure of the optimal predictor. Specifically, when the principal's utility is event-independent and for L1-norm ECE, we show: (1) the optimal predictor is over-(resp. under-) confident for high (resp. low) true expected outcomes, while remaining perfectly calibrated in the middle; (2) the miscalibrated predictions exhibit a collinearity structure with the principal's utility function. On the algorithmic side, we provide a FPTAS for computing approximately optimal predictor for general principal utility and general Lt-norm ECE. Moreover, for the L1- and L-Infinity-norm ECE, we provide polynomial-time algorithms that compute the exact optimal predictor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03211v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiding Feng, Wei Tang</dc:creator>
    </item>
    <item>
      <title>A Polynomial-Time Algorithm for Variational Inequalities under the Minty Condition</title>
      <link>https://arxiv.org/abs/2504.03432</link>
      <description>arXiv:2504.03432v1 Announce Type: cross 
Abstract: Solving (Stampacchia) variational inequalities (SVIs) is a foundational problem at the heart of optimization, with a host of critical applications ranging from engineering to economics. However, this expressivity comes at the cost of computational hardness. As a result, most research has focused on carving out specific subclasses that elude those intractability barriers. A classical property that goes back to the 1960s is the Minty condition, which postulates that the Minty VI (MVI) problem -- the weak dual of the SVI problem -- admits a solution.
  In this paper, we establish the first polynomial-time algorithm -- that is, with complexity growing polynomially in the dimension $d$ and $\log(1/\epsilon)$ -- for solving $\epsilon$-SVIs for Lipschitz continuous mappings under the Minty condition. Prior approaches either incurred an exponentially worse dependence on $1/\epsilon$ (and other natural parameters of the problem) or made overly restrictive assumptions -- such as strong monotonicity. To do so, we introduce a new variant of the ellipsoid algorithm wherein separating hyperplanes are obtained after taking a gradient descent step from the center of the ellipsoid. It succeeds even though the set of SVIs can be nonconvex and not fully dimensional. Moreover, when our algorithm is applied to an instance with no MVI solution and fails to identify an SVI solution, it produces a succinct certificate of MVI infeasibility. We also show that deciding whether the Minty condition holds is $\mathsf{coNP}$-complete.
  We provide several extensions and new applications of our main results. Specifically, we obtain the first polynomial-time algorithms for i) solving monotone VIs, ii) globally minimizing a (potentially nonsmooth) quasar-convex function, and iii) computing Nash equilibria in multi-player harmonic games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03432v1</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ioannis Anagnostides, Gabriele Farina, Tuomas Sandholm, Brian Hu Zhang</dc:creator>
    </item>
    <item>
      <title>Optimistic Online Learning in Symmetric Cone Games</title>
      <link>https://arxiv.org/abs/2504.03592</link>
      <description>arXiv:2504.03592v1 Announce Type: cross 
Abstract: Optimistic online learning algorithms have led to significant advances in equilibrium computation, particularly for two-player zero-sum games, achieving an iteration complexity of $\mathcal{O}(1/\epsilon)$ to reach an $\epsilon$-saddle point. These advances have been established in normal-form games, where strategies are simplex vectors, and quantum games, where strategies are trace-one positive semidefinite matrices. We extend optimistic learning to symmetric cone games (SCGs), a class of two-player zero-sum games where strategy spaces are generalized simplices (trace-one slices of symmetric cones). A symmetric cone is the cone of squares of a Euclidean Jordan Algebra; canonical examples include the nonnegative orthant, the second-order cone, the cone of positive semidefinite matrices, and their products, all fundamental to convex optimization. SCGs unify normal-form and quantum games and, as we show, offer significantly greater modeling flexibility, allowing us to model applications such as distance metric learning problems and the Fermat-Weber problem. To compute approximate saddle points in SCGs, we introduce the Optimistic Symmetric Cone Multiplicative Weights Update algorithm and establish an iteration complexity of $\mathcal{O}(1/\epsilon)$ to reach an $\epsilon$-saddle point. Our analysis builds on the Optimistic Follow-the-Regularized-Leader framework, with a key technical contribution being a new proof of the strong convexity of the symmetric cone negative entropy with respect to the trace-one norm, a result that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03592v1</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anas Barakat, Wayne Lin, John Lazarsfeld, Antonios Varvitsiotis</dc:creator>
    </item>
    <item>
      <title>Fair Division Beyond Monotone Valuations</title>
      <link>https://arxiv.org/abs/2501.14609</link>
      <description>arXiv:2501.14609v3 Announce Type: replace 
Abstract: This paper studies fair division of divisible and indivisible items among agents whose cardinal preferences are not necessarily monotone. We establish the existence of fair divisions and develop approximation algorithms to compute them.
  We address two complementary valuation classes, subadditive and nonnegative, which go beyond monotone functions. Considering both the division of cake (divisible resources) and allocation of indivisible items, we obtain fairness guarantees in terms of (approximate) envy-freeness (EF) and equability (EQ)
  In the context of envy-freeness, we prove that an EF division of a cake always exists under cake valuations that are subadditive and globally nonnegative. This result complements the nonexistence of EF allocations for burnt cakes known for more general valuations. In the indivisible-items setting, we establish the existence of EF3 allocations for subadditive and globally nonnegative valuations. In addition, we obtain universal existence of EF3 allocations under nonnegative valuations.
  We study equitability under nonnegative valuations. Here, we prove that EQ3 allocations always exist when the agents' valuations are nonnegative. Also, in the indivisible-items setting, we develop an approximation algorithm that, for given nonnegative valuations, finds allocations that are equitable within additive margins.
  Our results have combinatorial implications. For instance, the developed results imply the universal existence of proximately dense subgraphs: Given any graph $G=(V, E)$ and integer $k$ (at most $|V|$), there always exists a partition $V_1, V_2, \ldots, V_k$ of the vertex set such that the edge densities within the parts, $V_i$, are additively within four of each other. Further, such a partition can be computed efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14609v3</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddharth Barman, Paritosh Verma</dc:creator>
    </item>
    <item>
      <title>Controlled Social Learning: Altruism vs. Bias</title>
      <link>https://arxiv.org/abs/2504.02648</link>
      <description>arXiv:2504.02648v2 Announce Type: replace-cross 
Abstract: We introduce a model of controlled sequential social learning in which a planner may pay a cost to adjust the private information structure of agents. The planner may seek to induce correct actions that are consistent with an unknown true state of the world (altruistic planner) or to induce a specific action the planner prefers (biased planner). Our framework presents a new optimization problem for social learning that combines dynamic programming with decentralized action choices and Bayesian belief updates. This sheds light on practical policy questions, such as how the socially optimal level of ad personalization changes according to current beliefs or how a political campaign may selectively illuminate or obfuscate the winning potential of its candidate among voters. We then prove the convexity of the value function and characterize the optimal policies of altruistic and biased planners, which attain desired tradeoffs between the costs they incur and the payoffs they earn from the choices they induce in the agents. Even for a planner who has equivalent knowledge to an individual, cannot lie or cherry-pick information, and is fully observable, we demonstrate that it is possible to dramatically influence social welfare in both positive and negative directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02648v2</guid>
      <category>eess.SY</category>
      <category>cs.GT</category>
      <category>cs.SI</category>
      <category>cs.SY</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raghu Arghal, Kevin He, Shirin Saeedi Bidokhti, Saswati Sarkar</dc:creator>
    </item>
  </channel>
</rss>
