<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 31 Dec 2024 03:19:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Empirical likelihood for Fr\'echet means on open books</title>
      <link>https://arxiv.org/abs/2412.18818</link>
      <description>arXiv:2412.18818v1 Announce Type: new 
Abstract: Empirical Likelihood (EL) is a type of nonparametric likelihood that is useful in many statistical inference problems, including confidence region construction and $k$-sample problems. It enjoys some remarkable theoretical properties, notably Bartlett correctability. One area where EL has potential but is under-developed is in non-Euclidean statistics where the Fr\'echet mean is the population characteristic of interest. Only recently has a general EL method been proposed for smooth manifolds. In this work, we continue progress in this direction and develop an EL method for the Fr\'echet mean on a stratified metric space that is not a manifold: the open book, obtained by gluing copies of a Euclidean space along their common boundaries. The structure of an open book captures the essential behaviour of the Fr\'echet mean around certain singular regions of more general stratified spaces for complex data objects, and relates intimately to the local geometry of non-binary trees in the well-studied phylogenetic treespace. We derive a version of Wilks' theorem for the EL statistic, and elucidate on the delicate interplay between the asymptotic distribution and topology of the neighbourhood around the population Fr\'echet mean. We then present a bootstrap calibration of the EL, which proves that under mild conditions, bootstrap calibration of EL confidence regions have coverage error of size $O(n^{-2})$ rather than $O(n^{-1})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18818v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik Bharath, Huiling Le, Andrew T A Wood, Xi Yan</dc:creator>
    </item>
    <item>
      <title>Optimal Federated Learning for Functional Mean Estimation under Heterogeneous Privacy Constraints</title>
      <link>https://arxiv.org/abs/2412.18992</link>
      <description>arXiv:2412.18992v1 Announce Type: new 
Abstract: Federated learning (FL) is a distributed machine learning technique designed to preserve data privacy and security, and it has gained significant importance due to its broad range of applications. This paper addresses the problem of optimal functional mean estimation from discretely sampled data in a federated setting.
  We consider a heterogeneous framework where the number of individuals, measurements per individual, and privacy parameters vary across one or more servers, under both common and independent design settings. In the common design setting, the same design points are measured for each individual, whereas in the independent design, each individual has their own random collection of design points. Within this framework, we establish minimax upper and lower bounds for the estimation error of the underlying mean function, highlighting the nuanced differences between common and independent designs under distributed privacy constraints.
  We propose algorithms that achieve the optimal trade-off between privacy and accuracy and provide optimality results that quantify the fundamental limits of private functional mean estimation across diverse distributed settings. These results characterize the cost of privacy and offer practical insights into the potential for privacy-preserving statistical analysis in federated environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18992v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tony Cai, Abhinav Chakraborty, Lasse Vuursteen</dc:creator>
    </item>
    <item>
      <title>Outlier-Bias Removal with Alpha Divergence: A Robust Non-Convex Estimator for Linear Regression</title>
      <link>https://arxiv.org/abs/2412.19183</link>
      <description>arXiv:2412.19183v1 Announce Type: new 
Abstract: Convex and penalized robust methods often suffer from bias induced by large outliers, limiting their effectiveness in adversarial or heavy-tailed settings. In this study, we propose a novel approach that eliminates this bias (when possible) by leveraging a non-convex $M$-estimator based on the alpha divergence. We address the problem of estimating the parameters vector in high dimensional linear regression, even when a subset of the data has been deliberately corrupted by an adversary with full knowledge of the dataset and its underlying distribution.
  Our primary contribution is to demonstrate that the objective function, although non-convex, exhibits convexity within a carefully chosen basin of attraction, enabling robust and unbiased estimation. Additionally, we establish three key theoretical guarantees for the estimator: (a) a deviation bound that is minimax optimal up to a logarithmic factor, (b) an improved unbiased bound when the outliers are large and (c) asymptotic normality as the sample size increases. Finally, we validate the theoretical findings through empirical comparisons with state-of-the-art estimators on both synthetic and real-world datasets, highlighting the proposed method's superior robustness, efficiency, and ability to mitigate outlier-induced bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19183v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilyes Hammouda, Mohamed Ndaoud, and Abd-Krim Seghouane</dc:creator>
    </item>
    <item>
      <title>On optimal linear prediction</title>
      <link>https://arxiv.org/abs/2412.19186</link>
      <description>arXiv:2412.19186v1 Announce Type: new 
Abstract: The main purpose of this article is to prove that, under certain assumptions in a linear prediction setting, optimal methods based upon model reduction and even an optimal predictor can be provided. The optimality is formulated in terms of the expected mean square prediction error. The optimal model reduction turns out, under a certain assumption, to correspond to the statistical model for partial least squares discussed by the author elsewhere, and under a certain specific condition, the partial least squares predictors is proved to be good compared to all other predictors. It is also proved in this article that the situation with two different model reductions can be fit into a quantum mechanical setting. Thus, the article contains a synthesis of three cultures: mathematical statistics as a basis, algorithms introduced by chemometricians and used very much by applied scientists as a background, and finally, notions from quantum foundation as an alternative point of view.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19186v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Inge S. Helland</dc:creator>
    </item>
    <item>
      <title>Priors for second-order unbiased Bayes estimators</title>
      <link>https://arxiv.org/abs/2412.19187</link>
      <description>arXiv:2412.19187v1 Announce Type: new 
Abstract: Asymptotically unbiased priors, introduced by Hartigan (1965), are designed to achieve second-order unbiasedness of Bayes estimators. This paper extends Hartigan's framework to non-i.i.d. models by deriving a system of partial differential equations that characterizes asymptotically unbiased priors. Furthermore, we establish a necessary and sufficient condition for the existence of such priors and propose a simple procedure for constructing them.
  The proposed method is applied to several examples, including the linear regression model and the nested error regression (NER) model (also known as the random effects model). Simulation studies evaluate the frequentist properties of the Bayes estimator under the asymptotically unbiased prior for the NER model, highlighting its effectiveness in small-sample settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19187v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mana Sakai, Takeru Matsuda, Tatsuya Kubokawa</dc:creator>
    </item>
    <item>
      <title>Central limit theorems for vector-valued composite functionals with smoothing and applications</title>
      <link>https://arxiv.org/abs/2412.19367</link>
      <description>arXiv:2412.19367v2 Announce Type: new 
Abstract: This paper focuses on vector-valued composite functionals, which may be nonlinear in probability. Our primary goal is to establish central limit theorems for these functionals when mixed estimators are employed. Our study is relevant to the evaluation and comparison of risk in decision-making contexts and extends to functionals that arise in machine learning methods. A generalized family of composite risk functionals is presented, which encompasses most of the known coherent risk measures including systemic measures of risk. The paper makes two main contributions. First, we analyze vector-valued functionals, providing a framework for evaluating high-dimensional risks. This framework facilitates the comparison of multiple risk measures, as well as the estimation and asymptotic analysis of systemic risk and its optimal value in decision-making problems. Second, we derive novel central limit theorems for optimized composite functionals when mixed types of estimators: empirical and smoothed estimators are used. We provide verifiable sufficient conditions for the central limit formulae and show their applicability to several popular measures of risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19367v2</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huhui Chen, Darinka Dentcheva, Yang Lin, Gregory J. Stock</dc:creator>
    </item>
    <item>
      <title>High-accuracy sampling from constrained spaces with the Metropolis-adjusted Preconditioned Langevin Algorithm</title>
      <link>https://arxiv.org/abs/2412.18701</link>
      <description>arXiv:2412.18701v1 Announce Type: cross 
Abstract: In this work, we propose a first-order sampling method called the Metropolis-adjusted Preconditioned Langevin Algorithm for approximate sampling from a target distribution whose support is a proper convex subset of $\mathbb{R}^{d}$. Our proposed method is the result of applying a Metropolis-Hastings filter to the Markov chain formed by a single step of the preconditioned Langevin algorithm with a metric $\mathscr{G}$, and is motivated by the natural gradient descent algorithm for optimisation. We derive non-asymptotic upper bounds for the mixing time of this method for sampling from target distributions whose potentials are bounded relative to $\mathscr{G}$, and for exponential distributions restricted to the support. Our analysis suggests that if $\mathscr{G}$ satisfies stronger notions of self-concordance introduced in Kook and Vempala (2024), then these mixing time upper bounds have a strictly better dependence on the dimension than when is merely self-concordant. We also provide numerical experiments that demonstrates the practicality of our proposed method. Our method is a high-accuracy sampler due to the polylogarithmic dependence on the error tolerance in our mixing time upper bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18701v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishwak Srinivasan, Andre Wibisono, Ashia Wilson</dc:creator>
    </item>
    <item>
      <title>Towards a Statistical Understanding of Neural Networks: Beyond the Neural Tangent Kernel Theories</title>
      <link>https://arxiv.org/abs/2412.18756</link>
      <description>arXiv:2412.18756v1 Announce Type: cross 
Abstract: A primary advantage of neural networks lies in their feature learning characteristics, which is challenging to theoretically analyze due to the complexity of their training dynamics. We propose a new paradigm for studying feature learning and the resulting benefits in generalizability. After reviewing the neural tangent kernel (NTK) theory and recent results in kernel regression, which address the generalization issue of sufficiently wide neural networks, we examine limitations and implications of the fixed kernel theory (as the NTK theory) and review recent theoretical advancements in feature learning. Moving beyond the fixed kernel/feature theory, we consider neural networks as adaptive feature models. Finally, we propose an over-parameterized Gaussian sequence model as a prototype model to study the feature learning characteristics of neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18756v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haobo Zhang, Jianfa Lai, Yicheng Li, Qian Lin, Jun S. Liu</dc:creator>
    </item>
    <item>
      <title>Network double autoregression</title>
      <link>https://arxiv.org/abs/2412.19251</link>
      <description>arXiv:2412.19251v1 Announce Type: cross 
Abstract: Modeling high-dimensional time series with simple structures is a challenging problem. This paper proposes a network double autoregression (NDAR) model, which combines the advantages of network structure and the double autoregression (DAR) model, to handle high-dimensional, conditionally heteroscedastic, and network-structured data within a simple framework. The parameters of the model are estimated using quasi-maximum likelihood estimation, and the asymptotic properties of the estimators are derived. The selection of the model's lag order will be based on the Bayesian information criterion. Finite-sample simulations show that the proposed model performs well even with moderate time dimensions and network sizes. Finally, the model is applied to analyze three different categories of stock data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19251v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tingting Li, Hao Wang</dc:creator>
    </item>
    <item>
      <title>Asymptotic Properties of the Maximum Likelihood Estimator for Markov-switching Observation-driven Models</title>
      <link>https://arxiv.org/abs/2412.19555</link>
      <description>arXiv:2412.19555v1 Announce Type: cross 
Abstract: A Markov-switching observation-driven model is a stochastic process $((S_t,Y_t))_{t \in \mathbb{Z}}$ where (i) $(S_t)_{t \in \mathbb{Z}}$ is an unobserved Markov process taking values in a finite set and (ii) $(Y_t)_{t \in \mathbb{Z}}$ is an observed process such that the conditional distribution of $Y_t$ given all past $Y$'s and the current and all past $S$'s depends only on all past $Y$'s and $S_t$. In this paper, we prove the consistency and asymptotic normality of the maximum likelihood estimator for such model. As a special case hereof, we give conditions under which the maximum likelihood estimator for the widely applied Markov-switching generalised autoregressive conditional heteroscedasticity model introduced by Haas et al. (2004b) is consistent and asymptotic normal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19555v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederik Krabbe</dc:creator>
    </item>
    <item>
      <title>Generalized Grade-of-Membership Estimation for High-dimensional Locally Dependent Data</title>
      <link>https://arxiv.org/abs/2412.19796</link>
      <description>arXiv:2412.19796v1 Announce Type: cross 
Abstract: This work focuses on the mixed membership models for multivariate categorical data widely used for analyzing survey responses and population genetics data. These grade of membership (GoM) models offer rich modeling power but present significant estimation challenges for high-dimensional polytomous data. Popular existing approaches, such as Bayesian MCMC inference, are not scalable and lack theoretical guarantees in high-dimensional settings. To address this, we first observe that data from this model can be reformulated as a three-way (quasi-)tensor, with many subjects responding to many items with varying numbers of categories. We introduce a novel and simple approach that flattens the three-way quasi-tensor into a "fat" matrix, and then perform a singular value decomposition of it to estimate parameters by exploiting the singular subspace geometry. Our fast spectral method can accommodate a broad range of data distributions with arbitrarily locally dependent noise, which we formalize as the generalized-GoM models. We establish finite-sample entrywise error bounds for the generalized-GoM model parameters. This is supported by a new sharp two-to-infinity singular subspace perturbation theory for locally dependent and flexibly distributed noise, a contribution of independent interest. Simulations and applications to data in political surveys, population genetics, and single-cell sequencing demonstrate our method's superior performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19796v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ling Chen, Chengzhu Huang, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>LASER: A new method for locally adaptive nonparametric regression</title>
      <link>https://arxiv.org/abs/2412.19802</link>
      <description>arXiv:2412.19802v1 Announce Type: cross 
Abstract: In this article, we introduce \textsf{LASER} (Locally Adaptive Smoothing Estimator for Regression), a computationally efficient locally adaptive nonparametric regression method that performs variable bandwidth local polynomial regression. We prove that it adapts (near-)optimally to the local H\"{o}lder exponent of the underlying regression function \texttt{simultaneously} at all points in its domain. Furthermore, we show that there is a single ideal choice of a global tuning parameter under which the above mentioned local adaptivity holds. Despite the vast literature on nonparametric regression, instances of practicable methods with provable guarantees of such a strong notion of local adaptivity are rare. The proposed method achieves excellent performance across a broad range of numerical experiments in comparison to popular alternative locally adaptive methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19802v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sabyasachi Chatterjee, Subhajit Goswami, Soumendu Sundar Mukherjee</dc:creator>
    </item>
    <item>
      <title>Fast estimation of Kendall's Tau and conditional Kendall's Tau matrices under structural assumptions</title>
      <link>https://arxiv.org/abs/2204.03285</link>
      <description>arXiv:2204.03285v2 Announce Type: replace 
Abstract: Kendall's tau and conditional Kendall's tau matrices are multivariate (conditional) dependence measures between the components of a random vector. For large dimensions, available estimators are computationally expensive and can be improved by averaging. Under structural assumptions on the underlying Kendall's tau and conditional Kendall's tau matrices, we introduce new estimators that have a significantly reduced computational cost while keeping a similar error level. In the unconditional setting we assume that, up to reordering, the underlying Kendall's tau matrix is block-structured with constant values in each of the off-diagonal blocks. Consequences on the underlying correlation matrix are then discussed. The estimators take advantage of this block structure by averaging over (part of) the pairwise estimates in each of the off-diagonal blocks. Derived explicit variance expressions show their improved efficiency. In the conditional setting, the conditional Kendall's tau matrix is assumed to have a constant block structure, independently of the conditioning variable. Conditional Kendall's tau matrix estimators are constructed similarly as in the unconditional case by averaging over (part of) the pairwise conditional Kendall's tau estimators. We establish their joint asymptotic normality, and show that the asymptotic variance is reduced compared to the naive estimators. Then, we perform a simulation study which displays the improved performance of both the unconditional and conditional estimators. Finally, the estimators are used for estimating the value at risk of a large stock portfolio; backtesting illustrates the obtained improvements compared to the previous estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.03285v2</guid>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rutger van der Spek, Alexis Derumigny</dc:creator>
    </item>
    <item>
      <title>On estimation of skewed stable linear regression</title>
      <link>https://arxiv.org/abs/2404.10448</link>
      <description>arXiv:2404.10448v2 Announce Type: replace 
Abstract: We study the parameter estimation method for linear regression models with possibly skewed stable distributed errors. Our estimation procedure consists of two stages: first, for the regression coefficients, the Cauchy quasi-maximum likelihood estimator (CQMLE) is considered after taking the differences to remove the skewness of noise, and we prove its asymptotic normality and tail-probability estimate; second, as for stable-distribution parameters, we consider the moment estimators based on the symmetrized and centered residuals and prove their $\sqrt{n}$-consistency. To derive the $\sqrt{n}$-consistency, we essentially used the tail-probability estimate of the CQMLE. The proposed estimation procedure has a very low computational load and is much less time-consuming compared with the maximum-likelihood estimator. Further, our estimator can be effectively used as an initial value of the numerical optimization of the log-likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10448v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eitaro Kawamo, Hiroki Masuda</dc:creator>
    </item>
    <item>
      <title>Geometric quantile-based measures of multivariate distributional characteristics</title>
      <link>https://arxiv.org/abs/2407.07297</link>
      <description>arXiv:2407.07297v2 Announce Type: replace 
Abstract: Several new geometric quantile-based measures for multivariate dispersion, skewness, kurtosis, and spherical asymmetry are defined. These measures differ from existing measures, which use volumes and are easy to calculate. Some theoretical justification is given, followed by experiments illustrating that they are reasonable measures of these distributional characteristics and computing confidence regions with the desired coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07297v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ha-Young Shin, Hee-Seok Oh</dc:creator>
    </item>
    <item>
      <title>Decentralized Sparse Linear Regression via Gradient-Tracking: Linear Convergence and Statistical Guarantees</title>
      <link>https://arxiv.org/abs/2201.08507</link>
      <description>arXiv:2201.08507v2 Announce Type: replace-cross 
Abstract: We study sparse linear regression over a network of agents, modeled as an undirected graph and no server node. The estimation of the $s$-sparse parameter is formulated as a constrained LASSO problem wherein each agent owns a subset of the $N$ total observations. We analyze the convergence rate and statistical guarantees of a distributed projected gradient tracking-based algorithm under high-dimensional scaling, allowing the ambient dimension $d$ to grow with (and possibly exceed) the sample size $N$. Our theory shows that, under standard notions of restricted strong convexity and smoothness of the loss functions, suitable conditions on the network connectivity and algorithm tuning, the distributed algorithm converges globally at a {\it linear} rate to an estimate that is within the centralized {\it statistical precision} of the model, $O(s\log d/N)$. When $s\log d/N=o(1)$, a condition necessary for statistical consistency, an $\varepsilon$-optimal solution is attained after $\mathcal{O}(\kappa \log (1/\varepsilon))$ gradient computations and $O (\kappa/(1-\rho) \log (1/\varepsilon))$ communication rounds, where $\kappa$ is the restricted condition number of the loss function and $\rho$ measures the network connectivity. The computation cost matches that of the centralized projected gradient algorithm despite having data distributed; whereas the communication rounds reduce as the network connectivity improves. Overall, our study reveals interesting connections between statistical efficiency, network connectivity \&amp; topology, and convergence rate in high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.08507v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marie Maros, Gesualdo Scutari, Ying Sun, Guang Cheng</dc:creator>
    </item>
    <item>
      <title>Identification enhanced generalised linear model estimation with nonignorable missing outcomes</title>
      <link>https://arxiv.org/abs/2204.10508</link>
      <description>arXiv:2204.10508v4 Announce Type: replace-cross 
Abstract: Missing data often result in undesirable bias and loss of efficiency. These issues become substantial when the response mechanism is nonignorable, meaning that the response model depends on unobserved variables. To manage nonignorable nonresponse, it is necessary to estimate the joint distribution of unobserved variables and response indicators. However, model misspecification and identification issues can prevent robust estimates, even with careful estimation of the target joint distribution. In this study, we modeled the distribution of the observed parts and derived sufficient conditions for model identifiability, assuming a logistic regression model as the response mechanism and generalized linear models as the main outcome model of interest. More importantly, the derived sufficient conditions do not require any instrumental variables, which are often assumed to guarantee model identifiability but cannot be practically determined beforehand. To analyze missing data in applications, we propose practical guidelines and sensitivity analysis to determine the response mechanism. Furthermore, we present the performance of the proposed estimators in numerical studies and apply the proposed method to two sets of real data: exit polls from the 19th South Korean election and public data collected from the Korean Survey of Household Finances and Living Conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10508v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenji Beppu, Jinung Choi, Kosuke Morikawa, Jongho Im</dc:creator>
    </item>
    <item>
      <title>Semiparametric Efficient Fusion of Individual Data and Summary Statistics</title>
      <link>https://arxiv.org/abs/2210.00200</link>
      <description>arXiv:2210.00200v5 Announce Type: replace-cross 
Abstract: Suppose we have individual data from an internal study and various summary statistics from relevant external studies. External summary statistics have the potential to improve statistical inference for the internal population; however, it may lead to efficiency loss or bias if not used properly. We study the fusion of individual data and summary statistics in a semiparametric framework to investigate the efficient use of external summary statistics. Under a weak transportability assumption, we establish the semiparametric efficiency bound for estimating a general functional of the internal data distribution, which is no larger than that using only internal data and underpins the potential efficiency gain of integrating individual data and summary statistics. We propose a data-fused efficient estimator that achieves this efficiency bound. In addition, an adaptive fusion estimator is proposed to eliminate the bias of the original data-fused estimator when the transportability assumption fails. We establish the asymptotic oracle property of the adaptive fusion estimator. Simulations and application to a Helicobacter pylori infection dataset demonstrate the promising numerical performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.00200v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenjie Hu, Ruoyu Wang, Wei Li, Wang Miao</dc:creator>
    </item>
    <item>
      <title>Robust Point Matching with Distance Profiles</title>
      <link>https://arxiv.org/abs/2312.12641</link>
      <description>arXiv:2312.12641v4 Announce Type: replace-cross 
Abstract: We show the outlier robustness and noise stability of practical matching procedures based on distance profiles. Although the idea of matching points based on invariants like distance profiles has a long history in the literature, there has been little understanding of the theoretical properties of such procedures, especially in the presence of outliers and noise. We provide a theoretical analysis showing that under certain probabilistic settings, the proposed matching procedure is successful with high probability even in the presence of outliers and noise. We demonstrate the performance of the proposed method using a real data example and provide simulation studies to complement the theoretical findings. Lastly, we extend the concept of distance profiles to the abstract setting and connect the proposed matching procedure to the Gromov-Wasserstein distance and its lower bound, with a new sample complexity result derived based on the properties of distance profiles. This paper contributes to the literature by providing theoretical underpinnings of the matching procedures based on invariants like distance profiles, which have been widely used in practice but have rarely been analyzed theoretically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12641v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>YoonHaeng Hur, Yuehaw Khoo</dc:creator>
    </item>
    <item>
      <title>Two-stage Quantum Estimation and the Asymptotics of Quantum-enhanced Transmittance Sensing</title>
      <link>https://arxiv.org/abs/2402.17922</link>
      <description>arXiv:2402.17922v2 Announce Type: replace-cross 
Abstract: We consider estimation of a single unknown parameter embedded in a quantum state. Quantum Cram\'er-Rao bound (QCRB) is the ultimate limit of the mean squared error for any unbiased estimator. While it can be achieved asymptotically for a large number of quantum state copies, the measurement required often depends on the true value of the parameter of interest. Prior work addresses this paradox using a two-stage approach: in the first stage, a preliminary estimate is obtained by applying, on a vanishing fraction of quantum state copies, a sub-optimal measurement that does not depend on the parameter of interest. In the second stage, the preliminary estimate is used to construct the QCRB-achieving measurement that is applied to the remaining quantum state copies. This is akin to two-step estimators for classical problems with nuisance parameters. Unfortunately, the original analysis imposes conditions that severely restrict the class of classical estimators applied to the quantum measurement outcomes, hindering applications of this method. We relax these conditions to substantially broaden the class of usable estimators for single-parameter problems at the cost of slightly weakening the asymptotic properties of the two-stage method. We also account for nuisance parameters. We apply our results to obtain the asymptotics of quantum-enhanced transmittance sensing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17922v2</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Gong, Boulat A. Bash</dc:creator>
    </item>
    <item>
      <title>Model checking for high dimensional generalized linear models based on random projections</title>
      <link>https://arxiv.org/abs/2412.10721</link>
      <description>arXiv:2412.10721v2 Announce Type: replace-cross 
Abstract: Most existing tests in the literature for model checking do not work in high dimension settings due to challenges arising from the "curse of dimensionality", or dependencies on the normality of parameter estimators. To address these challenges, we proposed a new goodness of fit test based on random projections for generalized linear models, when the dimension of covariates may substantially exceed the sample size. The tests only require the convergence rate of parameter estimators to derive the limiting distribution. The growing rate of the dimension is allowed to be of exponential order in relation to the sample size. As random projection converts covariates to one-dimensional space, our tests can detect the local alternative departing from the null at the rate of $n^{-1/2}h^{-1/4}$ where $h$ is the bandwidth, and $n$ is the sample size. This sensitive rate is not related to the dimension of covariates, and thus the "curse of dimensionality" for our tests would be largely alleviated. An interesting and unexpected result is that for randomly chosen projections, the resulting test statistics can be asymptotic independent. We then proposed combination methods to enhance the power performance of the tests. Detailed simulation studies and a real data analysis are conducted to illustrate the effectiveness of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10721v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wen Chen, Jie Liu, Heng Peng, Falong Tan, Lixing Zhu</dc:creator>
    </item>
    <item>
      <title>Growth-Optimal E-Variables and an extension to the multivariate Csisz\'ar-Sanov-Chernoff Theorem</title>
      <link>https://arxiv.org/abs/2412.17554</link>
      <description>arXiv:2412.17554v2 Announce Type: replace-cross 
Abstract: We consider growth-optimal e-variables with maximal e-power, both in an absolute and relative sense, for simple null hypotheses for a $d$-dimensional random vector, and multivariate composite alternatives represented as a set of $d$-dimensional means $\meanspace_1$. These include, among others, the set of all distributions with mean in $\meanspace_1$, and the exponential family generated by the null restricted to means in $\meanspace_1$. We show how these optimal e-variables are related to Csisz\'ar-Sanov-Chernoff bounds, first for the case that $\meanspace_1$ is convex (these results are not new; we merely reformulate them) and then for the case that $\meanspace_1$ `surrounds' the null hypothesis (these results are new).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17554v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Gr\"unwald, Yunda Hao, Akshay Balsubramani</dc:creator>
    </item>
  </channel>
</rss>
