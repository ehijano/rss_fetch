<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 04:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Random Sampling over Spatial Range Joins</title>
      <link>https://arxiv.org/abs/2508.15070</link>
      <description>arXiv:2508.15070v1 Announce Type: new 
Abstract: Spatial range joins have many applications, including geographic information systems, location-based social networking services, neuroscience, and visualization. However, joins incur not only expensive computational costs but also too large result sets. A practical and reasonable approach to alleviating these issues is to return random samples of the join results. Although this is promising and sufficient for many applications involving spatial range joins, efficiently computing random samples is not trivial. This is because we must obtain random join samples without running spatial range joins. We address this challenging problem for the first time and aim at designing a time- and space-efficient algorithm. First, we design two baseline algorithms that employ existing techniques for random sampling and show that they are not efficient. Then, we propose a new data structure that can deal with our problem in $\tilde{O}(n + m + t)$ expected time and $O(n+m)$ space, where $n$ and $m$ are the sizes of two point sets and $t$ is the required number of samples. We conduct extensive experiments using four real spatial datasets, and the results demonstrate that our algorithm is significantly faster than the baselines in most tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15070v1</guid>
      <category>cs.DB</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daichi Amagata</dc:creator>
    </item>
    <item>
      <title>Temporal $k$-Core Query, Revisited</title>
      <link>https://arxiv.org/abs/2508.15238</link>
      <description>arXiv:2508.15238v1 Announce Type: new 
Abstract: Querying cohesive subgraphs in temporal graphs is essential for understanding the dynamic structure of real-world networks, such as evolving communities in social platforms, shifting hyperlink structures on the Web, and transient communication patterns in call networks. Recently, research has focused on the temporal $k$-core query, which aims to identify all $k$-cores across all possible time sub-intervals within a given query interval. The state-of-the-art algorithm OTCD mitigates redundant computations over overlapping sub-intervals by exploiting inclusion relationships among $k$-cores in different time intervals. Nevertheless, OTCD remains limited in scalability due to the combinatorial growth in interval enumeration and repeated processing. In this paper, we revisit the temporal $k$-core query problem and introduce a novel algorithm CoreT, which dynamically records the earliest timestamp at which each vertex or edge enters a $k$-core. This strategy enables substantial pruning of redundant computations. As a result, CoreT requires only a single pass over the query interval and achieves improved time complexity, which is linear in both the number of temporal edges within the query interval and the duration of the interval, making it highly scalable for long-term temporal analysis. Experimental results on large real-world datasets show that CoreT achieves up to four orders of magnitude speedup compared to the existing state-of-the-art OTCD, demonstrating its effectiveness and scalability for temporal $k$-core analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15238v1</guid>
      <category>cs.DB</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinyu Liu, Kaiqiang Yu, Shengxin Liu, Cheng Long, Zhaoquan Gu</dc:creator>
    </item>
    <item>
      <title>AmbiSQL: Interactive Ambiguity Detection and Resolution for Text-to-SQL</title>
      <link>https://arxiv.org/abs/2508.15276</link>
      <description>arXiv:2508.15276v1 Announce Type: new 
Abstract: Text-to-SQL systems translate natural language questions into SQL queries, providing substantial value for non-expert users. While large language models (LLMs) show promising results for this task, they remain error-prone. Query ambiguity has been recognized as a major obstacle for LLM-based Text-to-SQL systems, leading to misinterpretation of user intent and inaccurate SQL generation. We demonstrate AmbiSQL, an interactive system that automatically detects query ambiguities and guides users through intuitive multiple-choice questions to clarify their intent. Our approach introduces a fine-grained ambiguity taxonomy for identifying ambiguities that affect database element mapping and LLM reasoning, then incorporates user feedback to rewrite ambiguous questions. Evaluation on an ambiguous query dataset shows that AmbiSQL achieves 87.2% precision in ambiguity detection and improves SQL exact match accuracy by 50% when integrated with Text-to-SQL systems. Our demonstration showcases the significant performance gains and highlights the system's practical usability. Code repo and demonstration are available at: https://github.com/JustinzjDing/AmbiSQL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15276v1</guid>
      <category>cs.DB</category>
      <category>cs.CL</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhongjun Ding, Yin Lin, Tianjing Zeng</dc:creator>
    </item>
    <item>
      <title>Efficient Cloud-Edge-Device Query Execution Based on Collaborative Scan Operator</title>
      <link>https://arxiv.org/abs/2508.15285</link>
      <description>arXiv:2508.15285v1 Announce Type: new 
Abstract: In cloud-edge-device (CED) collaborative query (CQ) processing, by leveraging CED collaboration, the advantages of both cloud computing and edge resources can be fully integrated. However, it is difficult to implement collaborative operators that can flexibly switch between the cloud and the edge during query execution. Thus, in this paper, we aim to improve the query performance when the edge resources reach a bottleneck. To achieve seamless switching of query execution between the cloud and edge, we propose a CQ processing method by establishing a CED collaborative framework based on the collaborative scan operator, so that query execution can be transferred to the cloud at any time when the edge resources are saturated. Extensive experiments show that, under sufficient network download bandwidth, the CED collaborative scan operator can effectively alleviate the performance degradation of scan operators caused by high I/O load and CPU wait time at the edge. It also achieves balanced resource scheduling between the cloud and edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15285v1</guid>
      <category>cs.DB</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chunyu Zhao, Hongzhi Wang, Kaixin Zhang, Hongliang Li, Yihan Zhang, Jiawei Zhang, Kunkai Gu, Yuan Tian, Xiangdong Huang, Jingyi Xu</dc:creator>
    </item>
    <item>
      <title>Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional Vector Search</title>
      <link>https://arxiv.org/abs/2508.15290</link>
      <description>arXiv:2508.15290v1 Announce Type: new 
Abstract: Similarity-based vector search underpins many important applications, but a key challenge is processing massive vector datasets (e.g., in TBs). To reduce costs, some systems utilize SSDs as the primary data storage. They employ a proximity graph, which connects similar vectors to form a graph and is the state-of-the-art index for vector search. However, these systems are hindered by sub-optimal data layouts that fail to effectively utilize valuable memory space to reduce disk access and suffer from poor locality for accessing disk-resident data. Through extensive profiling and analysis, we found that the structure of the proximity graph index is accessed more frequently than the vectors themselves, yet existing systems do not distinguish between the two. To address this problem, we design the Gorgeous system with the principle of prioritizing graph structure over vectors. Specifically, Gorgeous features a memory cache that keeps the adjacency lists of graph nodes to improve cache hits and a disk block format that explicitly stores neighbors' adjacency lists along with a vector to enhance data locality. Experimental results show that Gorgeous consistently outperforms two state-of-the-art disk-based systems for vector search, boosting average query throughput by over 60% and reducing query latency by over 35%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15290v1</guid>
      <category>cs.DB</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Peiqi Yin, Xiao Yan, Qihui Zhou, Hui Li, Xiaolu Li, Lin Zhang, Meiling Wang, Xin Yao, James Cheng</dc:creator>
    </item>
    <item>
      <title>GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector Nearest Neighbor Search</title>
      <link>https://arxiv.org/abs/2508.15694</link>
      <description>arXiv:2508.15694v1 Announce Type: new 
Abstract: Graph-based high-dimensional vector indices have become a mainstream solution for large-scale approximate nearest neighbor search (ANNS). However, their substantial memory footprint often requires storage on secondary devices, where frequent on-demand loading of graph and vector data leads to I/O becoming the dominant bottleneck, accounting for over 90\% of query latency. Existing static caching strategies mitigate this issue only in the initial navigation phase by preloading entry points and multi-hop neighbors, but they fail in the second phase where query-dependent nodes must be dynamically accessed to achieve high recall. We propose GoVector, an I/O-efficient caching strategy tailored for disk-based graph indices. GoVector combines (1) a static cache that stores entry points and frequently accessed neighbors, and (2) a dynamic cache that adaptively captures nodes with high spatial locality during the second search phase. To further align storage layout with similarity-driven search patterns, GoVector reorders nodes on disk so that similar vectors are colocated on the same or adjacent pages, thereby improving locality and reducing I/O overhead. Extensive experiments on multiple public datasets show that GoVector achieves substantial performance improvements. At 90% recall, it reduces I/O operations by 46% on average, increases query throughput by 1.73x, and lowers query latency by 42% compared to state-of-the-art disk-based graph indexing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15694v1</guid>
      <category>cs.DB</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijie Zhou, Shengyuan Lin, Shufeng Gong, Song Yu, Shuhao Fan, Yanfeng Zhang, Ge Yu</dc:creator>
    </item>
    <item>
      <title>On the Effectiveness of Graph Reordering for Accelerating Approximate Nearest Neighbor Search on GPU</title>
      <link>https://arxiv.org/abs/2508.15436</link>
      <description>arXiv:2508.15436v1 Announce Type: cross 
Abstract: We present the first systematic investigation of graph reordering effects for graph-based Approximate Nearest Neighbor Search (ANNS) on a GPU. While graph-based ANNS has become the dominant paradigm for modern AI applications, recent approaches focus on algorithmic innovations while neglecting memory layout considerations that significantly affect execution time. Our unified evaluation framework enables comprehensive evaluation of diverse reordering strategies across different graph indices through a graph adapter that converts arbitrary graph topologies into a common representation and a GPU-optimized graph traversal engine. We conduct a comprehensive analysis across diverse datasets and state-of-the-art graph indices, introducing analysis metrics that quantify the relationship between structural properties and memory layout effectiveness. Our GPU-targeted reordering achieves up to 15$\%$ QPS improvements while preserving search accuracy, demonstrating that memory layout optimization operates orthogonally to existing algorithmic innovations. We will release all code upon publication to facilitate reproducibility and foster further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15436v1</guid>
      <category>cs.IR</category>
      <category>cs.CV</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutaro Oguri, Mai Nishimura, Yusuke Matsui</dc:creator>
    </item>
    <item>
      <title>Integrated data-driven biotechnology research environments</title>
      <link>https://arxiv.org/abs/2503.21274</link>
      <description>arXiv:2503.21274v2 Announce Type: replace 
Abstract: In the past few decades, the life sciences have experienced an unprecedented accumulation of data, ranging from genomic sequences and proteomic profiles to heavy-content imaging, clinical assays, and commercial biological products for research. Traditional static databases have been invaluable in providing standardized and structured information. However, they fall short when it comes to facilitating exploratory data interrogation, real-time query, multidimensional comparison and dynamic visualization. Integrated data-driven research environments aiming at supporting user-driven data queries and visualization offer promising new avenues for making the best use of the vast and heterogeneous data streams collected in biological research. This perspective article discusses the potential of interactive and integrated frameworks, highlighting the importance of implementing this model in biotechnology research, while going through the state-of-the-art in database design, technical choices behind modern data management systems, and emerging needs in multidisciplinary research. Special attention is given to data interrogation strategies, user interface design, and comparative analysis capabilities, along with challenges such as data standardization and scalability in data-heavy applications. Conceptual features for developing interactive data environments along diverse life science domains are then presented in the user case of cell line selection for in vitro research to bridge the gap between research data generation, actionable biological insight, experimental design, and clinical relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21274v2</guid>
      <category>cs.DB</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rosalia Moreddu</dc:creator>
    </item>
    <item>
      <title>Efficient Adaptable High-throughput Streaming Aggregation Engines</title>
      <link>https://arxiv.org/abs/2405.18168</link>
      <description>arXiv:2405.18168v3 Announce Type: replace-cross 
Abstract: Aggregation queries are a series of computationally-demanding analytics operations on counted, grouped or time series data. They include tasks such as summation or finding the median among the items of the same group, and within a specified number of the last observed tuples for sliding window aggregation (SWAG). They have a wide range of applications including database analytics, operating systems, bank security and medical sensors. Existing challenges include the hardware complexity that comes with efficiently handling per-group states using hash-based approaches. This paper presents an adaptable pipeline for calculating a wide range of aggregation queries with high throughput. It is then adapted for SWAG and achieves up to 476x speedup over the CPU core of the same platform. It achieves unparalleled levels of performance and functionality such as a throughput of 1 GT/s on our setup for SWAG without groups, and more advanced operators with up to 4x the window sizes than the state-of-the-art with groups as an approximation for SWAG featuring per-group windows using a fraction of the resources and no DRAM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18168v3</guid>
      <category>cs.AR</category>
      <category>cs.DB</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippos Papaphilippou, Wayne Luk</dc:creator>
    </item>
    <item>
      <title>Advancing the Database of Cross-Linguistic Colexifications with New Workflows and Data</title>
      <link>https://arxiv.org/abs/2503.11377</link>
      <description>arXiv:2503.11377v2 Announce Type: replace-cross 
Abstract: Lexical resources are crucial for cross-linguistic analysis and can provide new insights into computational models for natural language learning. Here, we present an advanced database for comparative studies of words with multiple meanings, a phenomenon known as colexification. The new version includes improvements in the handling, selection and presentation of the data. We compare the new database with previous versions and find that our improvements provide a more balanced sample covering more language families worldwide, with enhanced data quality, given that all word forms are provided in phonetic transcription. We conclude that the new Database of Cross-Linguistic Colexifications has the potential to inspire exciting new studies that link cross-linguistic data to open questions in linguistic typology, historical linguistics, psycholinguistics, and computational linguistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11377v2</guid>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annika Tjuka, Robert Forkel, Christoph Rzymski, Johann-Mattis List</dc:creator>
    </item>
  </channel>
</rss>
