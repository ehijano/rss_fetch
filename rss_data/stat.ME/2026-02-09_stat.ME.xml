<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Feb 2026 05:01:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Compound Logistic Regression Model for Binary Responses</title>
      <link>https://arxiv.org/abs/2602.06153</link>
      <description>arXiv:2602.06153v1 Announce Type: new 
Abstract: Logistic regression is the most commonly used method for constructing predictive models for binary responses. One significant drawback to this approach, however, is that the asymptotes of the logistic response function are fixed at 0 and 1, and there are many applications for which this constraint is inappropriate. More flexible models have been proposed for this application, most proceeding by supplementing the logistic response function with additional parameters. In this article we extend these models to allow correlated responses and the inclusion of covariates. This is achieved through the \emph{compound logistic regression model}, for which the mean response is a function of several logistic regression functions. This permits a greater variety of models, while retaining the advantages of logistic regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06153v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Almudevar, Jacob Almudevar</dc:creator>
    </item>
    <item>
      <title>Latent variation in pathogen strain-specific effects under multiple-versions-of-treatment theory</title>
      <link>https://arxiv.org/abs/2602.06262</link>
      <description>arXiv:2602.06262v1 Announce Type: new 
Abstract: Evidence-informed policy on infections requires estimates of their effects on health. However, pathogenic variation, whereby occurrence of adverse outcomes depends on the infecting strain, might complicate the study of many infectious agents. Here, we consider the interpretation of epidemiologic studies on effects of infections on health when there is heterogeneity in strain-specific effects and information on strain composition is unavailable. We use potential outcomes and causal inference theory for analyses in the presence of multiple versions of treatment to argue that oft-reported quantities in these studies have a causal interpretation that depends on population frequencies of infecting strains. Moreover, as in other contexts where the treatment-variation-irrelevance assumption might be violated, transportability requires additional considerations, beyond those needed for non-compound exposures. This discussion, that considers potential heterogeneity in strain-specific effects, will facilitate interpretation of these studies, and for the reasons mentioned above, also highlights the value of pathogen subtype data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06262v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bronner P. Gon\c{c}alves</dc:creator>
    </item>
    <item>
      <title>Conformal changepoint localization</title>
      <link>https://arxiv.org/abs/2602.06267</link>
      <description>arXiv:2602.06267v1 Announce Type: new 
Abstract: We study the problem of offline changepoint localization in a distribution-free setting. One observes a vector of data with a single changepoint, assuming that the data before and after the changepoint are iid (or more generally exchangeable) from arbitrary and unknown distributions. The goal is to produce a finite-sample confidence set for the index at which the change occurs without making any other assumptions. Existing methods often rely on parametric assumptions, tail conditions, or asymptotic approximations, or only produce point estimates. In contrast, our distribution-free algorithm, CONformal CHangepoint localization (CONCH), only leverages exchangeability arguments to construct confidence sets with finite sample coverage. By proving a conformal Neyman-Pearson lemma, we derive principled score functions that yield informative (small) sets. Moreover, with such score functions, the normalized length of the confidence set shrinks to zero under weak assumptions. We also establish a universality result showing that any distribution-free changepoint localization method must be an instance of CONCH. Experiments suggest that CONCH delivers precise confidence sets even in challenging settings involving images or text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06267v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohan Hore, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Design-Conditional Prior Elicitation for Dirichlet Process Mixtures: A Unified Framework for Cluster Counts and Weight Control</title>
      <link>https://arxiv.org/abs/2602.06301</link>
      <description>arXiv:2602.06301v1 Announce Type: new 
Abstract: Dirichlet process mixture (DPM) models are widely used for semiparametric Bayesian analysis in educational and behavioral research, yet specifying the concentration parameter remains a critical barrier. Default hyperpriors often impose strong, unintended assumptions about clustering, while existing calibration methods based on cluster counts suffer from computational inefficiency and fail to control the distribution of mixture weights. This article introduces Design-Conditional Elicitation (DCE), a unified framework that translates practitioner beliefs about cluster structure into coherent Gamma hyperpriors for a fixed design size J. DCE makes three contributions. First, it solves the computational bottleneck using Two-Stage Moment Matching (TSMM), which couples a closed-form approximation with an exact Newton refinement to calibrate hyperparameters without grid search. Second, addressing the "unintended prior" phenomenon, DCE incorporates a Dual-Anchor protocol to diagnose and optionally constrain the risk of weight dominance while transparently reporting the resulting trade-off against cluster-count fidelity. Third, the complete workflow is implemented in the open-source DPprior R package with reproducible diagnostics and a reporting checklist. Simulation studies demonstrate that common defaults such as Gamma(1, 1) induce posterior collapse rates exceeding 60% regardless of the true cluster structure, while DCE-calibrated priors substantially reduce bias and improve recovery across varying levels of data informativeness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06301v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>JoonHo Lee</dc:creator>
    </item>
    <item>
      <title>E-values for Adaptive Clinical Trials: Anytime-Valid Monitoring in Practice</title>
      <link>https://arxiv.org/abs/2602.06379</link>
      <description>arXiv:2602.06379v1 Announce Type: new 
Abstract: Adaptive clinical trials rely on interim analyses, flexible stopping, and data-dependent design modifications that complicate statistical guarantees when fixed-horizon test statistics are repeatedly inspected or reused after adaptations. E-values and e-processes provide anytime-valid tests and confidence sequences that remain valid under optional stopping and optional continuation without requiring a prespecified monitoring schedule.
  This paper is a methodology guide for practitioners. We develop the betting-martingale construction of e-processes for two-arm randomized controlled trials, show how e-values naturally handle composite null hypotheses and support futility monitoring, and provide guidance on when e-values are appropriate, when established alternatives are preferable, and how to integrate e-value monitoring with group sequential and Bayesian adaptive workflows.
  A numerical study compares five monitoring rules -- naive and calibrated versions of frequentist, Bayesian, and e-value approaches -- in a two-arm binary-endpoint trial. Naive repeated testing and naive posterior thresholds inflate Type I error substantially under frequent interim looks. Among the valid methods, the calibrated group sequential rule achieves the highest power, the e-value rule provides robust anytime-valid control with moderate power, and the calibrated Bayesian rule is the most conservative.
  Extended simulations show that the power gap between group sequential and e-value methods depends on the monitoring schedule and reverses under continuous monitoring. The methodology, including futility monitoring, platform trial multiplicity control, and hybrid strategies combining e-values with established methods, is implemented in the open-source R package `evalinger` and situated within the regulatory framework of the January 2026 FDA draft guidance on Bayesian methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06379v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandra Sokolova, Vadim Sokolov</dc:creator>
    </item>
    <item>
      <title>Social Interactions Models with Latent Structures</title>
      <link>https://arxiv.org/abs/2602.06435</link>
      <description>arXiv:2602.06435v1 Announce Type: new 
Abstract: This paper studies estimation and inference of heterogeneous peer effects featuring group fixed effects and slope heterogeneity under latent structure. We adapt the Classifier-Lasso algorithm to consistently discover latent structures and determine the number of clusters. To solve the incidental parameter problem in the binary choice model with social interactions, we propose a parametric bootstrap method to debias and establish its asymptotic validity. Monte Carlo simulations confirm strong finite sample performance of our methods. In an application to students' risky behaviors, the algorithm detects two latent clusters and finds that peer effects are significant within one of the clusters, demonstrating the practical applicability in uncovering heterogeneous social interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06435v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhongjian Lin, Zhentao Shi, Yapeng Zheng</dc:creator>
    </item>
    <item>
      <title>On Stein's Method of Moments and Generalized Score Matching</title>
      <link>https://arxiv.org/abs/2602.06482</link>
      <description>arXiv:2602.06482v1 Announce Type: new 
Abstract: We show that a special case of method of moment estimator derived from the Stein class coincides with the class of generalized score matching estimator. Choosing a suitable weight function for generalized score matching is not straightforward. However, by placing it within the method of moment framework we can alleviate this problem by extending the Stein class to generalized method of moments. As a consequence we can work with a number of functions and hence derive generalized score matching estimators with optimal properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06482v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alfred Kume, Stephen G. Walker</dc:creator>
    </item>
    <item>
      <title>Efficient Online Variational Estimation via Monte Carlo Sampling</title>
      <link>https://arxiv.org/abs/2602.06579</link>
      <description>arXiv:2602.06579v1 Announce Type: new 
Abstract: This article addresses online variational estimation in parametric state-space models. We propose a new procedure for efficiently computing the evidence lower bound and its gradient in a streaming-data setting, where observations arrive sequentially. The algorithm allows for the simultaneous training of the model parameters and the distribution of the latent states given the observations. It is based on i.i.d. Monte Carlo sampling, coupled with a well-chosen deep architecture, enabling both computational efficiency and flexibility. The performance of the method is illustrated on both synthetic data and real-world air-quality data. The proposed approach is theoretically motivated by the existence of an asymptotic contrast function and the ergodicity of the underlying Markov chain, and applies more generally to the computation of additive expectations under posterior distributions in state-space models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06579v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathis Chagneux (KTH STOCKHOLM), Mathias M\"uller (KTH STOCKHOLM), Pierre Gloaguen (UBS), Sylvain Le Corff (LPSM), Jimmy Olsson (KTH STOCKHOLM)</dc:creator>
    </item>
    <item>
      <title>A prediction interval for the population-wise error rate</title>
      <link>https://arxiv.org/abs/2602.06828</link>
      <description>arXiv:2602.06828v1 Announce Type: new 
Abstract: We construct an asymptotic prediction interval for the population-wise error rate (PWER), which is a multiple type I error criterion for clinical trials with overlapping patient populations. The PWER is the probability that a randomly selected patient will receive an ineffective treatment. It must usually be estimated due to unknown population strata sizes, such that only an estimate can be controlled at the given significance level. We apply the delta method to find a prediction interval for the resulting true PWER, we demonstrate by simulations that the interval has the required coverage probability, and illustrate the approach with real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06828v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Remi Luschei, Werner Brannath</dc:creator>
    </item>
    <item>
      <title>Assessment of evidence against homogeneity in exhaustive subgroup treatment effect plots</title>
      <link>https://arxiv.org/abs/2602.06910</link>
      <description>arXiv:2602.06910v1 Announce Type: new 
Abstract: Exhaustive subgroup treatment effect plots are constructed by displaying all subgroup treatment effects of interest against subgroup sample size, providing a useful overview of the observed treatment effect heterogeneity in a clinical trial. As in any exploratory subgroup analysis, however, the observed estimates suffer from small sample sizes and multiplicity issues. To facilitate more interpretable exploratory assessments, this paper introduces a computationally efficient method to generate homogeneity regions within exhaustive subgroup treatment effect plots. Using the Doubly Robust (DR) learner, pseudo-outcomes are used to estimate subgroup effects and derive reference distributions, quantifying how surprising observed heterogeneity is under a homogeneous effects model. Explicit formulas are derived for the homogeneity region and different methods for calculation of the critical values are compared. The method is illustrated with a cardiovascular trial and evaluated via simulation, showing well-calibrated inference and improved performance over standard approaches using simple differences of observed group means.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06910v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bj\"orn Bornkamp, Jiarui Lu, Frank Bretz</dc:creator>
    </item>
    <item>
      <title>Modeling the Hazard Function with Non-linear Systems in Dynamical Survival Analysis</title>
      <link>https://arxiv.org/abs/2602.06322</link>
      <description>arXiv:2602.06322v1 Announce Type: cross 
Abstract: Hazard functions play a central role in survival analysis, offering insight into the underlying risk dynamics of time to event data, with broad applications in medicine, epidemiology, and related fields. First order ordinary differential equation (ODE) formulations of the hazard function have been explored as extensions beyond classical parametric models. However, such approaches typically produce monotonic hazard patterns, limiting their ability to represent oscillatory behavior, nonlinear damping, or coupled growth decay dynamics. We propose a new statistical framework for modeling and simulating hazard functions governed by higher-order ODEs, allowing risk to depend on both its current level, its rate of change, and time. This class of models captures complex time dependent risk behaviors relevant to survival analysis and reliability studies. We develop a simulation procedure by reformulating the higher order ODE as a system of nonlinear first order equations solved numerically, with failure times generated via cumulative hazard inversion. Likelihood based inference under right censoring is also developed, and moment generating function analysis is used to characterize tail behavior. The proposed framework is evaluated through simulation studies and illustrated using real world survival data, where oscillatory hazard dynamics capture temporal risk patterns beyond standard monotone models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06322v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dananjani Liyanage, Mahmudul Bari Hridoy, Fahad Mostafa</dc:creator>
    </item>
    <item>
      <title>Joint survival annuity derivative valuation in the linear-rational Wishart mortality model</title>
      <link>https://arxiv.org/abs/2602.06415</link>
      <description>arXiv:2602.06415v1 Announce Type: cross 
Abstract: This study proposes a linear-rational joint survival mortality model based on the Wishart process. The Wishart process, which is a stochastic continuous matrix affine process, allows for a general dependency between the mortality intensities that are constructed to be positive. Using the linear-rational framework along with the Wishart process as state variable, we derive a closed-form expression for the joint survival annuity, as well as the guaranteed joint survival annuity option. Exploiting our parameterisation of the Wishart process, we explicit the distribution of the mortality intensities and their dependency. We provide the distribution (density and cumulative distribution) of the joint survival annuity. We also develop some polynomial expansions for the underlying state variable that lead to fast and accurate approximations for the guaranteed joint survival annuity option. These polynomial expansions also significantly simplify the implementation of the model. Overall, the linear-rational Wishart mortality model provides a flexible and unified framework for modelling and managing joint mortality risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06415v1</guid>
      <category>q-fin.MF</category>
      <category>q-fin.PR</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jose Da Fonseca, Patrick Wong</dc:creator>
    </item>
    <item>
      <title>Inferring Microscopic Explanatory Structures from Observational Constraints via Large Deviations</title>
      <link>https://arxiv.org/abs/2602.06458</link>
      <description>arXiv:2602.06458v1 Announce Type: cross 
Abstract: We study how macroscopic observational constraints restrict admissible microscopic explanatory structures when no intrinsic order or dynamics is assumed a priori. Starting from an unordered collection of measurement outcomes, we formulate inference as a constrained large deviation problem, selecting probability assignments that minimize relative entropy with respect to a reference measure determined solely by the measurement setup. We show that among all microscopic structures compatible with a given macroscopic constraint, those rendering the observation statistically most typical are selected. As an explicit illustration, we demonstrate how ordered microscopic structures can emerge purely from inference under constraint, even when the reference measure is fully permutation symmetric. Order is thus not assumed but inferred, serving here only as an illustrative example of a broader class of relational explanatory hypotheses constrained by observation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06458v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akihisa Ichiki</dc:creator>
    </item>
    <item>
      <title>Sequential Auditing for f-Differential Privacy</title>
      <link>https://arxiv.org/abs/2602.06518</link>
      <description>arXiv:2602.06518v1 Announce Type: cross 
Abstract: We present new auditors to assess Differential Privacy (DP) of an algorithm based on output samples. Such empirical auditors are common to check for algorithmic correctness and implementation bugs. Most existing auditors are batch-based or targeted toward the traditional notion of $(\varepsilon,\delta)$-DP; typically both. In this work, we shift the focus to the highly expressive privacy concept of $f$-DP, in which the entire privacy behavior is captured by a single tradeoff curve. Our auditors detect violations across the full privacy spectrum with statistical significance guarantees, which are supported by theory and simulations. Most importantly, and in contrast to prior work, our auditors do not require a user-specified sample size as an input. Rather, they adaptively determine a near-optimal number of samples needed to reach a decision, thereby avoiding the excessively large sample sizes common in many auditing studies. This reduction in sampling cost becomes especially beneficial for expensive training procedures such as DP-SGD. Our method supports both whitebox and blackbox settings and can also be executed in single-run frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06518v1</guid>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Kutta, Martin Dunsche, Yu Wei, Vassilis Zikas</dc:creator>
    </item>
    <item>
      <title>Prediction-based inference for integrated diffusions with high-frequency data</title>
      <link>https://arxiv.org/abs/2602.06764</link>
      <description>arXiv:2602.06764v1 Announce Type: cross 
Abstract: We consider parametric inference for an ergodic and stationary diffusion process, when the data are high-frequency observations of the integral of the diffusion process. Such data are obtained via certain measurement devices, or if positions are recorded and speed is modelled by a diffusion. In finance, realized volatility or variations thereof can be used to construct observations of the latent integrated volatility process. Specifically, we assume that the integrated process is observed at equidistant, deterministic time points and consider the high-frequency/infinite horizon asymptotic scenario, where the number of observations, the sampling frequency and the time of the last observation all go to infinity. Subject to mild standard regularity conditions on the diffusion model, we prove the asymptotic existence and uniqueness of a consistent estimator for useful and tractable classes of prediction-based estimating functions. Asymptotic normality of the estimator is obtained under an additional assumption on the rates. The proofs are based on the useful Euler-Ito expansions of transformations of diffusions and integrated diffusions, which we study in some detail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06764v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Emil S. J{\o}rgensen, Michael S{\o}rensen</dc:creator>
    </item>
    <item>
      <title>Robustness and Efficiency of Rosenbaum's Rank-based Estimator in Randomized Trials: A Design-based Perspective</title>
      <link>https://arxiv.org/abs/2111.15524</link>
      <description>arXiv:2111.15524v5 Announce Type: replace 
Abstract: Mean-based estimators of causal effects in randomized experiments may behave poorly if the potential outcomes have a heavy tail or contain outliers. An alternative estimator proposed by Rosenbaum (1993) estimates a constant additive treatment effect by inverting a randomization test using ranks. We develop a design-based asymptotic theory for this rank-based estimator and study its robustness and efficiency properties. We show that Rosenbaum's estimator is robust against outliers with a breakdown point that uniformly dominates that of any weighted quantile estimator. When pretreatment covariates are available, a regression-adjusted version of Rosenbaum's estimator uses an agnostic linear regression on the covariates and bases inference on the ranks of residuals. Under mild integrability conditions, we show that this estimator is at most 13.6% less efficient, in the worst case, than the commonly used mean-based regression adjustment method proposed by Lin (2013); often outperforming it when the residuals have heavy tails. Moreover, under suitable assumptions, Rosenbaum's regression-adjusted estimator is at least as efficient as the unadjusted one. Finally, we initiate the study of Rosenbaum's estimator when the constant treatment effect assumption may be violated. To analyze the regression-adjusted estimator, we develop local asymptotics of rank statistics under the design-based framework, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.15524v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Ghosh, Nabarun Deb, Bikram Karmakar, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>Bayesian random-effects meta-analysis of aggregate data on clinical events</title>
      <link>https://arxiv.org/abs/2504.12214</link>
      <description>arXiv:2504.12214v2 Announce Type: replace 
Abstract: To investigate intervention effects on rare events, meta-analysis techniques are commonly applied in order to assess the accumulated evidence. When it comes to adverse effects in clinical trials, these are often most adequately handled using survival methods. A common-effect model that is able to process data in commonly quoted formats in terms of hazard ratios has been proposed for this purpose. In order to accommodate potential heterogeneity between studies, we have extended the model by Holzhauer to a random-effects approach. The Bayesian model is described in detail, and applications to realistic data sets are discussed along with sensitivity analyses and Monte Carlo simulations to support the conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12214v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian R\"over, Qiong Wu, Anja Loos, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Deep Generative Modeling with Spatial and Network Images: An Explainable AI (XAI) Approach</title>
      <link>https://arxiv.org/abs/2505.12743</link>
      <description>arXiv:2505.12743v3 Announce Type: replace 
Abstract: This article addresses the challenge of modeling the amplitude of spatially indexed low frequency fluctuations (ALFF) in resting state functional MRI as a function of cortical structural features and a multi-task coactivation network in the Adolescent Brain Cognitive Development (ABCD) Study. It proposes a generative model that integrates effects of spatially-varying inputs and a network-valued input using deep neural networks to capture complex non-linear and spatial associations with the output. The method models spatial smoothness, accounts for subject heterogeneity and complex associations between network and spatial images at different scales, enables accurate inference of each images effect on the output image, and allows prediction with uncertainty quantification via Monte Carlo dropout, contributing to one of the first Explainable AI (XAI) frameworks for heterogeneous imaging data. The model is highly scalable to high-resolution data without the heavy pre-processing or summarization often required by Bayesian methods. Empirical results demonstrate its strong performance compared to existing statistical and deep learning methods. We applied the XAI model to the ABCD data which revealed associations between cortical features and ALFF throughout the entire brain. Our model performed comparably to existing methods in predictive accuracy but provided superior uncertainty quantification and faster computation, demonstrating its effectiveness for large-scale neuroimaging analysis. Open-source software in Python for XAI is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12743v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeseul Jeon, Rajarshi Guhaniyogi, Aaron Scheffler</dc:creator>
    </item>
    <item>
      <title>A Kolmogorov-Arnold Neural Model for Cascading Extremes</title>
      <link>https://arxiv.org/abs/2505.13370</link>
      <description>arXiv:2505.13370v2 Announce Type: replace 
Abstract: This paper addresses the growing concern of cascading extreme events, such as an extreme earthquake followed by a tsunami, by presenting a novel method for risk assessment focused on these domino effects. The proposed approach develops an extreme value theory framework within a Kolmogorov-Arnold network (KAN) to estimate the probability of one extreme event triggering another, conditionally on a feature vector. An extra layer is added to the KAN architecture to ensure that the parameter of interest lies within the unit interval, and we refer to the resulting neural model as KANE (KAN with Natural Enforcement). The proposed method is backed by exhaustive numerical studies and further illustrated with real-world applications to seismology and climatology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13370v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miguel de Carvalho, Clemente Ferrer, Ronny Vallejos</dc:creator>
    </item>
    <item>
      <title>Optimal Adjustment Sets for Nonparametric Estimation of Weighted Controlled Direct Effect</title>
      <link>https://arxiv.org/abs/2506.09871</link>
      <description>arXiv:2506.09871v4 Announce Type: replace 
Abstract: The weighted controlled direct effect (WCDE) generalizes the standard controlled direct effect (CDE) by averaging over the mediator distribution, providing a robust estimate when treatment effects vary across mediator levels. This makes the WCDE especially relevant in fairness analysis, where it isolates the direct effect of an exposure on an outcome, independent of mediating pathways. This work establishes three fundamental advances for WCDE in observational studies: First, we establish necessary and sufficient conditions for the unique identifiability of the WCDE, clarifying when it diverges from the CDE. Next, we consider nonparametric estimation of the WCDE and derive its influence function, focusing on the class of regular and asymptotically linear estimators. Lastly, we characterize the optimal covariate adjustment set that minimizes the asymptotic variance, demonstrating how mediator-confounder interactions introduce distinct requirements compared to average treatment effect estimation. Our results offer a principled framework for efficient estimation of direct effects in complex causal systems, with practical applications in fairness and mediation analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09871v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruiyang Lin (University of Science,Technology of China), Yongyi Guo (University of Wisconsin-Madison), Kyra Gan (Cornell Tech)</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing for community structure in temporal networks using e-values</title>
      <link>https://arxiv.org/abs/2507.23034</link>
      <description>arXiv:2507.23034v3 Announce Type: replace 
Abstract: Community structure in networks naturally arises in various applications. But while the topic has received significant attention for static networks, the literature on community structure in temporally evolving networks is more scarce. In particular, there are currently no statistical methods available to test for the presence of community structure in a sequence of networks evolving over time. In this work, we propose a simple yet powerful test using e-values, an alternative to p-values that is more flexible in certain ways. Specifically, an e-value framework retains valid testing properties even after combining dependent information, a relevant feature in the context of testing temporal networks. We apply the proposed test to synthetic and real-world networks, demonstrating various features inherited from the e-value formulation and exposing some of the inherent difficulties of testing on temporal networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23034v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko, Jonathan P. Williams, Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Nonparametric Modeling of Continuous-Time Markov Chains</title>
      <link>https://arxiv.org/abs/2511.03954</link>
      <description>arXiv:2511.03954v2 Announce Type: replace 
Abstract: Inferring the infinitesimal rates of continuous-time Markov chains (CTMCs) is a central challenge in many scientific domains. This task is hindered by three factors: quadratic growth in the number of rates as the CTMC state space expands, strong dependencies among rates, and incomplete information for many transitions. We introduce a new Bayesian framework that flexibly models the CTMC rates by incorporating covariates through Gaussian processes (GPs). This approach improves inference by integrating new information and contributes to the understanding of the CTMC stochastic behavior by shedding light on potential external drivers. Unlike previous approaches limited to linear covariate effects, our method captures complex non-linear relationships, enabling fuller use of covariate information and more accurate characterization of their influence. To perform efficient inference, we employ a scalable Hamiltonian Monte Carlo (HMC) sampler. We address the prohibitive cost of computing the exact likelihood gradient by integrating the HMC trajectories with a scalable gradient approximation, reducing the computational complexity from $O(K^5)$ to $O(K^2)$, where $K$ is the number of CTMC states. Finally, we demonstrate our method on Bayesian phylogeography inference -- a domain where CTMCs are central -- showing effectiveness on both synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03954v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filippo Monti, Xiang Ji, Marc A. Suchard</dc:creator>
    </item>
    <item>
      <title>Testing similarity of competing risks models by comparing transition probabilities</title>
      <link>https://arxiv.org/abs/2512.00583</link>
      <description>arXiv:2512.00583v2 Announce Type: replace 
Abstract: Assessing whether two patient populations exhibit comparable event dynamics is essential for evaluating treatment equivalence, pooling data across cohorts, or comparing clinical pathways across hospitals or strategies. We introduce a statistical framework for formally testing the similarity of competing risks models based on transition probabilities, which represent the cumulative risk of each event over time. Our method defines a maximum-type distance between the transition probability matrices of two multistate processes and employs a novel constrained parametric bootstrap test to evaluate similarity under both administrative and random right censoring. We theoretically establish the asymptotic validity and consistency of the bootstrap test. Through extensive simulation studies, we show that our method reliably controls the type I error and achieves higher statistical power than existing intensity-based approaches. Applying the framework to routine clinical data of prostate cancer patients treated with radical prostatectomy, we identify the smallest similarity threshold at which patients with and without prior in-house fusion biopsy exhibit comparable readmission dynamics. The proposed method provides a robust and interpretable tool for quantifying similarity in event history models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00583v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zoe Kristin Lange, Maryam Farhadizadeh, Holger Dette, Nadine Binder</dc:creator>
    </item>
    <item>
      <title>Distributional Balancing for Causal Inference: A Unified Framework via Characteristic Function Distance</title>
      <link>https://arxiv.org/abs/2601.15449</link>
      <description>arXiv:2601.15449v2 Announce Type: replace 
Abstract: Weighting methods are essential tools for estimating causal effects in observational studies, with the goal of balancing pre-treatment covariates across treatment groups. Traditional approaches pursue this objective indirectly, for example, via inverse propensity score weighting or by matching a finite number of covariate moments, and therefore do not guarantee balance of the full joint covariate distributions. Recently, distributional balancing methods have emerged as robust, nonparametric alternatives that directly target alignment of entire covariate distributions, but they lack a unified framework, formal theoretical guarantees, and valid inferential procedures. We introduce a unified framework for nonparametric distributional balancing based on the characteristic function distance (CFD) and show that widely used discrepancy measures, including the maximum mean discrepancy and energy distance, arise as special cases. Our theoretical analysis establishes conditions under which the resulting CFD-based weighting estimator achieves $\sqrt{n}$-consistency. Since the standard bootstrap may fail for this estimator, we propose subsampling as a valid alternative for inference. We further extend our approach to an instrumental variable setting to address potential unmeasured confounding. Finally, we evaluate the performance of our method through simulation studies and a real-world application, where the proposed estimator performs well and exhibits results consistent with our theoretical predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15449v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Diptanil Santra, Guanhua Chen, Chan Park</dc:creator>
    </item>
    <item>
      <title>Forecasting with Hyper-Trees</title>
      <link>https://arxiv.org/abs/2405.07836</link>
      <description>arXiv:2405.07836v4 Announce Type: replace-cross 
Abstract: We introduce Hyper-Trees as a novel framework for modeling time series data using gradient boosted trees. Unlike conventional tree-based approaches that forecast time series directly, Hyper-Trees learn the parameters of a target time series model, such as ARIMA or Exponential Smoothing, as functions of features. These parameters are then used by the target model to generate the final forecasts. Our framework combines the effectiveness of decision trees on tabular data with classical forecasting models, thereby inducing a time series inductive bias into tree-based models. To resolve the scaling limitations of boosted trees when estimating a high-dimensional set of target model parameters, we combine decision trees and neural networks within a unified framework. In this hybrid approach, the trees generate informative representations from the input features, which a shallow network then uses as input to learn the parameters of a time series model. With our research, we explore the effectiveness of Hyper-Trees across a range of forecasting tasks and extend tree-based modeling beyond its conventional use in time series analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07836v4</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander M\"arz, Kashif Rasul</dc:creator>
    </item>
    <item>
      <title>Optimal Bias-variance Tradeoff in Matrix and Tensor Estimation</title>
      <link>https://arxiv.org/abs/2509.17382</link>
      <description>arXiv:2509.17382v3 Announce Type: replace-cross 
Abstract: We study matrix and tensor denoising when the underlying signal is \textbf{not} necessarily low-rank. In the tensor setting, we observe \[ Y = X^\ast + Z \in \mathbb{R}^{p_1 \times p_2 \times p_3}, \] where $X^\ast$ is an unknown signal tensor and $Z$ is a noise tensor. We propose a one-step variant of the higher-order SVD (HOSVD) estimator, denoted $\widetilde X$, and show that, uniformly over any user-specified Tucker ranks $(r_1,r_2,r_3)$, with high probability, \[ \|\widetilde X - X^\ast\|_{\mathrm F}^2 = O\Big( \kappa^2\Big\{r_1r_2r_3 + \sum_{k=1}^3 p_k r_k\Big\} + \xi_{(r_1,r_2,r_3)}^2 \Big). \] Here, $\xi_{(r_1,r_2,r_3)}$ is the best achievable Tucker rank-$(r_1,r_2,r_3)$ approximation error of $X^\ast$ (bias), $\kappa^2$ quantifies the noise level, and $\kappa^2\{r_1r_2r_3+\sum_{k=1}^3 p_k r_k\}$ is the variance term scaling with the effective degrees of freedom of $\widetilde X$. This yields a rank-adaptive bias-variance tradeoff: increasing $(r_1,r_2,r_3)$ decreases the bias $\xi_{(r_1,r_2,r_3)}$ while increasing variance. In the matrix setting, we show that truncated SVD achieves an analogous bias-variance tradeoff for arbitrary signal matrices. Notably, our matrix result requires \textbf{no} assumptions on the signal matrix, such as finite rank or spectral gaps. Finally, we complement our upper bounds with matching information-theoretic lower bounds, showing that the resulting bias-variance tradeoff is minimax optimal up to universal constants in both the matrix and tensor settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17382v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shivam Kumar, Xiaokai Luo, Haotian Xu, Carlos Misael Madrid Padilla, Oscar Hernan Madrid Padilla, Daren Wang</dc:creator>
    </item>
    <item>
      <title>Radon--Wasserstein Gradient Flows for Interacting-Particle Sampling in High Dimensions</title>
      <link>https://arxiv.org/abs/2602.05227</link>
      <description>arXiv:2602.05227v2 Announce Type: replace-cross 
Abstract: Gradient flows of the Kullback--Leibler (KL) divergence, such as the Fokker--Planck equation and Stein Variational Gradient Descent, evolve a distribution toward a target density known only up to a normalizing constant. We introduce new gradient flows of the KL divergence with a remarkable combination of properties: they admit accurate interacting-particle approximations in high dimensions, and the per-step cost scales linearly in both the number of particles and the dimension. These gradient flows are based on new transportation-based Riemannian geometries on the space of probability measures: the Radon--Wasserstein geometry and the related Regularized Radon--Wasserstein (RRW) geometry. We define these geometries using the Radon transform so that the gradient-flow velocities depend only on one-dimensional projections. This yields interacting-particle-based algorithms whose per-step cost follows from efficient Fast Fourier Transform-based evaluation of the required 1D convolutions. We additionally provide numerical experiments that study the performance of the proposed algorithms and compare convergence behavior and quantization. Finally, we prove some theoretical results including well-posedness of the flows and long-time convergence guarantees for the RRW flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05227v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elias Hess-Childs, Dejan Slep\v{c}ev, Lantian Xu</dc:creator>
    </item>
  </channel>
</rss>
