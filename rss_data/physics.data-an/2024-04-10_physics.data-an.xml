<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Apr 2024 04:00:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Causality for Earth Science -- A Review on Time-series and Spatiotemporal Causality Methods</title>
      <link>https://arxiv.org/abs/2404.05746</link>
      <description>arXiv:2404.05746v1 Announce Type: new 
Abstract: This survey paper covers the breadth and depth of time-series and spatiotemporal causality methods, and their applications in Earth Science. More specifically, the paper presents an overview of causal discovery and causal inference, explains the underlying causal assumptions, and enlists evaluation techniques and key terminologies of the domain area. The paper elicits the various state-of-the-art methods introduced for time-series and spatiotemporal causal analysis along with their strengths and limitations. The paper further describes the existing applications of several methods for answering specific Earth Science questions such as extreme weather events, sea level rise, teleconnections etc. This survey paper can serve as a primer for Data Science researchers interested in data-driven causal study as we share a list of resources, such as Earth Science datasets (synthetic, simulated and observational data) and open source tools for causal analysis. It will equally benefit the Earth Science community interested in taking an AI-driven approach to study the causality of different dynamic and thermodynamic processes as we present the open challenges and opportunities in performing causality-based Earth Science study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05746v1</guid>
      <category>physics.data-an</category>
      <category>cs.AI</category>
      <category>physics.ao-ph</category>
      <category>physics.geo-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahara Ali, Uzma Hasan, Xingyan Li, Omar Faruque, Akila Sampath, Yiyi Huang, Md Osman Gani, Jianwu Wang</dc:creator>
    </item>
    <item>
      <title>Physics Event Classification Using Large Language Models</title>
      <link>https://arxiv.org/abs/2404.05752</link>
      <description>arXiv:2404.05752v1 Announce Type: new 
Abstract: The 2023 AI4EIC hackathon was the culmination of the third annual AI4EIC workshop at The Catholic University of America. This workshop brought together researchers from physics, data science and computer science to discuss the latest developments in Artificial Intelligence (AI) and Machine Learning (ML) for the Electron Ion Collider (EIC), including applications for detectors, accelerators, and experimental control. The hackathon, held on the final day of the workshop, involved using a chatbot powered by a Large Language Model, ChatGPT-3.5, to train a binary classifier neutrons and photons in simulated data from the \textsc{GlueX} Barrel Calorimeter. In total, six teams of up to four participants from all over the world took part in this intense educational and research event. This article highlights the hackathon challenge, the resources and methodology used, and the results and insights gained from analyzing physics data using the most cutting-edge tools in AI/ML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05752v1</guid>
      <category>physics.data-an</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cristiano Fanelli, James Giroux, Patrick Moran, Hemalata Nayak, Karthik Suresh, Eric Walter</dc:creator>
    </item>
    <item>
      <title>Implicit Assimilation of Sparse In Situ Data for Dense &amp; Global Storm Surge Forecasting</title>
      <link>https://arxiv.org/abs/2404.05758</link>
      <description>arXiv:2404.05758v1 Announce Type: new 
Abstract: Hurricanes and coastal floods are among the most disastrous natural hazards. Both are intimately related to storm surges, as their causes and effects, respectively. However, the short-term forecasting of storm surges has proven challenging, especially when targeting previously unseen locations or sites without tidal gauges. Furthermore, recent work improved short and medium-term weather forecasting but the handling of raw unassimilated data remains non-trivial. In this paper, we tackle both challenges and demonstrate that neural networks can implicitly assimilate sparse in situ tide gauge data with coarse ocean state reanalysis in order to forecast storm surges. We curate a global dataset to learn and validate the dense prediction of storm surges, building on preceding efforts. Other than prior work limited to known gauges, our approach extends to ungauged sites, paving the way for global storm surge forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05758v1</guid>
      <category>physics.data-an</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Ebel, Brandon Victor, Peter Naylor, Gabriele Meoni, Federico Serva, Rochelle Schneider</dc:creator>
    </item>
    <item>
      <title>A feature-based information-theoretic approach for detecting interpretable, long-timescale pairwise interactions from time series</title>
      <link>https://arxiv.org/abs/2404.05929</link>
      <description>arXiv:2404.05929v1 Announce Type: new 
Abstract: Quantifying relationships between components of a complex system is critical to understanding the rich network of interactions that characterize the behavior of the system. Traditional methods for detecting pairwise dependence of time series, such as Pearson correlation, Granger causality, and mutual information, are computed directly in the space of measured time-series values. But for systems in which interactions are mediated by statistical properties of the time series (`time-series features') over longer timescales, this approach can fail to capture the underlying dependence from limited and noisy time-series data, and can be challenging to interpret. Addressing these issues, here we introduce an information-theoretic method for detecting dependence between time series mediated by time-series features that provides interpretable insights into the nature of the interactions. Our method extracts a candidate set of time-series features from sliding windows of the source time series and assesses their role in mediating a relationship to values of the target process. Across simulations of three different generative processes, we demonstrate that our feature-based approach can outperform a traditional inference approach based on raw time-series values, especially in challenging scenarios characterized by short time-series lengths, high noise levels, and long interaction timescales. Our work introduces a new tool for inferring and interpreting feature-mediated interactions from time-series data, contributing to the broader landscape of quantitative analysis in complex systems research, with potential applications in various domains including but not limited to neuroscience, finance, climate science, and engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05929v1</guid>
      <category>physics.data-an</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aria Nguyen, Oscar McMullin, Joseph T. Lizier, Ben D. Fulcher</dc:creator>
    </item>
    <item>
      <title>Sen2Chain: An Open-Source Toolbox for Processing Sentinel-2 Satellite Images and Producing Time-Series of Spectral Indices</title>
      <link>https://arxiv.org/abs/2404.04305</link>
      <description>arXiv:2404.04305v1 Announce Type: cross 
Abstract: The increasing availability of free high-resolution earth observation data covering any point on the globe every few days led to the emergence of new remote sensing tools that can manipulate the very large volumes of data generated by those satellites. We present Sen2Chain, an open-source Python tool that can automate the processing of large time series of Sentinel-2 images for their use in various fields (e.g., environmental health, natural hazards, ecology). Sen2Chain allows downloading images from various earth observation data suppliers, applying geometric and atmospheric corrections using ESA's Sen2Cor tool, and generating and applying cloud masks. Sen2Chain's ability to extract time series of spectral indices (e.g NDVI, NDWI) provides simplified access to value-added environmental information for a wide range of end-users and applications. Sen2Chain enables all data processing stages to be customized and chained together, with the possibility to automate and parallelize the processing, and optimize data management. Sen2Chain is paving the way for the creation and processing of a large earth observation image database dedicated to users who require time-series and/or perform regular environmental observations. The Web tool Sen2Extract is also presented, which enables end-users with no expertise in remote sensing to easily extract time-series for 11 spectral indices values for specific regions of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04305v1</guid>
      <category>astro-ph.IM</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christophe Revillion, Pascal Mouquet, Jeremy Commins, Juliette Miranville, Charlotte Wolff, Thomas Germain, Sylvaine Jego, Lucas Longour, Florian Girond, Didier Bouche, Rodolphe Devillers, Gwenaelle Pennober, Vincent Herbreteau</dc:creator>
    </item>
    <item>
      <title>Dynamical stability and chaos in artificial neural network trajectories along training</title>
      <link>https://arxiv.org/abs/2404.05782</link>
      <description>arXiv:2404.05782v1 Announce Type: cross 
Abstract: The process of training an artificial neural network involves iteratively adapting its parameters so as to minimize the error of the network's prediction, when confronted with a learning task. This iterative change can be naturally interpreted as a trajectory in network space -- a time series of networks -- and thus the training algorithm (e.g. gradient descent optimization of a suitable loss function) can be interpreted as a dynamical system in graph space. In order to illustrate this interpretation, here we study the dynamical properties of this process by analyzing through this lens the network trajectories of a shallow neural network, and its evolution through learning a simple classification task. We systematically consider different ranges of the learning rate and explore both the dynamical and orbital stability of the resulting network trajectories, finding hints of regular and chaotic behavior depending on the learning rate regime. Our findings are put in contrast to common wisdom on convergence properties of neural networks and dynamical systems theory. This work also contributes to the cross-fertilization of ideas between dynamical systems theory, network theory and machine learning</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05782v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>nlin.CD</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaloyan Danovski, Miguel C. Soriano, Lucas Lacasa</dc:creator>
    </item>
    <item>
      <title>Resolution enhancement of SOHO/MDI Magnetograms</title>
      <link>https://arxiv.org/abs/2404.05968</link>
      <description>arXiv:2404.05968v1 Announce Type: cross 
Abstract: Research on the solar magnetic field and its effects on solar dynamo mechanisms and space weather events has benefited from the continual improvements in instrument resolution and measurement frequency. The augmentation and assimilation of historical observational data timelines also play a significant role in understanding the patterns of solar magnetic field variation. Within the realm of astronomical data processing, superresolution reconstruction refers to the process of using a substantial corpus of training data to learn the nonlinear mapping between low-resolution and high-resolution images,thereby achieving higher-resolution astronomical images. This paper is an application study in highdimensional non-linear regression. Deep learning models were employed to perform SR modeling on SOHO/MDI magnetograms and SDO/HMI magnetograms, thus reliably achieving resolution enhancement of full-disk SOHO/MDI magnetograms and enhancing the image resolution to obtain more detailed information. For this study, a dataset comprising 9717 pairs of data from April 2010 to February 2011 was used as the training set,1332 pairs from March 2011 were used as the validation set, and 1,034 pairs from April 2011 were used as the test set. After data preprocessing, we randomly cropped 128x128 sub-images as the LR from the full-disk MDI magnetograms, and the corresponding 512x512 sub-images as HR from the HMI full-disk magnetograms for model training. The tests conducted have shown that the study successfully produced reliable 4x super-resolution reconstruction of full-disk MDI magnetograms.The MESR model'sresults (0.911) were highly correlated with the target HMI magnetographs as indicated by the correlation coefficient values. Furthermore, the method achieved the best PSNR, SSIM, MAE and RMSE values, indicating that the MESR model can effectively reconstruct magnetog.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05968v1</guid>
      <category>astro-ph.SR</category>
      <category>astro-ph.IM</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ying Qin, Kai-Fan Ji, Hui Liu, Xiao-Guang Yu</dc:creator>
    </item>
    <item>
      <title>Misspecification uncertainties in near-deterministic regression</title>
      <link>https://arxiv.org/abs/2402.01810</link>
      <description>arXiv:2402.01810v2 Announce Type: replace-cross 
Abstract: The expected loss is an upper bound to the model generalization error which admits robust PAC-Bayes bounds for learning. However, loss minimization is known to ignore misspecification, where models cannot exactly reproduce observations. This leads to significant underestimates of parameter uncertainties in the large data, or underparameterized, limit. We analyze the generalization error of near-deterministic, misspecified and underparametrized surrogate models, a regime of broad relevance in science and engineering. We show posterior distributions must cover every training point to avoid a divergent generalization error and derive an ensemble \textit{ansatz} that respects this constraint, which for linear models incurs minimal overhead. The efficient approach is demonstrated on model problems before application to high dimensional datasets in atomistic machine learning. Parameter uncertainties from misspecification survive in the underparametrized limit, giving accurate prediction and bounding of test errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01810v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas D Swinburne, Danny Perez</dc:creator>
    </item>
  </channel>
</rss>
