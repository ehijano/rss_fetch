<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Jun 2025 01:32:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>R\'enyi-Induced Information Geometry and Hartigan's Prior Family</title>
      <link>https://arxiv.org/abs/2506.12028</link>
      <description>arXiv:2506.12028v1 Announce Type: new 
Abstract: We derive the information geometry induced by the statistical R\'enyi divergence, namely its metric tensor, its dual parametrized connections, as well as its dual Laplacians. Based on these results, we demonstrate that the R\'enyi-geometry, though closely related, differs in structure from Amari's well-known $\alpha$-geometry. Subsequently, we derive the canonical uniform prior distributions for a statistical manifold endowed with a R\'enyi-geometry, namely the dual R\'enyi-covolumes. We find that the R\'enyi-priors can be made to coincide with Takeuchi and Amari's $\alpha$-priors by a reparameterization, which is itself of particular significance in statistics. Herewith, we demonstrate that Hartigan's parametrized ($\alpha_H$) family of priors is precisely the parametrized ($\rho$) family of R\'enyi-priors ($\alpha_H = \rho$).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12028v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebecca Maria Kuntz, Heinrich von Campe, Bj\"orn Malte Sch\"afer</dc:creator>
    </item>
    <item>
      <title>The empirical discrete copula process</title>
      <link>https://arxiv.org/abs/2506.12316</link>
      <description>arXiv:2506.12316v1 Announce Type: new 
Abstract: This paper develops a general inferential framework for discrete copulas on finite supports in any dimension. The copula of a multivariate discrete distribution is defined as Csiszar's I-projection (i.e., the minimum-Kullback-Leibler divergence projection) of its joint probability array onto the polytope of uniform-margins probability arrays of the same size, and its empirical estimator is obtained by applying that same projection to the array of empirical frequencies observed on the sample. Under the assumption of random sampling, strong consistency and root-n-asymptotic normality of the empirical copula array is established, with an explicit "sandwich" form for its covariance. The theory is illustrated by deriving the large-sample distribution of Yule's concordance coefficient (the natural analogue of Spearman's rho for bivariate discrete distributions) and by constructing a test for quasi-independence in multivariate contingency tables. Our results not only complete the foundations of discrete-copula inference but also connect directly to entropically regularised optimal transport and other minimum-divergence problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12316v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Gery Geenens, Ivan Kojadinovic, Tommaso Martini</dc:creator>
    </item>
    <item>
      <title>An Easily Tunable Approach to Robust and Sparse High-Dimensional Linear Regression</title>
      <link>https://arxiv.org/abs/2506.12591</link>
      <description>arXiv:2506.12591v1 Announce Type: new 
Abstract: Sparse linear regression methods such as Lasso require a tuning parameter that depends on the noise variance, which is typically unknown and difficult to estimate in practice. In the presence of heavy-tailed noise or adversarial outliers, this problem becomes more challenging. In this paper, we propose an estimator for robust and sparse linear regression that eliminates the need for explicit prior knowledge of the noise scale. Our method builds on the Huber loss and incorporates an iterative scheme that alternates between coefficient estimation and adaptive noise calibration via median-of-means. The approach is theoretically grounded and achieves sharp non-asymptotic error bounds under both sub-Gaussian and heavy-tailed noise assumptions. Moreover, the proposed method accommodates arbitrary outlier contamination in the response without requiring prior knowledge of the number of outliers or the sparsity level. While previous robust estimators avoid tuning parameters related to the noise scale or sparsity, our procedure achieves comparable error bounds when the number of outliers is unknown, and improved bounds when it is known. In particular, the improved bounds match the known minimax lower bounds up to constant factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12591v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takeyuki Sasai, Hironori Fujisawa</dc:creator>
    </item>
    <item>
      <title>Asymptotic Distribution of Low-Dimensional Patterns Induced by Non-Differentiable Regularizers under General Loss Functions</title>
      <link>https://arxiv.org/abs/2506.12621</link>
      <description>arXiv:2506.12621v1 Announce Type: new 
Abstract: This article investigates the asymptotic distribution of penalized estimators with non-differentiable penalties designed to recover low-dimensional structures; that is, subspaces in which the true parameter lies. We study the asymptotic distribution of the scaled estimation error in the regime where the parameter dimension p is fixed and the number of observations n tends to infinity. Our focus is on the asymptotic probability of pattern recovery, a question not addressed by classical results for the LASSO. In our recent work, we derived such results for the LASSO and broader classes of penalties, including non-separable ones such as SLOPE, within the standard linear model. We now extend this analysis to general loss functions, including, for example, robust regression with Huber and quantile loss functions, as well as generalized linear models from the exponential family. The main contribution of the paper is the development of an asymptotic framework for pattern convergence of regularized M-estimators under general loss functions that satisfy a suitable stochastic differentiability condition. The proofs rely on tools from empirical process theory, including Donsker classes and VC-dimension techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12621v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivan Hejn\'y, Jonas Wallin, Ma{\l}gorzata Bogdan</dc:creator>
    </item>
    <item>
      <title>Beyond Sin-Squared Error: Linear-Time Entrywise Uncertainty Quantification for Streaming PCA</title>
      <link>https://arxiv.org/abs/2506.12655</link>
      <description>arXiv:2506.12655v1 Announce Type: new 
Abstract: We propose a novel statistical inference framework for streaming principal component analysis (PCA) using Oja's algorithm, enabling the construction of confidence intervals for individual entries of the estimated eigenvector. Most existing works on streaming PCA focus on providing sharp sin-squared error guarantees. Recently, there has been some interest in uncertainty quantification for the sin-squared error. However, uncertainty quantification or sharp error guarantees for entries of the estimated eigenvector in the streaming setting remains largely unexplored. We derive a sharp Bernstein-type concentration bound for elements of the estimated vector matching the optimal error rate up to logarithmic factors. We also establish a Central Limit Theorem for a suitably centered and scaled subset of the entries. To efficiently estimate the coordinate-wise variance, we introduce a provably consistent subsampling algorithm that leverages the median-of-means approach, empirically achieving similar accuracy to multiplier bootstrap methods while being significantly more computationally efficient. Numerical experiments demonstrate its effectiveness in providing reliable uncertainty estimates with a fraction of the computational cost of existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12655v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Syamantak Kumar, Shourya Pandey, Purnamrita Sarkar</dc:creator>
    </item>
    <item>
      <title>On the attainment of the Wasserstein--Cramer--Rao lower bound</title>
      <link>https://arxiv.org/abs/2506.12732</link>
      <description>arXiv:2506.12732v2 Announce Type: new 
Abstract: Recently, a Wasserstein analogue of the Cramer--Rao inequality has been developed using the Wasserstein information matrix (Otto metric). This inequality provides a lower bound on the Wasserstein variance of an estimator, which quantifies its robustness against additive noise. In this study, we investigate conditions for an estimator to attain the Wasserstein--Cramer--Rao lower bound (asymptotically), which we call the (asymptotic) Wasserstein efficiency. We show a condition under which Wasserstein efficient estimators exist for one-parameter statistical models. This condition corresponds to a recently proposed Wasserstein analogue of one-parameter exponential families (e-geodesics). We also show that the Wasserstein estimator, a Wasserstein analogue of the maximum likelihood estimator based on the Wasserstein score function, is asymptotically Wasserstein efficient in location-scale families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12732v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hayato Nishimori, Takeru Matsuda</dc:creator>
    </item>
    <item>
      <title>IAPO estimators in Exponentiated Frechet case</title>
      <link>https://arxiv.org/abs/2506.12743</link>
      <description>arXiv:2506.12743v1 Announce Type: new 
Abstract: In 2017 Jordanova and co-authors consider probabilities for p-outside values, and later on, they use them in order to construct distribution sensitive IPO estimators. These works do not take into account the asymmetry of the distribution. This shortcoming was recently overcome and the corresponding probabilities for asymmetric p-outside values, together with the so-called IAPO estimators, were defined. Here we apply these results to Exponentiated-Frechet distribution, introduced in 2003 by Nadarajah and Kotz. The abbreviation "IAPO" comes from "Inverse Probabilities for Asymmetric P-Outside Values". These estimators use as an auxiliary characteristic the empirical asymmetric $p$-fences. In this way, the system relating the estimated parameters and the asymmetric probabilities for $p$-outside values has an easier solution. The comparison with our previous study about the corresponding IPO and IPO-NM estimators shows that IAPO estimators give better results for the index of regular variation of the right tail of the cumulative distribution function. A simulation study depicts their rates of convergence, and finishes this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12743v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pavlina Jordanova, Evelina Veleva</dc:creator>
    </item>
    <item>
      <title>Finite sample-optimal adjustment sets in linear Gaussian causal models</title>
      <link>https://arxiv.org/abs/2506.12869</link>
      <description>arXiv:2506.12869v1 Announce Type: new 
Abstract: Traditional covariate selection methods for causal inference focus on achieving unbiasedness and asymptotic efficiency. In many practical scenarios, researchers must estimate causal effects from observational data with limited sample sizes or in cases where covariates are difficult or costly to measure. Their needs might be better met by selecting adjustment sets that are finite sample-optimal in terms of mean squared error. In this paper, we aim to find the adjustment set that minimizes the mean squared error of the causal effect estimator, taking into account the joint distribution of the variables and the sample size. We call this finite sample-optimal set the MSE-optimal adjustment set and present examples in which the MSE-optimal adjustment set differs from the asymptotically optimal adjustment set. To identify the MSE-optimal adjustment set, we then introduce a sample size criterion for comparing adjustment sets in linear Gaussian models. We also develop graphical criteria to reduce the search space for this adjustment set based on the causal graph. In experiments with simulated data, we show that the MSE-optimal adjustment set can outperform the asymptotically optimal adjustment set in finite sample size settings, making causal inference more practical in such scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12869v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nadja Rutsch, Sara Magliacane, St\'ephanie van der Pas</dc:creator>
    </item>
    <item>
      <title>Limiting distributions of ratios of Binomial random variables</title>
      <link>https://arxiv.org/abs/2506.13071</link>
      <description>arXiv:2506.13071v1 Announce Type: new 
Abstract: We consider the limiting distribution of the quantity $X^s/(X+Y)^r$, where $X$ and $Y$ are two independent Binomial random variables with a common success probability and a number of trials $n$ and $m$, respectively, and $r,s$ are positive real numbers. Under several settings, we prove that this converges to a Normal distribution with a given mean and variance, and demonstrate these theoretical results through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13071v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The Oculus, 2025, pp. 80-91</arxiv:journal_reference>
      <dc:creator>Adriel Barretto, Zachary Lubberts</dc:creator>
    </item>
    <item>
      <title>From Graphical Lasso to Atomic Norms: High-Dimensional Pattern Recovery</title>
      <link>https://arxiv.org/abs/2506.13353</link>
      <description>arXiv:2506.13353v1 Announce Type: new 
Abstract: Estimating high-dimensional precision matrices is a fundamental problem in modern statistics, with the graphical lasso and its $\ell_1$-penalty being a standard approach for recovering sparsity patterns. However, many statistical models, e.g. colored graphical models, exhibit richer structures like symmetry or equality constraints, which the $\ell_1$-norm cannot adequately capture. This paper addresses the gap by extending the high-dimensional analysis of pattern recovery to a general class of atomic norm penalties, particularly those whose unit balls are polytopes, where patterns correspond to the polytope's facial structure. We establish theoretical guarantees for recovering the true pattern induced by these general atomic norms in precision matrix estimation.
  Our framework builds upon and refines the primal-dual witness methodology of Ravikumar et al. (2011). Our analysis provides conditions on the deviation between sample and true covariance matrices for successful pattern recovery, given a novel, generalized irrepresentability condition applicable to any atomic norm. When specialized to the $\ell_1$-penalty, our results offer improved conditions -- including weaker deviation requirements and a less restrictive irrepresentability condition -- leading to tighter bounds and better asymptotic performance than prior work. The proposed general irrepresentability condition, based on a new thresholding concept, provides a unified perspective on model selection consistency. Numerical examples demonstrate the tightness of the derived theoretical bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13353v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piotr Graczyk, Bartosz Ko{\l}odziejek, Hideto Nakashima, Maciej Wilczy\'nski</dc:creator>
    </item>
    <item>
      <title>Characteristic Imsets for Cyclic Linear Causal Models and the Chickering Ideal</title>
      <link>https://arxiv.org/abs/2506.13407</link>
      <description>arXiv:2506.13407v1 Announce Type: new 
Abstract: Two directed graphs are called covariance equivalent if they induce the same set of covariance matrices, up to a Lebesgue measure zero set, on the random variables of their associated linear structural equation models. For acyclic graphs, covariance equivalence is characterized both structurally, via essential graphs and characteristic imsets, and transformationally, through sequences of covered edge flips. However, when cycles are allowed, only a transformational characterization of covariance equivalence has been discovered. We consider a linear map whose fibers correspond to the sets of graphs with identical characteristic imset vectors, and study the toric ideal associated to its integer matrix. Using properties of this ideal we show that directed graphs with the same characteristic imset vectors are covariance equivalent. In applications, imsets form a smaller search space for solving causal discovery via greedy search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13407v1</guid>
      <category>math.ST</category>
      <category>math.AG</category>
      <category>math.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Johnson, Pardis Semnani</dc:creator>
    </item>
    <item>
      <title>Computational lower bounds in latent models: clustering, sparse-clustering, biclustering</title>
      <link>https://arxiv.org/abs/2506.13647</link>
      <description>arXiv:2506.13647v1 Announce Type: new 
Abstract: In many high-dimensional problems, like sparse-PCA, planted clique, or clustering, the best known algorithms with polynomial time complexity fail to reach the statistical performance provably achievable by algorithms free of computational constraints. This observation has given rise to the conjecture of the existence, for some problems, of gaps -- so called statistical-computational gaps -- between the best possible statistical performance achievable without computational constraints, and the best performance achievable with poly-time algorithms. A powerful approach to assess the best performance achievable in poly-time is to investigate the best performance achievable by polynomials with low-degree. We build on the seminal paper of Schramm and Wein (2022) and propose a new scheme to derive lower bounds on the performance of low-degree polynomials in some latent space models. By better leveraging the latent structures, we obtain new and sharper results, with simplified proofs. We then instantiate our scheme to provide computational lower bounds for the problems of clustering, sparse clustering, and biclustering. We also prove matching upper-bounds and some additional statistical results, in order to provide a comprehensive description of the statistical-computational gaps occurring in these three problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13647v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bertrand Even, Christophe Giraud, Nicolas Verzelen</dc:creator>
    </item>
    <item>
      <title>A Unifying Integral Representation of the Gamma Function and Its Reciprocal</title>
      <link>https://arxiv.org/abs/2506.12112</link>
      <description>arXiv:2506.12112v1 Announce Type: cross 
Abstract: We derive an integral expression $G(z)$ for the reciprocal gamma function, $1/\Gamma(z)=G(z)/\pi$, that is valid for all $z\in\mathbb{C}$, without the need for analytic continuation. The same integral avoids the singularities of the gamma function and satisfies $G(1-z)=\Gamma(z)\sin(\pi z)$ for all $z\in\mathbb{C}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12112v1</guid>
      <category>math.CV</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Reinhard Hansen, Chen Tong</dc:creator>
    </item>
    <item>
      <title>Functional Multi-Reference Alignment via Deconvolution</title>
      <link>https://arxiv.org/abs/2506.12201</link>
      <description>arXiv:2506.12201v1 Announce Type: cross 
Abstract: This paper studies the multi-reference alignment (MRA) problem of estimating a signal function from shifted, noisy observations. Our functional formulation reveals a new connection between MRA and deconvolution: the signal can be estimated from second-order statistics via Kotlarski's formula, an important identification result in deconvolution with replicated measurements. To design our MRA algorithms, we extend Kotlarski's formula to general dimension and study the estimation of signals with vanishing Fourier transform, thus also contributing to the deconvolution literature. We validate our deconvolution approach to MRA through both theory and numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12201v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Al-Ghattas, Anna Little, Daniel Sanz-Alonso, Mikhail Sweeney</dc:creator>
    </item>
    <item>
      <title>A Generalized Framework for Approximate Co-Sufficient Sampling</title>
      <link>https://arxiv.org/abs/2506.12334</link>
      <description>arXiv:2506.12334v1 Announce Type: cross 
Abstract: Approximate co-sufficient sampling (aCSS) offers a principled route to hypothesis testing when null distributions are unknown, yet current implementations are confined to maximum likelihood estimators with smooth or linear regularization and provide little theoretical insight into power. We present a generalized framework that widens the scope of the aCSS method to embrace nonlinear regularization, such as group lasso and nonconvex penalties, as well as robust and nonparametric estimators. Moreover, we introduce a weighted sampling scheme for enhanced flexibility and propose a generalized aCSS framework that unifies existing conditional sampling methods. Our theoretical analysis rigorously establishes validity and, for the first time, characterizes the power optimality of aCSS procedures in certain high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12334v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Xie, Dongming Huang</dc:creator>
    </item>
    <item>
      <title>A data-based notion of quantiles on Hadamard spaces</title>
      <link>https://arxiv.org/abs/2506.12534</link>
      <description>arXiv:2506.12534v1 Announce Type: cross 
Abstract: This paper defines an alternative notion, described as data-based, of geometric quantiles on Hadamard spaces, in contrast to the existing methodology, described as parameter-based. In addition to having the same desirable properties as parameter-based quantiles, these data-based quantiles are shown to have several theoretical advantages related to large-sample properties like strong consistency and asymptotic normality, breakdown points, extreme quantiles and the gradient of the loss function. Using simulations, we explore some other advantages of the data-based framework, including simpler computation and better adherence to the shape of the distribution, before performing experiments with real diffusion tensor imaging data lying on a manifold of symmetric positive definite matrices. These experiments illustrate some of the uses of these quantiles by testing the equivalence of the generating distributions of different data sets and measuring distributional characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12534v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ha-Young Shin, Hee-Seok Oh</dc:creator>
    </item>
    <item>
      <title>Joint Quantile Shrinkage: A State-Space Approach toward Non-Crossing Bayesian Quantile Models</title>
      <link>https://arxiv.org/abs/2506.13257</link>
      <description>arXiv:2506.13257v1 Announce Type: cross 
Abstract: Crossing of fitted conditional quantiles is a prevalent problem for quantile regression models. We propose a new Bayesian modelling framework that penalises multiple quantile regression functions toward the desired non-crossing space. We achieve this by estimating multiple quantiles jointly with a prior on variation across quantiles, a fused shrinkage prior with quantile adaptivity. The posterior is derived from a decision-theoretic general Bayes perspective, whose form yields a natural state-space interpretation aligned with Time-Varying Parameter (TVP) models. Taken together our approach leads to a Quantile- Varying Parameter (QVP) model, for which we develop efficient sampling algorithms. We demonstrate that our proposed modelling framework provides superior parameter recovery and predictive performance compared to competing Bayesian and frequentist quantile regression estimators in simulated experiments and a real-data application to multivariate quantile estimation in macroeconomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13257v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Kohns, Tibor Szendrei</dc:creator>
    </item>
    <item>
      <title>Mitigating loss of variance in ensemble data assimilation: machine learning-based and distance-free localizations for better covariance estimation</title>
      <link>https://arxiv.org/abs/2506.13362</link>
      <description>arXiv:2506.13362v1 Announce Type: cross 
Abstract: We propose two new methods based/inspired by machine learning for tabular data and distance-free localization to enhance the covariance estimations in an ensemble data assimilation. The main goal is to enhance the data assimilation results by mitigating loss of variance due to sampling errors. We also analyze the suitability of several machine learning models and the balance between accuracy and computational cost of the covariance estimations. We introduce two distance-free localization techniques leveraging machine learning methods specifically tailored for tabular data. The methods are integrated into the Ensemble Smoother with Multiple Data Assimilation (ES-MDA) framework. The results show that the proposed localizations improve covariance accuracy and enhance data assimilation and uncertainty quantification results. We observe reduced variance loss for the input variables using the proposed methods. Furthermore, we compare several machine learning models, assessing their suitability for the problem in terms of computational cost, and quality of the covariance estimation and data match. The influence of ensemble size is also investigated, providing insights into balancing accuracy and computational efficiency. Our findings demonstrate that certain machine learning models are more suitable for this problem. This study introduces two novel methods that mitigate variance loss for model parameters in ensemble-based data assimilation, offering practical solutions that are easy to implement and do not require any additional numerical simulation or hyperparameter tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13362v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>physics.comp-ph</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vinicius L. S. Silva, Gabriel S. Seabra, Alexandre A. Emerick</dc:creator>
    </item>
    <item>
      <title>From Data-Driven to Purpose-Driven Artificial Intelligence: Systems Thinking for Data-Analytic Automation of Patient Care</title>
      <link>https://arxiv.org/abs/2506.13584</link>
      <description>arXiv:2506.13584v1 Announce Type: cross 
Abstract: In this work, we reflect on the data-driven modeling paradigm that is gaining ground in AI-driven automation of patient care. We argue that the repurposing of existing real-world patient datasets for machine learning may not always represent an optimal approach to model development as it could lead to undesirable outcomes in patient care. We reflect on the history of data analysis to explain how the data-driven paradigm rose to popularity, and we envision ways in which systems thinking and clinical domain theory could complement the existing model development approaches in reaching human-centric outcomes. We call for a purpose-driven machine learning paradigm that is grounded in clinical theory and the sociotechnical realities of real-world operational contexts. We argue that understanding the utility of existing patient datasets requires looking in two directions: upstream towards the data generation, and downstream towards the automation objectives. This purpose-driven perspective to AI system development opens up new methodological opportunities and holds promise for AI automation of patient care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13584v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daniel Anadria, Roel Dobbe, Anastasia Giachanou, Ruurd Kuiper, Richard Bartels, \'I\~nigo Mart\'inez de Rituerto de Troya, Carmen Z\"urcher, Daniel Oberski</dc:creator>
    </item>
    <item>
      <title>Understanding Learning Invariance in Deep Linear Networks</title>
      <link>https://arxiv.org/abs/2506.13714</link>
      <description>arXiv:2506.13714v1 Announce Type: cross 
Abstract: Equivariant and invariant machine learning models exploit symmetries and structural patterns in data to improve sample efficiency. While empirical studies suggest that data-driven methods such as regularization and data augmentation can perform comparably to explicitly invariant models, theoretical insights remain scarce. In this paper, we provide a theoretical comparison of three approaches for achieving invariance: data augmentation, regularization, and hard-wiring. We focus on mean squared error regression with deep linear networks, which parametrize rank-bounded linear maps and can be hard-wired to be invariant to specific group actions. We show that the critical points of the optimization problems for hard-wiring and data augmentation are identical, consisting solely of saddles and the global optimum. By contrast, regularization introduces additional critical points, though they remain saddles except for the global optimum. Moreover, we demonstrate that the regularization path is continuous and converges to the hard-wired solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13714v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Duan, Guido Mont\'ufar</dc:creator>
    </item>
    <item>
      <title>Error analysis for a statistical finite element method</title>
      <link>https://arxiv.org/abs/2201.07543</link>
      <description>arXiv:2201.07543v4 Announce Type: replace 
Abstract: The recently proposed statistical finite element (statFEM) approach synthesises measurement data with finite element models and allows for making predictions about the unknown true system response. We provide a probabilistic error analysis for a prototypical statFEM setup based on a Gaussian process prior under the assumption that the noisy measurement data are generated by a deterministic true system response function that satisfies a second-order elliptic partial differential equation for an unknown true source term. In certain cases, properties such as the smoothness of the source term may be misspecified by the Gaussian process model. The error estimates we derive are for the expectation with respect to the measurement noise of the $L^2$-norm of the difference between the true system response and the mean of the statFEM posterior. The estimates imply polynomial rates of convergence in the numbers of measurement points and finite element basis functions and depend on the Sobolev smoothness of the true source term and the Gaussian process model. A numerical example for Poisson's equation is used to illustrate these theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.07543v4</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toni Karvonen, Fehmi Cirak, Mark Girolami</dc:creator>
    </item>
    <item>
      <title>How good is your Laplace approximation of the Bayesian posterior? Finite-sample computable error bounds for a variety of useful divergences</title>
      <link>https://arxiv.org/abs/2209.14992</link>
      <description>arXiv:2209.14992v3 Announce Type: replace 
Abstract: The Laplace approximation is a popular method for constructing a Gaussian approximation to the Bayesian posterior and thereby approximating the posterior mean and variance. But approximation quality is a concern. One might consider using rate-of-convergence bounds from certain versions of the Bayesian Central Limit Theorem (BCLT) to provide quality guarantees. But existing bounds require assumptions that are unrealistic even for relatively simple real-life Bayesian analyses; more specifically, existing bounds either (1) require knowing the true data-generating parameter, (2) are asymptotic in the number of samples, (3) do not control the Bayesian posterior mean, or (4) require strongly log concave models to compute. In this work, we provide the first computable bounds on quality that simultaneously (1) do not require knowing the true parameter, (2) apply to finite samples, (3) control posterior means and variances, and (4) apply generally to models that satisfy the conditions of the asymptotic BCLT. Moreover, we substantially improve the dimension dependence of existing bounds; in fact, we achieve the lowest-order dimension dependence possible in the general case. We compute exact constants in our bounds for a variety of standard models, including logistic regression, and numerically demonstrate their utility. We provide a framework for analysis of more complex models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.14992v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research 26(87) 2025</arxiv:journal_reference>
      <dc:creator>Miko{\l}aj J. Kasprzak, Ryan Giordano, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Berry-Esseen theorems for the asymptotic normality of incomplete U-statistics with Bernoulli sampling</title>
      <link>https://arxiv.org/abs/2406.05394</link>
      <description>arXiv:2406.05394v3 Announce Type: replace 
Abstract: There has been a resurgence of interest in incomplete U-statistics that only sum over a subset of kernel evaluations, due to their computational efficiency and asymptotic normality which can be leveraged to quantify the uncertainty of ensemble predictions in machine learning. In this paper, we study the weak convergences to normality of one such construction, the incomplete U-statistic with Bernoulli sampling, under three different regimes on the relative sizes of the raw sample and the computational budget. Under minimalistic moment assumptions, we establish accompanying Berry-Esseen bounds with the natural rates that characterize the accuracy of these normal approximations. The key ingredients in our proofs include a variable censoring technique and a methodology for establishing Berry-Esseen bounds for the so-called Studentized nonlinear statistics recently formalized in the Stein's method literature, as well as an exponential lower tail bound for non-negative kernel U-statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05394v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dennis Leung</dc:creator>
    </item>
    <item>
      <title>On importance sampling and independent Metropolis-Hastings with an unbounded weight function</title>
      <link>https://arxiv.org/abs/2411.09514</link>
      <description>arXiv:2411.09514v2 Announce Type: replace 
Abstract: Importance sampling and independent Metropolis-Hastings (IMH) are among the fundamental building blocks of Monte Carlo methods. Both require a proposal distribution that globally approximates the target distribution. The Radon-Nikodym derivative of the target distribution relative to the proposal is called the weight function. Under the assumption that the weight is unbounded but has finite moments under the proposal distribution, we study the approximation error of importance sampling and of the particle independent Metropolis-Hastings algorithm (PIMH), which includes IMH as a special case. For the chains generated by such algorithms, we show that the common random numbers coupling is maximal. Using that coupling we derive bounds on the total variation distance of a PIMH chain to its target distribution. Our results allow a formal comparison of the finite-time biases of importance sampling and IMH, and we find the latter to be have a smaller bias. We further consider bias removal techniques using couplings, and provide conditions under which the resulting unbiased estimators have finite moments. These unbiased estimators provide an alternative to self-normalized importance sampling, implementable in the same settings. We compare their asymptotic efficiency as the number of particles goes to infinity, and consider their use in robust mean estimation techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09514v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Deligiannidis (University of Oxford), Pierre E. Jacob (ESSEC Business School), El Mahdi Khribch (ESSEC Business School), Guanyang Wang (Rutgers University)</dc:creator>
    </item>
    <item>
      <title>Upper and lower bounds on the subgeometric convergence of adaptive Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2411.17084</link>
      <description>arXiv:2411.17084v2 Announce Type: replace 
Abstract: We investigate lower bounds on the subgeometric convergence of adaptive Markov chain Monte Carlo under any adaptation strategy. In particular, we prove general lower bounds in total variation and on the weak convergence rate under general adaptation plans. If the adaptation diminishes sufficiently fast, we also develop comparable convergence rate upper bounds that are capable of approximately matching the convergence rate in the subgeometric lower bound. These results provide insight into the optimal design of adaptation strategies and also limitations on the convergence behavior of adaptive Markov chain Monte Carlo. Applications to an adaptive unadjusted Langevin algorithm as well as adaptive Metropolis-Hastings with independent proposals and random-walk proposals are explored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17084v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Austin Brown, Jeffrey S. Rosenthal</dc:creator>
    </item>
    <item>
      <title>A new and flexible class of sharp asymptotic time-uniform confidence sequences</title>
      <link>https://arxiv.org/abs/2502.10380</link>
      <description>arXiv:2502.10380v2 Announce Type: replace 
Abstract: Confidence sequences are anytime-valid analogues of classical confidence intervals that do not suffer from multiplicity issues under optional continuation of the data collection. As in classical statistics, asymptotic confidence sequences are a nonparametric tool showing under which high-level assumptions asymptotic coverage is achieved so that they also give a certain robustness guarantee against distributional deviations. In this paper, we propose a new flexible class of confidence sequences yielding sharp asymptotic time-uniform confidence sequences under mild assumptions. Furthermore, we highlight the connection to corresponding sequential testing problems and detail the underlying limit theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10380v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spl.2025.110462</arxiv:DOI>
      <arxiv:journal_reference>Statistics &amp; Probability Letters, Volume 226, 2025, 110462, ISSN 0167-7152</arxiv:journal_reference>
      <dc:creator>Felix Gnettner, Claudia Kirch</dc:creator>
    </item>
    <item>
      <title>Computational Equivalence of Spiked Covariance and Spiked Wigner Models via Gram-Schmidt Perturbation</title>
      <link>https://arxiv.org/abs/2503.02802</link>
      <description>arXiv:2503.02802v2 Announce Type: replace 
Abstract: In this work, we show the first average-case reduction transforming the sparse Spiked Covariance Model into the sparse Spiked Wigner Model and as a consequence obtain the first computational equivalence result between two well-studied high-dimensional statistics models. Our approach leverages a new perturbation equivariance property for Gram-Schmidt orthogonalization, enabling removal of dependence in the noise while preserving the signal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02802v2</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Bresler, Alina Harbuzova</dc:creator>
    </item>
    <item>
      <title>Distribution free M-estimation</title>
      <link>https://arxiv.org/abs/2505.22807</link>
      <description>arXiv:2505.22807v3 Announce Type: replace 
Abstract: The basic question of delineating those statistical problems that are solvable without making any assumptions on the underlying data distribution has long animated statistics and learning theory. This paper characterizes when a convex M-estimation or stochastic optimization problem is solvable in such an assumption-free setting, providing a precise dividing line between solvable and unsolvable problems. The conditions we identify show, perhaps surprisingly, that Lipschitz continuity of the loss being minimized is not necessary for distribution free minimization, and they are also distinct from classical characterizations of learnability in machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22807v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felipe Areces, John C. Duchi</dc:creator>
    </item>
    <item>
      <title>Upper and lower bounds for local Lipschitz stability of Bayesian posteriors</title>
      <link>https://arxiv.org/abs/2505.23541</link>
      <description>arXiv:2505.23541v2 Announce Type: replace 
Abstract: The work of Sprungk (Inverse Problems, 2020) established the local Lipschitz continuity of the misfit-to-posterior and prior-to-posterior maps with respect to the Kullback--Leibler divergence and the total variation, Hellinger, and 1-Wasserstein metrics, by proving certain upper bounds. The upper bounds were also used to show that if a posterior measure is more concentrated, then it can be more sensitive to perturbations in the misfit or prior. We prove upper bounds and lower bounds that emphasise the importance of the evidence. The lower bounds show that the sensitivity of posteriors to perturbations in the misfit or the prior not only can increase, but in general will increase as the posterior measure becomes more concentrated, i.e. as the evidence decreases to zero. Using the explicit dependence of our bounds on the evidence, we identify sufficient conditions for the misfit-to-posterior and prior-to-posterior maps to be locally bi-Lipschitz continuous.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23541v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nada Cvetkovi\'c, Han Cheng Lie</dc:creator>
    </item>
    <item>
      <title>Conformal Risk Control</title>
      <link>https://arxiv.org/abs/2208.02814</link>
      <description>arXiv:2208.02814v4 Announce Type: replace-cross 
Abstract: We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an $\mathcal{O}(1/n)$ factor. We also introduce extensions of the idea to distribution shift, quantile risk control, multiple and adversarial risk control, and expectations of U-statistics. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.02814v4</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios N. Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, Tal Schuster</dc:creator>
    </item>
    <item>
      <title>An invitation to the sample complexity of quantum hypothesis testing</title>
      <link>https://arxiv.org/abs/2403.17868</link>
      <description>arXiv:2403.17868v4 Announce Type: replace-cross 
Abstract: Quantum hypothesis testing (QHT) has been traditionally studied from the information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of samples of an unknown state. In this paper, we study the sample complexity of QHT, wherein the goal is to determine the minimum number of samples needed to reach a desired error probability. By making use of the wealth of knowledge that already exists in the literature on QHT, we characterize the sample complexity of binary QHT in the symmetric and asymmetric settings, and we provide bounds on the sample complexity of multiple QHT. In more detail, we prove that the sample complexity of symmetric binary QHT depends logarithmically on the inverse error probability and inversely on the negative logarithm of the fidelity. As a counterpart of the quantum Stein's lemma, we also find that the sample complexity of asymmetric binary QHT depends logarithmically on the inverse type II error probability and inversely on the quantum relative entropy, provided that the type II error probability is sufficiently small. We then provide lower and upper bounds on the sample complexity of multiple QHT, with it remaining an intriguing open question to improve these bounds. The final part of our paper outlines and reviews how sample complexity of QHT is relevant to a broad swathe of research areas and can enhance understanding of many fundamental concepts, including quantum algorithms for simulation and search, quantum learning and classification, and foundations of quantum mechanics. As such, we view our paper as an invitation to researchers coming from different communities to study and contribute to the problem of sample complexity of QHT, and we outline a number of open directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17868v4</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1038/s41534-025-00980-8</arxiv:DOI>
      <arxiv:journal_reference>npj Quantum Information, volume 11, Article number 94, June 2025</arxiv:journal_reference>
      <dc:creator>Hao-Chung Cheng, Nilanjana Datta, Nana Liu, Theshani Nuradha, Robert Salzmann, Mark M. Wilde</dc:creator>
    </item>
    <item>
      <title>Feature learning in finite-width Bayesian deep linear networks with multiple outputs and convolutional layers</title>
      <link>https://arxiv.org/abs/2406.03260</link>
      <description>arXiv:2406.03260v3 Announce Type: replace-cross 
Abstract: Deep linear networks have been extensively studied, as they provide simplified models of deep learning. However, little is known in the case of finite-width architectures with multiple outputs and convolutional layers. In this manuscript, we provide rigorous results for the statistics of functions implemented by the aforementioned class of networks, thus moving closer to a complete characterization of feature learning in the Bayesian setting. Our results include: (i) an exact and elementary non-asymptotic integral representation for the joint prior distribution over the outputs, given in terms of a mixture of Gaussians; (ii) an analytical formula for the posterior distribution in the case of squared error loss function (Gaussian likelihood); (iii) a quantitative description of the feature learning infinite-width regime, using large deviation theory. From a physical perspective, deep architectures with multiple outputs or convolutional layers represent different manifestations of kernel shape renormalization, and our work provides a dictionary that translates this physics intuition and terminology into rigorous Bayesian statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03260v3</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research 26 (2025) 1-35 Journal of Machine Learning Research 26 (2025) 1-35 http://jmlr.org/papers/v26/24-1158.html</arxiv:journal_reference>
      <dc:creator>Federico Bassetti, Marco Gherardi, Alessandro Ingrosso, Mauro Pastore, Pietro Rotondo</dc:creator>
    </item>
    <item>
      <title>Learning an Optimal Assortment Policy under Observational Data</title>
      <link>https://arxiv.org/abs/2502.06777</link>
      <description>arXiv:2502.06777v3 Announce Type: replace-cross 
Abstract: We study the fundamental problem of offline assortment optimization under the Multinomial Logit (MNL) model, where sellers must determine the optimal subset of the products to offer based solely on historical customer choice data. While most existing approaches to learning-based assortment optimization focus on the online learning of the optimal assortment through repeated interactions with customers, such exploration can be costly or even impractical in many real-world settings. In this paper, we consider the offline learning paradigm and investigate the minimal data requirements for efficient offline assortment optimization. To this end, we introduce Pessimistic Rank-Breaking (PRB), an algorithm that combines rank-breaking with pessimistic estimation. We prove that PRB is nearly minimax optimal by establishing the tight suboptimality upper bound and a nearly matching lower bound. This further shows that "optimal item coverage" - where each item in the optimal assortment appears sufficiently often in the historical data - is both sufficient and necessary for efficient offline learning. This significantly relaxes the previous requirement of observing the complete optimal assortment in the data. Our results provide fundamental insights into the data requirements for offline assortment optimization under the MNL model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06777v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxuan Han, Han Zhong, Miao Lu, Jose Blanchet, Zhengyuan Zhou</dc:creator>
    </item>
    <item>
      <title>Optimal treatment regimes for the net benefit of a treatment</title>
      <link>https://arxiv.org/abs/2503.22580</link>
      <description>arXiv:2503.22580v2 Announce Type: replace-cross 
Abstract: We develop a mathematical framework to define an optimal individualized treatment rule (ITR) within the context of prioritized outcomes in a randomized controlled trial. Our optimality criterion is based on the framework of generalized pairwise comparisons. We propose two approaches for estimating optimal ITRs on a pairwise basis. The first approach is a variant of the k-nearest neighbors algorithm. The second approach is a meta-learning method based on a randomized bagging scheme, which enables the use of any classification algorithm to construct an ITR. We investigate the theoretical properties of these estimation procedures, evaluate their performance through Monte Carlo simulations, and demonstrate their application to clinical trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22580v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Petit, G\'erard Biau, Rapha\"el Porcher</dc:creator>
    </item>
  </channel>
</rss>
