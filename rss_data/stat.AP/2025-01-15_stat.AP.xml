<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Jan 2025 02:28:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Using Statistical Precision Medicine to Identify Optimal Treatments in a Heart Failure Setting</title>
      <link>https://arxiv.org/abs/2501.07789</link>
      <description>arXiv:2501.07789v1 Announce Type: new 
Abstract: Identifying optimal medical treatments to improve survival has long been a critical goal of pharmacoepidemiology. Traditionally, we use an average treatment effect measure to compare outcomes between treatment plans. However, new methods leveraging advantages of machine learning combined with the foundational tenets of causal inference are offering an alternative to the average treatment effect. Here, we use three unique, precision medicine algorithms (random forests, residual weighted learning, efficient augmentation relaxed learning) to identify optimal treatment rules where patients receive the optimal treatment as indicated by their clinical history. First, we present a simple hypothetical example and a real-world application among heart failure patients using Medicare claims data. We next demonstrate how the optimal treatment rule improves the absolute risk in a hypothetical, three-modifier setting. Finally, we identify an optimal treatment rule that optimizes the time to outcome in a real-world heart failure setting. In both examples, we compare the average time to death under the optimized, tailored treatment rule with the average time to death under a universal treatment rule to show the benefit of precision medicine methods. The improvement under the optimal treatment rule in the real-world setting is greatest (additional ~9 days under the tailored rule) for survival time free of heart failure readmission.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07789v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arti Virkud, Jessie K. Edwards, Michele Jonsson Funk, Patricia Chang, Abhijit V. Kshirsagar, Emily W. Gower, Michael R. Kosorok</dc:creator>
    </item>
    <item>
      <title>GeoWarp: Warped spatial processes for inferring subsea sediment properties</title>
      <link>https://arxiv.org/abs/2501.07841</link>
      <description>arXiv:2501.07841v1 Announce Type: new 
Abstract: For offshore structures like wind turbines, subsea infrastructure, pipelines, and cables, it is crucial to quantify the properties of the seabed sediments at a proposed site. However, data collection offshore is costly, so analysis of the seabed sediments must be made from measurements that are spatially sparse. Adding to this challenge, the structure of the seabed sediments exhibits both nonstationarity and anisotropy. To address these issues, we propose GeoWarp, a hierarchical spatial statistical modeling framework for inferring the 3-D geotechnical properties of subsea sediments. GeoWarp decomposes the seabed properties into a region-wide vertical mean profile (modeled using B-splines), and a nonstationary 3-D spatial Gaussian process. Process nonstationarity and anisotropy are accommodated by warping space in three dimensions and by allowing the process variance to change with depth. We apply GeoWarp to measurements of the seabed made using cone penetrometer tests (CPTs) at six sites on the North West Shelf of Australia. We show that GeoWarp captures the complex spatial distribution of the sediment properties, and produces realistic 3-D simulations suitable for downstream engineering analyses. Through cross-validation, we show that GeoWarp has predictive performance superior to other state-of-the-art methods, demonstrating its value as a tool in offshore geotechnical engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07841v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Bertolacci, Andrew Zammit-Mangion, Juan Valderrama Giraldo, Michael O'Neill, Fraser Bransby, Phil Watson</dc:creator>
    </item>
    <item>
      <title>Individual causal effect estimation accounting for latent disease state modification among bipolar participants in mobile health studies</title>
      <link>https://arxiv.org/abs/2501.08270</link>
      <description>arXiv:2501.08270v1 Announce Type: new 
Abstract: Individuals with bipolar disorder tend to cycle through disease states such as depression and mania. The heterogeneous nature of disease across states complicates the evaluation of interventions for bipolar disorder patients, as varied interventional success is observed within and across individuals. In fact, we hypothesize that disease state acts as an effect modifier for the causal effect of a given intervention on health outcomes. To address this dilemma, we propose an N-of-1 approach using an adapted autoregressive hidden Markov model, applied to longitudinal mobile health data collected from individuals with bipolar disorder. This method allows us to identify a latent variable from mobile health data to be treated as an effect modifier between the exposure and outcome of interest while allowing for missing data in the outcome. A counterfactual approach is employed for causal inference and to obtain a g-formula estimator to recover said effect. The performance of the proposed method is compared with a naive approach across extensive simulations and application to a multi-year smartphone study of bipolar patients, evaluating the individual effect of digital social activity on sleep duration across different latent disease states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08270v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Charlotte R. Fowler, Xiaoxuan Cai, Habiballah Rahimi-Eichi, Lisa Dixon, Dost Ongur, Justin T. Baker, Jukka-Pekka Onnela, Linda Valeri</dc:creator>
    </item>
    <item>
      <title>Adaptive sequential Monte Carlo for automated cross validation in structural Bayesian hierarchical models</title>
      <link>https://arxiv.org/abs/2501.07685</link>
      <description>arXiv:2501.07685v1 Announce Type: cross 
Abstract: Importance sampling (IS) is widely used for approximate Bayesian cross validation (CV) due to its efficiency, requiring only the re-weighting of a single set of posterior draws. With structural Bayesian hierarchical models, vanilla IS can produce unreliable results, as out-of-sample replication may involve non-standard case-deletion schemes which significantly alter the posterior geometry. This inevitably necessitates computationally expensive re-runs of Markov chain Monte Carlo (MCMC), making structural CV impracticable. To address this challenge, we consider sampling from a sequence of posteriors leading to the case-deleted posterior(s) via adaptive sequential Monte Carlo (SMC). We design the sampler to (a) support a broad range of structural CV schemes, (b) enhance efficiency by adaptively selecting Markov kernels, intervening in parallelizable MCMC re-runs only when necessary, and (c) streamline the workflow by automating the design of intermediate bridging distributions. Its practical utility is demonstrated through three real-world applications involving three types of predictive model assessments: leave-group-out CV, group $K$-fold CV, and sequential one-step-ahead validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07685v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geonhee Han, Andrew Gelman</dc:creator>
    </item>
    <item>
      <title>Prediction Inference Using Generalized Functional Mixed Effects Models</title>
      <link>https://arxiv.org/abs/2501.07842</link>
      <description>arXiv:2501.07842v1 Announce Type: cross 
Abstract: We introduce inferential methods for prediction based on functional random effects in generalized functional mixed effects models. This is similar to the inference for random effects in generalized linear mixed effects models (GLMMs), but for functional instead of scalar outcomes. The method combines: (1) local GLMMs to extract initial estimators of the functional random components on the linear predictor scale; (2) structural functional principal components analysis (SFPCA) for dimension reduction; and (3) global Bayesian multilevel model conditional on the eigenfunctions for inference on the functional random effects. Extensive simulations demonstrate excellent coverage properties of credible intervals for the functional random effects in a variety of scenarios and for different data sizes. To our knowledge, this is the first time such simulations are conducted and reported, likely because prediction inference was not viewed as a priority and existing methods are too slow to calculate coverage. Methods are implemented in a reproducible R package and demonstrated using the NHANES 2011-2014 accelerometry data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07842v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinkai Zhou, Erjia Cui, Joseph Sartini, Ciprian Crainiceanu</dc:creator>
    </item>
    <item>
      <title>Enhanced Sparse Bayesian Learning Methods with Application to Massive MIMO Channel Estimation</title>
      <link>https://arxiv.org/abs/2501.07969</link>
      <description>arXiv:2501.07969v1 Announce Type: cross 
Abstract: We consider the problem of sparse channel estimation in massive multiple-input multiple-output systems. In this context, we propose an enhanced version of the sparse Bayesian learning (SBL) framework, referred to as enhanced SBL (E-SBL), which is based on a reparameterization of the original SBL model. Specifically, we introduce a scale vector that brings extra flexibility to the model, which is estimated along with the other unknowns. Moreover, we introduce a variant of E-SBL, referred to as modified E-SBL (M-E-SBL), which is based on a computationally more efficient parameter estimation. We compare the proposed E-SBL and M-E-SBL with the baseline SBL and with a method based on variational message passing (VMP) in terms of computational complexity and performance. Numerical results show that the proposed E-SBL and M-E-SBL outperform the baseline SBL and VMP in terms of mean squared error of the channel estimation in all the considered scenarios. Furthermore, we show that M-E-SBL produces results comparable with E-SBL with considerably cheaper computations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07969v1</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arttu Arjas, Italo Atzeni</dc:creator>
    </item>
    <item>
      <title>Evaluating Policy Effects through Network Dynamics and Sampling</title>
      <link>https://arxiv.org/abs/2501.08150</link>
      <description>arXiv:2501.08150v1 Announce Type: cross 
Abstract: In the process of enacting or introducing a new policy, policymakers frequently consider the population's responses. These considerations are critical for effective governance. There are numerous methods to gauge the ground sentiment from a subset of the population; examples include surveys or listening to various feedback channels. Many conventional approaches implicitly assume that opinions are static; however, in reality, the population will discuss and debate these new policies among themselves, and reform new opinions in the process. In this paper, we pose the following questions: Can we quantify the effect of these social dynamics on the broader opinion towards a new policy? Given some information about the relationship network that underlies the population, how does overall opinion change post-discussion? We investigate three different settings in which the policy is revealed: respondents who do not know each other, groups of respondents who all know each other, and respondents chosen randomly. By controlling who the policy is revealed to, we control the degree of discussion among the population. We quantify how these factors affect the changes in policy beliefs via the Wasserstein distance between the empirically observed data post-discussion and its distribution pre-discussion. We also provide several numerical analyses based on generated network and real-life network datasets. Our work aims to address the challenges associated with network topology and social interactions, and provide policymakers with a quantitative lens to assess policy effectiveness in the face of resource constraints and network complexities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08150v1</guid>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eugene T. Y. Ang, Yong Sheng Soh</dc:creator>
    </item>
    <item>
      <title>Fast and Cheap Covariance Smoothing</title>
      <link>https://arxiv.org/abs/2501.08265</link>
      <description>arXiv:2501.08265v1 Announce Type: cross 
Abstract: We introduce the Tensorized-and-Restricted Krylov (TReK) method, a simple and efficient algorithm for estimating covariance tensors with large observational sizes. TReK extends the conjugate gradient method to incorporate range restrictions, enabling its use in a variety of covariance smoothing applications. By leveraging matrix-level operations, it achieves significant improvements in both computational speed and memory cost, improving over existing methods by an order of magnitude. TReK ensures finite-step convergence in the absence of rounding errors and converges fast in practice, making it well-suited for large-scale problems. The algorithm is also highly flexible, supporting a wide range of forward and projection tensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08265v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ho Yun, Victor M. Panaretos</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach for Earthquake Impact Modelling</title>
      <link>https://arxiv.org/abs/2412.15791</link>
      <description>arXiv:2412.15791v2 Announce Type: replace 
Abstract: Immediately following a disaster event, such as an earthquake, estimates of the damage extent play a key role in informing the coordination of response and recovery efforts. We develop a novel impact estimation tool that leverages a generalised Bayesian approach to generate earthquake impact estimates across three impact types: mortality, population displacement, and building damage. Inference is performed within a likelihood-free framework, and a scoring-rule-based posterior avoids information loss from non-sufficient summary statistics. We propose an adaptation of existing scoring-rule-based loss functions that accommodates the use of an approximate Bayesian computation sequential Monte Carlo (ABC-SMC) framework. The fitted model achieves results comparable to those of two leading impact estimation tools in the prediction of total mortality when tested on a set of held-out past events. The proposed method provides four advantages over existing empirical approaches: modelling produces a gridded spatial map of the estimated impact, predictions benefit from the Bayesian quantification and interpretation of uncertainty, there is direct handling of multi-shock earthquake events, and the use of a joint model between impact types allows predictions to be updated as impact observations become available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15791v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Anderson Loake, Hamish Patten, David Steinsaltz</dc:creator>
    </item>
    <item>
      <title>Bind Recovery of Sparse Factor Structures by Signal Cancellation</title>
      <link>https://arxiv.org/abs/2404.03781</link>
      <description>arXiv:2404.03781v2 Announce Type: replace-cross 
Abstract: Blind factor recovery follows from the principle that the signal of variables exclusive to a factor can be combined in a contrast (weighted sum) that cancels their factor contributions, leaving only a compound of the variables unique variances. Successful contrasts, uncorrelated with any remaining variable, become the signature of factors with at least two unique indicator variables. Pairwise signal cancellation, usually incomplete for variables affected by different factors, nevertheless succeeds for variables with proportional loadings on two factors, which places three cancelling clusters in the plane of two factors. This is recognized by successful cancellation among variable triplets representing the three clusters. The Signal Cancellation Recovery of Factors (SCRoF) algorithm implements these principles, only requiring that each factor has at least two unique indicators, not even requiring having pre-estimated the number of factors. Alternate sparse factor solutions are obtained through a two significance-threshold strategy. The individually estimated factor loadings and factor correlations of each potential solution are globally optimized for maximum likelihood, yielding a chi-square indication of compatibility with observed data. SCRoF is illustrated with synthetic data from a complex six-factor structure. Actual data then document that SCRoF can even benefit confirmatory factor analysis when the initial model appears inadequate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03781v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'e Achim</dc:creator>
    </item>
    <item>
      <title>Deep Learning-Based Residual Useful Lifetime Prediction for Assets with Uncertain Failure Modes</title>
      <link>https://arxiv.org/abs/2405.06068</link>
      <description>arXiv:2405.06068v2 Announce Type: replace-cross 
Abstract: Industrial prognostics focuses on utilizing degradation signals to forecast and continually update the residual useful life of complex engineering systems. However, existing prognostic models for systems with multiple failure modes face several challenges in real-world applications, including overlapping degradation signals from multiple components, the presence of unlabeled historical data, and the similarity of signals across different failure modes. To tackle these issues, this research introduces two prognostic models that integrate the mixture (log)-location-scale distribution with deep learning. This integration facilitates the modeling of overlapping degradation signals, eliminates the need for explicit failure mode identification, and utilizes deep learning to capture complex nonlinear relationships between degradation signals and residual useful lifetimes. Numerical studies validate the superior performance of these proposed models compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06068v2</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuqi Su, Xiaolei Fang</dc:creator>
    </item>
    <item>
      <title>Hierarchical Latent Class Models for Mortality Surveillance Using Partially Verified Verbal Autopsies</title>
      <link>https://arxiv.org/abs/2410.09274</link>
      <description>arXiv:2410.09274v2 Announce Type: replace-cross 
Abstract: Monitoring data on causes of death is an important part of understanding the burden of diseases and the effects of public health interventions. Verbal autopsy (VA) is a well-established method for gathering information about deaths outside of hospitals by conducting an interview to family members or caregivers of a deceased person, and is usually the only tool for cause-of-death surveillance in low-resource settings. A critical limitation with the current practice of VA analysis is that all algorithms require either highly informative domain knowledge about symptom-cause relationships or large labeled datasets for model training. Therefore, they cannot be quickly adopted during public health emergencies when new diseases emerge with rapidly evolving epidemiological patterns. In this paper, we consider the task of estimating the fraction of deaths due to an emerging disease using continuously collected VAs where causes of death are only partially verified. We develop a novel Bayesian framework using a hierarchical latent class model to account for the informative verification process. Our model flexibly captures the joint distribution of symptoms and how they change over time in different sub-populations. We also propose structured priors to further improve the precision of prevalence estimation for small sub-populations. Our model is motivated by mortality surveillance of COVID-19 related deaths in low-resource settings. We apply our method to a dataset that includes suspected COVID-19 related deaths in Brazil in 2021. We show that standard modeling approaches can be severely biased under selective verification and our model leads to more robust and accurate quantification of disease prevalence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09274v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Zhu, Zehang Richard Li</dc:creator>
    </item>
    <item>
      <title>Optimal Sampling for Generalized Linear Model under Measurement Constraint with Surrogate Variables</title>
      <link>https://arxiv.org/abs/2501.00972</link>
      <description>arXiv:2501.00972v2 Announce Type: replace-cross 
Abstract: Measurement-constrained datasets, often encountered in semi-supervised learning, arise when data labeling is costly, time-intensive, or hindered by confidentiality or ethical concerns, resulting in a scarcity of labeled data. In certain cases, surrogate variables are accessible across the entire dataset and can serve as approximations to the true response variable; however, these surrogates often contain measurement errors and thus cannot be directly used for accurate prediction. We propose an optimal sampling strategy that effectively harnesses the available information from surrogate variables. This approach provides consistent estimators under the assumption of a generalized linear model, achieving theoretically lower asymptotic variance than existing optimal sampling algorithms that do not use surrogate data information. By employing the A-optimality criterion from optimal experimental design, our strategy maximizes statistical efficiency. Numerical studies demonstrate that our approach surpasses existing optimal sampling methods, exhibiting reduced empirical mean squared error and enhanced robustness in algorithmic performance. These findings highlight the practical advantages of our strategy in scenarios where measurement constraints exist and surrogates are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00972v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yixin Shen, Yang Ning</dc:creator>
    </item>
  </channel>
</rss>
