<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Nov 2024 05:00:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Future-Proofing IoT: Unleashing the Power of AWS Greengrass in Propelling Smart Devices to New Heights</title>
      <link>https://arxiv.org/abs/2411.08914</link>
      <description>arXiv:2411.08914v1 Announce Type: new 
Abstract: The advent of edge computing is set to revolutionize cloud computing in various sectors, including Agriculture, Health, and more. AWS Greengrass Core Device plays a pivotal role in this transformative process by bridging connections between IoT devices by improving data sharing between them, unlocking new possibilities in Smart Home, Agriculture, Health, Vehicular Cloud, Smart City, Industry Automation, and beyond. However, these advancements also introduce novel challenges for testing and quality assurance in cloud computing. This paper explores the impact of AWS Greengrass in different fields, addressing challenges, opportunities, and potential benefits of edge and cloud computing in terms of processing speed, latency, and bandwidth usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08914v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahasra Kokkula, Ankit Vatsa, Savaridassan P</dc:creator>
    </item>
    <item>
      <title>Enhancing Scalability and Performance in Influence Maximization with Optimized Parallel Processing</title>
      <link>https://arxiv.org/abs/2411.09473</link>
      <description>arXiv:2411.09473v1 Announce Type: new 
Abstract: Influence Maximization (IM) is vital in viral marketing and biological network analysis for identifying key influencers. Given its NP-hard nature, approximate solutions are employed. This paper addresses scalability challenges in scale-out shared memory system by focusing on the state-of-the-art Influence Maximization via Martingales (IMM) benchmark. To enhance the work efficiency of the current IMM implementation, we propose EFFICIENTIMM with key strategies, including new parallelization scheme, NUMA-aware memory usage, dynamic load balancing and fine-grained adaptive data structures. Benchmarking on a 128-core CPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance improvements, achieving an average 5.9x speedup over Ripples across 8 diverse SNAP datasets, when compared to the best execution times of the original Ripples framework. Additionally, on the Youtube graph, EFFICIENTIMM demonstrates a better memory access pattern with 357.4x reduction in L1+L2 cache misses as compared to Ripples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09473v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanjiang Wu, Huan Xu, Joongun Park, Jesmin Jahan Tithi, Fabio Checconi, Jordi Wolfson-Pou, Fabrizio Petrini, Tushar Krishna</dc:creator>
    </item>
    <item>
      <title>Lynx: Enabling Efficient MoE Inference through Dynamic Batch-Aware Expert Selection</title>
      <link>https://arxiv.org/abs/2411.08982</link>
      <description>arXiv:2411.08982v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) architectures have recently gained popularity in enabling efficient scaling of large language models. However, we uncover a fundamental tension: while MoEs are designed for selective expert activation, production serving requires request batching, which forces the activation of all experts and negates MoE's efficiency benefits during the decode phase. We present Lynx, a system that enables efficient MoE inference through dynamic, batch-aware expert selection. Our key insight is that expert importance varies significantly across tokens and inference phases, creating opportunities for runtime optimization. Lynx leverages this insight through a lightweight framework that dynamically reduces active experts while preserving model accuracy. Our evaluations show that Lynx achieves up to 1.55x reduction in inference latency while maintaining negligible accuracy loss from baseline model across complex code generation and mathematical reasoning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08982v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vima Gupta, Kartik Sinha, Ada Gavrilovska, Anand Padmanabha Iyer</dc:creator>
    </item>
    <item>
      <title>Anomaly Detection in Large-Scale Cloud Systems: An Industry Case and Dataset</title>
      <link>https://arxiv.org/abs/2411.09047</link>
      <description>arXiv:2411.09047v1 Announce Type: cross 
Abstract: As Large-Scale Cloud Systems (LCS) become increasingly complex, effective anomaly detection is critical for ensuring system reliability and performance. However, there is a shortage of large-scale, real-world datasets available for benchmarking anomaly detection methods.
  To address this gap, we introduce a new high-dimensional dataset from IBM Cloud, collected over 4.5 months from the IBM Cloud Console. This dataset comprises 39,365 rows and 117,448 columns of telemetry data. Additionally, we demonstrate the application of machine learning models for anomaly detection and discuss the key challenges faced in this process.
  This study and the accompanying dataset provide a resource for researchers and practitioners in cloud system monitoring. It facilitates more efficient testing of anomaly detection methods in real-world data, helping to advance the development of robust solutions to maintain the health and performance of large-scale cloud infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09047v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Saiful Islam, Mohamed Sami Rakha, William Pourmajidi, Janakan Sivaloganathan, John Steinbacher, Andriy Miranskyy</dc:creator>
    </item>
    <item>
      <title>Pkd-tree: Parallel $k$d-tree with Batch Updates</title>
      <link>https://arxiv.org/abs/2411.09275</link>
      <description>arXiv:2411.09275v1 Announce Type: cross 
Abstract: The $k$d-tree is one of the most widely used data structures to manage multi-dimensional data. Due to the ever-growing data volume, it is imperative to consider parallelism in $k$d-trees. However, we observed challenges in existing parallel kd-tree implementations, for both constructions and updates.
  The goal of this paper is to develop efficient in-memory $k$d-trees by supporting high parallelism and cache-efficiency. We propose the Pkd-tree (Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and in practice. The Pkd-tree supports parallel tree construction, batch update (insertion and deletion), and various queries including k-nearest neighbor search, range query, and range count. We proved that our algorithms have strong theoretical bounds in work (sequential time complexity), span (parallelism), and cache complexity. Our key techniques include 1) an efficient construction algorithm that optimizes work, span, and cache complexity simultaneously, and 2) reconstruction-based update algorithms that guarantee the tree to be weight-balanced. With the new algorithmic insights and careful engineering effort, we achieved a highly optimized implementation of the Pkd-tree.
  We tested Pkd-tree with various synthetic and real-world datasets, including both uniform and highly skewed data. We compare the Pkd-tree with state-of-the-art parallel $k$d-tree implementations. In all tests, with better or competitive query performance, Pkd-tree is much faster in construction and updates consistently than all baselines. We released our code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09275v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Men, Zheqi Shen, Yan Gu, Yihan Sun</dc:creator>
    </item>
    <item>
      <title>Pie: Pooling CPU Memory for LLM Inference</title>
      <link>https://arxiv.org/abs/2411.09317</link>
      <description>arXiv:2411.09317v1 Announce Type: cross 
Abstract: The rapid growth of LLMs has revolutionized natural language processing and AI analysis, but their increasing size and memory demands present significant challenges. A common solution is to spill over to CPU memory; however, traditional GPU-CPU memory swapping often results in higher latency and lower throughput.
  This paper introduces Pie, an LLM inference framework that addresses these challenges with performance-transparent swapping and adaptive expansion. By leveraging predictable memory access patterns and the high bandwidth of modern hardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent data swapping without affecting foreground computation, expanding effective memory without added latency. Adaptive expansion dynamically adjusts CPU memory allocation based on real-time information, optimizing memory usage and performance under varying conditions.
  Pie maintains low computation latency, high throughput, and high elasticity. Our experimental evaluation demonstrates that Pie achieves optimal swapping policy during cache warmup and effectively balances increased memory capacity with negligible impact on computation. With its extended capacity, Pie outperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally, Pie can reduce GPU memory usage by up to 1.67X while maintaining the same performance. Compared to FlexGen, an offline profiling-based swapping solution, Pie achieves magnitudes lower latency and 9.4X higher throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09317v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Xu, Ziming Mao, Xiangxi Mo, Shu Liu, Ion Stoica</dc:creator>
    </item>
    <item>
      <title>Perpetual Exploration of a Ring in Presence of Byzantine Black Hole</title>
      <link>https://arxiv.org/abs/2407.05280</link>
      <description>arXiv:2407.05280v2 Announce Type: replace 
Abstract: Perpetual exploration is a fundamental problem in the domain of mobile agents, where an agent needs to visit each node infinitely often. This issue has received lot of attention, mainly for ring topologies, presence of black holes adds more complexity. A black hole can destroy any incoming agent without any observable trace. In \cite{BampasImprovedPeriodicDataRetrieval,KralovivcPeriodicDataRetrievalFirst}, the authors considered this problem in the context of \textit{ Periodic data retrieval}. They introduced a variant of black hole called gray hole (where the adversary chooses whether to destroy an agent or let it pass) among others and showed that 4 asynchronous and co-located agents are essential to solve this problem (hence perpetual exploration) in presence of such a gray hole if each node of the ring has a whiteboard. This paper investigates the exploration of a ring in presence of a ``byzantine black hole''. In addition to the capabilities of a gray hole, in this variant, the adversary chooses whether to erase any previously stored information on that node. Previously, one particular initial scenario (i.e., agents are co-located) and one particular communication model (i.e., whiteboard) are investigated. Now, there can be other initial scenarios where all agents may not be co-located. Also, there are many weaker models of communications (i.e., Face-to-Face, Pebble) where this problem is yet to be investigated. The agents are synchronous. The main results focus on minimizing the agent number while ensuring that perpetual exploration is achieved even in presence of such a node under various communication models and starting positions. Further, we achieved a better upper and lower bound result (i.e., 3 agents) for this problem (where the malicious node is a generalized version of a gray hole), by trading-off scheduler capability, for co-located and in presence of a whiteboard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05280v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pritam Goswami, Adri Bhattacharya, Raja Das, Partha Sarathi Mandal</dc:creator>
    </item>
    <item>
      <title>Cephalo: Harnessing Heterogeneous GPU Clusters for Training Transformer Models</title>
      <link>https://arxiv.org/abs/2411.01075</link>
      <description>arXiv:2411.01075v2 Announce Type: replace 
Abstract: Training transformer models requires substantial GPU compute and memory resources. In homogeneous clusters, distributed strategies allocate resources evenly, but this approach is inefficient for heterogeneous clusters, where GPUs differ in power and memory. As high-end GPUs are costly and limited in availability, heterogeneous clusters with diverse GPU types are becoming more common. Existing methods attempt to balance compute across GPUs based on capacity but often underutilize compute due to memory constraints. We present Cephalo, a system that optimizes compute and memory usage by decoupling compute distribution from training state assignment. Cephalo outperforms state-of-the-art methods by achieving significantly higher training throughput while supporting larger models and batch sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01075v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Runsheng Benson Guo, Utkarsh Anand, Arthur Chen, Khuzaima Daudjee</dc:creator>
    </item>
    <item>
      <title>Topological Characterization of Stabilizing Consensus</title>
      <link>https://arxiv.org/abs/2411.07106</link>
      <description>arXiv:2411.07106v2 Announce Type: replace 
Abstract: We provide a complete characterization of the solvability/impossibility of deterministic stabilizing consensus in any computing model with benign process and communication faults using point-set topology. Relying on the topologies for infinite executions introduced by Nowak, Schmid and Winkler (JACM, 2024) for terminating consensus, we prove that semi-open decision sets and semi-continuous decision functions as introduced by Levin (AMM, 1963) are the appropriate means for this characterization: Unlike the decision functions for terminating consensus, which are continuous, semi-continuous functions do not require the inverse image of an open set to be open and hence allow to map a connected space to a disconnected one. We also show that multi-valued stabilizing consensus with weak and strong validity are equivalent, as is the case for terminating consensus. By applying our results to (variants of) all the known possibilities/impossibilities for stabilizing consensus, we easily provide a topological explanation of these results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07106v2</guid>
      <category>cs.DC</category>
      <category>math.GN</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ulrich Schmid, Stephan Felber, Hugo Rincon-Galeana</dc:creator>
    </item>
    <item>
      <title>Dual-Segment Clustering Strategy for Hierarchical Federated Learning in Heterogeneous Wireless Environments</title>
      <link>https://arxiv.org/abs/2405.09276</link>
      <description>arXiv:2405.09276v2 Announce Type: replace-cross 
Abstract: Non-independent and identically distributed (Non- IID) data adversely affects federated learning (FL) while heterogeneity in communication quality can undermine the reliability of model parameter transmission, potentially degrading wireless FL convergence. This paper proposes a novel dual-segment clustering (DSC) strategy that jointly addresses communication and data heterogeneity in FL. This is achieved by defining a new signal-to-noise ratio (SNR) matrix and information quantity matrix to capture the communication and data heterogeneity, respectively. The celebrated affinity propagation algorithm is leveraged to iteratively refine the clustering of clients based on the newly defined matrices effectively enhancing model aggregation in heterogeneous environments. The convergence analysis and experimental results show that the DSC strategy can improve the convergence rate of wireless FL and demonstrate superior accuracy in heterogeneous environments compared to classical clustering methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09276v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengcheng Sun, Erwu Liu, Wei Ni, Kanglei Yu, Xinyu Qu, Rui Wang, Yanlong Bi, Chuanchun Zhang, Abbas Jamalipour</dc:creator>
    </item>
  </channel>
</rss>
