<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Oct 2024 03:31:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Adaptive sparsening and smoothing of the treatment model for longitudinal causal inference using outcome-adaptive LASSO and marginal fused LASSO</title>
      <link>https://arxiv.org/abs/2410.08283</link>
      <description>arXiv:2410.08283v1 Announce Type: new 
Abstract: Causal variable selection in time-varying treatment settings is challenging due to evolving confounding effects. Existing methods mainly focus on time-fixed exposures and are not directly applicable to time-varying scenarios. We propose a novel two-step procedure for variable selection when modeling the treatment probability at each time point. We first introduce a novel approach to longitudinal confounder selection using a Longitudinal Outcome Adaptive LASSO (LOAL) that will data-adaptively select covariates with theoretical justification of variance reduction of the estimator of the causal effect. We then propose an Adaptive Fused LASSO that can collapse treatment model parameters over time points with the goal of simplifying the models in order to improve the efficiency of the estimator while minimizing model misspecification bias compared with naive pooled logistic regression models. Our simulation studies highlight the need for and usefulness of the proposed approach in practice. We implemented our method on data from the Nicotine Dependence in Teens study to estimate the effect of the timing of alcohol initiation during adolescence on depressive symptoms in early adulthood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08283v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mireille E Schnitzer, Denis Talbot, Yan Liu, David Berger, Guanbo Wang, Jennifer O'Loughlin, Marie-Pierre Sylvestre, Ashkan Ertefaie</dc:creator>
    </item>
    <item>
      <title>Bivariate Variable Ranking for censored time-to-event data via Copula Link Based Additive models</title>
      <link>https://arxiv.org/abs/2410.08382</link>
      <description>arXiv:2410.08382v1 Announce Type: new 
Abstract: In this paper, we present a variable ranking approach established on a novel measure to select important variables in bivariate Copula Link-Based Additive Models (Marra &amp; Radice, 2020). The proposal allows for identifying two sets of relevant covariates for the two time-to-events without neglecting the dependency structure that may exist between the two survivals. The procedure suggested is evaluated via a simulation study and then is applied for analyzing the Age-Related Eye Disease Study dataset. The algorithm is implemented in a new R package, called BRBVS..</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08382v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danilo Petti, Marcella Niglio, Marialuisa Restaino</dc:creator>
    </item>
    <item>
      <title>Principal Component Analysis in the Graph Frequency Domain</title>
      <link>https://arxiv.org/abs/2410.08422</link>
      <description>arXiv:2410.08422v1 Announce Type: new 
Abstract: We propose a novel principal component analysis in the graph frequency domain for dimension reduction of multivariate data residing on graphs. The proposed method not only effectively reduces the dimensionality of multivariate graph signals, but also provides a closed-form reconstruction of the original data. In addition, we investigate several propositions related to principal components and the reconstruction errors, and introduce a graph spectral envelope that aids in identifying common graph frequencies in multivariate graph signals. We demonstrate the validity of the proposed method through a simulation study and further analyze the boarding and alighting patterns of Seoul Metropolitan Subway passengers using the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08422v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyusoon Kim, Hee-Seok Oh</dc:creator>
    </item>
    <item>
      <title>A scientific review on advances in statistical methods for crossover design</title>
      <link>https://arxiv.org/abs/2410.08441</link>
      <description>arXiv:2410.08441v1 Announce Type: new 
Abstract: A comprehensive review of the literature on crossover design is needed to highlight its evolution, applications, and methodological advancements across various fields. Given its widespread use in clinical trials and other research domains, understanding this design's challenges, assumptions, and innovations is essential for optimizing its implementation and ensuring accurate, unbiased results. This article extensively reviews the history and statistical inference methods for crossover designs. A primary focus is given to the AB-BA design as it is the most widely used design in literature. Extension from two periods to higher-order designs is discussed, and a general inference procedure for continuous response is studied. Analysis of multivariate and categorical responses is also reviewed in this context. A bunch of open problems in this area are shortlisted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08441v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Salil Koner</dc:creator>
    </item>
    <item>
      <title>Fractional binomial regression model for count data with excess zeros</title>
      <link>https://arxiv.org/abs/2410.08488</link>
      <description>arXiv:2410.08488v1 Announce Type: new 
Abstract: This paper proposes a new generalized linear model with fractional binomial distribution.
  Zero-inflated Poisson/negative binomial distributions are used for count data that has many zeros. To analyze the association of such a count variable with covariates, zero-inflated Poisson/negative binomial regression models are widely used. In this work, we develop a regression model with the fractional binomial distribution that can serve as an additional tool for modeling the count response variable with covariates. Data analysis results show that on some occasions, our model outperforms the existing zero-inflated regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08488v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Chloe Breece, Jeonghwa Lee</dc:creator>
    </item>
    <item>
      <title>Exact MLE for Generalized Linear Mixed Models</title>
      <link>https://arxiv.org/abs/2410.08492</link>
      <description>arXiv:2410.08492v1 Announce Type: new 
Abstract: Exact MLE for generalized linear mixed models (GLMMs) is a long-standing problem unsolved until today. The proposed research solves the problem. In this problem, the main difficulty is caused by intractable integrals in the likelihood function when the response does not follow normal and the prior distribution for the random effects is specified by normal. Previous methods use Laplace approximations or Monte Carol simulations to compute the MLE approximately. These methods cannot provide the exact MLEs of the parameters and the hyperparameters. The exact MLE problem remains unsolved until the proposed work. The idea is to construct a sequence of mathematical functions in the optimization procedure. Optimization of the mathematical functions can be numerically computed. The result can lead to the exact MLEs of the parameters and hyperparameters. Because computing the likelihood is unnecessary, the proposed method avoids the main difficulty caused by the intractable integrals in the likelihood function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08492v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tonglin Zhang</dc:creator>
    </item>
    <item>
      <title>Parametric multi-fidelity Monte Carlo estimation with applications to extremes</title>
      <link>https://arxiv.org/abs/2410.08523</link>
      <description>arXiv:2410.08523v1 Announce Type: new 
Abstract: In a multi-fidelity setting, data are available from two sources, high- and low-fidelity. Low-fidelity data has larger size and can be leveraged to make more efficient inference about quantities of interest, e.g. the mean, for high-fidelity variables. In this work, such multi-fidelity setting is studied when the goal is to fit more efficiently a parametric model to high-fidelity data. Three multi-fidelity parameter estimation methods are considered, joint maximum likelihood, (multi-fidelity) moment estimation and (multi-fidelity) marginal maximum likelihood, and are illustrated on several parametric models, with the focus on parametric families used in extreme value analysis. An application is also provided concerning quantification of occurrences of extreme ship motions generated by two computer codes of varying fidelity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08523v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minji Kim, Brendan Brown, Vladas Pipiras</dc:creator>
    </item>
    <item>
      <title>An alignment-agnostic methodology for the analysis of designed separations data</title>
      <link>https://arxiv.org/abs/2410.08733</link>
      <description>arXiv:2410.08733v1 Announce Type: new 
Abstract: Chemical separations data are typically analysed in the time domain using methods that integrate the discrete elution bands. Integrating the same chemical components across several samples must account for retention time drift over the course of an entire experiment as the physical characteristics of the separation are altered through several cycles of use. Failure to consistently integrate the components within a matrix of $M \times N$ samples and variables create artifacts that have a profound effect on the analysis and interpretation of the data. This work presents an alternative where the raw separations data are analysed in the frequency domain to account for the offset of the chromatographic peaks as a matrix of complex Fourier coefficients. We present a generalization of the permutation testing, and visualization steps in ANOVA-Simultaneous Component Analysis (ASCA) to handle complex matrices, and use this method to analyze a synthetic dataset with known significant factors and compare the interpretation of a real dataset via its peak table and frequency domain representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08733v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Sorochan Armstrong, Jos\'e Camacho</dc:creator>
    </item>
    <item>
      <title>Half-KFN: An Enhanced Detection Method for Subtle Covariate Drift</title>
      <link>https://arxiv.org/abs/2410.08782</link>
      <description>arXiv:2410.08782v1 Announce Type: new 
Abstract: Detecting covariate drift is a common task of significant practical value in supervised learning. Once covariate drift occurs, the models may no longer be applicable, hence numerous studies have been devoted to the advancement of detection methods. However, current research methods are not particularly effective in handling subtle covariate drift when dealing with small proportions of drift samples. In this paper, inspired by the $k$-nearest neighbor (KNN) approach, a novel method called Half $k$-farthest neighbor (Half-KFN) is proposed in response to specific scenarios. Compared to traditional ones, Half-KFN exhibits higher power due to the inherent capability of the farthest neighbors which could better characterize the nature of drift. Furthermore, with larger sample sizes, the employment of the bootstrap for hypothesis testing is recommended. It is leveraged to calculate $p$-values dramatically faster than permutation tests, with speed undergoing an exponential growth as sample size increases. Numerical experiments on simulated and real data are conducted to evaluate our proposed method, and the results demonstrate that it consistently displays superior sensitivity and rapidity in covariate drift detection across various cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08782v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingbing Wang, Dong Xu, Yu Tang</dc:creator>
    </item>
    <item>
      <title>Generalised logistic regression with vine copulas</title>
      <link>https://arxiv.org/abs/2410.08803</link>
      <description>arXiv:2410.08803v1 Announce Type: new 
Abstract: We propose a generalisation of the logistic regression model, that aims to account for non-linear main effects and complex interactions, while keeping the model inherently explainable. This is obtained by starting with log-odds that are linear in the covariates, and adding non-linear terms that depend on at least two covariates. More specifically, we use a generative specification of the model, consisting of a combination of certain margins on natural exponential form, combined with vine copulas. The estimation of the model is however based on the discriminative likelihood, and dependencies between covariates are included in the model, only if they contribute significantly to the distinction between the two classes. Further, a scheme for model selection and estimation is presented. The methods described in this paper are implemented in the R package LogisticCopula. In order to assess the performance of our model, we ran an extensive simulation study. The results from the study, as well as from a couple of examples on real data, showed that our model performs at least as well as natural competitors, especially in the presence of non-linearities and complex interactions, even when $n$ is not large compared to $p$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08803v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ingrid Hob{\ae}k Haff, Simon Boge Brant, Haakon Bakka</dc:creator>
    </item>
    <item>
      <title>Causal inference targeting a concentration index for studies of health inequalities</title>
      <link>https://arxiv.org/abs/2410.08849</link>
      <description>arXiv:2410.08849v1 Announce Type: new 
Abstract: A concentration index, a standardized covariance between a health variable and relative income ranks, is often used to quantify income-related health inequalities. There is a lack of formal approach to study the effect of an exposure, e.g., education, on such measures of inequality. In this paper we contribute by filling this gap and developing the necessary theory and method. Thus, we define a counterfactual concentration index for different levels of an exposure. We give conditions for their identification, and then deduce their efficient influence function. This allows us to propose estimators, which are regular asymptotic linear under certain conditions. In particular, these estimators are $\sqrt n$-consistent and asymptotically normal, as well as locally efficient. The implementation of the estimators is based on the fit of several nuisance functions. The estimators proposed have rate robustness properties allowing for convergence rates slower than $\sqrt{n}$-rate for some of the nuisance function fits. The relevance of the asymptotic results for finite samples is studied with simulation experiments. We also present a case study of the effect of education on income-related health inequalities for a Swedish cohort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08849v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Ghasempour, Xavier de Luna, Per E. Gustafsson</dc:creator>
    </item>
    <item>
      <title>Variance reduction combining pre-experiment and in-experiment data</title>
      <link>https://arxiv.org/abs/2410.09027</link>
      <description>arXiv:2410.09027v1 Announce Type: new 
Abstract: Online controlled experiments (A/B testing) are essential in data-driven decision-making for many companies. Increasing the sensitivity of these experiments, particularly with a fixed sample size, relies on reducing the variance of the estimator for the average treatment effect (ATE). Existing methods like CUPED and CUPAC use pre-experiment data to reduce variance, but their effectiveness depends on the correlation between the pre-experiment data and the outcome. In contrast, in-experiment data is often more strongly correlated with the outcome and thus more informative. In this paper, we introduce a novel method that combines both pre-experiment and in-experiment data to achieve greater variance reduction than CUPED and CUPAC, without introducing bias or additional computation complexity. We also establish asymptotic theory and provide consistent variance estimators for our method. Applying this method to multiple online experiments at Etsy, we reach substantial variance reduction over CUPAC with the inclusion of only a few in-experiment covariates. These results highlight the potential of our approach to significantly improve experiment sensitivity and accelerate decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09027v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhexiao Lin, Pablo Crespo</dc:creator>
    </item>
    <item>
      <title>Semi-Supervised Learning of Noisy Mixture of Experts Models</title>
      <link>https://arxiv.org/abs/2410.09039</link>
      <description>arXiv:2410.09039v1 Announce Type: new 
Abstract: The mixture of experts (MoE) model is a versatile framework for predictive modeling that has gained renewed interest in the age of large language models. A collection of predictive ``experts'' is learned along with a ``gating function'' that controls how much influence each expert is given when a prediction is made. This structure allows relatively simple models to excel in complex, heterogeneous data settings. In many contemporary settings, unlabeled data are widely available while labeled data are difficult to obtain. Semi-supervised learning methods seek to leverage the unlabeled data. We propose a novel method for semi-supervised learning of MoE models. We start from a semi-supervised MoE model that was developed by oceanographers that makes the strong assumption that the latent clustering structure in unlabeled data maps directly to the influence that the gating function should give each expert in the supervised task. We relax this assumption, imagining a noisy connection between the two, and propose an algorithm based on least trimmed squares, which succeeds even in the presence of misaligned data. Our theoretical analysis characterizes the conditions under which our approach yields estimators with a near-parametric rate of convergence. Simulated and real data examples demonstrate the method's efficacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09039v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oh-Ran Kwon, Gourab Mukherjee, Jacob Bien</dc:creator>
    </item>
    <item>
      <title>Towards Optimal Environmental Policies: Policy Learning under Arbitrary Bipartite Network Interference</title>
      <link>https://arxiv.org/abs/2410.08362</link>
      <description>arXiv:2410.08362v1 Announce Type: cross 
Abstract: The substantial effect of air pollution on cardiovascular disease and mortality burdens is well-established. Emissions-reducing interventions on coal-fired power plants -- a major source of hazardous air pollution -- have proven to be an effective, but costly, strategy for reducing pollution-related health burdens. Targeting the power plants that achieve maximum health benefits while satisfying realistic cost constraints is challenging. The primary difficulty lies in quantifying the health benefits of intervening at particular plants. This is further complicated because interventions are applied on power plants, while health impacts occur in potentially distant communities, a setting known as bipartite network interference (BNI). In this paper, we introduce novel policy learning methods based on Q- and A-Learning to determine the optimal policy under arbitrary BNI. We derive asymptotic properties and demonstrate finite sample efficacy in simulations. We apply our novel methods to a comprehensive dataset of Medicare claims, power plant data, and pollution transport networks. Our goal is to determine the optimal strategy for installing power plant scrubbers to minimize ischemic heart disease (IHD) hospitalizations under various cost constraints. We find that annual IHD hospitalization rates could be reduced in a range from 20.66-44.51 per 10,000 person-years through optimal policies under different cost constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08362v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Raphael C. Kim, Falco J. Bargagli-Stoffi, Kevin L. Chen, Rachel C. Nethery</dc:creator>
    </item>
    <item>
      <title>Deep Generative Quantile Bayes</title>
      <link>https://arxiv.org/abs/2410.08378</link>
      <description>arXiv:2410.08378v1 Announce Type: cross 
Abstract: We develop a multivariate posterior sampling procedure through deep generative quantile learning. Simulation proceeds implicitly through a push-forward mapping that can transform i.i.d. random vector samples from the posterior. We utilize Monge-Kantorovich depth in multivariate quantiles to directly sample from Bayesian credible sets, a unique feature not offered by typical posterior sampling methods. To enhance the training of the quantile mapping, we design a neural network that automatically performs summary statistic extraction. This additional neural network structure has performance benefits, including support shrinkage (i.e., contraction of our posterior approximation) as the observation sample size increases. We demonstrate the usefulness of our approach on several examples where the absence of likelihood renders classical MCMC infeasible. Finally, we provide the following frequentist theoretical justifications for our quantile learning framework: {consistency of the estimated vector quantile, of the recovered posterior distribution, and of the corresponding Bayesian credible sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08378v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jungeum Kim, Percy S. Zhai, Veronika Ro\v{c}kov\'a</dc:creator>
    </item>
    <item>
      <title>Change-point detection in regression models for ordered data via the max-EM algorithm</title>
      <link>https://arxiv.org/abs/2410.08574</link>
      <description>arXiv:2410.08574v1 Announce Type: cross 
Abstract: We consider the problem of breakpoint detection in a regression modeling framework. To that end, we introduce a novel method, the max-EM algorithm which combines a constrained Hidden Markov Model with the Classification-EM (CEM) algorithm. This algorithm has linear complexity and provides accurate breakpoints detection and parameter estimations. We derive a theoretical result that shows that the likelihood of the data as a function of the regression parameters and the breakpoints location is increased at each step of the algorithm. We also present two initialization methods for the location of the breakpoints in order to deal with local maxima issues. Finally, a statistical test in the one breakpoint situation is developed. Simulation experiments based on linear, logistic, Poisson and Accelerated Failure Time regression models show that the final method that includes the initialization procedure and the max-EM algorithm has a strong performance both in terms of parameters estimation and breakpoints detection. The statistical test is also evaluated and exhibits a correct rejection rate under the null hypothesis and a strong power under various alternatives. Two real dataset are analyzed, the UCI bike sharing and the health disease data, where the interest of the method to detect heterogeneity in the distribution of the data is illustrated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08574v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Modibo Diabat\'e (UPCit\'e, MAP5 - UMR 8145), Gr\'egory Nuel (SU, LPSM), Olivier Bouaziz (UPCit\'e, MAP5 - UMR 8145)</dc:creator>
    </item>
    <item>
      <title>Distribution-free uncertainty quantification for inverse problems: application to weak lensing mass mapping</title>
      <link>https://arxiv.org/abs/2410.08831</link>
      <description>arXiv:2410.08831v1 Announce Type: cross 
Abstract: In inverse problems, distribution-free uncertainty quantification (UQ) aims to obtain error bars with coverage guarantees that are independent of any prior assumptions about the data distribution. In the context of mass mapping, uncertainties could lead to errors that affects our understanding of the underlying mass distribution, or could propagate to cosmological parameter estimation, thereby impacting the precision and reliability of cosmological models. Current surveys, such as Euclid or Rubin, will provide new weak lensing datasets of very high quality. Accurately quantifying uncertainties in mass maps is therefore critical to perform reliable cosmological parameter inference. In this paper, we extend the conformalized quantile regression (CQR) algorithm, initially proposed for scalar regression, to inverse problems. We compare our approach with another distribution-free approach based on risk-controlling prediction sets (RCPS). Both methods are based on a calibration dataset, and offer finite-sample coverage guarantees that are independent of the data distribution. Furthermore, they are applicable to any mass mapping method, including blackbox predictors. In our experiments, we apply UQ on three mass-mapping method: the Kaiser-Squires inversion, iterative Wiener filtering, and the MCALens algorithm. Our experiments reveal that RCPS tends to produce overconservative confidence bounds with small calibration sets, whereas CQR is designed to avoid this issue. Although the expected miscoverage rate is guaranteed to stay below a user-prescribed threshold regardless of the mass mapping method, selecting an appropriate reconstruction algorithm remains crucial for obtaining accurate estimates, especially around peak-like structures, which are particularly important for inferring cosmological parameters. Additionally, the choice of mass mapping method influences the size of the error bars.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08831v1</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hubert Leterme, Jalal Fadili, Jean-Luc Starck</dc:creator>
    </item>
    <item>
      <title>Linear-cost unbiased posterior estimates for crossed effects and matrix factorization models via couplings</title>
      <link>https://arxiv.org/abs/2410.08939</link>
      <description>arXiv:2410.08939v1 Announce Type: cross 
Abstract: We design and analyze unbiased Markov chain Monte Carlo (MCMC) schemes based on couplings of blocked Gibbs samplers (BGSs), whose total computational costs scale linearly with the number of parameters and data points. Our methodology is designed for and applicable to high-dimensional BGS with conditionally independent blocks, which are often encountered in Bayesian modeling. We provide bounds on the expected number of iterations needed for coalescence for Gaussian targets, which imply that practical two-step coupling strategies achieve coalescence times that match the relaxation times of the original BGS scheme up to a logarithmic factor. To illustrate the practical relevance of our methodology, we apply it to high-dimensional crossed random effect and probabilistic matrix factorization models, for which we develop a novel BGS scheme with improved convergence speed. Our methodology provides unbiased posterior estimates at linear cost (usually requiring only a few BGS iterations for problems with thousands of parameters), matching state-of-the-art procedures for both frequentist and Bayesian estimation of those models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08939v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paolo Maria Ceriani (Department of Decision Sciences, Bocconi University, Milan, Italy), Giacomo Zanella (Department of Decision Sciences, Bocconi University, Milan, Italy, Bocconi Institute for Data Science and Analytics, Bocconi University, Milan, Italy)</dc:creator>
    </item>
    <item>
      <title>A Flexible Quasi-Copula Distribution for Statistical Modeling</title>
      <link>https://arxiv.org/abs/2205.03505</link>
      <description>arXiv:2205.03505v2 Announce Type: replace 
Abstract: Copulas, generalized estimating equations, and generalized linear mixed models promote the analysis of grouped data where non-normal responses are correlated. Unfortunately, parameter estimation remains challenging in these three frameworks. Based on prior work of Tonda, we derive a new class of probability density functions that allow explicit calculation of moments, marginal and conditional distributions, and the score and observed information needed in maximum likelihood estimation. We also illustrate how the new distribution flexibly models longitudinal data following a non-Gaussian distribution. Finally, we conduct a tri-variate genome-wide association analysis on dichotomized systolic and diastolic blood pressure and body mass index data from the UK-Biobank, showcasing the modeling prowess and computational scalability of the new distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.03505v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah S. Ji, Benjamin B. Chu, Hua Zhou, Kenneth Lange</dc:creator>
    </item>
    <item>
      <title>On the application of Gaussian graphical models to paired data problems</title>
      <link>https://arxiv.org/abs/2307.14160</link>
      <description>arXiv:2307.14160v2 Announce Type: replace 
Abstract: Gaussian graphical models are nowadays commonly applied to the comparison of groups sharing the same variables, by jointy learning their independence structures. We consider the case where there are exactly two dependent groups and the association structure is represented by a family of coloured Gaussian graphical models suited to deal with paired data problems. To learn the two dependent graphs, together with their across-graph association structure, we implement a fused graphical lasso penalty. We carry out a comprehensive analysis of this approach, with special attention to the role played by some relevant submodel classes. In this way, we provide a broad set of tools for the application of Gaussian graphical models to paired data problems. These include results useful for the specification of penalty values in order to obtain a path of lasso solutions and an ADMM algorithm that solves the fused graphical lasso optimization problem. Finally, we present an application of our method to cancer genomics where it is of interest to compare cancer cells with a control sample from histologically normal tissues adjacent to the tumor. All the methods described in this article are implemented in the $\texttt{R}$ package $\texttt{pdglasso}$ availabe at: https://github.com/savranciati/pdglasso.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14160v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saverio Ranciati, Alberto Roverato</dc:creator>
    </item>
    <item>
      <title>A versatile trivariate wrapped Cauchy copula with applications to toroidal and cylindrical data</title>
      <link>https://arxiv.org/abs/2401.10824</link>
      <description>arXiv:2401.10824v2 Announce Type: replace 
Abstract: In this paper, we propose a new flexible distribution for data on the three-dimensional torus which we call a trivariate wrapped Cauchy copula. Our trivariate copula has several attractive properties. It has a simple form of density and is unimodal. its parameters are interpretable and allow adjustable degree of dependence between every pair of variables and these can be easily estimated. The conditional distributions of the model are well studied bivariate wrapped Cauchy distributions. Furthermore, the distribution can be easily simulated. Parameter estimation via maximum likelihood for the distribution is given and we highlight the simple implementation procedure to obtain these estimates. We compare our model to its competitors for analysing trivariate data and provide some evidence of its advantages. Another interesting feature of this model is that it can be extended to cylindrical copula as we describe this new cylindrical copula and then gives its properties. We illustrate our trivariate wrapped Cauchy copula on data from protein bioinformatics of conformational angles, and our cylindrical copula using climate data related to buoy in the Adriatic Sea. The paper is motivated by these real trivariate datasets, but we indicate how the model can be extended to multivariate copulas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10824v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shogo Kato, Christophe Ley, Sophia Loizidou, Kanti V. Mardia</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal point process intensity estimation using zero-deflated subsampling applied to a lightning strikes dataset in France</title>
      <link>https://arxiv.org/abs/2403.11564</link>
      <description>arXiv:2403.11564v2 Announce Type: replace 
Abstract: Cloud-to-ground lightning strikes observed in a specific geographical domain over time can be naturally modeled by a spatio-temporal point process. Our focus lies in the parametric estimation of its intensity function, incorporating both spatial factors (such as altitude) and spatio-temporal covariates (such as field temperature, precipitation, etc.). The events are observed in France over a span of three years. Spatio-temporal covariates are observed with resolution $0.1^\circ \times 0.1^\circ$  ($\approx 100$km$^2$) and six-hour periods. This results in an extensive dataset, further characterized by a significant excess of zeroes (i.e., spatio-temporal cells with no observed events). We reexamine composite likelihood methods commonly employed for spatial point processes, especially in situations where covariates are piecewise constant. Additionally, we extend these methods to account for zero-deflated subsampling, a strategy involving dependent subsampling, with a focus on selecting more cells in regions where events are observed. A simulation study is conducted to illustrate these novel methodologies, followed by their application to the dataset of lightning strikes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11564v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Fran\c{c}ois Coeurjolly (SVH), Thibault Espinasse (PSPM, ICJ, UCBL FS), Anne-Laure Foug\`eres (PSPM, ICJ, UCBL FS), Mathieu Ribatet (Nantes Univ - ECN, LMJL)</dc:creator>
    </item>
    <item>
      <title>A Generalized Difference-in-Differences Estimator for Randomized Stepped-Wedge and Observational Staggered Adoption Settings</title>
      <link>https://arxiv.org/abs/2405.08730</link>
      <description>arXiv:2405.08730v3 Announce Type: replace 
Abstract: Staggered treatment adoption arises in the evaluation of policy impact and implementation in many settings, including both randomized stepped-wedge trials and non-randomized quasi-experiments with panel data. In both settings, getting an interpretable, unbiased effect estimate requires careful consideration of the target estimand and possible treatment effect heterogeneities. This paper proposes a novel non-parametric approach to this estimation for either setting. By constructing an estimator using weighted averages of two-by-two difference-in-differences comparisons as building blocks, the investigator can target the desired estimand for any assumed treatment effect heterogeneities. This provides desirable bias and interpretation properties while using the comparisons efficiently to mitigate the loss of precision, without requiring correct variance specification. The methods are demonstrated for both a randomized stepped-wedge trial on the impact of novel tuberculosis diagnostic tools and an observational staggered adoption study on the effects of COVID-19 vaccine financial incentive lotteries in U.S. states; these are compared to analyses using previous methods. A full algorithm with R code is provided to implement this method and to compare against existing methods. The proposed method allows for high flexibility and clear targeting of desired effects, providing one solution to the bias-variance-generalizability tradeoff.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08730v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lee Kennedy-Shaffer</dc:creator>
    </item>
    <item>
      <title>Annotation aggregation of multi-label ecological datasets via Bayesian modeling</title>
      <link>https://arxiv.org/abs/2406.15844</link>
      <description>arXiv:2406.15844v2 Announce Type: replace 
Abstract: Ecological and conservation studies monitoring bird communities typically rely on species classification based on bird vocalizations. Historically, this has been based on expert volunteers going into the field and making lists of the bird species that they observe. Recently, machine learning algorithms have emerged that can accurately classify bird species based on audio recordings of their vocalizations. Such algorithms crucially rely on training data that are labeled by experts. Automated classification is challenging when multiple species are vocalizing simultaneously, there is background noise, and/or the bird is far from the microphone. In continuously monitoring different locations, the size of the audio data become immense and it is only possible for human experts to label a tiny proportion of the available data. In addition, experts can vary in their accuracy and breadth of knowledge about different species. This article focuses on the important problem of combining sparse expert annotations to improve bird species classification while providing uncertainty quantification. We additionally are interested in providing expert performance scores to increase their engagement and encourage improvements. We propose a Bayesian hierarchical modeling approach and evaluate this approach on a new community science platform developed in Finland.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15844v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoxuan Wang, Patrik Lauha, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Logit unfolding choice models for binary data</title>
      <link>https://arxiv.org/abs/2407.06395</link>
      <description>arXiv:2407.06395v2 Announce Type: replace 
Abstract: Discrete choice models with non-monotonic response functions are important in many areas of application, especially political sciences and marketing. This paper describes a novel unfolding model for binary data that allows for heavy-tailed shocks to the underlying utilities. One of our key contributions is a Markov chain Monte Carlo algorithm that requires little or no parameter tuning, fully explores the support of the posterior distribution, and can be used to fit various extensions of our core model that involve (Bayesian) hypothesis testing on the latent construct. Our empirical evaluations of the model and the associated algorithm suggest that they provide better complexity-adjusted fit to voting data from the United States House of Representatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06395v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rayleigh Lei, Abel Rodriguez</dc:creator>
    </item>
    <item>
      <title>Bounds on causal effects in $2^{K}$ factorial experiments with non-compliance</title>
      <link>https://arxiv.org/abs/2407.12114</link>
      <description>arXiv:2407.12114v3 Announce Type: replace 
Abstract: Factorial experiments are ubiquitous in the social and biomedical sciences, but when units fail to comply with each assigned factors, identification and estimation of the average treatment effects become impossible without strong assumptions. Leveraging an instrumental variables approach, previous studies have shown how to identify and estimate the causal effect of treatment uptake among respondents who comply with treatment. A major caveat is that these identification results rely on strong assumptions on the effect of randomization on treatment uptake. This paper shows how to bound these complier average treatment effects for bounded outcomes under more mild assumptions on non-compliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12114v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Matthew Blackwell, Nicole E. Pashley</dc:creator>
    </item>
    <item>
      <title>Low-complexity Image and Video Coding Based on an Approximate Discrete Tchebichef Transform</title>
      <link>https://arxiv.org/abs/1609.07630</link>
      <description>arXiv:1609.07630v4 Announce Type: replace-cross 
Abstract: The usage of linear transformations has great relevance for data decorrelation applications, like image and video compression. In that sense, the discrete Tchebichef transform (DTT) possesses useful coding and decorrelation properties. The DTT transform kernel does not depend on the input data and fast algorithms can be developed to real time applications. However, the DTT fast algorithm presented in literature possess high computational complexity. In this work, we introduce a new low-complexity approximation for the DTT. The fast algorithm of the proposed transform is multiplication-free and requires a reduced number of additions and bit-shifting operations. Image and video compression simulations in popular standards shows good performance of the proposed transform. Regarding hardware resource consumption for FPGA shows 43.1% reduction of configurable logic blocks and ASIC place and route realization shows 57.7% reduction in the area-time figure when compared with the 2-D version of the exact DTT.</description>
      <guid isPermaLink="false">oai:arXiv.org:1609.07630v4</guid>
      <category>cs.MM</category>
      <category>cs.CV</category>
      <category>cs.DS</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCSVT.2016.2515378</arxiv:DOI>
      <dc:creator>P. A. M. Oliveira, R. J. Cintra, F. M. Bayer, S. Kulasekera, A. Madanayake, V. A. Coutinho</dc:creator>
    </item>
    <item>
      <title>Mixed-type Distance Shrinkage and Selection for Clustering via Kernel Metric Learning</title>
      <link>https://arxiv.org/abs/2306.01890</link>
      <description>arXiv:2306.01890v3 Announce Type: replace-cross 
Abstract: Distance-based clustering and classification are widely used in various fields to group mixed numeric and categorical data. In many algorithms, a predefined distance measurement is used to cluster data points based on their dissimilarity. While there exist numerous distance-based measures for data with pure numerical attributes and several ordered and unordered categorical metrics, an efficient and accurate distance for mixed-type data that utilizes the continuous and discrete properties simulatenously is an open problem. Many metrics convert numerical attributes to categorical ones or vice versa. They handle the data points as a single attribute type or calculate a distance between each attribute separately and add them up. We propose a metric called KDSUM that uses mixed kernels to measure dissimilarity, with cross-validated optimal bandwidth selection. We demonstrate that KDSUM is a shrinkage method from existing mixed-type metrics to a uniform dissimilarity metric, and improves clustering accuracy when utilized in existing distance-based clustering algorithms on simulated and real-world datasets containing continuous-only, categorical-only, and mixed-type data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01890v3</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s00357-024-09493-z</arxiv:DOI>
      <arxiv:journal_reference>Journal of Classification (2024)</arxiv:journal_reference>
      <dc:creator>Jesse S. Ghashti, John R. J. Thompson</dc:creator>
    </item>
    <item>
      <title>Probabilistic Inversion Modeling of Gas Emissions: A Gradient-Based MCMC Estimation of Gaussian Plume Parameters</title>
      <link>https://arxiv.org/abs/2408.01298</link>
      <description>arXiv:2408.01298v2 Announce Type: replace-cross 
Abstract: In response to global concerns regarding air quality and the environmental impact of greenhouse gas emissions, detecting and quantifying sources of emissions has become critical. To understand this impact and target mitigations effectively, methods for accurate quantification of greenhouse gas emissions are required. In this paper, we focus on the inversion of concentration measurements to estimate source location and emission rate. In practice, such methods often rely on atmospheric stability class-based Gaussian plume dispersion models. However, incorrectly identifying the atmospheric stability class can lead to significant bias in estimates of source characteristics. We present a robust approach that reduces this bias by jointly estimating the horizontal and vertical dispersion parameters of the Gaussian plume model, together with source location and emission rate, atmospheric background concentration, and sensor measurement error variance. Uncertainty in parameter estimation is quantified through probabilistic inversion using gradient-based MCMC methods. A simulation study is performed to assess the inversion methodology. We then focus on inference for the published Chilbolton dataset which contains controlled methane releases and demonstrates the practical benefits of estimating dispersion parameters in source inversion problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01298v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Newman, Christopher Nemeth, Matthew Jones, Philip Jonathan</dc:creator>
    </item>
  </channel>
</rss>
