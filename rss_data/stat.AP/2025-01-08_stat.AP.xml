<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 Jan 2025 02:32:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On factors influencing consumer preference in pipeline stages: an experiment</title>
      <link>https://arxiv.org/abs/2501.03418</link>
      <description>arXiv:2501.03418v1 Announce Type: new 
Abstract: This paper presents a case study on the eClinical data of Intelligent Medical Objects, which currently employs eight pipeline stages. Historically, the pipeline stage progresses inversely with the number of customers. Our objective is to identify the key factors that significantly affect consumer presences at the more advanced stages of the pipeline. Logistic regression is utilized for this analysis. This technique estimates the probability of an event occurring, enabling researchers to evaluate how various factors influence specific outcomes. Widely applied across disciplines such as medicine, finance, and social sciences, logistic regression is particularly useful for classification tasks and identifying the importance of predictors, thus supporting data-driven decision-making. In this study, logistic regression is used to model the likelihood of reaching the eighth pipeline stage as the dependent variable, revealing that only a few independent variables significantly contribute to explaining this outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03418v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paramahansa Pramanik, Joel Graff, Mike Decaro</dc:creator>
    </item>
    <item>
      <title>How to verify that a given process is a L\'evy-Driven Ornstein-Uhlenbeck Process</title>
      <link>https://arxiv.org/abs/2501.03434</link>
      <description>arXiv:2501.03434v1 Announce Type: new 
Abstract: Assuming that a L\'evy-Driven Ornstein-Uhlenbeck (or CAR(1)) processes is observed at discrete times $0$, $h$, $2h$,$\cdots$ $[T/h]h$. Here we introduced a step-by-step methodological approach on how a person would verify the model assumptions supported by real-life data examples using this methodology. The model parameter needs to be estimated and the driving process must be approximated. Approximated increments with estimated parameter of the driving process are used to test the assumptions that the CAR(1) process is L\'evy-driven. Performance of the test is illustrated through simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03434v1</guid>
      <category>stat.AP</category>
      <category>q-fin.ST</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ibrahim Abdelrazeq (Rhodes College), Hardy Smith (Rhodes College), Dinmukhammed Zhanbyrshy (Rhodes College)</dc:creator>
    </item>
    <item>
      <title>Modeling Cell Type Developmental Trajectory using Multinomial Unbalanced Optimal Transport</title>
      <link>https://arxiv.org/abs/2501.03501</link>
      <description>arXiv:2501.03501v1 Announce Type: new 
Abstract: Single-cell trajectory analysis aims to reconstruct the biological developmental processes of cells as they evolve over time, leveraging temporal correlations in gene expression. During cellular development, gene expression patterns typically change and vary across different cell types. A significant challenge in this analysis is that RNA sequencing destroys the cell, making it impossible to track gene expression across multiple stages for the same cell. Recent advances have introduced the use of optimal transport tools to model the trajectory of individual cells. In this paper, our focus shifts to a question of greater practical importance: we examine the differentiation of cell types over time. Specifically, we propose a novel method based on discrete unbalanced optimal transport to model the developmental trajectory of cell types. Our method detects biological changes in cell types and infers their transitions to different states by analyzing the transport matrix. We validated our method using single-cell RNA sequencing data from mouse embryonic fibroblasts. The results accurately identified major developmental changes in cell types, which were corroborated by experimental evidence. Furthermore, the inferred transition probabilities between cell types are highly congruent to biological ground truth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03501v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junhao Zhu, Kevin Zhang, Zhaolei Zhang, Dehan Kong</dc:creator>
    </item>
    <item>
      <title>Reconstructing ecological community dynamics from limited observations</title>
      <link>https://arxiv.org/abs/2501.03820</link>
      <description>arXiv:2501.03820v1 Announce Type: new 
Abstract: Ecosystems tend to fluctuate around stable equilibria in response to internal dynamics and environmental factors. Occasionally, they enter an unstable tipping region and collapse into an alternative stable state. Our understanding of how ecological communities vary over time and respond to perturbations depends on our ability to quantify and predict these dynamics. However, the scarcity of long, dense time series data poses a severe bottleneck for characterising community dynamics using existing methods. We overcome this limitation by combining information across multiple short time series using Bayesian inference. By decomposing dynamics into deterministic and stochastic components using Gaussian process priors, we predict stable and tipping regions along the community landscape and quantify resilience while addressing uncertainty. After validation with simulated and real ecological time series, we use the model to question common assumptions underlying classical potential analysis and re-evaluate the stability of previously proposed "tipping elements" in the human gut microbiota.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03820v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chandler Ross, Ville Laitinen, Moein Khalighi, Jarkko Saloj\"arvi, Willem de Vos, Guilhem Sommeria-Klein, Leo Lahti</dc:creator>
    </item>
    <item>
      <title>Statistical Distributions for Transient Transport</title>
      <link>https://arxiv.org/abs/2501.03969</link>
      <description>arXiv:2501.03969v1 Announce Type: new 
Abstract: This paper introduces the use of statistical distributions based on transport differential equations for clear distinction of transport modes within transient kinetic experiments. More specifically,novel techniques are developed for the transient data obtained through the Temporal Analysis of Products (TAP) reactor. The methodology allows distinguishing between two domains of diffusion transport in heterogeneous catalytic systems, i.e., Knudsen and non-Knudsen diffusion, using statistical fingerprints, and finding the transition domain. Two distribution parameters were obtained that directly result in coefficients that correspond to the concentration and the rate of transport. Using a linear relationship between the rate and concentration coefficients, Knudsen diffusion is revealed when the rate of transport is constant and non-Knudsen diffusion is confirmed when the rate of transport coefficient is a function of the concentration coefficient. As a result, accurate transport information is obtained while in the presence of instrument drift or noise while investigating higher pressure pulse responses. As such, experiments where the influence of gas phase reactions can be more directly studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03969v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Ross Kunz, Debtanu Maiti, Gregory Yablonsky, Rebecca Fushimi</dc:creator>
    </item>
    <item>
      <title>The permuted score test for robust differential expression analysis</title>
      <link>https://arxiv.org/abs/2501.03530</link>
      <description>arXiv:2501.03530v1 Announce Type: cross 
Abstract: Negative binomial (NB) regression is a popular method for identifying differentially expressed genes in genomics data, such as bulk and single-cell RNA sequencing data. However, NB regression makes stringent parametric and asymptotic assumptions, which can fail to hold in practice, leading to excess false positive and false negative results. We propose the permuted score test, a new strategy for robust regression based on permuting score test statistics. The permuted score test provably controls type-I error across a much broader range of settings than standard NB regression while nevertheless approximately matching standard NB regression with respect to power (when the assumptions of standard NB regression obtain) and computational efficiency. We accelerate the permuted score test by leveraging emerging techniques for sequential Monte-Carlo testing and novel algorithms for efficiently computing GLM score tests. We apply the permuted score test to real and simulated RNA sequencing data, finding that it substantially improves upon the error control of existing NB regression implementations, including DESeq2. The permuted score test could enhance the reliability of differential expression analysis across diverse biological contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03530v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy Barry, Ziang Niu, Eugene Katsevich, Xihong Lin</dc:creator>
    </item>
    <item>
      <title>Context-Alignment: Activating and Enhancing LLM Capabilities in Time Series</title>
      <link>https://arxiv.org/abs/2501.03747</link>
      <description>arXiv:2501.03747v1 Announce Type: cross 
Abstract: Recently, leveraging pre-trained Large Language Models (LLMs) for time series (TS) tasks has gained increasing attention, which involves activating and enhancing LLMs' capabilities. Many methods aim to activate LLMs' capabilities based on token-level alignment but overlook LLMs' inherent strength on natural language processing -- their deep understanding of linguistic logic and structure rather than superficial embedding processing. We propose Context-Alignment, a new paradigm that aligns TS with a linguistic component in the language environments familiar to LLMs to enable LLMs to contextualize and comprehend TS data, thereby activating their capabilities. Specifically, such context-level alignment comprises structural alignment and logical alignment, which is achieved by a Dual-Scale Context-Alignment GNNs (DSCA-GNNs) applied to TS-language multimodal inputs. Structural alignment utilizes dual-scale nodes to describe hierarchical structure in TS-language, enabling LLMs treat long TS data as a whole linguistic component while preserving intrinsic token features. Logical alignment uses directed edges to guide logical relationships, ensuring coherence in the contextual semantics. Demonstration examples prompt are employed to construct Demonstration Examples based Context-Alignment (DECA) following DSCA-GNNs framework. DECA can be flexibly and repeatedly integrated into various layers of pre-trained LLMs to improve awareness of logic and structure, thereby enhancing performance. Extensive experiments show the effectiveness of DECA and the importance of Context-Alignment across tasks, particularly in few-shot and zero-shot forecasting, confirming that Context-Alignment provide powerful prior knowledge on context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03747v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxiao Hu, Qian Li, Dongxiao Zhang, Jinyue Yan, Yuntian Chen</dc:creator>
    </item>
    <item>
      <title>Fractional Tackles: Leveraging Player Tracking Data for Within-Play Tackling Evaluation in American Football</title>
      <link>https://arxiv.org/abs/2403.14769</link>
      <description>arXiv:2403.14769v2 Announce Type: replace 
Abstract: Tackling is a fundamental defensive move in American football, with the main purpose of stopping the forward motion of the ball-carrier. However, current tackling metrics are manually recorded outcomes that are inherently flawed due to their discrete and subjective nature. Using player tracking data, we present a novel framework for assessing tackling contribution in a continuous and objective manner. Our approach first identifies when a defender is in a ``contact window'' of the ball-carrier during a play, before assigning value to each window and the players involved. This enables us to devise a new metric called fractional tackles, which credits defenders for halting the ball-carrier's forward motion toward the end zone. We demonstrate that fractional tackles overcome the shortcomings of traditional metrics such as tackles and assists, by providing greater variation and measurable information for players lacking recorded statistics like defensive linemen. We view our contribution as a significant step forward in measuring defensive performance in American football and a clear demonstration of the capabilities of player tracking data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14769v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quang Nguyen, Ruitong Jiang, Meg Ellingwood, Ronald Yurko</dc:creator>
    </item>
    <item>
      <title>Modeling Insurance Claims using Bayesian Nonparametric Regression</title>
      <link>https://arxiv.org/abs/2311.11487</link>
      <description>arXiv:2311.11487v2 Announce Type: replace-cross 
Abstract: The prediction of future insurance claims based on observed risk factors, or covariates, help the actuary set insurance premiums. Typically, actuaries use parametric regression models to predict claims based on the covariate information. Such models assume the same functional form tying the response to the covariates for each data point. These models are not flexible enough and can fail to accurately capture at the individual level, the relationship between the covariates and the claims frequency and severity, which are often multimodal, highly skewed, and heavy-tailed. In this article, we explore the use of Bayesian nonparametric (BNP) regression models to predict claims frequency and severity based on covariates. In particular, we model claims frequency as a mixture of Poisson regression, and the logarithm of claims severity as a mixture of normal regression. We use the Dirichlet process (DP) and Pitman-Yor process (PY) as a prior for the mixing distribution over the regression parameters. Unlike parametric regression, such models allow each data point to have its individual parameters, making them highly flexible, resulting in improved prediction accuracy. We describe model fitting using MCMC and illustrate their applicability using French motor insurance claims data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11487v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mostafa Shams Esfand Abadi, Kaushik Ghosh</dc:creator>
    </item>
    <item>
      <title>Efficient Generative Modeling via Penalized Optimal Transport Network</title>
      <link>https://arxiv.org/abs/2402.10456</link>
      <description>arXiv:2402.10456v2 Announce Type: replace-cross 
Abstract: The generation of synthetic data with distributions that faithfully emulate the underlying data-generating mechanism holds paramount significance. Wasserstein Generative Adversarial Networks (WGANs) have emerged as a prominent tool for this task; however, due to the delicate equilibrium of the minimax formulation and the instability of Wasserstein distance in high dimensions, WGAN often manifests the pathological phenomenon of mode collapse. This results in generated samples that converge to a restricted set of outputs and fail to adequately capture the tail behaviors of the true distribution. Such limitations can lead to serious downstream consequences. To this end, we propose the Penalized Optimal Transport Network (POTNet), a versatile deep generative model based on the marginally-penalized Wasserstein (MPW) distance. Through the MPW distance, POTNet effectively leverages low-dimensional marginal information to guide the overall alignment of joint distributions. Furthermore, our primal-based framework enables direct evaluation of the MPW distance, thus eliminating the need for a critic network. This formulation circumvents training instabilities inherent in adversarial approaches and avoids the need for extensive parameter tuning. We derive a non-asymptotic bound on the generalization error of the MPW loss and establish convergence rates of the generative distribution learned by POTNet. Our theoretical analysis together with extensive empirical evaluations demonstrate the superior performance of POTNet in accurately capturing underlying data structures, including their tail behaviors and minor modalities. Moreover, our model achieves orders of magnitude speedup during the sampling stage compared to state-of-the-art alternatives, which enables computationally efficient large-scale synthetic data generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10456v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenhui Sophia Lu, Chenyang Zhong, Wing Hung Wong</dc:creator>
    </item>
    <item>
      <title>Modelling Loss of Complexity in Intermittent Time Series and its Application</title>
      <link>https://arxiv.org/abs/2411.14635</link>
      <description>arXiv:2411.14635v2 Announce Type: replace-cross 
Abstract: In this paper, we developed a novel method of nonparametric relative entropy (RlEn) for modelling loss of complexity in intermittent time series. The method consists of two steps. We first fit a nonlinear autoregressive model to each intermittent time series, where the corresponding lag order and the loss of complexity are determined by Bayesian Information Criterion (BIC) and relative entropy respectively. Then, change-points in the complexity are detected by a cumulative sum (CUSUM) based statistic. Compared to approximate entropy (ApEn), a popular method in literature, the performance of RlEn was assessed by simulations in terms of (1) ability to localize complexity change-points in intermittent time series; (2) ability to faithfully estimate underlying nonlinear models. The performance of the proposal was then examined in a real analysis of fatigue-induced changes in the complexity of human motor outputs. The results showed that the proposed method outperformed the ApEn in accurately detecting changes of complexity in intermittent time series segments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14635v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Li, Jian Zhang, Samantha L. Winter, Mark Burnley</dc:creator>
    </item>
    <item>
      <title>Fast data inversion for high-dimensional dynamical systems from noisy measurements</title>
      <link>https://arxiv.org/abs/2501.01324</link>
      <description>arXiv:2501.01324v2 Announce Type: replace-cross 
Abstract: In this work, we develop a scalable approach for a flexible latent factor model for high-dimensional dynamical systems. Each latent factor process has its own correlation and variance parameters, and the orthogonal factor loading matrix can be either fixed or estimated. We utilize an orthogonal factor loading matrix that avoids computing the inversion of the posterior covariance matrix at each time of the Kalman filter, and derive closed-form expressions in an expectation-maximization algorithm for parameter estimation, which substantially reduces the computational complexity without approximation. Our study is motivated by inversely estimating slow slip events from geodetic data, such as continuous GPS measurements. Extensive simulated studies illustrate higher accuracy and scalability of our approach compared to alternatives. By applying our method to geodetic measurements in the Cascadia region, our estimated slip better agrees with independently measured seismic data of tremor events. The substantial acceleration from our method enables the use of massive noisy data for geological hazard quantification and other applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01324v2</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizi Lin, Xubo Liu, Paul Segall, Mengyang Gu</dc:creator>
    </item>
  </channel>
</rss>
