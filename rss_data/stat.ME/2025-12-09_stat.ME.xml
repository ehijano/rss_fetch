<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Dec 2025 02:43:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Bag-and-Whisker Plot: A New Bagplot for Bivariate Data</title>
      <link>https://arxiv.org/abs/2512.06314</link>
      <description>arXiv:2512.06314v1 Announce Type: new 
Abstract: The bagplot, also known as the "bag-and-bolster plot", is a notable extension of the boxplot from univariate to bivariate data. Although widely used, its practical application is hindered by two key limitations: the fixed inflation factor for outlier detection that does not adapt to the sample size, and the unstable convex hull used to visualize its fence. In this paper, we propose a new bagplot, namely the "bag-and-whisker plot'', as an improvement method to address these limitations. Our framework recasts outlier detection as a multiple testing problem, yielding a data-adaptive fence that controls statistical error rates and enhances the reliability of outlier identification. To further resolve graphical instability, we introduce a refined visualization that abandons the convex hull (the bolster) with a direct rendering of the statistical fence, complemented by granular whiskers that effectively illustrate the data's spread. Extensive simulations and real-world data analyses demonstrate that our new bagplot exhibits superior adaptivity and robustness compared to the existing standard, and thus can be highly recommended for practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06314v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shenghao Qin, Bowen Gang, Tiejun Tong, Hengjian Cui</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit Tests for Heavy-tailed Random Fields</title>
      <link>https://arxiv.org/abs/2512.06412</link>
      <description>arXiv:2512.06412v1 Announce Type: new 
Abstract: We develop goodness-of-fit tests for max-stable random fields, which are used to model heavy-tailed spatial data. The test statistics are constructed based on the Fourier transforms of the indicators of extreme values in the heavy-tailed spatial data, whose asymptotic distribution is a Gaussian random field under a hypothesized max-stable random field. Since the covariance structure of the limiting Gaussian random field lacks an explicit expression, we propose a stationary bootstrap procedure for spatial fields to approximate critical values. Simulation studies confirm the theoretical distributional results, and applications to PM2.5 and temperature data illustrate the practical utility of the proposed method for model assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06412v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Niu, Zhao Chen, Christina Dan Wang, Yuwei Zhao</dc:creator>
    </item>
    <item>
      <title>Community detection in heterogeneous signed networks</title>
      <link>https://arxiv.org/abs/2512.06428</link>
      <description>arXiv:2512.06428v1 Announce Type: new 
Abstract: Network data has attracted growing interest across scientific domains, prompting the development of various network models. Existing network analysis methods mainly focus on unsigned networks, whereas signed networks, consisting of both positive and negative edges, have been frequently encountered in practice but much less investigated. In this paper, we formally define strong and weak balance in signed networks, and propose a signed block $\beta$-model, which is capable of modeling strong- and weak-balanced signed networks simultaneously. We establish the identifiability of the proposed model by leveraging properties of bipartite graphs, and develop an efficient alternating updating algorithm to optimize the resulting log-likelihood function. More importantly, we establish the asymptotic consistencies of the proposed model in terms of both probability estimation and community detection. Its advantages are also demonstrated through extensive numerical experiments and the application to a real-world international relationship network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06428v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuwen Wang, Shiwen Ye, Jingnan Zhang, Junhui Wang</dc:creator>
    </item>
    <item>
      <title>Simultaneous Heterogeneity and Reduced-rank Learning for Multivariate Response Regression</title>
      <link>https://arxiv.org/abs/2512.06514</link>
      <description>arXiv:2512.06514v1 Announce Type: new 
Abstract: Heterogeneous data are now ubiquitous in many applications in which correctly identifying the subgroups from a heterogeneous population is critical. Although there is an increasing body of literature on subgroup detection, existing methods mainly focus on the univariate response setting. In this paper, we propose a joint heterogeneity and reduced-rank learning framework to simultaneously identify the subgroup structure and estimate the covariate effects for heterogeneous multivariate response regression. In particular, our approach uses rank-constrained pairwise fusion penalization and conducts the subgroup analysis without requiring prior knowledge regarding the individual subgroup memberships. We implement the proposed approach by an alternating direction method of multipliers (ADMM) algorithm and show its convergence. We also establish the asymptotic properties for the resulting estimators under mild and interpretable conditions. A predictive information criterion is proposed to select the rank of the coefficient matrix with theoretical support. The effectiveness of the proposed approach is demonstrated through simulation studies and a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06514v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jmva.2025.105578</arxiv:DOI>
      <arxiv:journal_reference>Journal of Multivariate Analysis, Volume 213, May 2026, 105578</arxiv:journal_reference>
      <dc:creator>Jie Wu, Bo Zhang, Daoji Li, Zemin Zheng</dc:creator>
    </item>
    <item>
      <title>Hierarchical Clustering With Confidence</title>
      <link>https://arxiv.org/abs/2512.06522</link>
      <description>arXiv:2512.06522v1 Announce Type: new 
Abstract: Agglomerative hierarchical clustering is one of the most widely used approaches for exploring how observations in a dataset relate to each other. However, its greedy nature makes it highly sensitive to small perturbations in the data, often producing different clustering results and making it difficult to separate genuine structure from spurious patterns. In this paper, we show how randomizing hierarchical clustering can be useful not just for measuring stability but also for designing valid hypothesis testing procedures based on the clustering results.
  We propose a simple randomization scheme together with a method for constructing a valid p-value at each node of the hierarchical clustering dendrogram that quantifies evidence against performing the greedy merge. Our test controls the Type I error rate, works with any hierarchical linkage without case-specific derivations, and simulations show it is substantially more powerful than existing selective inference approaches. To demonstrate the practical utility of our p-values, we develop an adaptive $\alpha$-spending procedure that estimates the number of clusters, with a probabilistic guarantee on overestimation. Experiments on simulated and real data show that this estimate yields powerful clustering and can be used, for example, to assess clustering stability across multiple runs of the randomized algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06522v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Di Wu, Jacob Bien, Snigdha Panigrahi</dc:creator>
    </item>
    <item>
      <title>Controlling the False Discovery Proportion in Matched Observational Studies</title>
      <link>https://arxiv.org/abs/2512.06601</link>
      <description>arXiv:2512.06601v2 Announce Type: new 
Abstract: We provide an approach to exploratory data analysis in matched observational studies with a single intervention and multiple endpoints. In such settings, the researcher would like to explore evidence for actual treatment effects among these variables while accounting not only for the possibility of false discoveries, but also for the potential impact of unmeasured confounding. For any candidate subset of hypotheses about these outcomes, we provide sensitivity sets for the proportion of the hypotheses within the subset which are actually true. The resulting sensitivity statements are valid simultaneously over all possible choices for the rejected set, allowing the researcher to search for promising subsets of hypotheses that maintain a large estimated fraction of true discoveries even if hidden bias is present. The approach is well suited to sensitivity analysis, as conclusions that some fraction of outcomes are affected by the treatment exhibit larger robustness to unmeasured confounding than findings that any particular outcome is affected. We show how a sequence of integer programs, in tandem with screening steps, facilitate the efficient computation of the required sensitivity sets. We illustrate the practical utility of our method through both simulation studies and a data example on the long-term impacts of childhood abuse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06601v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengqi Lin, Colin Fogarty</dc:creator>
    </item>
    <item>
      <title>Monotone data augmentation algorithm for longitudinal continuous, binary and ordinal outcomes: a unifying approach</title>
      <link>https://arxiv.org/abs/2512.06621</link>
      <description>arXiv:2512.06621v1 Announce Type: new 
Abstract: The monotone data augmentation (MDA) algorithm has been widely used to impute missing data for longitudinal continuous outcomes. Compared to a full data augmentation approach, the MDA scheme accelerates the mixing of the Markov chain, reduces computational costs per iteration, and aids in missing data imputation under nonignorable dropouts. We extend the MDA algorithm to the multivariate probit (MVP) model for longitudinal binary and ordinal outcomes. The MVP model assumes the categorical outcomes are discretized versions of underlying longitudinal latent Gaussian outcomes modeled by a mixed effects model for repeated measures. A parameter expansion strategy is employed to facilitate the posterior sampling, and expedite the convergence of the Markov chain in MVP. The method enables the sampling of the regression coefficients and covariance matrix for longitudinal continuous, binary and ordinal outcomes in a unified manner. This property aids in understanding the algorithm and developing computer codes for MVP. We also introduce independent Metropolis-Hasting samplers to handle complex priors, and evaluate how the choice between flat and diffuse normal priors for regression coefficients influences parameter estimation and missing data imputation. Numerical examples are used to illustrate the methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06621v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongqiang Tang</dc:creator>
    </item>
    <item>
      <title>SLOACI: Surrogate-Leveraged Online Adaptive Causal Inference</title>
      <link>https://arxiv.org/abs/2512.06872</link>
      <description>arXiv:2512.06872v1 Announce Type: new 
Abstract: Adaptive experimental designs have gained increasing attention across a range of domains. In this paper, we propose a new methodological framework, surrogate-leveraged online adaptive causal inference (SLOACI), which integrates predictive surrogate outcomes into adaptive designs to enhance efficiency. For downstream analysis, we construct the adaptive augmented inverse probability weighting estimator for the average treatment effect using collected data. Our procedure remains robust even when surrogates are noisy or weak. We provide a comprehensive theoretical foundation for SLOACI. Under the asymptotic regime, we show that the proposed estimator attains the semiparametric efficiency bound. From a non-asymptotic perspective, we derive a regret bound to provide practical insights. We also develop a toolbox of sequential testing procedures that accommodates both asymptotic and non-asymptotic regimes, allowing experimenters to choose the perspective that best aligns with their practical needs. Extensive simulations and a synthetic case study are conducted to showcase the superior finite-sample performance of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06872v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingying Fan, Zihan Wang, Waverly Wei</dc:creator>
    </item>
    <item>
      <title>Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length</title>
      <link>https://arxiv.org/abs/2512.07019</link>
      <description>arXiv:2512.07019v1 Announce Type: new 
Abstract: The proliferation of Large Language Models (LLMs) necessitates valid evaluation methods to provide guidance for both downstream applications and actionable future improvements. The Item Response Theory (IRT) model with Computerized Adaptive Testing has recently emerged as a promising framework for evaluating LLMs via their response accuracy. Beyond simple response accuracy, LLMs' chain of thought (CoT) lengths serve as a vital indicator of their reasoning ability. To leverage the CoT length information to assist the evaluation of LLMs, we propose the Latency-Response Theory (LaRT) model, which jointly models both the response accuracy and CoT length by introducing a key correlation parameter between the latent ability and the latent speed. We derive an efficient stochastic approximation Expectation-Maximization algorithm for parameter estimation. We establish rigorous identifiability results for the latent ability and latent speed parameters to ensure the statistical validity of their estimation. Through both theoretical asymptotic analyses and simulation studies, we demonstrate LaRT's advantages over IRT in terms of superior estimation accuracy and shorter confidence intervals for latent trait estimation. To evaluate LaRT in real data, we collect responses from diverse LLMs on popular benchmark datasets. We find that LaRT yields different LLM rankings than IRT and outperforms IRT across multiple key evaluation metrics including predictive power, item efficiency, ranking validity, and LLM evaluation efficiency. Code and data are available at https://github.com/Toby-X/Latency-Response-Theory-Model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07019v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyu Xu, Jia Liu, Yixin Wang, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>Easy-to-Implement Two-Way Effect Decomposition for Any Outcome Variable with Endogenous Mediator</title>
      <link>https://arxiv.org/abs/2512.07058</link>
      <description>arXiv:2512.07058v1 Announce Type: new 
Abstract: Given a binary treatment D and a binary mediator M, mediation analysis decomposes the total effect of D on an outcome Y into the direct and indirect effects. Typically, both D and M are assumed to be exogenous, but this paper allows M to be endogenous while maintaining the exogeneity of D, which holds certainly if D is randomized. The endogeneity problem of M is then overcome using a binary instrumental variable Z. We derive a nonparametric "causal reduced form (CRF)" for Y with either (D,Z,DZ) or (D,M,DZ) as the regressors. The CRF enables estimating the direct and indirect effects easily with ordinary least squares or instrumental variable estimator, instead of matching or inverse probability weighting that have difficulties in finding the asymptotic distribution or in dealing with near-zero denominators. Not just this ease in implementation, our approach is applicable to any Y (binary, count, continuous, etc.). Simulation and empirical studies illustrate our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07058v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bora Kim, Myoung-jae Lee</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Failures and Condition-Number Diagnostics in Double Machine Learning</title>
      <link>https://arxiv.org/abs/2512.07083</link>
      <description>arXiv:2512.07083v1 Announce Type: new 
Abstract: Standard Double Machine Learning (DML; Chernozhukov et al., 2018) confidence intervals can exhibit substantial finite-sample coverage distortions when the underlying score equations are ill-conditioned, even if nuisance functions are estimated with state-of-the-art methods. Focusing on the partially linear regression (PLR) model, we show that a simple, easily computed condition number for the orthogonal score, denoted kappa_DML := 1 / |J_theta|, largely determines when DML inference is reliable. Our first result derives a nonasymptotic, Berry-Esseen-type bound showing that the coverage error of the usual DML t-statistic is of order n^{-1/2} + sqrt(n) * r_n, where r_n is the standard DML remainder term summarizing nuisance estimation error. Our second result provides a refined linearization in which both estimation error and confidence interval length scale as kappa_DML / sqrt(n) + kappa_DML * r_n, so that ill-conditioning directly inflates both variance and bias. These expansions yield three conditioning regimes - well-conditioned, moderately ill-conditioned, and severely ill-conditioned - and imply that informative, shrinking confidence sets require kappa_DML = o_p(sqrt(n)) and kappa_DML * r_n -&gt; 0. We conduct Monte Carlo experiments across overlap levels, nuisance learners (OLS, Lasso, random forests), and both low- and high-dimensional (p &gt; n) designs. Across these designs, kappa_DML is highly predictive of finite-sample performance: well-conditioned designs with kappa_DML &lt; 1 deliver near-nominal coverage with short intervals, whereas severely ill-conditioned designs can exhibit large bias and coverage around 40% for nominal 95% intervals, despite flexible nuisance fitting. We propose reporting kappa_DML alongside DML estimates as a routine diagnostic of score conditioning, in direct analogy to condition-number checks and weak-instrument diagnostics in IV settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07083v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Saco</dc:creator>
    </item>
    <item>
      <title>Asymptotic theory and statistical inference for the samples problems with heavy-tailed data using the functional empirical process</title>
      <link>https://arxiv.org/abs/2512.07088</link>
      <description>arXiv:2512.07088v1 Announce Type: new 
Abstract: This paper introduces the Trimmed Functional Empirical Process (TFEP) as a robust framework for statistical inference when dealing with heavy-tailed or skewed distributions, where classical moments such as the mean or variance may be infinite or undefined. Standard approaches including the classical Functional Empirical Process (FEP), break down under such conditions, especially for distributions like Pareto, Cauchy, low degree of freedom Student-t, due to their reliance on finite-variance assumptions to guarantee asymptotic convergence. The TFEP approach addresses these limitations by trimming a controlled proportion of extreme order statistics, thereby stabilizing the empirical process and restoring asymptotic Gaussian behavior. We establish the weak convergence of the TFEP under mild regularity conditions and derive new asymptotic distributions for one-sample and twosample problems. These theoretical developments lead to robust confidence intervals for truncated means, variances, and their differences or ratios. The efficiency and reliability of the TFEP are supported by extensive Monte Carlo experiments and an empirical application to Senegalese income data. In all scenarios, the TFEP provides accurate inference where both Gaussian-based methods and the classical FEP break down. The methodology thus offers a powerful and flexible tool for statistical analysis in heavy-tailed and non-standard environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07088v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abdoulaye Camara, Saliou Diouf, Moumouni Diallo, Gane Samb Lo</dc:creator>
    </item>
    <item>
      <title>Surprisingly-early bias in forecasts for unscheduled events</title>
      <link>https://arxiv.org/abs/2512.07575</link>
      <description>arXiv:2512.07575v1 Announce Type: new 
Abstract: When a dataset contains forecasts on unscheduled events, such as natural catastrophes, outcomes may be censored or ``hidden'' since some events have not yet occurred. This article finds that this can lead to a selection bias which affects the perceived accuracy and calibration of forecasts. This selection bias can be eliminated by excluding forecasts on outcomes which have been verified surprisingly early.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07575v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niklas V. Lehmann</dc:creator>
    </item>
    <item>
      <title>Symmetric Vaccine Efficacy</title>
      <link>https://arxiv.org/abs/2512.07739</link>
      <description>arXiv:2512.07739v1 Announce Type: new 
Abstract: Traditional measures of vaccine efficacy (VE) are inherently asymmetric, constrained above by $1$ but unbounded below. As a result, VE estimates and corresponding confidence intervals can extend far below zero, making interpretation difficult and potentially obscuring whether the apparent effect reflects true harm or simply statistical uncertainty. The proposed symmetric vaccine efficacy (SVE) is a bounded and interpretable alternative to VE that maintains desirable statistical properties while resolving these asymmetries. SVE is defined as a symmetric transformation of infection risks, with possible values within $[-1, 1]$, providing a common scale for both beneficial and harmful vaccine effects. This paper describes the relationship between SVE and traditional VE, considers inference about SVE, and illustrates the utility of the proposed measure by reanalyzing data from a randomized trial of a candidate HIV vaccine. Open-source tools for computing estimates of SVE and corresponding confidence intervals are available in R through the sve package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07739v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucy D'Agostino McGowan, Sarah C. Lotspeich, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>Assessing the Information Content of Individual Spikes in Population-Level Models of Neural Spiking Activity</title>
      <link>https://arxiv.org/abs/2512.06280</link>
      <description>arXiv:2512.06280v1 Announce Type: cross 
Abstract: In the last decade, there have been major advances in clusterless decoding algorithms for neural data analysis. These algorithms use the theory of marked point processes to describe the joint activity of many neurons simultaneously, without the need for spike sorting. In this study, we examine information-theoretic metrics to analyze the information extracted from each observed spike under such clusterless models. In an analysis of spatial coding in the rat hippocampus, we compared the entropy reduction between spike-sorted and clusterless models for both individual spikes observed in isolation and when the prior information from all previously observed spikes is accounted for. Our analysis demonstrates that low-amplitude spikes, which are difficult to cluster and often left out of spike sorting, provide reduced information compared to sortable, high-amplitude spikes when considered in isolation, but the two provide similar levels of information when considering all the prior information available from past spiking. These findings demonstrate the value of combining information measures with state-space modeling and yield new insights into the underlying mechanisms of neural computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06280v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Azar Ghahari, Uri T. Eden</dc:creator>
    </item>
    <item>
      <title>Modeling Spatio-temporal Extremes via Conditional Variational Autoencoders</title>
      <link>https://arxiv.org/abs/2512.06348</link>
      <description>arXiv:2512.06348v1 Announce Type: cross 
Abstract: Extreme weather events are widely studied in fields such as agriculture, ecology, and meteorology. The spatio-temporal co-occurrence of extreme events can strengthen or weaken under changing climate conditions. In this paper, we propose a novel approach to model spatio-temporal extremes by integrating climate indices via a conditional variational autoencoder (cXVAE). A convolutional neural network (CNN) is embedded in the decoder to convolve climatological indices with the spatial dependence within the latent space, thereby allowing the decoder to be dependent on the climate variables. There are three main contributions here. First, we demonstrate through extensive simulations that the proposed conditional XVAE accurately emulates spatial fields and recovers spatially and temporally varying extremal dependence with very low computational cost post training. Second, we provide a simple, scalable approach to detecting condition-driven shifts and whether the dependence structure is invariant to the conditioning variable. Third, when dependence is found to be condition-sensitive, the conditional XVAE supports counterfactual experiments allowing intervention on the climate covariate and propagating the associated change through the learned decoder to quantify differences in joint tail risk, co-occurrence ranges, and return metrics. To demonstrate the practical utility and performance of the model in real-world scenarios, we apply our method to analyze the monthly maximum Fire Weather Index (FWI) over eastern Australia from 2014 to 2024 conditioned on the El Ni\~{n}o/Southern Oscillation (ENSO) index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06348v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xiaoyu Ma, Likun Zhang, Christopher K. Wikle</dc:creator>
    </item>
    <item>
      <title>A multivariate extension of Azadkia-Chatterjee's rank coefficient</title>
      <link>https://arxiv.org/abs/2512.07443</link>
      <description>arXiv:2512.07443v1 Announce Type: cross 
Abstract: The Azadkia-Chatterjee coefficient is a rank-based measure of dependence between a random variable $Y \in \mathbb{R}$ and a random vector ${\boldsymbol Z} \in \mathbb{R}^{d_Z}$. This paper proposes a multivariate extension that measures dependence between random vectors ${\boldsymbol Y} \in \mathbb{R}^{d_Y}$ and ${\boldsymbol Z} \in \mathbb{R}^{d_Z}$, based on $n$ i.i.d. samples. The proposed coefficient converges almost surely to a limit with the following properties: i) it lies in $[0, 1]$; ii) it equals zero if and only if ${\boldsymbol Y}$ and ${\boldsymbol Z}$ are independent; and iii) it equals one if and only if ${\boldsymbol Y}$ is almost surely a function of ${\boldsymbol Z}$. Remarkably, the only assumption required by this convergence is that ${\boldsymbol Y}$ is not almost surely a constant. We further prove that under the same mild condition, the coefficient is asymptotically normal when ${\boldsymbol Y}$ and ${\boldsymbol Z}$ are independent and propose a merge sort based algorithm to calculate this coefficient in time complexity $O(n (\log n)^{d_Y})$. Finally, we show that it can be used to measure conditional dependence between ${\boldsymbol Y}$ and ${\boldsymbol Z}$ conditional on a third random vector ${\boldsymbol X}$, and prove that the measure is monotonic with respect to the deviation from an independence distribution under certain model restrictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07443v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenjie Huang, Zonghan Li, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>$\phi$-test: Global Feature Selection and Inference for Shapley Additive Explanations</title>
      <link>https://arxiv.org/abs/2512.07578</link>
      <description>arXiv:2512.07578v1 Announce Type: cross 
Abstract: We propose $\phi$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $\phi$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $\phi$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $\phi$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07578v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongseok Kim, Hyoungsun Choi, Mohamed Jismy Aashik Rasool, Gisung Oh</dc:creator>
    </item>
    <item>
      <title>Location and scatter halfspace median under {\alpha}-symmetric distributions</title>
      <link>https://arxiv.org/abs/2512.07634</link>
      <description>arXiv:2512.07634v1 Announce Type: cross 
Abstract: In a landmark result, Chen et al. (2018) showed that multivariate medians induced by halfspace depth attain the minimax optimal convergence rate under Huber contamination and elliptical symmetry, for both location and scatter estimation. We extend some of these findings to the broader family of {\alpha}-symmetric distributions, which includes both elliptically symmetric and multivariate heavy-tailed distributions. For location estimation, we establish an upper bound on the estimation error of the location halfspace median under the Huber contamination model. An analogous result for the standard scatter halfspace median matrix is feasible only under the assumption of elliptical symmetry, as ellipticity is deeply embedded in the definition of scatter halfspace depth. To address this limitation, we propose a modified scatter halfspace depth that better accommodates {\alpha}-symmetric distributions, and derive an upper bound for the corresponding {\alpha}-scatter median matrix. Additionally, we identify several key properties of scatter halfspace depth for {\alpha}-symmetric distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07634v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10485252.2025.2600417</arxiv:DOI>
      <dc:creator>Filip Bo\v{c}inec, Stanislav Nagy</dc:creator>
    </item>
    <item>
      <title>Bounds on inequality with incomplete data</title>
      <link>https://arxiv.org/abs/2512.07709</link>
      <description>arXiv:2512.07709v1 Announce Type: cross 
Abstract: We develop a unified, nonparametric framework for sharp partial identification and inference on inequality indices when income or wealth are only coarsely observed -- for example via grouped tables or individual interval reports -- possibly together with linear restrictions such as known means or subgroup totals. First, for a broad class of Schur-convex inequality measures, we characterize extremal allocations and show that sharp bounds are attained by distributions with simple, finite support, reducing the underlying infinite-dimensional problem to finite-dimensional optimization. Second, for indices that admit linear-fractional representations after suitable ordering of the data (including the Gini coefficient, quantile ratios, and the Hoover index), we recast the bound problems as linear or quadratic programs, yielding fast computation of numerically sharp bounds. Third, we establish $\sqrt{n}$ inference for bound endpoints using a uniform directional delta method and a bootstrap procedure for standard errors. In ELSA wealth data with mixed point and interval observations, we obtain sharp Gini bounds of 0.714--0.792 for liquid savings and 0.686--0.767 for a broad savings measure; historical U.S. income tables deliver time-series bounds for the Gini, quantile ratios, and Hoover index under grouped information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07709v1</guid>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Banks, Thomas Glinnan, Tatiana Komarova</dc:creator>
    </item>
    <item>
      <title>AR-sieve Bootstrap for High-dimensional Time Series</title>
      <link>https://arxiv.org/abs/2112.00414</link>
      <description>arXiv:2112.00414v2 Announce Type: replace 
Abstract: This paper proposes a new AR-sieve bootstrap approach to high-dimensional time series. The major challenge of classical bootstrap methods on high-dimensional time series is two-fold: curse of dimensionality and temporal dependence. To address such a difficulty, we utilize factor modeling to reduce dimension and capture temporal dependence simultaneously. A factor-based bootstrap procedure is constructed, which performs an AR-sieve bootstrap on the extracted low-dimensional common factor time series and then recovers the bootstrap samples for the original data from the factor model. Asymptotic properties for bootstrap mean statistics and extreme eigenvalues are established. Various simulation studies further demonstrate the advantages of the new AR-sieve bootstrap in high-dimensional scenarios. An empirical application on particulate matter (PM) concentration data is studied, where bootstrap confidence intervals for mean vectors and autocovariance matrices are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.00414v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daning Bi, Han Lin Shang, Yanrong Yang, Huanjun Zhu</dc:creator>
    </item>
    <item>
      <title>Static and Dynamic BART for Rank-Order Data</title>
      <link>https://arxiv.org/abs/2308.10231</link>
      <description>arXiv:2308.10231v5 Announce Type: replace 
Abstract: Ranking lists are often provided at regular time intervals in a range of applications, including economics, sports, marketing, and politics. Most popular methods for rank-order data postulate a linear specification for the latent scores, which determine the observed ranks, and ignore the temporal dependence of the ranking lists. To address these issues, novel nonparametric static (ROBART) and autoregressive (ARROBART) models are developed, with latent scores defined as nonlinear Bayesian additive regression tree functions of covariates. To make inferences in the dynamic ARROBART model, closed-form filtering, predictive, and smoothing distributions for the latent time-varying scores are derived. These results are applied in a Gibbs sampler with data augmentation for posterior inference. The proposed methods are shown to outperform existing competitors in simulation studies, static data applications to electoral data, stated preferences for sushi and movies, and dynamic data applications to economic complexity rankings of countries and weekly pollster rankings of NCAA football teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10231v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Iacopini, Eoghan O'Neill, Luca Rossini</dc:creator>
    </item>
    <item>
      <title>A Latent Variable Approach to Learning High-dimensional Multivariate longitudinal Data</title>
      <link>https://arxiv.org/abs/2405.15053</link>
      <description>arXiv:2405.15053v3 Announce Type: replace 
Abstract: High-dimensional multivariate longitudinal data, which arise when many outcome variables are measured repeatedly over time, are becoming increasingly common in social, behavioral and health sciences. We propose a latent variable model for drawing statistical inferences on covariate effects and predicting future outcomes based on high-dimensional multivariate longitudinal data. This model introduces unobserved factors to account for the between-variable and across-time dependence and assist the prediction. Statistical inference and prediction tools are developed under a general setting that allows outcome variables to be of mixed types and possibly unobserved for certain time points, for example, due to right censoring. A central limit theorem is established for drawing statistical inferences on regression coefficients. Additionally, an information criterion is introduced to choose the number of factors. The proposed model is applied to customer grocery shopping records to predict and understand shopping behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15053v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sze Ming Lee, Yunxiao Chen, Tony Sit</dc:creator>
    </item>
    <item>
      <title>Adaptive Learning with Blockwise Missing and Semi-Supervised Data</title>
      <link>https://arxiv.org/abs/2405.18722</link>
      <description>arXiv:2405.18722v3 Announce Type: replace 
Abstract: Data fusion enables powerful and generalizable analyses across multiple sources. However, different data collection capacities across different sources lead to blockwise missingness (BM), which poses challenges in practice. Meanwhile, the high cost of obtaining gold-standard labels leaves the majority of samples unlabeled, known as the semi-supervised (SS) problem. In this paper, we propose a novel Data-adaptive Estimation approach for data FUsion in the SEmi-supervised setting (DEFUSE) that handles both BM and SS issues in the presence of distributional shifts across data sources under a missing at random (MAR) mechanism}. DEFUSE starts with a complete-data-only estimator derived from the primary data source, and uses data-adaptive and distributional-shift-adjusted procedures to successively incorporate the data with BM covariates and the large unlabeled sample to effectively reduce the estimation variance without incurring bias. To further avoid bias due to fusion of misaligned data violating of the MAR assumption, a screening method is developed to identify and exclude data sources that are not aligned with the primary source. Compared to existing approaches, DEFUSE offers two main improvements. First, it offers a new data-adaptive control variate approach to handle BM, which achieves intrinsic efficiency and robustness against distributional shifts. Second, it reveals a more essential role for the unlabeled sample in the BM regression problem, leading to improved estimation. These advantages are theoretically guaranteed and empirically supported by simulation studies and two real-world biomedical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18722v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiming Li, Ying Wei, Molei Liu</dc:creator>
    </item>
    <item>
      <title>Identifying arbitrary transformation between the slopes in scalar-on-function regression</title>
      <link>https://arxiv.org/abs/2407.19502</link>
      <description>arXiv:2407.19502v3 Announce Type: replace 
Abstract: In this article, we study whether the slope functions of two scalar-on-function regression models in two samples are associated with any arbitrary transformation along the vertical axis. The problem is formally stated as a statistical hypothesis test, and corresponding test statistic is formed based on the estimated second derivative of the unknown transformation. The asymptotic properties of the test statistic are investigated using some advanced techniques related to the empirical process. Moreover, to implement the test for small sample size data, a bootstrap algorithm is proposed, and it is shown that the bootstrap version of the test is as good as the original test for sufficiently large sample size. Furthermore, the utility of the proposed methodology is shown for simulated datasets, and DTI data is analyzed using the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19502v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratim Guha Niyogi, Subhra Sankar Dhar</dc:creator>
    </item>
    <item>
      <title>"6 choose 4": A framework to understand and facilitate discussion of strategies for overall survival safety monitoring</title>
      <link>https://arxiv.org/abs/2410.04020</link>
      <description>arXiv:2410.04020v3 Announce Type: replace 
Abstract: Advances in anticancer therapies have significantly contributed to declining death rates in certain disease and clinical settings. However, they have also made it difficult to power a clinical trial in these settings with overall survival (OS) as the primary efficacy endpoint. Therefore, two approaches have been recently proposed for the pre-specified analysis of OS as a safety endpoint (Fleming et al., 2024; Rodriguez et al., 2024). In this paper, we provide a simple, unifying framework that includes the aforementioned approaches (and a couple others) as special cases. By highlighting each approach's focus, priority, tolerance for risk, and strengths or challenges for practical implementation, this framework can help to facilitate discussions between stakeholders on "fit-for-purpose OS data collection and assessment of harm" (American Association for Cancer Research, 2024). We apply this framework to a real clinical trial in large B-cell lymphoma to illustrate its application and value. Several recommendations and open questions are also raised.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04020v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Godwin Yung, Kaspar Rufibach, Marcel Wolbers, Mark Yan, Jue Wang</dc:creator>
    </item>
    <item>
      <title>Treatment Effect Estimation in Causal Survival Analysis: Practical Recommendations</title>
      <link>https://arxiv.org/abs/2501.05836</link>
      <description>arXiv:2501.05836v3 Announce Type: replace 
Abstract: The restricted mean survival time (RMST) difference offers an interpretable causal contrast to estimate the treatment effect for time-to-event outcomes, yet a wide range of available estimators leaves limited guidance for practice. We provide a unified review of RMST estimators for randomized trials and observational studies, establish identification and asymptotic properties, and supply new derivations where needed. Our extensive simulation study compares simple nonparametric methods (such as unweighted Kaplan-Meier estimators) alongside parametric and nonparametric implementations of the G-formula, weighting approaches, Buckley-James transformations, and augmented estimators under diverse censoring mechanisms and model specifications. Across scenarios, classical Kaplan-Meier estimators (weighted when required by the censoring process) and G-formula methods perform well in randomized settings, while in observational data G-formula estimators remain competitive; however, augmented estimators such as AIPTW-AIPCW generally offer robustness to model misspecification and a favorable bias-variance trade-off. Parametric estimators perform best under correct specification, whereas nonparametric methods avoid functional assumptions but require large sample sizes to achieve reliable performance. We offer practical recommendations for estimator choice and provide open-source R code to support reproducibility and application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05836v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charlotte Voinot (PREMEDICAL, Sanofi Gentilly), Cl\'ement Berenfeld (Sanofi Gentilly), Imke Mayer (Sanofi Gentilly), Bernard Sebastien (Sanofi Gentilly), Julie Josse (PREMEDICAL)</dc:creator>
    </item>
    <item>
      <title>Estimating Covariate-balanced Survival Curve in Distributed Data Environment using Data Collaboration Quasi-Experiment</title>
      <link>https://arxiv.org/abs/2505.06035</link>
      <description>arXiv:2505.06035v3 Announce Type: replace 
Abstract: The sharing of patient-level data necessary for covariate-adjusted survival analysis between medical institutions is difficult due to privacy protection restrictions. We propose a privacy-preserving framework that estimates balanced Kaplan-Meier curves from distributed observational data without exchanging raw data. Each institution sends only the low-dimensional representation obtained through dimensionality reduction of the covariate matrix. Analysts reconstruct the aggregated dataset, perform propensity score matching, and estimate survival curves. Experiments using simulation datasets and five publicly available medical datasets showed that the proposed method consistently outperformed single-site analyses. This method can handle both horizontal and vertical data distribution scenarios and enables the collaborative acquisition of reliable survival curves with minimal communication and no disclosure of raw data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06035v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akihiro Toyoda, Yuji Kawamata, Tomoru Nakayama, Akira Imakura, Tetsuya Sakurai, Yukihiko Okada</dc:creator>
    </item>
    <item>
      <title>A General Approach to Visualizing Uncertainty in Statistical Graphics</title>
      <link>https://arxiv.org/abs/2508.00937</link>
      <description>arXiv:2508.00937v3 Announce Type: replace 
Abstract: We present a general approach to visualizing uncertainty in static 2-D statistical graphics. If we treat a visualization as a function of its underlying quantities, uncertainty in those quantities induces a distribution over images. We show how to aggregate these images into a single visualization that represents the uncertainty. The approach can be viewed as a generalization of sample-based approaches that use overlay. Notably, standard representations, such as confidence intervals and bands, emerge with their usual coverage guarantees without being explicitly quantified or visualized. As a proof of concept, we implement our approach in the IID setting using resampling, provided as an open-source Python library. Because the approach operates directly on images, the user needs only to supply the data and the code for visualizing the quantities of interest without uncertainty. Through several examples, we show how both familiar and novel forms of uncertainty visualization can be created. The implementation is not only a practical validation of the underlying theory but also an immediately usable tool that can complement existing uncertainty-visualization libraries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00937v3</guid>
      <category>stat.ME</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernarda Petek, David Nabergoj, Erik \v{S}trumbelj</dc:creator>
    </item>
    <item>
      <title>A self-supervised learning approach for denoising autoregressive models with additive noise: finite and infinite variance cases</title>
      <link>https://arxiv.org/abs/2508.12970</link>
      <description>arXiv:2508.12970v2 Announce Type: replace 
Abstract: The autoregressive time series model is a popular second-order stationary process, modeling a wide range of real phenomena. However, in applications, autoregressive signals are often corrupted by additive noise. Further, the autoregressive process and the corruptive noise may be highly impulsive, stemming from an infinite-variance distribution. The model estimation techniques that account for additional noise tend to show reduced efficacy when there is very strong noise present in the data, especially when the noise is heavy-tailed. In this paper, we propose a novel self-supervised learning method to denoise the additive noise-corrupted autoregressive model. Our approach is motivated by recent work in computer vision and does not require full knowledge of the noise distribution. We use the proposed method to recover exemplary finite- and infinite-variance autoregressive signals, namely, Gaussian and alpha-stable distributed signals, respectively, from their noise-corrupted versions. The simulation study conducted on both synthetic and semi-synthetic data demonstrates strong denoising performance of our method compared to several baseline methods, particularly when the corruption is significant and impulsive in nature. Finally, we apply the presented methodology to forecast the pure autoregressive signal from the noise-corrupted data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12970v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sayantan Banerjee, Agnieszka Wylomanska, Sundar S</dc:creator>
    </item>
    <item>
      <title>Can language models boost the power of randomized experiments without statistical bias?</title>
      <link>https://arxiv.org/abs/2510.05545</link>
      <description>arXiv:2510.05545v2 Announce Type: replace 
Abstract: Randomized experiments or randomized controlled trials (RCTs) are gold standards for causal inference, yet cost and sample-size constraints limit power. We introduce CALM (Causal Analysis leveraging Language Models), a statistical framework that integrates large language models (LLMs) generated insights of RCTs with established causal estimators to increase precision while preserving statistical validity. In particular, CALM treats LLM-generated outputs as auxiliary prognostic information and corrects their potential bias via a heterogeneous calibration step that residualizes and optimally reweights predictions. We prove that CALM remains consistent even when LLM predictions are biased and achieves efficiency gains over augmented inverse probability weighting estimators for various causal effects. In particular, CALM develops a few-shot variant that aggregates predictions across randomly sampled demonstration sets. The resulting U-statistic-like predictor restores i.i.d. structure and also mitigates prompt-selection variability. Empirically, in simulations calibrated to a mobile-app depression RCT, CALM delivers lower variance relative to other benchmarking methods, is effective in zero- and few-shot settings, and remains stable across prompt designs. By principled use of LLMs to harness unstructured data and external knowledge learned during pretraining, CALM provides a practical path to more precise causal analyses in RCTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05545v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinrui Ruan, Xinwei Ma, Yingfei Wang, Waverly Wei, Jingshen Wang</dc:creator>
    </item>
    <item>
      <title>Multiply Robust Estimation of Conditional Survival Probability with Time-Varying Covariates</title>
      <link>https://arxiv.org/abs/2510.10372</link>
      <description>arXiv:2510.10372v3 Announce Type: replace 
Abstract: It is often of interest to study the association between covariates and the cumulative incidence of a right-censored time-to-event outcome. When time-varying covariates are measured on a fixed discrete time scale, it is desirable to account for these more up-to-date covariates when addressing censoring. For example, in vaccine trials, it is of interest to study the association between immune response levels after administering the vaccine and the cumulative incidence of the endpoint, while accounting for loss to follow-up explained by immune response levels measured after at multiple post-vaccination visits. Existing methods rely on stringent parametric assumptions, do not account for informative censoring due to time-varying covariates when time is continuous, only estimate a marginal survival probability, or do not fully use the discrete-time structure of post-treatment covariates. We propose a nonparametric estimator of the continuous-time survival probability conditional on covariates, accounting for censoring due to time-varying covariates measured on a fixed discrete time scale. We show that the estimator is multiply robust: it is Fisher consistent if, within each time window between adjacent visits, the censoring distribution is correctly specified, or both the time-to-event distribution and a conditional mean probability are correctly specified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10372v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongxiang Qiu, Marco Carone, Alex Luedtke, Peter B. Gilbert</dc:creator>
    </item>
    <item>
      <title>Unifiedly Efficient Inference on All-Dimensional Targets for Large-Scale GLMs</title>
      <link>https://arxiv.org/abs/2511.06070</link>
      <description>arXiv:2511.06070v2 Announce Type: replace 
Abstract: The scalability of Generalized Linear Models (GLMs) for large-scale, high-dimensional data often forces a trade-off between computational feasibility and statistical accuracy, particularly for inference on pre-specified parameters. While subsampling methods mitigate computational costs, existing estimators are typically constrained by a suboptimal $r^{-1/2}$ convergence rate, where $r$ is the subsample size. This paper introduces a unified framework that systematically breaks this barrier, enabling efficient and precise inference regardless of the dimension of the target parameters. To overcome the accuracy loss and enhance computational efficiency, we propose three estimators tailored to different scenarios. For low-dimensional targets, we propose a de-variance subsampling (DVS) estimator that achieves a sharply improved convergence rate of $\max\{r^{-1}, n^{-1/2}\}$, permitting valid inference even with very small subsamples. As $r$ grows, a multi-step refinement of our estimator is proven to be asymptotically normal and semiparametric efficient when $r/\sqrt{n} \to \infty$, matching the performance of the full-sample estimator-a property confirmed by its Bahadur representation. Critically, we provide an improved principle to high-dimensional targets, developing a novel decorrelated score function that facilitates simultaneous inference for a diverging number of pre-specified parameters. Comprehensive numerical experiments demonstrate that our framework delivers a superior balance of computational efficiency and statistical accuracy across both low- and high-dimensional inferential tasks in large-scale GLM, thereby realizing the promise of unifiedly efficient inference for large-scale GLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06070v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Fu, Dandan Jiang</dc:creator>
    </item>
    <item>
      <title>Interpolated stochastic interventions based on propensity scores, target policies and treatment-specific costs</title>
      <link>https://arxiv.org/abs/2511.11353</link>
      <description>arXiv:2511.11353v2 Announce Type: replace 
Abstract: We introduce families of stochastic interventions for discrete treatments that connect causal modeling to cost-sensitive decision making. The interventions arise from a cost-penalized information projection of the independent product of the organic propensity and a user-specified target, yielding closed-form Boltzmann-Gibbs couplings. The induced marginals define modified stochastic policies that interpolate smoothly, via a single tilt parameter, from the organic law or from the target distribution toward a product-of-experts limit when all destination costs are strictly positive. One of these families recovers and extends incremental propensity score interventions, retaining identification without global positivity. For inference, we derive efficient influence functions under a nonparametric model for the expected outcomes after these policies and construct one-step estimators with uniform confidence bands. In simulations, the proposed estimators improve stability and robustness to nuisance misspecification relative to plug-in baselines. The framework can operationalize graded scientific hypotheses under realistic constraints: because inputs are modular, analysts can sweep feasible policy spaces, prototype candidates, and align interventions with budgets and logistics before committing experimental resources. This could help close the loop between observational evidence and resource-aware experimental design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11353v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Johan de Aguas</dc:creator>
    </item>
    <item>
      <title>Wilcoxon-Mann-Whitney Test of No Group Discrimination</title>
      <link>https://arxiv.org/abs/2511.20308</link>
      <description>arXiv:2511.20308v2 Announce Type: replace 
Abstract: The traditional WMW null hypothesis $H_0: F = G$ is erroneously too broad. WMW actually tests narrower $H_0: AUC = 0.5$. Asymptotic distribution of the standardized $U$ statistic (i.e., the empirical AUC) under the correct $H_0$ is derived along with finite sample bias corrections. The traditional alternative hypothesis of stochastic dominance is too narrow. WMW is consistent against $H_1: AUC \neq 0.5$, as established by Van Dantzig in 1951.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20308v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marian Grendar</dc:creator>
    </item>
    <item>
      <title>Univariate-Guided Sparse Regression for Biobank-Scale High-Dimensional -omics Data</title>
      <link>https://arxiv.org/abs/2511.22049</link>
      <description>arXiv:2511.22049v2 Announce Type: replace 
Abstract: We present a scalable framework for computing polygenic risk scores (PRS) in high-dimensional genomic settings using the recently introduced Univariate-Guided Sparse Regression (uniLasso). UniLasso is a two-stage penalized regression procedure that leverages univariate coefficients and magnitudes to stabilize feature selection and enhance interpretability. Building on its theoretical and empirical advantages, we adapt uniLasso for application to the UK Biobank, a population-based repository comprising over one million genetic variants measured on hundreds of thousands of individuals from the United Kingdom. We further extend the framework to incorporate external summary statistics to increase predictive accuracy. Our results demonstrate that the adapted uniLasso attains predictive performance comparable to standard Lasso while selecting substantially fewer variants, yielding sparser and more interpretable models. Moreover, it exhibits superior performance in estimating PRS relative to its competitors, such as PRS-CS. Integrating external scores further improves prediction while maintaining sparsity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22049v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Richland, Tuomo Kiiskinen, William Wang, Sophia Lu, Balasubramanian Narasimhan, Manuel Rivas, Robert Tibshirani</dc:creator>
    </item>
    <item>
      <title>Comparing Two Proxy Methods for Causal Identification</title>
      <link>https://arxiv.org/abs/2512.00175</link>
      <description>arXiv:2512.00175v2 Announce Type: replace 
Abstract: Identifying causal effects in the presence of unmeasured variables is a fundamental challenge in causal inference, for which proxy variable methods have emerged as a powerful solution. We contrast two major approaches in this framework: (1) bridge equation methods, which leverage solutions to integral equations to recover causal targets, and (2) array decomposition methods, which recover latent factors composing counterfactual quantities by exploiting unique determination of eigenspaces. We compare the model restrictions underlying these two approaches and provide insight into implications of the underlying assumptions, clarifying the scope of applicability for each method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00175v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helen Guo, Elizabeth L. Ogburn, Ilya Shpitser</dc:creator>
    </item>
    <item>
      <title>Difference-in-differences with stochastic policy shifts of continuous treatments</title>
      <link>https://arxiv.org/abs/2512.00296</link>
      <description>arXiv:2512.00296v2 Announce Type: replace 
Abstract: Treatment effects under stochastic policy shifts quantify differences in outcomes across counterfactual scenarios with varying treatment distributions. Stochastic policy shifts generalize common notions of treatment effects since they include deterministic interventions (e.g., all individuals treated versus none treated) as a special case. While stochastic policy effects have been examined under causal exchangeability, they have not been integrated into the difference-in-differences (DiD) framework, which relies on parallel trends rather than exchangeability. In this paper, nonparametric efficient estimators of stochastic intervention effects are developed under a DiD setup with continuous treatments. The proposed causal estimand is the average stochastic dose effect among the treated, where the stochastic dose effect is the contrast between potential outcomes under a counterfactual dose distribution and no treatment. Several possible stochastic interventions are discussed, including those that do and do not depend on the observed data distribution. For generic stochastic interventions, the causal estimand is identified under standard conditions and estimators are proposed. Then, we focus on a specific stochastic policy shift, the exponential tilt, that increments the conditional density function of the continuous dose. For the exponential tilt intervention, a nonparametric estimator is proposed that allows for data-adaptive, machine learning nuisance function estimation. Under mild convergence rate conditions, the estimator is shown to be root-$n$ consistent and asymptotically normal with variance attaining the nonparametric efficiency bound. The proposed method is used to study the effect of hydraulic fracturing activity on employment and income.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00296v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Jetsupphasuk, Chenwei Fang, Didong Li, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>Estimation of Semiparametric Factor Models with Missing Data</title>
      <link>https://arxiv.org/abs/2512.03235</link>
      <description>arXiv:2512.03235v2 Announce Type: replace 
Abstract: We study semiparametric factor models in high-dimensional panels where the factor loadings consist of a nonparametric component explained by observed covariates and an idiosyncratic component capturing unobserved heterogeneity. A key challenge in empirical applications is the presence of missing observations, which can distort both factor recovery and loading estimation. To address this issue, we develop a projected principal component analysis (PPCA) procedure that accommodates general missing-at-random mechanisms through inverse-probability weighting. We establish consistency and derive the asymptotic distributions of the estimated factors and loading functions, allowing the sieve dimension to diverge and permitting the time dimension to be either fixed or growing. Unlike classical PCA, PPCA achieves consistent factor estimation even when T is fixed, and the limiting distributions under missing data exhibit mixture normality with enlarged asymptotic variances. Theoretical results are supported by simulations and an empirical application. Our findings demonstrate that PPCA provides an effective and robust framework for estimating semiparametric factor models in the presence of missing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03235v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sijie Zheng</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Differential Variance Inference for Heterogeneous Treatment Effect Detection</title>
      <link>https://arxiv.org/abs/2512.03254</link>
      <description>arXiv:2512.03254v3 Announce Type: replace 
Abstract: The conditional average treatment effect (CATE) is frequently estimated to refute the homogeneous treatment effect assumption. Under this assumption, all units making up the population under study experience identical benefit from a given treatment. Uncovering heterogeneous treatment effects through inference about the CATE, however, requires that covariates truly modifying the treatment effect be reliably collected at baseline. CATE-based techniques will necessarily fail to detect violations when effect modifiers are omitted from the data due to, for example, resource constraints. Severe measurement error has a similar impact. To address these limitations, we prove that the homogeneous treatment effect assumption can be gauged through inference about contrasts of the potential outcomes' variances. We derive causal machine learning estimators of these contrasts and study their asymptotic properties. We establish that these estimators are doubly robust and asymptotically linear under mild conditions, permitting formal hypothesis testing about the homogeneous treatment effect assumption even when effect modifiers are missing or mismeasured. Numerical experiments demonstrate that these estimators' asymptotic guarantees are approximately achieved in experimental and observational data alike. These inference procedures are then used to detect heterogeneous treatment effects in the re-analysis of randomized controlled trials investigating targeted temperature management in cardiac arrest patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03254v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe A. Boileau, Hani Zaki, Gabriele Lileikyte, Niklas Nielsen, Patrick R. Lawler, Mireille E. Schnitzer</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Estimation of High-dimensional Conditional Factor Models</title>
      <link>https://arxiv.org/abs/2209.00391</link>
      <description>arXiv:2209.00391v2 Announce Type: replace-cross 
Abstract: This paper presents a general framework for estimating high-dimensional conditional latent factor models via constrained nuclear norm regularization. We establish large sample properties of the estimators and provide efficient algorithms for their computation. To improve practical applicability, we propose a cross-validation procedure for selecting the regularization parameter. Our framework unifies the estimation of various conditional factor models, enabling the derivation of new asymptotic results while addressing limitations of existing methods, which are often model-specific or restrictive. Empirical analyses of the cross section of individual US stock returns suggest that imposing homogeneity improves the model's out-of-sample predictability, with our new method outperforming existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00391v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qihui Chen</dc:creator>
    </item>
    <item>
      <title>Covariate-Elaborated Robust Partial Information Transfer with Conditional Spike-and-Slab Prior</title>
      <link>https://arxiv.org/abs/2404.03764</link>
      <description>arXiv:2404.03764v3 Announce Type: replace-cross 
Abstract: The popularity of transfer learning stems from the fact that it can borrow information from useful auxiliary datasets. Existing statistical transfer learning methods usually adopt a global similarity measure between the source data and the target data, which may lead to inefficiency when only partial information is shared. In this paper, we propose a novel Bayesian transfer learning method named ``CONCERT'' to allow robust partial information transfer for high-dimensional data analysis. A conditional spike-and-slab prior is introduced in the joint distribution of target and source parameters for information transfer. By incorporating covariate-specific priors, we can characterize partial similarities and integrate source information collaboratively to improve the performance on the target. In contrast to existing work, the CONCERT is a one-step procedure which achieves variable selection and information transfer simultaneously. We establish variable selection consistency, as well as estimation and prediction error bounds for CONCERT. Our theory demonstrates the covariate-specific benefit of transfer learning. To ensure the scalability of the algorithm, we adopt the variational Bayes framework to facilitate implementation. Extensive experiments and two real data applications showcase the validity and advantages of CONCERT over existing cutting-edge transfer learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03764v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2025.2591232</arxiv:DOI>
      <dc:creator>Ruqian Zhang, Yijiao Zhang, Annie Qu, Zhongyi Zhu, Juan Shen</dc:creator>
    </item>
    <item>
      <title>A Data Envelopment Analysis Approach for Assessing Fairness in Resource Allocation: Application to Kidney Exchange Programs</title>
      <link>https://arxiv.org/abs/2410.02799</link>
      <description>arXiv:2410.02799v2 Announce Type: replace-cross 
Abstract: Kidney exchange programs have substantially increased transplantation rates but also raise critical concerns about fairness in organ allocation. We propose a novel framework leveraging Data Envelopment Analysis (DEA) to evaluate multiple dimensions of fairness-Priority, Access, and Outcome-within a unified model. This approach captures complexities often missed in single-metric analyses. Using data from the United Network for Organ Sharing, we separately quantify fairness across these dimensions: Priority fairness through waitlist durations, Access fairness via the Living Kidney Donor Profile Index (LKDPI) scores, and Outcome fairness based on graft lifespan. We then apply our conditional DEA model with covariate adjustment to demonstrate significant disparities in kidney allocation efficiency across ethnic groups. To quantify uncertainty, we employ conformal prediction within a novel Reference Frontier Mapping (RFM) framework, yielding group-conditional prediction intervals with finite-sample coverage guarantees. Our findings show notable differences in efficiency distributions between ethnic groups. Our study provides a rigorous framework for evaluating fairness in complex resource allocation systems with resource scarcity and mutual compatibility constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02799v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Kaazempur-Mofrad, Xiaowu Dai</dc:creator>
    </item>
    <item>
      <title>Order-Flow Filtration and Directional Association with Short-Horizon Returns</title>
      <link>https://arxiv.org/abs/2507.22712</link>
      <description>arXiv:2507.22712v2 Announce Type: replace-cross 
Abstract: Electronic markets generate dense order flow with many transient orders, which degrade directional signals derived from the limit order book (LOB). We study whether simple structural filters on order lifetime, modification count, and modification timing sharpen the association between order book imbalance (OBI) and short-horizon returns in BankNifty index futures, where unfiltered OBI is already known to be a strong short-horizon directional indicator. The efficacy of each filter is evaluated using a three-step diagnostic ladder: contemporaneous correlations, linear association between discretised regimes, and Hawkes event-time excitation between OBI and return regimes. Our results indicate that filtration of the aggregate order flow produces only modest changes relative to the unfiltered benchmark. By contrast, when filters are applied on the parent orders of executed trades, the resulting OBI series exhibits systematically stronger directional association. Motivated by recent regulatory initiatives to curb noisy order flow, we treat the association between OBI and short-horizon returns as a policy-relevant diagnostic of market quality. We then compare unfiltered and filtered OBI series, using tick-by-tick data from the National Stock Exchange of India, to infer how structural filters on the order flow affect OBI-return dynamics in an emerging market setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22712v2</guid>
      <category>q-fin.TR</category>
      <category>q-fin.CP</category>
      <category>q-fin.GN</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aditya Nittur Anantha, Shashi Jain, Prithwish Maiti</dc:creator>
    </item>
    <item>
      <title>SADA: Safe and Adaptive Aggregation of Multiple Black-Box Predictions in Semi-Supervised Learning</title>
      <link>https://arxiv.org/abs/2509.21707</link>
      <description>arXiv:2509.21707v2 Announce Type: replace-cross 
Abstract: Semi-supervised learning (SSL) arises in practice when labeled data are scarce or expensive to obtain, while large quantities of unlabeled data are readily available.
  With the growing adoption of machine learning techniques, it has become increasingly feasible to generate multiple predicted labels using a variety of models and algorithms, including deep learning, large language models, and generative AI. In this paper, we propose a novel approach that safely and adaptively aggregates multiple black-box predictions of uncertain quality for both inference and prediction tasks. Our method provides two key guarantees: (i) it never performs worse than using the labeled data alone, regardless of the quality of the predictions; and (ii) if any one of the predictions (without knowing which one) perfectly fits the ground truth, the algorithm adaptively exploits this to achieve either a faster convergence rate or the semiparametric efficiency bound. We demonstrate the effectiveness of the proposed algorithm through small-scale simulations and two real-data analyses with distinct scientific goals. A user-friendly R package, sada, is provided to facilitate practical implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21707v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Shan, Zhifeng Chen, Yiming Dong, Yazhen Wang, Jiwei Zhao</dc:creator>
    </item>
    <item>
      <title>SSLfmm: An R Package for Semi-Supervised Learning with a Mixed-Missingness Mechanism in Finite Mixture Models</title>
      <link>https://arxiv.org/abs/2512.03322</link>
      <description>arXiv:2512.03322v2 Announce Type: replace-cross 
Abstract: Semi-supervised learning (SSL) constructs classifiers from datasets in which only a subset of observations is labelled, a situation that naturally arises because obtaining labels often requires expert judgement or costly manual effort. This motivates methods that integrate labelled and unlabelled data within a learning framework. Most SSL approaches assume that label absence is harmless, typically treated as missing completely at random or ignored, but in practice, the missingness process can be informative, as the chances of an observation being unlabelled may depend on the ambiguity of its feature vector. In such cases, the missingness indicators themselves provide additional information that, if properly modelled, may improve estimation efficiency. The \textbf{SSLfmm} package for R is designed to capture this behaviour by estimating the Bayes' classifier under a finite mixture model in which each component corresponding to a class follows a multivariate normal distribution. It incorporates a mixed-missingness mechanism that combines a missing completely at random (MCAR) component with a (non-ignorable) missing at random (MAR) component, the latter modelling the probability of label missingness as a logistic function of the entropy based on the features. Parameters are estimated via an Expectation--Conditional Maximisation algorithm. In the two-class Gaussian setting with arbitrary covariance matrices, the resulting classifier trained on partially labelled data may, in some cases, achieve a lower misclassification rate than the supervised version in the case where all the labels are known. The package includes a practical tool for modelling and illustrates its performance through simulated examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03322v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geoffrey J. McLachlan, Jinran Wu</dc:creator>
    </item>
  </channel>
</rss>
