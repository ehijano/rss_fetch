<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Mar 2024 14:39:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Mar 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On the Hardness of Fair Allocation under Ternary Valuations</title>
      <link>https://arxiv.org/abs/2403.00943</link>
      <description>arXiv:2403.00943v1 Announce Type: new 
Abstract: We study the problem of fair allocation of indivisible items when agents have ternary additive valuations -- each agent values each item at some fixed integer values $a$, $b$, or $c$ that are common to all agents. The notions of fairness we consider are max Nash welfare (MNW), when $a$, $b$, and $c$ are non-negative, and max egalitarian welfare (MEW). We show that for any distinct non-negative $a$, $b$, and $c$, maximizing Nash welfare is APX-hard -- i.e., the problem does not admit a PTAS unless P = NP. We also show that for any distinct $a$, $b$, and $c$, maximizing egalitarian welfare is APX-hard except for a few cases when $b = 0$ that admit efficient algorithms. These results make significant progress towards completely characterizing the complexity of computing exact MNW allocations and MEW allocations. En route, we resolve open questions left by prior work regarding the complexity of computing MNW allocations under bivalued valuations, and MEW allocations under ternary mixed manna.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00943v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zack Fitzsimmons, Vignesh Viswanathan, Yair Zick</dc:creator>
    </item>
    <item>
      <title>Understanding Police Force Resource Allocation using Adversarial Optimal Transport with Incomplete Information</title>
      <link>https://arxiv.org/abs/2403.00972</link>
      <description>arXiv:2403.00972v1 Announce Type: new 
Abstract: Adversarial optimal transport has been proven useful as a mathematical formulation to model resource allocation problems to maximize the efficiency of transportation with an adversary, who modifies the data. It is often the case, however, that only the adversary knows which nodes are malicious and which are not. In this paper we formulate the problem of seeking adversarial optimal transport into Bayesian games. We construct the concept of Bayesian equilibrium and design a distributed algorithm that achieve those equilibria, making our model applicable to large-scale networks. Keywords: game theory, crime control, Markov games</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00972v1</guid>
      <category>cs.GT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinan Hu, Juntao Chen, Quanyan Zhu</dc:creator>
    </item>
    <item>
      <title>Public Projects with Preferences and Predictions</title>
      <link>https://arxiv.org/abs/2403.01042</link>
      <description>arXiv:2403.01042v1 Announce Type: new 
Abstract: In the public projects problem, a group of decisionmakers aggregate their preferences to choose one alternative. Recent work on public projects has proposed the Quadratic Transfers Mechanism (QTM) and shown asymptotic welfare guarantees in some cases. We begin by giving new non-asymptotic Price of Anarchy guarantees for the QTM.
  We then incorporate an alternative philosophy toward group decisionmaking, aggregation of information about which is the best alternative. We propose a public projects mechanism based on the QTM that aggregates both preferences and predictions, modeled as forecasts of the projects' welfare impacts. When the predictions come from a prediction market or wagering mechanism, we show the entire mechanism is robust to manipulation and give Price of Anarchy guarantees, though under strong assumptions on the mechanism's knowledge. Our results focus primarily on the case of deciding between two alternatives, showing the Price of Anarchy tends to $1$ as natural measures of the "size" of the population grow large. In most cases, the mechanisms achieve a balanced budget as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01042v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mary Monroe, Bo Waggoner</dc:creator>
    </item>
    <item>
      <title>Envy-Free House Allocation with Minimum Subsidy</title>
      <link>https://arxiv.org/abs/2403.01162</link>
      <description>arXiv:2403.01162v1 Announce Type: new 
Abstract: House allocation refers to the problem where $m$ houses are to be allocated to $n$ agents so that each agent receives one house. Since an envy-free house allocation does not always exist, we consider finding such an allocation in the presence of subsidy. We show that computing an envy-free allocation with minimum subsidy is NP-hard in general, but can be done efficiently if $m$ differs from $n$ by an additive constant or if the agents have identical utilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01162v1</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.orl.2024.107103</arxiv:DOI>
      <dc:creator>Davin Choo, Yan Hao Ling, Warut Suksompong, Nicholas Teh, Jian Zhang</dc:creator>
    </item>
    <item>
      <title>Policy Space Response Oracles: A Survey</title>
      <link>https://arxiv.org/abs/2403.02227</link>
      <description>arXiv:2403.02227v1 Announce Type: new 
Abstract: In game theory, a game refers to a model of interaction among rational decision-makers or players, making choices with the goal of achieving their individual objectives. Understanding their behavior in games is often referred to as game reasoning. This survey provides a comprehensive overview of a fast-developing game-reasoning framework for large games, known as Policy Space Response Oracles (PSRO). We first motivate PSRO, provide historical context, and position PSRO within game-reasoning approaches. We then focus on the strategy exploration issue for PSRO, the challenge of assembling an effective strategy portfolio for modeling the underlying game with minimum computational cost. We also survey current research directions for enhancing the efficiency of PSRO, and explore the applications of PSRO across various domains. We conclude by discussing open questions and future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02227v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariyan Bighashdel, Yongzhao Wang, Stephen McAleer, Rahul Savani, Frans A. Oliehoek</dc:creator>
    </item>
    <item>
      <title>Contract Design for Pandora's Box</title>
      <link>https://arxiv.org/abs/2403.02317</link>
      <description>arXiv:2403.02317v1 Announce Type: new 
Abstract: We study a natural application of contract design to search problems with probabilistic prior and exploration costs. These problems have a plethora of applications and are expressed concisely within the Pandora's Box model. Its optimal solution is the ingenious index policy proposed originally by Weitzman in 1979.
  In our principal-agent setting, the search task is delegated to an agent. The agent performs a sequential exploration of $n$ boxes, suffers the exploration cost for each inspected box, and selects the content (called the prize) of one inspected box as outcome. Agent and principal obtain an individual value based on the selected prize. To influence the search, the principal a-priori designs a contract with a non-negative payment to the agent for each potential prize. The goal of the principal to maximize her expected reward, i.e., value minus payment. We show how to compute optimal contracts for the principal in several scenarios.
  A popular and important subclass are linear contracts, and we show how to compute optimal linear contracts in polynomial time. For general contracts, we consider the standard assumption that the agent suffers cost but obtains value only from the transfers by the principal. Interestingly, a suitable adaptation of the index policy results in an optimal contract here. More generally, for general contracts with non-zero agent values for outcomes we show how to compute an optimal contract in two cases: (1) when each box has only one prize with non-zero value for principal and agent, (2) for i.i.d. boxes with a single prize with positive value for the principal. These results show that optimal contracts can be highly non-trivial, and their design goes significantly beyond the application or re-interpretation of the index policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02317v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Hoefer, Conrad Schecker, Kevin Schewior</dc:creator>
    </item>
    <item>
      <title>Offline Fictitious Self-Play for Competitive Games</title>
      <link>https://arxiv.org/abs/2403.00841</link>
      <description>arXiv:2403.00841v1 Announce Type: cross 
Abstract: Offline Reinforcement Learning (RL) has received significant interest due to its ability to improve policies in previously collected datasets without online interactions. Despite its success in the single-agent setting, offline multi-agent RL remains a challenge, especially in competitive games. Firstly, unaware of the game structure, it is impossible to interact with the opponents and conduct a major learning paradigm, self-play, for competitive games. Secondly, real-world datasets cannot cover all the state and action space in the game, resulting in barriers to identifying Nash equilibrium (NE). To address these issues, this paper introduces Off-FSP, the first practical model-free offline RL algorithm for competitive games. We start by simulating interactions with various opponents by adjusting the weights of the fixed dataset with importance sampling. This technique allows us to learn best responses to different opponents and employ the Offline Self-Play learning framework. In this framework, we further implement Fictitious Self-Play (FSP) to approximate NE. In partially covered real-world datasets, our methods show the potential to approach NE by incorporating any single-agent offline RL method. Experimental results in Leduc Hold'em Poker show that our method significantly improves performances compared with state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00841v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingxiao Chen, Weiji Xie, Weinan Zhang, Yong yu, Ying Wen</dc:creator>
    </item>
    <item>
      <title>Improved Online Learning Algorithms for CTR Prediction in Ad Auctions</title>
      <link>https://arxiv.org/abs/2403.00845</link>
      <description>arXiv:2403.00845v1 Announce Type: cross 
Abstract: In this work, we investigate the online learning problem of revenue maximization in ad auctions, where the seller needs to learn the click-through rates (CTRs) of each ad candidate and charge the price of the winner through a pay-per-click manner. We focus on two models of the advertisers' strategic behaviors. First, we assume that the advertiser is completely myopic; i.e.~in each round, they aim to maximize their utility only for the current round. In this setting, we develop an online mechanism based on upper-confidence bounds that achieves a tight $O(\sqrt{T})$ regret in the worst-case and negative regret when the values are static across all the auctions and there is a gap between the highest expected value (i.e.~value multiplied by their CTR) and second highest expected value ad. Next, we assume that the advertiser is non-myopic and cares about their long term utility. This setting is much more complex since an advertiser is incentivized to influence the mechanism by bidding strategically in earlier rounds. In this setting, we provide an algorithm to achieve negative regret for the static valuation setting (with a positive gap), which is in sharp contrast with the prior work that shows $O(T^{2/3})$ regret when the valuation is generated by adversary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00845v1</guid>
      <category>cs.IR</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Feng, Christopher Liaw, Zixin Zhou</dc:creator>
    </item>
    <item>
      <title>Team Formation amidst Conflicts</title>
      <link>https://arxiv.org/abs/2403.00859</link>
      <description>arXiv:2403.00859v1 Announce Type: cross 
Abstract: In this work, we formulate the problem of team formation amidst conflicts. The goal is to assign individuals to tasks, with given capacities, taking into account individuals' task preferences and the conflicts between them. Using dependent rounding schemes as our main toolbox, we provide efficient approximation algorithms. Our framework is extremely versatile and can model many different real-world scenarios as they arise in educational settings and human-resource management. We test and deploy our algorithms on real-world datasets and we show that our algorithms find assignments that are better than those found by natural baselines. In the educational setting we also show how our assignments are far better than those done manually by human experts. In the human resource management application we show how our assignments increase the diversity of teams. Finally, using a synthetic dataset we demonstrate that our algorithms scale very well in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00859v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3589334.3645444</arxiv:DOI>
      <dc:creator>Iasonas Nikolaou, Evimaria Terzi</dc:creator>
    </item>
    <item>
      <title>Bandit Profit-maximization for Targeted Marketing</title>
      <link>https://arxiv.org/abs/2403.01361</link>
      <description>arXiv:2403.01361v1 Announce Type: cross 
Abstract: We study a sequential profit-maximization problem, optimizing for both price and ancillary variables like marketing expenditures. Specifically, we aim to maximize profit over an arbitrary sequence of multiple demand curves, each dependent on a distinct ancillary variable, but sharing the same price. A prototypical example is targeted marketing, where a firm (seller) wishes to sell a product over multiple markets. The firm may invest different marketing expenditures for different markets to optimize customer acquisition, but must maintain the same price across all markets. Moreover, markets may have heterogeneous demand curves, each responding to prices and marketing expenditures differently. The firm's objective is to maximize its gross profit, the total revenue minus marketing costs.
  Our results are near-optimal algorithms for this class of problems in an adversarial bandit setting, where demand curves are arbitrary non-adaptive sequences, and the firm observes only noisy evaluations of chosen points on the demand curves. We prove a regret upper bound of $\widetilde{\mathcal{O}}\big(nT^{3/4}\big)$ and a lower bound of $\Omega\big((nT)^{3/4}\big)$ for monotonic demand curves, and a regret bound of $\widetilde{\Theta}\big(nT^{2/3}\big)$ for demands curves that are monotonic in price and concave in the ancillary variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01361v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.GN</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joon Suk Huh, Ellen Vitercik, Kirthevasan Kandasamy</dc:creator>
    </item>
    <item>
      <title>Optimistic and pessimistic approaches for cooperative games</title>
      <link>https://arxiv.org/abs/2403.01442</link>
      <description>arXiv:2403.01442v1 Announce Type: cross 
Abstract: Cooperative game theory aims to study how to divide a joint value created by a set of players. These games are often studied through the characteristic function form with transferable utility, which represents the value obtainable by each coalition. In the presence of externalities, there are many ways to define this value. Various models that account for different levels of player cooperation and the influence of external players on coalition value have been studied. Although there are different approaches, typically, the optimistic and pessimistic approaches provide sufficient insights into strategic interactions. This paper clarifies the interpretation of these approaches by providing a unified framework. We show that making sure that no coalition receives more than their (optimistic) upper bounds is always at least as difficult as guaranteeing their (pessimistic) lower bounds. We also show that if externalities are negative, providing these guarantees is always feasible. Then, we explore applications and show how our findings can be applied to derive results from the existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01442v1</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ata Atay, Christian Trudeau</dc:creator>
    </item>
    <item>
      <title>Mixed-Strategy Nash Equilibrium for Crowd Navigation</title>
      <link>https://arxiv.org/abs/2403.01537</link>
      <description>arXiv:2403.01537v1 Announce Type: cross 
Abstract: We address the problem of finding mixed-strategy Nash equilibrium for crowd navigation. Mixed-strategy Nash equilibrium provides a rigorous model for the robot to anticipate uncertain yet cooperative human behavior in crowds, but the computation cost is often too high for scalable and real-time decision-making. Here we prove that a simple iterative Bayesian updating scheme converges to the Nash equilibrium of a mixed-strategy social navigation game. Furthermore, we propose a data-driven framework to construct the game by initializing agent strategies as Gaussian processes learned from human datasets. Based on the proposed mixed-strategy Nash equilibrium model, we develop a sampling-based crowd navigation framework that can be integrated into existing navigation methods and runs in real-time on a laptop CPU. We evaluate our framework in both simulated environments and real-world human datasets in unstructured environments. Our framework consistently outperforms both non-learning and learning-based methods on both safety and navigation efficiency and reaches human-level crowd navigation performance on top of a meta-planner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01537v1</guid>
      <category>cs.RO</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muchen Sun, Francesca Baldini, Peter Trautman, Todd Murphey</dc:creator>
    </item>
    <item>
      <title>Identity Concealment Games: How I Learned to Stop Revealing and Love the Coincidences</title>
      <link>https://arxiv.org/abs/2105.05377</link>
      <description>arXiv:2105.05377v2 Announce Type: replace 
Abstract: In an adversarial environment, a hostile player performing a task may behave like a non-hostile one in order not to reveal its identity to an opponent. To model such a scenario, we define identity concealment games: zero-sum stochastic reachability games with a zero-sum objective of identity concealment. To measure the identity concealment of the player, we introduce the notion of an average player. The average player's policy represents the expected behavior of a non-hostile player. We show that there exists an equilibrium policy pair for every identity concealment game and give the optimality equations to synthesize an equilibrium policy pair. If the player's opponent follows a non-equilibrium policy, the player can hide its identity better. For this reason, we study how the hostile player may learn the opponent's policy. Since learning via exploration policies would quickly reveal the hostile player's identity to the opponent, we consider the problem of learning a near-optimal policy for the hostile player using the game runs collected under the average player's policy. Consequently, we propose an algorithm that provably learns a near-optimal policy and give an upper bound on the number of sample runs to be collected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.05377v2</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Karabag, Mustafa O., Melkior Ornik, and Ufuk Topcu. "Identity concealment games: How I learned to stop revealing and love the coincidences." Automatica 161 (2024): 111482</arxiv:journal_reference>
      <dc:creator>Mustafa O. Karabag, Melkior Ornik, Ufuk Topcu</dc:creator>
    </item>
    <item>
      <title>Strategyproof Facility Location in Perturbation Stable Instances</title>
      <link>https://arxiv.org/abs/2107.11977</link>
      <description>arXiv:2107.11977v2 Announce Type: replace 
Abstract: We consider $k$-Facility Location games, where $n$ strategic agents report their locations on the real line, and a mechanism maps them to $k\ge 2$ facilities. Each agent seeks to minimize her distance to the nearest facility. We are interested in (deterministic or randomized) strategyproof mechanisms without payments that achieve a reasonable approximation ratio to the optimal social cost of the agents. To circumvent the inapproximability of $k$-Facility Location by deterministic strategyproof mechanisms, we restrict our attention to perturbation stable instances. An instance of $k$-Facility Location on the line is $\gamma$-perturbation stable (or simply, $\gamma$-stable), for some $\gamma\ge 1$, if the optimal agent clustering is not affected by moving any subset of consecutive agent locations closer to each other by a factor at most $\gamma$. We show that the optimal solution is strategyproof in $(2+\sqrt{3})$-stable instances whose optimal solution does not include any singleton clusters, and that allocating the facility to the agent next to the rightmost one in each optimal cluster (or to the unique agent, for singleton clusters) is strategyproof and $(n-2)/2$-approximate for $5$-stable instances (even if their optimal solution includes singleton clusters). On the negative side, we show that for any $k\ge 3$ and any $\delta &gt; 0$, there is no deterministic anonymous mechanism that achieves a bounded approximation ratio and is strategyproof in $(\sqrt{2}-\delta)$-stable instances. We also prove that allocating the facility to a random agent of each optimal cluster is strategyproof and $2$-approximate in $5$-stable instances. To the best of our knowledge, this is the first time that the existence of deterministic (resp. randomized) strategyproof mechanisms with a bounded (resp. constant) approximation ratio is shown for a large and natural class of $k$-Facility Location instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.11977v2</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitris Fotakis, Panagiotis Patsilinakos</dc:creator>
    </item>
    <item>
      <title>Exploration and Incentivizing Participation in Clinical Trials</title>
      <link>https://arxiv.org/abs/2202.06191</link>
      <description>arXiv:2202.06191v5 Announce Type: replace 
Abstract: Participation incentives a well-known issue inhibiting clinical trials. We frame this issue as a non-standard exploration-exploitation tradeoff: the trial would like to explore as uniformly as possible, whereas each patient prefers "exploitation", i.e., treatments that seem best. We incentivize participation by leveraging information asymmetry between the trial and the patients. We measure statistical performance via worst-case estimation error under adversarially generated outcomes, a standard objective for clinical trials. We obtain a near-optimal solution in terms of this objective: an incentive-compatible mechanism with a particular guarantee, and a nearly matching impossibility result for any incentive-compatible mechanism. Our results extend to heterogeneous agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.06191v5</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingkai Li, Aleksandrs Slivkins</dc:creator>
    </item>
    <item>
      <title>Hedonic Diversity Games: A Complexity Picture with More than Two Colors</title>
      <link>https://arxiv.org/abs/2202.09210</link>
      <description>arXiv:2202.09210v2 Announce Type: replace 
Abstract: Hedonic diversity games are a variant of the classical Hedonic games designed to better model a variety of questions concerning diversity and fairness. Previous works mainly targeted the case with two diversity classes (represented as colors in the model) and provided some initial complexity-theoretic and existential results concerning Nash and individually stable outcomes. Here, we design new algorithms accompanied with lower bounds which provide a complete parameterized-complexity picture for computing Nash and individually stable outcomes with respect to the most natural parameterizations of the problem. Crucially, our results hold for general Hedonic diversity games where the number of colors is not necessarily restricted to two, and show that -- apart from two trivial cases -- a necessary condition for tractability in this setting is that the number of colors is bounded by the parameter. Moreover, for the special case of two colors we resolve an open question asked in previous work (Boehmer and Elkind, AAAI 2020).</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.09210v2</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.artint.2023.104017</arxiv:DOI>
      <arxiv:journal_reference>Artificial Intelligence 325 (2023) 104017:1-20</arxiv:journal_reference>
      <dc:creator>Robert Ganian, Thekla Hamm, Du\v{s}an Knop, \v{S}imon Schierreich, Ond\v{r}ej Such\'y</dc:creator>
    </item>
    <item>
      <title>Approximate Nash Equilibrium Learning for n-Player Markov Games in Dynamic Pricing</title>
      <link>https://arxiv.org/abs/2207.06492</link>
      <description>arXiv:2207.06492v3 Announce Type: replace 
Abstract: We investigate Nash equilibrium learning in a competitive Markov Game (MG) environment, where multiple agents compete, and multiple Nash equilibria can exist. In particular, for an oligopolistic dynamic pricing environment, exact Nash equilibria are difficult to obtain due to the curse-of-dimensionality. We develop a new model-free method to find approximate Nash equilibria. Gradient-free black box optimization is then applied to estimate $\epsilon$, the maximum reward advantage of an agent unilaterally deviating from any joint policy, and to also estimate the $\epsilon$-minimizing policy for any given state. The policy-$\epsilon$ correspondence and the state to $\epsilon$-minimizing policy are represented by neural networks, the latter being the Nash Policy Net. During batch update, we perform Nash Q learning on the system, by adjusting the action probabilities using the Nash Policy Net. We demonstrate that an approximate Nash equilibrium can be learned, particularly in the dynamic pricing domain where exact solutions are often intractable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.06492v3</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Larkin Liu</dc:creator>
    </item>
    <item>
      <title>Bayesian Mechanism Design for Blockchain Transaction Fee Allocation</title>
      <link>https://arxiv.org/abs/2209.13099</link>
      <description>arXiv:2209.13099v5 Announce Type: replace 
Abstract: In blockchain systems, the design of transaction fee mechanisms is essential for stability and satisfaction for both miners and users. A recent work has proven the impossibility of collusion-proof mechanisms that achieve both non-zero miner revenue and Dominating-Strategy-Incentive-Compatible (DSIC) for users. However, a positive miner revenue is important in practice to motivate miners. To address this challenge, we consider a Bayesian game setting and relax the DSIC requirement for users to Bayesian-Nash-Incentive-Compatibility (BNIC). In particular, we propose an auxiliary mechanism method that makes connections between BNIC and DSIC mechanisms. With the auxiliary mechanism method, we design a transaction fee mechanism (TFM) based on the multinomial logit (MNL) choice model, and prove that the TFM has both BNIC and collusion-proof properties with an asymptotic constant-factor approximation of optimal miner revenue for i.i.d. bounded valuations. Our result breaks the zero-revenue barrier while preserving truthfulness and collusion-proof properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.13099v5</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Chen, David Simchi-Levi, Zishuo Zhao, Yuan Zhou</dc:creator>
    </item>
    <item>
      <title>Almost-Bayesian Quadratic Persuasion (Extended Version)</title>
      <link>https://arxiv.org/abs/2212.13619</link>
      <description>arXiv:2212.13619v4 Announce Type: replace 
Abstract: In this article, we relax the Bayesianity assumption in the now-traditional model of Bayesian Persuasion introduced by Kamenica &amp; Gentzkow. Unlike preexisting approaches -- which have tackled the possibility of the receiver (Bob) being non-Bayesian by considering that his thought process is not Bayesian yet known to the sender (Alice), possibly up to a parameter -- we let Alice merely assume that Bob behaves 'almost like' a Bayesian agent, in some sense, without resorting to any specific model.
  Under this assumption, we study Alice's strategy when both utilities are quadratic and the prior is isotropic. We show that, contrary to the Bayesian case, Alice's optimal response may not be linear anymore. This fact is unfortunate as linear policies remain the only ones for which the induced belief distribution is known. What is more, evaluating linear policies proves difficult except in particular cases, let alone finding an optimal one. Nonetheless, we derive bounds that prove linear policies are near-optimal and allow Alice to compute a near-optimal linear policy numerically. With this solution in hand, we show that Alice shares less information with Bob as he departs more from Bayesianity, much to his detriment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.13619v4</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier Massicot, C\'edric Langbort</dc:creator>
    </item>
    <item>
      <title>Online Learning under Budget and ROI Constraints via Weak Adaptivity</title>
      <link>https://arxiv.org/abs/2302.01203</link>
      <description>arXiv:2302.01203v3 Announce Type: replace 
Abstract: We study online learning problems in which a decision maker has to make a sequence of costly decisions, with the goal of maximizing their expected reward while adhering to budget and return-on-investment (ROI) constraints. Existing primal-dual algorithms designed for constrained online learning problems under adversarial inputs rely on two fundamental assumptions. First, the decision maker must know beforehand the value of parameters related to the degree of strict feasibility of the problem (i.e. Slater parameters). Second, a strictly feasible solution to the offline optimization problem must exist at each round. Both requirements are unrealistic for practical applications such as bidding in online ad auctions. In this paper, we show how such assumptions can be circumvented by endowing standard primal-dual templates with weakly adaptive regret minimizers. This results in a ``dual-balancing'' framework which ensures that dual variables stay sufficiently small, even in the absence of knowledge about Slater's parameter. We prove the first best-of-both-worlds no-regret guarantees which hold in absence of the two aforementioned assumptions, under stochastic and adversarial inputs. Finally, we show how to instantiate the framework to optimally bid in various mechanisms of practical relevance, such as first- and second-price auctions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.01203v3</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Castiglioni, Andrea Celli, Christian Kroer</dc:creator>
    </item>
    <item>
      <title>Refined Characterizations of Approval-based Committee Scoring Rules</title>
      <link>https://arxiv.org/abs/2312.08799</link>
      <description>arXiv:2312.08799v2 Announce Type: replace 
Abstract: In approval-based committee (ABC) elections, the goal is to select a fixed-size subset of the candidates, a so-called committee, based on the voters' approval ballots over the candidates. One of the most popular classes of ABC voting rules are ABC scoring rules, which have recently been characterized by Lackner and Skowron (2021). However, this characterization relies on a model where the output is a ranking of committees instead of a set of winning committees and no full characterization of ABC scoring rules exists in the latter standard setting. We address this issue by characterizing two important subclasses of ABC scoring rules in the standard ABC election model, thereby both extending the result of Lackner and Skowron (2021) to the standard setting and refining it to subclasses. In more detail, by relying on a consistency axiom for variable electorates, we characterize (i) the prominent class of Thiele rules and (ii) a new class of ABC voting rules called ballot size weighted approval voting. Based on these theorems, we also infer characterizations of three well-known ABC voting rules, namely multi-winner approval voting, proportional approval voting, and satisfaction approval voting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08799v2</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris Dong, Patrick Lederer</dc:creator>
    </item>
    <item>
      <title>Multi-Player Resource-Sharing Games with Fair Reward Allocation</title>
      <link>https://arxiv.org/abs/2402.05300</link>
      <description>arXiv:2402.05300v2 Announce Type: replace 
Abstract: This paper considers a multi-player resource-sharing game with a fair reward allocation model. Multiple players choose from a collection of resources. Each resource brings a random reward equally divided among the players who choose it. We consider two settings. The first setting is a one-slot game where the mean rewards of the resources are known to all the players, and the objective of player 1 is to maximize their worst-case expected utility. Certain special cases of this setting have explicit solutions. These cases provide interesting yet non-intuitive insights into the problem. The second setting is an online setting, where the game is played over a finite time horizon, where the mean rewards are unknown to the first player. Instead, the first player receives, as feedback, the rewards of the resources they chose after the action. We develop a novel Upper Confidence Bound (UCB) algorithm that minimizes the worst-case regret of the first player using the feedback received.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05300v2</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mevan Wijewardena, Michael. J Neely</dc:creator>
    </item>
    <item>
      <title>Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games</title>
      <link>https://arxiv.org/abs/2402.18781</link>
      <description>arXiv:2402.18781v2 Announce Type: replace 
Abstract: Asymmetric information stochastic games (\textsc{aisg}s) arise in many complex socio-technical systems, such as cyber-physical systems and IT infrastructures. Existing computational methods for \textsc{aisg}s are primarily offline and can not adapt to equilibrium deviations. Further, current methods are limited to special classes of \textsc{aisg}s to avoid belief hierarchies. To address these limitations, we propose conjectural online learning (\textsc{col}), an online method for generic \textsc{aisg}s. \textsc{col} uses a forecaster-actor-critic (\textsc{fac}) architecture where subjective forecasts is used to conjecture the opponents' strategies and break belief hierarchies (forecaster), online rollout is used to adapt strategies to nonstationary environments (actor), Monte-Carlo simulation is used to estimate costs (critic), and Bayesian learning is used to calibrate conjectures. We prove that the conjectures produced by \textsc{col} are asymptotically consistent with the information feedback in the sense of a relaxed Bayesian consistency. We also prove that the empirical strategy profile induced by \textsc{col} converges to the Berk-Nash equilibrium, a solution concept characterizing rationality under subjectivity. Experimental results from an intrusion response use case demonstrate \textsc{col}'s superiority over state-of-the-art reinforcement learning methods against nonstationary attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18781v2</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tao Li, Kim Hammar, Rolf Stadler, Quanyan Zhu</dc:creator>
    </item>
    <item>
      <title>Dynamic Partial Computation Offloading for the Metaverse in In-Network Computing</title>
      <link>https://arxiv.org/abs/2306.06022</link>
      <description>arXiv:2306.06022v2 Announce Type: replace-cross 
Abstract: The computing in the network (COIN) paradigm is a promising solution that leverages unused network resources to perform tasks to meet computation-demanding applications, such as the metaverse. In this vein, we consider the partial computation offloading problem in the metaverse for multiple subtasks in a COIN environment to minimize energy consumption and delay while dynamically adjusting the offloading policy based on the changing computational resource status. The problem is NP-hard, and we transform it into two subproblems: the task-splitting problem (TSP) on the user side and the task-offloading problem (TOP) on the COIN side. We model the TSP as an ordinal potential game and propose a decentralized algorithm to obtain its Nash equilibrium (NE). Then, we model the TOP as a Markov decision process and propose the double deep Q-network (DDQN) to solve for the optimal offloading policy. Unlike the conventional DDQN algorithm, where intelligent agents sample offloading decisions randomly within a certain probability, the COIN agent explores the NE of the TSP and the deep neural network. Finally, the simulation results reveal that the proposed model approach allows the COIN agent to update its policies and make more informed decisions, leading to improved performance over time compared to the traditional baseline</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06022v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2023.3344817</arxiv:DOI>
      <dc:creator>Ibrahim Aliyu, Seungmin Oh, Namseok Ko, Tai-Won Um, Jinsul Kim</dc:creator>
    </item>
    <item>
      <title>Recursive Joint Simulation in Games</title>
      <link>https://arxiv.org/abs/2402.08128</link>
      <description>arXiv:2402.08128v2 Announce Type: replace-cross 
Abstract: Game-theoretic dynamics between AI agents could differ from traditional human-human interactions in various ways. One such difference is that it may be possible to accurately simulate an AI agent, for example because its source code is known. Our aim is to explore ways of leveraging this possibility to achieve more cooperative outcomes in strategic settings. In this paper, we study an interaction between AI agents where the agents run a recursive joint simulation. That is, the agents first jointly observe a simulation of the situation they face. This simulation in turn recursively includes additional simulations (with a small chance of failure, to avoid infinite recursion), and the results of all these nested simulations are observed before an action is chosen. We show that the resulting interaction is strategically equivalent to an infinitely repeated version of the original game, allowing a direct transfer of existing results such as the various folk theorems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08128v2</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vojtech Kovarik, Caspar Oesterheld, Vincent Conitzer</dc:creator>
    </item>
  </channel>
</rss>
