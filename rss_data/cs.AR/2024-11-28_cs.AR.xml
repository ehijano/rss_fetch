<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Nov 2024 05:01:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>SoftmAP: Software-Hardware Co-design for Integer-Only Softmax on Associative Processors</title>
      <link>https://arxiv.org/abs/2411.17847</link>
      <description>arXiv:2411.17847v1 Announce Type: new 
Abstract: Recent research efforts focus on reducing the computational and memory overheads of Large Language Models (LLMs) to make them feasible on resource-constrained devices. Despite advancements in compression techniques, non-linear operators like Softmax and Layernorm remain bottlenecks due to their sensitivity to quantization. We propose SoftmAP, a software-hardware co-design methodology that implements an integer-only low-precision Softmax using In-Memory Compute (IMC) hardware. Our method achieves up to three orders of magnitude improvement in the energy-delay product compared to A100 and RTX3090 GPUs, making LLMs more deployable without compromising performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17847v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mariam Rakka, Jinhao Li, Guohao Dai, Ahmed Eltawil, Mohammed E. Fouda, Fadi Kurdahi</dc:creator>
    </item>
    <item>
      <title>Calibrating DRAMPower Model: A Runtime Perspective from Real-System HPC Measurements</title>
      <link>https://arxiv.org/abs/2411.17960</link>
      <description>arXiv:2411.17960v1 Announce Type: new 
Abstract: The escalating energy demands of main memory have become a concern in modern computing architectures, particularly in large-scale systems, due to frequent access patterns, increasing data volumes, and the lack of efficient power management strategies. Accurate modeling of DRAM power consumption is essential to address this challenge and optimize energy efficiency. However, existing modeling tools that heavily rely on vendor-provided datasheet values lead to a big discrepancy between the estimation result and the real-word power consumption. In this work, we propose a calibration towards the DRAMPower model by leveraging runtime energy measurements collected from real-system experiments. Using custom memory benchmarks and runtime data from the HPC cluster, we refine key DRAM current parameters within the DRAMPower model, aligning its predictions more closely with real-world observations. Our calibration reduces the average energy estimation error to less than 5%, demonstrating substantial improvements in modeling accuracy and making the DRAMPower model a more reliable tool for power-aware system design and optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17960v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Shi, Dina Ali Abdelhamid, Thomas Ilsche, Saeideh Alinezhad Chamazcoti, Timon Evenblij, Mohit Gupta, Dwaipayan Biswas, Francky Catthoor</dc:creator>
    </item>
    <item>
      <title>Addressing Architectural Obstacles for Overlay with Stream Network Abstraction</title>
      <link>https://arxiv.org/abs/2411.17966</link>
      <description>arXiv:2411.17966v1 Announce Type: new 
Abstract: Overlay is an effective approach for creating FPGA-based AI accelerators, enabling software-programmable specialized hardware datapaths to flexibly support various DNN operations. Traditional DNN overlays typically base their instruction set design on the von Neumann model but adapt them to be more coarse-grained. These instruction sets control execution at the layer granularity and impose restricted patterns for mapping computation and bandwidth resources. Such constraints cause inefficiencies from the imperfect match between supported execution patterns and diverse DNN layer shapes and types. This work proposes a Reconfigurable Stream Network architecture, a unique ISA abstraction tailored for flexible FPGA overlay execution at low cost, marking it as the first known FPGA design to support dynamic sequential linear layer pipelining. This novel architecture presents a datapath abstraction modeled after a specialized circuit-switched network with stateful functional units (FUs) as nodes and data streaming on edges. Programming a computation corresponds to triggering a network path in this stream-connected datapath. The program can individually control FUs to form paths that exploit both spatial and pipeline parallelism between independent and dependent concurrent computations. We present a proof-of-concept design RSN-XNN on the Versal VCK190. Evaluations show a 22x latency reduction for BERT compared to the state of the art, along with throughput improvements of 3.2x, 2.4x, 2.5x, and 2.8x for BERT, VIT, NCF, and MLP, respectively. RSN-XNN matches the latency of the T4 GPU with the same FP32 performance but only 18% of the memory bandwidth. Compared to the A100 GPU under the same 7nm process node, it achieves 2.1x/4.5x better operating/dynamic energy efficiency in FP32.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17966v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengyue Wang, Xiaofan Zhang, Jason Cong, James C. Hoe</dc:creator>
    </item>
    <item>
      <title>FlexiBit: Fully Flexible Precision Bit-parallel Accelerator Architecture for Arbitrary Mixed Precision AI</title>
      <link>https://arxiv.org/abs/2411.18065</link>
      <description>arXiv:2411.18065v1 Announce Type: new 
Abstract: Recent research has shown that large language models (LLMs) can utilize low-precision floating point (FP) quantization to deliver high efficiency while maintaining original model accuracy. In particular, recent works have shown the effectiveness of non-power-of-two precisions, such as FP6 and FP5, and diverse sensitivity to low-precision arithmetic of LLM layers, which motivates mixed precision arithmetic including non-power-of-two precisions in LLMs. Although low-precision algorithmically leads to low computational overheads, such benefits cannot be fully exploited due to hardware constraints that support a limited set of power-of-two precisions (e.g., FP8, 16, 32, and 64 in NVIDIA H100 Tensor Core). In addition, the hardware compute units are designed to support standard formats (e.g., E4M3 and E5M2 for FP8). Such practices require re-designing the hardware whenever new precision and format emerge, which leads to high hardware replacement costs to exploit the benefits of new precisions and formats. Therefore, in this paper, we propose a new accelerator architecture, FlexiBit, which efficiently supports FP and INT arithmetic in arbitrary precisions and formats. Unlike previous bit-serial designs, which also provide flexibility but at the cost of performance due to its bit-wise temporal processing nature, FlexiBit's architecture enables bit-parallel processing of any precision and format without compute unit underutilization. FlexiBit's new capability to exploit non-power of two precision and format led to 1.66x and 1.62x higher performance per area on GPT-3 in FP6 targeting a cloud-scale accelerator, compared to a Tensor Core-like architecture and a state-of-the-art bit-parallel flexible precision accelerator, BitFusion, respectively. Also, the bit-parallel nature of FlexiBit's architecture led to 3.9x higher performance/area compared to a state-of-the-art bit-serial architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18065v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faraz Tahmasebi, Yian Wang, Benji Y. H. Huang, Hyoukjun Kwon</dc:creator>
    </item>
    <item>
      <title>CIM-Based Parallel Fully FFNN Surface Code High-Level Decoder for Quantum Error Correction</title>
      <link>https://arxiv.org/abs/2411.18090</link>
      <description>arXiv:2411.18090v1 Announce Type: new 
Abstract: Due to the high sensitivity of qubits to environmental noise, which leads to decoherence and information loss, active quantum error correction(QEC) is essential. Surface codes represent one of the most promising fault-tolerant QEC schemes, but they require decoders that are accurate, fast, and scalable to large-scale quantum platforms. In all types of decoders, fully neural network-based high-level decoders offer decoding thresholds that surpass baseline decoder-Minimum Weight Perfect Matching (MWPM), and exhibit strong scalability, making them one of the ideal solutions for addressing surface code challenges. However, current fully neural network-based high-level decoders can only operate serially and do not meet the current latency requirements (below 440 ns). To address these challenges, we first propose a parallel fully feedforward neural network (FFNN) high-level surface code decoder, and comprehensively measure its decoding performance on a computing-in-memory (CIM) hardware simulation platform. With the currently available hardware specifications, our work achieves a decoding threshold of 14.22%, surpassing the MWPM baseline of 10.3%, and achieves high pseudo-thresholds of 10.4%, 11.3%, 12%, and 11.6% with decoding latencies of 197.03 ns, 234.87 ns, 243.73 ns, and 251.65 ns for distances of 3, 5, 7 and 9, respectively. The impact of hardware parameters and non-idealities on these results is discussed, and the hardware simulation results are extrapolated to a 4K quantum cryogenic environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18090v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Wang, Erjia Xiao, Songhuan He, Zhongyi Ni, Lingfeng Zhang, Xiaokun Zhan, Yifei Cui, Jinguo Liu, Cheng Wang, Zhongrui Wang, Renjing Xu</dc:creator>
    </item>
    <item>
      <title>A 65-nm Reliable 6T CMOS SRAM Cell with Minimum Size Transistors</title>
      <link>https://arxiv.org/abs/2411.18114</link>
      <description>arXiv:2411.18114v1 Announce Type: new 
Abstract: As minimum area SRAM bit-cells are obtained when using cell ratio and pull-up ratio of 1, we analyze the possibility of decreasing the cell ratio from the conventional values comprised between 1.5-2.5 to 1. The impact of this option on area, power, performance and stability is analyzed showing that the most affected parameter is read stability, although this impact can be overcome using some of the read assist circuits proposed in the literature. The main benefits are layout regularity enhancement, with its consequent higher tolerance to variability, cell area reduction by 25% (with respect to a cell having a cell ratio of 2), leakage current improvement by a 35%, as well as energy dissipation reduction and a soft error rate per bit improvement of around 30%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18114v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TETC.2017.2721932</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Nuclear Science, vol. 7, no. 3, pp. 447-455, Jul-Sep 2019</arxiv:journal_reference>
      <dc:creator>Gabriel Torrens (University of the Balearic Islands), Bartomeu Alorda (University of the Balearic Islands), Cristian Carmona (University of the Balearic Islands), Daniel Malagon-Perianez (University of the Balearic Islands), Jaume Segura (University of the Balearic Islands), Sebastia Antoni Bota (University of the Balearic Islands)</dc:creator>
    </item>
    <item>
      <title>A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs</title>
      <link>https://arxiv.org/abs/2411.18148</link>
      <description>arXiv:2411.18148v1 Announce Type: new 
Abstract: Transformer neural networks (TNN) excel in natural language processing (NLP), machine translation, and computer vision (CV) without relying on recurrent or convolutional layers. However, they have high computational and memory demands, particularly on resource-constrained devices like FPGAs. Moreover, transformer models vary in processing time across applications, requiring custom models with specific parameters. Designing custom accelerators for each model is complex and time-intensive. Some custom accelerators exist with no runtime adaptability, and they often rely on sparse matrices to reduce latency. However, hardware designs become more challenging due to the need for application-specific sparsity patterns. This paper introduces ADAPTOR, a runtime-adaptive accelerator for dense matrix computations in transformer encoders and decoders on FPGAs. ADAPTOR enhances the utilization of processing elements and on-chip memory, enhancing parallelism and reducing latency. It incorporates efficient matrix tiling to distribute resources across FPGA platforms and is fully quantized for computational efficiency and portability. Evaluations on Xilinx Alveo U55C data center cards and embedded platforms like VC707 and ZCU102 show that our design is 1.2$\times$ and 2.87$\times$ more power efficient than the NVIDIA K80 GPU and the i7-8700K CPU respectively. Additionally, it achieves a speedup of 1.7 to 2.25$\times$ compared to some state-of-the-art FPGA-based accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18148v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ehsan Kabir, Austin R. J. Downey, Jason D. Bakos, David Andrews, Miaoqing Huang</dc:creator>
    </item>
    <item>
      <title>Efficient Nonlinear Function Approximation in Analog Resistive Crossbars for Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2411.18271</link>
      <description>arXiv:2411.18271v1 Announce Type: new 
Abstract: Analog In-memory Computing (IMC) has demonstrated energy-efficient and low latency implementation of convolution and fully-connected layers in deep neural networks (DNN) by using physics for computing in parallel resistive memory arrays. However, recurrent neural networks (RNN) that are widely used for speech-recognition and natural language processing have tasted limited success with this approach. This can be attributed to the significant time and energy penalties incurred in implementing nonlinear activation functions that are abundant in such models. In this work, we experimentally demonstrate the implementation of a non-linear activation function integrated with a ramp analog-to-digital conversion (ADC) at the periphery of the memory to improve in-memory implementation of RNNs. Our approach uses an extra column of memristors to produce an appropriately pre-distorted ramp voltage such that the comparator output directly approximates the desired nonlinear function. We experimentally demonstrate programming different nonlinear functions using a memristive array and simulate its incorporation in RNNs to solve keyword spotting and language modelling tasks. Compared to other approaches, we demonstrate manifold increase in area-efficiency, energy-efficiency and throughput due to the in-memory, programmable ramp generator that removes digital processing overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18271v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyi Yang, Ruibin Mao, Mingrui Jiang, Yichuan Cheng, Pao-Sheng Vincent Sun, Shuai Dong, Giacomo Pedretti, Xia Sheng, Jim Ignowski, Haoliang Li, Can Li, Arindam Basu</dc:creator>
    </item>
    <item>
      <title>CXL-Interference: Analysis and Characterization in Modern Computer Systems</title>
      <link>https://arxiv.org/abs/2411.18308</link>
      <description>arXiv:2411.18308v1 Announce Type: new 
Abstract: Compute Express Link (CXL) is a promising technology that addresses memory and storage challenges. Despite its advantages, CXL faces performance threats from external interference when co-existing with current memory and storage systems. This interference is under-explored in existing research. To address this, we develop CXL-Interplay, systematically characterizing and analyzing interference from memory and storage systems. To the best of our knowledge, we are the first to characterize CXL interference on real CXL hardware. We also provide reverse-reasoning analysis with performance counters and kernel functions. In the end, we propose and evaluate mitigating solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18308v1</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shunyu Mao, Jiajun Luo, Yixin Li, Jiapeng Zhou, Weidong Zhang, Zheng Liu, Teng Ma, Shuwen Deng</dc:creator>
    </item>
    <item>
      <title>QUADOL: A Quality-Driven Approximate Logic Synthesis Method Exploiting Dual-Output LUTs for Modern FPGAs</title>
      <link>https://arxiv.org/abs/2411.18330</link>
      <description>arXiv:2411.18330v1 Announce Type: new 
Abstract: Approximate computing is a new computing paradigm. One important area of it is designing approximate circuits for FPGA. Modern FPGAs support dual-output LUT, which can significantly reduce the area of FPGA designs. Several existing works explored the use of dual-output in approximate computing. However, they are limited to small-scale arithmetic circuits. To address the problem, this work proposes QUADOL, a quality-driven ALS method by exploiting dual-output LUTs for modern FPGAs. We propose a technique to approximately merge two single-output LUTs (i.e., a LUT pair) into a dual-output LUT. In addition, we transform the problem of selecting multiple LUT pairs for simultaneous approximate merging into a maximum matching problem to maximize the area reduction. Since QUADOL exploits a new dimension, i.e., approximately merging a LUT pair into a dual-output LUT, it can be integrated with any existing ALS methods to strengthen them. Therefore, we also introduce QUADOL+, which is a generic framework to integrate QUADOL into existing ALS methods. The experimental results showed that QUADOL+ can reduce the LUT count by up to 18% compared to the state-of-the-art ALS methods for FPGA. Moreover, the approximate multipliers optimized by QUADOL+ dominate most prior FPGA-based approximate multipliers in the area-error plane.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18330v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Shi, Xuan Wang, Chang Meng, Weikang Qian</dc:creator>
    </item>
    <item>
      <title>Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments</title>
      <link>https://arxiv.org/abs/2411.17741</link>
      <description>arXiv:2411.17741v1 Announce Type: cross 
Abstract: The widespread adoption of LLMs has driven an exponential rise in their deployment, imposing substantial demands on inference clusters. These clusters must handle numerous concurrent queries for different LLM downstream tasks. To handle multi-task settings with vast LLM parameter counts, methods like Low-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most of the base LLM model across tasks. Hence, they allow concurrent task serving with minimal memory requirements. However, existing LLM serving systems face inefficiencies: they overlook workload heterogeneity, impose high link bandwidth from frequent adapter loading, and suffer from head-of-line blocking in their schedulers. To address these challenges, we present Chameleon, a novel LLM serving system optimized for many adapter environments, that relies on two core ideas: adapter caching and adapter-aware scheduling. First, Chameleon caches popular adapters in GPU memory, minimizing the adapter loading times. Importantly, it uses the otherwise idle GPU memory, avoiding extra memory costs. Second, Chameleon uses a non-preemptive multi-queue scheduling to efficiently account for workload heterogeneity. In this way, Chameleon simultaneously prevents head of line blocking and starvation. We implement Chameleon on top of a state-of-the-art LLM serving platform and evaluate it with real-world production traces and open-source LLMs. Under high loads, Chameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively, while improving throughput by 1.5x compared to state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17741v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.OS</category>
      <category>cs.PF</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nikoleta Iliakopoulou, Jovan Stojkovic, Chloe Alverti, Tianyin Xu, Hubertus Franke, Josep Torrellas</dc:creator>
    </item>
    <item>
      <title>Optimising Iteration Scheduling for Full-State Vector Simulation of Quantum Circuits on FPGAs</title>
      <link>https://arxiv.org/abs/2411.18354</link>
      <description>arXiv:2411.18354v1 Announce Type: cross 
Abstract: As the field of quantum computing grows, novel algorithms which take advantage of quantum phenomena need to be developed. As we are currently in the NISQ (noisy intermediate scale quantum) era, quantum algorithm researchers cannot reliably test their algorithms on real quantum hardware, which is still too limited. Instead, quantum computing simulators on classical computing systems are used. In the quantum circuit model, quantum bits (qubits) are operated on by quantum gates. A quantum circuit is a sequence of such quantum gates operating on some number of qubits. A quantum gate applied to a qubit can be controlled by other qubits in the circuit. This applies the gate only to the states which satisfy the required control qubit state. We particularly target FPGAs as our main simulation platform, as these offer potential energy savings when compared to running simulations on CPUs/GPUs.
  In this work, we present a memory access pattern to optimise the number of iterations that need to be scheduled to execute a quantum gate such that only the iterations which access the required pairs (determined according to the control qubits imposed on the gate) are scheduled. We show that this approach results in a significant reduction in the time required to simulate a gate for each added control qubit. We also show that this approach benefits the simulation time on FPGAs more than CPUs and GPUs and allows to outperform both CPU and GPU platforms in terms of energy efficiency, which is the main factor for scalability of the simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18354v1</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youssef Moawad, Andrew Brown, Ren\'e Steijl, Wim Vanderbauwhede</dc:creator>
    </item>
    <item>
      <title>Hecaton: Training Large Language Models with Scalable Chiplet Systems</title>
      <link>https://arxiv.org/abs/2407.05784</link>
      <description>arXiv:2407.05784v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved remarkable success in various fields, but their training and finetuning require massive computation and memory, necessitating parallelism which introduces heavy communication overheads. Driven by advances in packaging, the chiplet architecture emerges as a potential solution, as it can integrate computing power, as well as utilize on-package links with better signal integrity, higher bandwidth, and lower energy consumption. However, most existing chiplet-related works focus on DNN inference. Directly porting them to LLM training introduces significantly large quantities of DRAM access and network-on-package (NoP) overheads which make state-of-the-art chiplet designs fail, highlighting a research gap.
  This work proposes Hecaton, a scalable and cost-effective chiplet system for LLM training. We first provide a chiplet architecture with tailored scheduling that can largely reduce DRAM accesses. We further design an efficient distributed training method that reduces NoP communication complexity and relieves constraints on SRAM capacity and layout. Theoretical analysis shows that the entire system achieves weak scaling: as the workload and hardware resources grow proportionally, the computation-to-communication ratio remains nearly constant. Experiments with various workloads and hardware configurations verify the property, and Hecaton achieves $5.29\times$ performance improvement and $3.46\times$ energy reduction on Llama3.1-405B, compared to the tensor parallelism in Megatron. To the best of our knowledge, we propose the first chiplet architecture specifically used for LLM training or finetuning, with guaranteed performance regardless of the problem scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05784v2</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongle Huang, Shupei Fan, Chen Tang, Xinyuan Lin, Shuwen Deng, Yongpan Liu</dc:creator>
    </item>
    <item>
      <title>MFIT: Multi-Fidelity Thermal Modeling for 2.5D and 3D Multi-Chiplet Architectures</title>
      <link>https://arxiv.org/abs/2410.09188</link>
      <description>arXiv:2410.09188v2 Announce Type: replace 
Abstract: Rapidly evolving artificial intelligence and machine learning applications require ever-increasing computational capabilities, while monolithic 2D design technologies approach their limits. Heterogeneous integration of smaller chiplets using a 2.5D silicon interposer and 3D packaging has emerged as a promising paradigm to address this limit and meet performance demands. These approaches offer a significant cost reduction and higher manufacturing yield than monolithic 2D integrated circuits. However, the compact arrangement and high compute density exacerbate the thermal management challenges, potentially compromising performance. Addressing these thermal modeling challenges is critical, especially as system sizes grow and different design stages require varying levels of accuracy and speed. Since no single thermal modeling technique meets all these needs, this paper introduces MFIT, a range of multi-fidelity thermal models that effectively balance accuracy and speed. These multi-fidelity models can enable efficient design space exploration and runtime thermal management. Our extensive testing on systems with 16, 36, and 64 2.5D integrated chiplets and 16x3 3D integrated chiplets demonstrates that these models can reduce execution times from days to mere seconds and milliseconds with negligible loss in accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09188v2</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pfromm, Alish Kanani, Harsh Sharma, Parth Solanki, Eric Tervo, Jaehyun Park, Janardhan Rao Doppa, Partha Pratim Pande, Umit Y. Ogras</dc:creator>
    </item>
    <item>
      <title>CKTSO: High-Performance Parallel Sparse Linear Solver for General Circuit Simulations</title>
      <link>https://arxiv.org/abs/2411.14082</link>
      <description>arXiv:2411.14082v2 Announce Type: replace 
Abstract: This paper introduces CKTSO (abbreviation of "circuit solver"), a novel sparse linear solver specially designed for the simulation program with integrated circuit emphasis (SPICE). CKTSO is a parallel solver and can be run on a multi-core, shared-memory computer. The algorithms of CKTSO are designed by considering the features of matrices involved in SPICE simulations. CKTSO is superior to existing similar solvers mainly in the following three aspects. First, the matrix ordering step of CKTSO combines different types of ordering algorithms such that it can generally obtain the fewest fill-ins for a wide range of circuit matrices. Second, CKTSO provides a parallel fast LU factorization algorithm with pivot check, which behaves good performance, scalability, and numerical stability. Third, CKTSO provides a structure-adaptive hybrid parallel triangular solving algorithm, which can adapt to various circuit matrices. Experiments including both benchmark tests and SPICE simulations demonstrate the superior performance of CKTSO. The libraries of CKTSO are available at https://github.com/chenxm1986/cktso.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14082v2</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoming Chen</dc:creator>
    </item>
    <item>
      <title>Managing Classical Processing Requirements for Quantum Error Correction</title>
      <link>https://arxiv.org/abs/2406.17995</link>
      <description>arXiv:2406.17995v2 Announce Type: replace-cross 
Abstract: Quantum Error Correction requires decoders to process syndromes generated by the error-correction circuits. These decoders must process syndromes faster than they are being generated to prevent a backlog of undecoded syndromes. This backlog can exponentially increase the time required to execute the program, which has resulted in the development of fast hardware decoders that accelerate decoding. Applications utilizing error-corrected quantum computers will require hundreds to thousands of logical qubits and provisioning a hardware decoder for every logical qubit can be very costly. In this work, we present a framework to reduce the number of hardware decoders and navigate the compute-performace trade-offs without sacrificing the performance or reliability of program execution. Through workload-centric characterizations performed by our framework, we propose efficient decoder scheduling policies that can reduce the number of hardware decoders required to run a program by up to 10X. With the proposed framework, scalability can be achieved via decoder virtualization, and individual decoders can be built to maximize accuracy and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17995v2</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satvik Maurya, Swamit Tannu</dc:creator>
    </item>
    <item>
      <title>An investigation of the Online Payment and Banking System Apps in Bangladesh</title>
      <link>https://arxiv.org/abs/2407.07766</link>
      <description>arXiv:2407.07766v2 Announce Type: replace-cross 
Abstract: Presently, Bangladesh is expending substantial efforts to digitize its national infrastructure, with a significant emphasis on achieving this goal through mobile applications that facilitate online payments and banking system advancements. Despite the lack of knowledge about the security level of these systems, they are currently in frequent use without much consideration. To observe whether they follow the minimum global set standards, we choose to conduct static and dynamic analysis of the applications using available open-source analyzers and open-source tools. This allows us to attempt to extract sensitive information, if possible, and determine whether the applications adhere to the standards of MASVS set by OWASP. We show how we analyzed 17 .apks and a SDK using open source scanner and discover security flaws to the applications, such as weaknesses related to data storage, vulnerable cryptographic elements, insecure network communications, and unsafe utilization of WebViews, detected by the scanner. These outputs demonstrate the need for extensive manual analysis of the application through source code review and dynamic analysis. We further implement reverse engineering and dynamic approach to verify the outputs and expose some applications do not comply with the standard method of network communication. Moreover, we attempt to verify the rest of the potential vulnerabilities in the next phase of our ongoing investigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07766v2</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <category>cs.SE</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahriar Hasan Mickey, Muhammad Nur Yanhaona</dc:creator>
    </item>
  </channel>
</rss>
