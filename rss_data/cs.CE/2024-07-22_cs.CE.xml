<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Jul 2024 04:00:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Incorporating lane-change prediction into energy-efficient speed control of connected autonomous vehicles at intersections</title>
      <link>https://arxiv.org/abs/2407.15004</link>
      <description>arXiv:2407.15004v1 Announce Type: new 
Abstract: Connected and autonomous vehicles (CAVs) possess the capability of perception and information broadcasting with other CAVs and connected intersections. Additionally, they exhibit computational abilities and can be controlled strategically, offering energy benefits. One potential control strategy is real-time speed control, which adjusts the vehicle speed by taking advantage of broadcasted traffic information, such as signal timings. However, the optimal control is likely to increase the gap in front of the controlled CAV, which induces lane changing by other drivers. This study proposes a modified traffic flow model that aims to predict lane-changing occurrences and assess the impact of lane changes on future traffic states. The primary objective is to improve energy efficiency. The prediction model is based on a cell division platform and is derived considering the additional flow during lane changing. An optimal control strategy is then developed, subject to the predicted trajectory generated for the preceding vehicle. Lane change prediction estimates future speed and gap of vehicles, based on predicted traffic states. The proposed framework outperforms the non-lane change traffic model, resulting in up to 13% energy savings when lane changing is predicted 4-6 seconds in advance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15004v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maziar Zamanpour, Suiyi He, Michael W. Levin, Zongxuan Sun</dc:creator>
    </item>
    <item>
      <title>A Spatio-Temporal Approach with Self-Corrective Causal Inference for Flight Delay Prediction</title>
      <link>https://arxiv.org/abs/2407.15185</link>
      <description>arXiv:2407.15185v1 Announce Type: new 
Abstract: Accurate flight delay prediction is crucial for the secure and effective operation of the air traffic system. Recent advances in modeling inter-airport relationships present a promising approach for investigating flight delay prediction from the multi-airport scenario. However, the previous prediction works only accounted for the simplistic relationships such as traffic flow or geographical distance, overlooking the intricate interactions among airports and thus proving inadequate. In this paper, we leverage causal inference to precisely model inter-airport relationships and propose a self-corrective spatio-temporal graph neural network (named CausalNet) for flight delay prediction. Specifically, Granger causality inference coupled with a self-correction module is designed to construct causality graphs among airports and dynamically modify them based on the current airport's delays. Additionally, the features of the causality graphs are adaptively extracted and utilized to address the heterogeneity of airports. Extensive experiments are conducted on the real data of top-74 busiest airports in China. The results show that CausalNet is superior to baselines. Ablation studies emphasize the power of the proposed self-correction causality graph and the graph feature extraction module. All of these prove the effectiveness of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15185v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qihui Zhu, Shenwen Chen, Tong Guo, Yisheng Lv, Wenbo Du</dc:creator>
    </item>
    <item>
      <title>Hurricane Evacuation Analysis with Large-scale Mobile Device Location Data during Hurricane Ian</title>
      <link>https://arxiv.org/abs/2407.15249</link>
      <description>arXiv:2407.15249v1 Announce Type: new 
Abstract: Hurricane Ian is the deadliest and costliest hurricane in Florida's history, with 2.5 million people ordered to evacuate. As we witness increasingly severe hurricanes in the context of climate change, mobile device location data offers an unprecedented opportunity to study hurricane evacuation behaviors. With a terabyte-level GPS dataset, we introduce a holistic hurricane evacuation behavior algorithm with a case study of Ian: we infer evacuees' departure time and categorize them into different behavioral groups, including self, voluntary, mandatory, shadow and in-zone evacuees. Results show the landfall area (Fort Myers, Lee County) had lower out-of-zone but higher overall evacuation rate, while the predicted landfall area (Tampa, Hillsborough County) had the opposite, suggesting the effects of delayed evacuation order. Out-of-zone evacuation rates would increase from shore to inland. Spatiotemporal analysis identified three evacuation waves: during formation, before landfall, and after landfall. These insights are valuable for enhancing future disaster planning and management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15249v1</guid>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luyu Liu, Xiaojian Zhang, Shangkun Jiang, Xilei Zhao</dc:creator>
    </item>
    <item>
      <title>A spatiotemporal deep learning framework for prediction of crack dynamics in heterogeneous solids: efficient mapping of concrete microstructures to its fracture properties</title>
      <link>https://arxiv.org/abs/2407.15665</link>
      <description>arXiv:2407.15665v1 Announce Type: new 
Abstract: A spatiotemporal deep learning framework is proposed that is capable of 2D full-field prediction of fracture in concrete mesostructures. This framework not only predicts fractures but also captures the entire history of the fracture process, from the crack initiation in the interfacial transition zone to the subsequent propagation of the cracks in the mortar matrix. In addition, a convolutional neural network is developed which can predict the averaged stress-strain curve of the mesostructures. The UNet modeling framework, which comprises an encoder-decoder section with skip connections, is used as the deep learning surrogate model. Training and test data are generated from high-fidelity fracture simulations of randomly generated concrete mesostructures. These mesostructures include geometric variabilities such as different aggregate particle geometrical features, spatial distribution, and the total volume fraction of aggregates. The fracture simulations are carried out in Abaqus, utilizing the cohesive phase-field fracture modeling technique as the fracture modeling approach. In this work, to reduce the number of training datasets, the spatial distribution of three sets of material properties for three-phase concrete mesostructures, along with the spatial phase-field damage index, are fed to the UNet to predict the corresponding stress and spatial damage index at the subsequent step. It is shown that after the training process using this methodology, the UNet model is capable of accurately predicting damage on the unseen test dataset by using 470 datasets. Moreover, another novel aspect of this work is the conversion of irregular finite element data into regular grids using a developed pipeline. This approach allows for the implementation of less complex UNet architecture and facilitates the integration of phase-field fracture equations into surrogate models for future developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15665v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rasoul Najafi Koopas, Shahed Rezaei, Natalie Rauter, Richard Ostwald, Rolf Lammering</dc:creator>
    </item>
    <item>
      <title>Economy Watchers Survey provides Datasets and Tasks for Japanese Financial Domain</title>
      <link>https://arxiv.org/abs/2407.14727</link>
      <description>arXiv:2407.14727v1 Announce Type: cross 
Abstract: Many natural language processing (NLP) tasks in English or general domains are widely available and are often used to evaluate pre-trained language models. In contrast, there are fewer tasks available for languages other than English and for the financial domain. In particular, tasks in Japanese and the financial domain are limited. We construct two large datasets using materials published by a Japanese central government agency. The datasets provide three Japanese financial NLP tasks, which include a 3-class and 12-class classification for categorizing sentences, as well as a 5-class classification task for sentiment analysis. Our datasets are designed to be comprehensive and up-to-date, leveraging an automatic update framework that ensures the latest task datasets are publicly available anytime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14727v1</guid>
      <category>cs.CL</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Suzuki, Hiroki Sakaji</dc:creator>
    </item>
    <item>
      <title>${\it Asparagus}$: A Toolkit for Autonomous, User-Guided Construction of Machine-Learned Potential Energy Surfaces</title>
      <link>https://arxiv.org/abs/2407.15175</link>
      <description>arXiv:2407.15175v1 Announce Type: cross 
Abstract: With the establishment of machine learning (ML) techniques in the scientific community, the construction of ML potential energy surfaces (ML-PES) has become a standard process in physics and chemistry. So far, improvements in the construction of ML-PES models have been conducted independently, creating an initial hurdle for new users to overcome and complicating the reproducibility of results. Aiming to reduce the bar for the extensive use of ML-PES, we introduce ${\it Asparagus}$, a software package encompassing the different parts into one coherent implementation that allows an autonomous, user-guided construction of ML-PES models. ${\it Asparagus}$ combines capabilities of initial data sampling with interfaces to ${\it ab
  initio}$ calculation programs, ML model training, as well as model evaluation and its application within other codes such as ASE or CHARMM. The functionalities of the code are illustrated in different examples, including the dynamics of small molecules, the representation of reactive potentials in organometallic compounds, and atom diffusion on periodic surface structures. The modular framework of ${\it Asparagus}$ is designed to allow simple implementations of further ML-related methods and models to provide constant user-friendly access to state-of-the-art ML techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15175v1</guid>
      <category>physics.chem-ph</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kai T\"opfer, Luis Itza Vazquez-Salazar, Markus Meuwly</dc:creator>
    </item>
    <item>
      <title>Examining Inequality in Park Quality for Promoting Health Across 35 Global Cities</title>
      <link>https://arxiv.org/abs/2407.15770</link>
      <description>arXiv:2407.15770v1 Announce Type: cross 
Abstract: Urban parks provide significant health benefits by offering spaces and facilities for various recreational and leisure activities. However, the capacity of specific park spaces and elements to foster health remains underexamined. Traditional studies have focused on parks' size, greenery, and accessibility, often overlooking their ability to facilitate specific health-promoting activities. To address this gap, we propose a taxonomy consisting of six categories of health-promoting activities in parks: physical, mind-body, nature appreciation, environmental, social, and cultural. We estimate the capacity of parks in 35 global cities to promote health by establishing a lexicon linking park spaces and elements with specific health-promoting activities from our taxonomy. Using this lexicon, we collected data on elements and spaces in all parks in 35 cities from OpenStreetMap. Our analysis covers 23,477 parks with a total of 827,038 elements and spaces. By first comparing similarly sized parks across cities, we found that North American parks offer more spaces for physical activities, while European parks focus more on nature appreciation. Second, by scoring parks based on both elements and spaces, we investigated the variability in their health-promoting potential. We found the most uniform provision across parks for physical activities and the highest disparities regarding social activities. Additionally, parks offering a variety of activities are usually located in city centers, while offerings diminish in parks towards the suburbs. Lastly, we identified significant inequalities in park standards across cities, regardless of their continental location: Tokyo and Paris offer the most uniform park standards, while Copenhagen and Rio de Janeiro exhibit the most pronounced disparities. Our study provides insights for making urban parks more equitable, engaging, and health-promoting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15770v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linus W. Dietz, Sanja \v{S}\'cepanovi\'c, Ke Zhou, Andr\'e Felipe Zanella, Daniele Quercia</dc:creator>
    </item>
    <item>
      <title>Fusion of Movement and Naive Predictions for Point Forecasting in Univariate Random Walks</title>
      <link>https://arxiv.org/abs/2406.14469</link>
      <description>arXiv:2406.14469v4 Announce Type: replace 
Abstract: Point forecasting in univariate random walks is an important but challenging research topic that has attracted numerous researchers. Unfortunately, traditional regression methods for this task often fail to surpass naive benchmarks due to data unpredictability. From a decision fusion perspective, this study proposes a novel forecasting method, which is derived from a variant definition of random walks, where the random error term for the future value is expressed as a positive random error multiplied by a direction sign. This method, based on the fusion of movement and naive predictions, does not require a loss function for optimization and can be optimized by estimating movement prediction accuracy on the validation set. This characteristic prevents the fusion method from reverting to traditional regression methods and allows it to integrate various machine learning and deep learning models for movement prediction. The method's efficacy is demonstrated through simulations and real-world data experiments. It reliably outperforms naive forecasts with moderate movement prediction accuracies, such as 0.55, and is superior to baseline models such as the ARIMA, linear regression, MLP, and LSTM networks in forecasting the S&amp;P 500 index and Bitcoin prices. This method is particularly advantageous when accurate point predictions are challenging but accurate movement predictions are attainable, translating movement predictions into point forecasts in random walk contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14469v4</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cheng Zhang</dc:creator>
    </item>
    <item>
      <title>Physics-informed active learning with simultaneous weak-form latent space dynamics identification</title>
      <link>https://arxiv.org/abs/2407.00337</link>
      <description>arXiv:2407.00337v2 Announce Type: replace 
Abstract: The parametric greedy latent space dynamics identification (gLaSDI) framework has demonstrated promising potential for accurate and efficient modeling of high-dimensional nonlinear physical systems. However, it remains challenging to handle noisy data. To enhance robustness against noise, we incorporate the weak-form estimation of nonlinear dynamics (WENDy) into gLaSDI. In the proposed weak-form gLaSDI (WgLaSDI) framework, an autoencoder and WENDy are trained simultaneously to discover intrinsic nonlinear latent-space dynamics of high-dimensional data. Compared to the standard sparse identification of nonlinear dynamics (SINDy) employed in gLaSDI, WENDy enables variance reduction and robust latent space discovery, therefore leading to more accurate and efficient reduced-order modeling. Furthermore, the greedy physics-informed active learning in WgLaSDI enables adaptive sampling of optimal training data on the fly for enhanced modeling accuracy. The effectiveness of the proposed framework is demonstrated by modeling various nonlinear dynamical problems, including viscous and inviscid Burgers' equations, time-dependent radial advection, and the Vlasov equation for plasma physics. With data that contains 5-10% Gaussian white noise, WgLaSDI outperforms gLaSDI by orders of magnitude, achieving 1-7% relative errors. Compared with the high-fidelity models, WgLaSDI achieves 121 to 1,779x speed-up.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00337v2</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaolong He, April Tran, David M. Bortz, Youngsoo Choi</dc:creator>
    </item>
    <item>
      <title>The Dark Side of NFTs: A Large-Scale Empirical Study of Wash Trading</title>
      <link>https://arxiv.org/abs/2312.12544</link>
      <description>arXiv:2312.12544v3 Announce Type: replace-cross 
Abstract: NFTs (Non-Fungible Tokens) have seen significant growth since they first captured public attention in 2021. However, the NFT market is plagued by fake transactions and economic bubbles, e.g., NFT wash trading. Wash trading typically refers to a transaction involving the same person or two colluding individuals, and has become a major threat to the NFT ecosystem. Previous studies only detect NFT wash trading from the financial aspect, while the real-world wash trading cases are much more complicated (e.g., not aiming at inflating the market value). There is still a lack of multi-dimension analysis to better understand NFT wash trading. Therefore, we present the most comprehensive study of NFT wash trading, analyzing 8,717,031 transfer events and 3,830,141 sale events from 2,701,883 NFTs. We first optimize the dataset collected via the OpenSea API. Next, we identify three types of NFT wash trading and propose identification algorithms. Our experimental results reveal 824 transfer events and 5,330 sale events (accounting for a total of \$8,857,070.41) and 370 address pairs related to NFT wash trading behaviors, causing a minimum loss of \$3,965,247.13. Furthermore, we provide insights from six aspects, i.e., marketplace design, profitability, NFT project design, payment token, user behavior, and NFT ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12544v3</guid>
      <category>cs.CR</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijian Chen, Jiachi Chen, Jiangshan Yu, Xiapu Luo, Yanlin Wang</dc:creator>
    </item>
    <item>
      <title>A Physics Preserving Neural Network Based Approach for Constitutive Modeling of Isotropic Fibrous Materials</title>
      <link>https://arxiv.org/abs/2403.13357</link>
      <description>arXiv:2403.13357v3 Announce Type: replace-cross 
Abstract: We develop a new neural network architecture that strictly enforces constitutive constraints such as polyconvexity, frame-indifference, and the symmetry of the stress and material stiffness. Additionally, we show that the accuracy of the stress and material stiffness predictions is significantly improved for this neural network by using a Sobolev minimization strategy that includes derivative terms. Using our neural network, we model the constitutive behavior of fibrous-type discrete network material. With Sobolev minimization, we obtain a normalized mean square error of 0.15% for the strain energy density, 0.815% averaged across the components of the stress, and 5.4% averaged across the components of the stiffness tensor. This machine-learned constitutive model was deployed in a finite element simulation of a facet capsular ligament. The displacement fields and stress-strain curves were compared to a multiscale simulation that required running on a GPU-based supercomputer. The new approach maintained upward of 85% accuracy in stress up to 70% strain while reducing the computation cost by orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13357v3</guid>
      <category>physics.bio-ph</category>
      <category>cond-mat.soft</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nishan Parvez, Jacob S. Merson</dc:creator>
    </item>
    <item>
      <title>JaFIn: Japanese Financial Instruction Dataset</title>
      <link>https://arxiv.org/abs/2404.09260</link>
      <description>arXiv:2404.09260v2 Announce Type: replace-cross 
Abstract: We construct an instruction dataset for the large language model (LLM) in the Japanese finance domain. Domain adaptation of language models, including LLMs, is receiving more attention as language models become more popular. This study demonstrates the effectiveness of domain adaptation through instruction tuning. To achieve this, we propose an instruction tuning data in Japanese called JaFIn, the Japanese Financial Instruction Dataset. JaFIn is manually constructed based on multiple data sources, including Japanese government websites, which provide extensive financial knowledge. We then utilize JaFIn to apply instruction tuning for several LLMs, demonstrating that our models specialized in finance have better domain adaptability than the original models. The financial-specialized LLMs created were evaluated using a quantitative Japanese financial benchmark and qualitative response comparisons, showing improved performance over the originals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09260v2</guid>
      <category>cs.CL</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kota Tanabe, Masahiro Suzuki, Hiroki Sakaji, Itsuki Noda</dc:creator>
    </item>
    <item>
      <title>Deformation-Recovery Diffusion Model (DRDM): Instance Deformation for Image Manipulation and Synthesis</title>
      <link>https://arxiv.org/abs/2407.07295</link>
      <description>arXiv:2407.07295v2 Announce Type: replace-cross 
Abstract: In medical imaging, the diffusion models have shown great potential in synthetic image generation tasks. However, these models often struggle with the interpretable connections between the generated and existing images and could create illusions. To address these challenges, our research proposes a novel diffusion-based generative model based on deformation diffusion and recovery. This model, named Deformation-Recovery Diffusion Model (DRDM), diverges from traditional score/intensity and latent feature-based approaches, emphasizing morphological changes through deformation fields rather than direct image synthesis. This is achieved by introducing a topological-preserving deformation field generation method, which randomly samples and integrates a set of multi-scale Deformation Vector Fields (DVF). DRDM is trained to learn to recover unreasonable deformation components, thereby restoring each randomly deformed image to a realistic distribution. These innovations facilitate the generation of diverse and anatomically plausible deformations, enhancing data augmentation and synthesis for further analysis in downstream tasks, such as few-shot learning and image registration. Experimental results in cardiac MRI and pulmonary CT show DRDM is capable of creating diverse, large (over 10\% image size deformation scale), and high-quality (negative rate of the Jacobian matrix's determinant is lower than 1\%) deformation fields. The further experimental results in downstream tasks, 2D image segmentation and 3D image registration, indicate significant improvements resulting from DRDM, showcasing the potential of our model to advance image manipulation and synthesis in medical imaging and beyond.
  Project page: https://jianqingzheng.github.io/def_diff_rec/</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07295v2</guid>
      <category>eess.IV</category>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian-Qing Zheng, Yuanhan Mo, Yang Sun, Jiahua Li, Fuping Wu, Ziyang Wang, Tonia Vincent, Bart{\l}omiej W. Papie\.z</dc:creator>
    </item>
  </channel>
</rss>
