<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Feb 2026 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Some Asymptotic Results on Multiple Testing under Weak Dependence</title>
      <link>https://arxiv.org/abs/2602.21359</link>
      <description>arXiv:2602.21359v1 Announce Type: new 
Abstract: This paper studies the means-testing problem under weakly correlated Normal setups. Although quite common in genomic applications, test procedures having exact FWER control under such dependence structures are nonexistent. We explore the asymptotic behaviors of the classical Bonferroni (when adjusted suitably) and the Sidak procedure; and show that both of these control FWER at the desired level exactly as the number of hypotheses approaches infinity. We derive analogous limiting results on the generalized family-wise error rate and power. Simulation studies depict the asymptotic exactness of the procedures empirically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21359v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Swarnadeep Datta, Monitirtha Dey</dc:creator>
    </item>
    <item>
      <title>Causal Inference with High-Dimensional Treatments</title>
      <link>https://arxiv.org/abs/2602.21423</link>
      <description>arXiv:2602.21423v1 Announce Type: new 
Abstract: In this work, we consider causal inference in various high-dimensional treatment settings, including for single multi-valued treatments and vector treatments with binary or continuous components, when the number of treatments can be comparable to or even larger than the number of observations. These settings bring unique challenges: first, the treatment effects of interest are a high-dimensional vector rather than a low-dimensional scalar; second, positivity violations are often unavoidable; and third, estimation can be based on a smaller effective sample size. We first discuss fundamental limits of estimating effects here, showing that consistent estimation is impossible without further assumptions. We go on to propose a novel sparse pseudo-outcome regression framework for arbitrary high-dimensional statistical functionals, which includes generic constrained regression estimators and error guarantees. We use the framework to derive new doubly robust estimators for mean potential outcomes of high-dimensional treatments, though it can also be applied to other scenarios. We analyze the proposed estimators under exact and approximate sparsity assumptions, giving finite-sample risk bounds. Finally, we derive minimax lower bounds to characterize optimal rates of convergence and show our risk bounds are unimprovable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21423v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Kramer, Edward H. Kennedy, Isaac M. Opper</dc:creator>
    </item>
    <item>
      <title>Exponential Concentration Inequalities For Independent Random Vectors Under Sublinear Expectations</title>
      <link>https://arxiv.org/abs/2602.21465</link>
      <description>arXiv:2602.21465v1 Announce Type: new 
Abstract: Li and Hu recently established variance-type O(1/n) bounds for the sample mean of independent random vectors under sublinear expectations. We extend their results to the exponential concentration regime. For bounded, independent R^d-valued random vectors under a regular sublinear expectation, we prove: (i) a general concentration principle that reduces vector-valued tail bounds to scalar martingale inequalities via a three-layer architecture; (ii) an Azuma-Hoeffding inequality showing that the distance from the sample mean to the Minkowski average of the expectation sets has sub-Gaussian tails; (iii) a Bernstein inequality incorporating the variance parameter of Li and Hu, interpolating between sub-Gaussian and sub-exponential regimes; (iv) a dimension-free bound replacing the exponential covering prefactor with a polynomial one via the matrix Freedman inequality; and (v) an explicit construction demonstrating that the sub-Gaussian rate is optimal. To the best of our knowledge, these constitute the first exponential concentration inequalities for the multivariate sample mean under sublinear expectations in terms of the set-valued distance to the Minkowski average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21465v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nahom Seyoum</dc:creator>
    </item>
    <item>
      <title>Moment bounds for condition numbers and singular values of high-dimensional Gaussian random matrices: Applications and limitations</title>
      <link>https://arxiv.org/abs/2602.21487</link>
      <description>arXiv:2602.21487v1 Announce Type: new 
Abstract: Spectral properties of Gram matrices are central to high dimensional asymptotic analyses of statistical estimators in regression and covariance estimation. These properties, in turn, depend critically on the extreme singular values and condition numbers of Gaussian random matrices. For many applications, sharp positive and negative moment bounds for these quantities are required to control expected prediction risk and related performance metrics. Although extensive work provides concentration and tail bounds for extreme singular values of Gaussian random matrices, these results do not readily yield the moment bounds needed in such analyses. Motivated by this gap, we establish non asymptotic moment bounds for arbitrary positive moments of the largest singular value and arbitrary negative moments of the smallest singular value, and uniform bounds for arbitrary positive moments of the condition number of high dimensional Gaussian random matrices. We demonstrate the utility of these bounds by applying them to derive explicit risk guarantees in high dimensional regression and covariance estimation, as well as to obtain bounds on the mean iteration complexity of gradient descent for solving Gram linear systems. Finally, we present counterexamples demonstrating that the positive condition number moment bounds and negative smallest singular value moment bounds cannot, in general, be extended to the broader class of sub Gaussian random matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21487v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Partha Sarkar, Kshitij Khare, Sanvesh Srivastava</dc:creator>
    </item>
    <item>
      <title>How many asymmetric communities are there in multi-layer directed networks?</title>
      <link>https://arxiv.org/abs/2602.21569</link>
      <description>arXiv:2602.21569v1 Announce Type: new 
Abstract: Estimating the asymmetric numbers of communities in multi-layer directed networks is a challenging problem due to the multi-layer structures and inherent directional asymmetry, leading to possibly different numbers of sender and receiver communities. This work addresses this issue under the multi-layer stochastic co-block model, a model for multi-layer directed networks with distinct community structures in sending and receiving sides, by proposing a novel goodness-of-fit test. The test statistic relies on the deviation of the largest singular value of an aggregated normalized residual matrix from the constant 2. The test statistic exhibits a sharp dichotomy: Under the null hypothesis of correct model specification, its upper bound converges to zero with high probability; under underfitting, the test statistic itself diverges to infinity. With this property, we develop a sequential testing procedure that searches through candidate pairs of sender and receiver community numbers in a lexicographic order. The process stops at the smallest such pair where the test statistic drops below a decaying threshold. For robustness, we also propose a ratio-based variant algorithm, which detects sharp changes in the sequence of test statistics by comparing consecutive candidates. Both methods are proven to consistently determine the true numbers of sender and receiver communities under the multi-layer stochastic co-block model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21569v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Qing</dc:creator>
    </item>
    <item>
      <title>Estimation of the Self-similarity Index of Non-stationary Increments Self-similar Processes via Lamperti Transformations</title>
      <link>https://arxiv.org/abs/2602.21764</link>
      <description>arXiv:2602.21764v1 Announce Type: new 
Abstract: We introduce a novel method for estimating the self-similarity index of a general $H$-self-similar process with either stationary or non-stationary increments. The estimation algorithm is developed based on a modified Lamperti transformation, which transforms $H$-self-similar processes to stationary ones. As an application, we show how to use this approach to estimate the self-similarity index of fractional Brownian motion, subfractional Brownian motion, bifractional Brownian motion, and trifractional Brownian motion. Simulation study is performed to support the consistency of our estimators. Implementation in Python is publicly shared. Application on the estimation of the self-similarity index of the Nile river water level data from the year 900 to 1200 C.E..</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21764v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>William Wu, Qidi Peng</dc:creator>
    </item>
    <item>
      <title>Confidence in confidence distributions!</title>
      <link>https://arxiv.org/abs/2602.22178</link>
      <description>arXiv:2602.22178v1 Announce Type: new 
Abstract: The recent article `Satellite conjunction analysis and the false confidence theorem' (Balch, Martin, and Ferson, 2019, Proceedings of the Royal Society, Series A) points to certain difficulties with Bayesian analysis when used for models for satellite conjuntion and ensuing operative decisions. Here we supplement these previous analyses and findings with further insights, uncovering what we perceive of as being the crucial points, explained in a prototype setup where exact analysis is attainable. We also show that a different and frequentist method, involving confidence distributions, is free of the false confidence syndrome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22178v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C\'eline Cunen, Nils Lid Hjort, Tore Schweder</dc:creator>
    </item>
    <item>
      <title>Linguistic Approach to Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2207.00985</link>
      <description>arXiv:2207.00985v1 Announce Type: cross 
Abstract: This paper proposes methods of predicting dynamic time series (including non-stationary ones) based on a linguistic approach, namely, the study of occurrences and repetition of so-called N-grams. This approach is used in computational linguistics to create statistical translators, detect plagiarism and duplicate documents. However, the scope of application can be extended beyond linguistics by taking into account the correlations of sequences of stable word combinations, as well as trends. The proposed methods do not require a preliminary study and determination of the characteristics of time series or complex tuning of the input parameters of the forecasting model. They allow, with a high level of automation, to carry out short-term and medium-term forecasts of time series, characterized by trends and cyclicality, in particular, series of publication dynamics in content monitoring systems. Also, the proposed methods can be used to predict the values of the parameters of a large complex system with the aim of monitoring its state, when the number of such parameters is significant, and therefore a high level of automation of the forecasting process is desirable. A significant advantage of the approach is the absence of requirements for time series stationarity and a small number of tuning parameters. Further research may focus on the study of various criteria for the similarity of time series fragments, the use of nonlinear similarity criteria, the search for ways to automatically determine the rational step of quantization of the time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.00985v1</guid>
      <category>math.NA</category>
      <category>cs.DM</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmytro Lande, Volodymyr Yuzefovych, Yevheniia Tsybulska</dc:creator>
    </item>
    <item>
      <title>Efficient Inference after Directionally Stable Adaptive Experiments</title>
      <link>https://arxiv.org/abs/2602.21478</link>
      <description>arXiv:2602.21478v1 Announce Type: cross 
Abstract: We study inference on scalar-valued pathwise differentiable targets after adaptive data collection, such as a bandit algorithm. We introduce a novel target-specific condition, directional stability, which is strictly weaker than previously imposed target-agnostic stability conditions. Under directional stability, we show that estimators that would have been efficient under i.i.d. data remain asymptotically normal and semiparametrically efficient when computed from adaptively collected trajectories. The canonical gradient has a martingale form, and directional stability guarantees stabilization of its predictable quadratic variation, enabling high-dimensional asymptotic normality. We characterize efficiency using a convolution theorem for the adaptive-data setting, and give a condition under which the one-step estimator attains the efficiency bound. We verify directional stability for LinUCB, yielding the first semiparametric efficiency guarantee for a regular scalar target under LinUCB sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21478v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zikai Shen, Houssam Zenati, Nathan Kallus, Arthur Gretton, Koulik Khamaru, Aur\'elien Bibaut</dc:creator>
    </item>
    <item>
      <title>A Researcher's Guide to Empirical Risk Minimization</title>
      <link>https://arxiv.org/abs/2602.21501</link>
      <description>arXiv:2602.21501v1 Announce Type: cross 
Abstract: This guide develops high-probability regret bounds for empirical risk minimization (ERM). The presentation is modular: we state broadly applicable guarantees under high-level conditions and give tools for verifying them for specific losses and function classes. We emphasize that many ERM rate derivations can be organized around a three-step recipe -- a basic inequality, a uniform local concentration bound, and a fixed-point argument -- which yields regret bounds in terms of a critical radius, defined via localized Rademacher complexity, under a mild Bernstein-type variance--risk condition. To make these bounds concrete, we upper bound the critical radius using local maximal inequalities and metric-entropy integrals, recovering familiar rates for VC-subgraph, Sobolev/H\"older, and bounded-variation classes.
  We also review ERM with nuisance components -- including weighted ERM and Neyman-orthogonal losses -- as they arise in causal inference, missing data, and domain adaptation. Following the orthogonal learning framework, we highlight that these problems often admit regret-transfer bounds linking regret under an estimated loss to population regret under the target loss. These bounds typically decompose regret into (i) statistical error under the estimated (optimized) loss and (ii) approximation error due to nuisance estimation. Under sample splitting or cross-fitting, the first term can be controlled using standard fixed-loss ERM regret bounds, while the second term depends only on nuisance-estimation accuracy. We also treat the in-sample regime, where nuisances and the ERM are fit on the same data, deriving regret bounds and giving sufficient conditions for fast rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21501v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan</dc:creator>
    </item>
    <item>
      <title>Adaptive Penalized Doubly Robust Regression for Longitudinal Data</title>
      <link>https://arxiv.org/abs/2602.21711</link>
      <description>arXiv:2602.21711v1 Announce Type: cross 
Abstract: Longitudinal data often involve heterogeneity, sparse signals, and contamination from response outliers or high-leverage observations especially in biomedical science. Existing methods usually address only part of this problem, either emphasizing penalized mixed effects modeling without robustness or robust mixed effects estimation without high-dimensional variable selection. We propose a doubly adaptive robust regression (DAR-R) framework for longitudinal linear mixed effects models. It combines a robust pilot fit, doubly adaptive observation weights for residual outliers and leverage points, and folded concave penalization for fixed effect selection, together with weighted updates of random effects and variance components. We develop an iterative reweighting algorithm and establish estimation and prediction error bounds, support recovery consistency, and oracle-type asymptotic normality. Simulations show that DAR-R improves estimation accuracy, false-positive control, and covariance estimation under both vertical outliers and bad leverage contamination. In the TADPOLE/ADNI Alzheimer's disease application, DAR-R achieves accurate and stable prediction of ADAS13 while selecting clinically meaningful predictors with strong resampling stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21711v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuyao Wang, Yu Lu, Tianni Zhang, Mengfei Ran</dc:creator>
    </item>
    <item>
      <title>Scalable Kernel-Based Distances for Statistical Inference and Integration</title>
      <link>https://arxiv.org/abs/2602.21846</link>
      <description>arXiv:2602.21846v1 Announce Type: cross 
Abstract: Representing, comparing, and measuring the distance between probability distributions is a key task in computational statistics and machine learning. The choice of representation and the associated distance determine properties of the methods in which they are used: for example, certain distances can allow one to encode robustness or smoothness of the problem. Kernel methods offer flexible and rich Hilbert space representations of distributions that allow the modeller to enforce properties through the choice of kernel, and estimate associated distances at efficient nonparametric rates. In particular, the maximum mean discrepancy (MMD), a kernel-based distance constructed by comparing Hilbert space mean functions, has received significant attention due to its computational tractability and is favoured by practitioners.
  In this thesis, we conduct a thorough study of kernel-based distances with a focus on efficient computation, with core contributions in Chapters 3 to 6. Part I of the thesis is focused on the MMD, specifically on improved MMD estimation. In Chapter 3 we propose a theoretically sound, improved estimator for MMD in simulation-based inference. Then, in Chapter 4, we propose an MMD-based estimator for conditional expectations, a ubiquitous task in statistical computation. Closing Part I, in Chapter 5 we study the problem of calibration when MMD is applied to the task of integration.
  In Part II, motivated by the recent developments in kernel embeddings beyond the mean, we introduce a family of novel kernel-based discrepancies: kernel quantile discrepancies. These address some of the pitfalls of MMD, and are shown through both theoretical results and an empirical study to offer a competitive alternative to MMD and its fast approximations. We conclude with a discussion on broader lessons and future work emerging from the thesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21846v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Masha Naslidnyk</dc:creator>
    </item>
    <item>
      <title>Stochastic Optimal Control with Side Information and Bayesian Learning</title>
      <link>https://arxiv.org/abs/2602.22047</link>
      <description>arXiv:2602.22047v1 Announce Type: cross 
Abstract: We study infinite-horizon stochastic optimal control problems with observable side information: a Markov chain that modulates an unknown context-conditional randomness distribution. Since this distribution is unknown, we propose a Bayesian reformulation based on a parametric density model and posterior predictive dynamics, which yields a Bayesian Bellman equation. We prove posterior consistency under Markov samples and, under correct specification and identifiability, uniform convergence of the Bayesian value function. Finally, we establish Bernstein--von Mises-type asymptotic normality for the data-driven contextual optimal value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22047v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Milz, Alexander Shapiro, Enlu Zhou</dc:creator>
    </item>
    <item>
      <title>Robust Model Selection for Discovery of Latent Mechanistic Processes</title>
      <link>https://arxiv.org/abs/2602.22062</link>
      <description>arXiv:2602.22062v1 Announce Type: cross 
Abstract: When learning interpretable latent structures using model-based approaches, even small deviations from modeling assumptions can lead to inferential results that are not mechanistically meaningful. In this work, we consider latent structures that consist of $K_o$ mechanistic processes, where $K_o$ is unknown. When the model is misspecified, likelihood-based model selection methods can substantially overestimate $K_o$ while more robust nonparametric methods can be overly conservative. Hence, there is a need for approaches that combine the sensitivity of likelihood-based methods with the robustness of nonparametric ones. We formalize this objective in terms of a robust model selection consistency property, which is based on a component-level discrepancy measure that captures the mechanistic structure of the model. We then propose the accumulated cutoff discrepancy criterion (ACDC), which leverages plug-in estimates of component-level discrepancies. To apply ACDC, we develop mechanistically meaningful component-level discrepancies for a general class of latent variable models that includes unsupervised and supervised variants of probabilistic matrix factorization and mixture modeling. We show that ACDC is robustly consistent when applied to unsupervised matrix factorization and mixture models. Numerical results demonstrate that in practice our approach reliably identifies a mechanistically meaningful number of latent processes in numerous illustrative applications, outperforming existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22062v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Li, Nguyen Nguyen, Meng Lai, Ioannis Ch. Paschalidis, Jonathan H. Huggins</dc:creator>
    </item>
    <item>
      <title>Estimation of a single parameter of some probability distributions using L2 optimization</title>
      <link>https://arxiv.org/abs/2311.02858</link>
      <description>arXiv:2311.02858v3 Announce Type: replace 
Abstract: We propose a minimum distance estimation of a rate parameter of some probability distributions. This paper discusses asymptotic properties of the resulting estimator. Next, we compare the proposed estimator with other estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02858v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiwoong Kim</dc:creator>
    </item>
    <item>
      <title>Quasi-Monte Carlo confidence intervals using quantiles of randomized nets</title>
      <link>https://arxiv.org/abs/2504.19138</link>
      <description>arXiv:2504.19138v2 Announce Type: replace 
Abstract: Recent advances in quasi-Monte Carlo integration have shown that for linearly scrambled digital net estimators, the convergence rate can be dramatically improved by taking the median rather than the mean of multiple independent replicates. In this work, we demonstrate that the quantiles of such estimators can be used to construct confidence intervals with asymptotically valid coverage for high-dimensional integrals. By analyzing the error distribution for a class of infinitely differentiable integrands, we prove that as the sample size increases, the integration error decomposes into an asymptotically symmetric component and a vanishing remainder. Consequently, the asymptotic error distribution is symmetric about zero, ensuring that a quantile-based interval constructed from independent replicates captures the true integral with probability converging to a nominal level determined by the binomial distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19138v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zexin Pan</dc:creator>
    </item>
    <item>
      <title>Gaussian Approximation for High-Dimensional Second-Order $U$- and $V$-statistics with Size-Dependent Kernels under i.n.i.d. Sampling</title>
      <link>https://arxiv.org/abs/2511.08870</link>
      <description>arXiv:2511.08870v3 Announce Type: replace 
Abstract: We develop Gaussian approximations for high-dimensional vectors formed by second-order $U$- and $V$-statistics whose kernels depend on sample size under independent but not identically distributed (i.n.i.d.) sampling. Our results hold irrespective of which component of the Hoeffding decomposition is dominant, thereby covering both non-degenerate and degenerate regimes as special cases. By allowing i.n.i.d.~sampling, the class of statistics we analyze includes weighted $U$- and $V$-statistics and two-sample $U$- and $V$-statistics as special cases, which cover estimators of parameters in regression models with many covariates, many-weak instruments as well as a broad class of smoothed two-sample tests and the separately exchangeable arrays, among others. In addition, we extend sharp maximal inequalities for high-dimensional $U$-statistics with size-dependent kernels from the i.i.d.~to the i.n.i.d.~setting, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08870v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunsuke Imai</dc:creator>
    </item>
    <item>
      <title>Minimum Variance Designs With Constrained Maximum Bias</title>
      <link>https://arxiv.org/abs/2512.21806</link>
      <description>arXiv:2512.21806v5 Announce Type: replace 
Abstract: Designs which are minimax in the presence of model misspecifications have been constructed so as to minimize the maximum, over classes of alternate response models, of the integrated mean squared error of the predicted values. This mean squared error decomposes into a term arising solely from variation, and a bias term arising from the model errors. Here we consider the problem of designing so as to minimize the variance of the predictors, subject to a bound on the maximum (over model misspecifications) bias. We consider as well designing so as to minimize the maximum bias, subject to a bound on the variance. We show that solutions to both problems are given by the minimax designs, with appropriately chosen values of their tuning constants. Conversely, any minimax design solves each problem for an appropriate choice of the bound on the maximum bias or on the variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21806v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Douglas P. Wiens</dc:creator>
    </item>
    <item>
      <title>Teachable normal approximations to binomial and related probabilities or confidence bounds</title>
      <link>https://arxiv.org/abs/2503.20852</link>
      <description>arXiv:2503.20852v2 Announce Type: replace-cross 
Abstract: For the usual normal approximations to binomial, hypergeometric, or Poisson interval probabilities, we collect some simple but then reasonably sharp error bounds. For the Clopper-Pearson~(1934) binomial confidence bounds, we present, following Michael Short's~(2023) approach, bounds similar to, but necessarily more complicated than, Lagrange's (1776) success rate plus/minus normal quantile times estimated standard deviation.
  The bounds, as presented here in four theorems, should be teachable, to people ranging from sufficiently advanced high school pupils to university students in mathematics or statistics: For understanding most of the proposed approximation results, it should suffice to know binomial laws, their means and variances, and the standard normal distribution function, but not necessarily the concept of a corresponding normal random variable.
  Accompanying technical remarks, references, and proofs are meant for assuring teachers or for stimulating further research.
  Of the proposed approximations, some are essentially well-known at least to experts, and some are based on teaching experience and research at Trier University.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20852v2</guid>
      <category>stat.OT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lutz Mattner</dc:creator>
    </item>
    <item>
      <title>Minimax Rates for Learning Pairwise Interactions in Attention-Style Models</title>
      <link>https://arxiv.org/abs/2510.11789</link>
      <description>arXiv:2510.11789v2 Announce Type: replace-cross 
Abstract: We study the convergence rate of learning pairwise interactions in single-layer attention-style models, where tokens interact through a weight matrix and a nonlinear activation function. We prove that the minimax rate is $M^{-\frac{2\beta}{2\beta+1}}$, where $M$ is the sample size and $\beta$ is the H\"older smoothness of the activation function. Importantly, this rate is independent of the embedding dimension $d$, the number of tokens $N$, and the rank $r$ of the weight matrix, provided that $rd \le (M/\log M)^{\frac{1}{2\beta+1}}$. These results highlight a fundamental statistical efficiency of attention-style models, even when the weight matrix and activation are not separately identifiable, and provide a theoretical understanding of attention mechanisms and guidance on training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11789v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shai Zucker, Xiong Wang, Fei Lu, Inbar Seroussi</dc:creator>
    </item>
    <item>
      <title>The generalized underlap coefficient with an application in clustering</title>
      <link>https://arxiv.org/abs/2602.19473</link>
      <description>arXiv:2602.19473v2 Announce Type: replace-cross 
Abstract: Quantifying distributional separation across groups is fundamental in statistical learning and scientific discovery, yet most classical discrepancy measures are tailored to two-group comparisons. We generalize the underlap coefficient (UNL), a multi-group separation measure, to multivariate variables. We establish key properties of the UNL and provide an explicit connection to total variation. We further interpret the UNL as a dependence measure between a group label and variables of interest and compare it with mutual information. We propose an efficient importance sampling estimator of the UNL that can be combined with flexible density estimators. The utility of the UNL for assessing partition-covariate dependence in clustering is highlighted in detail, where it is particularly useful for evaluating whether the latent group structure can be explained by specific covariates. Finally we illustrate the application of the UNL in clustering using two real world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19473v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoxi Zhang, Vanda Inacio, Sara Wade</dc:creator>
    </item>
  </channel>
</rss>
