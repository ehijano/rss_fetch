<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Apr 2024 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Finite-memory Strategies for Almost-sure Energy-MeanPayoff Objectives in MDPs</title>
      <link>https://arxiv.org/abs/2404.14522</link>
      <description>arXiv:2404.14522v1 Announce Type: new 
Abstract: We consider finite-state Markov decision processes with the combined Energy-MeanPayoff objective. The controller tries to avoid running out of energy while simultaneously attaining a strictly positive mean payoff in a second dimension. We show that finite memory suffices for almost surely winning strategies for the Energy-MeanPayoff objective. This is in contrast to the closely related Energy-Parity objective, where almost surely winning strategies require infinite memory in general. We show that exponential memory is sufficient (even for deterministic strategies) and necessary (even for randomized strategies) for almost surely winning Energy-MeanPayoff. The upper bound holds even if the strictly positive mean payoff part of the objective is generalized to multidimensional strictly positive mean payoff. Finally, it is decidable in pseudo-polynomial time whether an almost surely winning strategy exists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14522v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohan Dantam, Richard Mayr</dc:creator>
    </item>
    <item>
      <title>A Multi-Dimensional Online Contention Resolution Scheme for Revenue Maximization</title>
      <link>https://arxiv.org/abs/2404.14679</link>
      <description>arXiv:2404.14679v1 Announce Type: new 
Abstract: We study multi-buyer multi-item sequential item pricing mechanisms for revenue maximization with the goal of approximating a natural fractional relaxation -- the ex ante optimal revenue. We assume that buyers' values are subadditive but make no assumptions on the value distributions. While the optimal revenue, and therefore also the ex ante benchmark, is inapproximable by any simple mechanism in this context, previous work has shown that a weaker benchmark that optimizes over so-called ``buy-many" mechanisms can be approximable. Approximations are known, in particular, for settings with either a single buyer or many unit-demand buyers. We extend these results to the much broader setting of many subadditive buyers. We show that the ex ante buy-many revenue can be approximated via sequential item pricings to within an $O(\log^2 m)$ factor, where $m$ is the number of items. We also show that a logarithmic dependence on $m$ is necessary.
  Our approximation is achieved through the construction of a new multi-dimensional Online Contention Resolution Scheme (OCRS), that provides an online rounding of the optimal ex ante solution. Chawla et al. arXiv:2204.01962 previously constructed an OCRS for revenue for unit-demand buyers, but their construction relied heavily on the ``almost single dimensional" nature of unit-demand values. Prior to that work, OCRSes have only been studied in the context of social welfare maximization for single-parameter buyers. For the welfare objective, constant-factor approximations have been demonstrated for a wide range of combinatorial constraints on item allocations and classes of buyer valuation functions. Our work opens up the possibility of a similar success story for revenue maximization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14679v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuchi Chawla, Dimitris Christou, Trung Dang, Zhiyi Huang, Gregory Kehne, Rojin Rezvan</dc:creator>
    </item>
    <item>
      <title>Using deep reinforcement learning to promote sustainable human behaviour on a common pool resource problem</title>
      <link>https://arxiv.org/abs/2404.15059</link>
      <description>arXiv:2404.15059v1 Announce Type: cross 
Abstract: A canonical social dilemma arises when finite resources are allocated to a group of people, who can choose to either reciprocate with interest, or keep the proceeds for themselves. What resource allocation mechanisms will encourage levels of reciprocation that sustain the commons? Here, in an iterated multiplayer trust game, we use deep reinforcement learning (RL) to design an allocation mechanism that endogenously promotes sustainable contributions from human participants to a common pool resource. We first trained neural networks to behave like human players, creating a stimulated economy that allowed us to study how different mechanisms influenced the dynamics of receipt and reciprocation. We then used RL to train a social planner to maximise aggregate return to players. The social planner discovered a redistributive policy that led to a large surplus and an inclusive economy, in which players made roughly equal gains. The RL agent increased human surplus over baseline mechanisms based on unrestricted welfare or conditional cooperation, by conditioning its generosity on available resources and temporarily sanctioning defectors by allocating fewer resources to them. Examining the AI policy allowed us to develop an explainable mechanism that performed similarly and was more popular among players. Deep reinforcement learning can be used to discover mechanisms that promote sustainable human behaviour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15059v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Koster, Miruna P\^islar, Andrea Tacchetti, Jan Balaguer, Leqi Liu, Romuald Elie, Oliver P. Hauser, Karl Tuyls, Matt Botvinick, Christopher Summerfield</dc:creator>
    </item>
    <item>
      <title>$\widetilde{O}(T^{-1})$ Convergence to (Coarse) Correlated Equilibria in Full-Information General-Sum Markov Games</title>
      <link>https://arxiv.org/abs/2403.07890</link>
      <description>arXiv:2403.07890v2 Announce Type: replace 
Abstract: No-regret learning has a long history of being closely connected to game theory. Recent works have devised uncoupled no-regret learning dynamics that, when adopted by all the players in normal-form games, converge to various equilibrium solutions at a near-optimal rate of $\widetilde{O}(T^{-1})$, a significant improvement over the $O(1/\sqrt{T})$ rate of classic no-regret learners. However, analogous convergence results are scarce in Markov games, a more generic setting that lays the foundation for multi-agent reinforcement learning. In this work, we close this gap by showing that the optimistic-follow-the-regularized-leader (OFTRL) algorithm, together with appropriate value update procedures, can find $\widetilde{O}(T^{-1})$-approximate (coarse) correlated equilibria in full-information general-sum Markov games within $T$ iterations. Numerical results are also included to corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07890v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weichao Mao, Haoran Qiu, Chen Wang, Hubertus Franke, Zbigniew Kalbarczyk, Tamer Ba\c{s}ar</dc:creator>
    </item>
    <item>
      <title>Dynamic Population Games: A Tractable Intersection of Mean-Field Games and Population Games</title>
      <link>https://arxiv.org/abs/2104.14662</link>
      <description>arXiv:2104.14662v2 Announce Type: replace-cross 
Abstract: In many real-world large-scale decision problems, self-interested agents have individual dynamics and optimize their own long-term payoffs. Important examples include the competitive access to shared resources (e.g., roads, energy, or bandwidth) but also non-engineering domains like epidemic propagation and control. These problems are natural to model as mean-field games. However, existing mathematical formulations of mean field games have had limited applicability in practice, since they require solving non-standard initial-terminal-value problems that are tractable only in limited special cases. In this letter, we propose a novel formulation, along with computational tools, for a practically relevant class of Dynamic Population Games (DPGs), which correspond to discrete-time, finite-state-and-action, stationary mean-field games. Our main contribution is a mathematical reduction of Stationary Nash Equilibria (SNE) in DPGs to standard Nash Equilibria (NE) in static population games. This reduction is leveraged to guarantee the existence of a SNE, develop an evolutionary dynamics-based SNE computation algorithm, and derive simple conditions that guarantee stability and uniqueness of the SNE. Additionally, DPGs enable us to tractably incorporate multiple agent types, which is of particular importance to assess fairness concerns in resource allocation problems. We demonstrate our results by computing the SNE in two complex application examples: fair resource allocation with heterogeneous agents and control of epidemic propagation.
  Open source software for SNE computation: https://gitlab.ethz.ch/elokdae/dynamic-population-games</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.14662v2</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <category>cs.SY</category>
      <category>econ.TH</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ezzat Elokda, Saverio Bolognani, Andrea Censi, Florian D\"orfler, Emilio Frazzoli</dc:creator>
    </item>
    <item>
      <title>Common Knowledge, Regained</title>
      <link>https://arxiv.org/abs/2311.04374</link>
      <description>arXiv:2311.04374v2 Announce Type: replace-cross 
Abstract: For common knowledge to arise in dynamic settings, all players must simultaneously come to know it has arisen. Consequently, common knowledge cannot arise in many realistic settings with timing frictions. This counterintuitive observation of Halpern and Moses (1990) was discussed by Arrow et al. (1987) and Aumann (1989), was called a paradox by Morris (2014), and has evaded satisfactory resolution for four decades. We resolve this paradox by proposing a new definition for common knowledge, which coincides with the traditional one in static settings but is more permissive in dynamic settings. Under our definition, common knowledge can arise without simultaneity, particularly in canonical examples of the Haplern-Moses paradox. We demonstrate its usefulness by deriving for it an agreement theorem \`a la Aumann (1976), showing it arises in the setting of Geanakoplos and Polemarchakis (1982) with timing frictions added, and applying it to characterize equilibrium behavior in a dynamic coordination game.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04374v2</guid>
      <category>econ.TH</category>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yannai A. Gonczarowski, Yoram Moses</dc:creator>
    </item>
  </channel>
</rss>
