<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Aug 2025 01:26:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Modelling Skewed and Heavy-Tailed Errors in Bayesian Mediation Analysis</title>
      <link>https://arxiv.org/abs/2508.09311</link>
      <description>arXiv:2508.09311v1 Announce Type: new 
Abstract: Traditional mediation models in both the frequentist and Bayesian frameworks typically assume normality of the error terms. Violations of this assumption can impair the estimation and hypothesis testing of the mediation effect in conventional approaches. This study addresses the non-normality issue by explicitly modelling skewed and heavy-tailed error terms within the Bayesian mediation framework. Building on the work of Fernandez and Steel (1998), this study introduces a novel family of distributions, termed the Centred Two-Piece Student $t$ Distribution (CTPT). The new distribution incorporates a skewness parameter into the Student t distribution and centres it to have a mean of zero, enabling flexible modelling of error terms in Bayesian regression and mediation analysis. A class of standard improper priors is employed, and conditions for the existence of the posterior distribution and posterior moments are established, while enabling inference on both skewness and tail parameters. Simulation studies are conducted to examine parameter recovery accuracy and statistical power in testing mediation effects. Compared to traditional Bayesian and frequentist methods, particularly bootstrap-based approaches, our method gives greater statistical power when correctly specified, while maintaining robustness against model misspecification. The application of the proposed approach is illustrated through real data analysis. Additionally, we have developed an R package FlexBayesMed to implement our methods in linear regression and mediation analysis, available at https://github.com/Zongyu-Li/FlexBayesMed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09311v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongyu Li, Mark Steel, Zhiyong Zhang</dc:creator>
    </item>
    <item>
      <title>Pseudo Empirical Likelihood Inference for Non-Probability Survey Samples</title>
      <link>https://arxiv.org/abs/2508.09356</link>
      <description>arXiv:2508.09356v1 Announce Type: new 
Abstract: In this paper, the authors first provide an overview of two major developments on complex survey data analysis: the empirical likelihood methods and statistical inference with non-probability survey samples, and highlight the important research contributions to the field of survey sampling in general and the two topics in particular by Canadian survey statisticians. The authors then propose new inferential procedures on analyzing non-probability survey samples through the pseudo empirical likelihood approach. The proposed methods lead to asymptotically equivalent point estimators that have been discussed in the recent literature but possess more desirable features on confidence intervals such as range-respecting and data-driven orientation. Results from a simulation study demonstrate the superiority of the proposed methods in dealing with binary response variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09356v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>The Canadian Journal of Statistics, 50, 1166-1185 (2022)</arxiv:journal_reference>
      <dc:creator>Yilin Chen, Pengfei Li, J. N. K. Rao, Changbao Wu</dc:creator>
    </item>
    <item>
      <title>Consistency assessment and regional sample size calculation for MRCTs under random effects model</title>
      <link>https://arxiv.org/abs/2508.09443</link>
      <description>arXiv:2508.09443v1 Announce Type: new 
Abstract: Multi-regional clinical trials (MRCTs) have become common practice for drug development and global registration. Once overall significance is established, demonstrating regional consistency is critical for local health authorities. Methods for evaluating such consistency and calculating regional sample sizes have been proposed based on the fixed effects model using various criteria. To better account for the heterogeneity of treatment effects across regions, the random effects model naturally arises as a more effective alternative for both design and inference. In this paper, we present the design of the overall sample size along with regional sample fractions. We also provide the theoretical footage for assessing consistency probability using Method 1 of MHLW (2007), based on the empirical shrinkage estimator. The latter is then used to determine the regional sample size of interest. We elaborate on the applications to common continuous, binary, and survival endpoints in detail. Simulation studies show that the proposed method retains the consistency probability at the desired level. We illustrate the application using a real cardiovascular outcome trial in diabetes. An R package is provided for implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09443v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinru Ren, Jin Xu</dc:creator>
    </item>
    <item>
      <title>Bayesian inference of antibody evolutionary dynamics using multitype branching processes</title>
      <link>https://arxiv.org/abs/2508.09519</link>
      <description>arXiv:2508.09519v1 Announce Type: new 
Abstract: When our immune system encounters foreign antigens (i.e., from pathogens), the B cells that produce our antibodies undergo a cyclic process of proliferation, mutation, and selection, improving their ability to bind to the specific antigen. Immunologists have recently developed powerful experimental techniques to investigate this process in mouse models. In one such experiment, mice are engineered with a monoclonal B-cell precursor and immunized with a model antigen. B cells are sampled from sacrificed mice after the immune response has progressed, and the mutated genetic loci encoding antibodies are sequenced. This experiment allows parallel replay of antibody evolution, but produces data at only one time point; we are unable to observe the evolutionary trajectories that lead to optimized antibody affinity in each mouse. To address this, we model antibody evolution as a multitype branching process and integrate over unobserved histories conditioned on phylogenetic signal in sequence data, leveraging parallel experimental replays for parameter inference. We infer the functional relationship between B-cell fitness and antigen binding affinity in a Bayesian framework, equipped with an efficient likelihood calculation algorithm and Markov chain Monte Carlo posterior approximation. In a simulation study, we demonstrate that a sigmoidal relationship between fitness and binding affinity can be recovered from realizations of the branching process. We then perform inference for experimental data from 52 replayed B-cell lineages sampled 15 days after immunization, yielding a total of 3,758 sampled B cells. The recovered sigmoidal curve indicates that the fitness of high-affinity B cells is over six times larger than that of low-affinity B cells, with a sharp transition from low to high fitness values as affinity increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09519v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Athanasios G. Bakis, Ashni A. Vora, Tatsuya Araki, Tongqiu Jia, Jared G. Galloway, Chris Jennings-Shaffer, Gabriel D. Victora, Yun S. Song, William S. DeWitt, Frederick A. Matsen IV, Volodymyr M. Minin</dc:creator>
    </item>
    <item>
      <title>Optimal Designs for Gamma Degradation Tests</title>
      <link>https://arxiv.org/abs/2508.09569</link>
      <description>arXiv:2508.09569v1 Announce Type: new 
Abstract: This paper analytically investigates the optimal design of gamma degradation tests, including the number of test units, the number of inspections, and inspection times. We first derive optimal designs with periodic inspection times under various scenarios. Unlike previous studies that typically rely on numerical methods or fix certain design parameters, our approach provides an analytical framework to determine optimal designs. In addition, the results are directly applicable to destructive degradation tests when number of inspection is one. The investigation is then extended to designs with aperiodic inspection times, a topic that has not been thoroughly explored in the existing literature. Interestingly, we show that designs with periodic inspection times are the least efficient. We then derive the optimal aperiodic inspection times and the corresponding optimal designs under two cost constraints. Finally, two examples are presented to validate the proposed methods and demonstrate their efficiency in improving reliability estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09569v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hung-Ping Tung, Yu-Wen Chen</dc:creator>
    </item>
    <item>
      <title>Decoding Neuronal Ensembles from Spatially-Referenced Calcium Traces: A Bayesian Semiparametric Approach</title>
      <link>https://arxiv.org/abs/2508.09576</link>
      <description>arXiv:2508.09576v1 Announce Type: new 
Abstract: Understanding how neurons coordinate their activity is a fundamental question in neuroscience, with implications for learning, memory, and neurological disorders. Calcium imaging has emerged as a powerful method to observe large-scale neuronal activity in freely moving animals, providing time-resolved recordings of hundreds of neurons. However, fluorescence signals are noisy and only indirectly reflect underlying spikes of neuronal activity, complicating the extraction of reliable patterns of neuronal coordination. We introduce a fully Bayesian, semiparametric model that jointly infers spiking activity and identifies functionally coherent neuronal ensembles from calcium traces. Our approach models each neuron's spiking probability through a latent Gaussian process and encourages anatomically coherent clustering using a location-dependent stick-breaking prior. A spike-and-slab Dirichlet process captures heterogeneity in spike amplitudes while filtering out negligible events. We consider calcium imaging data from the hippocampal CA1 region of a mouse as it navigates a circular arena, a setting critical for understanding spatial memory and neuronal representation of environments. Our model uncovers spatially structured co-activation patterns among neurons and can be employed to reveal how ensemble structures vary with the animal's position.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09576v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura D'Angelo, Francesco Denti, Antonio Canale, Michele Guindani</dc:creator>
    </item>
    <item>
      <title>Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders</title>
      <link>https://arxiv.org/abs/2508.09154</link>
      <description>arXiv:2508.09154v1 Announce Type: cross 
Abstract: Estimating peer causal effects within complex real-world networks such as social networks is challenging, primarily due to simultaneous feedback between peers and unobserved confounders. Existing methods either address unobserved confounders while ignoring the simultaneous feedback, or account for feedback but under restrictive linear assumptions, thus failing to obtain accurate peer effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning framework which leverages I-G transformation (matrix operation) and 2SRI (an instrumental variable or IV technique) to address both simultaneous feedback and unobserved confounding, while accommodating complex, nonlinear and high-dimensional relationships. DIG2RSI first applies the I-G transformation to disentangle mutual peer influences and eliminate the bias due to the simultaneous feedback. To deal with unobserved confounding, we first construct valid IVs from network data. In stage 1 of 2RSI, we train a neural network on these IVs to predict peer exposure, and extract residuals as proxies for the unobserved confounders. In the stage 2, we fit a separate neural network augmented by an adversarial discriminator that incorporates these residuals as a control function and enforces the learned representation to contain no residual confounding signal. The expressive power of deep learning models in capturing complex non-linear relationships and adversarial debiasing enhances the effectiveness of DIG2RSI in eliminating bias from both feedback loops and hidden confounders. We prove consistency of our estimator under standard regularity conditions, ensuring asymptotic recovery of the true peer effect. Empirical results on two semi-synthetic benchmarks and a real-world dataset demonstrate that DIG2RSI outperforms existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09154v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojing Du, Jiuyong Li, Lin Liu, Debo Cheng, Thuc. Le</dc:creator>
    </item>
    <item>
      <title>Forecasting Binary Economic Events in Modern Mercantilism: Traditional methodologies coupled with PCA and K-means Quantitative Analysis of Qualitative Sentimental Data</title>
      <link>https://arxiv.org/abs/2508.09243</link>
      <description>arXiv:2508.09243v1 Announce Type: cross 
Abstract: This paper examines Modern Mercantilism, characterized by rising economic nationalism, strategic technological decoupling, and geopolitical fragmentation, as a disruptive shift from the post-1945 globalization paradigm. It applies Principal Component Analysis (PCA) to 768-dimensional SBERT-generated semantic embeddings of curated news articles to extract orthogonal latent factors that discriminate binary event outcomes linked to protectionism, technological sovereignty, and bloc realignments. Analysis of principal component loadings identifies key semantic features driving classification performance, enhancing interpretability and predictive accuracy. This methodology provides a scalable, data-driven framework for quantitatively tracking emergent mercantilist dynamics through high-dimensional text analytics</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09243v1</guid>
      <category>econ.GN</category>
      <category>cs.LG</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Kot</dc:creator>
    </item>
    <item>
      <title>Over-Squashing in GNNs and Causal Inference of Rewiring Strategies</title>
      <link>https://arxiv.org/abs/2508.09265</link>
      <description>arXiv:2508.09265v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) have exhibited state-of-the-art performance across wide-range of domains such as recommender systems, material design, and drug repurposing. Yet message-passing GNNs suffer from over-squashing -- exponential compression of long-range information from distant nodes -- which limits expressivity. Rewiring techniques can ease this bottleneck; but their practical impacts are unclear due to the lack of a direct empirical over-squashing metric. We propose a rigorous, topology-focused method for assessing over-squashing between node pairs using the decay rate of their mutual sensitivity. We then extend these pairwise assessments to four graph-level statistics (prevalence, intensity, variability, extremity). Coupling these metrics with a within-graph causal design, we quantify how rewiring strategies affect over-squashing on diverse graph- and node-classification benchmarks. Our extensive empirical analyses show that most graph classification datasets suffer from over-squashing (but to various extents), and rewiring effectively mitigates it -- though the degree of mitigation, and its translation into performance gains, varies by dataset and method. We also found that over-squashing is less notable in node classification datasets, where rewiring often increases over-squashing, and performance variations are uncorrelated with over-squashing changes. These findings suggest that rewiring is most beneficial when over-squashing is both substantial and corrected with restraint -- while overly aggressive rewiring, or rewiring applied to minimally over-squashed graphs, is unlikely to help and may even harm performance. Our plug-and-play diagnostic tool lets practitioners decide -- before any training -- whether rewiring is likely to pay off.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09265v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danial Saber, Amirali Salehi-Abari</dc:creator>
    </item>
    <item>
      <title>Weighted Estimation of the Tail Index under Right Censorship: A Unified Approach Based on Kaplan-Meier and Nelson-Aalen Integrals</title>
      <link>https://arxiv.org/abs/2508.09289</link>
      <description>arXiv:2508.09289v1 Announce Type: cross 
Abstract: Kaplan-Meier and Nelson-Aalen integral estimators to the tail index of right-censored Pareto-type data traditionally rely on the assumption that the proportion p of upper uncensored observations exceeds one-half, corresponding to weak censoring regime. However, this condition excludes many practical settings characterized by strong censorship, where p is less than or equal to one-half. To address this bothering limitation, we propose a modification that incorporates a tuning parameter. This parameter, greater than one, assigns appropriate weights to the estimators, thereby extending the applicability of the method to the entire censoring range, where p is between zero and one. Under suitable regularity conditions, we establish the consistency and asymptotic normality of the proposed estimators. Extensive simulation studies reveal a clear improvement over existing methods in terms of bias and mean squared error, particularly in the strong censoring situation. These results highlight the significant practical and theoretical impact of our approach, offering a more flexible and accurate framework for tail index estimation under censoring. The usefulness of the method is further illustrated through its application to two real datasets: one on insurance losses (weak censoring) and the other on AIDS cases (strong censoring).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09289v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdelhakim Necir, Nour Elhouda Guesmia, Djamel Meraghni</dc:creator>
    </item>
    <item>
      <title>An Asymptotically Exact Multiple Testing Procedure under Dependence</title>
      <link>https://arxiv.org/abs/2508.09671</link>
      <description>arXiv:2508.09671v1 Announce Type: cross 
Abstract: We propose a simple single-step multiple testing procedure that asymptotically controls the family-wise error rate (FWER) at the desired level exactly under the equicorrelated multivariate Gaussian setup. The method is shown to be asymptotically exact using an explicit plug-in estimator for the equicorrelation, and does not require stepwise adjustments. We establish its theoretical properties, including the convergence to the desired error level, and demonstrate its effectiveness through simulation results. We also spell out related extensions to block-correlated structures and generalized FWER control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09671v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Swarnadeep Datta, Monitirtha Dey</dc:creator>
    </item>
    <item>
      <title>Inference on the proportion of variance explained in principal component analysis</title>
      <link>https://arxiv.org/abs/2402.16725</link>
      <description>arXiv:2402.16725v3 Announce Type: replace 
Abstract: Principal component analysis (PCA) is a longstanding and well-studied approach for dimension reduction. It rests upon the assumption that the underlying signal in the data has low rank, and thus can be well-summarized using a small number of dimensions. The output of PCA is typically represented using a scree plot, which displays the proportion of variance explained (PVE) by each principal component. While the PVE is extensively reported in routine data analyses, to the best of our knowledge the notion of inference on the PVE remains unexplored.
  In this paper, we consider inference on the PVE. We first introduce a new population quantity for the PVE with respect to an unknown matrix mean. Critically, our interest lies in the PVE of the sample principal components (as opposed to unobserved population principal components); thus, the population PVE that we introduce is defined conditional on the sample singular vectors. We show that it is possible to conduct inference, in the sense of confidence intervals, p-values, and point estimates, on this population quantity. Furthermore, we can conduct valid inference on the PVE of a subset of the principal components, even when the subset is selected using a data-driven approach such as the elbow rule. We demonstrate the proposed approach in simulation and in an application to a gene expression dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16725v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2025.2538895</arxiv:DOI>
      <dc:creator>Ronan Perry, Snigdha Panigrahi, Jacob Bien, Daniela Witten</dc:creator>
    </item>
    <item>
      <title>Inference under Staggered Adoption: Case Study of the Affordable Care Act</title>
      <link>https://arxiv.org/abs/2412.09482</link>
      <description>arXiv:2412.09482v2 Announce Type: replace 
Abstract: Panel data consists of a collection of $N$ units that are observed over $T$ units of time. A policy or treatment is subject to staggered adoption if different units take on treatment at different times and remains treated (or never at all). Assessing the effectiveness of such a policy requires estimating the treatment effect, corresponding to the difference between outcomes for treated versus untreated units. We develop inference procedures that build upon a computationally efficient matrix estimator for treatment effects in panel data. Our routines return confidence intervals (CIs) both for individual treatment effects, as well as for more general bilinear functionals of treatment effects, with prescribed coverage guarantees. We apply these inferential methods to analyze the effectiveness of Medicaid expansion portion of the Affordable Care Act. Based on our analysis, Medicaid expansion has led to substantial reductions in uninsurance rates, has reduced infant mortality rates, and has had no significant effects on healthcare expenditures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09482v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Xia, Yuling Yan, Martin J. Wainwright</dc:creator>
    </item>
    <item>
      <title>Covariate-Adjusted Response-Adaptive Design with Delayed Outcomes</title>
      <link>https://arxiv.org/abs/2502.01062</link>
      <description>arXiv:2502.01062v2 Announce Type: replace 
Abstract: Covariate-adjusted response-adaptive (CARA) designs have gained widespread adoption for their clear benefits in enhancing experimental efficiency and participant welfare. These designs dynamically adjust treatment allocations during interim analyses based on participant responses and covariates collected during the experiment. However, delayed responses can significantly compromise the effectiveness of CARA designs, as they hinder timely adjustments to treatment assignments when certain participant outcomes are not immediately observed. In this paper, we propose a fully forward-looking CARA design that dynamically updates treatment assignments throughout the experiment as response delay mechanisms are progressively estimated. Our design strategy is informed by novel semiparametric efficiency calculations that explicitly account for outcome delays in a multi-stage setting. Through both theoretical investigations and simulation studies, we demonstrate that our proposed design offers a robust solution for handling delayed outcomes in CARA designs, yielding significant improvements in both statistical power and participant welfare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01062v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinwei Ma, Jingshen Wang, Waverly Wei</dc:creator>
    </item>
    <item>
      <title>A method for sparse and robust independent component analysis</title>
      <link>https://arxiv.org/abs/2502.04046</link>
      <description>arXiv:2502.04046v2 Announce Type: replace 
Abstract: This work presents sparse invariant coordinate selection, SICS, a new method for sparse and robust independent component analysis. SICS is based on classical invariant coordinate selection, which is presented in such a form that a LASSO-type penalty can be applied to promote sparsity. Robustness is achieved by using robust scatter matrices. In the first part of the paper, the background and building blocks: scatter matrices, measures of robustness, ICS and independent component analysis, are carefully introduced. Then the proposed new method and its algorithm are derived and presented. This part also includes a consistency result for a general case of sparse ICS-like methods. The performance of SICS in identifying sparse independent component loadings is investigated with multiple simulations. The method is illustrated with an example in constructing sparse causal graphs and we also propose a graphical tool for selecting the appropriate sparsity level in SICS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04046v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lauri Heinonen, Joni Virta</dc:creator>
    </item>
    <item>
      <title>Testing of tempered fractional Brownian motions</title>
      <link>https://arxiv.org/abs/2504.11906</link>
      <description>arXiv:2504.11906v2 Announce Type: replace 
Abstract: We propose here a testing methodology based on the autocovariance, detrended moving average, and time-averaged mean-squared displacement statistics for tempered fractional Brownian motions (TFBMs) which are related to the notions of semi-long range dependence and transient anomalous diffusion. In this framework, we consider three types of TFBMs: two with a tempering factor incorporated into their moving-average representation, and one with a tempering parameter added to the autocorrelation formula. We illustrate their dynamics with the use of quantile lines. Using the proposed methodology, we provide a comprehensive power analysis of the tests. It appears that the tests allow distinguishing between the tempered processes with different Hurst parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11906v2</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katarzyna Macioszek, Farzad Sabzikar, Krzysztof Burnecki</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian Multi-Treatment Mixture Cure Survival Model with Application in Pediatric Oncology</title>
      <link>https://arxiv.org/abs/2508.08975</link>
      <description>arXiv:2508.08975v3 Announce Type: replace 
Abstract: Heterogeneous treatment effect estimation is critical in oncology, particularly in multi-arm trials with overlapping therapeutic components and long-term survivors. These shared mechanisms pose a central challenge to identifying causal effects in precision medicine. We propose a novel covariate-dependent nonparametric Bayesian multi-treatment cure survival model that jointly accounts for common structures among treatments and cure fractions. Through latent link functions, our model leverages sharing among treatments through a flexible modeling approach, enabling individualized survival inference. We adopt a Bayesian route for inference and implement an efficient MCMC algorithm for approximating the posterior. Simulation studies demonstrate the method's robustness and superiority in various specification scenarios. Finally, application to the AALL0434 trial reveals clinically meaningful differences in survival across methotrexate-based regimens and their associations with different covariates, underscoring its practical utility for learning treatment effects in real-world pediatric oncology data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08975v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Chang, John Kairalla, Arkaprava Roy</dc:creator>
    </item>
    <item>
      <title>Inference in Cluster Randomized Trials with Matched Pairs</title>
      <link>https://arxiv.org/abs/2211.14903</link>
      <description>arXiv:2211.14903v5 Announce Type: replace-cross 
Abstract: This paper studies inference in cluster randomized trials where treatment status is determined according to a "matched pairs" design. Here, by a cluster randomized experiment, we mean one in which treatment is assigned at the level of the cluster; by a "matched pairs" design, we mean that a sample of clusters is paired according to baseline, cluster-level covariates and, within each pair, one cluster is selected at random for treatment. We study the large-sample behavior of a weighted difference-in-means estimator and derive two distinct sets of results depending on if the matching procedure does or does not match on cluster size. We then propose a single variance estimator which is consistent in either regime. Combining these results establishes the asymptotic exactness of tests based on these estimators. Next, we consider the properties of two common testing procedures based on t-tests constructed from linear regressions, and argue that both are generally conservative in our framework. We additionally study the behavior of a randomization test which permutes the treatment status for clusters within pairs, and establish its finite-sample and asymptotic validity for testing specific null hypotheses. Finally, we propose a covariate-adjusted estimator which adjusts for additional baseline covariates not used for treatment assignment, and establish conditions under which such an estimator leads to strict improvements in precision. A simulation study confirms the practical relevance of our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.14903v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Jizhou Liu, Azeem M. Shaikh, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>On the Robustness of Kernel Goodness-of-Fit Tests</title>
      <link>https://arxiv.org/abs/2408.05854</link>
      <description>arXiv:2408.05854v4 Announce Type: replace-cross 
Abstract: Goodness-of-fit testing is often criticized for its lack of practical relevance: since ``all models are wrong'', the null hypothesis that the data conform to our model is ultimately always rejected as the sample size grows. Despite this, probabilistic models are still used extensively, raising the more pertinent question of whether the model is \emph{good enough} for the task at hand. This question can be formalized as a robust goodness-of-fit testing problem by asking whether the data were generated from a distribution that is a mild perturbation of the model. In this paper, we show that existing kernel goodness-of-fit tests are not robust under common notions of robustness including both qualitative and quantitative robustness. We further show that robustification techniques using tilted kernels, while effective in the parameter estimation literature, are not sufficient to ensure both types of robustness in the testing setting. To address this, we propose the first robust kernel goodness-of-fit test, which resolves this open problem by using kernel Stein discrepancy (KSD) balls. This framework encompasses many well-known perturbation models, such as Huber's contamination and density-band models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05854v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xing Liu, Fran\c{c}ois-Xavier Briol</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach for fitting semi-Markov mixture models of cancer latency to individual-level data</title>
      <link>https://arxiv.org/abs/2408.14625</link>
      <description>arXiv:2408.14625v2 Announce Type: replace-cross 
Abstract: Multi-state models of cancer natural history are widely used for designing and evaluating cancer early detection strategies. Calibrating such models against longitudinal data from screened cohorts is challenging, especially when fitting non-Markovian mixture models against individual-level data. Here, we consider a family of semi-Markov mixture models of cancer natural history and introduce an efficient data-augmented Markov chain Monte Carlo sampling algorithm for fitting these models to individual-level screening and cancer diagnosis histories. Our fully Bayesian approach supports rigorous uncertainty quantification and model selection through leave-one-out cross-validation, and it enables the estimation of screening-related overdiagnosis rates. We demonstrate the effectiveness of our approach using simulated data, showing that the sampling algorithm efficiently explores the joint posterior distribution of model parameters and latent variables. Finally, we apply our method to data from the US Breast Cancer Surveillance Consortium and estimate the extent of breast cancer overdiagnosis associated with mammography screening. The sampler and model comparison method are available in the R package baclava.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14625v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Morsomme, Shannon Holloway, Marc Ryser, Jason Xu</dc:creator>
    </item>
    <item>
      <title>A spectral method for multi-view subspace learning using the product of projections</title>
      <link>https://arxiv.org/abs/2410.19125</link>
      <description>arXiv:2410.19125v2 Announce Type: replace-cross 
Abstract: Multi-view data provides complementary information on the same set of observations, with multi-omics and multimodal sensor data being common examples. Analyzing such data typically requires distinguishing between shared (joint) and unique (individual) signal subspaces from noisy, high-dimensional measurements. Despite many proposed methods, the conditions for reliably identifying joint and individual subspaces remain unclear. We rigorously quantify these conditions, which depend on the ratio of the signal rank to the ambient dimension, principal angles between true subspaces, and noise levels. Our approach characterizes how spectrum perturbations of the product of projection matrices, derived from each view's estimated subspaces, affect subspace separation. Using these insights, we provide an easy-to-use and scalable estimation algorithm. In particular, we employ rotational bootstrap and random matrix theory to partition the observed spectrum into joint, individual, and noise subspaces. Diagnostic plots visualize this partitioning, providing practical and interpretable insights into the estimation performance. In simulations, our method estimates joint and individual subspaces more accurately than existing approaches. Applications to multi-omics data from colorectal cancer patients and nutrigenomic study of mice demonstrate improved performance in downstream predictive tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19125v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renat Sergazinov, Armeen Taeb, Irina Gaynanova</dc:creator>
    </item>
    <item>
      <title>M-learner:A Flexible And Powerful Framework To Study Heterogeneous Treatment Effect In Mediation Model</title>
      <link>https://arxiv.org/abs/2505.17917</link>
      <description>arXiv:2505.17917v3 Announce Type: replace-cross 
Abstract: We propose a novel method, termed the M-learner, for estimating heterogeneous indirect and total treatment effects and identifying relevant subgroups within a mediation framework. The procedure comprises four key steps. First, we compute individual-level conditional average indirect/total treatment effect Second, we construct a distance matrix based on pairwise differences. Third, we apply tSNE to project this matrix into a low-dimensional Euclidean space, followed by K-means clustering to identify subgroup structures. Finally, we calibrate and refine the clusters using a threshold-based procedure to determine the optimal configuration. To the best of our knowledge, this is the first approach specifically designed to capture treatment effect heterogeneity in the presence of mediation. Experimental results validate the robustness and effectiveness of the proposed framework. Application to the real-world Jobs II dataset highlights the broad adaptability and potential applicability of our method.Code is available at https: //anonymous.4open.science/r/M-learner-C4BB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17917v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingyu Li, Qing Liu, Tony Jiang, Hong Amy Xia, Brian P. Hobbs, Peng Wei</dc:creator>
    </item>
    <item>
      <title>How Much is Too Much? Learning Personalised Risk Thresholds in Real-World Driving</title>
      <link>https://arxiv.org/abs/2508.00888</link>
      <description>arXiv:2508.00888v2 Announce Type: replace-cross 
Abstract: While naturalistic driving studies have become foundational for providing real-world driver behaviour data, the existing frameworks for identifying risk based on such data have two fundamental limitations: (i) they rely on predefined time windows and fixed thresholds to disentangle risky and normal episodes of driving behaviour, and (ii) they assume stationary behavioural distribution across drivers and trips. These limitations have hindered the ability of the existing frameworks to capture behavioural nuances, adapt to individual variability, or respond to stochastic fluctuations in driving contexts. Thus, there is a need for a unified framework that jointly adapts risk labels and model learning to per-driver behavioural dynamics, a gap this study aims to bridge. We present an adaptive and personalised risk detection framework, built on Belgian naturalistic driving data, integrating a rolling time window with bi-level optimisation and dynamically calibrating both model hyperparameters and driver-specific risk thresholds at the same time. The framework was tested using two safety indicators, speed-weighted time headway and harsh driving events, and three models: Random Forest, XGBoost, and Deep Neural Network (DNN). Speed-weighted time headway yielded more stable and context-sensitive classifications than harsh-event counts. XGBoost maintained consistent performance under changing thresholds, while the DNN excelled in early-risk detection at lower thresholds but exhibited higher variability. The ensemble calibration integrates model-specific thresholds and confidence scores into a unified risk decision, balancing sensitivity and stability. Overall, the framework demonstrates the potential of adaptive and personalised risk detection to enhance real-time safety feedback and support driver-specific interventions within intelligent transport systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00888v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Hossein Kalantari, Eleonora Papadimitriou, Amir Pooyan Afghari</dc:creator>
    </item>
  </channel>
</rss>
