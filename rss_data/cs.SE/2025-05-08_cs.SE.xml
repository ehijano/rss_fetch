<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 May 2025 01:43:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Evaluation of Open Source Software Innovativeness</title>
      <link>https://arxiv.org/abs/2505.03855</link>
      <description>arXiv:2505.03855v1 Announce Type: new 
Abstract: Product innovation assessment in software sector is a timely topic. Nevertheless, research on that subject is particularly scant. As a result, there is a lack of criteria to measure software innovativeness. In a context of theoretical and practical controversy in the open source field, this article assesses open source software innovativeness. Based on almost 500 cases studies and with the collaboration of 125 experts from industry, services and research fields, it suggests an innovation typology supported by the notion of functional added value. It provides also an innovation modelling framework that combines main evaluation methodologies. By showing the shortcomings of widely used innovation metrics, this research supports a new approach of innovativeness assessment specialized in each sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03855v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3917/sim.133.0037</arxiv:DOI>
      <arxiv:journal_reference>Syst\`emes d'Information et Management, 2014, Volume 18 (3), pp.37-84</arxiv:journal_reference>
      <dc:creator>Nordine Benkeltoum</dc:creator>
    </item>
    <item>
      <title>Enhancing Women's Experiences in Software Engineering</title>
      <link>https://arxiv.org/abs/2505.03866</link>
      <description>arXiv:2505.03866v1 Announce Type: new 
Abstract: Context: Women face many challenges in their lives, which affect their daily experiences and influence major life decisions, starting before they enroll in bachelor's programs, setting a difficult path for those aspiring to enter the software development industry. Goal: To explore the challenges that women face across three different life stages, beginning as high school students, continuing as university undergraduates, and extending into their professional lives, as well as potential solutions to address these challenges. Research Method: We conducted a literature review followed by workshops to understand the perspectives of high school women, undergraduates, and practitioners regarding the same set of challenges and solutions identified in the literature. Results: Regardless of the life stage, women feel discouraged in a toxic environment often characterized by a lack of inclusion, harassment, and the exhausting need to prove themselves. We also discovered that some challenges are specific to certain life stages; for example, issues related to maternity were mentioned only by practitioners. Conclusions: Gender-related challenges arise before women enter the software development field when the proportion of men and women is still similar. While the need to prove themselves is mentioned at all three stages, high school women's challenges are more often directed toward convincing their parents that they are mature enough to handle their responsibilities. As they progress, the emphasis shifts to proving their competence in managing responsibilities for which they have received training. Increasing the inclusion of women in the field should, therefore, start earlier, and profound societal changes may be necessary to boost women's participation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03866v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J\'ulia Rocha Fortunato, Luana Ribeiro Soares, Gabriela Silva Alves, Edna Dias Canedo, Fabiana Freitas Mendes</dc:creator>
    </item>
    <item>
      <title>Unveiling the Role of ChatGPT in Software Development: Insights from Developer-ChatGPT Interactions on GitHub</title>
      <link>https://arxiv.org/abs/2505.03901</link>
      <description>arXiv:2505.03901v1 Announce Type: new 
Abstract: The advent of Large Language Models (LLMs) has introduced a new paradigm in software engineering, with generative AI tools like ChatGPT gaining widespread adoption among developers. While ChatGPT's potential has been extensively discussed, there is limited empirical evidence exploring its real-world usage by developers. This study bridges this gap by conducting a large-scale empirical analysis of ChatGPT-assisted development activities, leveraging a curated dataset, DevChat, comprising 2,547 unique shared ChatGPT links collected from GitHub between May 2023 and June 2024. Our study examines the characteristics of ChatGPT's usage on GitHub (including the tendency, prompt turns distribution, and link descriptions) and identifies five categories of developers' purposes for sharing developer-ChatGPT conversations during software development. Additionally, we analyzed the development-related activities where developers shared ChatGPT links to facilitate their workflows. We then established a mapping framework among data sources, activities, and SE tasks associated with these shared ChatGPT links. Our study offers a comprehensive view of ChatGPT's application in real-world software development scenarios and provides a foundation for its future integration into software development workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03901v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruiyin Li, Peng Liang, Yifei Wang, Yangxiao Cai, Weisong Sun, Zengyang Li</dc:creator>
    </item>
    <item>
      <title>Racing Against the Clock: Exploring the Impact of Scheduled Deadlines on Technical Debt</title>
      <link>https://arxiv.org/abs/2505.04027</link>
      <description>arXiv:2505.04027v1 Announce Type: new 
Abstract: Background: Technical Debt (TD) describes suboptimal software development practices with long-term consequences, such as defects and vulnerabilities. Deadlines are a leading cause of the emergence of TD in software systems. While multiple aspects of TD have been studied, the empirical research findings on the impact of deadlines are still inconclusive. Aims: This study investigates the impact of scheduled deadlines on TD. It analyzes how scheduled deadlines affect code quality, commit activities, and issues in issue-tracking systems. Method: We analyzed eight Open Source Software (OSS) projects with regular release schedules using SonarQube. We analyzed 12.3k commits and 371 releases across these eight OSS projects. The study combined quantitative metrics with qualitative analyses to comprehensively understand TD accumulation under scheduled deadlines. Results: Our findings indicated that some projects had a clear increase in TD as deadlines approached (with above 50% of releases having increasing TD accumulation as deadlines approached), while others managed to maintain roughly the same amount of TD. Analysis of commit activities and issue tracking revealed that deadline proximity could lead to increased commit frequency and bug-related issue creation. Conclusions: Our study highlights that, in some cases, impending deadlines have a clear impact on TD. The findings pinpoint the need to mitigate last-minute coding rushes and the risks associated with deadline-driven TD accumulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04027v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Joshua Aldrich Edbert, Zadia Codabux, Roberto Verdecchia</dc:creator>
    </item>
    <item>
      <title>Identification and Optimization of Redundant Code Using Large Language Models</title>
      <link>https://arxiv.org/abs/2505.04040</link>
      <description>arXiv:2505.04040v1 Announce Type: new 
Abstract: Redundant code is a persistent challenge in software development that makes systems harder to maintain, scale, and update. It adds unnecessary complexity, hinders bug fixes, and increases technical debt. Despite their impact, removing redundant code manually is risky and error-prone, often introducing new bugs or missing dependencies. While studies highlight the prevalence and negative impact of redundant code, little focus has been given to Artificial Intelligence (AI) system codebases and the common patterns that cause redundancy. Additionally, the reasons behind developers unintentionally introducing redundant code remain largely unexplored. This research addresses these gaps by leveraging large language models (LLMs) to automatically detect and optimize redundant code in AI projects. Our research aims to identify recurring patterns of redundancy and analyze their underlying causes, such as outdated practices or insufficient awareness of best coding principles. Additionally, we plan to propose an LLM agent that will facilitate the detection and refactoring of redundancies on a large scale while preserving original functionality. This work advances the application of AI in identifying and optimizing redundant code, ultimately helping developers maintain cleaner, more readable, and scalable codebases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04040v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shamse Tasnim Cynthia</dc:creator>
    </item>
    <item>
      <title>An Empirical Study of OpenAI API Discussions on Stack Overflow</title>
      <link>https://arxiv.org/abs/2505.04084</link>
      <description>arXiv:2505.04084v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs), represented by OpenAI's GPT series, has significantly impacted various domains such as natural language processing, software development, education, healthcare, finance, and scientific research. However, OpenAI APIs introduce unique challenges that differ from traditional APIs, such as the complexities of prompt engineering, token-based cost management, non-deterministic outputs, and operation as black boxes. To the best of our knowledge, the challenges developers encounter when using OpenAI APIs have not been explored in previous empirical studies. To fill this gap, we conduct the first comprehensive empirical study by analyzing 2,874 OpenAI API-related discussions from the popular Q&amp;A forum Stack Overflow. We first examine the popularity and difficulty of these posts. After manually categorizing them into nine OpenAI API-related categories, we identify specific challenges associated with each category through topic modeling analysis. Based on our empirical findings, we finally propose actionable implications for developers, LLM vendors, and researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04084v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Chen, Jibin Wang, Chaoyang Gao, Xiaolin Ju, Zhanqi Cui</dc:creator>
    </item>
    <item>
      <title>Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering</title>
      <link>https://arxiv.org/abs/2505.04251</link>
      <description>arXiv:2505.04251v1 Announce Type: new 
Abstract: Multi-agent autonomous systems (MAS) are better at addressing challenges that spans across multiple domains than singular autonomous agents. This holds true within the field of software engineering (SE) as well. The state-of-the-art research on MAS within SE focuses on integrating LLMs at the core of autonomous agents to create LLM-based multi-agent autonomous (LMA) systems. However, the introduction of LMA systems into SE brings a plethora of challenges. One of the major challenges is the strategic allocation of tasks between humans and the LMA system in a trustworthy manner. To address this challenge, a RACI-based framework is proposed in this work in progress article, along with implementation guidelines and an example implementation of the framework. The proposed framework can facilitate efficient collaboration, ensure accountability, and mitigate potential risks associated with LLM-driven automation while aligning with the Trustworthy AI guidelines. The future steps for this work delineating the planned empirical validation method are also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04251v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Krishna Ronanki</dc:creator>
    </item>
    <item>
      <title>CompileAgent: Automated Real-World Repo-Level Compilation with Tool-Integrated LLM-based Agent System</title>
      <link>https://arxiv.org/abs/2505.04254</link>
      <description>arXiv:2505.04254v1 Announce Type: new 
Abstract: With open-source projects growing in size and complexity, manual compilation becomes tedious and error-prone, highlighting the need for automation to improve efficiency and accuracy. However, the complexity of compilation instruction search and error resolution makes automatic compilation challenging. Inspired by the success of LLM-based agents in various fields, we propose CompileAgent, the first LLM-based agent framework dedicated to repo-level compilation. CompileAgent integrates five tools and a flow-based agent strategy, enabling interaction with software artifacts for compilation instruction search and error resolution. To measure the effectiveness of our method, we design a public repo-level benchmark CompileAgentBench, and we also design two baselines for comparison by combining two compilation-friendly schemes. The performance on this benchmark shows that our method significantly improves the compilation success rate, ranging from 10% to 71%. Meanwhile, we evaluate the performance of CompileAgent under different agent strategies and verify the effectiveness of the flow-based strategy. Additionally, we emphasize the scalability of CompileAgent, further expanding its application prospects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04254v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li Hu, Guoqiang Chen, Xiuwei Shang, Shaoyin Cheng, Benlong Wu, Gangyang Li, Xu Zhu, Weiming Zhang, Nenghai Yu</dc:creator>
    </item>
    <item>
      <title>Revolutionizing Newcomers' Onboarding Process in OSS Communities: The Future AI Mentor</title>
      <link>https://arxiv.org/abs/2505.04277</link>
      <description>arXiv:2505.04277v1 Announce Type: new 
Abstract: Onboarding newcomers is vital for the sustainability of open-source software (OSS) projects. To lower barriers and increase engagement, OSS projects have dedicated experts who provide guidance for newcomers. However, timely responses are often hindered by experts' busy schedules. The recent rapid advancements of AI in software engineering have brought opportunities to leverage AI as a substitute for expert mentoring. However, the potential role of AI as a comprehensive mentor throughout the entire onboarding process remains unexplored. To identify design strategies of this ``AI mentor'', we applied Design Fiction as a participatory method with 19 OSS newcomers. We investigated their current onboarding experience and elicited 32 design strategies for future AI mentor. Participants envisioned AI mentor being integrated into OSS platforms like GitHub, where it could offer assistance to newcomers, such as ``recommending projects based on personalized requirements'' and ``assessing and categorizing project issues by difficulty''. We also collected participants' perceptions of a prototype, named ``OSSerCopilot'', that implemented the envisioned strategies. They found the interface useful and user-friendly, showing a willingness to use it in the future, which suggests the design strategies are effective. Finally, in order to identify the gaps between our design strategies and current research, we conducted a comprehensive literature review, evaluating the extent of existing research support for this concept. We find that research is relatively scarce in certain areas where newcomers highly anticipate AI mentor assistance, such as ``discovering an interested project''. Our study has the potential to revolutionize the current newcomer-expert mentorship and provides valuable insights for researchers and tool designers aiming to develop and enhance AI mentor systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04277v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3715767</arxiv:DOI>
      <dc:creator>Xin Tan, Xiao Long, Yinghao Zhu, Lin Shi, Xiaoli Lian, Li Zhang</dc:creator>
    </item>
    <item>
      <title>Tracing Vulnerability Propagation Across Open Source Software Ecosystems</title>
      <link>https://arxiv.org/abs/2505.04307</link>
      <description>arXiv:2505.04307v1 Announce Type: new 
Abstract: The paper presents a traceability analysis of how over 84 thousand vulnerabilities have propagated across 28 open source software ecosystems. According to the results, the propagation sequences have been complex in general, although GitHub, Debian, and Ubuntu stand out. Furthermore, the associated propagation delays have been lengthy, and these do not correlate well with the number of ecosystems involved in the associated sequences. Nor does the presence or absence of particularly ecosystems in the sequences yield clear, interpretable patterns. With these results, the paper contributes to the overlapping knowledge bases about software ecosystems, traceability, and vulnerabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04307v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jukka Ruohonen, Qusai Ramadan</dc:creator>
    </item>
    <item>
      <title>How the Misuse of a Dataset Harmed Semantic Clone Detection</title>
      <link>https://arxiv.org/abs/2505.04311</link>
      <description>arXiv:2505.04311v1 Announce Type: new 
Abstract: BigCloneBench is a well-known and widely used large-scale dataset for the evaluation of recall of clone detection tools. It has been beneficial for research on clone detection and has become a standard in evaluating the performance of clone detection tools. More recently, it has also been widely used as a dataset to evaluate machine learning approaches to semantic clone detection or code similarity detection for functional or semantic similarity.
  This paper demonstrates that BigCloneBench is problematic to use as ground truth for learning or evaluating semantic code similarity, and highlights the aspects of BigCloneBench that affect the ground truth quality. A manual investigation of a statistically significant random sample of 406 Weak Type-3/Type-4 clone pairs revealed that 93% of them do not have a similar functionality and are therefore mislabelled. In a literature review of 179 papers that use BigCloneBench as a dataset, we found 139 papers that used BigCloneBench to evaluate semantic clone detection and where the results are threatened in their validity by the mislabelling. As such, these papers often report high F1 scores (e.g., above 0.9), which indicates overfitting to dataset-specific artefacts rather than genuine semantic similarity detection.
  We emphasise that using BigCloneBench remains valid for the intended purpose of evaluating syntactic or textual clone detection of Type-1, Type-2, and Type-3 clones. We acknowledge the important contributions of BigCloneBench to two decades of traditional clone detection research. However, the usage of BigCloneBench beyond the intended purpose without careful consideration of its limitations has led to misleading results and conclusions, and potentially harmed the field of semantic clone detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04311v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jens Krinke, Chaiyong Ragkhitwetsagul</dc:creator>
    </item>
    <item>
      <title>Verification of Digital Twins using Classical and Statistical Model Checking</title>
      <link>https://arxiv.org/abs/2505.04322</link>
      <description>arXiv:2505.04322v1 Announce Type: new 
Abstract: With the increasing adoption of digital techniques, the concept of digital twin (DT) has received a widespread attention in both industry and academia. While several definitions exist for a DT, most definitions focus on the existence of a virtual entity (VE) of a real-world object or process, often comprising interconnected models which interact with each other, undergoing changes continuously owing to the synchronization with the real-world object. These interactions might lead to inconsistencies at execution time, due to their highly stochastic and/or time-critical nature, which may lead to undesirable behavior. In addition, the continuously varying nature of VE owing to its synchronization with the real-world object further contributes to the complexity arising from these interactions and corresponding model execution times, which could possibly affect its overall functioning at runtime. This creates a need to perform (continuous) verification of the VE, to ensure that it behaves consistently at runtime by adhering to desired properties such as deadlock freeness, functional correctness, liveness and timeliness. Some critical properties such as deadlock freeness can only be verified using classical model checking; on the other hand, statistical model checking provides the possibility to model actual stochastic temporal behavior. We therefore propose to use both these techniques to verify the correctness and the fulfillment of desirable properties of VE. We present our observations and findings from applying these techniques on the DT of an autonomously driving truck. Results from these verification techniques suggest that this DT adheres to properties of deadlock freeness and functional correctness, but not adhering to timeliness properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04322v1</guid>
      <category>cs.SE</category>
      <category>cs.ET</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.418.2</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 418, 2025, pp. 16-23</arxiv:journal_reference>
      <dc:creator>Raghavendran Gunasekaran (Tilburg University), Boudewijn Haverkort (University of Twente)</dc:creator>
    </item>
    <item>
      <title>Towards Federated Digital Twin Platforms</title>
      <link>https://arxiv.org/abs/2505.04324</link>
      <description>arXiv:2505.04324v1 Announce Type: new 
Abstract: Digital Twin (DT) technology has become rather popular in recent years, promising to optimize production processes, manage the operation of cyber-physical systems, with an impact spanning across multiple application domains (e.g., manufacturing, robotics, space etc.).  DTs can include different kinds of assets, e.g., models, data, which could potentially be reused across DT projects by multiple users, directly affecting development costs, as well as enabling collaboration and further development of these assets. To provide user support for these purposes, dedicated DT frameworks and platforms are required, that take into account user needs, providing the infrastructure and building blocks for DT development and management.  In this demo paper, we show how the DT as a Service (DTaaS) platform has been extended to enable a federated approach to DT development and management, that allows multiple users across multiple instances of DTaaS to discover, reuse, reconfigure, and modify existing DT assets.
</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04324v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.418.4</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 418, 2025, pp. 32-38</arxiv:journal_reference>
      <dc:creator>Mirgita Frasheri (Aarhus University), Prasad Talasila (Aarhus University), Vanessa Scherma (Politecnico Di Torino)</dc:creator>
    </item>
    <item>
      <title>Uncovering Key Features for Model-Driven Engineering of Complex Performance Indicators: A Scoping Review</title>
      <link>https://arxiv.org/abs/2505.04498</link>
      <description>arXiv:2505.04498v1 Announce Type: new 
Abstract: This paper addresses challenges of designing and managing Complex Performance Indicators (CPI), which amalgamate individual indicators to measure latent, yet crucial business factors like customer satisfaction or sustainability indices. Despite their significant value, designing and managing CPI is intricate; they evolve with rapidly changing business contexts and present comprehension and explanation challenges for end-users. Model-Driven Engineering (MDE) emerges as a potent solution to overcome these hurdles and ensure CPI adoption, though its application to CPI remains an understudied research area. While prior efforts targeted specific CPI modeling objectives, a comprehensive overview of literature advancements is lacking. This study addresses this gap by conducting a scoping review yielding dual outcomes: (1) a comprehensive mapping of modeling features in the literature and (2) a comparative analysis of the coverage offered by the modeling frameworks. These outcomes enhance CPI understanding in academic and practitioner circles and offer insights for future MDE CPI advancements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04498v1</guid>
      <category>cs.SE</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benito Giunta, Corentin Burnay</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code Development</title>
      <link>https://arxiv.org/abs/2505.04521</link>
      <description>arXiv:2505.04521v1 Announce Type: new 
Abstract: Large Language Models (LLM) have significantly transformed various domains, including software development. These models assist programmers in generating code, potentially increasing productivity and efficiency. However, the environmental impact of utilising these AI models is substantial, given their high energy consumption during both training and inference stages. This research aims to compare the energy consumption of manual software development versus an LLM-assisted approach, using Codeforces as a simulation platform for software development. The goal is to quantify the environmental impact and propose strategies for minimising the carbon footprint of using LLM in software development. Our results show that the LLM-assisted code generation leads on average to 32.72 higher carbon footprint than the manual one. Moreover, there is a significant correlation between task complexity and the difference in the carbon footprint of the two approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04521v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3711919.3728678</arxiv:DOI>
      <dc:creator>Kuen Sum Cheung, Mayuri Kaul, Gunel Jahangirova, Mohammad Reza Mousavi, Eric Zie</dc:creator>
    </item>
    <item>
      <title>Runtime Advocates: A Persona-Driven Framework for Requirements@Runtime Decision Support</title>
      <link>https://arxiv.org/abs/2505.04551</link>
      <description>arXiv:2505.04551v1 Announce Type: new 
Abstract: Complex systems, such as small Uncrewed Aerial Systems (sUAS) swarms dispatched for emergency response, often require dynamic reconfiguration at runtime under the supervision of human operators. This introduces human-on-the-loop requirements, where evolving needs shape ongoing system functionality and behaviors. While traditional personas support upfront, static requirements elicitation, we propose a persona-based advocate framework for runtime requirements engineering to provide ethically informed, safety-driven, and regulatory-aware decision support. Our approach extends standard personas into event-driven personas. When triggered by events such as adverse environmental conditions, evolving mission state, or operational constraints, the framework updates the sUAS operator's view of the personas, ensuring relevance to current conditions. We create three key advocate personas, namely Safety Controller, Ethical Governor, and Regulatory Auditor, to manage trade-offs among risk, ethical considerations, and regulatory compliance. We perform a proof-of-concept validation in an emergency response scenario using sUAS, showing how our advocate personas provide context-aware guidance grounded in safety, regulatory, and ethical constraints. By evolving static, design-time personas into adaptive, event-driven advocates, the framework surfaces mission-critical runtime requirements in response to changing conditions. These requirements shape operator decisions in real time, aligning actions with the operational demands of the moment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04551v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Demetrius Hernandez, Jane Cleland-Huang</dc:creator>
    </item>
    <item>
      <title>OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue Resolution</title>
      <link>https://arxiv.org/abs/2505.04606</link>
      <description>arXiv:2505.04606v1 Announce Type: new 
Abstract: The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04606v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lianghong Guo, Wei Tao, Runhan Jiang, Yanlin Wang, Jiachi Chen, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using Large Language Models</title>
      <link>https://arxiv.org/abs/2505.03906</link>
      <description>arXiv:2505.03906v1 Announce Type: cross 
Abstract: Large language models (LLMs) have transformed software development through code generation capabilities, yet their effectiveness for high-performance computing (HPC) remains limited. HPC code requires specialized optimizations for parallelism, memory efficiency, and architecture-specific considerations that general-purpose LLMs often overlook. We present MARCO (Multi-Agent Reactive Code Optimizer), a novel framework that enhances LLM-generated code for HPC through a specialized multi-agent architecture. MARCO employs separate agents for code generation and performance evaluation, connected by a feedback loop that progressively refines optimizations. A key innovation is MARCO's web-search component that retrieves real-time optimization techniques from recent conference proceedings and research publications, bridging the knowledge gap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem set demonstrates that MARCO achieves a 14.6% average runtime reduction compared to Claude 3.5 Sonnet alone, while the integration of the web-search component yields a 30.9% performance improvement over the base MARCO system. These results highlight the potential of multi-agent systems to address the specialized requirements of high-performance code generation, offering a cost-effective alternative to domain-specific model fine-tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03906v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asif Rahman, Veljko Cvetkovic, Kathleen Reece, Aidan Walters, Yasir Hassan, Aneesh Tummeti, Bryan Torres, Denise Cooney, Margaret Ellis, Dimitrios S. Nikolopoulos</dc:creator>
    </item>
    <item>
      <title>SolPhishHunter: Towards Detecting and Understanding Phishing on Solana</title>
      <link>https://arxiv.org/abs/2505.04094</link>
      <description>arXiv:2505.04094v1 Announce Type: cross 
Abstract: Solana is a rapidly evolving blockchain platform that has attracted an increasing number of users. However, this growth has also drawn the attention of malicious actors, with some phishers extending their reach into the Solana ecosystem. Unlike platforms such as Ethereum, Solana has distinct designs of accounts and transactions, leading to the emergence of new types of phishing transactions that we term SolPhish. We define three types of SolPhish and develop a detection tool called SolPhishHunter. Utilizing SolPhishHunter, we detect a total of 8,058 instances of SolPhish and conduct an empirical analysis of these detected cases. Our analysis explores the distribution and impact of SolPhish, the characteristics of the phishers, and the relationships among phishing gangs. Particularly, the detected SolPhish transactions have resulted in nearly \$1.1 million in losses for victims. We report our detection results to the community and construct SolPhishDataset, the \emph{first} Solana phishing-related dataset in academia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04094v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziwei Li, Zigui Jiang, Ming Fang, Jiaxin Chen, Zhiying Wu, Jiajing Wu, Lun Zhang, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>A Case Study on the Application of Digital Twins for Enhancing CPS Operations</title>
      <link>https://arxiv.org/abs/2505.04323</link>
      <description>arXiv:2505.04323v1 Announce Type: cross 
Abstract: To ensure the availability and reduce the downtime of complex cyber-physical systems across different domains, e.g., agriculture and manufacturing, fault tolerance mechanisms are implemented which are complex in both their development and operation. In addition, cyber-physical systems are often confronted with limited hardware resources or are legacy systems, both often hindering the addition of new functionalities directly on the onboard hardware. Digital Twins can be adopted to offload expensive computations, as well as providing support through fault tolerance mechanisms, thus decreasing costs and operational downtime of cyber-physical systems. In this paper, we show the feasibility of a Digital Twin used for enhancing cyber-physical system operations, specifically through functional augmentation and increased fault tolerance, in an industry-oriented use case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04323v1</guid>
      <category>cs.RO</category>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.418.3</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 418, 2025, pp. 24-31</arxiv:journal_reference>
      <dc:creator>Irina Muntean (TYTAN Technologies, Munich, Germany), Mirgita Frasheri (Aarhus University, Denmark), Tiziano Munaro (fortiss GmbH, Munich, Germany)</dc:creator>
    </item>
    <item>
      <title>YABLoCo: Yet Another Benchmark for Long Context Code Generation</title>
      <link>https://arxiv.org/abs/2505.04406</link>
      <description>arXiv:2505.04406v1 Announce Type: cross 
Abstract: Large Language Models demonstrate the ability to solve various programming tasks, including code generation. Typically, the performance of LLMs is measured on benchmarks with small or medium-sized context windows of thousands of lines of code. At the same time, in real-world software projects, repositories can span up to millions of LoC. This paper closes this gap by contributing to the long context code generation benchmark (YABLoCo). The benchmark featured a test set of 215 functions selected from four large repositories with thousands of functions. The dataset contained metadata of functions, contexts of the functions with different levels of dependencies, docstrings, functions bodies, and call graphs for each repository. This paper presents three key aspects of the contribution. First, the benchmark aims at function body generation in large repositories in C and C++, two languages not covered by previous benchmarks. Second, the benchmark contains large repositories from 200K to 2,000K LoC. Third, we contribute a scalable evaluation pipeline for efficient computing of the target metrics and a tool for visual analysis of generated code. Overall, these three aspects allow for evaluating code generation in large repositories in C and C++.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04406v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aidar Valeev (Research Center of the Artificial Intelligence Institute, Innopolis University, Russia), Roman Garaev (Research Center of the Artificial Intelligence Institute, Innopolis University, Russia), Vadim Lomshakov (St. Petersburg Department of the Steklov Institute of Mathematics, Russia), Irina Piontkovskaya (Huawei Noah's Ark Lab), Vladimir Ivanov (Research Center of the Artificial Intelligence Institute, Innopolis University, Russia), Israel Adewuyi (Research Center of the Artificial Intelligence Institute, Innopolis University, Russia)</dc:creator>
    </item>
    <item>
      <title>DeMuVGN: Effective Software Defect Prediction Model by Learning Multi-view Software Dependency via Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2410.19550</link>
      <description>arXiv:2410.19550v2 Announce Type: replace 
Abstract: Software defect prediction (SDP) aims to identify high-risk defect modules in software development, optimizing resource allocation. While previous studies show that dependency network metrics improve defect prediction, most methods focus on code-based dependency graphs, overlooking developer factors. Current metrics, based on handcrafted features like ego and global network metrics, fail to fully capture defect-related information. To address this, we propose DeMuVGN, a defect prediction model that learns multi-view software dependency via graph neural networks. We introduce a Multi-view Software Dependency Graph (MSDG) that integrates data, call, and developer dependencies. DeMuVGN also leverages the Synthetic Minority Oversampling Technique (SMOTE) to address class imbalance and enhance defect module identification. In a case study of eight open-source projects across 20 versions, DeMuVGN demonstrates significant improvements: i) models based on multi-view graphs improve F1 scores by 11.1% to 12.1% over single-view models; ii) DeMuVGN improves F1 scores by 17.4% to 45.8% in within-project contexts and by 17.9% to 41.0% in cross-project contexts. Additionally, DeMuVGN excels in software evolution, showing more improvement in later-stage software versions. Its strong performance across different projects highlights its generalizability. We recommend future research focus on multi-view dependency graphs for defect prediction in both mature and newly developed projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19550v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Qiao, Lina Gong, Yu Zhao, Yongwei Wang, Mingqiang Wei</dc:creator>
    </item>
    <item>
      <title>Gender Disparities in Contributions, Leadership, and Collaboration: An Exploratory Study on Software Systems Research</title>
      <link>https://arxiv.org/abs/2412.15661</link>
      <description>arXiv:2412.15661v3 Announce Type: replace 
Abstract: Gender diversity enhances research by bringing diverse perspectives and innovative approaches. It ensures equitable solutions that address the needs of diverse populations. However, gender disparity persists in research where women remain underrepresented, which might limit diversity and innovation. Many even leave scientific careers as their contributions often go unnoticed and undervalued. Therefore, understanding gender-based contributions and collaboration dynamics is crucial to addressing this gap and creating a more inclusive research environment. In this study, we analyzed 2,000 articles published over the past decade in the Journal of Systems and Software (JSS). From these, we selected 384 articles that detailed authors' contributions and contained both female and male authors to investigate gender-based contributions. Our contributions are fourfold. First, we analyzed women's engagement in software systems research. Our analysis showed that only 32.74% of the total authors are women and female-led or supervised studies were fewer than those of men. Second, we investigated female authors' contributions across 14 major roles. Interestingly, we found that women contributed comparably to men in most roles, with more contributions in conceptualization, writing, and reviewing articles. Third, we explored the areas of software systems research and found that female authors are more actively involved in human-centric research domains. Finally, we analyzed gender-based collaboration dynamics. Our findings revealed that female supervisors tended to collaborate locally more often than national-level collaborations. Our study highlights that females' contributions to software systems research are comparable to those of men. Therefore, the barriers need to be addressed to enhance female participation and ensure equity and inclusivity in research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15661v3</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shamse Tasnim Cynthia, Saikat Mondal, Joy Krishan Das, Banani Roy</dc:creator>
    </item>
    <item>
      <title>Checkification: A Practical Approach for Testing Static Analysis Truths</title>
      <link>https://arxiv.org/abs/2501.12093</link>
      <description>arXiv:2501.12093v3 Announce Type: replace 
Abstract: Static analysis is an essential component of many modern software development tools. Unfortunately, the ever-increasing complexity of static analyzers makes their coding error-prone. Even analysis tools based on rigorous mathematical techniques, such as abstract interpretation, are not immune to bugs. Ensuring the correctness and reliability of software analyzers is critical if they are to be inserted in production compilers and development environments. While compiler validation has seen notable success, formal validation of static analysis tools remains relatively unexplored. In this paper, we propose a method for testing abstract interpretation-based static analyzers. Broadly, it consists in checking, over a suite of benchmarks, that the properties inferred statically are satisfied dynamically. The main advantage of our approach lies in its simplicity, which stems directly from framing it within the Ciao assertion-based validation framework, and its blended static/dynamic assertion checking approach. We demonstrate that in this setting, the analysis can be tested with little effort by combining the following components already present in the framework: 1) the static analyzer, which outputs its results as the original program source with assertions interspersed; 2) the assertion run-time checking mechanism, which instruments a program to ensure that no assertion is violated at run time; 3) the random test case generator, which generates random test cases satisfying the properties present in assertion preconditions; and 4) the unit-test framework, which executes those test cases. We have applied our approach to the CiaoPP static analyzer, resulting in the identification of many bugs with reasonable overhead. Most of these bugs have been either fixed or confirmed, helping us detect a range of errors not only related to analysis soundness but also within other aspects of the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12093v3</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniela Ferreiro, Ignacio Casso, Jose F. Morales, Pedro L\'opez-Garc\'ia, Manuel V. Hermenegildo</dc:creator>
    </item>
    <item>
      <title>Does Functional Package Management Enable Reproducible Builds at Scale? Yes</title>
      <link>https://arxiv.org/abs/2501.15919</link>
      <description>arXiv:2501.15919v2 Announce Type: replace 
Abstract: Reproducible Builds (R-B) guarantee that rebuilding a software package from source leads to bitwise identical artifacts. R-B is a promising approach to increase the integrity of the software supply chain, when installing open source software built by third parties. Unfortunately, despite success stories like high build reproducibility levels in Debian packages, uncertainty remains among field experts on the scalability of R-B to very large package repositories. In this work, we perform the first large-scale study of bitwise reproducibility, in the context of the Nix functional package manager, rebuilding 709 816 packages from historical snapshots of the nixpkgs repository, the largest cross-ecosystem open source software distribution, sampled in the period 2017-2023. We obtain very high bitwise reproducibility rates, between 69 and 91% with an upward trend, and even higher rebuildability rates, over 99%. We investigate unreproducibility causes, showing that about 15% of failures are due to embedded build dates. We release a novel dataset with all build statuses, logs, as well as full ''diffoscopes'': recursive diffs of where unreproducible build artifacts differ.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15919v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>22nd International Conference on Mining Software Repositories, Apr 2025, Ottawa, Canada</arxiv:journal_reference>
      <dc:creator>Julien Malka (ACES, INFRES), Stefano Zacchiroli (ACES, INFRES), Th\'eo Zimmermann (ACES, INFRES)</dc:creator>
    </item>
    <item>
      <title>Test It Before You Trust It: Applying Software Testing for Trustworthy In-context Learning</title>
      <link>https://arxiv.org/abs/2504.18827</link>
      <description>arXiv:2504.18827v2 Announce Type: replace 
Abstract: In-context learning (ICL) has emerged as a powerful capability of large language models (LLMs), enabling them to perform new tasks based on a few provided examples without explicit fine-tuning. Despite their impressive adaptability, these models remain vulnerable to subtle adversarial perturbations and exhibit unpredictable behavior when faced with linguistic variations. Inspired by software testing principles, we introduce a software testing-inspired framework, called MMT4NL, for evaluating the trustworthiness of in-context learning by utilizing adversarial perturbations and software testing techniques. It includes diverse evaluation aspects of linguistic capabilities for testing the ICL capabilities of LLMs. MMT4NL is built around the idea of crafting metamorphic adversarial examples from a test set in order to quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is to treat any LLM as software and validate its functionalities just like testing the software. Finally, we demonstrate applications of MMT4NL on the sentiment analysis and question-answering tasks. Our experiments could reveal various linguistic bugs in state-of-the-art LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18827v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Teeradaj Racharak, Chaiyong Ragkhitwetsagul, Chommakorn Sontesadisai, Thanwadee Sunetnanta</dc:creator>
    </item>
    <item>
      <title>A Defect Taxonomy for Infrastructure as Code: A Replication Study</title>
      <link>https://arxiv.org/abs/2505.01568</link>
      <description>arXiv:2505.01568v2 Announce Type: replace 
Abstract: Background: As Infrastructure as Code (IaC) becomes standard practice, ensuring the reliability of IaC scripts is essential. Defect taxonomies are valuable tools for this, offering a common language for issues and enabling systematic tracking. A significant prior study developed such a taxonomy, but based it exclusively on the declarative language Puppet. It remained unknown whether this taxonomy applies to programming language-based IaC (PL-IaC) tools like Pulumi, Terraform CDK, and AWS CDK. Aim: We replicated this foundational work to assess the generalizability of the taxonomy across a broader and more diverse landscape. Method: We performed qualitative analysis on 3,364 defect-related commits from 285 open-source PL-IaC repositories (PIPr dataset) to derive a PL-IaC-specific defect taxonomy. We then enhanced the ACID tool, originally developed for the prior study, to automatically classify and analyze defect distributions across an expanded dataset-447 open-source repositories and 94 proprietary projects from VTEX (e-commerce) and Nubank (financial). Results: Our research confirmed the same eight defect categories identified in the original study, with idempotency and security defects appearing infrequently but persistently across projects. Configuration Data defects maintain high frequency in both open-source and proprietary codebases. Conclusions: Our replication supports the generalizability of the original taxonomy, suggesting IaC development challenges surpass organizational boundaries. Configuration Data defects emerge as a persistent high-frequency problem, while idempotency and security defects remain important concerns despite lower frequency. These patterns appear consistent across open-source and proprietary projects, indicating they are fundamental to the IaC paradigm itself, transcending specific tools or project types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01568v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filipe Paiva, Jo\~ao Brunet, Thiago Emmanuel Pereira, Wendell Oliveira</dc:creator>
    </item>
    <item>
      <title>Requirements-Based Test Generation: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2505.02015</link>
      <description>arXiv:2505.02015v2 Announce Type: replace 
Abstract: As an important way of assuring software quality, software testing generates and executes test cases to identify software failures. Many strategies have been proposed to guide test-case generation, such as source-code-based approaches and methods based on bug reports. Requirements-based test generation (RBTG) constructs test cases based on specified requirements, aligning with user needs and expectations, without requiring access to the source code. Since its introduction in 1994, there have been many contributions to the development of RBTG, including various approaches, implementations, tools, assessment and evaluation methods, and applications. This paper provides a comprehensive survey on RBTG, categorizing requirement types, classifying approaches, investigating types of test cases, summarizing available tools, and analyzing experimental evaluations. This paper also summarizes the domains and industrial applications of RBTG, and discusses some open research challenges and potential future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02015v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenzhen Yang, Rubing Huang, Chenhui Cui, Nan Niu, Dave Towey</dc:creator>
    </item>
    <item>
      <title>QbC: Quantum Correctness by Construction</title>
      <link>https://arxiv.org/abs/2307.15641</link>
      <description>arXiv:2307.15641v3 Announce Type: replace-cross 
Abstract: Thanks to the rapid progress and growing complexity of quantum algorithms, correctness of quantum programs has become a major concern. Pioneering research over the past years has proposed various approaches to formally verify quantum programs using proof systems such as quantum Hoare logic. All these prior approaches are post-hoc: one first implements a program and only then verifies its correctness. Here we propose Quantum Correctness by Construction (QbC): an approach to constructing quantum programs from their specification in a way that ensures correctness. We use pre- and postconditions to specify program properties, and propose sound and complete refinement rules for constructing programs in a quantum while language from their specification. We validate QbC by constructing quantum programs for idiomatic problems and patterns. We find that the approach naturally suggests how to derive program details, highlighting key design choices along the way. As such, we believe that QbC can play a role in supporting the design and taxonomization of quantum algorithms and software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15641v3</guid>
      <category>quant-ph</category>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proc. ACM Program. Lang. 9, OOPSLA1, Article 99 (April 2025)</arxiv:journal_reference>
      <dc:creator>Anurudh Peduri, Ina Schaefer, Michael Walter</dc:creator>
    </item>
    <item>
      <title>Human-Robot Interaction and Perceived Irrationality: A Study of Trust Dynamics and Error Acknowledgment</title>
      <link>https://arxiv.org/abs/2403.14293</link>
      <description>arXiv:2403.14293v2 Announce Type: replace-cross 
Abstract: As robots become increasingly integrated into various industries, understanding how humans respond to robotic failures is critical. This study systematically examines trust dynamics and system design by analyzing human reactions to robot failures. We conducted a four-stage survey to explore how trust evolves throughout human-robot interactions. The first stage collected demographic data and initial trust levels. The second stage focused on preliminary expectations and perceptions of robotic capabilities. The third stage examined interaction details, including robot precision and error acknowledgment. Finally, the fourth stage assessed post-interaction perceptions, evaluating trust dynamics, forgiveness, and willingness to recommend robotic technologies. Results indicate that trust in robotic systems significantly increased when robots acknowledged their errors or limitations. Additionally, participants showed greater willingness to suggest robots for future tasks, highlighting the importance of direct engagement in shaping trust dynamics. These findings provide valuable insights for designing more transparent, responsive, and trustworthy robotic systems. By enhancing our understanding of human-robot interaction (HRI), this study contributes to the development of robotic technologies that foster greater public acceptance and adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14293v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ponkoj Chandra Shill, Md. Azizul Hakim</dc:creator>
    </item>
  </channel>
</rss>
