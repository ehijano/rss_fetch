<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Jun 2025 04:00:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Practically significant differences between conditional distribution functions</title>
      <link>https://arxiv.org/abs/2506.06545</link>
      <description>arXiv:2506.06545v1 Announce Type: new 
Abstract: In the framework of semiparametric distribution regression, we consider the problem of comparing the conditional distribution functions corresponding to two samples. In contrast to testing for exact equality, we are interested in the (null) hypothesis that the $L^2$ distance between the conditional distribution functions does not exceed a certain threshold in absolute value. The consideration of these hypotheses is motivated by the observation that in applications, it is rare, and perhaps impossible, that a null hypothesis of exact equality is satisfied and that the real question of interest is to detect a practically significant deviation between the two conditional distribution functions.
  The consideration of a composite null hypothesis makes the testing problem challenging, and in this paper we develop a pivotal test for such hypotheses. Our approach is based on self-normalization and therefore requires neither the estimation of (complicated) variances nor bootstrap approximations. We derive the asymptotic limit distribution of the (appropriately normalized) test statistic and show consistency under local alternatives. A simulation study and an application to German SOEP data reveal the usefulness of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06545v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Holger Dette, Kathrin M\"ollenhoff, Dominik Wied</dc:creator>
    </item>
    <item>
      <title>Inference on the value of linear programs</title>
      <link>https://arxiv.org/abs/2506.06776</link>
      <description>arXiv:2506.06776v1 Announce Type: new 
Abstract: This paper studies inference on the value of linear programs (LPs) when both the objective function and constraints are possibly unknown and must be estimated from data. We show that many inference problems in partially identified models can be reformulated in this way. Building on Shapiro (1991) and Fang and Santos (2019), we develop a pointwise valid inference procedure for the value of LPs. We modify this pointwise inference procedure to construct one-sided inference procedures that are uniformly valid over large classes of data-generating processes (DGPs). Our results provide alternative testing procedures for problems considered in Andrews et al. (2023), Cox and Shi (2023), and Fang et al. (2023) (in the low-dimensional case), and remain valid when key components--such as the coefficient matrix--are unknown and must be estimated. Moreover, our framework also accommodates inference on the identified set of a subvector, in models defined by linear moment inequalities, and does so under weaker constraint qualifications than those in Gafarov (2025).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06776v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Leonard Goff, Eric Mbakop</dc:creator>
    </item>
    <item>
      <title>Does Residuals-on-Residuals Regression Produce Representative Estimates of Causal Effects?</title>
      <link>https://arxiv.org/abs/2506.07462</link>
      <description>arXiv:2506.07462v1 Announce Type: new 
Abstract: Double Machine Learning is commonly used to estimate causal effects in large observational datasets. The "residuals-on-residuals" regression estimator (RORR) is especially popular for its simplicity and computational tractability. However, when treatment effects are heterogeneous, the proper interpretation of RORR may not be well understood. We show that, for many-valued treatments with continuous dose-response functions, RORR converges to a conditional variance-weighted average of derivatives evaluated at points not in the observed dataset, which generally differs from the Average Causal Derivative (ACD). Hence, even if all units share the same dose-response function, RORR does not in general converge to an average treatment effect in the population represented by the sample. We propose an alternative estimator suitable for large datasets. We demonstrate the pitfalls of RORR and the favorable properties of the proposed estimator in both an illustrative numerical example and an application to real-world data from Netflix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07462v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Apoorva Lal, Winston Chou</dc:creator>
    </item>
    <item>
      <title>Economic and Policy Uncertainties and Firm Value: The Case of Consumer Durable Goods</title>
      <link>https://arxiv.org/abs/2506.07476</link>
      <description>arXiv:2506.07476v1 Announce Type: new 
Abstract: The objective of this study is to analyze the response of firm value, represented by the Tobin's Q (Q) for a group of twelve U.S. durable goods producers to uncertainties in the US Economy. The results, based on an estimated panel quantile regressions (PQR) and panel vector autoregressive MIDAS model (PVM), show that Q for these firms reacts negatively to the positive shocks to the current ratio, and debt-to-asset ratio and positively to operating income after depreciation and the quick ratio in most quantiles. The Q of the firms under study reacts negatively to the economic policy uncertainty, risk of recession, and inflationary expectation, but positively to consumer confidence in most quantiles of its distribution. Finally, Granger causality tests confirm that the uncertainty indicators considered in the study are significant predictors of changes in the value of these companies as reflected by Q.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07476v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bahram Adrangi, Saman Hatamerad, Madhuparna Kolay, Kambiz Raffiee</dc:creator>
    </item>
    <item>
      <title>Enterprise value, economic and policy uncertainties: the case of US air carriers</title>
      <link>https://arxiv.org/abs/2506.07766</link>
      <description>arXiv:2506.07766v1 Announce Type: new 
Abstract: The enterprise value (EV) is a crucial metric in company valuation as it encompasses not only equity but also assets and liabilities, offering a comprehensive measure of total value, especially for companies with diverse capital structures. The relationship between economic uncertainty and firm value is rooted in economic theory, with early studies dating back to Sandmo's work in 1971 and further elaborated upon by John Kenneth Galbraith in 1977. Subsequent significant events have underscored the pivotal role of uncertainty in the financial and economic realm. Using a VAR-MIDAS methodology, analysis of accumulated impulse responses reveals that the EV of air carrier firms responds heterogeneously to financial and economic uncertainties, suggesting unique coping strategies. Most firms exhibit negative reactions to recessionary risks and economic policy uncertainties. Financial shocks also elicit varied responses, with positive impacts observed on EV in response to increases in the current ratio and operating income after depreciation. However, high debt levels are unfavorably received by the market, leading to negative EV responses to debt-to-asset ratio shocks. Other financial shocks show mixed or indeterminate impacts on EV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07766v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>J. of Theoretical Accounting Research 20 (2024) 123-175</arxiv:journal_reference>
      <dc:creator>Bahram Adrangi, Arjun Chatrath, Madhuparna Kolay, Kambiz Raffiee</dc:creator>
    </item>
    <item>
      <title>Evaluating Large Language Model Capabilities in Assessing Spatial Econometrics Research</title>
      <link>https://arxiv.org/abs/2506.06377</link>
      <description>arXiv:2506.06377v1 Announce Type: cross 
Abstract: This paper investigates Large Language Models (LLMs) ability to assess the economic soundness and theoretical consistency of empirical findings in spatial econometrics. We created original and deliberately altered "counterfactual" summaries from 28 published papers (2005-2024), which were evaluated by a diverse set of LLMs. The LLMs provided qualitative assessments and structured binary classifications on variable choice, coefficient plausibility, and publication suitability. The results indicate that while LLMs can expertly assess the coherence of variable choices (with top models like GPT-4o achieving an overall F1 score of 0.87), their performance varies significantly when evaluating deeper aspects such as coefficient plausibility and overall publication suitability. The results further revealed that the choice of LLM, the specific characteristics of the paper and the interaction between these two factors significantly influence the accuracy of the assessment, particularly for nuanced judgments. These findings highlight LLMs' current strengths in assisting with initial, more surface-level checks and their limitations in performing comprehensive, deep economic reasoning, suggesting a potential assistive role in peer review that still necessitates robust human oversight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06377v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giuseppe Arbia, Luca Morandini, Vincenzo Nardelli</dc:creator>
    </item>
    <item>
      <title>Quantile-Optimal Policy Learning under Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2506.07140</link>
      <description>arXiv:2506.07140v1 Announce Type: cross 
Abstract: We study quantile-optimal policy learning where the goal is to find a policy whose reward distribution has the largest $\alpha$-quantile for some $\alpha \in (0, 1)$. We focus on the offline setting whose generating process involves unobserved confounders. Such a problem suffers from three main challenges: (i) nonlinearity of the quantile objective as a functional of the reward distribution, (ii) unobserved confounding issue, and (iii) insufficient coverage of the offline dataset. To address these challenges, we propose a suite of causal-assisted policy learning methods that provably enjoy strong theoretical guarantees under mild conditions. In particular, to address (i) and (ii), using causal inference tools such as instrumental variables and negative controls, we propose to estimate the quantile objectives by solving nonlinear functional integral equations. Then we adopt a minimax estimation approach with nonparametric models to solve these integral equations, and propose to construct conservative policy estimates that address (iii). The final policy is the one that maximizes these pessimistic estimates. In addition, we propose a novel regularized policy learning method that is more amenable to computation. Finally, we prove that the policies learned by these methods are $\tilde{\mathscr{O}}(n^{-1/2})$ quantile-optimal under a mild coverage assumption on the offline dataset. Here, $\tilde{\mathscr{O}}(\cdot)$ omits poly-logarithmic factors. To the best of our knowledge, we propose the first sample-efficient policy learning algorithms for estimating the quantile-optimal policy when there exist unmeasured confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07140v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhongren Chen, Siyu Chen, Zhengling Qi, Xiaohong Chen, Zhuoran Yang</dc:creator>
    </item>
    <item>
      <title>Individual Treatment Effect: Prediction Intervals and Sharp Bounds</title>
      <link>https://arxiv.org/abs/2506.07469</link>
      <description>arXiv:2506.07469v1 Announce Type: cross 
Abstract: Individual treatment effect (ITE) is often regarded as the ideal target of inference in causal analyses and has been the focus of several recent studies. In this paper, we describe the intrinsic limits regarding what can be learned concerning ITEs given data from large randomized experiments. We consider when a valid prediction interval for the ITE is informative and when it can be bounded away from zero. The joint distribution over potential outcomes is only partially identified from a randomized trial. Consequently, to be valid, an ITE prediction interval must be valid for all joint distribution consistent with the observed data and hence will in general be wider than that resulting from knowledge of this joint distribution. We characterize prediction intervals in the binary treatment and outcome setting, and extend these insights to models with continuous and ordinal outcomes. We derive sharp bounds on the probability mass function (pmf) of the individual treatment effect (ITE). Finally, we contrast prediction intervals for the ITE and confidence intervals for the average treatment effect (ATE). This also leads to the consideration of Fisher versus Neyman null hypotheses. While confidence intervals for the ATE shrink with increasing sample size due to its status as a population parameter, prediction intervals for the ITE generally do not vanish, leading to scenarios where one may reject the Neyman null yet still find evidence consistent with the Fisher null, highlighting the challenges of individualized decision-making under partial identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07469v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhehao Zhang, Thomas S. Richardson</dc:creator>
    </item>
    <item>
      <title>Using Forests in Multivariate Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2303.11721</link>
      <description>arXiv:2303.11721v3 Announce Type: replace 
Abstract: We discuss estimation and inference of conditional treatment effects in regression discontinuity (RD) designs with multiple scores. In addition to local linear regressions and the minimax-optimal estimator more recently proposed by Imbens and Wager (2019), we argue that two variants of random forests, honest regression forests and local linear forests, should be added to the toolkit of applied researchers working with multivariate RD designs; their validity follows from results in Wager and Athey (2018) and Friedberg et al. (2020). We design a systematic Monte Carlo study with data generating processes built both from functional forms that we specify and from Wasserstein Generative Adversarial Networks that closely mimic the observed data. We find no single estimator dominates across all specifications: (i) local linear regressions perform well in univariate settings, but the common practice of reducing multivariate scores to a univariate one can incur under-coverage, possibly due to vanishing density at the transformed cutoff; (ii) good performance of the minimax-optimal estimator depends on accurate estimation of a nuisance parameter and its current implementation only accepts up to two scores; (iii) forest-based estimators are not designed for estimation at boundary points and are susceptible to finite-sample bias, but their flexibility in modeling multivariate scores opens the door to a wide range of empirical applications, as illustrated by an empirical study of COVID-19 hospital funding with three eligibility criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.11721v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiqi Liu, Yuan Qi</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for dynamic spatial quantile models with interactive effects</title>
      <link>https://arxiv.org/abs/2503.00772</link>
      <description>arXiv:2503.00772v2 Announce Type: replace 
Abstract: With the rapid advancement of information technology and data collection systems, large-scale spatial panel data presents new methodological and computational challenges. This paper introduces a dynamic spatial panel quantile model that incorporates unobserved heterogeneity. The proposed model captures the dynamic structure of panel data, high-dimensional cross-sectional dependence, and allows for heterogeneous regression coefficients. To estimate the model, we propose a novel Bayesian Markov Chain Monte Carlo (MCMC) algorithm. Contributions to Bayesian computation include the development of quantile randomization, a new Gibbs sampler for structural parameters, and stabilization of the tail behavior of the inverse Gaussian random generator. We establish Bayesian consistency for the proposed estimation method as both the time and cross-sectional dimensions of the panel approach infinity. Monte Carlo simulations demonstrate the effectiveness of the method. Finally, we illustrate the applicability of the approach through a case study on the quantile co-movement structure of the gasoline market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00772v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tomohiro Ando, Jushan Bai, Kunpeng Li, Yong Song</dc:creator>
    </item>
    <item>
      <title>Post Reinforcement Learning Inference</title>
      <link>https://arxiv.org/abs/2302.08854</link>
      <description>arXiv:2302.08854v4 Announce Type: replace-cross 
Abstract: We consider estimation and inference using data collected from reinforcement learning algorithms. These algorithms, characterized by their adaptive experimentation, interact with individual units over multiple stages, dynamically adjusting their strategies based on previous interactions. Our goal is to evaluate a counterfactual policy post-data collection and estimate structural parameters, like dynamic treatment effects, which can be used for credit assignment and determining the effect of earlier actions on final outcomes. Such parameters of interest can be framed as solutions to moment equations, but not minimizers of a population loss function, leading to Z-estimation approaches for static data. However, in the adaptive data collection environment of reinforcement learning, where algorithms deploy nonstationary behavior policies, standard estimators do not achieve asymptotic normality due to the fluctuating variance. We propose a weighted Z-estimation approach with carefully designed adaptive weights to stabilize the time-varying estimation variance. We identify proper weighting schemes to restore the consistency and asymptotic normality of the weighted Z-estimators for target parameters, which allows for hypothesis testing and constructing uniform confidence regions. Primary applications include dynamic treatment effect estimation and dynamic off-policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.08854v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vasilis Syrgkanis, Ruohan Zhan</dc:creator>
    </item>
    <item>
      <title>Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2402.14264</link>
      <description>arXiv:2402.14264v4 Announce Type: replace-cross 
Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, the statistical optimality of these methods has still remained an open area of investigation, especially in regimes where these methods do not achieve parametric rates. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATT), as well as weighted variants of the former, which arise in policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14264v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Automatic Doubly Robust Forests</title>
      <link>https://arxiv.org/abs/2412.07184</link>
      <description>arXiv:2412.07184v2 Announce Type: replace-cross 
Abstract: This paper proposes the automatic Doubly Robust Random Forest (DRRF) algorithm for estimating the conditional expectation of a moment functional in the presence of high-dimensional nuisance functions. DRRF extends the automatic debiasing framework based on the Riesz representer to the conditional setting and enables nonparametric, forest-based estimation (Athey et al., 2019; Oprescu et al., 2019). In contrast to existing methods, DRRF does not require prior knowledge of the form of the debiasing term or impose restrictive parametric or semi-parametric assumptions on the target quantity. Additionally, it is computationally efficient in making predictions at multiple query points. We establish consistency and asymptotic normality results for the DRRF estimator under general assumptions, allowing for the construction of valid confidence intervals. Through extensive simulations in heterogeneous treatment effect (HTE) estimation, we demonstrate the superior performance of DRRF over benchmark approaches in terms of estimation accuracy, robustness, and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07184v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaomeng Chen, Junting Duan, Victor Chernozhukov, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Characterization of Efficient Influence Function for Off-Policy Evaluation Under Optimal Policies</title>
      <link>https://arxiv.org/abs/2505.13809</link>
      <description>arXiv:2505.13809v3 Announce Type: replace-cross 
Abstract: Off-policy evaluation (OPE) provides a powerful framework for estimating the value of a counterfactual policy using observational data, without the need for additional experimentation. Despite recent progress in robust and efficient OPE across various settings, rigorous efficiency analysis of OPE under an estimated optimal policy remains limited. In this paper, we establish a concise characterization of the efficient influence function (EIF) for the value function under optimal policy within canonical Markov decision process models. Specifically, we provide the sufficient conditions for the existence of the EIF and characterize its expression. We also give the conditions under which the EIF does not exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13809v3</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyu Wei</dc:creator>
    </item>
    <item>
      <title>Learning from Double Positive and Unlabeled Data for Potential-Customer Identification</title>
      <link>https://arxiv.org/abs/2506.00436</link>
      <description>arXiv:2506.00436v2 Announce Type: replace-cross 
Abstract: In this study, we propose a method for identifying potential customers in targeted marketing by applying learning from positive and unlabeled data (PU learning). We consider a scenario in which a company sells a product and can observe only the customers who purchased it. Decision-makers seek to market products effectively based on whether people have loyalty to the company. Individuals with loyalty are those who are likely to remain interested in the company even without additional advertising. Consequently, those loyal customers would likely purchase from the company if they are interested in the product. In contrast, people with lower loyalty may overlook the product or buy similar products from other companies unless they receive marketing attention. Therefore, by focusing marketing efforts on individuals who are interested in the product but do not have strong loyalty, we can achieve more efficient marketing. To achieve this goal, we consider how to learn, from limited data, a classifier that identifies potential customers who (i) have interest in the product and (ii) do not have loyalty to the company. Although our algorithm comprises a single-stage optimization, its objective function implicitly contains two losses derived from standard PU learning settings. For this reason, we refer to our approach as double PU learning. We verify the validity of the proposed algorithm through numerical experiments, confirming that it functions appropriately for the problem at hand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00436v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato, Yuki Ikeda, Kentaro Baba, Takashi Imai, Ryo Inokuchi</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Learning in Finance</title>
      <link>https://arxiv.org/abs/2506.03780</link>
      <description>arXiv:2506.03780v2 Announce Type: replace-cross 
Abstract: Recent advances in machine learning have shown promising results for financial prediction using large, over-parameterized models. This paper provides theoretical foundations and empirical validation for understanding when and how these methods achieve predictive success. I examine two key aspects of high-dimensional learning in finance. First, I prove that within-sample standardization in Random Fourier Features implementations fundamentally alters the underlying Gaussian kernel approximation, replacing shift-invariant kernels with training-set dependent alternatives. Second, I establish information-theoretic lower bounds that identify when reliable learning is impossible no matter how sophisticated the estimator. A detailed quantitative calibration of the polynomial lower bound shows that with typical parameter choices, e.g., 12,000 features, 12 monthly observations, and R-square 2-3%, the required sample size to escape the bound exceeds 25-30 years of data--well beyond any rolling-window actually used. Thus, observed out-of-sample success must originate from lower-complexity artefacts rather than from the intended high-dimensional mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03780v2</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hasan Fallahgoul</dc:creator>
    </item>
  </channel>
</rss>
