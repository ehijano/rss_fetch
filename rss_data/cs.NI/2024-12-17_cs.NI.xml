<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Dec 2024 02:56:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Contextual Combinatorial Beam Management via Online Probing for Multiple Access mmWave Wireless Networks</title>
      <link>https://arxiv.org/abs/2412.10385</link>
      <description>arXiv:2412.10385v1 Announce Type: new 
Abstract: Due to the exponential increase in wireless devices and a diversification of network services, unprecedented challenges, such as managing heterogeneous data traffic and massive access demands, have arisen in next-generation wireless networks. To address these challenges, there is a pressing need for the evolution of multiple access schemes with advanced transceivers. Millimeter-wave (mmWave) communication emerges as a promising solution by offering substantial bandwidth and accommodating massive connectivities. Nevertheless, the inherent signaling directionality and susceptibility to blockages pose significant challenges for deploying multiple transceivers with narrow antenna beams. Consequently, beam management becomes imperative for practical network implementations to identify and track the optimal transceiver beam pairs, ensuring maximum received power and maintaining high-quality access service. In this context, we propose a Contextual Combinatorial Beam Management (CCBM) framework tailored for mmWave wireless networks. By leveraging advanced online probing techniques and integrating predicted contextual information, such as dynamic link qualities in spatial-temporal domain, CCBM aims to jointly optimize transceiver pairing and beam selection while balancing the network load. This approach not only facilitates multiple access effectively but also enhances bandwidth utilization and reduces computational overheads for real-time applications. Theoretical analysis establishes the asymptotically optimality of the proposed approach, complemented by extensive evaluation results showcasing the superiority of our framework over other state-of-the-art schemes in multiple dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10385v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhizhen Li, Xuanhao Luo, Mingzhe Chen, Chenhan Xu, Shiwen Mao, Yuchen Liu</dc:creator>
    </item>
    <item>
      <title>Compact Probe Request Fingerprinting with Asymmetric Pairwise Boosting</title>
      <link>https://arxiv.org/abs/2412.10548</link>
      <description>arXiv:2412.10548v1 Announce Type: new 
Abstract: Probe Requests are Wi-Fi management frames periodically sent by devices during network discovery. Tracking Probe Requests over time offers insights into movement patterns, traffic flows, and behavior trends, which are keys in applications such as urban planning, human mobility analysis, and retail analytics. To protect user privacy, techniques such as MAC address randomization are employed, periodically altering device MAC addresses to limit tracking. However, research has shown that these privacy measures can be circumvented. By analyzing the Information Elements (IE) within the Probe Request body, it is possible to fingerprint devices and track users over time. This paper presents a machine learning-based approach for fingerprinting Wi-Fi Probe Requests in a compact fashion. We utilize Asymmetric Pairwise Boosting to learn discriminating filters which are then used to process specific bit sequences in Probe Request frames, and quantize the results into a compact binary format. Extensive evaluation on public datasets demonstrates a two-order-of-magnitude storage reduction compared to existing methods while maintaining robust fingerprinting performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10548v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Baccichet, Fabio Palmese, Alessandro E. C. Redondi, Matteo Cesana</dc:creator>
    </item>
    <item>
      <title>RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic Service Provisioning in Software-Defined SDM-EONs</title>
      <link>https://arxiv.org/abs/2412.10685</link>
      <description>arXiv:2412.10685v1 Announce Type: new 
Abstract: The implementation of 5G and the future deployment of 6G necessitate the utilization of optical networks that possess substantial capacity and exhibit minimal latency. The dynamic arrival and departure of connection requests in optical networks result in particular central links experiencing more traffic and congestion than non-central links. The occurrence of congested links leads to service blocking despite the availability of resources within the network, restricting the efficient utilization of network resources. The available algorithms in the literature that aim to balance load among network links offer a trade-off between blocking performance and algorithmic complexity, thus increasing service provisioning time. This work proposes a dynamic routing-based congestion-aware routing, modulation, core, and spectrum assignment (RMCSA) algorithm for space division multiplexing elastic optical networks (SDM-EONs). The algorithm finds alternative candidate paths based on real-time link occupancy metrics to minimize blocking due to link congestion under dynamic traffic scenarios. As a result, the algorithm reduces the formation of congestion hotspots in the network owing to link-betweenness centrality. We have performed extensive simulations using two realistic network topologies to compare the performance of the proposed algorithm with relevant RMCSA algorithms available in the literature. The simulation results verify the superior performance of our proposed algorithm compared to the benchmark Yen's K-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection blocking ratio and spectrum utilization efficiency. To expedite the route-finding process, we present a novel caching strategy that allows the proposed algorithm to demonstrate a much-reduced service delay time compared to the recently developed adaptive link weight-based load-balancing RMCSA algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10685v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Baljinder Singh Heera, Shrinivas Petale, Yatindra Nath Singh, Suresh Subramaniam</dc:creator>
    </item>
    <item>
      <title>Identification of Path Congestion Status for Network Performance Tomography using Deep Spatial-Temporal Learning</title>
      <link>https://arxiv.org/abs/2412.10762</link>
      <description>arXiv:2412.10762v1 Announce Type: new 
Abstract: Network tomography plays a crucial role in assessing the operational status of internal links within networks through end-to-end path-level measurements, independently of cooperation from the network infrastructure. However, the accuracy of performance inference in internal network links heavily relies on comprehensive end-to-end path performance data. Most network tomography algorithms employ conventional threshold-based methods to identify congestion along paths, while these methods encounter limitations stemming from network complexities, resulting in inaccuracies such as misidentifying abnormal links and overlooking congestion attacks, thereby impeding algorithm performance. This paper introduces the concept of Additive Congestion Status to address these challenges effectively. Using a framework that combines Adversarial Autoencoders (AAE) with Long Short-Term Memory (LSTM) networks, this approach robustly categorizes (as uncongested, single-congested, or multiple-congested) and quantifies (regarding the number of congested links) the Additive Congestion Status. Leveraging prior path information and capturing spatio-temporal characteristics of probing flows, this method significantly enhances the localization of congested links and the inference of link performance compared to conventional network tomography algorithms, as demonstrated through experimental evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10762v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengze Du, Zhiwei Yu, Xiangyu Wang</dc:creator>
    </item>
    <item>
      <title>RIS-assisted Seamless Connectivity in Wireless Multi-Hop Relay Networks</title>
      <link>https://arxiv.org/abs/2412.10867</link>
      <description>arXiv:2412.10867v1 Announce Type: new 
Abstract: In recent years, reconfigurable intelligent surfaces (RIS) have garnered significant attention for their ability to control the phase shifts in reflected signals. By intelligently adjusting these phases, RIS can establish seamless direct paths between communication devices obstructed by obstacles, eliminating the need for forwarding and significantly reducing system overhead associated with relaying. This capability is crucial in multi-hop ad hoc networks requiring multiple relay steps. Consequently, the concept of incorporating multi-hop RIS into wireless multi-hop relay networks has emerged. In this paper, we propose a novel network model where each UAV communication node is equipped with a RIS, facilitating seamless connections in multi-hop relay wireless networks. We analyze the performance of this model by integrating RIS-assisted physical layer modeling into the seamless connection network framework and conducting a detailed comparative analysis of RIS-assisted and conventional connections. At the medium access layer, we introduce a RIS-DCF MAC protocol based on the IEEE 802.11 distributed coordination function (DCF), modeling the medium access process as a two-hop access scenario. Our results demonstrate that the seamless connections and diversity gain provided by RIS significantly enhance the performance of multi-hop relay wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10867v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peini Yi, Wenchi Cheng, Jingqing Wang, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Fair AI-STA for Legacy Wi-Fi: Enhancing Sensing and Power Management with Deep Q-Learning</title>
      <link>https://arxiv.org/abs/2412.10874</link>
      <description>arXiv:2412.10874v1 Announce Type: new 
Abstract: With the increasing complexity of Wi-Fi networks and the iterative evolution of 802.11 protocols, the Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) protocol faces significant challenges in achieving fair channel access and efficient resource allocation between legacy and modern Wi-Fi devices. To address these challenges, we propose an AI-driven Station (AI-STA) equipped with a Deep Q-Learning (DQN) module that dynamically adjusts its receive sensitivity threshold and transmit power. The AI-STA algorithm aims to maximize fairness in resource allocation while ensuring diverse Quality of Service (QoS) requirements are met. The performance of the AI-STA is evaluated through discrete event simulations in a Wi-Fi network, demonstrating that it outperforms traditional stations in fairness and QoS metrics. Although the AI-STA does not exhibit exceptionally superior performance, it holds significant potential for meeting QoS and fairness requirements with the inclusion of additional MAC parameters. The proposed AI-driven Sensitivity and Power algorithm offers a robust framework for optimizing sensitivity and power control in AI-STA devices within legacy Wi-Fi networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10874v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peini Yi, Wenchi Cheng, Zhanyu Ju, Jingqing Wang, Jinzhe Pan, Yuehui Ouyang, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Warping the Edge: Where Instant Mobility in 5G Meets Stateful Applications</title>
      <link>https://arxiv.org/abs/2412.10927</link>
      <description>arXiv:2412.10927v1 Announce Type: new 
Abstract: Edge computing is considered a key paradigm for supporting real-time applications over 5G networks, as hosting applications at the network edge can substantially reduce delays. A significant fraction of real-time applications over 5G are expected to be highly mobile applications. However, one challenge with hosting mobile applications on the network edge is ensuring that users continue to get low latency as they move across different locations. This requires the support to handover clients to different edge sites with negligible application delays. However, many edge applications are stateful and can experience significant downtime during state migration over 5G. This paper addresses the problem of enabling stateful mobile edge applications in 5G networks. We first identify the key architectural issues and then propose a new system design, EdgeWarp, that mitigates delays during mobility through proactive application state migration. To enable this, we extend the existing edge data stores with the design of a novel two-step application state synchronization protocol, that leverages the early prediction of the target edge host. Additionally, EdgeWarp prioritizes the handover of latency-sensitive edge applications by communicating their latency requirements to the 5G control plane at the beginning of a data session. Our evaluation with real edge applications shows up to a 15.4x reduction in application downtime under mobility. We have made our anonymized code publicly accessible here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10927v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mukhtiar Ahmad, Faaiq Bilal, Mutahar Ali, Muhammad Ali Nawazish, Amir Salman, Shazer Ali, Fawad Ahmad, Zafar Ayyub Qazi</dc:creator>
    </item>
    <item>
      <title>Grey Wolf-Based Task Scheduling in Vehicular Fog Computing Systems</title>
      <link>https://arxiv.org/abs/2412.11230</link>
      <description>arXiv:2412.11230v1 Announce Type: new 
Abstract: Vehicular fog computing (VFC) can be considered as an important alternative to address the existing challenges in intelligent transportation systems (ITS). The main purpose of VFC is to perform computational tasks through various vehicles. At present, VFCs include powerful computing resources that bring the computational resources nearer to the requesting devices. This paper presents a new algorithm based on meta-heuristic optimization method for task scheduling problem in VFC. The task scheduling in VFC is formulated as a multi-objective optimization problem, which aims to reduce makespan and monetary cost. The proposed method utilizes the grey wolf optimization (GWO) and assigns the different priorities to static and dynamic fog nodes. Dynamic fog nodes represent the parked or moving vehicles and static fog nodes show the stationary servers. Afterwards, the tasks that require the most processing resources are chosen and allocated to fog nodes. The GWO-based method is extensively evaluated in more details. Furthermore, the effectiveness of various parameters in GWO algorithm is analyzed. We also assess the proposed algorithm on real application and random data. The outcomes of our experiments confirm that, in comparison to previous works, our algorithm is capable of offering the lowest monetary cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11230v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.22108/jcs.2024.142162.1145</arxiv:DOI>
      <dc:creator>Maryam Taghizadeh, Mahmood Ahmadi</dc:creator>
    </item>
    <item>
      <title>Towards 6G Network Slicing</title>
      <link>https://arxiv.org/abs/2412.11366</link>
      <description>arXiv:2412.11366v1 Announce Type: new 
Abstract: Networks should connect communicating peers, supporting vertical services requirements. The network evolution towards 6G requires native network slicing techniques. Some literature approaches claim network slice realization, but they do not convincingly address the deployment across multiple Autonomous Systems. This work investigates the current 6G network slicing landscape, presents some gaps, and introduces the concept of the recursive network slicing between multiple Autonomous Systems, supported by the NASOR approach. This innovative concept supports implementing new network services required by the 6G vision. This work also sheds light on the 6G requirements for network slicing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11366v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5753/w6g.2021.17231.</arxiv:DOI>
      <dc:creator>Rodrigo Moreira, Fl\'avio de Oliveira Silva</dc:creator>
    </item>
    <item>
      <title>SareQuant: Towards a quantum-based communication network</title>
      <link>https://arxiv.org/abs/2412.11898</link>
      <description>arXiv:2412.11898v1 Announce Type: new 
Abstract: This paper presents the SareQuant project, which aims to evolve the Basque NREN (National Research and Education Networks) into a quantum-based communication infrastructure. SareQuant focuses on the network design and on the integration of quantum technologies into real-world scenarios and applications. Therefore, this paper provides insights into the opportunities and challenges regarding the integration of quantum technologies, thus paving the way for a secure and advanced Quantum Internet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11898v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>XVI Jornadas de Ingenier\'ia Telem\'atica JITEL (2023),51-54, ISBN: 978-84-09-58148-1, (https://web.salleurl.edu/docsmkt/JITEL/libro-actas-jitel.pdf)</arxiv:journal_reference>
      <dc:creator>Ane Sanz, David Franco, Asier Atutxa, Jasone Astorga, Eduardo Jacob</dc:creator>
    </item>
    <item>
      <title>Edge AI-based Radio Frequency Fingerprinting for IoT Networks</title>
      <link>https://arxiv.org/abs/2412.10553</link>
      <description>arXiv:2412.10553v1 Announce Type: cross 
Abstract: The deployment of the Internet of Things (IoT) in smart cities and critical infrastructure has enhanced connectivity and real-time data exchange but introduced significant security challenges. While effective, cryptography can often be resource-intensive for small-footprint resource-constrained (i.e., IoT) devices. Radio Frequency Fingerprinting (RFF) offers a promising authentication alternative by using unique RF signal characteristics for device identification at the Physical (PHY)-layer, without resorting to cryptographic solutions. The challenge is two-fold: how to deploy such RFF in a large scale and for resource-constrained environments. Edge computing, processing data closer to its source, i.e., the wireless device, enables faster decision-making, reducing reliance on centralized cloud servers. Considering a modest edge device, we introduce two truly lightweight Edge AI-based RFF schemes tailored for resource-constrained devices. We implement two Deep Learning models, namely a Convolution Neural Network and a Transformer-Encoder, to extract complex features from the IQ samples, forming device-specific RF fingerprints. We convert the models to TensorFlow Lite and evaluate them on a Raspberry Pi, demonstrating the practicality of Edge deployment. Evaluations demonstrate the Transformer-Encoder outperforms the CNN in identifying unique transmitter features, achieving high accuracy (&gt; 0.95) and ROC-AUC scores (&gt; 0.90) while maintaining a compact model size of 73KB, appropriate for resource-constrained devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10553v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmed Mohamed Hussain, Nada Abughanam, Panos Papadimitratos</dc:creator>
    </item>
    <item>
      <title>A technical solution for the rule of law, peace, security, and evolvability of global cyberspace -- solve the three genetic defects of IP network</title>
      <link>https://arxiv.org/abs/2412.10722</link>
      <description>arXiv:2412.10722v1 Announce Type: cross 
Abstract: Since its inception in the 1960s, the internet has profoundly transformed human life. However, its original design now struggles to meet the evolving demands of modern society. Three primary defects have emerged: First, the concentration of power among a few dominant entities has intensified international conflicts and widened the technological divide. Second, the Internet Protocol (IP)-based system lacks inherent security, leading to frequent global cybersecurity incidents. Third, the rigidity of the IP protocol has hindered the sustainable development of cyberspace, as it resists necessary adaptations and innovations. Addressing these issues is crucial for the future resilience and security of the global digital landscape.
  To address these challenges, we propose the Co-governed Multi-Identifier Network (CoG-MIN briefly as MIN), a novel network architecture that leverages blockchain technology to ensure equal participation of countries worldwide in cyberspace governance and the rule of law. As a next-generation network system, CoG-MIN integrates mechanisms such as user authentication, data signatures, and encryption to significantly enhance network security. In testing environments, CoG-MIN has consistently withstood extensive attacks during various international cybersecurity competitions. Additionally, CoG-MIN supports the evolution and interoperability of different identifier systems, remains IP-compatible, and facilitates a gradual transition away from IP, providing an adaptable ecosystem for diverse network architectures. This adaptability fosters the development and evolution of diverse network architectures within CoG-MIN, making it a natural progression for the internet's future development.
  We further introduce a trilogy of cyberspace security theorems... (Due to character limitations, the full abstract is available in the paper PDF.)</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10722v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hui Li, Kedan Li, Jiaqing Lv, Yuanshao Liang, Feng Han, Shuo-Yen Robert Li</dc:creator>
    </item>
    <item>
      <title>Adaptive Quantization Resolution and Power Control for Federated Learning over Cell-free Networks</title>
      <link>https://arxiv.org/abs/2412.10878</link>
      <description>arXiv:2412.10878v1 Announce Type: cross 
Abstract: Federated learning (FL) is a distributed learning framework where users train a global model by exchanging local model updates with a server instead of raw datasets, preserving data privacy and reducing communication overhead. However, the latency grows with the number of users and the model size, impeding the successful FL over traditional wireless networks with orthogonal access. Cell-free massive multiple-input multipleoutput (CFmMIMO) is a promising solution to serve numerous users on the same time/frequency resource with similar rates. This architecture greatly reduces uplink latency through spatial multiplexing but does not take application characteristics into account. In this paper, we co-optimize the physical layer with the FL application to mitigate the straggler effect. We introduce a novel adaptive mixed-resolution quantization scheme of the local gradient vector updates, where only the most essential entries are given high resolution. Thereafter, we propose a dynamic uplink power control scheme to manage the varying user rates and mitigate the straggler effect. The numerical results demonstrate that the proposed method achieves test accuracy comparable to classic FL while reducing communication overhead by at least 93% on the CIFAR-10, CIFAR-100, and Fashion-MNIST datasets. We compare our methods against AQUILA, Top-q, and LAQ, using the max-sum rate and Dinkelbach power control schemes. Our approach reduces the communication overhead by 75% and achieves 10% higher test accuracy than these benchmarks within a constrained total latency budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10878v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Afsaneh Mahmoudi, Emil Bj\"ornson</dc:creator>
    </item>
    <item>
      <title>C3: Learning Congestion Controllers with Formal Certificates</title>
      <link>https://arxiv.org/abs/2412.10915</link>
      <description>arXiv:2412.10915v1 Announce Type: cross 
Abstract: Learning-based congestion controllers offer better adaptability compared to traditional heuristic algorithms. However, the inherent unreliability of learning techniques can cause learning-based controllers to behave poorly, creating a need for formal guarantees. While methods for formally verifying learned congestion controllers exist, these methods offer binary feedback that cannot optimize the controller toward better behavior. We improve this state-of-the-art via C3, a new learning framework for congestion control that integrates the concept of formal certification in the learning loop. C3 uses an abstract interpreter that can produce robustness and performance certificates to guide the training process, rewarding models that are robust and performant even on worst-case inputs. Our evaluation demonstrates that unlike state-of-the-art learned controllers, C3-trained controllers provide both adaptability and worst-case reliability across a range of network conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10915v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenxi Yang, Divyanshu Saxena, Rohit Dwivedula, Kshiteej Mahajan, Swarat Chaudhuri, Aditya Akella</dc:creator>
    </item>
    <item>
      <title>Communications over Unlicensed sub-8 GHz Spectrum: Opportunities and Challenges</title>
      <link>https://arxiv.org/abs/2412.11002</link>
      <description>arXiv:2412.11002v1 Announce Type: cross 
Abstract: The utilization of unlicensed spectrum presents a promising solution to the issue of spectrum scarcity in densely populated areas, while also offering a cost-effective means to connect underserved regions. In response to this potential, both academia and industry are actively exploring innovative applications of unlicensed spectrum. This work offers a thorough overview of unlicensed spectrum bands below 8 GHz, including TV White Spaces, Civil Broadband Radio Services, Industrial Scientific Medical bands, and the Unlicensed National Information Infrastructure. The paper focuses on three key aspects: regulations, existing technologies, and applications. It is essential to recognize that "unlicensed" does not equate to "unregulated"; therefore, a clear understanding of permissible and prohibited activities is crucial. From a technological perspective, we examine the current technologies, their capabilities, and relevant applications. Additionally, the shared nature of this spectrum introduces challenges related to interference among users. These collisions can be managed through two primary strategies, that we described: a database-driven approach and coexistence mechanisms at the MAC and PHY layers. This work may serve as a starting point for those who are interested in the unlicensed spectrum, both in academia and industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11002v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Karim Saifullin, Hussein Al-Shatri, Mohamed-Slim Alouini</dc:creator>
    </item>
    <item>
      <title>ProFe: Communication-Efficient Decentralized Federated Learning via Distillation and Prototypes</title>
      <link>https://arxiv.org/abs/2412.11207</link>
      <description>arXiv:2412.11207v1 Announce Type: cross 
Abstract: Decentralized Federated Learning (DFL) trains models in a collaborative and privacy-preserving manner while removing model centralization risks and improving communication bottlenecks. However, DFL faces challenges in efficient communication management and model aggregation within decentralized environments, especially with heterogeneous data distributions. Thus, this paper introduces ProFe, a novel communication optimization algorithm for DFL that combines knowledge distillation, prototype learning, and quantization techniques. ProFe utilizes knowledge from large local models to train smaller ones for aggregation, incorporates prototypes to better learn unseen classes, and applies quantization to reduce data transmitted during communication rounds. The performance of ProFe has been validated and compared to the literature by using benchmark datasets like MNIST, CIFAR10, and CIFAR100. Results showed that the proposed algorithm reduces communication costs by up to ~40-50% while maintaining or improving model performance. In addition, it adds ~20% training time due to increased complexity, generating a trade-off.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11207v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Miguel S\'anchez S\'anchez, Enrique Tom\'as Mart\'inez Beltr\'an, Miguel Fern\'andez Llamas, G\'er\^ome Bovet, Gregorio Mart\'inez P\'erez, Alberto Huertas Celdr\'an</dc:creator>
    </item>
    <item>
      <title>BA-BFL: Barycentric Aggregation for Bayesian Federated Learning</title>
      <link>https://arxiv.org/abs/2412.11646</link>
      <description>arXiv:2412.11646v1 Announce Type: cross 
Abstract: In this work, we study the problem of aggregation in the context of Bayesian Federated Learning (BFL). Using an information geometric perspective, we interpret the BFL aggregation step as finding the barycenter of the trained posteriors for a pre-specified divergence metric. We study the barycenter problem for the parametric family of $\alpha$-divergences and, focusing on the standard case of independent and Gaussian distributed parameters, we recover the closed-form solution of the reverse Kullback-Leibler barycenter and develop the analytical form of the squared Wasserstein-2 barycenter. Considering a non-IID setup, where clients possess heterogeneous data, we analyze the performance of the developed algorithms against state-of-the-art (SOTA) Bayesian aggregation methods in terms of accuracy, uncertainty quantification (UQ), model calibration (MC), and fairness. Finally, we extend our analysis to the framework of Hybrid Bayesian Deep Learning (HBDL), where we study how the number of Bayesian layers in the architecture impacts the considered performance metrics. Our experimental results show that the proposed methodology presents comparable performance with the SOTA while offering a geometric interpretation of the aggregation phase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11646v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nour Jamoussi, Giuseppe Serra, Photios A. Stavrou, Marios Kountouris</dc:creator>
    </item>
    <item>
      <title>Security and Fairness in Multi-Party Quantum Secret Sharing Protocol</title>
      <link>https://arxiv.org/abs/2412.11667</link>
      <description>arXiv:2412.11667v1 Announce Type: cross 
Abstract: Quantum secret sharing (QSS) is a cryptographic protocol that leverages quantum mechanics to distribute a secret among multiple parties. With respect to the classical counterpart, in QSS the secret is encoded into quantum states and shared by a dealer such that only an authorized subsets of participants, i.e., the players, can reconstruct it. Several state-of-the-art studies aim to transpose classical Secret Sharing into the quantum realm, while maintaining their reliance on traditional network topologies (e.g., star, ring, fully-connected) and require that all the n players calculate the secret. These studies exploit the Greenberger-Horne-Zeilinger (GHZ) state, which is a type of maximally entangled quantum state involving three or more qubits. However, none of these works account for redundancy, enhanced security/privacy features or authentication mechanisms able to fingerprint players. To address these gaps, in this paper we introduce a new concept of QSS which leans on a generic distributed quantum-network, based on a threshold scheme, where all the players collaborate also to the routing of quantum information among them. The dealer, by exploiting a custom flexible weighting system, takes advantage of a newly defined quantum Dijkstra algorithm to select the most suitable subset of t players, out of the entire set on n players, to involve in the computation. To fingerprint and authenticate users, CRYSTAL-Kyber primitives are adopted, while also protecting each player's privacy by hiding their identities. We show the effectiveness and performance of the proposed protocol by testing it against the main classical and quantum attacks, thereby improving the state-of-the-art security measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11667v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alessio Di Santo, Walter Tiberti, Dajana Cassioli</dc:creator>
    </item>
    <item>
      <title>Cooperative Graceful Degradation In Containerized Clouds</title>
      <link>https://arxiv.org/abs/2312.12809</link>
      <description>arXiv:2312.12809v3 Announce Type: replace 
Abstract: Cloud resilience is crucial for cloud operators and the myriad of applications that rely on the cloud. Today, we lack a mechanism that enables cloud operators to perform graceful degradation of applications while satisfying the application's availability requirements. In this paper, we put forward a vision for automated cloud resilience management with cooperative graceful degradation between applications and cloud operators. First, we investigate techniques for graceful degradation and identify an opportunity for cooperative graceful degradation in public clouds. Second, leveraging criticality tags on containers, we propose diagonal scaling -- turning off non-critical containers during capacity crunch scenarios -- to maximize the availability of critical services. Third, we design Phoenix, an automated cloud resilience management system that maximizes critical service availability of applications while also considering operator objectives, thereby improving the overall resilience of the infrastructure during failures. We experimentally show that the Phoenix controller running atop Kubernetes can improve critical service availability by up to $2\times$ during large-scale failures. Phoenix can handle failures in a cluster of 100,000 nodes within 10 seconds. We also develop AdaptLab, an open-source resilience benchmarking framework that can emulate realistic cloud environments with real-world application dependency graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12809v3</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kapil Agrawal, Sangeetha Abdu Jyothi</dc:creator>
    </item>
    <item>
      <title>Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)</title>
      <link>https://arxiv.org/abs/2403.07573</link>
      <description>arXiv:2403.07573v2 Announce Type: replace 
Abstract: In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent requirements. ACNC encompasses two primary functionalities: state recognition and context detection. Given the intricate nature of the user-service-computing-network space, the paper employs dimension reduction to generate live, holistic, abstract system states in a hierarchical structure. To address the challenges posed by dynamic changes, Continual Learning (CL) is employed, classifying the system state into contexts controlled by dedicated ML agents, enabling them to operate efficiently. These two functionalities are intricately linked within a closed loop overseen by the End-to-End (E2E) orchestrator to allocate resources. The paper introduces the components of ACNC, proposes a Metaverse scenario to exemplify ACNC's role in resource provisioning with Segment Routing v6 (SRv6), outlines ACNC's workflow, details a numerical analysis for efficiency assessment, and concludes with discussions on relevant challenges and potential avenues for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07573v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masoud Shokrnezhad, Hao Yu, Tarik Taleb, Richard Li, Kyunghan Lee, Jaeseung Song, Cedric Westphal</dc:creator>
    </item>
    <item>
      <title>Integrated Sensing and Communications for Low-Altitude Economy: A Deep Reinforcement Learning Approach</title>
      <link>https://arxiv.org/abs/2412.04074</link>
      <description>arXiv:2412.04074v2 Announce Type: replace 
Abstract: This paper studies an integrated sensing and communications (ISAC) system for low-altitude economy (LAE), where a ground base station (GBS) provides communication and navigation services for authorized unmanned aerial vehicles (UAVs), while sensing the low-altitude airspace to monitor the unauthorized mobile target. The expected communication sum-rate over a given flight period is maximized by jointly optimizing the beamforming at the GBS and UAVs' trajectories, subject to the constraints on the average signal-to-noise ratio requirement for sensing, the flight mission and collision avoidance of UAVs, as well as the maximum transmit power at the GBS. Typically, this is a sequential decision-making problem with the given flight mission. Thus, we transform it to a specific Markov decision process (MDP) model called episode task. Based on this modeling, we propose a novel LAE-oriented ISAC scheme, referred to as Deep LAE-ISAC (DeepLSC), by leveraging the deep reinforcement learning (DRL) technique. In DeepLSC, a reward function and a new action selection policy termed constrained noise-exploration policy are judiciously designed to fulfill various constraints. To enable efficient learning in episode tasks, we develop a hierarchical experience replay mechanism, where the gist is to employ all experiences generated within each episode to jointly train the neural network. Besides, to enhance the convergence speed of DeepLSC, a symmetric experience augmentation mechanism, which simultaneously permutes the indexes of all variables to enrich available experience sets, is proposed. Simulation results demonstrate that compared with benchmarks, DeepLSC yields a higher sum-rate while meeting the preset constraints, achieves faster convergence, and is more robust against different settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04074v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaowen Ye, Yuyi Mao, Xianghao Yu, Shu Sun, Liqun Fu, Jie Xu</dc:creator>
    </item>
    <item>
      <title>GDSG: Graph Diffusion-based Solution Generator for Optimization Problems in MEC Networks</title>
      <link>https://arxiv.org/abs/2412.08296</link>
      <description>arXiv:2412.08296v2 Announce Type: replace 
Abstract: Optimization is crucial for MEC networks to function efficiently and reliably, most of which are NP-hard and lack efficient approximation algorithms. This leads to a paucity of optimal solution, constraining the effectiveness of conventional deep learning approaches. Most existing learning-based methods necessitate extensive optimal data and fail to exploit the potential benefits of suboptimal data that can be obtained with greater efficiency and effectiveness. Taking the multi-server multi-user computation offloading (MSCO) problem, which is widely observed in systems like Internet-of-Vehicles (IoV) and Unmanned Aerial Vehicle (UAV) networks, as a concrete scenario, we present a Graph Diffusion-based Solution Generation (GDSG) method. This approach is designed to work with suboptimal datasets while converging to the optimal solution large probably. We transform the optimization issue into distribution-learning and offer a clear explanation of learning from suboptimal training datasets. We build GDSG as a multi-task diffusion model utilizing a Graph Neural Network (GNN) to acquire the distribution of high-quality solutions. We use a simple and efficient heuristic approach to obtain a sufficient amount of training data composed entirely of suboptimal solutions. In our implementation, we enhance the backbone GNN and achieve improved generalization. GDSG also reaches nearly 100\% task orthogonality, ensuring no interference between the discrete and continuous generation tasks. We further reveal that this orthogonality arises from the diffusion-related training loss, rather than the neural network architecture itself. The experiments demonstrate that GDSG surpasses other benchmark methods on both the optimal and suboptimal training datasets. The MSCO datasets has open-sourced at http://ieee-dataport.org/13824, as well as the GDSG algorithm codes at https://github.com/qiyu3816/GDSG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08296v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruihuai Liang, Bo Yang, Pengyu Chen, Xuelin Cao, Zhiwen Yu, M\'erouane Debbah, Dusit Niyato, H. Vincent Poor, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>Teola: Towards End-to-End Optimization of LLM-based Applications</title>
      <link>https://arxiv.org/abs/2407.00326</link>
      <description>arXiv:2407.00326v2 Announce Type: replace-cross 
Abstract: Large language model (LLM)-based applications consist of both LLM and non-LLM components, each contributing to the end-to-end latency. Despite great efforts to optimize LLM inference, end-to-end workflow optimization has been overlooked. Existing frameworks employ coarse-grained orchestration with task modules, which confines optimizations to within each module and yields suboptimal scheduling decisions. We propose fine-grained end-to-end orchestration, which utilizes task primitives as the basic units and represents each query's workflow as a primitive-level dataflow graph. This explicitly exposes a much larger design space, enables optimizations in parallelization and pipelining across primitives of different modules, and enhances scheduling to improve application-level performance. We build Teola, a novel orchestration framework for LLM-based applications that implements this scheme. Comprehensive experiments show that Teola can achieve up to 2.09x speedup over existing systems across various popular LLM applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00326v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Tan, Yimin Jiang, Yitao Yang, Hong Xu</dc:creator>
    </item>
    <item>
      <title>Diffusion Models as Network Optimizers: Explorations and Analysis</title>
      <link>https://arxiv.org/abs/2411.00453</link>
      <description>arXiv:2411.00453v3 Announce Type: replace-cross 
Abstract: Network optimization is a fundamental challenge in the Internet of Things (IoT) network, often characterized by complex features that make it difficult to solve these problems. Recently, generative diffusion models (GDMs) have emerged as a promising new approach to network optimization, with the potential to directly address these optimization problems. However, the application of GDMs in this field is still in its early stages, and there is a noticeable lack of theoretical research and empirical findings. In this study, we first explore the intrinsic characteristics of generative models. Next, we provide a concise theoretical proof and intuitive demonstration of the advantages of generative models over discriminative models in network optimization. Based on this exploration, we implement GDMs as optimizers aimed at learning high-quality solution distributions for given inputs, sampling from these distributions during inference to approximate or achieve optimal solutions. Specifically, we utilize denoising diffusion probabilistic models (DDPMs) and employ a classifier-free guidance mechanism to manage conditional guidance based on input parameters. We conduct extensive experiments across three challenging network optimization problems. By investigating various model configurations and the principles of GDMs as optimizers, we demonstrate the ability to overcome prediction errors and validate the convergence of generated solutions to optimal solutions.We provide code and data at https://github.com/qiyu3816/DiffSG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00453v3</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruihuai Liang, Bo Yang, Pengyu Chen, Xianjin Li, Yifan Xue, Zhiwen Yu, Xuelin Cao, Yan Zhang, M\'erouane Debbah, H. Vincent Poor, Chau Yuen</dc:creator>
    </item>
  </channel>
</rss>
