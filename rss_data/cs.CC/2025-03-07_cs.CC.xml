<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Mar 2025 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers</title>
      <link>https://arxiv.org/abs/2503.03961</link>
      <description>arXiv:2503.03961v1 Announce Type: cross 
Abstract: Recent theoretical results show transformers cannot express sequential reasoning problems over long input lengths, intuitively because their computational depth is bounded. However, prior work treats the depth as a constant, leaving it unclear to what degree bounded depth may suffice for solving problems over short inputs, or how increasing the transformer's depth affects its expressive power. We address these questions by analyzing the expressive power of transformers whose depth can grow minimally with context length $n$. We show even highly uniform transformers with depth $\Theta(\log n)$ can express two important problems: recognizing regular languages, which captures state tracking abilities, and graph connectivity, which underlies multi-step reasoning. Notably, both of these problems cannot be expressed by fixed-depth transformers under standard complexity conjectures, demonstrating the expressivity benefit of growing depth. Moreover, our theory quantitatively predicts how depth must grow with input length to express these problems, showing that depth scaling is more efficient than scaling width or chain-of-thought steps. Empirically, we find our theoretical depth requirements for regular language recognition match the practical depth requirements of transformers remarkably well. Thus, our results clarify precisely how depth affects transformers' reasoning capabilities, providing potential practical insights for designing models that are better at sequential reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03961v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Merrill, Ashish Sabharwal</dc:creator>
    </item>
    <item>
      <title>Succinct Perfect Zero-knowledge for MIP*</title>
      <link>https://arxiv.org/abs/2503.04517</link>
      <description>arXiv:2503.04517v1 Announce Type: cross 
Abstract: In the recent breakthrough result of Slofstra and Mastel (STOC'24), they show that there is a two-player one-round perfect zero-knowledge MIP* protocol for RE. We build on their result to show that there exists a succinct two-player one-round perfect zero-knowledge MIP* protocol for RE with polylog question size and O(1) answer size, or with O(1) question size and polylog answer size. To prove our result, we analyze the four central compression techniques underlying the MIP*= RE proof (Ji et al. '20) -- question reduction, oracularization, answer reduction, and parallel repetition -- and show that they all preserve the perfect (as well as statistical and computational) zero-knowledge properties of the original protocol. Furthermore, we complete the study of the conversion between constraint-constraint and constraint-variable binary constraint system (BCS) nonlocal games, which provide a quantum information characterization of MIP* protocols. While Paddock (QIP'23) established that any near-perfect strategy for a constraint-variable game can be mapped to a constraint-constraint version, we prove the converse, fully establishing their equivalence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04517v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Honghao Fu, Xingjian Zhang</dc:creator>
    </item>
    <item>
      <title>Descriptive complexity for neural networks via Boolean networks</title>
      <link>https://arxiv.org/abs/2308.06277</link>
      <description>arXiv:2308.06277v3 Announce Type: replace 
Abstract: We investigate the descriptive complexity of a class of neural networks with unrestricted topologies and piecewise polynomial activation functions. We consider the general scenario where the networks run for an unlimited number of rounds and floating-point numbers are used to simulate reals. We characterize these neural networks with a recursive rule-based logic for Boolean networks. In particular, we show that the sizes of the neural networks and the corresponding Boolean rule formulae are polynomially related. In fact, in the direction from Boolean rules to neural networks, the blow-up is only linear. Our translations result in a time delay, which is the number of rounds that it takes for an object's translation to simulate a single round of the object. In the translation from neural networks to Boolean rules, the time delay of the resulting formula is polylogarithmic in the neural network size. In the converse translation, the time delay of the neural network is linear in the formula size. As a corollary, by restricting our logic, we obtain a similar characterization for classical feedforward neural networks. We also obtain translations between the rule-based logic for Boolean networks, the diamond-free fragment of modal substitution calculus and a class of recursive Boolean circuits where the number of input and output gates match. Ultimately, our translations offer a method of translating a given neural network into an equivalent neural network with different activation functions, including linear activation functions!</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.06277v3</guid>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Veeti Ahvonen, Damian Heiman, Antti Kuusisto</dc:creator>
    </item>
    <item>
      <title>On the computational power of $C$-random strings</title>
      <link>https://arxiv.org/abs/2409.04448</link>
      <description>arXiv:2409.04448v2 Announce Type: replace 
Abstract: Denote by $H$ the Halting problem. Let $R_U: = \{ x | C_U(x) \ge |x|\}$, where $C_U(x)$ is the plain Kolmogorov complexity of $x$ under a universal decompressor $U$. We prove that there exists a universal $U$ such that $H \in P^{R_U}$, solving the problem posted by Eric Allender.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04448v2</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexey Milovanov</dc:creator>
    </item>
    <item>
      <title>Quantum computation with indefinite causal structures</title>
      <link>https://arxiv.org/abs/1706.09854</link>
      <description>arXiv:1706.09854v4 Announce Type: replace-cross 
Abstract: One way to study the physical plausibility of closed timelike curves (CTCs) is to examine their computational power. This has been done for Deutschian CTCs (D-CTCs) and post-selection CTCs (P-CTCs), with the result that they allow for the efficient solution of problems in PSPACE and PP, respectively. Since these are extremely powerful complexity classes, which are not expected to be solvable in reality, this can be taken as evidence that these models for CTCs are pathological. This problem is closely related to the nonlinearity of this models, which also allows for example cloning quantum states, in the case of D-CTCs, or distinguishing non-orthogonal quantum states, in the case of P-CTCs. In contrast, the process matrix formalism allows one to model indefinite causal structures in a linear way, getting rid of these effects, and raising the possibility that its computational power is rather tame. In this paper we show that process matrices correspond to a linear particular case of P-CTCs, and therefore that its computational power is upperbounded by that of PP. We show, furthermore, a family of processes that can violate causal inequalities but nevertheless can be simulated by a causally ordered quantum circuit with only a constant overhead, showing that indefinite causality is not necessarily hard to simulate.</description>
      <guid isPermaLink="false">oai:arXiv.org:1706.09854v4</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevA.96.052315</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. A 96, 052315 (2017)</arxiv:journal_reference>
      <dc:creator>Mateus Ara\'ujo, Philippe Allard Gu\'erin, \"Amin Baumeler</dc:creator>
    </item>
    <item>
      <title>Computational complexity of isometric tensor network states</title>
      <link>https://arxiv.org/abs/2402.07975</link>
      <description>arXiv:2402.07975v2 Announce Type: replace-cross 
Abstract: We determine the computational power of isometric tensor network states (isoTNS), a variational ansatz originally developed to numerically find and compute properties of gapped ground states and topological states in two dimensions. By mapping 2D isoTNS to 1+1D unitary quantum circuits, we find that computing local expectation values in isoTNS is $\textsf{BQP}$-complete. We then introduce injective isoTNS, which are those isoTNS that are the unique ground states of frustration-free Hamiltonians, and which are characterized by an injectivity parameter $\delta\in(0,1/D]$, where $D$ is the bond dimension of the isoTNS. We show that injectivity necessarily adds depolarizing noise to the circuit at a rate $\eta=\delta^2D^2$. We show that weakly injective isoTNS (small $\delta$) are still $\textsf{BQP}$-complete, but that there exists an efficient classical algorithm to compute local expectation values in strongly injective isoTNS ($\eta\geq0.41$). Sampling from isoTNS corresponds to monitored quantum dynamics and we exhibit a family of isoTNS that undergo a phase transition from a hard regime to an easy phase where the monitored circuit can be sampled efficiently. Our results can be used to design provable algorithms to contract isoTNS. Our mapping between ground states of certain frustration-free Hamiltonians to open circuit dynamics in one dimension fewer may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07975v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Malz, Rahul Trivedi</dc:creator>
    </item>
    <item>
      <title>The Illusion of State in State-Space Models</title>
      <link>https://arxiv.org/abs/2404.08819</link>
      <description>arXiv:2404.08819v3 Announce Type: replace-cross 
Abstract: State-space models (SSMs) have emerged as a potential alternative architecture for building large language models (LLMs) compared to the previously ubiquitous transformer architecture. One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill &amp; Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks (RNNs). But do SSMs truly have an advantage (over transformers) in expressive power for state tracking? Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class $\mathsf{TC}^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its recurrent formulation, the "state" in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08819v3</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.CL</category>
      <category>cs.FL</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Merrill, Jackson Petty, Ashish Sabharwal</dc:creator>
    </item>
  </channel>
</rss>
