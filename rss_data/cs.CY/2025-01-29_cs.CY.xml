<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Jan 2025 05:00:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Trustworthiness in Stochastic Systems: Towards Opening the Black Box</title>
      <link>https://arxiv.org/abs/2501.16461</link>
      <description>arXiv:2501.16461v1 Announce Type: new 
Abstract: AI systems are increasingly tasked to complete responsibilities with decreasing oversight. This delegation requires users to accept certain risks, typically mitigated by perceived or actual alignment of values between humans and AI, leading to confidence that the system will act as intended. However, stochastic behavior by an AI system threatens to undermine alignment and potential trust. In this work, we take a philosophical perspective to the tension and potential conflict between stochasticity and trustworthiness. We demonstrate how stochasticity complicates traditional methods of establishing trust and evaluate two extant approaches to managing it: (1) eliminating user-facing stochasticity to create deterministic experiences, and (2) allowing users to independently control tolerances for stochasticity. We argue that both approaches are insufficient, as not all forms of stochasticity affect trustworthiness in the same way or to the same degree. Instead, we introduce a novel definition of stochasticity and propose latent value modeling for both AI systems and users to better assess alignment. This work lays a foundational step toward understanding how and when stochasticity impacts trustworthiness, enabling more precise trust calibration in complex AI systems, and underscoring the importance of sociotechnical analyses to effectively address these challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16461v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jennifer Chien, David Danks</dc:creator>
    </item>
    <item>
      <title>Classroom Activities and New Classroom Apps for Enhancing Children's Understanding of Social Media Mechanisms</title>
      <link>https://arxiv.org/abs/2501.16494</link>
      <description>arXiv:2501.16494v1 Announce Type: new 
Abstract: Young people are increasingly exposed to adverse effects of data-driven profiling, recommending, and manipulation on social media platforms, most of them without adequate understanding of the mechanisms that drive these platforms. In the context of computing education, educating learners about mechanisms and data practices of social media may improve young learners' data agency, digital literacy, and understanding how their digital services work. A four-hour technology -- supported intervention was designed and implemented in 12 schools involving 209 5th and 8th grade learners. Two new classroom apps were developed to support the classroom activities. Using Likert-scale questions borrowed from a data agency questionnaire and open-ended questions that mapped learners' data-driven reasoning on social media phenomena, this article shows significant improvement between pre- and post-tests in learners' data agency and data-driven explanations of social media mechanisms. Results present an example of improving young learners' understanding of social media mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16494v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Henriikka Vartiainen, Nicolas Pope, Juho Kahila, Sonsoles L\'opez-Pernas, Matti Tedre</dc:creator>
    </item>
    <item>
      <title>Towards Frontier Safety Policies Plus</title>
      <link>https://arxiv.org/abs/2501.16500</link>
      <description>arXiv:2501.16500v1 Announce Type: new 
Abstract: This paper examines the state of affairs on Frontier Safety Policies in light of capability progress and growing expectations held by government actors and AI safety researchers from these safety policies. It subsequently argues that FSPs should evolve to a more granular version, which this paper calls FSPs Plus. Compared to the first wave of FSPs led by a subset of frontier AI companies, FSPs Plus should be built around two main pillars. First, FSPs Plus should adopt precursory capabilities as a new, clearer, and more comprehensive set of metrics. In this respect, this paper recommends that international or domestic standardization bodies develop a standardized taxonomy of precursory components to high-impact capabilities that FSPs Plus could then adopt by reference. The Frontier Model Forum could lead the way by establishing preliminary consensus amongst frontier AI developers on this topic. Second, FSPs Plus should expressly incorporate AI safety cases and establish a mutual feedback mechanism between FSPs Plus and AI safety cases. To establish such a mutual feedback mechanism, FSPs Plus could be updated to include a clear commitment to make AI safety cases at different milestones during development and deployment, to build and adopt safety measures based on the content and confidence of AI safety cases, and, also on this basis, to keep updating and adjusting FSPs Plus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16500v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Pistillo</dc:creator>
    </item>
    <item>
      <title>Responsible Generative AI Use by Product Managers: Recoupling Ethical Principles and Practices</title>
      <link>https://arxiv.org/abs/2501.16531</link>
      <description>arXiv:2501.16531v1 Announce Type: new 
Abstract: Since 2022, generative AI (genAI) has rapidly become integrated into workplaces. Though organizations have made commitments to use this technology "responsibly", how organizations and their employees prioritize responsibility in their decision-making remains absent from extant management theorizing. In this paper, we examine how product managers - who often serve as gatekeepers in decision-making processes - implement responsible practices in their day-to-day work when using genAI. Using Institutional Theory, we illuminate the factors that constrain or support proactive responsible development and usage of genAI technologies. We employ a mixed methods research design, drawing on 25 interviews with product managers and a global survey of 300 respondents in product management-related roles. The majority of our respondents report (1) widespread uncertainty regarding what "responsibility" means or looks like, (2) diffused responsibility given assumed ethical actions by other teams, (3) lack of clear incentives and guidance within organizations, and (4) the importance of leadership buy-in and principles for navigating tensions between ethical commitments and profit motives. However, our study finds that even in highly uncertain environments, absent guidance from leadership, product managers can "recouple" ethical commitments and practices by finding responsibility "micro-moments". Product managers seek out low-risk, small-scale actions they can take without explicit buy-in from higher-level managers, such as individual or team-wide checks and reviews and safeguarding standards for data. Our research highlights how genAI poses unique challenges to organizations trying to couple ethical principles and daily practices and the role that middle-level management can play in recoupling the two.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16531v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Genevieve Smith, Natalia Luka, Merrick Osborne, Brian Lattimore, Jessica Newman, Brandie Nonnecke, Brent Mittelstadt</dc:creator>
    </item>
    <item>
      <title>From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate</title>
      <link>https://arxiv.org/abs/2501.16548</link>
      <description>arXiv:2501.16548v1 Announce Type: new 
Abstract: As the climate crisis deepens, artificial intelligence (AI) has emerged as a contested force: some champion its potential to advance renewable energy, materials discovery, and large-scale emissions monitoring, while others underscore its growing carbon footprint, water consumption, and material resource demands. Much of this debate has concentrated on direct impact -- energy and water usage in data centers, e-waste from frequent hardware upgrades -- without addressing the significant indirect effects. This paper examines how the problem of Jevons' Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption. We argue that understanding these second-order impacts requires an interdisciplinary approach, combining lifecycle assessments with socio-economic analyses. Rebound effects undermine the assumption that improved technical efficiency alone will ensure net reductions in environmental harm. Instead, the trajectory of AI's impact also hinges on business incentives and market logics, governance and policymaking, and broader social and cultural norms. We contend that a narrow focus on direct emissions misrepresents AI's true climate footprint, limiting the scope for meaningful interventions. We conclude with recommendations that address rebound effects and challenge the market-driven imperatives fueling uncontrolled AI growth. By broadening the analysis to include both direct and indirect consequences, we aim to inform a more comprehensive, evidence-based dialogue on AI's role in the climate crisis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16548v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alexandra Sasha Luccioni, Emma Strubell, Kate Crawford</dc:creator>
    </item>
    <item>
      <title>Reconciling Predictive Multiplicity in Practice</title>
      <link>https://arxiv.org/abs/2501.16549</link>
      <description>arXiv:2501.16549v1 Announce Type: new 
Abstract: Many machine learning applications predict individual probabilities, such as the likelihood that a person develops a particular illness. Since these probabilities are unknown, a key question is how to address situations in which different models trained on the same dataset produce varying predictions for certain individuals. This issue is exemplified by the model multiplicity (MM) phenomenon, where a set of comparable models yield inconsistent predictions. Roth, Tolbert, and Weinstein recently introduced a reconciliation procedure, the Reconcile algorithm, to address this problem. Given two disagreeing models, the algorithm leverages their disagreement to falsify and improve at least one of the models. In this paper, we empirically analyze the Reconcile algorithm using five widely-used fairness datasets: COMPAS, Communities and Crime, Adult, Statlog (German Credit Data), and the ACS Dataset. We examine how Reconcile fits within the model multiplicity literature and compare it to existing MM solutions, demonstrating its effectiveness. We also discuss potential improvements to the Reconcile algorithm theoretically and practically. Finally, we extend the Reconcile algorithm to the setting of causal inference, given that different competing estimators can again disagree on specific causal average treatment effect (CATE) values. We present the first extension of the Reconcile algorithm in causal inference, analyze its theoretical properties, and conduct empirical tests. Our results confirm the practical effectiveness of Reconcile and its applicability across various domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16549v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tina Behzad, S\'ilvia Casacuberta, Emily Ruth Diana, Alexander Williams Tolbert</dc:creator>
    </item>
    <item>
      <title>Enhancing Soft Skills in Network Management Education: A Study on the Impact of GenAI-based Virtual Assistants</title>
      <link>https://arxiv.org/abs/2501.16901</link>
      <description>arXiv:2501.16901v1 Announce Type: new 
Abstract: The rapid evolution of technology in educational settings has opened new avenues for enhancing learning experiences, particularly in specialized fields like network management. This paper explores the novel integration of a GenAI-based virtual assistant in a university-level network management course, focusing on its impact on developing students' soft skills, notably critical thinking and problem-solving abilities. Recognizing the increasing importance of these skills in the digital age, our study aims to assess the empirical effectiveness of this artificial intelligence-driven educational tool in fostering these competencies among students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16901v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/EDUCON60312.2024.10578597</arxiv:DOI>
      <dc:creator>Dimitris Pantazatos, Mary Grammatikou, Vasilis Maglaris</dc:creator>
    </item>
    <item>
      <title>Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development</title>
      <link>https://arxiv.org/abs/2501.16946</link>
      <description>arXiv:2501.16946v1 Announce Type: new 
Abstract: This paper examines the systemic risks posed by incremental advancements in artificial intelligence, developing the concept of `gradual disempowerment', in contrast to the abrupt takeover scenarios commonly discussed in AI safety. We analyze how even incremental improvements in AI capabilities can undermine human influence over large-scale systems that society depends on, including the economy, culture, and nation-states. As AI increasingly replaces human labor and cognition in these domains, it can weaken both explicit human control mechanisms (like voting and consumer choice) and the implicit alignments with human interests that often arise from societal systems' reliance on human participation to function. Furthermore, to the extent that these systems incentivise outcomes that do not line up with human preferences, AIs may optimize for those outcomes more aggressively. These effects may be mutually reinforcing across different domains: economic power shapes cultural narratives and political decisions, while cultural shifts alter economic and political behavior. We argue that this dynamic could lead to an effectively irreversible loss of human influence over crucial societal systems, precipitating an existential catastrophe through the permanent disempowerment of humanity. This suggests the need for both technical research and governance approaches that specifically address the risk of incremental erosion of human influence across interconnected societal systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16946v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Kulveit, Raymond Douglas, Nora Ammann, Deger Turan, David Krueger, David Duvenaud</dc:creator>
    </item>
    <item>
      <title>The Third Moment of AI Ethics: Developing Relatable and Contextualized Tools</title>
      <link>https://arxiv.org/abs/2501.16954</link>
      <description>arXiv:2501.16954v1 Announce Type: new 
Abstract: Artificial intelligence (AI) ethics has gained significant momentum, evidenced by the growing body of published literature, policy guidelines, and public discourse. However, the practical implementation and adoption of AI ethics principles among practitioners has not kept pace with this theoretical development. Common barriers to adoption include overly abstract language, poor accessibility, and insufficient practical guidance for implementation. Through participatory design with industry practitioners, we developed an open-source tool that bridges this gap. Our tool is firmly grounded in normative ethical frameworks while offering concrete, actionable guidance in an intuitive format that aligns with established software development workflows. We validated this approach through a proof of concept study in the United States autonomous driving industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16954v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah Hladikova, Yuling Wang, Andreia Martinho</dc:creator>
    </item>
    <item>
      <title>Standardised schema and taxonomy for AI incident databases in critical digital infrastructure</title>
      <link>https://arxiv.org/abs/2501.17037</link>
      <description>arXiv:2501.17037v1 Announce Type: new 
Abstract: The rapid deployment of Artificial Intelligence (AI) in critical digital infrastructure introduces significant risks, necessitating a robust framework for systematically collecting AI incident data to prevent future incidents. Existing databases lack the granularity as well as the standardized structure required for consistent data collection and analysis, impeding effective incident management. This work proposes a standardized schema and taxonomy for AI incident databases, addressing these challenges by enabling detailed and structured documentation of AI incidents across sectors. Key contributions include developing a unified schema, introducing new fields such as incident severity, causes, and harms caused, and proposing a taxonomy for classifying AI incidents in critical digital infrastructure. The proposed solution facilitates more effective incident data collection and analysis, thus supporting evidence-based policymaking, enhancing industry safety measures, and promoting transparency. This work lays the foundation for a coordinated global response to AI incidents, ensuring trust, safety, and accountability in using AI across regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17037v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avinash Agarwal, Manisha J. Nene</dc:creator>
    </item>
    <item>
      <title>Non-Western Perspectives on Web Inclusivity: A Study of Accessibility Practices in the Global South</title>
      <link>https://arxiv.org/abs/2501.16601</link>
      <description>arXiv:2501.16601v1 Announce Type: cross 
Abstract: The Global South faces unique challenges in achieving digital inclusion due to a heavy reliance on mobile devices for internet access and the prevalence of slow or unreliable networks. While numerous studies have investigated web accessibility within specific sectors such as education, healthcare, and government services, these efforts have been largely constrained to individual countries or narrow contexts, leaving a critical gap in cross-regional, large-scale analysis. This paper addresses this gap by conducting the first large-scale comparative study of mobile web accessibility across the Global South. In this work, we evaluate 100,000 websites from 10 countries in the Global South to provide a comprehensive understanding of accessibility practices in these regions. Our findings reveal that websites from countries with strict accessibility regulations and enforcement tend to adhere better to Web Content Accessibility Guidelines (WCAG) guidelines. However, accessibility violations impact different disability groups in varying ways. Blind and low-vision individuals in the Global South are disproportionately affected, as only 40% of the evaluated websites meet critical accessibility guidelines. This significant shortfall is largely due to developers frequently neglecting to implement valid alt text for images and ARIA descriptions, which are essential specification mechanisms in the HTML standard for the effective operation of screen readers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16601v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masudul Hasan Masud Bhuiyan, Matteo Varvello, Cristian-Alexandru Staicu, Yasir Zaki</dc:creator>
    </item>
    <item>
      <title>Statistical Analysis of Risk Assessment Factors and Metrics to Evaluate Radicalisation in Twitter</title>
      <link>https://arxiv.org/abs/2501.16830</link>
      <description>arXiv:2501.16830v1 Announce Type: cross 
Abstract: Nowadays, Social Networks have become an essential communication tools producing a large amount of information about their users and their interactions, which can be analysed with Data Mining methods. In the last years, Social Networks are being used to radicalise people. In this paper, we study the performance of a set of indicators and their respective metrics, devoted to assess the risk of radicalisation of a precise individual on three different datasets. Keyword-based metrics, even though depending on the written language, performs well when measuring frustration, perception of discrimination as well as declaration of negative and positive ideas about Western society and Jihadism, respectively. However, metrics based on frequent habits such as writing ellipses are not well enough to characterise a user in risk of radicalisation. The paper presents a detailed description of both, the set of indicators used to asses the radicalisation in Social Networks and the set of datasets used to evaluate them. Finally, an experimental study over these datasets are carried out to evaluate the performance of the metrics considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16830v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.future.2017.10.046</arxiv:DOI>
      <arxiv:journal_reference>Future Generation Computer Systems, 93, 971-978 (2019)</arxiv:journal_reference>
      <dc:creator>Raul Lara-Cabrera, Antonio Gonzalez-Pardo, David Camacho</dc:creator>
    </item>
    <item>
      <title>"My Whereabouts, my Location, it's Directly Linked to my Physical Security": An Exploratory Qualitative Study of Location-Dependent Security and Privacy Perceptions among Activist Tech Users</title>
      <link>https://arxiv.org/abs/2501.16885</link>
      <description>arXiv:2501.16885v1 Announce Type: cross 
Abstract: Digital-safety research with at-risk users is particularly urgent. At-risk users are more likely to be digitally attacked or targeted by surveillance and could be disproportionately harmed by attacks that facilitate physical assaults. One group of such at-risk users are activists and politically active individuals. For them, as for other at-risk users, the rise of smart environments harbors new risks. Since digitization and datafication are no longer limited to a series of personal devices that can be switched on and off, but increasingly and continuously surround users, granular geolocation poses new safety challenges. Drawing on eight exploratory qualitative interviews of an ongoing research project, this contribution highlights what activists with powerful adversaries think about evermore data traces, including location data, and how they intend to deal with emerging risks. Responses of activists include attempts to control one's immediate technological surroundings and to more carefully manage device-related location data. For some activists, threat modeling has also shaped provider choices based on geopolitical considerations. Since many activists have not enough digital-safety knowledge for effective protection, feelings of insecurity and paranoia are widespread. Channeling the concerns and fears of our interlocutors, we call for more research on how activists can protect themselves against evermore fine-grained location data tracking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16885v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Eichenm\"uller, Lisa Kuhn, Zinaida Benenson</dc:creator>
    </item>
    <item>
      <title>Cultural Differences and Perverse Incentives in Science Create a Bad Mix: Exploring Country-Level Publication Bias in Select ACM Conferences</title>
      <link>https://arxiv.org/abs/2501.17150</link>
      <description>arXiv:2501.17150v1 Announce Type: cross 
Abstract: In the era of big science, many national governments are helping to build well-funded teams of scientists to serve nationalistic ambitions, providing financial incentives for certain outcomes for purposes other than advancing science. This in turn can impact the behavior of scientists and create distorted country-level bias in publication rates, frequency, and publication venues targeted. To that end, we have found evidence that indicates significant inequality among the publication rates of individual scientists from various countries, based on an intensive analysis of papers published in several well-known ACM conferences (HRI, IUI, KDD, CHI, SIGGRAPH, UIST, and UBICOMP) over 15 years between 2010 to 2024. Furthermore, scientists who were affiliated with the top-5 countries (in terms of research expenditure) were found to be contributing significantly more to the inequality in publication rates than others. Given evidence of certain countries aggressively pushing their scientists via $\textit{perverse incentives}$ to publish in well-regarded publication venues and produce significant results (by any means necessary), we detected and present several examples of potential ethical problems in publications caused by such systems. Additionally, topic modeling using LDA and semantic similarity revealed that some countries are not pursuing diverse scientific topics relative to others, indicating those incentives may be limiting genuine scientific curiosity. All in all, our findings raise awareness of systems put in place by certain national governments that not only erodes the pursuit of truth through science, but also appears to be gradually undermining the integrity of the global scientific community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17150v1</guid>
      <category>cs.DL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aksheytha Chelikavada, Casey C. Bennett</dc:creator>
    </item>
    <item>
      <title>AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?</title>
      <link>https://arxiv.org/abs/2312.10833</link>
      <description>arXiv:2312.10833v4 Announce Type: replace 
Abstract: This study delves into the pervasive issue of gender issues in artificial intelligence (AI), specifically within automatic scoring systems for student-written responses. The primary objective is to investigate the presence of gender biases, disparities, and fairness in generally targeted training samples with mixed-gender datasets in AI scoring outcomes. Utilizing a fine-tuned version of BERT and GPT-3.5, this research analyzes more than 1000 human-graded student responses from male and female participants across six assessment items. The study employs three distinct techniques for bias analysis: Scoring accuracy difference to evaluate bias, mean score gaps by gender (MSG) to evaluate disparity, and Equalized Odds (EO) to evaluate fairness. The results indicate that scoring accuracy for mixed-trained models shows an insignificant difference from either male- or female-trained models, suggesting no significant scoring bias. Consistently with both BERT and GPT-3.5, we found that mixed-trained models generated fewer MSG and non-disparate predictions compared to humans. In contrast, compared to humans, gender-specifically trained models yielded larger MSG, indicating that unbalanced training data may create algorithmic models to enlarge gender disparities. The EO analysis suggests that mixed-trained models generated more fairness outcomes compared with gender-specifically trained models. Collectively, the findings suggest that gender-unbalanced data do not necessarily generate scoring bias but can enlarge gender disparities and reduce scoring fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10833v4</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsan Latif, Xiaoming Zhai, Lei Liu</dc:creator>
    </item>
    <item>
      <title>Intelligent Tutors for Adult Learners: An Analysis of Needs and Challenges</title>
      <link>https://arxiv.org/abs/2412.04477</link>
      <description>arXiv:2412.04477v2 Announce Type: replace 
Abstract: This research examines the sociotechnical factors that influence the adoption and usage of intelligent tutoring systems in self-directed learning contexts, focusing specifically on adult learners. The study is divided into two parts. First, we present Apprentice Tutors, a novel intelligent tutoring system designed to address the unique needs of adult learners. The platform includes adaptive problem selection, real-time feedback, and visual dashboards to support learning in college algebra topics. Second, we investigate the specific needs and experiences of adult users through a deployment study and a series of focus groups. Using thematic analysis, we identify key challenges and opportunities for improving tutor design and adoption. Based on these findings, we offer actionable design recommendations to help developers create intelligent tutoring systems that better align with the motivations and learning preferences of adult learners. This work contributes to the broader understanding of how to enhance educational technologies to support lifelong learning and professional development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04477v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adit Gupta, Momin Siddiqui, Glen Smith, Jenn Reddig, Christopher MacLellan</dc:creator>
    </item>
    <item>
      <title>Measuring NIST Authentication Standards Compliance by Higher Education Institutions</title>
      <link>https://arxiv.org/abs/2409.00546</link>
      <description>arXiv:2409.00546v2 Announce Type: replace-cross 
Abstract: Technical standards are a longstanding method of communicating best practice recommendations based on expert consensus. Cybersecurity standards are particularly important for informing practices that protect critical systems and sensitive data. Measuring standards compliance is therefore essential to identify vulnerabilities arising from continued use of outdated practices and to determine whether expert advice has effectively diffused to practitioners. In this paper, we examine the authentication practices of a diverse set of 136 colleges and universities in the United States and Canada to determine compliance with four standards from NIST Special Publication 800-63-3 Digital Identity Guidelines. These standards have been in place since 2017, pose a relatively low barrier to implementation, yet are substantive revisions from pre-2017 versions, making them an excellent case study for measuring the responsiveness of institutions to updated expert guidance. We find widespread, but not universal, compliance with multi-factor authentication (MFA) standards across institutions. We also find widespread noncompliance with standards for password expiration, password composition rules, and knowledge-based authentication. These results are a wake-up call that many expert cybersecurity recommendations are not effectively reaching practitioners, suggesting a need for alternative outreach strategies, increased investment in education and training initiatives, and an examination of incentive structures that result in noncompliant and insecure practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00546v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noah Apthorpe, Boen Beavers, Yan Shvartzshnaider, Brett Frischmann</dc:creator>
    </item>
    <item>
      <title>Pseudo-Automation: How Labor-Offsetting Technologies Reconfigure Roles and Relationships in Frontline Retail Work</title>
      <link>https://arxiv.org/abs/2410.02888</link>
      <description>arXiv:2410.02888v4 Announce Type: replace-cross 
Abstract: Self-service machines are a form of pseudo-automation; rather than actually automate tasks, they offset them to unpaid customers. Typically implemented for customer convenience and to reduce labor costs, self-service is often criticized for worsening customer service and increasing loss and theft for retailers. Though millions of frontline service workers continue to interact with these technologies on a day-to-day basis, little is known about how these machines change the nature of frontline labor. Through interviews with current and former cashiers who work with self-checkout technologies, we investigate how technology that offsets labor from an employee to a customer can reconfigure frontline work. We find three changes to cashiering tasks as a result of self-checkout: (1) Working at self-checkout involved parallel demands from multiple customers, (2) self-checkout work was more problem-oriented (including monitoring and policing customers), and (3) traditional checkout began to become more demanding as easier transactions were filtered to self-checkout. As their interactions with customers became more focused on problem solving and rule enforcement, cashiers were often positioned as adversaries to customers at self-checkout. To cope with perceived adversarialism, cashiers engaged in a form of relational patchwork, using techniques like scapegoating the self-checkout machine and providing excessive customer service in order to maintain positive customer interactions in the face of potential conflict. Our findings highlight how even under pseudo-automation, workers must engage in relational work to manage and mend negative human-to-human interactions so that machines can be properly implemented in context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02888v4</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3711051</arxiv:DOI>
      <dc:creator>Pegah Moradi, Karen Levy, Cristobal Cheyre</dc:creator>
    </item>
    <item>
      <title>Cyber Shadows: Neutralizing Security Threats with AI and Targeted Policy Measures</title>
      <link>https://arxiv.org/abs/2501.09025</link>
      <description>arXiv:2501.09025v2 Announce Type: replace-cross 
Abstract: The digital age, driven by the AI revolution, brings significant opportunities but also conceals security threats, which we refer to as cyber shadows. These threats pose risks at individual, organizational, and societal levels. This paper examines the systemic impact of these cyber threats and proposes a comprehensive cybersecurity strategy that integrates AI-driven solutions, such as Intrusion Detection Systems (IDS), with targeted policy interventions. By combining technological and regulatory measures, we create a multilevel defense capable of addressing both direct threats and indirect negative externalities. We emphasize that the synergy between AI-driven solutions and policy interventions is essential for neutralizing cyber threats and mitigating their negative impact on the digital economy. Finally, we underscore the need for continuous adaptation of these strategies, especially in response to the rapid advancement of autonomous AI-driven attacks, to ensure the creation of secure and resilient digital ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09025v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TAI.2025.3527398</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Artificial Intelligence (2025)</arxiv:journal_reference>
      <dc:creator>Marc Schmitt, Pantelis Koutroumpis</dc:creator>
    </item>
  </channel>
</rss>
