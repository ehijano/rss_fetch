<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 Jan 2026 05:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Particle Filtering for a Class of State-Space Models with Low and Degenerate Observational Noise</title>
      <link>https://arxiv.org/abs/2601.08411</link>
      <description>arXiv:2601.08411v1 Announce Type: new 
Abstract: We consider the discrete-time filtering problem in scenarios where the observation noise is degenerate or low. We focus on the case where the observation equation is a linear function of the state and that additive noise is low or degenerate, however, we place minimal assumptions on the hidden state process. In this scenario we derive new particle filtering (PF) algorithms and, under assumptions, in such a way that as the noise becomes more degenerate a PF which approximates the low noise filtering problem provably inherits the properties of the PF used in the degenerate case. We extend our framework to the case where the hidden states are drawn from a diffusion process. In this scenario we develop new PFs which are robust to both low noise and fine levels of time discretization. We illustrate our algorithms numerically on several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08411v1</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abylay Zhumekenov, Alexandros Beskos, Dan Crisan, Ajay Jasra, Nikolas Kantas</dc:creator>
    </item>
    <item>
      <title>A Statistical Assessment of Amortized Inference Under Signal-to-Noise Variation and Distribution Shift</title>
      <link>https://arxiv.org/abs/2601.07944</link>
      <description>arXiv:2601.07944v1 Announce Type: cross 
Abstract: Since the turn of the century, approximate Bayesian inference has steadily evolved as new computational techniques have been incorporated to handle increasingly complex and large-scale predictive problems. The recent success of deep neural networks and foundation models has now given rise to a new paradigm in statistical modeling, in which Bayesian inference can be amortized through large-scale learned predictors. In amortized inference, substantial computation is invested upfront to train a neural network that can subsequently produce approximate posterior or predictions at negligible marginal cost across a wide range of tasks. At deployment, amortized inference offers substantial computational savings compared with traditional Bayesian procedures, which generally require repeated likelihood evaluations or Monte Carlo simulations for predictions for each new dataset.
  Despite the growing popularity of amortized inference, its statistical interpretation and its role within Bayesian inference remain poorly understood. This paper presents statistical perspectives on the working principles of several major neural architectures, including feedforward networks, Deep Sets, and Transformers, and examines how these architectures naturally support amortized Bayesian inference. We discuss how these models perform structured approximation and probabilistic reasoning in ways that yield controlled generalization error across a wide range of deployment scenarios, and how these properties can be harnessed for Bayesian computation. Through simulation studies, we evaluate the accuracy, robustness, and uncertainty quantification of amortized inference under varying signal-to-noise ratios and distributional shifts, highlighting both its strengths and its limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07944v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roy Shivam Ram Shreshtth, Arnab Hazra, Gourab Mukherjee</dc:creator>
    </item>
    <item>
      <title>Likelihood ratio for a binary Bayesian classifier under a noise-exclusion model</title>
      <link>https://arxiv.org/abs/2601.07982</link>
      <description>arXiv:2601.07982v1 Announce Type: cross 
Abstract: We develop a new statistical ideal observer model that performs holistic visual search (or gist) processing in part by placing thresholds on minimum extractable image features. In this model, the ideal observer reduces the number of free parameters thereby shrinking down the system. The applications of this novel framework is in medical image perception (for optimizing imaging systems and algorithms), computer vision, benchmarking performance and enabling feature selection/evaluations. Other applications are in target detection and recognition in defense/security as well as evaluating sensors and detectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07982v1</guid>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Howard C. Gifford</dc:creator>
    </item>
    <item>
      <title>Closed-form approximations of the two-sample Pearson Bayes factor</title>
      <link>https://arxiv.org/abs/2310.11313</link>
      <description>arXiv:2310.11313v4 Announce Type: replace 
Abstract: In this paper, I present three closed-form approximations of the two-sample Pearson Bayes factor, a recently developed index of evidential value for data in two-group designs. The techniques rely on some classical asymptotic results about Gamma functions. These approximations permit simple closed-form calculation of the Pearson Bayes factor in cases where only minimal summary statistics are available (i.e., the t-score and degrees of freedom). Moreover, these approximations vastly outperform the classic BIC method for approximating Bayes factors from experimental designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11313v4</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas J. Faulkenberry</dc:creator>
    </item>
    <item>
      <title>Modified Delayed Acceptance MCMC for Quasi-Bayesian Inference with Linear Moment Conditions</title>
      <link>https://arxiv.org/abs/2511.17117</link>
      <description>arXiv:2511.17117v3 Announce Type: replace 
Abstract: We develop a computationally efficient framework for quasi-Bayesian inference based on linear moment conditions. The approach employs a delayed acceptance Markov chain Monte Carlo (DA-MCMC) algorithm that uses a surrogate target kernel and a proposal distribution derived from an approximate conditional posterior, thereby exploiting the structure of the quasi-likelihood. Two implementations are introduced. DA-MCMC-Exact fully incorporates prior information into the proposal distribution and maximizes per-iteration efficiency, whereas DA-MCMC-Approx omits the prior in the proposal to reduce matrix inversions, improving numerical stability and computational speed in higher dimensions. Simulation studies on heteroskedastic linear regressions show substantial gains over standard MCMC and conventional DA-MCMC baselines, measured by multivariate effective sample size per iteration and per second. The Approx variant yields the best overall throughput, while the Exact variant attains the highest per-iteration efficiency. Applications to two empirical instrumental variable regressions corroborate these findings: the Approx implementation scales to larger designs where other methods become impractical, while still delivering precise inference. Although developed for moment-based quasi-posteriors, the proposed approach also extends to risk-based quasi-Bayesian formulations when first-order conditions are linear and can be transformed analogously. Overall, the proposed algorithms provide a practical and robust tool for quasi-Bayesian analysis in statistical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17117v3</guid>
      <category>stat.CO</category>
      <category>econ.EM</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Tanaka</dc:creator>
    </item>
    <item>
      <title>tidychangepoint: A Unified Framework for Analyzing Changepoint Detection in Univariate Time Series</title>
      <link>https://arxiv.org/abs/2407.14369</link>
      <description>arXiv:2407.14369v3 Announce Type: replace-cross 
Abstract: We present tidychangepoint, a new R package for changepoint detection analysis. Most R packages for segmenting univariate time series focus on providing one or two algorithms for changepoint detection that work with a small set of models and penalized objective functions, and all of them return a custom, nonstandard object type. This makes comparing results across various algorithms, models, and penalized objective functions unnecessarily difficult. tidychangepoint solves this problem by wrapping functions from a variety of existing packages and storing the results in a common S3 class called tidycpt. The package then provides functionality for easily extracting comparable numeric or graphical information from a tidycpt object, all in a tidyverse-compliant framework. tidychangepoint is versatile: it supports both deterministic algorithms like PELT (from changepoint), and also flexible, randomized, genetic algorithms (via GA) that -- via new functionality built into tidychangepoint -- can be used with any compliant model-fitting function and any penalized objective function. By bringing all of these disparate tools together in a cohesive fashion, tidychangepoint facilitates comparative analysis of changepoint detection algorithms and models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14369v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin S. Baumer, Biviana Marcela Suarez Sierra</dc:creator>
    </item>
    <item>
      <title>Reliability-Targeted Simulation of Item Response Data: Solving the Inverse Design Problem</title>
      <link>https://arxiv.org/abs/2512.16012</link>
      <description>arXiv:2512.16012v2 Announce Type: replace-cross 
Abstract: Monte Carlo simulations are the primary methodology for evaluating Item Response Theory (IRT) methods, yet marginal reliability - the fundamental metric of data informativeness - is rarely treated as an explicit design factor. Unlike in multilevel modeling where the intraclass correlation (ICC) is routinely manipulated, IRT studies typically treat reliability as an incidental outcome, creating a "reliability omission" that obscures the signal-to-noise ratio of generated data. To address this gap, we introduce a principled framework for reliability-targeted simulation, transforming reliability from an implicit by-product into a precise input parameter. We formalize the inverse design problem, solving for a global discrimination scaling factor that uniquely achieves a pre-specified target reliability. Two complementary algorithms are proposed: Empirical Quadrature Calibration (EQC) for rapid, deterministic precision, and Stochastic Approximation Calibration (SAC) for rigorous stochastic estimation. A comprehensive validation study across 960 conditions demonstrates that EQC achieves essentially exact calibration, while SAC remains unbiased across non-normal latent distributions and empirical item pools. Furthermore, we clarify the theoretical distinction between average-information and error-variance-based reliability metrics, showing they require different calibration scales due to Jensen's inequality. An accompanying open-source R package, IRTsimrel, enables researchers to standardize reliability as a controlled experimental input.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16012v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>JoonHo Lee</dc:creator>
    </item>
  </channel>
</rss>
