<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Dec 2025 05:00:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A unified framework for equation discovery and dynamic prediction of hysteretic systems</title>
      <link>https://arxiv.org/abs/2512.02408</link>
      <description>arXiv:2512.02408v1 Announce Type: new 
Abstract: Hysteresis is a nonlinear phenomenon with memory effects, where a system's output depends on both its current state and past states. It is prevalent in various physical and mechanical systems, such as yielding structures under seismic excitation, ferromagnetic materials, and piezoelectric actuators. Analytical models like the Bouc-Wen model are often employed but rely on idealized assumptions and careful parameter calibration, limiting their applicability to diverse or mechanism-unknown behaviors. Existing equation discovery approaches for hysteresis are often system-specific or rely on predefined model libraries, which limit their flexibility and ability to capture the hidden mechanisms. To address these, this research develops a unified framework that integrates learning of internal variables (commonly used in modeling hysteresis) and symbolic regression to automatically extract internal hysteretic variable, and discover explicit governing equations directly from data without predefined libraries as required by methods such as sparse identification of nonlinear dynamics (SINDy). Solving the discovered equations naturally enables prediction of the dynamic responses of hysteretic systems. This work provides a systematic view and approach for both equation discovery and characterization of hysteretic dynamics, defining a unified framework for these types of problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02408v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyuan Yang, Wei Liu, Zhilu Lai</dc:creator>
    </item>
    <item>
      <title>The Invisible Hand: Characterizing Generative AI Adoption and its Effects on An Online Freelancing Market</title>
      <link>https://arxiv.org/abs/2512.02509</link>
      <description>arXiv:2512.02509v1 Announce Type: new 
Abstract: Since the COVID-19 pandemic, freelancing platforms have experienced significant growth in both worker registrations and job postings. However, the rise of generative AI (GenAI) technologies has raised questions about how it affect the job posting in freelancer market. Despite growing discussions, there is limited empirical research on the GenAI adoption and its effect on job demand and worker engagement. We present a large-scale analysis of Freelancer.com, utilizing over 1.8 million job posts and 3.8 million users. We investigate the emergence of jobs with the adoption of GenAI and identify leading position of ChatGPT in the freelancing market. With a focus on ChatGPT related jobs, we inspect their specific skill requirements, and the tasks that workers are asked to perform. Our findings provide insights into the evolving landscape of freelancing in the age of AI, offering a comprehensive profile of GenAI's effects on employment, skills, and user behaviors in freelancing market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02509v1</guid>
      <category>cs.CE</category>
      <category>cs.SI</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yiming Zhu, Gareth Tyson, Pan Hui</dc:creator>
    </item>
    <item>
      <title>Sparse Computations in Deep Learning Inference</title>
      <link>https://arxiv.org/abs/2512.02550</link>
      <description>arXiv:2512.02550v1 Announce Type: new 
Abstract: The computational demands of modern Deep Neural Networks (DNNs) are immense and constantly growing. While training costs usually capture public attention, inference demands are also contributing in significant computational, energy and environmental footprints. Sparsity stands out as a critical mechanism for drastically reducing these resource demands. However, its potential remains largely untapped and is not yet fully incorporated in production AI systems. To bridge this gap, this work provides the necessary knowledge and insights for performance engineers keen to get involved in deep learning inference optimization. In particular, in this work we: a) discuss the various forms of sparsity that can be utilized in DNN inference, b) explain how the original dense computations translate to sparse kernels, c) provide an extensive bibliographic review of the state-of-the-art in the implementation of these kernels for CPUs and GPUs, d) discuss the availability of sparse datasets in support of sparsity-related research and development, e) explore the current software tools and frameworks that provide robust sparsity support, and f) present evaluation results of different implementations of the key SpMM and SDDMM kernels on CPU and GPU platforms. Ultimately, this paper aims to serve as a resource for performance engineers seeking to develop and deploy highly efficient sparse deep learning models in productions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02550v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ioanna Tasou, Panagiotis Mpakos, Angelos Vlachos, Dionysios Adamopoulos, Georgios Giannakopoulos, Konstantinos Katsikopoulos, Ioannis Karaparisis, Maria Lazou, Spyridon Loukovitis, Areti Mei, Anastasia Poulopoulou, Angeliki Dimitriou, Giorgos Filandrianos, Dimitrios Galanopoulos, Vasileios Karampinis, Ilias Mitsouras, Nikolaos Spanos, Petros Anastasiadis, Ioannis Doudalis, Konstantinos Nikas, George Retsinas, Paraskevi Tzouveli, Christina Giannoula, Nectarios Koziris, Nikela Papadopoulou, Giorgos Stamou, Athanasios Voulodimos, Georgios Goumas</dc:creator>
    </item>
    <item>
      <title>Beyond N-grams: A Hierarchical Reward Learning Framework for Clinically-Aware Medical Report Generation</title>
      <link>https://arxiv.org/abs/2512.02710</link>
      <description>arXiv:2512.02710v1 Announce Type: new 
Abstract: Automatic medical report generation can greatly reduce the workload of doctors, but it is often unreliable for real-world deployment. Current methods can write formally fluent sentences but may be factually flawed, introducing serious medical errors known as clinical hallucinations, which make them untrustworthy for diagnosis. To bridge this gap, we introduce HiMed-RL, a Hierarchical Medical Reward Learning Framework designed to explicitly prioritize clinical quality. HiMed-RL moves beyond simple text matching by deconstructing reward learning into three synergistic levels: it first ensures linguistic fluency at the token-level, then enforces factual grounding at the concept-level by aligning key medical terms with expert knowledge, and finally assesses high-level diagnostic consistency at the semantic-level using a specialized LLM verifier. This hierarchical reward is implemented via a Human-inspired Dynamic Reward Adjustment, a strategy which first teaches the model to learn basic facts before progressing to more complex diagnostic reasoning. Experimentally, HiMed-3B achieves state-of-the-art performance on both in-domain and out-of-domain benchmarks, particularly on the latter, with an improvement of 12.1% over the second-best baseline. Our work provides a robust paradigm for generating reports that not only improve fluency but clinical fine-grained quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02710v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Wang, Shujian Gao, Jiaxiang Liu, Songtao Jiang, Haoxiang Xia, Xiaotian Zhang, Zhaolu Kang, Yemin Wang, Zuozhu Liu</dc:creator>
    </item>
    <item>
      <title>Ada-MoGE: Adaptive Mixture of Gaussian Expert Model for Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2512.02061</link>
      <description>arXiv:2512.02061v1 Announce Type: cross 
Abstract: Multivariate time series forecasts are widely used, such as industrial, transportation and financial forecasts. However, the dominant frequencies in time series may shift with the evolving spectral distribution of the data. Traditional Mixture of Experts (MoE) models, which employ a fixed number of experts, struggle to adapt to these changes, resulting in frequency coverage imbalance issue. Specifically, too few experts can lead to the overlooking of critical information, while too many can introduce noise. To this end, we propose Ada-MoGE, an adaptive Gaussian Mixture of Experts model. Ada-MoGE integrates spectral intensity and frequency response to adaptively determine the number of experts, ensuring alignment with the input data's frequency distribution. This approach prevents both information loss due to an insufficient number of experts and noise contamination from an excess of experts. Additionally, to prevent noise introduction from direct band truncation, we employ Gaussian band-pass filtering to smoothly decompose the frequency domain features, further optimizing the feature representation. The experimental results show that our model achieves state-of-the-art performance on six public benchmarks with only 0.2 million parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02061v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenliang Ni, Xiaowen Ma, Zhenkai Wu, Shuai Xiao, Han Shu, Xinghao Chen</dc:creator>
    </item>
    <item>
      <title>Scalable, Cloud-Based Simulations of Blood Flow and Targeted Drug Delivery in Retinal Capillaries</title>
      <link>https://arxiv.org/abs/2512.02090</link>
      <description>arXiv:2512.02090v1 Announce Type: cross 
Abstract: We investigate the capabilities of cloud computing for large-scale,tightly-coupled simulations of biological fluids in complex geometries, traditionally performed in supercomputing centers. We demonstrate scalable and efficient simulations in the public cloud. We perform meso-scale simulations of blood flow in image-reconstructed capillaries, and examine targeted drug delivery by artificial bacterial flagella (ABFs). The simulations deploy dissipative particle dynamics (DPD) with two software frameworks, Mirheo (developed by our team) and LAMMPS. Mirheo exhibits remarkable weak scalability for up to 512 GPUs. Similarly, LAMMPS demonstrated excellent weak scalability for pure solvent as well as for blood suspensions and ABFs in reconstructed retinal capillaries. In particular, LAMMPS maintained weak scaling above 90% on the cloud for up to 2,000 cores. Our findings demonstrate that cloud computing can support tightly coupled, large-scale scientific simulations with competitive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02090v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.CE</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lucas Amoudruz, Sergey Litvinov, Riccardo Murri, Volker Eyrich, Jens Zudrop, Costas Bekas, Petros Koumoutsakos</dc:creator>
    </item>
    <item>
      <title>Orchestration Framework for Financial Agents: From Algorithmic Trading to Agentic Trading</title>
      <link>https://arxiv.org/abs/2512.02227</link>
      <description>arXiv:2512.02227v1 Announce Type: cross 
Abstract: The financial market is a mission-critical playground for AI agents due to its temporal dynamics and low signal-to-noise ratio. Building an effective algorithmic trading system may require a professional team to develop and test over the years. In this paper, we propose an orchestration framework for financial agents, which aims to democratize financial intelligence to the general public. We map each component of the traditional algorithmic trading system to agents, including planner, orchestrator, alpha agents, risk agents, portfolio agents, backtest agents, execution agents, audit agents, and memory agent. We present two in-house trading examples. For the stock trading task (hourly data from 04/2024 to 12/2024), our approach achieved a return of $20.42\%$, a Sharpe ratio of 2.63, and a maximum drawdown of $-3.59\%$, while the S&amp;P 500 index yielded a return of $15.97\%$. For the BTC trading task (minute data from 27/07/2025 to 13/08/2025), our approach achieved a return of $8.39\%$, a Sharpe ratio of $0.38$, and a maximum drawdown of $-2.80\%$, whereas the BTC price increased by $3.80\%$. Our code is available on \href{https://github.com/Open-Finance-Lab/AgenticTrading}{GitHub}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02227v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jifeng Li, Arnav Grover, Abraham Alpuerto, Yupeng Cao, Xiao-Yang Liu</dc:creator>
    </item>
    <item>
      <title>Mechanistic Modeling of Continuous Lyophilization for Biopharmaceutical Manufacturing</title>
      <link>https://arxiv.org/abs/2409.06251</link>
      <description>arXiv:2409.06251v3 Announce Type: replace 
Abstract: Lyophilization (aka freeze drying) is a typical process in pharmaceutical manufacturing used for improving the stability of various drug products, including its recent applications to mRNA vaccines. While extensive efforts have been dedicated to shifting the pharmaceutical industry toward continuous manufacturing, the majority of industrial-scale lyophilization is still being operated in a batch mode. This article presents the first mechanistic model for a complete continuous lyophilization process, which comprehensively incorporates and describes key transport phenomena in all three steps of lyophilization, namely freezing, primary drying, and secondary drying. The proposed model considers the state-of-the-art lyophilization technology, in which vials are suspended and move continuously through the process. The validated model can accurately predict the evolution of critical process parameters, including the product temperature, ice/water fraction, sublimation front position, and concentration of bound water, for the entire process. Several applications related to model-based process design and optimization of continuous lyophilization are also demonstrated. The final model is made available in MATLAB and Julia as an open-source software package called ContLyo, which can ultimately be leveraged for guiding the design and development of future continuous lyophilization processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06251v3</guid>
      <category>cs.CE</category>
      <category>physics.bio-ph</category>
      <category>physics.chem-ph</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/advs.202511693</arxiv:DOI>
      <dc:creator>Prakitr Srisuma, Gang Chen, Richard D. Braatz</dc:creator>
    </item>
    <item>
      <title>Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms</title>
      <link>https://arxiv.org/abs/2510.23166</link>
      <description>arXiv:2510.23166v4 Announce Type: replace 
Abstract: Machine learning (ML) is transforming modeling and control in the physical, engineering, and biological sciences. However, rapid development has outpaced the creation of standardized, objective benchmarks - leading to weak baselines, reporting bias, and inconsistent evaluations across methods. This undermines reproducibility, misguides resource allocation, and obscures scientific progress. To address this, we propose a Common Task Framework (CTF) for scientific machine learning. The CTF features a curated set of datasets and task-specific metrics spanning forecasting, state reconstruction, and generalization under realistic constraints, including noise and limited data. Inspired by the success of CTFs in fields like natural language processing and computer vision, our framework provides a structured, rigorous foundation for head-to-head evaluation of diverse algorithms. As a first step, we benchmark methods on two canonical nonlinear systems: Kuramoto-Sivashinsky and Lorenz. These results illustrate the utility of the CTF in revealing method strengths, limitations, and suitability for specific classes of problems and diverse objectives. Next, we are launching a competition around a global real world sea surface temperature dataset with a true holdout dataset to foster community engagement. Our long-term vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets that raise the bar for rigor and reproducibility in scientific ML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23166v4</guid>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe Martin Wyder, Judah Goldfeder, Alexey Yermakov, Yue Zhao, Stefano Riva, Jan P. Williams, David Zoro, Amy Sara Rude, Matteo Tomasetto, Joe Germany, Joseph Bakarji, Georg Maierhofer, Miles Cranmer, J. Nathan Kutz</dc:creator>
    </item>
    <item>
      <title>An Enhanced Whale Optimization Algorithm with Log-Normal Distribution for Optimizing Coverage of Wireless Sensor Networks</title>
      <link>https://arxiv.org/abs/2511.15970</link>
      <description>arXiv:2511.15970v2 Announce Type: replace 
Abstract: Wireless Sensor Networks (WSNs) are essential for monitoring and communication in complex environments, where coverage optimization directly affects performance and energy efficiency. However, traditional algorithms such as the Whale Optimization Algorithm (WOA) often suffer from limited exploration and premature convergence. To overcome these issues, this paper proposes an enhanced WOA which is called GLNWOA. GLNWOA integrates a log-normal distribution model into WOA to improve convergence dynamics and search diversity. GLNWOA employs a Good Nodes Set initialization for uniform population distribution, a Leader Cognitive Guidance Mechanism for efficient information sharing, and an Enhanced Spiral Updating Strategy to balance global exploration and local exploitation. Tests on benchmark functions verify its superior convergence accuracy and robustness. In WSN coverage optimization, deploying 25 nodes in a 60 m $\times$ 60 m area achieved a 99.0013\% coverage rate, outperforming AROA, WOA, HHO, ROA, and WOABAT by up to 15.5\%. These results demonstrate that GLNWOA offers fast convergence, high stability, and excellent optimization capability for intelligent network deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15970v2</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhao Wei, Yanzhao Gu, Ran Zhang, Yanxiao Li, Wenxuan Zhu, Jinhong Song, Yapeng Wang, Xu Yang, Ngai Cheong</dc:creator>
    </item>
    <item>
      <title>Scalable ADER-DG Transport Method with Polynomial Order Independent CFL Limit</title>
      <link>https://arxiv.org/abs/2507.07304</link>
      <description>arXiv:2507.07304v2 Announce Type: replace-cross 
Abstract: Discontinuous Galerkin (DG) methods are known to suffer from increasingly restrictive explicit time-step constraints as the polynomial order increases, limiting their efficiency at high orders for explicit time-stepping schemes. In this paper, we introduce a novel \emph{locally implicit}, but \emph{globally explicit} ADER-DG scheme designed for transport-dominated problems. The method achieves a maximum stable time step governed by an element-width based CFL condition that is independent of the polynomial degree. By solving a set of element-local implicit problems at each time step, our approach more effectively utilises the domain of dependence. As a result, our method remains stable for CFL numbers up to $\approx 1/\sqrt{d}$ in $d$ spatial dimensions. We provide a rigorous stability proof in one dimension, and extend the analysis to two and three dimensions using a semi-analytical von Neumann stability analysis. The accuracy and convergence of the method are demonstrated through numerical experiments for both linear and nonlinear test cases, including numerical simulations of a transport problem on a cubed sphere 2D manifold embedded in 3D.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07304v2</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>physics.ao-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kieran Ricardo, Kenneth Duru</dc:creator>
    </item>
  </channel>
</rss>
