<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Nov 2025 05:03:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Unified Spatiotemporal Framework for Modeling Censored and Missing Areal Responses</title>
      <link>https://arxiv.org/abs/2511.17725</link>
      <description>arXiv:2511.17725v1 Announce Type: new 
Abstract: We propose a new Bayesian approach for spatiotemporal areal data with censored and missing observations. The method introduces a flexible random effect that combines the spatial dependence structures of the Simultaneous Autoregressive (SAR) and Directed Acyclic Graph Autoregressive (DAGAR) models with a temporal autoregressive component. We demonstrate that this formulation extends both spatial models into a unified spatiotemporal framework, expressing them as Gaussian Markov random fields in their innovation form. The resulting model captures spatial, temporal, and joint spatiotemporal correlations in an interpretable way. Simulation studies show that the proposed model outperforms common ad hoc imputation strategies, such as replacing censored values with the limit of detection (LOD) or imputing missing data by the sample mean. We further apply the method to carbon monoxide (CO) concentration data from Beijing's air quality network, comparing the proposed DAGAR-AR model with the traditional Conditional Autoregressive (CAR) approach. The results indicate that while the CAR model achieves slightly better predictive performance, the DAGAR-AR specification offers clearer interpretability and a more coherent representation of the spatiotemporal dependence structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17725v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jose A. Ordo\~nez, Tsung-I Lin, Victor H. Lachos, Luis M. Castro</dc:creator>
    </item>
    <item>
      <title>Single Changepoint Procedures</title>
      <link>https://arxiv.org/abs/2511.17870</link>
      <description>arXiv:2511.17870v1 Announce Type: new 
Abstract: Single changepoint tests have become a staple check for homogeneity of a climate time series, suggesting how climate has changed should non-homogeneity be declared. This paper summarizes the most prominent single changepoint tests used in today's climate literature, relating them to one and other and unifying their presentations. Asymptotic quantiles for the individual tests are presented. Derivations of the quantiles are given, enabling the reader to tackle cases not considered within. Our work here studies both mean and trend shifts, covering the most common settings arising in climatology. SOI and global temperature series are analyzed within to illustrate the techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17870v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Robert Lund, Xueheng Shi</dc:creator>
    </item>
    <item>
      <title>Why Is the Double-Robust Estimator for Causal Inference Not Doubly Robust for Variance Estimation?</title>
      <link>https://arxiv.org/abs/2511.17907</link>
      <description>arXiv:2511.17907v1 Announce Type: new 
Abstract: Doubly robust estimators (DRE) are widely used in causal inference because they yield consistent estimators of average causal effect when at least one of the nuisance models, the propensity for treatment (exposure) or the outcome regression, is correct. However, double robustness does not extend to variance estimation; the influence-function (IF)-based variance estimator is consistent only when both nuisance parameters are correct. This raises concerns about applying DRE in practice, where model misspecification is inevitable. The recent paper by Shook-Sa et al. (2025, Biometrics, 81(2), ujaf054) demonstrated through Monte Carlo simulations that the IF-based variance estimator is biased. However, the paper's findings are empirical. The key question remains: why does the variance estimator fail in double robustness, and under what conditions do alternatives succeed, such as the ones demonstrated in Shook-Sa et al. 2025. In this paper, we develop a formal theory to clarify the efficiency properties of DRE that underlie these empirical findings. We also introduce alternative strategies, including a mixture-based framework underlying the sample-splitting and crossfitting approaches, to achieve valid inference with misspecified nuisance parameters. Our considerations are illustrated with simulation and real study data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17907v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Wu, Lucy Shao, Toni Gui, Tsungchin Wu, Zhuochao Huang, Shengjia Tu, Xin Tu, Jinyuan Liu, Tuo Lin</dc:creator>
    </item>
    <item>
      <title>The Asymptotic Distribution for a Single Joinpoint Changepoint Model</title>
      <link>https://arxiv.org/abs/2511.17942</link>
      <description>arXiv:2511.17942v1 Announce Type: new 
Abstract: A single joinpoint changepoint model partitions a time series into two segments, joined at the changepoint time by constraining the estimated piecewise linear regression responses to be continuous. This manuscript derives the exact asymptotic distribution of the changepoint existence test statistic gauging whether or not a second segment is necessary. The identified asymptotic distribution, a supremum of a Gaussian process over the unit interval, is rather unwieldy. The work presented here provides the result and its derivation; quantiles of the asymptotic distribution are presented for the user. This addresses a subtle gap in the changepoint literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17942v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xueheng Shi, Robert Lund</dc:creator>
    </item>
    <item>
      <title>Hierarchical biomarker thresholding: a model-agnostic framework for stability</title>
      <link>https://arxiv.org/abs/2511.18030</link>
      <description>arXiv:2511.18030v1 Announce Type: new 
Abstract: Many biomarker pipelines require patient-level decisions aggregated from instance-level (cell/patch) scores. Thresholds tuned on pooled instances often fail across sites due to hierarchical dependence, prevalence shift, and score-scale mismatch. We present a selection-honest framework for hierarchical thresholding that makes patient-level decisions reproducible and more defensible. At its core is a risk decomposition theorem for selection-honest thresholds. The theorem separates contributions from (i) internal fit and patient-level generalization, (ii) operating-point shift reflecting prevalence and shape changes, and (iii) a stability term that penalizes sensitivity to threshold perturbations. The stability component is computable via patient-block bootstraps mapped through a monotone modulus of risk. This framework is model-agnostic, reconciles heterogeneous decision rules on a quantile scale, and yields monotone-invariant ensembles and reportable diagnostics (e.g. flip-rate, operating-point shift).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18030v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>O. Debeaupuis</dc:creator>
    </item>
    <item>
      <title>On a Reinforcement Learning Methodology for Epidemic Control, with application to COVID-19</title>
      <link>https://arxiv.org/abs/2511.18035</link>
      <description>arXiv:2511.18035v1 Announce Type: new 
Abstract: This paper presents a real time, data driven decision support framework for epidemic control. We combine a compartmental epidemic model with sequential Bayesian inference and reinforcement learning (RL) controllers that adaptively choose intervention levels to balance disease burden, such as intensive care unit (ICU) load, against socio economic costs. We construct a context specific cost function using empirical experiments and expert feedback. We study two RL policies: an ICU threshold rule computed via Monte Carlo grid search, and a policy based on a posterior averaged Q learning agent. We validate the framework by fitting the epidemic model to publicly available ICU occupancy data from the COVID 19 pandemic in England and then generating counterfactual roll out scenarios under each RL controller, which allows us to compare the RL policies to the historical government strategy. Over a 300 day period and for a range of cost parameters, both controllers substantially reduce ICU burden relative to the observed interventions, illustrating how Bayesian sequential learning combined with RL can support the design of epidemic control policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18035v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giacomo Iannucci, Petros Barmpounakis, Alexandros Beskos, Nikolaos Demiris</dc:creator>
    </item>
    <item>
      <title>Sequential Bootstrap for Out-of-Bag Error Estimation: A Simulation-Based Replication and Stability-Oriented Refinement</title>
      <link>https://arxiv.org/abs/2511.18065</link>
      <description>arXiv:2511.18065v1 Announce Type: new 
Abstract: Bootstrap resampling is the foundation of many ensemble learning methods, and out-of-bag (OOB) error estimation is the most widely used internal measure of generalization performance. In the standard multinomial bootstrap, the number of distinct observations in each resample is random. Although this source of variability exists, it has rarely been studied in isolation to understand how much it affects OOB-based quantities. To address this gap, we investigate Sequential Bootstrap, a resampling method that forces every bootstrap replicate to contain the same number of distinct observations, and treat it as a controlled modification of the classical bootstrap within the OOB framework. We reproduce Breiman's five original OOB experiments on both synthetic and real-world datasets, repeating all analyses across many different random seeds. Our results show that switching from the classical bootstrap to Sequential Bootstrap leaves accuracy-related metrics essentially unchanged, but yields measurable and data-dependent reductions in several variance-related measures. Therefore, Sequential Bootstrap should not be viewed as a new method for improving predictive performance, but rather as a tool for understanding how randomness in the number of distinct samples contributes to the variance of OOB estimators. This work provides a reproducible setting for studying the statistical properties of resampling-based ensemble estimators and offers empirical evidence that may support future theoretical work on variance decomposition in bootstrap-based systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18065v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng Peng</dc:creator>
    </item>
    <item>
      <title>A sensitivity analysis for non-inferiority studies with non-randomised data</title>
      <link>https://arxiv.org/abs/2511.18094</link>
      <description>arXiv:2511.18094v1 Announce Type: new 
Abstract: Background: Non-inferiority studies based on non-randomised data are increasingly used in clinical research but remain prone to unmeasured confounding. The classical E-value offers a simple way to quantify such bias but has been applied almost exclusively with respect to the statistical null. We reformulated the E-value framework to make explicit its applicability to predefined clinical margins, thereby extending its utility to non-inferiority analyses.
  Development: Using the bias-factor formulation by Ding and VanderWeele, we defined the non-inferiority E-value as the minimum strength of association that an unmeasured confounder would need with both treatment and outcome, on the risk-ratio scale, to move the 95% confidence-limit estimate to the prespecified non-inferiority margin.
  Application: This approach was applied to three observational studies and one single-arm trial with external controls to illustrate interpretation and range. The resulting non-inferiority E-values for the confidence limits varied from about one to three, depending on design and findings. In the single-arm trial, a large gap between the confidence-limit and point-estimate NIEs reflected small sample size and wide confidence intervals, highlighting that both should be reported for a balanced assessment of robustness.
  Conclusion: This study reformulates the E-value to focus on clinically meaningful margins rather than the statistical null, enabling its application to non-inferiority analyses. Although the non-inferiority E-value inherits the limitations of the original method and cannot address all bias sources, it offers a transparent framework for interpreting non-randomised evidence and for generating insights that inform the design of future, more definitive randomised controlled trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18094v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daijiro Kabata, Takumi Imai</dc:creator>
    </item>
    <item>
      <title>Sparse-Smooth Spatially Varying Coefficient Quantile Regression</title>
      <link>https://arxiv.org/abs/2511.18106</link>
      <description>arXiv:2511.18106v1 Announce Type: new 
Abstract: We develop a convex framework for spatially varying coefficient quantile regression that, for each predictor, separates a location-invariant \emph{global} effect from a \emph{spatial deviation}. An adaptive group penalty selects whether a predictor varies over space, while a graph\textendash Laplacian quadratic promotes spatial continuity of the deviations on irregular networks. The formulation is identifiable via degree-weighted centering and scales with sparse linear algebra. We provide two practical solvers\textemdash an ADMM algorithm with closed-form proximal maps for the check loss and a smoothed proximal-gradient scheme based on the Moreau envelope\textemdash together with implementation guidance (projection for identifiability, stopping diagnostics, and preconditioning). Under mild conditions on the sampling design, covariates, error density, and graph geometry, we establish selection consistency for the deviation groups, mean-squared error bounds that balance Laplacian bias and stochastic variability, and root-\(n\) asymptotic normality for the global coefficients with an oracle property. Simulations mimicking air-pollution applications demonstrate accurate recovery of global vs.\ local effects and competitive predictive performance under heteroskedastic, heavy-tailed noise. We discuss graph construction, spatially blocked cross-validation (to prevent leakage), and options for robust standard errors under spatial dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18106v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hou Jian, Meng Tan, Tian Maozai</dc:creator>
    </item>
    <item>
      <title>Revisiting Penalized Likelihood Estimation for Gaussian Processes</title>
      <link>https://arxiv.org/abs/2511.18111</link>
      <description>arXiv:2511.18111v1 Announce Type: new 
Abstract: Gaussian processes (GPs) are popular as nonlinear regression models for expensive computer simulations, yet GP performance relies heavily on estimation of unknown covariance parameters. Maximum likelihood estimation (MLE) is common, but it can be plagued by numerical issues in small data settings. The addition of a nugget helps but is not a cure-all. Penalized likelihood methods may improve upon traditional MLE, but their success depends on tuning parameter selection. We introduce a new cross-validation (CV) metric called ``decorrelated prediction error'' (DPE), within the penalized likelihood framework for GPs. Inspired by the Mahalanobis distance, DPE provides more consistent and reliable tuning parameter selection than traditional metrics like prediction error, particularly for $K$-fold CV. Our proposed metric performs comparably to standard MLE when penalization is unnecessary and outperforms traditional tuning parameter selection metrics in scenarios where regularization is beneficial, especially under the one-standard error rule.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18111v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayumi Mutoh, Annie S. Booth, Jonathan W. Stallrich</dc:creator>
    </item>
    <item>
      <title>Spatial deformation in a Bayesian spatiotemporal model for incomplete matrix-variate responses</title>
      <link>https://arxiv.org/abs/2511.18201</link>
      <description>arXiv:2511.18201v1 Announce Type: new 
Abstract: In this paper, we propose a flexible matrix-variate spatiotemporal model for analyzing multiple response variables observed at spatially distributed locations over time. Our approach relaxes the restrictive assumption of spatial isotropy, which is often unrealistic in environmental and ecological processes. We adopt a deformation-based method that allows the covariance structure to adapt to directional patterns and nonstationary behavior in space. Temporal dynamics are incorporated through dynamic linear models within a fully Bayesian framework, ensuring coherent uncertainty propagation and efficient state-space inference. Additionally, we introduce a strategy for handling missing observations across different variables, preserving the joint data structure without discarding entire time points or stations. Through a simulation study and an application to real-world air quality monitoring data, we demonstrate that incorporating spatial deformation substantially improves interpolation accuracy in anisotropic scenarios while maintaining competitive performance under near-isotropy. The proposed methodology provides a general and computationally tractable framework for multivariate spatiotemporal modeling with incomplete data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18201v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rodrigo de Souza Bulh\~oes, Marina Silva Paez, Dani Gamerman</dc:creator>
    </item>
    <item>
      <title>Efficient Covariance Estimation for Sparsified Functional Data</title>
      <link>https://arxiv.org/abs/2511.18237</link>
      <description>arXiv:2511.18237v1 Announce Type: new 
Abstract: Motivated by recent work involving the analysis of leveraging spatial correlations in sparsified mean estimation, we present a novel procedure for constructing covariance estimator. The proposed Random-knots (Random-knots-Spatial) and B-spline (Bspline-Spatial) estimators of the covariance function are computationally efficient. Asymptotic pointwise of the covariance are obtained for sparsified individual trajectories under some regularity conditions. Our proposed nonparametric method well perform the functional principal components analysis for the case of sparsified data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. Theoretical results are illustrated with Monte Carlo simulation experiments. Finally, we cluster multi-domain data by replacing the covariance function with our proposed covariance estimator during PCA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18237v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sijie Zheng, Fandong Meng, Jie Zhou</dc:creator>
    </item>
    <item>
      <title>Change-Point Detection With Multivariate Repeated Measures</title>
      <link>https://arxiv.org/abs/2511.18432</link>
      <description>arXiv:2511.18432v1 Announce Type: new 
Abstract: Graph-based methods have shown particular strengths in change-point detection (CPD) tasks for high-dimensional nonparametric settings. However, existing CPD research has rarely addressed data with repeated measurements or local group structures. A common treatment is to average repeated measurements, which can result in the loss of important within-individual information. In this paper, we propose a new graph-based method for detecting change-points in data with repeated measurements or local structures by incorporating both within-individual and between-individual information. Analytical approximations to the significance of the proposed statistics are derived, enabling efficient computation of p-values for the combined test statistic. The proposed method effectively detects change-points across a wide range of alternatives, particularly when within-individual differences are present. The new method is illustrated through an analysis of the New York City taxi dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18432v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Serim Han, Jingru Zhang, Hoseung Song</dc:creator>
    </item>
    <item>
      <title>Hyperevent network modelling of partially observed gossip data</title>
      <link>https://arxiv.org/abs/2511.18543</link>
      <description>arXiv:2511.18543v1 Announce Type: new 
Abstract: Gossiping is a widespread social phenomenon that shapes relationships and information flow in communities. From a network theoretic point of view, gossiping can be seen as a higher-order interaction, as it involves at least two persons talking about a non-present third. The mechanism of gossiping is complex: it is most likely dynamic, as its intensity changes over time, and possibly viral, if a gossiping event induces future gossiping, such as a repetition or retaliation. We define covariates of interest for these effects and propose a relational hyperevent model to study and quantify these complex dynamics. We consider survey data collected yearly from 44 secondary schools in Hungary. No information is available about the exact timing of the events nor about the aggregate number of events within the yearly time interval. What is measured is whether at least one gossiping event has occurred in a given time interval. We extend inference for relational hyperevent models to the case of rightcensored interval-time data and show how flexible and efficient generalized additive models can be used for estimation of effects of interest. Our analysis on the school data illustrates how a model that accounts for linear, smooth and random effects can identify the social drivers of gossiping, while revealing complex temporal dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18543v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Veronica Poda, Veronica Vinciotti, Ernst C. Wit</dc:creator>
    </item>
    <item>
      <title>A joint optimization approach to identifying sparse dynamics using least squares kernel collocation</title>
      <link>https://arxiv.org/abs/2511.18555</link>
      <description>arXiv:2511.18555v1 Announce Type: new 
Abstract: We develop an all-at-once modeling framework for learning systems of ordinary differential equations (ODE) from scarce, partial, and noisy observations of the states. The proposed methodology amounts to a combination of sparse recovery strategies for the ODE over a function library combined with techniques from reproducing kernel Hilbert space (RKHS) theory for estimating the state and discretizing the ODE. Our numerical experiments reveal that the proposed strategy leads to significant gains in terms of accuracy, sample efficiency, and robustness to noise, both in terms of learning the equation and estimating the unknown states. This work demonstrates capabilities well beyond existing and widely used algorithms while extending the modeling flexibility of other recent developments in equation discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18555v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander W. Hsu, Ike W. Griss Salas, Jacob M. Stevens-Haas, J. Nathan Kutz, Aleksandr Aravkin, Bamdad Hosseini</dc:creator>
    </item>
    <item>
      <title>Can discrete-time analyses be trusted for stepped wedge trials with continuous recruitment?</title>
      <link>https://arxiv.org/abs/2511.18731</link>
      <description>arXiv:2511.18731v1 Announce Type: new 
Abstract: In stepped wedge cluster randomized trials (SW-CRTs), interventions are sequentially rolled out to clusters over multiple periods. It is common practice to analyze SW-CRTs using discrete-time linear mixed models, in which measurements are considered to be taken at discrete time points. However, a recent systematic review found that 95.1\% of cross-sectional SW-CRTs recruit individuals continuously over time. Despite the high prevalence of designs with continuous recruitment, there has been limited guidance on how to draw model-robust inference when analyzing such SW-CRTs. In this article, we investigate through simulations the implications of using discrete-time linear mixed models in the case of continuous recruitment designs with a continuous outcome. First, in the data-generating process, we characterize continuous recruitment with a continuous-time exponential decay correlation structure in the presence or absence of a continuous period effect, addressing scenarios both with and without a random or exposure-time-dependent intervention effect. Then, we analyze the simulated data under three popular discrete-time working correlation structures: simple exchangeable, nested exchangeable, and discrete-time exponential decay, with a robust sandwich variance estimator. Our results demonstrate that discrete-time analysis often yields minimum bias, and the robust variance estimator with the Mancl and DeRouen correction consistently achieves nominal coverage and type I error rate. One important exception occurs when recruitment patterns vary systematically between control and intervention periods, where discrete-time analysis leads to slightly biased estimates. Finally, we illustrate these findings by reanalyzing a concluded SW-CRT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18731v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Wang, Guangyu Tong, Heather Allore, Monica Taljaard, Fan Li</dc:creator>
    </item>
    <item>
      <title>Gaussian process priors with Markov properties for effective reproduction number inference</title>
      <link>https://arxiv.org/abs/2511.18797</link>
      <description>arXiv:2511.18797v1 Announce Type: new 
Abstract: Many quantities characterizing infectious disease outbreaks - like the effective reproduction number ($R_t$), defined as the average number of secondary infections a newly infected individual will cause over the course of their infection - need to be modeled as time-varying parameters. It is common practice to use Gaussian random walks as priors for estimating such functions in Bayesian analyses of pathogen surveillance data. In this setting, however, the random walk prior may be too permissive, as it fails to capture prior scientific knowledge about the estimand and results in high posterior variance. We propose several Gaussian Markov process priors for $R_t$ inference, including the Integrated Brownian Motion (IBM), which can be represented as a Markov process when augmented with its corresponding Brownian Motion component, and is therefore computationally efficient and simple to implement and tune. We use simulated outbreak data to compare the performance of these proposed priors with the Gaussian random walk prior and another state-of-the-art Gaussian process prior based on an approximation to a Mat\'ern covariance function. We find that IBM can match or exceed the performance of other priors, and we show that it produces epidemiologically reasonable and precise results when applied to county-level SARS-CoV-2 data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18797v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jessalyn N. Sebastian, Volodymyr M. Minin</dc:creator>
    </item>
    <item>
      <title>X-chromosome Multilocus Association Studies for Common and Rare Variants</title>
      <link>https://arxiv.org/abs/2511.18948</link>
      <description>arXiv:2511.18948v1 Announce Type: new 
Abstract: X-chromosome association study has specific model uncertainty challenges, such as unknown X-chromosome inactivation status and baseline allele, and considering nonadditive and gene-sex interaction effects in the analysis or not. Although these challenges have been answered for single-locus X-chromosome variants, it remains unclear how to properly perform multilocus association studies when above uncertainties are present. We first carefully investigate the inferential consequences of these uncertainties on existing multilocus association analysis methods, and then propose a theoretically justified framework to analyze multilocus X-chromosome variants while all the uncertainty issues are addressed. We provide separate solutions for common and rare variants, and simulation results show that our solutions are overall more powerful than existing multilocus methods which were proposed to analyze autosomal variants. We finally provide supporting evidences of our approach by revisiting some published X-chromosome association studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18948v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruilin Bai, Bo Chen</dc:creator>
    </item>
    <item>
      <title>A novel nonparametric framework for DIF detection using kernel-smoothed item response curves</title>
      <link>https://arxiv.org/abs/2511.18963</link>
      <description>arXiv:2511.18963v1 Announce Type: new 
Abstract: This study introduces a novel nonparametric approach for detecting Differential Item Functioning (DIF) in binary items through direct comparison of Item Response Curves (IRCs). Building on prior work on nonparametric comparison of regression curves, we extend the methodology to accommodate binary response data, which is typical in psychometric applications. The proposed approach includes a new estimator of the asymptotic variance of the test statistic and derives optimal weight functions that maximise local power. Because the asymptotic distribution of the resulting test statistic is unknown, a wild bootstrap procedure is applied for inference. A Monte Carlo simulation study demonstrates that the nonparametric approach effectively controls Type I error and achieves power comparable to the traditional logistic regression method, outperforming it in cases with multiple intersections of the underlying IRCs. The impact of bandwidth and weight specification is explored. Application to a verbal aggression dataset further illustrates the method's ability to detect subtle DIF patterns missed by parametric models. Overall, the proposed nonparametric framework provides a flexible and powerful alternative for detecting DIF, particularly in complex scenarios where traditional model-based assumptions may not be applicable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18963v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ad\'ela Hladk\'a, Patr\'icia Martinkov\'a</dc:creator>
    </item>
    <item>
      <title>Can we detect treatment effect waning from time-to-event data?</title>
      <link>https://arxiv.org/abs/2511.19096</link>
      <description>arXiv:2511.19096v1 Announce Type: new 
Abstract: Understanding how the causal effect of a treatment evolves over time, including the potential for waning, is important for informed decisions on treatment discontinuation or repetition. For example, waning vaccine protection influences booster dose recommendations, while cost-effectiveness analyses require accounting for long-term efficacy of treatments. However, there is no consensus on the methodology to assess and account for treatment effect waning. Even in randomized controlled trials, the common na\"ive comparison of hazard functions can lead to misleading causal conclusions due to inherent selection bias. Although comparing survival curves is sometimes recommended as a safer measure of causal effect, it only represents a cumulative effect over time and does not address treatment effect waning. We also explore recent formulations of causal hazard ratios, based on the principal stratification approach or the controlled direct effect. These causal hazard ratios cannot be identified without strong modeling assumptions, but bounds can be derived accounting for unobserved heterogeneity and one could try to use them to detect treatment effect waning. However, we illustrate that an increase in causal hazard ratios towards one does not necessarily mean that the protective effect of the treatment is fading. Furthermore, the same survival functions may correspond to both scenarios with and without waning, which shows that treatment effect waning cannot be identified from standard time-to-event data without strong untestable modeling assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19096v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eni Musta, Joris Mooij</dc:creator>
    </item>
    <item>
      <title>Integrating Complex Covariate Transformations in Generalized Additive Models</title>
      <link>https://arxiv.org/abs/2511.19234</link>
      <description>arXiv:2511.19234v1 Announce Type: new 
Abstract: Transformations of covariates are widely used in applied statistics to improve interpretability and to satisfy assumptions required for valid inference. More broadly, feature engineering encompasses a wider set of practices aimed at enhancing predictive performance, and is typically performed as part of a data pre-processing step. In contrast, this paper integrates a substantial component of the feature engineering process directly into the modelling stage. This is achieved by introducing a novel general framework for embedding interpretable covariate transformations within multi-parameter Generalised Additive Models (GAMs). Our framework accommodates any sufficiently differentiable scalar-valued transformation of potentially high-dimensional and complex covariates. These transformations are treated as integral model components, with their parameters estimated jointly with regression coefficients via maximum a posteriori (MAP) methods, and joint uncertainty quantified via approximate Bayesian techniques. Smoothing parameters are selected in an empirical Bayes framework using a Laplace approximation to the marginal likelihood, supported by efficient computation based on implicit differentiation methods. We demonstrate the flexibility and practical value of the proposed methodology through applications to forecasting electricity net-demand in Great Britain and to modelling house prices in London. The proposed methods are implemented by the gamFactory R package, available at https://github.com/mfasiolo/gamFactory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19234v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudia Collarin, Matteo Fasiolo, Yannig Goude, Simon Wood</dc:creator>
    </item>
    <item>
      <title>Community-level core-periphery structures in collaboration networks</title>
      <link>https://arxiv.org/abs/2511.19305</link>
      <description>arXiv:2511.19305v1 Announce Type: new 
Abstract: Uncovering structural patterns in collaboration networks is key for understanding how knowledge flows and innovation emerges. These networks often exhibit a rich interplay of meso-scale structures, such as communities, core-periphery organization, and influential hubs, which shape the complexity of scientific collaboration. The coexistence of such structures challenges traditional approaches, which typically isolate specific network patterns at the node level. We introduce a novel framework for detecting core-periphery structures at the community level. Given a reference grouping of the nodes, the method optimizes an objective function that assigns core or peripheral roles to communities by accounting for the density and strength of their inter-community connections. The node-level partition may correspond to either inferred communities or to a node-attribute classification, such as discipline or location, enabling direct interpretation of how different social or organizational groups occupy central positions in the network. The method is motivated by an application to a co-authorship network of Italian academics in three different disciplines, where it reveals a hierarchical core-periphery structure associated with institutional role, regional location, and research topics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19305v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Geremia, Domenico De Stefano, Michael Fop</dc:creator>
    </item>
    <item>
      <title>Product Depth for Temporal Point Processes Observed Only Up to the First k Events</title>
      <link>https://arxiv.org/abs/2511.19375</link>
      <description>arXiv:2511.19375v1 Announce Type: new 
Abstract: Temporal point processes (TPPs) model the timing of discrete events along a timeline and are widely used in fields such as neuroscience and fi- nance. Statistical depth functions are powerful tools for analyzing centrality and ranking in multivariate and functional data, yet existing depth notions for TPPs remain limited. In this paper, we propose a novel product depth specifically designed for TPPs observed only up to the first k events. Our depth function comprises two key components: a normalized marginal depth, which captures the temporal distribution of the final event, and a conditional depth, which characterizes the joint distribution of the preceding events. We establish its key theoretical properties and demonstrate its practical utility through simulation studies and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19375v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chifeng Shen, Yuejiao Fu, Xiaoping Shi, Michael Chen</dc:creator>
    </item>
    <item>
      <title>Asymptotic linear dependence and ellipse statistics for multivariate two-sample homogeneity test</title>
      <link>https://arxiv.org/abs/2511.19381</link>
      <description>arXiv:2511.19381v1 Announce Type: new 
Abstract: Statistical depth, which measures the center-outward rank of a given sample with respect to its underlying distribution, has become a popular and powerful tool in nonparametric inference. In this paper, we investigate the use of statistical depth in multivariate two-sample problems. We propose a new depth-based nonparametric two-sample test, which has the Chi-square(1) asymptotic distribution under the null hypothesis. Simulations and real-data applications highlight the efficacy and practical value of the proposed test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19381v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chifeng Shen, Yuejiao Fu, Michael Chen, Xiaoping Shi</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian spectral analysis of multiple stationary time series</title>
      <link>https://arxiv.org/abs/2511.19406</link>
      <description>arXiv:2511.19406v1 Announce Type: new 
Abstract: The power spectrum of biomedical time series provides important indirect measurements of physiological processes underlying health and biological functions. However, simultaneously characterizing power spectra for multiple time series remains challenging due to extra spectral variability and varying time series lengths. We propose a method for hierarchical Bayesian estimation of stationary time series (HBEST) that provides an interpretable framework for efficiently modeling multiple power spectra. HBEST models log power spectra using a truncated cosine basis expansion with a novel global-local coefficient decomposition, enabling simultaneous estimation of population-level and individual-level power spectra and accommodating time series of varying lengths. The fully Bayesian framework provides shrinkage priors for regularized estimation and efficient information sharing. Simulations demonstrate HBEST's advantages over competing methods in computational efficiency and estimation accuracy. An application to heart rate variability time series demonstrates HBEST's ability to accurately characterize power spectra and capture associations with traditional cardiovascular risk factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19406v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rebecca Lee, Alexander Coulter, Greg J. Siegle, Scott A. Bruce, Anirban Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Random Text, Zipf's Law, Critical Length,and Implications for Large Language Models</title>
      <link>https://arxiv.org/abs/2511.17575</link>
      <description>arXiv:2511.17575v1 Announce Type: cross 
Abstract: We study a deliberately simple, fully non-linguistic model of text: a sequence of independent draws from a finite alphabet of letters plus a single space symbol. A word is defined as a maximal block of non-space symbols. Within this symbol-level framework, which assumes no morphology, syntax, or semantics, we derive several structural results. First, word lengths follow a geometric distribution governed solely by the probability of the space symbol. Second, the expected number of words of a given length, and the expected number of distinct words of that length, admit closed-form expressions based on a coupon-collector argument. This yields a critical word length k* at which word types transition from appearing many times on average to appearing at most once. Third, combining the exponential growth of the number of possible strings of length k with the exponential decay of the probability of each string, we obtain a Zipf-type rank-frequency law p(r) proportional to r^{-alpha}, with an exponent determined explicitly by the alphabet size and the space probability.
  Our contribution is twofold. Mathematically, we give a unified derivation linking word lengths, vocabulary growth, critical length, and rank-frequency structure in a single explicit model. Conceptually, we argue that this provides a structurally grounded null model for both natural-language word statistics and token statistics in large language models. The results show that Zipf-like patterns can arise purely from combinatorics and segmentation, without optimization principles or linguistic organization, and help clarify which phenomena require deeper explanation beyond random-text structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17575v1</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Berman</dc:creator>
    </item>
    <item>
      <title>Copula Based Fusion of Clinical and Genomic Machine Learning Risk Scores for Breast Cancer Risk Stratification</title>
      <link>https://arxiv.org/abs/2511.17605</link>
      <description>arXiv:2511.17605v1 Announce Type: cross 
Abstract: Clinical and genomic models are both used to predict breast cancer outcomes, but they are often combined using simple linear rules that do not account for how their risk scores relate, especially at the extremes. Using the METABRIC breast cancer cohort, we studied whether directly modeling the joint relationship between clinical and genomic machine learning risk scores could improve risk stratification for 5-year cancer-specific mortality. We created a binary 5-year cancer-death outcome and defined two sets of predictors: a clinical set (demographic, tumor, and treatment variables) and a genomic set (gene-expression $z$-scores). We trained several supervised classifiers, such as Random Forest and XGBoost, and used 5-fold cross-validated predicted probabilities as unbiased risk scores. These scores were converted to pseudo-observations on $(0,1)^2$ to fit Gaussian, Clayton, and Gumbel copulas. Clinical models showed good discrimination (AUC 0.783), while genomic models had moderate performance (AUC 0.681). The joint distribution was best captured by a Gaussian copula (bootstrap $p=0.997$), which suggests a symmetric, moderately strong positive relationship. When we grouped patients based on this relationship, Kaplan-Meier curves showed clear differences: patients who were high-risk in both clinical and genomic scores had much poorer survival than those high-risk in only one set. These results show that copula-based fusion works in real-world cohorts and that considering dependencies between scores can better identify patient subgroups with the worst prognosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17605v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agnideep Aich, Sameera Hewage, Md Monzur Murshed</dc:creator>
    </item>
    <item>
      <title>A novel k-means clustering approach using two distance measures for Gaussian data</title>
      <link>https://arxiv.org/abs/2511.17823</link>
      <description>arXiv:2511.17823v1 Announce Type: cross 
Abstract: Clustering algorithms have long been the topic of research, representing the more popular side of unsupervised learning. Since clustering analysis is one of the best ways to find some clarity and structure within raw data, this paper explores a novel approach to \textit{k}-means clustering. Here we present a \textit{k}-means clustering algorithm that takes both the within cluster distance (WCD) and the inter cluster distance (ICD) as the distance metric to cluster the data into \emph{k} clusters pre-determined by the Calinski-Harabasz criterion in order to provide a more robust output for the clustering analysis. The idea with this approach is that by including both the measurement metrics, the convergence of the data into their clusters becomes solidified and more robust. We run the algorithm with some synthetically produced data and also some benchmark data sets obtained from the UCI repository. The results show that the convergence of the data into their respective clusters is more accurate by using both WCD and ICD measurement metrics. The algorithm is also better at clustering the outliers into their true clusters as opposed to the traditional \textit{k} means method. We also address some interesting possible research topics that reveal themselves as we answer the questions we initially set out to address.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17823v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naitik Gada (Rochester Institute of Technology)</dc:creator>
    </item>
    <item>
      <title>Divergence-Minimization for Latent-Structure Models: Monotone Operators, Contraction Guarantees, and Robust Inference</title>
      <link>https://arxiv.org/abs/2511.17974</link>
      <description>arXiv:2511.17974v1 Announce Type: cross 
Abstract: We develop a divergence-minimization (DM) framework for robust and efficient inference in latent-mixture models. By optimizing a residual-adjusted divergence, the DM approach recovers EM as a special case and yields robust alternatives through different divergence choices. We establish that the sample objective decreases monotonically along the iterates, leading the DM sequence to stationary points under standard conditions, and that at the population level the operator exhibits local contractivity near the minimizer. Additionally, we verify consistency and $\sqrt{n}$-asymptotic normality of minimum-divergence estimators and of finitely many DM iterations, showing that under correct specification their limiting covariance matches the Fisher information. Robustness is analyzed via the residual-adjustment function, yielding bounded influence functions and a strictly positive breakdown bound for bounded-RAF divergences, and we contrast this with the non-robust behaviour of KL/EM. Next, we address the challenge of determining the number of mixture components by proposing a penalized divergence criterion combined with repeated sample splitting, which delivers consistent order selection and valid post-selection inference. Empirically, DM instantiations based on Hellinger and negative exponential divergences deliver accurate inference and remain stable under contamination in mixture and image-segmentation tasks. The results clarify connections to MM and proximal-point methods and offer practical defaults, making DM a drop-in alternative to EM for robust latent-structure inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17974v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Li, Anand N. Vidyashankar</dc:creator>
    </item>
    <item>
      <title>Hierarchical Linkage Clustering Beyond Binary Trees and Ultrametrics</title>
      <link>https://arxiv.org/abs/2511.18056</link>
      <description>arXiv:2511.18056v1 Announce Type: cross 
Abstract: Hierarchical clustering seeks to uncover nested structures in data by constructing a tree of clusters, where deeper levels reveal finer-grained relationships. Traditional methods, including linkage approaches, face three major limitations: (i) they always return a hierarchy, even if none exists, (ii) they are restricted to binary trees, even if the true hierarchy is non-binary, and (iii) they are highly sensitive to the choice of linkage function. In this paper, we address these issues by introducing the notion of a valid hierarchy and defining a partial order over the set of valid hierarchies. We prove the existence of a finest valid hierarchy, that is, the hierarchy that encodes the maximum information consistent with the similarity structure of the data set. In particular, the finest valid hierarchy is not constrained to binary structures and, when no hierarchical relationships exist, collapses to a star tree. We propose a simple two-step algorithm that first constructs a binary tree via a linkage method and then prunes it to enforce validity. We establish necessary and sufficient conditions on the linkage function under which this procedure exactly recovers the finest valid hierarchy, and we show that all linkage functions satisfying these conditions yield the same hierarchy after pruning. Notably, classical linkage rules such as single, complete, and average satisfy these conditions, whereas Ward's linkage fails to do so.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18056v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maximilien Dreveton, Matthias Grossglauser, Daichi Kuroda, Patrick Thiran</dc:creator>
    </item>
    <item>
      <title>An operator splitting analysis of Wasserstein--Fisher--Rao gradient flows</title>
      <link>https://arxiv.org/abs/2511.18060</link>
      <description>arXiv:2511.18060v1 Announce Type: cross 
Abstract: Wasserstein-Fisher-Rao (WFR) gradient flows have been recently proposed as a powerful sampling tool that combines the advantages of pure Wasserstein (W) and pure Fisher-Rao (FR) gradient flows. Existing algorithmic developments implicitly make use of operator splitting techniques to numerically approximate the WFR partial differential equation, whereby the W flow is evaluated over a given step size and then the FR flow (or vice versa). This works investigates the impact of the order in which the W and FR operator are evaluated and aims to provide a quantitative analysis. Somewhat surprisingly, we show that with a judicious choice of step size and operator ordering, the split scheme can converge to the target distribution faster than the exact WFR flow (in terms of model time). We obtain variational formulae describing the evolution over one time step of both sequential splitting schemes and investigate in which settings the W-FR split should be preferred to the FR-W split. As a step towards this goal we show that the WFR gradient flow preserves log-concavity and obtain the first sharp decay bound for WFR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18060v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca Romana Crucinio, Sahani Pathiraja</dc:creator>
    </item>
    <item>
      <title>Robust Inference Methods for Latent Group Panel Models under Possible Group Non-Separation</title>
      <link>https://arxiv.org/abs/2511.18550</link>
      <description>arXiv:2511.18550v1 Announce Type: cross 
Abstract: This paper presents robust inference methods for general linear hypotheses in linear panel data models with latent group structure in the coefficients. We employ a selective conditional inference approach, deriving the conditional distribution of coefficient estimates given the group structure estimated from the data. Our procedure provides valid inference under possible violations of group separation, where distributional properties of group-specific coefficients remain unestablished. Furthermore, even when group separation does hold, our method demonstrates superior finite-sample properties compared to traditional asymptotic approaches. This improvement stems from our procedure's ability to account for statistical uncertainty in the estimation of group structure. We demonstrate the effectiveness of our approach through Monte Carlo simulations and apply the methods to two datasets on: (i) the relationship between income and democracy, and (ii) the cyclicality of firm-level R&amp;D investment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18550v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oguzhan Akgun, Ryo Okui</dc:creator>
    </item>
    <item>
      <title>Majority of the Bests: Improving Best-of-N via Bootstrapping</title>
      <link>https://arxiv.org/abs/2511.18630</link>
      <description>arXiv:2511.18630v1 Announce Type: cross 
Abstract: Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18630v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amin Rakhsha, Kanika Madan, Tianyu Zhang, Amir-massoud Farahmand, Amir Khasahmadi</dc:creator>
    </item>
    <item>
      <title>Estimation of High-dimensional Nonlinear Vector Autoregressive Models</title>
      <link>https://arxiv.org/abs/2511.18641</link>
      <description>arXiv:2511.18641v1 Announce Type: cross 
Abstract: High-dimensional vector autoregressive (VAR) models have numerous applications in fields such as econometrics, biology, climatology, among others. While prior research has mainly focused on linear VAR models, these approaches can be restrictive in practice. To address this, we introduce a high-dimensional non-parametric sparse additive model, providing a more flexible framework. Our method employs basis expansions to construct high-dimensional nonlinear VAR models. We derive convergence rates and model selection consistency for least squared estimators, considering dependence measures of the processes, error moment conditions, sparsity, and basis expansions. Our theory significantly extends prior linear VAR models by incorporating both non-Gaussianity and non-linearity. As a key contribution, we derive sharp Bernstein-type inequalities for tail probabilities in both non-sub-Gaussian linear and nonlinear VAR processes, which match the classical Bernstein inequality for independent random variables. Additionally, we present numerical experiments that support our theoretical findings and demonstrate the advantages of the nonlinear VAR model for a gene expression time series dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18641v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuefeng Han, Likai Chen, Wei Biao Wu</dc:creator>
    </item>
    <item>
      <title>Uncertainty of Network Topology with Applications to Out-of-Distribution Detection</title>
      <link>https://arxiv.org/abs/2511.18813</link>
      <description>arXiv:2511.18813v1 Announce Type: cross 
Abstract: Persistent homology (PH) is a crucial concept in computational topology, providing a multiscale topological description of a space. It is particularly significant in topological data analysis, which aims to make statistical inference from a topological perspective. In this work, we introduce a new topological summary for Bayesian neural networks, termed the predictive topological uncertainty (pTU). The proposed pTU measures the uncertainty in the interaction between the model and the inputs. It provides insights from the model perspective: if two samples interact with a model in a similar way, then they are considered identically distributed. We also show that the pTU is insensitive to the model architecture. As an application, pTU is used to solve the out-of-distribution (OOD) detection problem, which is critical to ensure model reliability. Failure to detect OOD input can lead to incorrect and unreliable predictions. To address this issue, we propose a significance test for OOD based on the pTU, providing a statistical framework for this issue. The effectiveness of the framework is validated through various experiments, in terms of its statistical power, sensitivity, and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18813v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sing-Yuan Yeh, Chun-Hao Yang</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient Routing Protocol in Vehicular Opportunistic Networks: A Dynamic Cluster-based Routing Using Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2511.19026</link>
      <description>arXiv:2511.19026v1 Announce Type: cross 
Abstract: Opportunistic Networks (OppNets) employ the Store-Carry-Forward (SCF) paradigm to maintain communication during intermittent connectivity. However, routing performance suffers due to dynamic topology changes, unpredictable contact patterns, and resource constraints including limited energy and buffer capacity. These challenges compromise delivery reliability, increase latency, and reduce node longevity in highly dynamic environments. This paper proposes Cluster-based Routing using Deep Reinforcement Learning (CR-DRL), an adaptive routing approach that integrates an Actor-Critic learning framework with a heuristic function. CR-DRL enables real-time optimal relay selection and dynamic cluster overlap adjustment to maintain connectivity while minimizing redundant transmissions and enhancing routing efficiency. Simulation results demonstrate significant improvements over state-of-the-art baselines. CR-DRL extends node lifetimes by up to 21%, overall energy use is reduced by 17%, and nodes remain active for 15% longer. Communication performance also improves, with up to 10% higher delivery ratio, 28.5% lower delay, 7% higher throughput, and data requiring 30% fewer transmission steps across the network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19026v1</guid>
      <category>cs.NI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meisam Sahrifi Sani, Saeid Iranmanesh, Raad Raad, Faisel Tubbal</dc:creator>
    </item>
    <item>
      <title>The Unified Non-Convex Framework for Robust Causal Inference: Overcoming the Gaussian Barrier and Optimization Fragility</title>
      <link>https://arxiv.org/abs/2511.19284</link>
      <description>arXiv:2511.19284v1 Announce Type: cross 
Abstract: This document proposes a Unified Robust Framework that re-engineers the estimation of the Average Treatment Effect on the Overlap (ATO). It synthesizes gamma-Divergence for outlier robustness, Graduated Non-Convexity (GNC) for global optimization, and a "Gatekeeper" mechanism to address the impossibility of higher-order orthogonality in Gaussian regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19284v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eichi Uehara</dc:creator>
    </item>
    <item>
      <title>Unifying Summary Statistic Selection for Approximate Bayesian Computation</title>
      <link>https://arxiv.org/abs/2206.02340</link>
      <description>arXiv:2206.02340v4 Announce Type: replace 
Abstract: Extracting low-dimensional summary statistics from large datasets is essential for efficient (likelihood-free) inference. We characterize three different classes of summaries and demonstrate their importance for correctly analyzing dimensionality reduction algorithms. We demonstrate that minimizing the expected posterior entropy (EPE) under the prior predictive distribution of the model provides a unifying principle that subsumes many existing methods; they are shown to be equivalent to, or special or limiting cases of, minimizing the EPE. We offer a unifying framework for obtaining informative summaries and propose a practical method using conditional density estimation to learn high-fidelity summaries automatically. We evaluate this approach on diverse problems, including a challenging benchmark model with a multi-modal posterior, a population genetics model, and a dynamic network model of growing trees. The results show that EPE-minimizing summaries can lead to posterior inference that is competitive with, and in some cases superior to, dedicated likelihood-based approaches, providing a powerful and general tool for practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.02340v4</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Till Hoffmann, Jukka-Pekka Onnela</dc:creator>
    </item>
    <item>
      <title>Objective Bayesian FDR</title>
      <link>https://arxiv.org/abs/2404.00256</link>
      <description>arXiv:2404.00256v4 Announce Type: replace 
Abstract: Control of false discovery rate (FDR) is important for differential gene expression experiments in typical two-color DNA microarrays. However, control can be lost with the misspecification of FDR. In this study, we developed a Bayesian procedure for analyzing large-scale datasets that objectively provides the optimal posterior FDR. We obtained the estimated null number based on the Storey's $q$-value method, and propose setting the true null number so as to match the posterior null number with the estimated null number. By using the objective Bayesian FDR, we achieved a similar posterior probability to the real FDR, indicating effective control of the FDR level. Moreover, in the estimation process, we adapt a heavy-tailed distribution so that our method can be robust against outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00256v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiko Hayashi</dc:creator>
    </item>
    <item>
      <title>Blessing of dimension in Bayesian inference on covariance matrices</title>
      <link>https://arxiv.org/abs/2404.03805</link>
      <description>arXiv:2404.03805v2 Announce Type: replace 
Abstract: Bayesian factor analysis is routinely used for dimensionality reduction in modeling of high-dimensional covariance matrices. Factor analytic decompositions express the covariance as a sum of a low rank and diagonal matrix. In practice, Gibbs sampling algorithms are typically used for posterior computation, alternating between updating the latent factors, loadings, and residual variances. In this article, we exploit a blessing of dimensionality to develop a provably accurate pseudo-posterior for the covariance matrix that bypasses the need for Gibbs or other variants of Markov chain Monte Carlo sampling. Our proposed Factor Analysis with BLEssing of dimensionality (FABLE) approach relies on a first-stage singular value decomposition (SVD) to estimate the latent factors, and then defines a jointly conjugate prior for the loadings and residual variances. The accuracy of the resulting pseudo-posterior for the covariance improves with increasing dimensionality. We show that FABLE has excellent performance in high-dimensional covariance matrix estimation, including producing well calibrated credible intervals, both theoretically and through simulation experiments. We also demonstrate the strength of our approach in terms of accurate inference and computational efficiency by applying it to a gene expression data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03805v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shounak Chattopadhyay, Anru R. Zhang, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Factors affecting power in stepped wedge trials when the treatment effect varies with time</title>
      <link>https://arxiv.org/abs/2503.11472</link>
      <description>arXiv:2503.11472v2 Announce Type: replace 
Abstract: Stepped wedge cluster randomized trials (SW-CRTs) have historically been analyzed using immediate treatment (IT) models, which assume the effect of the treatment is immediate after treatment initiation and subsequently remains constant over time. However, recent research has shown that this assumption can lead to severely misleading results if treatment effects vary with exposure time, i.e. time since the intervention started. Models that account for time-varying treatment effects, such as the exposure time indicator (ETI) model, allow researchers to target estimands such as the time-averaged treatment effect (TATE) over an interval of exposure time, or the point treatment effect (PTE) representing a treatment contrast at one time point. However, this increased flexibility results in reduced power.
  In this paper, we use public power calculation software and simulation to characterize factors affecting SW-CRT power. Key elements include choice of estimand, study design considerations, and analysis model selection.
  or common SW-CRT designs, the sample size (clusters per sequence or individuals per cluster-period) must be increased substantially, commonly by a factor of 1.5 to 3, but often by much more, to maintain 90\% power when switching from an IT model to an ETI model (targeting the TATE over the study). However, the inflation factor is lower for TATE estimands over shorter periods that exclude longer exposure times. In general, SW-CRT designs (including the "staircase" variant) have much greater power for estimating "short-term effects" relative to "long-term effects". For an ETI model targeting a TATE estimand, substantial power can be gained by adding time points to the start of the study or increasing baseline sample size, but surprisingly little power is gained from adding time points to the end of the study. More restrictive choices for modeling the exposure... [truncated]</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11472v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avi Kenny, Emily C. Voldal, Fan Xia, Kwun Chuen Gary Chan, Patrick J. Heagerty, James P. Hughes</dc:creator>
    </item>
    <item>
      <title>Comparison of Bayesian methods for extrapolation of treatment effects: a large scale simulation study</title>
      <link>https://arxiv.org/abs/2504.01949</link>
      <description>arXiv:2504.01949v3 Announce Type: replace 
Abstract: Extrapolating treatment effects from related studies is a promising strategy for designing and analyzing clinical trials in situations where achieving an adequate sample size is challenging. Bayesian methods are well-suited for this purpose, as they enable the synthesis of prior information through the use of prior distributions. While the operating characteristics of Bayesian approaches for borrowing data from control arms have been extensively studied, methods that borrow treatment effects -- quantities derived from the comparison between two arms -- remain less well understood. In this paper, we present the findings of an extensive simulation study designed to address this gap. We evaluate the frequentist operating characteristics of these methods, including the probability of success, mean squared error, bias, precision, and credible interval coverage. Our results provide insights into the strengths and limitations of existing methods in the context of confirmatory trials. In particular, we show that the Conditional Power Prior and the Robust Mixture Prior perform better overall, while the test-then-pool variants and the p-value-based power prior display suboptimal performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01949v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tristan Fauvel, Julien Tanniou, Pascal Godbillot, Marie G\'enin, Billy Amzal</dc:creator>
    </item>
    <item>
      <title>Optimal Network-Guided Covariate Selection for High-Dimensional Data Integration</title>
      <link>https://arxiv.org/abs/2504.04866</link>
      <description>arXiv:2504.04866v2 Announce Type: replace 
Abstract: Modern data often arises with multiple modalities. For example, covariates and a network are observed on the same subjects, and both contain useful information. Effectively integrating these modalities is important and challenging, especially when the response is unavailable. We study the fundamental covariate selection problem for high-dimensional data by leveraging network information.
  We propose the Network-Guided Covariate Selection (NGCS) algorithm. NGCS exploits the spectral structure of the network to construct a network-guided screening statistic, and employs data-driven Higher Criticism Thresholding for covariate recovery. We establish consistency guarantees for NGCS under general networks. In particular, under two commonly used network models, we relate the projected signal strength to the individual signal strength, and demonstrate that NGCS is optimal for covariate selection. It could achieve the same rate as supervised learning.
  We further consider a two-study setting for downstream applications, where the network is observed only in Study 1. For clustering and regression, we propose NG-clu and NG-reg algorithms. NG-clu accurately clusters all subjects, while NG-reg improves prediction by using the post-selection covariate matrix. Experiments on synthetic and real datasets demonstrate the robustness and superior performance of our algorithms across various network models, noise distributions, and signal strengths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04866v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tao Shen, Wanjie Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric community detection in assortative stochastic block models</title>
      <link>https://arxiv.org/abs/2506.19576</link>
      <description>arXiv:2506.19576v2 Announce Type: replace 
Abstract: Structured data in the form of networks are increasingly common in a number of fields, including the social sciences, biology, physics, computer science, and many others. A key task in network analysis is community detection, which typically consists of dividing the nodes into groups such that nodes within a group are strongly connected, while connections between groups are relatively scarce. A generative model well suited for the formation of such communities is the assortative stochastic block model (SBM), which prescribes a higher probability of a connection between nodes belonging to the same block rather than to different blocks. A recent line of work has utilized Bayesian nonparametric methods to recover communities in the SBM by placing a prior distribution on the number of blocks and estimating block assignments via collapsed Gibbs samplers. However, efficiently incorporating the assortativity constraint through the prior remains an open problem. In this work, we address this gap by studying the effect of enforcing assortativity on Bayesian community detection and identifying the scenarios in which it pays dividends in comparison with standard SBM. We illustrate our findings through an extensive simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19576v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Amongero, Pierpaolo De Blasi</dc:creator>
    </item>
    <item>
      <title>Testing Conditional Independence via the Spectral Generalized Covariance Measure: Beyond Euclidean Data</title>
      <link>https://arxiv.org/abs/2511.15453</link>
      <description>arXiv:2511.15453v2 Announce Type: replace 
Abstract: We propose a conditional independence (CI) test based on a new measure, the \emph{spectral generalized covariance measure} (SGCM). The SGCM is constructed by approximating the basis expansion of the squared norm of the conditional cross-covariance operator, using data-dependent bases obtained via spectral decompositions of empirical covariance operators. This construction avoids direct estimation of conditional mean embeddings and reduces the problem to scalar-valued regressions, resulting in robust finite-sample size control. Theoretically, we derive the limiting distribution of the SGCM statistic, establish the validity of a wild bootstrap for inference, and obtain uniform asymptotic size control under doubly robust conditions. As an additional contribution, we show that exponential kernels induced by continuous semimetrics of negative type are characteristic on general Polish spaces -- with extensions to finite tensor products -- thereby providing a foundation for applying our test and other kernel methods to complex objects such as distribution-valued data and curves on metric spaces. Extensive simulations indicate that the SGCM-based CI test attains near-nominal size and exhibits power competitive with or superior to state-of-the-art alternatives across a range of challenging scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15453v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryunosuke Miyazaki, Yoshimasa Uematsu</dc:creator>
    </item>
    <item>
      <title>Experimental Design under Network Interference</title>
      <link>https://arxiv.org/abs/2003.08421</link>
      <description>arXiv:2003.08421v5 Announce Type: replace-cross 
Abstract: This paper studies how to design two-wave experiments in the presence of spillovers for precise inference on treatment effects. We consider units connected through a single network, local dependence among individuals, and a general class of estimands encompassing average treatment and average spillover effects. We introduce a statistical framework for designing two-wave experiments with networks, where the researcher optimizes over participants and treatment assignments to minimize the variance of the estimators of interest, using a first-wave (pilot) experiment to estimate the variance. We derive guarantees for inference on treatment effects and regret guarantees on the variance obtained from the proposed design mechanism. Our results illustrate the existence of a trade-off in the choice of the pilot study and formally characterize the pilot's size relative to the main experiment.
  Simulations using simulated and real-world networks illustrate the advantages of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2003.08421v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Davide Viviano</dc:creator>
    </item>
    <item>
      <title>When Does Bottom-up Beat Top-down in Hierarchical Community Detection?</title>
      <link>https://arxiv.org/abs/2306.00833</link>
      <description>arXiv:2306.00833v3 Announce Type: replace-cross 
Abstract: Hierarchical clustering of networks consists in finding a tree of communities, such that lower levels of the hierarchy reveal finer-grained community structures. There are two main classes of algorithms tackling this problem. Divisive (top-down) algorithms recursively partition the nodes into two communities, until a stopping rule indicates that no further split is needed. In contrast, agglomerative (bottom-up) algorithms first identify the smallest community structure and then repeatedly merge the communities using a linkage method. In this article, we establish theoretical guarantees for the recovery of the hierarchical tree and community structure of a Hierarchical Stochastic Block Model by a bottom-up algorithm. We also establish that this bottom-up algorithm attains the information-theoretic threshold for exact recovery at intermediate levels of the hierarchy. Notably, these recovery conditions are less restrictive compared to those existing for top-down algorithms. This shows that bottom-up algorithms extend the feasible region for achieving exact recovery at intermediate levels. Numerical experiments on both synthetic and real data sets confirm the superiority of bottom-up algorithms over top-down algorithms. We also observe that top-down algorithms can produce dendrograms with inversions. These findings contribute to a better understanding of hierarchical clustering techniques and their applications in network analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.00833v3</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2025.2569711</arxiv:DOI>
      <dc:creator>Maximilien Dreveton, Daichi Kuroda, Matthias Grossglauser, Patrick Thiran</dc:creator>
    </item>
    <item>
      <title>Reviving pseudo-inverses: Asymptotic properties of large dimensional Moore-Penrose and Ridge-type inverses with applications</title>
      <link>https://arxiv.org/abs/2403.15792</link>
      <description>arXiv:2403.15792v2 Announce Type: replace-cross 
Abstract: In this paper, we derive high-dimensional asymptotic properties of the Moore-Penrose inverse and, as a byproduct, of various ridge-type inverses of the sample covariance matrix. In particular, the analytical expressions of the asymptotic behavior of the weighted sample trace moments of generalized inverse matrices are deduced in terms of the partial exponential Bell polynomials which can be easily computed in practice. The existent results for pseudo-inverses are extended in several directions: (i) First, the population covariance matrix is not assumed to be a multiple of the identity matrix; (ii) Second, the assumption of normality is not used in the derivation; (iii) Third, the asymptotic results are derived under the high-dimensional asymptotic regime. Our findings provide universal methodology for construction of fully data-driven improved shrinkage estimators of the precision matrix, optimal portfolio weights and beyond. It is found that the Moore-Penrose inverse acts asymptotically as a certain regularizer of the true covariance matrix and it seems that its proper transformation (shrinkage) performs similarly to or even outperforms the existing benchmarks in many applications, while keeping the computational time as minimal as possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15792v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taras Bodnar, Nestor Parolya</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Treatment Effects and Causal Mechanisms</title>
      <link>https://arxiv.org/abs/2404.01566</link>
      <description>arXiv:2404.01566v4 Announce Type: replace-cross 
Abstract: The credibility revolution advances the use of research designs that permit identification and estimation of causal effects. However, understanding which mechanisms produce measured causal effects remains a challenge. The dominant current approach to the quantitative evaluation of mechanisms relies on the detection of heterogeneous treatment effects (HTEs) with respect to pre-treatment covariates. This paper develops a framework to understand when the existence of such heterogeneous treatment effects can support inferences about the activation of a mechanism. We show first that this design cannot provide evidence of mechanism activation without additional, generally implicit, exclusion assumptions. Further, even when these assumptions are satisfied, the presence of HTEs supports the inference that mechanism is active but the absence of HTEs is generally uninformative about mechanism activation. We provide novel guidance for interpretation and research design in light of these findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01566v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu, Tara Slough</dc:creator>
    </item>
    <item>
      <title>Large Neighborhood and Hybrid Genetic Search for Inventory Routing Problems</title>
      <link>https://arxiv.org/abs/2506.03172</link>
      <description>arXiv:2506.03172v2 Announce Type: replace-cross 
Abstract: The inventory routing problem (IRP) focuses on jointly optimizing inventory and distribution operations from a supplier to retailers over multiple days. Compared to other problems from the vehicle routing family, the interrelations between inventory and routing decisions render IRP optimization more challenging and call for advanced solution techniques. A few studies have focused on developing large neighborhood search approaches for this class of problems, but this remains a research area with vast possibilities due to the challenges related to the integration of inventory and routing decisions. In this study, we advance this research area by developing a new large neighborhood search operator tailored for the IRP. Specifically, the operator optimally removes and reinserts all visits to a specific retailer while minimizing routing and inventory costs. We propose an efficient tailored dynamic programming algorithm that exploits preprocessing and acceleration strategies. The operator is used to build an effective local search routine, and included in a state-of-the-art routing algorithm, i.e., Hybrid Genetic Search (HGS). Through extensive computational experiments, we demonstrate that the resulting heuristic algorithm leads to solutions of unmatched quality up to this date, especially on large-scale benchmark instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03172v2</guid>
      <category>cs.NE</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyi Zhao, Claudia Archetti, Tuan Anh Pham, Thibaut Vidal</dc:creator>
    </item>
  </channel>
</rss>
