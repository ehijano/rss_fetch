<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Apr 2025 02:48:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Statistical Validation in Cultural Adaptations of Cognitive Tests: A Multi- Regional Systematic Review</title>
      <link>https://arxiv.org/abs/2504.13495</link>
      <description>arXiv:2504.13495v1 Announce Type: new 
Abstract: This systematic review discusses the methodological approaches and statistical confirmations of cross-cultural adaptations of cognitive evaluation tools used with different populations. The review considers six seminal studies on the methodology of cultural adaptation in Europe, Asia, Africa, and South America. The results indicate that proper adaptations need holistic models with demographic changes, and education explained as much as 26.76% of the variance in MoCA-H scores. Cultural-linguistic factors explained 6.89% of the variance in European adaptations of MoCA-H; however, another study on adapted MMSE and BCSB among Brazilian Indigenous populations reported excellent diagnostic performance, with a sensitivity of 94.4% and specificity of 99.2%. There was 78.5% inter-rater agreement on the evaluation of cultural adaptation using the Manchester Translation Evaluation Checklist. A paramount message of the paper is that community feedback is necessary for culturally appropriate preparation, standardized translation protocols also must be included, along with robust statistical validation methodologies for developing cognitive assessment instruments. This review supplies evidence-based frameworks for the further adaptation of cognitive assessments in increasingly diverse global health settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13495v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miit Daga, Priyasha Mohanty, Ram Krishna, Swarna Priya RM</dc:creator>
    </item>
    <item>
      <title>Four Bottomless Errors and the Collapse of Statistical Fairness</title>
      <link>https://arxiv.org/abs/2504.13790</link>
      <description>arXiv:2504.13790v1 Announce Type: new 
Abstract: The AI ethics of statistical fairness is an error, the approach should be abandoned, and the accumulated academic work deleted. The argument proceeds by identifying four recurring mistakes within statistical fairness. One conflates fairness with equality, which confines thinking to similars being treated similarly. The second and third errors derive from a perspectival ethical view which functions by negating others and their viewpoints. The final mistake constrains fairness to work within predefined social groups instead of allowing unconstrained fairness to subsequently define group composition. From the nature of these misconceptions, the larger argument follows. Because the errors are integral to how statistical fairness works, attempting to resolve the difficulties only deepens them. Consequently, the errors cannot be corrected without undermining the larger project, and statistical fairness collapses from within. While the collapse ends a failure in ethics, it also provokes distinct possibilities for fairness, data, and algorithms. Quickly indicating some of these directions is a secondary aim of the paper, and one that aligns with what fairness has consistently meant and done since Aristotle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13790v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.36548.95367</arxiv:DOI>
      <dc:creator>James Brusseau (Philosophy,Computer Science, Pace University, New York City, USA,Department of Information Engineering,Computer Science. University of Trento, Italy)</dc:creator>
    </item>
    <item>
      <title>Audit Cards: Contextualizing AI Evaluations</title>
      <link>https://arxiv.org/abs/2504.13839</link>
      <description>arXiv:2504.13839v1 Announce Type: new 
Abstract: AI governance frameworks increasingly rely on audits, yet the results of their underlying evaluations require interpretation and context to be meaningfully informative. Even technically rigorous evaluations can offer little useful insight if reported selectively or obscurely. Current literature focuses primarily on technical best practices, but evaluations are an inherently sociotechnical process, and there is little guidance on reporting procedures and context. Through literature review, stakeholder interviews, and analysis of governance frameworks, we propose "audit cards" to make this context explicit. We identify six key types of contextual features to report and justify in audit cards: auditor identity, evaluation scope, methodology, resource access, process integrity, and review mechanisms. Through analysis of existing evaluation reports, we find significant variation in reporting practices, with most reports omitting crucial contextual information such as auditors' backgrounds, conflicts of interest, and the level and type of access to models. We also find that most existing regulations and frameworks lack guidance on rigorous reporting. In response to these shortcomings, we argue that audit cards can provide a structured format for reporting key claims alongside their justifications, enhancing transparency, facilitating proper interpretation, and establishing trust in reporting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13839v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leon Staufer, Mick Yang, Anka Reuel, Stephen Casper</dc:creator>
    </item>
    <item>
      <title>I Know What You Bought Last Summer: Investigating User Data Leakage in E-Commerce Platforms</title>
      <link>https://arxiv.org/abs/2504.13212</link>
      <description>arXiv:2504.13212v1 Announce Type: cross 
Abstract: In the digital age, e-commerce has transformed the way consumers shop, offering convenience and accessibility. Nevertheless, concerns about the privacy and security of personal information shared on these platforms have risen. In this work, we investigate user privacy violations, noting the risks of data leakage to third-party entities. Utilizing a semi-automated data collection approach, we examine a selection of popular online e-shops, revealing that nearly 30% of them violate user privacy by disclosing personal information to third parties. We unveil how minimal user interaction across multiple e-commerce websites can result in a comprehensive privacy breach. We observe significant data-sharing patterns with platforms like Facebook, which use personal information to build user profiles and link them to social media accounts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13212v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Vlachogiannakis, Emmanouil Papadogiannakis, Panagiotis Papadopoulos, Nicolas Kourtellis, Evangelos Markatos</dc:creator>
    </item>
    <item>
      <title>CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models</title>
      <link>https://arxiv.org/abs/2504.13261</link>
      <description>arXiv:2504.13261v1 Announce Type: cross 
Abstract: Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT has significantly impacted foreign language education, yet their pedagogical grammar competence remains under-assessed. This paper introduces CPG-EVAL, the first dedicated benchmark specifically designed to evaluate LLMs' knowledge of pedagogical grammar within the context of foreign language instruction. Methodology: The benchmark comprises five tasks designed to assess grammar recognition, fine-grained grammatical distinction, categorical discrimination, and resistance to linguistic interference. Findings: Smaller-scale models can succeed in single language instance tasks, but struggle with multiple instance tasks and interference from confusing instances. Larger-scale models show better resistance to interference but still have significant room for accuracy improvement. The evaluation indicates the need for better instructional alignment and more rigorous benchmarks, to effectively guide the deployment of LLMs in educational contexts. Value: This study offers the first specialized, theory-driven, multi-tiered benchmark framework for systematically evaluating LLMs' pedagogical grammar competence in Chinese language teaching contexts. CPG-EVAL not only provides empirical insights for educators, policymakers, and model developers to better gauge AI's current abilities in educational settings, but also lays the groundwork for future research on improving model alignment, enhancing educational suitability, and ensuring informed decision-making concerning LLM integration in foreign language instruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13261v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dong Wang</dc:creator>
    </item>
    <item>
      <title>Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces</title>
      <link>https://arxiv.org/abs/2504.13277</link>
      <description>arXiv:2504.13277v1 Announce Type: cross 
Abstract: Suicide is a critical global public health issue, with millions experiencing suicidal ideation (SI) each year. Online spaces enable individuals to express SI and seek peer support. While prior research has revealed the potential of detecting SI using machine learning and natural language analysis, a key limitation is the lack of a theoretical framework to understand the underlying factors affecting high-risk suicidal intent. To bridge this gap, we adopted the Interpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607 posts from Reddit's r/SuicideWatch, categorizing them into SI dimensions (Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk factors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired Capability of Suicide). We found that high-risk SI posts express planning and attempts, methods and tools, and weaknesses and pain. In addition, we also examined the language of supportive responses through psycholinguistic and content analyses to find that individuals respond differently to different stages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI chatbots in providing effective supportive responses to suicidal ideation posts. We found that although AI improved structural coherence, expert evaluations highlight persistent shortcomings in providing dynamic, personalized, and deeply empathetic support. These findings underscore the need for careful reflection and deeper understanding in both the development and consideration of AI-driven interventions for effective mental health support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13277v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soorya Ram Shimgekar, Violeta J. Rodriguez, Paul A. Bloom, Dong Whi Yoo, Koustuv Saha</dc:creator>
    </item>
    <item>
      <title>The Impact of AI on the Cyber Offense-Defense Balance and the Character of Cyber Conflict</title>
      <link>https://arxiv.org/abs/2504.13371</link>
      <description>arXiv:2504.13371v1 Announce Type: cross 
Abstract: Unlike other domains of conflict, and unlike other fields with high anticipated risk from AI, the cyber domain is intrinsically digital with a tight feedback loop between AI training and cyber application. Cyber may have some of the largest and earliest impacts from AI, so it is important to understand how the cyber domain may change as AI continues to advance. Our approach reviewed the literature, collecting nine arguments that have been proposed for offensive advantage in cyber conflict and nine proposed arguments for defensive advantage. We include an additional forty-eight arguments that have been proposed to give cyber conflict and competition its character as collected separately by Healey, Jervis, and Nandrajog. We then consider how each of those arguments and propositions might change with varying degrees of AI advancement. We find that the cyber domain is too multifaceted for a single answer to whether AI will enhance offense or defense broadly. AI will improve some aspects, hinder others, and leave some aspects unchanged. We collect and present forty-four ways that we expect AI to impact the cyber offense-defense balance and the character of cyber conflict and competition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13371v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew J. Lohn</dc:creator>
    </item>
    <item>
      <title>Understanding Adolescents' Perceptions of Benefits and Risks in Health AI Technologies through Design Fiction</title>
      <link>https://arxiv.org/abs/2504.13389</link>
      <description>arXiv:2504.13389v1 Announce Type: cross 
Abstract: Despite the growing research on users' perceptions of health AI, adolescents' perspectives remain underexplored. This study explores adolescents' perceived benefits and risks of health AI technologies in clinical and personal health settings. Employing Design Fiction, we conducted interviews with 16 adolescents (aged 13-17) using four fictional design scenarios that represent current and future health AI technologies as probes. Our findings reveal that with a positive yet cautious attitude, adolescents envision unique benefits and risks specific to their age group. While health AI technologies were seen as valuable learning resources, they also raised concerns about confidentiality with their parents. Additionally, we identified several factors, such as severity of health conditions and previous experience with AI, influencing their perceptions of trust and privacy in health AI. We explore how these insights can inform the future of design of health AI technologies to support learning, engagement, and trust as adolescents navigate their healthcare journey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13389v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jamie Lee, Kyuha Jung, Erin Gregg Newman, Emilie Chow, Yunan Chen</dc:creator>
    </item>
    <item>
      <title>Bitcoin's Edge: Embedded Sentiment in Blockchain Transactional Data</title>
      <link>https://arxiv.org/abs/2504.13598</link>
      <description>arXiv:2504.13598v1 Announce Type: cross 
Abstract: Cryptocurrency blockchains, beyond their primary role as distributed payment systems, are increasingly used to store and share arbitrary content, such as text messages and files. Although often non-financial, this hidden content can impact price movements by conveying private information, shaping sentiment, and influencing public opinion. However, current analyses of such data are limited in scope and scalability, primarily relying on manual classification or hand-crafted heuristics. In this work, we address these limitations by employing Natural Language Processing techniques to analyze, detect patterns, and extract public sentiment encoded within blockchain transactional data. Using a variety of Machine Learning techniques, we showcase for the first time the predictive power of blockchain-embedded sentiment in forecasting cryptocurrency price movements on the Bitcoin and Ethereum blockchains. Our findings shed light on a previously underexplored source of freely available, transparent, and immutable data and introduce blockchain sentiment analysis as a novel and robust framework for enhancing financial predictions in cryptocurrency markets. Incidentally, we discover an asymmetry between cryptocurrencies; Bitcoin has an informational advantage over Ethereum in that the sentiment embedded into transactional data is sufficient to predict its price movement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13598v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charalampos Kleitsikas, Nikolaos Korfiatis, Stefanos Leonardos, Carmine Ventre</dc:creator>
    </item>
    <item>
      <title>Propagational Proxy Voting</title>
      <link>https://arxiv.org/abs/2504.13641</link>
      <description>arXiv:2504.13641v1 Announce Type: cross 
Abstract: This paper proposes a voting process in which voters allocate fractional votes to their expected utility in different domains: over proposals, other participants, and sets containing proposals and participants. This approach allows for a more nuanced expression of preferences by calculating the result and relevance within each node. We modeled this by creating a voting matrix that reflects their preference. We use absorbing Markov chains to gain the consensus, and also calculate the influence within the participating nodes. We illustrate this method in action through an experiment with 69 students using a budget allocation topic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13641v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>econ.TH</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasushi Sakai, Parfait Atchade-Adelomou, Ryan Jiang, Luis Alonso, Kent Larson, Ken Suzuki</dc:creator>
    </item>
    <item>
      <title>A Survey for What Developers Require in AI-powered Tools that Aid in Component Selection in CBSD</title>
      <link>https://arxiv.org/abs/2504.13751</link>
      <description>arXiv:2504.13751v1 Announce Type: cross 
Abstract: Although it has been more than four decades that the first components-based software development (CBSD) studies were conducted, there is still no standard method or tool for component selection which is widely accepted by the industry. The gulf between industry and academia contributes to the lack of an accepted tool. We conducted a mixed methods survey of nearly 100 people engaged in component-based software engineering practice or research to better understand the problems facing industry, how these needs could be addressed, and current best practices employed in component selection. We also sought to identify and prioritize quality criteria for component selection from an industry perspective. In response to the call for CBSD component selection tools to incorporate recent technical advances, we also explored the perceptions of professionals about AI-driven tools, present and envisioned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13751v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The 29th International Conference on Evaluation and Assessment in Software Engineering 17 to 20 June 2025 Istanbul Turkey</arxiv:journal_reference>
      <dc:creator>Mahdi Jaberzadeh Ansari, Ann Barcomb</dc:creator>
    </item>
    <item>
      <title>United in Diversity? Contextual Biases in LLM-Based Predictions of the 2024 European Parliament Elections</title>
      <link>https://arxiv.org/abs/2409.09045</link>
      <description>arXiv:2409.09045v2 Announce Type: replace 
Abstract: "Synthetic samples" based on large language models (LLMs) have been argued to serve as efficient alternatives to surveys of humans, assuming that their training data includes information on human attitudes and behavior. However, LLM-synthetic samples might exhibit bias, for example due to training data and fine-tuning processes being unrepresentative of diverse contexts. Such biases risk reinforcing existing biases in research, policymaking, and society. Therefore, researchers need to investigate if and under which conditions LLM-generated synthetic samples can be used for public opinion prediction. In this study, we examine to what extent LLM-based predictions of individual public opinion exhibit context-dependent biases by predicting the results of the 2024 European Parliament elections. Prompting three LLMs with individual-level background information of 26,000 eligible European voters, we ask the LLMs to predict each person's voting behavior. By comparing them to the actual results, we show that LLM-based predictions of future voting behavior largely fail, their accuracy is unequally distributed across national and linguistic contexts, and they require detailed attitudinal information in the prompt. The findings emphasize the limited applicability of LLM-synthetic samples to public opinion prediction. In investigating their contextual biases, this study contributes to the understanding and mitigation of inequalities in the development of LLMs and their applications in computational social science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09045v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Leah von der Heyde, Anna-Carolina Haensch, Alexander Wenz, Bolei Ma</dc:creator>
    </item>
    <item>
      <title>Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant</title>
      <link>https://arxiv.org/abs/2501.17176</link>
      <description>arXiv:2501.17176v2 Announce Type: replace 
Abstract: The dream of achieving a student-teacher ratio of 1:1 is closer than ever thanks to the emergence of large language models (LLMs). One potential application of these models in the educational field would be to provide feedback to students in university introductory programming courses, so that a student struggling to solve a basic implementation problem could seek help from an LLM available 24/7. This article focuses on studying three aspects related to such an application. First, the performance of two well-known models, GPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The empirical results showed that GPT-4T performs much better than GPT-3.5T, however, it is not yet ready for use in a real-world scenario. This is due to the possibility of generating incorrect information that potential users may not always be able to detect. Second, the article proposes a carefully designed prompt using in-context learning techniques that allows automating important parts of the evaluation process, as well as providing a lower bound for the fraction of feedbacks containing incorrect information, saving time and effort. This was possible because the resulting feedback has a programmatically analyzable structure that incorporates diagnostic information about the LLM's performance in solving the requested task. Third, the article also suggests a possible strategy for implementing a practical learning tool based on LLMs, which is rooted on the proposed prompting techniques. This strategy opens up a whole range of interesting possibilities from a pedagogical perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17176v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Ballestero-Rib\'o, Daniel Ortiz-Mart\'inez</dc:creator>
    </item>
    <item>
      <title>Pitfalls of Evidence-Based AI Policy</title>
      <link>https://arxiv.org/abs/2502.09618</link>
      <description>arXiv:2502.09618v4 Announce Type: replace 
Abstract: Nations across the world are working to govern AI. However, from a technical perspective, there is uncertainty and disagreement on the best way to do this. Meanwhile, recent debates over AI regulation have led to calls for "evidence-based AI policy" which emphasize holding regulatory action to a high evidentiary standard. Evidence is of irreplaceable value to policymaking. However, holding regulatory action to too high an evidentiary standard can lead to systematic neglect of certain risks. In historical policy debates (e.g., over tobacco ca. 1965 and fossil fuels ca. 1985) "evidence-based policy" rhetoric is also a well-precedented strategy to downplay the urgency of action, delay regulation, and protect industry interests. Here, we argue that if the goal is evidence-based AI policy, the first regulatory objective must be to actively facilitate the process of identifying, studying, and deliberating about AI risks. We discuss a set of 15 regulatory goals to facilitate this and show that Brazil, Canada, China, the EU, South Korea, the UK, and the USA all have substantial opportunities to adopt further evidence-seeking policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09618v4</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Casper, David Krueger, Dylan Hadfield-Menell</dc:creator>
    </item>
    <item>
      <title>Assessing Judging Bias in Large Reasoning Models: An Empirical Study</title>
      <link>https://arxiv.org/abs/2504.09946</link>
      <description>arXiv:2504.09946v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have demonstrated remarkable reasoning capabilities, raising important questions about their biases in LLM-as-a-judge settings. We present a comprehensive benchmark comparing judging biases between LLMs and LRMs across both subjective preference-alignment datasets and objective fact-based datasets. Through investigation of bandwagon, authority, position, and distraction biases, we uncover four key findings: (1) despite their advanced reasoning capabilities, LRMs remain susceptible to the above biases; (2) LRMs demonstrate better robustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit notable position bias, preferring options in later positions; and (4) we identify a novel "superficial reflection bias" where phrases mimicking reasoning (e.g., "wait, let me think...") significantly influence model judgments. To address these biases, we design and evaluate three mitigation strategies: specialized system prompts that reduce judging biases by up to 19\% in preference alignment datasets and 14\% in fact-related datasets, in-context learning that provides up to 27\% improvement on preference tasks but shows inconsistent results on factual tasks, and a self-reflection mechanism that reduces biases by up to 10\% in preference datasets and 16\% in fact-related datasets, with self-reflection proving particularly effective for LRMs. Our work provides crucial insights for developing more reliable LLM-as-a-Judge frameworks, especially as LRMs become increasingly deployed as automated judges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09946v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qian Wang, Zhanzhi Lou, Zhenheng Tang, Nuo Chen, Xuandong Zhao, Wenxuan Zhang, Dawn Song, Bingsheng He</dc:creator>
    </item>
    <item>
      <title>Large Language Model-Based Knowledge Graph System Construction for Sustainable Development Goals: An AI-Based Speculative Design Perspective</title>
      <link>https://arxiv.org/abs/2504.12309</link>
      <description>arXiv:2504.12309v2 Announce Type: replace 
Abstract: From 2000 to 2015, the UN's Millennium Development Goals guided global priorities. The subsequent Sustainable Development Goals (SDGs) adopted a more dynamic approach, with annual indicator updates. As 2030 nears and progress lags, innovative acceleration strategies are critical. This study develops an AI-powered knowledge graph system to analyze SDG interconnections, discover potential new goals, and visualize them online. Using official SDG texts, Elsevier's keyword dataset, and 1,127 TED Talk transcripts (2020.01-2024.04), a pilot on 269 talks from 2023 applies AI-speculative design, large language models, and retrieval-augmented generation. Key findings include: (1) Heatmap analysis reveals strong associations between Goal 10 and Goal 16, and minimal coverage of Goal 6. (2) In the knowledge graph, simulated dialogue over time reveals new central nodes, showing how richer data supports divergent thinking and goal clarity. (3) Six potential new goals are proposed, centered on equity, resilience, and technology-driven inclusion. This speculative-AI framework offers fresh insights for policymakers and lays groundwork for future multimodal and cross-system SDG applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12309v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi-De Lin, Guan-Ze Liao</dc:creator>
    </item>
    <item>
      <title>Argumentative Experience: Reducing Confirmation Bias on Controversial Issues through LLM-Generated Multi-Persona Debates</title>
      <link>https://arxiv.org/abs/2412.04629</link>
      <description>arXiv:2412.04629v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are enabling designers to give life to exciting new user experiences for information access. In this work, we present a system that generates LLM personas to debate a topic of interest from different perspectives. How might information seekers use and benefit from such a system? Can centering information access around diverse viewpoints help to mitigate thorny challenges like confirmation bias in which information seekers over-trust search results matching existing beliefs? How do potential biases and hallucinations in LLMs play out alongside human users who are also fallible and possibly biased?
  Our study exposes participants to multiple viewpoints on controversial issues via a mixed-methods, within-subjects study. We use eye-tracking metrics to quantitatively assess cognitive engagement alongside qualitative feedback. Compared to a baseline search system, we see more creative interactions and diverse information-seeking with our multi-persona debate system, which more effectively reduces user confirmation bias and conviction toward their initial beliefs. Overall, our study contributes to the emerging design space of LLM-based information access systems, specifically investigating the potential of simulated personas to promote greater exposure to information diversity, emulate collective intelligence, and mitigate bias in information seeking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04629v3</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Li Shi, Houjiang Liu, Yian Wong, Utkarsh Mujumdar, Dan Zhang, Jacek Gwizdka, Matthew Lease</dc:creator>
    </item>
    <item>
      <title>Psycholinguistic Analyses in Software Engineering Text: A Systematic Literature Review</title>
      <link>https://arxiv.org/abs/2503.05992</link>
      <description>arXiv:2503.05992v2 Announce Type: replace-cross 
Abstract: Context: A deeper understanding of human factors in software engineering (SE) is essential for improving team collaboration, decision-making, and productivity. Communication channels like code reviews and chats provide insights into developers' psychological and emotional states. While large language models excel at text analysis, they often lack transparency and precision. Psycholinguistic tools like Linguistic Inquiry and Word Count (LIWC) offer clearer, interpretable insights into cognitive and emotional processes exhibited in text. Despite its wide use in SE research, no comprehensive review of LIWC's use has been conducted. Objective: We examine the importance of psycholinguistic tools, particularly LIWC, and provide a thorough analysis of its current and potential future applications in SE research. Methods: We conducted a systematic review of six prominent databases, identifying 43 SE-related papers using LIWC. Our analysis focuses on five research questions. Results: Our findings reveal a wide range of applications, including analyzing team communication to detect developer emotions and personality, developing ML models to predict deleted Stack Overflow posts, and more recently comparing AI-generated and human-written text. LIWC has been primarily used with data from project management platforms (e.g., GitHub) and Q&amp;A forums (e.g., Stack Overflow). Key BSE concepts include Communication, Organizational Climate, and Positive Psychology. 26 of 43 papers did not formally evaluate LIWC. Concerns were raised about some limitations, including difficulty handling SE-specific vocabulary. Conclusion: We highlight the potential of psycholinguistic tools and their limitations, and present new use cases for advancing the research of human factors in SE (e.g., bias in human-LLM conversations).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05992v2</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amirali Sajadi, Kostadin Damevski, Preetha Chatterjee</dc:creator>
    </item>
    <item>
      <title>A Dataset of the Representatives Elected in France During the Fifth Republic</title>
      <link>https://arxiv.org/abs/2504.02869</link>
      <description>arXiv:2504.02869v2 Announce Type: replace-cross 
Abstract: The electoral system is a cornerstone of democracy, shaping the structure of political competition, representation, and accountability. In the case of France, it is difficult to access data describing elected representatives, though, as they are scattered across a number of sources, including public institutions, but also academic and individual efforts. This article presents a unified relational database that aims at tackling this issue by gathering information regarding representatives elected in France over the whole Fifth Republic (1958-present). This database constitutes an unprecedented resource for analyzing the evolution of political representation in France, exploring trends in party system dynamics, gender equality, and the professionalization of politics. By providing a longitudinal view of French elected representatives, the database facilitates research on the institutional stability of the Fifth Republic, offering insights into the factors of political change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02869v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.dib.2025.111542</arxiv:DOI>
      <arxiv:journal_reference>Data in Brief, 2025, 60, pp.111542</arxiv:journal_reference>
      <dc:creator>No\'emie F\'evrat (FR 3621, JPEG), Vincent Labatut (LIA), \'Emilie Volpi (FR 3621), Guillaume Marrel (JPEG)</dc:creator>
    </item>
  </channel>
</rss>
