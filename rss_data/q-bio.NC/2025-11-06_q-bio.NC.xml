<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Nov 2025 02:35:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>FAPEX: Fractional Amplitude-Phase Expressor for Robust Cross-Subject Seizure Prediction</title>
      <link>https://arxiv.org/abs/2511.03263</link>
      <description>arXiv:2511.03263v1 Announce Type: new 
Abstract: Precise, generalizable subject-agnostic seizure prediction (SASP) remains a fundamental challenge due to the intrinsic complexity and significant spectral variability of electrophysiological signals across individuals and recording modalities. We propose FAPEX, a novel architecture that introduces a learnable fractional neural frame operator (FrNFO) for adaptive time-frequency decomposition. Unlike conventional models that exhibit spectral bias toward low frequencies, our FrNFO employs fractional-order convolutions to capture both high and low-frequency dynamics, achieving approximately 10% improvement in F1-score and sensitivity over state-of-the-art baselines. The FrNFO enables the extraction of instantaneous phase and amplitude representations that are particularly informative for preictal biomarker discovery and enhance out-of-distribution generalization. FAPEX further integrates structural state-space modeling and channelwise attention, allowing it to handle heterogeneous electrode montages. Evaluated across 12 benchmarks spanning species (human, rat, dog, macaque) and modalities (Scalp-EEG, SEEG, ECoG, LFP), FAPEX consistently outperforms 23 supervised and 10 self-supervised baselines under nested cross-validation, with gains of up to 15% in sensitivity on complex cross-domain scenarios. It further demonstrates superior performance in several external validation cohorts. To our knowledge, these establish FAPEX as the first epilepsy model to show consistent superiority in SASP, offering a promising solution for discovering epileptic biomarker evidence supporting the existence of a distinct and identifiable preictal state and clinical translation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03263v1</guid>
      <category>q-bio.NC</category>
      <category>eess.SP</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ruizhe Zheng, Lingyan Mao, Dingding Han, Tian Luo, Yi Wang, Jing Ding, Yuguo Yu</dc:creator>
    </item>
    <item>
      <title>Emergent tuning heterogeneity in cortical circuits is sensitive to cellular neuronal dynamics</title>
      <link>https://arxiv.org/abs/2511.03502</link>
      <description>arXiv:2511.03502v1 Announce Type: new 
Abstract: Cortical circuits exhibit high levels of response diversity, even across apparently uniform neuronal populations. While emerging data-driven approaches exploit this heterogeneity to infer effective models of cortical circuit computation (e.g. Genkin et al. Nature 2025), the power of response diversity to enable inference of mechanistic circuit models is largely unexplored. Within the landscape of cortical circuit models, spiking neuron networks in the balanced state naturally exhibit high levels of response and tuning diversity emerging from their internal dynamics. A statistical theory for this emergent tuning heterogeneity, however, has only been formulated for binary spin models (Vreeswijk &amp; Sompolinsky, 2005). Here we present a formulation of feature-tuned balanced state networks that allows for arbitrary and diverse dynamics of postsynaptic currents and variable levels of heterogeneity in cellular excitability but nevertheless is analytically exactly tractable with respect to the emergent tuning curve heterogeneity. Using this framework, we present a case study demonstrating that, for a wide range of parameters even the population mean response is non-universal and sensitive to mechanistic circuit details. As our theory enables exactly and analytically obtaining the likelihood-function of tuning heterogeneity given circuit parameters, we argue that it forms a powerful and rigorous basis for neural circuit inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03502v1</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammadreza Soltanipour, Stefan Treue, Fred Wolf</dc:creator>
    </item>
    <item>
      <title>Beta frequency shifts in decision making: Spectral fingerprints or communication channels?</title>
      <link>https://arxiv.org/abs/2511.03503</link>
      <description>arXiv:2511.03503v1 Announce Type: new 
Abstract: Recent evidence suggests that beta-band activity plays a key role in decision-making. Here we review our recent work in humans and non-human primates showing that beta-band frequency shifts in frontal cortex signal categorical decision outcomes. We revisit our previous proposal suggesting that content-specific beta reflects the flexible recruiting of transient neural ensembles and update it to emphasize frequency as the relevant parameter. We argue that beta frequency shifts arise from changes in connectivity between weakly coupled oscillators and that, more than a spectral fingerprint, they reflect an active mechanism to (re)-activate behaviorally relevant communication channels in the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03503v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Saskia Haegens, Julio Rodriguez-Larios, Elie Rassi</dc:creator>
    </item>
    <item>
      <title>Explaining Human Choice Probabilities with Simple Vector Representations</title>
      <link>https://arxiv.org/abs/2511.03643</link>
      <description>arXiv:2511.03643v1 Announce Type: new 
Abstract: When people pursue rewards in stochastic environments, they often match their choice frequencies to the observed target frequencies, even when this policy is demonstrably sub-optimal. We used a ``hide and seek'' task to evaluate this behavior under conditions where pursuit (seeking) could be toggled to avoidance (hiding), while leaving the probability distribution fixed, or varying complexity by changing the number of possible choices. We developed a model for participant choice built from choice frequency histograms treated as vectors. We posited the existence of a probability antimatching strategy for avoidance (hiding) rounds, and formalized this as a vector reflection of probability matching. We found that only two basis policies: matching/antimatching and maximizing/minimizing were sufficient to account for participant choices across a range of room numbers and opponent probability distributions. This schema requires only that people have the ability to remember the relative frequency of the different outcomes. With this knowledge simple operations can construct the maximizing and minimizing policies as well as matching and antimatching strategies. A mixture of these two policies captures human choice patterns in a stochastic environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03643v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter DiBerardino, Britt Anderson</dc:creator>
    </item>
    <item>
      <title>Final state sensitivity and fractal basin boundaries from coupled Chialvo neurons</title>
      <link>https://arxiv.org/abs/2511.03671</link>
      <description>arXiv:2511.03671v1 Announce Type: cross 
Abstract: We investigate and quantify the basin geometry and extreme final state uncertainty of two identical electrically asymmetrically coupled Chialvo neurons. The system's diverse behaviors are presented, along with the mathematical reasoning behind its chaotic and nonchaotic dynamics as determined by the structure of the coupled equations. The system is found to be multistable with two qualitatively different attractors. Although each neuron is individually nonchaotic, the chaotic basin takes up the vast majority of the coupled system's state space, but the nonchaotic basin stretches to infinity due to chance synchronization. The boundary between the basins is found to be fractal, leading to extreme final state sensitivity. This uncertainty and its potential effect on the synchronization of biological neurons may have significant implications for understanding human behavior and neurological disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03671v1</guid>
      <category>nlin.CD</category>
      <category>math.DS</category>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bennett Lamb, Brandon B. Le</dc:creator>
    </item>
    <item>
      <title>A 7T fMRI dataset of synthetic images for out-of-distribution modeling of vision</title>
      <link>https://arxiv.org/abs/2503.06286</link>
      <description>arXiv:2503.06286v3 Announce Type: replace 
Abstract: Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD) are boosting computational neuroscience research by enabling models of the brain with performances beyond what was possible just a decade ago. However, because the stimuli of these datasets typically live within a common naturalistic visual distribution, they do not allow for strict out-of-distribution (OOD) generalization tests which are crucial for the development of more robust models. Here, we address this limitation by releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the same eight NSD participants for 284 synthetic images. We show that NSD-synthetic's fMRI responses reliably encode stimulus-related information and are OOD with respect to NSD. Furthermore, we provide a proof of principle that OOD generalization tests on NSD-synthetic reveal differences between models of the brain that are not detected with the original NSD data; we demonstrate that the degree of OOD (quantified as the distance between a set of responses and the training data used for modeling) is predictive of the magnitude of model failures; and we show that less strict OOD generalization tests can can be usefully applied even within the domain of naturalistic stimuli. These results showcase how NSD-synthetic enables OOD generalization tests that facilitate the development of more robust models of visual processing and the formulation of more accurate theories of human vision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06286v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alessandro T. Gifford, Radoslaw M. Cichy, Thomas Naselaris, Kendrick Kay</dc:creator>
    </item>
    <item>
      <title>Response function as a quantitative measure of consciousness in brain dynamics</title>
      <link>https://arxiv.org/abs/2509.00730</link>
      <description>arXiv:2509.00730v2 Announce Type: replace 
Abstract: Understanding the neural correlates of consciousness remains a central challenge in neuroscience. In this study, we investigate the relationship between consciousness and neural responsiveness by analyzing intracranial ECoG recordings from non-human primates across three distinct states: wakefulness, anesthesia, and recovery. Using a nonequilibrium recurrent neural network (RNN) model, we fit state-dependent cortical dynamics to extract the neural response function as a dynamics complexity indicator. Our findings demonstrate that the amplitude of the neural response function serves as a robust dynamical indicator of conscious state, consistent with the role of a linear response function in statistical physics. Notably, this aligns with our previous theoretical results showing that the response function in RNNs peaks near the transition between ordered and chaotic regimes -- highlighting criticality as a potential principle for sustaining flexible and responsive cortical dynamics. Empirically, we find that during wakefulness, neural responsiveness is strong, widely distributed, and consistent with rich nonequilibrium fluctuations. Under anesthesia, response amplitudes are significantly suppressed, and the network dynamics become more chaotic, indicating a loss of dynamical sensitivity. During recovery, the neural response function is elevated, supporting the gradual re-establishment of flexible and responsive activity that parallels the restoration of conscious processing. Our work suggests that a robust, brain-state-dependent neural response function may be a necessary dynamical condition for consciousness, providing a principled framework for quantifying levels of consciousness in terms of nonequilibrium responsiveness in the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00730v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.stat-mech</category>
      <category>nlin.CD</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenkang Du, Haiping Huang</dc:creator>
    </item>
    <item>
      <title>Personalized Transcranial Electrical Stimulation: A Review of Computational Modeling and Optimization</title>
      <link>https://arxiv.org/abs/2509.01192</link>
      <description>arXiv:2509.01192v2 Announce Type: replace 
Abstract: Objective. Personalized transcranial electrical stimulation (tES) has gained growing attention due to the substantial inter-individual variability in brain anatomy and physiology. While previous reviews have discussed the physiological mechanisms and clinical applications of tES, there remains a critical gap in up-to-date syntheses focused on the computational modeling frameworks that enable individualized stimulation optimization. Approach. This review presents a comprehensive overview of recent advances in computational techniques supporting personalized tES. We systematically examine developments in forward modeling for simulating individualized electric fields, as well as inverse modeling approaches for optimizing stimulation parameters. We critically evaluate progress in head modeling pipelines, optimization algorithms, and the integration of multimodal brain data. Main results. Recent advances have substantially accelerated the construction of subject-specific head conductor models and expanded the landscape of optimization methods, including multi-objective optimization and brain network-informed optimization. These advances allow for dynamic and individualized stimulation planning, moving beyond empirical trial-and-error approaches.Significance. By integrating the latest developments in computational modeling for personalized tES, this review highlights current challenges, emerging opportunities, and future directions for achieving precision neuromodulation in both research and clinical contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01192v2</guid>
      <category>q-bio.NC</category>
      <category>cs.CE</category>
      <category>cs.NE</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mo Wang, Kexin Zheng, Yingyue Xin, Xiang Chen, Yiling Liu, Huichun Luo, Jingsheng Tang, Tifei Yuan, Hongkai Wen, Pengfei Wei, Quanying Liu</dc:creator>
    </item>
    <item>
      <title>State of Brain Emulation Report 2025</title>
      <link>https://arxiv.org/abs/2510.15745</link>
      <description>arXiv:2510.15745v3 Announce Type: replace 
Abstract: The State of Brain Emulation Report 2025 provides a comprehensive reassessment of the field's progress since Sandberg and Bostrom's 2008 Whole Brain Emulation roadmap. The report is organized around three core capabilities required for brain emulation: recording brain function (Neural Dynamics), mapping brain structure (Connectomics), and emulation and embodiment (Computational Neuroscience). It also identifies ongoing challenges and outlines strategic priorities to help the field move forward.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15745v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niccol\`o Zanichelli, Maximilian Schons, Isaak Freeman, Philip Shiu, Anton Arkhipov</dc:creator>
    </item>
    <item>
      <title>Microbes in the Moonlight: How the Gut Microbiota Influences Sleep</title>
      <link>https://arxiv.org/abs/2511.02766</link>
      <description>arXiv:2511.02766v2 Announce Type: replace 
Abstract: The gut microbiota has emerged as a fundamental regulator of sleep physiology, influencing neural, endocrine, and immune pathways through the gut-microbiota-brain axis (GMBA). This bidirectional communication system modulates neurotransmitter production, circadian rhythms, and metabolic homeostasis, while disruptions in microbial composition have been linked to sleep disorders, neuroinflammation, and systemic immune dysfunction. Recent findings suggest that gut dysbiosis contributes to sleep disturbances by altering serotonin, GABA, and short-chain fatty acid (SCFA) metabolism, with implications for neurodegenerative diseases, metabolic syndromes, and mood disorders. Additionally, the gut microbiota interacts with the endocrine and immune systems, shaping inflammatory responses and stress adaptation mechanisms. This review explores the intricate connections between sleep and the gut microbiota, integrating emerging research on microbiota-targeted therapies, such as probiotics, fecal microbiota transplantation (FMT), and chrononutrition, as potential interventions to restore sleep homeostasis and improve health outcomes</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02766v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enso Onill Torres Alegre</dc:creator>
    </item>
    <item>
      <title>High-dimensional dynamics in low-dimensional networks</title>
      <link>https://arxiv.org/abs/2504.13727</link>
      <description>arXiv:2504.13727v3 Announce Type: replace-cross 
Abstract: Many networks in nature and applications have an approximate low-rank structure in the sense that their connectivity structure is dominated by a few dimensions. It is natural to expect that dynamics on such networks would also be low-dimensional. Indeed, theoretical results show that low-rank networks produce low-dimensional dynamics whenever the network is isolated from external perturbations or input. However, networks in nature are rarely isolated. Here, we study the dimensionality of dynamics in recurrent networks with low-dimensional structure driven by high-dimensional inputs or perturbations. We find that dynamics in such networks can be high- or low-dimensional and we derive precise conditions on the network structure under which dynamics are high-dimensional. In many low-rank networks, dynamics are suppressed in directions aligned with the network's low-rank structure, a phenomenon we term ``low-rank suppression.'' We show that several low-rank network structures arising in nature satisfy the conditions for generating high-dimensional dynamics and low-rank suppression. Our results clarify important, but counterintuitive relationships between a recurrent network's connectivity structure and the structure of the dynamics it generates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13727v3</guid>
      <category>math.DS</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yue Wan, Robert Rosenbaum</dc:creator>
    </item>
    <item>
      <title>Fast weight programming and linear transformers: from machine learning to neurobiology</title>
      <link>https://arxiv.org/abs/2508.08435</link>
      <description>arXiv:2508.08435v2 Announce Type: replace-cross 
Abstract: Recent advances in artificial neural networks for machine learning, and language modeling in particular, have established a family of recurrent neural network (RNN) architectures that, unlike conventional RNNs with vector-form hidden states, use two-dimensional (2D) matrix-form hidden states. Such 2D-state RNNs, known as Fast Weight Programmers (FWPs), can be interpreted as a neural network whose synaptic weights (called fast weights) dynamically change over time as a function of input observations, and serve as short-term memory storage; corresponding synaptic weight modifications are controlled or programmed by another network (the programmer) whose parameters are trained (e.g., by gradient descent). In this Primer, we review the technical foundations of FWPs, their computational characteristics, and their connections to transformers and state space models. We also discuss connections between FWPs and models of synaptic plasticity in the brain, suggesting a convergence of natural and artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08435v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kazuki Irie, Samuel J. Gershman</dc:creator>
    </item>
    <item>
      <title>Mirror-Neuron Patterns in AI Alignment</title>
      <link>https://arxiv.org/abs/2511.01885</link>
      <description>arXiv:2511.01885v2 Announce Type: replace-cross 
Abstract: As artificial intelligence (AI) advances toward superhuman capabilities, aligning these systems with human values becomes increasingly critical. Current alignment strategies rely largely on externally specified constraints that may prove insufficient against future super-intelligent AI capable of circumventing top-down controls.
  This research investigates whether artificial neural networks (ANNs) can develop patterns analogous to biological mirror neurons cells that activate both when performing and observing actions, and how such patterns might contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in empathy, imitation, and social cognition in humans. The study therefore asks: (1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these patterns contribute to ethical and cooperative decision-making in AI systems?
  Using a novel Frog and Toad game framework designed to promote cooperative behaviors, we identify conditions under which mirror-neuron patterns emerge, evaluate their influence on action circuits, introduce the Checkpoint Mirror Neuron Index (CMNI) to quantify activation strength and consistency, and propose a theoretical framework for further study.
  Our findings indicate that appropriately scaled model capacities and self/other coupling foster shared neural representations in ANNs similar to biological mirror neurons. These empathy-like circuits support cooperative behavior and suggest that intrinsic motivations modeled through mirror-neuron dynamics could complement existing alignment techniques by embedding empathy-like mechanisms directly within AI architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01885v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robyn Wyrick</dc:creator>
    </item>
  </channel>
</rss>
