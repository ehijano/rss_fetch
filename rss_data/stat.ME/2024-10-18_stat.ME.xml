<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Oct 2024 04:00:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Probabilistic inference when the population space is open</title>
      <link>https://arxiv.org/abs/2410.12930</link>
      <description>arXiv:2410.12930v1 Announce Type: new 
Abstract: In using observed data to make inferences about a population quantity, it is commonly assumed that the sampling distribution from which the data were drawn belongs to a given parametric family of distributions, or at least, a given finite set of such families, i.e. the population space is assumed to be closed. Here, we address the problem of how to determine an appropriate post-data distribution for a given population quantity when such an assumption about the underlying sampling distribution is not made, i.e. when the population space is open. The strategy used to address this problem is based on the fact that even though, due to an open population space being non-measurable, we are not able to place a post-data distribution over all the sampling distributions contained in such a population space, it is possible to partition this type of space into a finite, countable or uncountable number of subsets such that a distribution can be placed over a variable that simply indicates which of these subsets contains the true sampling distribution. Moreover, it is argued that, by using sampling distributions that belong to a number of parametric families, it is possible to adequately and elegantly represent the sampling distributions that belong to each of the subsets of such a partition. Since a statistical model is conceived as being a model of a population space rather than a model of a sampling distribution, it is also argued that neither the type of models that are put forward nor the expression of pre-data knowledge via such models can be directly brought into question by the data. Finally, the case is made that, as well as not being required in the modelling process that is proposed, the standard practice of using P values to measure the absolute compatibility of an individual or family of sampling distributions with observed data is neither meaningful nor useful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12930v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Russell J. Bowater</dc:creator>
    </item>
    <item>
      <title>Improved control of Dirichlet location and scale near the boundary</title>
      <link>https://arxiv.org/abs/2410.13050</link>
      <description>arXiv:2410.13050v1 Announce Type: new 
Abstract: Dirichlet distributions are commonly used for modeling vectors in a probability simplex. When used as a prior or a proposal distribution, it is natural to set the mean of a Dirichlet to be equal to the location where one wants the distribution to be centered. However, if the mean is near the boundary of the probability simplex, then a Dirichlet distribution becomes highly concentrated either (i) at the mean or (ii) extremely close to the boundary. Consequently, centering at the mean provides poor control over the location and scale near the boundary. In this article, we introduce a method for improved control over the location and scale of Beta and Dirichlet distributions. Specifically, given a target location point and a desired scale, we maximize the density at the target location point while constraining a specified measure of scale. We consider various choices of scale constraint, such as fixing the concentration parameter, the mean cosine error, or the variance in the Beta case. In several examples, we show that this maximum density method provides superior performance for constructing priors, defining Metropolis-Hastings proposals, and generating simulated probability vectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13050v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Catherine Xue, Alessandro Zito, Jeffrey W. Miller</dc:creator>
    </item>
    <item>
      <title>Analyzing longitudinal electronic health records data with clinically informative visiting process: possible choices and comparisons</title>
      <link>https://arxiv.org/abs/2410.13113</link>
      <description>arXiv:2410.13113v1 Announce Type: new 
Abstract: Analyzing longitudinal electronic health records (EHR) data is challenging due to clinically informative observation processes, where the timing and frequency of patient visits often depend on their underlying health condition. Traditional longitudinal data analysis rarely considers observation times as stochastic or models them as functions of covariates. In this work, we evaluate the impact of informative visiting processes on two common statistical tasks: (1) estimating the effect of an exposure on a longitudinal biomarker, and (2) assessing the effect of longitudinal biomarkers on a time-to-event outcome, such as disease diagnosis. The methods we consider range from using simple summaries of the observed longitudinal data, imputation, inversely weighting by the estimated intensity of the visit process, and joint modeling. To our knowledge, this is the most comprehensive evaluation of inferential methods specifically tailored for EHR-like visiting processes. For the first task, we improve certain methods around 18 times faster, making them more suitable for large-scale EHR data analysis. For the second task, where no method currently accounts for the visiting process in joint models of longitudinal and time-to-event data, we propose incorporating the historical number of visits to adjust for informative visiting. Using data from the longitudinal biobank at the University of Michigan Health System, we investigate two case studies: 1) the association between genetic variants and lab markers with repeated measures (known as the LabWAS); and 2) the association between cardiometabolic health markers and time to hypertension diagnosis. We show how accounting for informative visiting processes affects the analysis results. We develop an R package CIMPLE (Clinically Informative Missingness handled through Probabilities, Likelihood, and Estimating equations) that integrates all these methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13113v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiacong Du, Xu Shi, Bhramar Mukherjee</dc:creator>
    </item>
    <item>
      <title>Online conformal inference for multi-step time series forecasting</title>
      <link>https://arxiv.org/abs/2410.13115</link>
      <description>arXiv:2410.13115v1 Announce Type: new 
Abstract: We consider the problem of constructing distribution-free prediction intervals for multi-step time series forecasting, with a focus on the temporal dependencies inherent in multi-step forecast errors. We establish that the optimal $h$-step-ahead forecast errors exhibit serial correlation up to lag $(h-1)$ under a general non-stationary autoregressive data generating process. To leverage these properties, we propose the Autocorrelated Multi-step Conformal Prediction (AcMCP) method, which effectively incorporates autocorrelations in multi-step forecast errors, resulting in more statistically efficient prediction intervals. This method ensures theoretical long-run coverage guarantees for multi-step prediction intervals, though we note that increased forecasting horizons may exacerbate deviations from the target coverage, particularly in the context of limited sample sizes. Additionally, we extend several easy-to-implement conformal prediction methods, originally designed for single-step forecasting, to accommodate multi-step scenarios. Through empirical evaluations, including simulations and applications to data, we demonstrate that AcMCP achieves coverage that closely aligns with the target within local windows, while providing adaptive prediction intervals that effectively respond to varying conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13115v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoqian Wang, Rob J Hyndman</dc:creator>
    </item>
    <item>
      <title>Agnostic Characterization of Interference in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2410.13142</link>
      <description>arXiv:2410.13142v1 Announce Type: new 
Abstract: We give an approach for characterizing interference by lower bounding the number of units whose outcome depends on certain groups of treated individuals, such as depending on the treatment of others, or others who are at least a certain distance away. The approach is applicable to randomized experiments with binary-valued outcomes. Asymptotically conservative point estimates and one-sided confidence intervals may be constructed with no assumptions beyond the known randomization design, allowing the approach to be used when interference is poorly understood, or when an observed network might only be a crude proxy for the underlying social mechanisms. Point estimates are equal to Hajek-weighted comparisons of units with differing levels of treatment exposure. Empirically, we find that the size of our interval estimates is competitive with (and often smaller than) those of the EATE, an assumption-lean treatment effect, suggesting that the proposed estimands may be intrinsically easier to estimate than treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13142v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Choi</dc:creator>
    </item>
    <item>
      <title>Markov Random Fields with Proximity Constraints for Spatial Data</title>
      <link>https://arxiv.org/abs/2410.13164</link>
      <description>arXiv:2410.13164v1 Announce Type: new 
Abstract: The conditional autoregressive (CAR) model, simultaneous autoregressive (SAR) model, and its variants have become the predominant strategies for modeling regional or areal-referenced spatial data. The overwhelming wide-use of the CAR/SAR model motivates the need for new classes of models for areal-referenced data. Thus, we develop a novel class of Markov random fields based on truncating the full-conditional distribution. We define this truncation in two ways leading to versions of what we call the truncated autoregressive (TAR) model. First, we truncate the full conditional distribution so that a response at one location is close to the average of its neighbors. This strategy establishes relationships between TAR and CAR. Second, we truncate on the joint distribution of the data process in a similar way. This specification leads to connection between TAR and SAR model. Our Bayesian implementation does not use Markov chain Monte Carlo (MCMC) for Bayesian computation, and generates samples directly from the posterior distribution. Moreover, TAR does not have a range parameter that arises in the CAR/SAR models, which can be difficult to learn. We present the results of the proposed truncated autoregressive model on several simulated datasets and on a dataset of average property prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13164v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sudipto Saha, Jonathan R. Bradley</dc:creator>
    </item>
    <item>
      <title>Adaptive LAD-Based Bootstrap Unit Root Tests under Unconditional Heteroskedasticity</title>
      <link>https://arxiv.org/abs/2410.13170</link>
      <description>arXiv:2410.13170v1 Announce Type: new 
Abstract: This paper explores testing unit roots based on least absolute deviations (LAD) regression under unconditional heteroskedasticity. We first derive the asymptotic properties of the LAD estimator for a first-order autoregressive process with the coefficient (local to) unity under unconditional heteroskedasticity and weak dependence, revealing that the limiting distribution of the LAD estimator (consequently the derived test statistics) is closely associated with unknown time-varying variances. To conduct feasible LAD-based unit root tests under heteroskedasticity and serial dependence, we develop an adaptive block bootstrap procedure, which accommodates time-varying volatility and serial dependence, both of unknown forms, to compute critical values for LAD-based tests. The asymptotic validity is established. We then extend the testing procedure to allow for deterministic components. Simulation results indicate that, in the presence of unconditional heteroskedasticity and serial dependence, the classic LAD-based tests demonstrate severe size distortion, whereas the proposed LAD-based bootstrap tests exhibit good size-control capability. Additionally, the newly developed tests show superior testing power in heavy-tailed distributed cases compared to considered benchmarks. Finally, empirical analysis of real effective exchange rates of 16 EU countries is conducted to illustrate the applicability of the newly proposed tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13170v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jilin Wu, Ruike Wu, Zhijie Xiao</dc:creator>
    </item>
    <item>
      <title>Novel Bayesian algorithms for ARFIMA long-memory processes: a comparison between MCMC and ABC approaches</title>
      <link>https://arxiv.org/abs/2410.13261</link>
      <description>arXiv:2410.13261v1 Announce Type: new 
Abstract: This paper presents a comparative study of two Bayesian approaches - Markov Chain Monte Carlo (MCMC) and Approximate Bayesian Computation (ABC) - for estimating the parameters of autoregressive fractionally-integrated moving average (ARFIMA) models, which are widely used to capture long-memory in time series data. We propose a novel MCMC algorithm that filters the time series into distinct long-memory and ARMA components, and benchmarked it against standard approaches. Additionally, a new ABC method is proposed, using three different summary statistics used for posterior estimation. The methods are implemented and evaluated through an extensive simulation study, as well as applied to a real-world financial dataset, specifically the quarterly U.S. Gross National Product (GNP) series. The results demonstrate the effectiveness of the Bayesian methods in estimating long-memory and short-memory parameters, with the filtered MCMC showing superior performance in various metrics. This study enhances our understanding of Bayesian techniques in ARFIMA modeling, providing insights into their advantages and limitations when applied to complex time series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13261v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Cohen Gabor, Clara Grazian</dc:creator>
    </item>
    <item>
      <title>Spatial Proportional Hazards Model with Differential Regularization</title>
      <link>https://arxiv.org/abs/2410.13420</link>
      <description>arXiv:2410.13420v1 Announce Type: new 
Abstract: This paper introduces a novel Spatial Proportional Hazards model that incorporates spatial dependence through differential regularization. We address limitations of existing methods that overlook domain geometry by proposing an approach based on the Generalized Spatial Regression with PDE Penalization. Our method handles complex-shaped domains, enabling accurate modeling of spatial fields in survival data. Using a penalized log-likelihood functional, we estimate both covariate effects and the spatial field. The methodology is implemented via finite element methods, efficiently handling irregular domain geometries. We demonstrate its efficacy through simulations and apply it to real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13420v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Tedesco</dc:creator>
    </item>
    <item>
      <title>Fair comparisons of causal parameters with many treatments and positivity violations</title>
      <link>https://arxiv.org/abs/2410.13522</link>
      <description>arXiv:2410.13522v1 Announce Type: new 
Abstract: Comparing outcomes across treatments is essential in medicine and public policy. To do so, researchers typically estimate a set of parameters, possibly counterfactual, with each targeting a different treatment. Treatment-specific means (TSMs) are commonly used, but their identification requires a positivity assumption -- that every subject has a non-zero probability of receiving each treatment. This assumption is often implausible, especially when treatment can take many values. Causal parameters based on dynamic stochastic interventions can be robust to positivity violations. However, comparing these parameters may be unfair because they may depend on outcomes under non-target treatments. To address this, and clarify when fair comparisons are possible, we propose a fairness criterion: if the conditional TSM for one treatment is greater than that for another, then the corresponding causal parameter should also be greater. We derive two intuitive properties equivalent to this criterion and show that only a mild positivity assumption is needed to identify fair parameters. We then provide examples that satisfy this criterion and are identifiable under the milder positivity assumption. These parameters are non-smooth, making standard nonparametric efficiency theory inapplicable, so we propose smooth approximations of them. We then develop doubly robust-style estimators that attain parametric convergence rates under nonparametric conditions. We illustrate our methods with an analysis of dialysis providers in New York State.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13522v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Yiting Li, Sunjae Bae, Mara A. McAdams-DeMarco, Iv\'an D\'iaz, Wenbo Wu</dc:creator>
    </item>
    <item>
      <title>A multiscale method for data collected from network edges via the line graph</title>
      <link>https://arxiv.org/abs/2410.13693</link>
      <description>arXiv:2410.13693v1 Announce Type: new 
Abstract: Data collected over networks can be modelled as noisy observations of an unknown function over the nodes of a graph or network structure, fully described by its nodes and their connections, the edges. In this context, function estimation has been proposed in the literature and typically makes use of the network topology such as relative node arrangement, often using given or artificially constructed node Euclidean coordinates. However, networks that arise in fields such as hydrology (for example, river networks) present features that challenge these established modelling setups since the target function may naturally live on edges (e.g., river flow) and/or the node-oriented modelling uses noisy edge data as weights. This work tackles these challenges and develops a novel lifting scheme along with its associated (second) generation wavelets that permit data decomposition across the network edges. The transform, which we refer to under the acronym LG-LOCAAT, makes use of a line graph construction that first maps the data in the line graph domain. We thoroughly investigate the proposed algorithm's properties and illustrate its performance versus existing methodologies. We conclude with an application pertaining to hydrology that involves the denoising of a water quality index over the England river network, backed up by a simulation study for a river flow dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13693v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dingjia Cao, Marina I. Knight, Guy P. Nason</dc:creator>
    </item>
    <item>
      <title>A Subsequence Approach to Topological Data Analysis for Irregularly-Spaced Time Series</title>
      <link>https://arxiv.org/abs/2410.13723</link>
      <description>arXiv:2410.13723v1 Announce Type: new 
Abstract: A time-delay embedding (TDE), grounded in the framework of Takens's Theorem, provides a mechanism to represent and analyze the inherent dynamics of time-series data. Recently, topological data analysis (TDA) methods have been applied to study this time series representation mainly through the lens of persistent homology. Current literature on the fusion of TDE and TDA are adept at analyzing uniformly-spaced time series observations. This work introduces a novel {\em subsequence} embedding method for irregularly-spaced time-series data. We show that this method preserves the original state space topology while reducing spurious homological features. Theoretical stability results and convergence properties of the proposed method in the presence of noise and varying levels of irregularity in the spacing of the time series are established. Numerical studies and an application to real data illustrates the performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13723v1</guid>
      <category>stat.ME</category>
      <category>cs.CG</category>
      <category>math.AT</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sixtus Dakurah, Jessi Cisewski-Kehe</dc:creator>
    </item>
    <item>
      <title>Inferring the dynamics of quasi-reaction systems via nonlinear local mean-field approximations</title>
      <link>https://arxiv.org/abs/2410.13744</link>
      <description>arXiv:2410.13744v1 Announce Type: new 
Abstract: In the modelling of stochastic phenomena, such as quasi-reaction systems, parameter estimation of kinetic rates can be challenging, particularly when the time gap between consecutive measurements is large. Local linear approximation approaches account for the stochasticity in the system but fail to capture the nonlinear nature of the underlying process. At the mean level, the dynamics of the system can be described by a system of ODEs, which have an explicit solution only for simple unitary systems. An analytical solution for generic quasi-reaction systems is proposed via a first order Taylor approximation of the hazard rate. This allows a nonlinear forward prediction of the future dynamics given the current state of the system. Predictions and corresponding observations are embedded in a nonlinear least-squares approach for parameter estimation. The performance of the algorithm is compared to existing SDE and ODE-based methods via a simulation study. Besides the increased computational efficiency of the approach, the results show an improvement in the kinetic rate estimation, particularly for data observed at large time intervals. Additionally, the availability of an explicit solution makes the method robust to stiffness, which is often present in biological systems. An illustration on Rhesus Macaque data shows the applicability of the approach to the study of cell differentiation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13744v1</guid>
      <category>stat.ME</category>
      <category>q-bio.MN</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Framba, Veronica Vinciotti, Ernst C. Wit</dc:creator>
    </item>
    <item>
      <title>Distributional Matrix Completion via Nearest Neighbors in the Wasserstein Space</title>
      <link>https://arxiv.org/abs/2410.13112</link>
      <description>arXiv:2410.13112v1 Announce Type: cross 
Abstract: We introduce the problem of distributional matrix completion: Given a sparsely observed matrix of empirical distributions, we seek to impute the true distributions associated with both observed and unobserved matrix entries. This is a generalization of traditional matrix completion where the observations per matrix entry are scalar valued. To do so, we utilize tools from optimal transport to generalize the nearest neighbors method to the distributional setting. Under a suitable latent factor model on probability distributions, we establish that our method recovers the distributions in the Wasserstein norm. We demonstrate through simulations that our method is able to (i) provide better distributional estimates for an entry compared to using observed samples for that entry alone, (ii) yield accurate estimates of distributional quantities such as standard deviation and value-at-risk, and (iii) inherently support heteroscedastic noise. We also prove novel asymptotic results for Wasserstein barycenters over one-dimensional distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13112v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob Feitelberg, Kyuseong Choi, Anish Agarwal, Raaz Dwivedi</dc:creator>
    </item>
    <item>
      <title>On uniqueness of the set of k-means</title>
      <link>https://arxiv.org/abs/2410.13495</link>
      <description>arXiv:2410.13495v1 Announce Type: cross 
Abstract: We provide necessary and sufficient conditions for the uniqueness of the k-means set of a probability distribution. This uniqueness problem is related to the choice of k: depending on the underlying distribution, some values of this parameter could lead to multiple sets of k-means, which hampers the interpretation of the results and/or the stability of the algorithms. We give a general assessment on consistency of the empirical k-means adapted to the setting of non-uniqueness and determine the asymptotic distribution of the within cluster sum of squares (WCSS). We also provide statistical characterizations of k-means uniqueness in terms of the asymptotic behavior of the empirical WCSS. As a consequence, we derive a bootstrap test for uniqueness of the set of k-means. The results are illustrated with examples of different types of non-uniqueness and we check by simulations the performance of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13495v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Javier C\'arcamo, Antonio Cuevas, Luis A. Rodr\'iguez</dc:creator>
    </item>
    <item>
      <title>Ab initio nonparametric variable selection for scalable Symbolic Regression with large $p$</title>
      <link>https://arxiv.org/abs/2410.13681</link>
      <description>arXiv:2410.13681v1 Announce Type: cross 
Abstract: Symbolic regression (SR) is a powerful technique for discovering symbolic expressions that characterize nonlinear relationships in data, gaining increasing attention for its interpretability, compactness, and robustness. However, existing SR methods do not scale to datasets with a large number of input variables (referred to as extreme-scale SR), which are common in modern scientific applications. This ``large $p$'' setting, often accompanied by measurement error, leads to slow performance of SR methods and overly complex expressions that are difficult to interpret. To address this scalability challenge, we propose a method called PAN+SR, which combines a key idea of ab initio nonparametric variable selection with SR to efficiently pre-screen large input spaces and reduce search complexity while maintaining accuracy. The use of nonparametric methods eliminates model misspecification, supporting a strategy called parametric-assisted nonparametric (PAN). We also extend SRBench, an open-source benchmarking platform, by incorporating high-dimensional regression problems with various signal-to-noise ratios. Our results demonstrate that PAN+SR consistently enhances the performance of 17 contemporary SR methods, enabling several to achieve state-of-the-art performance on these challenging datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13681v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shengbin Ye, Meng Li</dc:creator>
    </item>
    <item>
      <title>Optimizing Probabilistic Conformal Prediction with Vectorized Non-Conformity Scores</title>
      <link>https://arxiv.org/abs/2410.13735</link>
      <description>arXiv:2410.13735v1 Announce Type: cross 
Abstract: Generative models have shown significant promise in critical domains such as medical diagnosis, autonomous driving, and climate science, where reliable decision-making hinges on accurate uncertainty quantification. While probabilistic conformal prediction (PCP) offers a powerful framework for this purpose, its coverage efficiency -- the size of the uncertainty set -- is limited when dealing with complex underlying distributions and a finite number of generated samples. In this paper, we propose a novel PCP framework that enhances efficiency by first vectorizing the non-conformity scores with ranked samples and then optimizing the shape of the prediction set by varying the quantiles for samples at the same rank. Our method delivers valid coverage while producing discontinuous and more efficient prediction sets, making it particularly suited for high-stakes applications. We demonstrate the effectiveness of our approach through experiments on both synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13735v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minxing Zheng, Shixiang Zhu</dc:creator>
    </item>
    <item>
      <title>FAStEN: An Efficient Adaptive Method for Feature Selection and Estimation in High-Dimensional Functional Regressions</title>
      <link>https://arxiv.org/abs/2303.14801</link>
      <description>arXiv:2303.14801v3 Announce Type: replace 
Abstract: Functional regression analysis is an established tool for many contemporary scientific applications. Regression problems involving large and complex data sets are ubiquitous, and feature selection is crucial for avoiding overfitting and achieving accurate predictions. We propose a new, flexible and ultra-efficient approach to perform feature selection in a sparse high dimensional function-on-function regression problem, and we show how to extend it to the scalar-on-function framework. Our method, called FAStEN, combines functional data, optimization, and machine learning techniques to perform feature selection and parameter estimation simultaneously. We exploit the properties of Functional Principal Components and the sparsity inherent to the Dual Augmented Lagrangian problem to significantly reduce computational cost, and we introduce an adaptive scheme to improve selection accuracy. In addition, we derive asymptotic oracle properties, which guarantee estimation and selection consistency for the proposed FAStEN estimator. Through an extensive simulation study, we benchmark our approach to the best existing competitors and demonstrate a massive gain in terms of CPU time and selection performance, without sacrificing the quality of the coefficients' estimation. The theoretical derivations and the simulation study provide a strong motivation for our approach. Finally, we present an application to brain fMRI data from the AOMIC PIOP1 study. Complete FAStEN code is provided at https://github.com/IBM/funGCN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.14801v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2024.2407464</arxiv:DOI>
      <arxiv:journal_reference>Journal of Computational and Graphical Statistics 2024</arxiv:journal_reference>
      <dc:creator>Tobia Boschi, Lorenzo Testa, Francesca Chiaromonte, Matthew Reimherr</dc:creator>
    </item>
    <item>
      <title>Dependent Random Partitions by Shrinking Toward an Anchor</title>
      <link>https://arxiv.org/abs/2312.17716</link>
      <description>arXiv:2312.17716v2 Announce Type: replace 
Abstract: Although exchangeable processes from Bayesian nonparametrics have been used as a generating mechanism for random partition models, we deviate from this paradigm to explicitly incorporate clustering information in the formulation of our random partition model. Our shrinkage partition distribution takes any partition distribution and shrinks its probability mass toward an anchor partition. We show how this provides a framework to model hierarchically-dependent and temporally-dependent random partitions. The shrinkage parameter controls the degree of dependence, accommodating at its extremes both independence and complete equality. Since a priori knowledge of items may vary, our formulation allows the degree of shrinkage toward the anchor to be item-specific. Our random partition model has a tractable normalizing constant which allows for standard Markov chain Monte Carlo algorithms for posterior sampling. We prove intuitive theoretical properties for our distribution and compare it to related partition distributions. We show that our model provides better out-of-sample fit in a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17716v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David B. Dahl, Richard L. Warr, Thomas P. Jensen</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference Under Differential Privacy With Bounded Data</title>
      <link>https://arxiv.org/abs/2405.13801</link>
      <description>arXiv:2405.13801v2 Announce Type: replace 
Abstract: We describe Bayesian inference for the parameters of Gaussian models of bounded data protected by differential privacy. Using this setting, we demonstrate that analysts can and should take constraints imposed by the bounds into account when specifying prior distributions. Additionally, we provide theoretical and empirical results regarding what classes of default priors produce valid inference for a differentially private release in settings where substantial prior information is not available. We discuss how these results can be applied to Bayesian inference for regression with differentially private data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13801v2</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeki Kazan, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Generalised Bayes Linear Inference</title>
      <link>https://arxiv.org/abs/2405.14145</link>
      <description>arXiv:2405.14145v3 Announce Type: replace 
Abstract: Motivated by big data and the vast parameter spaces in modern machine learning models, optimisation approaches to Bayesian inference have seen a surge in popularity in recent years. In this paper, we address the connection between the popular new methods termed generalised Bayesian inference and Bayes linear methods. We propose a further generalisation to Bayesian inference that unifies these and other recent approaches by considering the Bayesian inference problem as one of finding the closest point in a particular solution space to a data generating process, where these notions differ depending on user-specified geometries and foundational belief systems. Motivated by this framework, we propose a generalisation to Bayes linear approaches that enables fast and principled inferences that obey the coherence requirements implied by domain restrictions on random quantities. We demonstrate the efficacy of generalised Bayes linear inference on a number of examples, including monotonic regression and inference for spatial counts. This paper is accompanied by an R package available at github.com/astfalckl/bayeslinear.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14145v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lachlan Astfalck, Cassandra Bird, Daniel Williamson</dc:creator>
    </item>
    <item>
      <title>On harmonic oscillator hazard functions</title>
      <link>https://arxiv.org/abs/2408.15964</link>
      <description>arXiv:2408.15964v2 Announce Type: replace 
Abstract: We propose a parametric hazard model obtained by enforcing positivity in the damped harmonic oscillator. The resulting model has closed-form hazard and cumulative hazard functions, facilitating likelihood and Bayesian inference on the parameters. We show that this model can capture a range of hazard shapes, such as increasing, decreasing, unimodal, bathtub, and oscillatory patterns, and characterize the tails of the corresponding survival function. We illustrate the use of this model in survival analysis using real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15964v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. A. Christen, F. J. Rubio</dc:creator>
    </item>
    <item>
      <title>Iterative Estimation of Nonparametric Regressions with Continuous Endogenous Variables and Discrete Instruments</title>
      <link>https://arxiv.org/abs/1905.07812</link>
      <description>arXiv:1905.07812v3 Announce Type: replace-cross 
Abstract: We consider a nonparametric regression model with continuous endogenous independent variables when only discrete instruments are available that are independent of the error term. Although this framework is very relevant for applied research, its implementation is challenging, as the regression function becomes the solution to a nonlinear integral equation. We propose a simple iterative procedure to estimate such models and showcase some of its asymptotic properties. In a simulation experiment, we detail its implementation in the case when the instrumental variable is binary. We conclude with an empirical application to returns to education.</description>
      <guid isPermaLink="false">oai:arXiv.org:1905.07812v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuele Centorrino, Fr\'ed\'erique F\`eve, Jean-Pierre Florens</dc:creator>
    </item>
    <item>
      <title>Data-Driven Estimation of Heterogeneous Treatment Effects</title>
      <link>https://arxiv.org/abs/2301.06615</link>
      <description>arXiv:2301.06615v2 Announce Type: replace-cross 
Abstract: Estimating how a treatment affects different individuals, known as heterogeneous treatment effect estimation, is an important problem in empirical sciences. In the last few years, there has been a considerable interest in adapting machine learning algorithms to the problem of estimating heterogeneous effects from observational and experimental data. However, these algorithms often make strong assumptions about the observed features in the data and ignore the structure of the underlying causal model, which can lead to biased estimation. At the same time, the underlying causal mechanism is rarely known in real-world datasets, making it hard to take it into consideration. In this work, we provide a survey of state-of-the-art data-driven methods for heterogeneous treatment effect estimation using machine learning, broadly categorizing them as methods that focus on counterfactual prediction and methods that directly estimate the causal effect. We also provide an overview of a third category of methods which rely on structural causal models and learn the model structure from data. Our empirical evaluation under various underlying structural model mechanisms shows the advantages and deficiencies of existing estimators and of the metrics for measuring their performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.06615v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Tran, Keith Burghardt, Kristina Lerman, Elena Zheleva</dc:creator>
    </item>
    <item>
      <title>Concentration inequalities for high-dimensional linear processes with dependent innovations</title>
      <link>https://arxiv.org/abs/2307.12395</link>
      <description>arXiv:2307.12395v2 Announce Type: replace-cross 
Abstract: We develop concentration inequalities for the $l_\infty$ norm of vector linear processes with sub-Weibull, mixingale innovations. This inequality is used to obtain a concentration bound for the maximum entrywise norm of the lag-$h$ autocovariance matrix of linear processes. We apply these inequalities to sparse estimation of large-dimensional VAR(p) systems and heterocedasticity and autocorrelation consistent (HAC) high-dimensional covariance estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12395v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduardo Fonseca Mendes, Fellipe Lopes</dc:creator>
    </item>
    <item>
      <title>Evaluating causal effects on time-to-event outcomes in an RCT in Oncology with treatment discontinuation</title>
      <link>https://arxiv.org/abs/2310.06653</link>
      <description>arXiv:2310.06653v2 Announce Type: replace-cross 
Abstract: In clinical trials, patients may discontinue treatments prematurely, breaking the initial randomization and, thus, challenging inference. Stakeholders in drug development are generally interested in going beyond the Intention-To-Treat (ITT) analysis, which provides valid causal estimates of the effect of treatment assignment but does not inform on the effect of the actual treatment receipt. Our study is motivated by an RCT in oncology, where patients assigned the investigational treatment may discontinue it due to adverse events. We propose adopting a principal stratum strategy and decomposing the overall ITT effect into principal causal effects for groups of patients defined by their potential discontinuation behavior. We first show how to implement a principal stratum strategy to assess causal effects on a survival outcome in the presence of continuous time treatment discontinuation, its advantages, and the conclusions one can draw. Our strategy deals with the time-to-event intermediate variable that may not be defined for patients who would not discontinue; moreover, discontinuation time and the primary endpoint are subject to censoring. We employ a flexible model-based Bayesian approach to tackle these complexities, providing easily interpretable results. We apply this Bayesian principal stratification framework to analyze synthetic data of the motivating oncology trial. We simulate data under different assumptions that reflect real scenarios where patients' behavior depends on critical baseline covariates. Supported by a simulation study, we shed light on the role of covariates in this framework: beyond making structural and parametric assumptions more credible, they lead to more precise inference and can be used to characterize patients' discontinuation behavior, which could help inform clinical practice and future protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06653v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Veronica Ballerini, Bj\"orn Bornkamp, Alessandra Mattei, Fabrizia Mealli, Craig Wang, Yufen Zhang</dc:creator>
    </item>
    <item>
      <title>Multi-CATE: Multi-Accurate Conditional Average Treatment Effect Estimation Robust to Unknown Covariate Shifts</title>
      <link>https://arxiv.org/abs/2405.18206</link>
      <description>arXiv:2405.18206v2 Announce Type: replace-cross 
Abstract: Estimating heterogeneous treatment effects is important to tailor treatments to those individuals who would most likely benefit. However, conditional average treatment effect predictors may often be trained on one population but possibly deployed on different, possibly unknown populations. We use methodology for learning multi-accurate predictors to post-process CATE T-learners (differenced regressions) to become robust to unknown covariate shifts at the time of deployment. The method works in general for pseudo-outcome regression, such as the DR-learner. We show how this approach can combine (large) confounded observational and (smaller) randomized datasets by learning a confounded predictor from the observational dataset, and auditing for multi-accuracy on the randomized controlled trial. We show improvements in bias and mean squared error in simulations with increasingly larger covariate shift, and on a semi-synthetic case study of a parallel large observational study and smaller randomized controlled experiment. Overall, we establish a connection between methods developed for multi-distribution learning and achieve appealing desiderata (e.g. external validity) in causal inference and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18206v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoph Kern, Michael Kim, Angela Zhou</dc:creator>
    </item>
    <item>
      <title>Orthogonalized Estimation of Difference of $Q$-functions</title>
      <link>https://arxiv.org/abs/2406.08697</link>
      <description>arXiv:2406.08697v2 Announce Type: replace-cross 
Abstract: Offline reinforcement learning is important in many settings with available observational data but the inability to deploy new policies online due to safety, cost, and other concerns. Many recent advances in causal inference and machine learning target estimation of causal contrast functions such as CATE, which is sufficient for optimizing decisions and can adapt to potentially smoother structure. We develop a dynamic generalization of the R-learner (Nie and Wager 2021, Lewis and Syrgkanis 2021) for estimating and optimizing the difference of $Q^\pi$-functions, $Q^\pi(s,1)-Q^\pi(s,0)$ (which can be used to optimize multiple-valued actions). We leverage orthogonal estimation to improve convergence rates in the presence of slower nuisance estimation rates and prove consistency of policy optimization under a margin condition. The method can leverage black-box nuisance estimators of the $Q$-function and behavior policy to target estimation of a more structured $Q$-function contrast.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08697v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Defu Cao, Angela Zhou</dc:creator>
    </item>
    <item>
      <title>Self-Organized State-Space Models with Artificial Dynamics</title>
      <link>https://arxiv.org/abs/2409.08928</link>
      <description>arXiv:2409.08928v3 Announce Type: replace-cross 
Abstract: We consider a state-space model (SSM) parametrized by some parameter $\theta$ and aim at performing joint parameter and state inference. A popular idea to carry out this task is to replace $\theta$ by a Markov chain $(\theta_t)_{t\geq 0}$ and then to apply a filtering algorithm to the extended, or self-organized SSM (SO-SSM). However, the practical implementation of this idea in a theoretically justified way has remained an open problem. In this paper we fill this gap by introducing constructions of $(\theta_t)_{t\geq 0}$ that ensure the validity of the SO-SSM for joint parameter and state inference. Notably, we show that such SO-SSMs can be defined even if $\|\mathrm{Var}(\theta_{t}|\theta_{t-1})\|\rightarrow 0$ slowly as $t\rightarrow\infty$. This result is important since these models can be efficiently approximated using a particle filter. While SO-SSMs have been introduced for online inference, the development of iterated filtering (IF) has shown that they can also serve for computing the maximum likelihood estimator of an SSM. We also derive constructions of $(\theta_t)_{t\geq 0}$ and theoretical guarantees tailored to these specific applications of SO-SSMs and introduce new IF algorithms. From a practical point of view, the algorithms we develop are simple to implement and only require minimal tuning to perform well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08928v3</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Chen, Mathieu Gerber, Christophe Andrieu, Randal Douc</dc:creator>
    </item>
    <item>
      <title>Impact of existence and nonexistence of pivot on the coverage of empirical best linear prediction intervals for small areas</title>
      <link>https://arxiv.org/abs/2410.11238</link>
      <description>arXiv:2410.11238v2 Announce Type: replace-cross 
Abstract: We advance the theory of parametric bootstrap in constructing highly efficient empirical best (EB) prediction intervals of small area means. The coverage error of such a prediction interval is of the order $O(m^{-3/2})$, where $m$ is the number of small areas to be pooled using a linear mixed normal model. In the context of an area level model where the random effects follow a non-normal known distribution except possibly for unknown hyperparameters, we analytically show that the order of coverage error of empirical best linear (EBL) prediction interval remains the same even if we relax the normality of the random effects by the existence of pivot for a suitably standardized random effects when hyperpameters are known. Recognizing the challenge of showing existence of a pivot, we develop a simple moment-based method to claim non-existence of pivot. We show that existing parametric bootstrap EBL prediction interval fails to achieve the desired order of the coverage error, i.e. $O(m^{-3/2})$, in absence of a pivot. We obtain a surprising result that the order $O(m^{-1})$ term is always positive under certain conditions indicating possible overcoverage of the existing parametric bootstrap EBL prediction interval. In general, we analytically show for the first time that the coverage problem can be corrected by adopting a suitably devised double parametric bootstrap. Our Monte Carlo simulations show that our proposed single bootstrap method performs reasonably well when compared to rival methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11238v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuting Chen, Masayo Y. Hirose, Partha Lahiri</dc:creator>
    </item>
    <item>
      <title>Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators</title>
      <link>https://arxiv.org/abs/2410.12690</link>
      <description>arXiv:2410.12690v2 Announce Type: replace-cross 
Abstract: A critical bottleneck for scientific progress is the costly nature of computer simulations for complex systems. Surrogate models provide an appealing solution: such models are trained on simulator evaluations, then used to emulate and quantify uncertainty on the expensive simulator at unexplored inputs. In many applications, one often has available data on related systems. For example, in designing a new jet turbine, there may be existing studies on turbines with similar configurations. A key question is how information from such "source" systems can be transferred for effective surrogate training on the "target" system of interest. We thus propose a new LOcal transfer Learning Gaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian process to transfer such information for surrogate modeling. The key novelty of the LOL-GP is a latent regularization model, which identifies regions where transfer should be performed and regions where it should be avoided. This "local transfer" property is desirable in scientific systems: at certain parameters, such systems may behave similarly and thus transfer is beneficial; at other parameters, they may behave differently and thus transfer is detrimental. By accounting for local transfer, the LOL-GP can rectify a critical limitation of "negative transfer" in existing transfer learning models, where the transfer of information worsens predictive performance. We derive a Gibbs sampling algorithm for efficient posterior predictive sampling on the LOL-GP, for both the multi-source and multi-fidelity transfer settings. We then show, via a suite of numerical experiments and an application for jet turbine design, the improved surrogate performance of the LOL-GP over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12690v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinming Wang, Simon Mak, John Miller, Jianguo Wu</dc:creator>
    </item>
  </channel>
</rss>
