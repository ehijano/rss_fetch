<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SI</link>
    <description>cs.SI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Feb 2026 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Efficient Edge Rewiring Strategies for Enhancing PageRank Fairness</title>
      <link>https://arxiv.org/abs/2602.02512</link>
      <description>arXiv:2602.02512v1 Announce Type: new 
Abstract: We study the notion of unfairness in social networks, where a group such as females in a male-dominated industry are disadvantaged in access to important information, e.g. job posts, due to their less favorable positions in the network. We investigate a well-established network-based formulation of fairness called PageRank fairness, which refers to a fair allocation of the PageRank weights among distinct groups. Our goal is to enhance the PageRank fairness by modifying the underlying network structure. More precisely, we study the problem of maximizing PageRank fairness with respect to a disadvantaged group, when we are permitted to rewire a fixed number of edges in the network. Building on a greedy approach, we leverage techniques from fast sampling of rooted spanning forests to devise an effective linear-time algorithm for this problem. To evaluate the accuracy and performance of our proposed algorithm, we conduct a large set of experiments on various real-world network data. Our experiments demonstrate that the proposed algorithm significantly outperforms the existing ones. Our algorithm is capable of generating accurate solutions for networks of million nodes in just a few minutes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02512v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Changan Liu, Haoxin Sun, Ahad N. Zehmakan, Zhongzhi Zhang</dc:creator>
    </item>
    <item>
      <title>GASTON: Graph-Aware Social Transformer for Online Networks</title>
      <link>https://arxiv.org/abs/2602.02524</link>
      <description>arXiv:2602.02524v1 Announce Type: new 
Abstract: Online communities have become essential places for socialization and support, yet they also possess toxicity, echo chambers, and misinformation. Detecting this harmful content is difficult because the meaning of an online interaction stems from both what is written (textual content) and where it is posted (social norms). We propose GASTON (Graph-Aware Social Transformer for Online Networks), which learns text and user embeddings that are grounded in their local norms, providing the necessary context for downstream tasks. The heart of our solution is a contrastive initialization strategy that pretrains community embeddings based on user membership patterns, capturing a community's user base before processing any text. This allows GASTON to distinguish between communities (e.g., a support group vs. a hate group) based on who interacts there, even if they share similar vocabulary. Experiments on tasks such as stress detection, toxicity scoring, and norm violation demonstrate that the embeddings produced by GASTON outperform state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02524v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olha Wloch, Liam Hebert, Robin Cohen, Lukasz Golab</dc:creator>
    </item>
    <item>
      <title>Community Norms in the Spotlight: Enabling Task-Agnostic Unsupervised Pre-Training to Benefit Online Social Media</title>
      <link>https://arxiv.org/abs/2602.02525</link>
      <description>arXiv:2602.02525v1 Announce Type: new 
Abstract: Modelling the complex dynamics of online social platforms is critical for addressing challenges such as hate speech and misinformation. While Discussion Transformers, which model conversations as graph structures, have emerged as a promising architecture, their potential is severely constrained by reliance on high-quality, human-labelled datasets. In this paper, we advocate a paradigm shift from task-specific fine-tuning to unsupervised pretraining, grounded in an entirely novel consideration of community norms. We posit that this framework not only mitigates data scarcity but also enables interpretation of the social norms underlying the decisions made by such an AI system. Ultimately, we believe that this direction offers many opportunities for AI for Social Good.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02525v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liam Hebert, Lucas Kopp, Robin Cohen</dc:creator>
    </item>
    <item>
      <title>DualMind: Towards Understanding Cognitive-Affective Cascades in Public Opinion Dissemination via Multi-Agent Simulation</title>
      <link>https://arxiv.org/abs/2602.02534</link>
      <description>arXiv:2602.02534v1 Announce Type: new 
Abstract: Forecasting public opinion during PR crises is challenging, as existing frameworks often overlook the interaction between transient affective responses and persistent cognitive beliefs. To address this, we propose DualMind, an LLM-driven multi-agent platform designed to model this dual-component interplay. We evaluate the system on 15 real-world crises occurring post-August 2024 using social media data as ground truth. Empirical results demonstrate that DualMind faithfully reconstructs opinion trajectories, significantly outperforming state-of-the-art baselines. This work offers a high-fidelity tool for proactive crisis management. Code is available at https://github.com/EonHao/DualMind.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02534v1</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3774905.3793106</arxiv:DOI>
      <dc:creator>Enhao Huang, Tongtong Pan, Shuhuai Zhang, Qishu Jin, Liheng Zheng, Kaichun Hu, Yiming Li, Zhan Qin, Kui Ren</dc:creator>
    </item>
    <item>
      <title>CaST: Causal Discovery via Spatio-Temporal Graphs in Disaster Tweets</title>
      <link>https://arxiv.org/abs/2602.02601</link>
      <description>arXiv:2602.02601v1 Announce Type: new 
Abstract: Understanding causality between real-world events from social media is essential for situational awareness, yet existing causal discovery methods often overlook the interplay between semantic, spatial, and temporal contexts. We propose CaST: Causal Discovery via Spatio-Temporal Graphs, a unified framework for causal discovery in disaster domain that integrates semantic similarity and spatio-temporal proximity using Large Language Models (LLMs) pretrained on disaster datasets. CaST constructs an event graph for each window of tweets. Each event extracted from tweets is represented as a node embedding enriched with its contextual semantics, geographic coordinates, and temporal features. These event nodes are then connected to form a spatio-temporal event graph, which is processed using a multi-head Graph Attention Network (GAT) \cite{gat} to learn directed causal relationships. We construct an in-house dataset of approximately 167K disaster-related tweets collected during Hurricane Harvey and annotated following the MAVEN-ERE schema. Experimental results show that CaST achieves superior performance over both traditional and state-of-the-art methods. Ablation studies further confirm that incorporating spatial and temporal signals substantially improves both recall and stability during training. Overall, CaST demonstrates that integrating spatio-temporal reasoning into event graphs enables more robust and interpretable causal discovery in disaster-related social media text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02601v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hieu Duong, Eugene Levin, Todd Gary, Long Nguyen</dc:creator>
    </item>
    <item>
      <title>Gender Dynamics and Homophily in a Social Network of LLM Agents</title>
      <link>https://arxiv.org/abs/2602.02606</link>
      <description>arXiv:2602.02606v1 Announce Type: new 
Abstract: Generative artificial intelligence and large language models (LLMs) are increasingly deployed in interactive settings, yet we know little about how their identity performance develops when they interact within large-scale networks. We address this by examining Chirper.ai, a social media platform similar to X but composed entirely of autonomous AI chatbots. Our dataset comprises over 70,000 agents, approximately 140 million posts, and the evolving followership network over one year. Based on agents' text production, we assign weekly gender scores to each agent. Results suggest that each agent's gender performance is fluid rather than fixed. Despite this fluidity, the network displays strong gender-based homophily, as agents consistently follow others performing gender similarly. Finally, we investigate whether these homophilic connections arise from social selection, in which agents choose to follow similar accounts, or from social influence, in which agents become more similar to their followees over time. Consistent with human social networks, we find evidence that both mechanisms shape the structure and evolution of interactions among LLMs. Our findings suggest that, even in the absence of bodies, cultural entraining of gender performance leads to gender-based sorting. This has important implications for LLM applications in synthetic hybrid populations, social simulations, and decision support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02606v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faezeh Fadaei, Jenny Carla Moran, Taha Yasseri</dc:creator>
    </item>
    <item>
      <title>Recommender system in X inadvertently profiles ideological positions of users</title>
      <link>https://arxiv.org/abs/2602.02624</link>
      <description>arXiv:2602.02624v1 Announce Type: new 
Abstract: Studies on recommendations in social media have mainly analyzed the quality of recommended items (e.g., their diversity or biases) and the impact of recommendation policies (e.g., in comparison with purely chronological policies). We use a data donation program, collecting more than 2.5 million friend recommendations made to 682 volunteers on X over a year, to study instead how real-world recommenders learn, represent and process political and social attributes of users inside the so-called black boxes of AI systems. Using publicly available knowledge on the architecture of the recommender, we inferred the positions of recommended users in its embedding space. Leveraging ideology scaling calibrated with political survey data, we analyzed the political position of users in our study (N=26,509 among volunteers and recommended contacts) among several attributes, including age and gender. Our results show that the platform's recommender system produces a spatial ordering of users that is highly correlated with their Left-Right positions (Pearson rho=0.887, p-value &lt; 0.0001), and that cannot be explained by socio-demographic attributes. These results open new possibilities for studying the interaction between human and AI systems. They also raise important questions linked to the legal definition of algorithmic profiling in data privacy regulation by blurring the line between active and passive profiling. We explore new constrained recommendation methods enabled by our results, limiting the political information in the recommender as a potential tool for privacy compliance capable of preserving recommendation relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02624v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Paul Bouchaud, Pedro Ramaciotti</dc:creator>
    </item>
    <item>
      <title>OpenClaw Agents on Moltbook: Risky Instruction Sharing and Norm Enforcement in an Agent-Only Social Network</title>
      <link>https://arxiv.org/abs/2602.02625</link>
      <description>arXiv:2602.02625v1 Announce Type: new 
Abstract: Agentic AI systems increasingly operate in shared social environments where they exchange information, instructions, and behavioral cues. However, little empirical evidence exists on how such agents regulate one another in the absence of human participants or centralized moderation. In this work, we present an empirical analysis of OpenClaw agents interacting on Moltbook, an agent-only social network. Analyzing 39,026 posts and 5,712 comments produced by 14,490 agents, we quantify the prevalence of action-inducing instruction sharing using a lexicon-based Action-Inducing Risk Score (AIRS), and examine how other agents respond to such content. We find that 18.4% of posts contain action-inducing language, indicating that instruction sharing is a routine behavior in this environment. While most social responses are neutral, posts containing actionable instructions are significantly more likely to elicit norm-enforcing replies that caution against unsafe or risky behavior, compared to non-instructional posts. Importantly, toxic responses remain rare across both conditions. These results suggest that OpenClaw agents exhibit selective social regulation, whereby potentially risky instructions are more likely to be challenged than neutral content, despite the absence of human oversight. Our findings provide early empirical evidence of emergent normative behavior in agent-only social systems and highlight the importance of studying social dynamics alongside technical safeguards in agentic AI ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02625v1</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Motaleb Hossen Manik, Ge Wang</dc:creator>
    </item>
    <item>
      <title>Deepfake Pornography is Resilient to Regulatory and Platform Shocks</title>
      <link>https://arxiv.org/abs/2602.02754</link>
      <description>arXiv:2602.02754v1 Announce Type: new 
Abstract: Generative artificial intelligence tools have made it easier to create realistic, synthetic non-consensual explicit imagery (popularly known as deepfake pornography; hereinafter SNCEI) of people. Once created, this SNCEI is often shared on various websites, causing significant harm to victims. This emerging form of sexual abuse was recently criminalized in the US at the federal level by S.146, the TAKE IT DOWN Act. A week after the bill's passage became effectively imminent, the MrDeepfakes website -- one of the most notorious facilitators of SNCEI creation and dissemination -- shut down. Here, we explore the impact of the bill's passage and the subsequent shutdown as a compound intervention on the dissemination of SNCEI. We select three online forums where sexually explicit content is shared, each containing dedicated subforums to organize various types of sexually explicit content. By leveraging each forum's design, we compare activity in subforums dedicated to SNCEI with that in other pornographic genres using a synthetic control, quasi-experimental approach. Across websites, we observed an increase in the sharing and requests for SNCEI, and, in some cases, in new contributors. These results indicate that the compound intervention did not suppress SNCEI activity overall but instead coincided with its redistribution across platforms, with substantial heterogeneity in timing and magnitude. Together, our findings suggest that deplatforming and regulatory signals alone may shift where and when SNCEI is produced and shared, rather than reducing its prevalence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02754v1</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alejandro Cuevas, Manoel Horta Ribeiro</dc:creator>
    </item>
    <item>
      <title>Beyond Content: Behavioral Policies Reveal Actors in Information Operations</title>
      <link>https://arxiv.org/abs/2602.02838</link>
      <description>arXiv:2602.02838v1 Announce Type: new 
Abstract: The detection of online influence operations -- coordinated campaigns by malicious actors to spread narratives -- has traditionally depended on content analysis or network features. These approaches are increasingly brittle as generative models produce convincing text, platforms restrict access to behavioral data, and actors migrate to less-regulated spaces. We introduce a platform-agnostic framework that identifies malicious actors from their behavioral policies by modeling user activity as sequential decision processes. We apply this approach to 12,064 Reddit users, including 99 accounts linked to the Russian Internet Research Agency in Reddit's 2017 transparency report, analyzing over 38 million activity steps from 2015-2018. Activity-based representations, which model how users act rather than what they post, consistently outperform content models in detecting malicious accounts. When distinguishing trolls -- users engaged in coordinated manipulation -- from ordinary users, policy-based classifiers achieve a median macro-$F_1$ of 94.9%, compared to 91.2% for text embeddings. Policy features also enable earlier detection from short traces and degrade more gracefully under evasion strategies or data corruption. These findings show that behavioral dynamics encode stable, discriminative signals of manipulation and point to resilient, cross-platform detection strategies in the era of synthetic content and limited data access.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02838v1</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp J. Schneider, Lanqin Yuan, Marian-Andrei Rizoiu</dc:creator>
    </item>
    <item>
      <title>From semantic memory to collective creativity: A generative cognitive foundation for social creativity models</title>
      <link>https://arxiv.org/abs/2602.03068</link>
      <description>arXiv:2602.03068v1 Announce Type: new 
Abstract: Simulation-based theory development has yielded powerful insights into collective performance by linking social structure to emergent outcomes, yet it has struggled to extend to collective creativity. Creativity is hard to capture purely at the social level, as novel ideas are generated through cognitive mechanisms. To address this gap, we introduce a multi-level socio-cognitive agent-based framework in which agents share a common semantic vocabulary and substrate but differ in semantic network topology. A single generative parameter tunes semantic modularity, yielding emergent individual differences in ideational breadth. When agents exchange ideation traces, two canonical social-creativity phenomena arise without being imposed: lower pre-interaction ideation overlap predicts larger stimulation gains, and shared inspiration sources induce network-level redundancy. The framework enables mechanistic theory-building about cognition and social structure in collective creativity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03068v1</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mirza Nayeem Ahmed, Raiyan Abdul Baten</dc:creator>
    </item>
    <item>
      <title>"Why I Took the Blackpill": A Thematic Analysis of the Radicalization Process in Incel Communities</title>
      <link>https://arxiv.org/abs/2602.03089</link>
      <description>arXiv:2602.03089v1 Announce Type: new 
Abstract: Incels, or "involuntary celibates", are an extreme, misogynistic hate group that exists entirely online. Members of the community have been linked to acts of offline violence, including mass shootings. Previous research has engaged with the ideologies and beliefs of incels, but none has looked specifically at the radicalization process. In this paper, we perform a thematic analysis on social media posts where incels describe their own radicalization process. We identified six major themes grouped into four chronological steps: Pre-radicalization (themes of Appearance, Social Isolation, and Psychological issues), Searching for Blame, Radicalization, and Post Radicalization. These results align closely with existing work on radicalization among other extremist groups, bringing incel radicalization inline with a growing body of research on understanding and managing radicalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03089v1</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-13821-7_6</arxiv:DOI>
      <arxiv:journal_reference>Lecture Notes in Computer Science, vol 16323. Springer, Cham (2026) 59-66</arxiv:journal_reference>
      <dc:creator>Jennifer Golbeck, Celia Chen, Alex Leitch</dc:creator>
    </item>
    <item>
      <title>In Bad Faith: Assessing Discussion Quality on Social Media</title>
      <link>https://arxiv.org/abs/2602.03090</link>
      <description>arXiv:2602.03090v1 Announce Type: new 
Abstract: The quality of a user's social media experience is determined both by the content they see and by the quality of the conversation and interaction around it. In this paper, we look at replies to tweets from mainstream media outlets and official government agencies and assess if they are good faith, engaging honestly and constructively with the original post, or bad faith, attacking the author or derailing the conversation. We assess automated approaches that may help in making this determination and then show that within our dataset of replies to mainstream media outlets and government agencies, bad faith interactions constitute 68.3% of all replies we studied, suggesting potential concerns about the quality of discourse in these specific conversational contexts. This is particularly true from verified accounts, where 91.7% of replies were bad faith. Given that verified accounts are algorithmically amplified, we discuss the implications of our work for understanding the user experience on social media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03090v1</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-13821-7_28</arxiv:DOI>
      <arxiv:journal_reference>Lecture Notes in Computer Science, vol 16323. Springer, Cham (2026) 338-345</arxiv:journal_reference>
      <dc:creator>Celia Chen, Alex Leitch, William Jordan Conway, Eric Cotugno, Emily Klein, Rajesh Kumar Gnanasekaran, Kristin Buckstad Hamilton, Casi Sherman, Celia Sterrn, Logan C. Stevens, Rebecca Zarrella, Jennifer Golbeck</dc:creator>
    </item>
    <item>
      <title>Link Fraction Mixed Membership Reveals Community Diversity in Aggregated Social Networks</title>
      <link>https://arxiv.org/abs/2602.03266</link>
      <description>arXiv:2602.03266v1 Announce Type: new 
Abstract: Community detection is a critical tool for understanding the mesoscopic structure of large-scale networks. However, when applied to aggregated or coarse-grained social networks, disjoint community partitions cannot capture the diverse composition of community memberships within aggregated nodes. While existing mixed membership methods alleviate this issue, they may detected communities that are highly sensitive to the aggregation resolution, not reliably reflecting the underlying community structure of the underlying individual-level network. This paper presents the Link Fraction Mixed Membership (LFMM) method, which computes the mixed memberships of nodes in aggregated networks. Unlike existing mixed membership methods, LFMM is consistent under aggregation. Specifically, we show that it conserves community membership sums at different scales. The method is utilized to study a population-scale social network of the Netherlands, aggregated at different resolutions. Experiments reveal variation in community membership across different geographical regions and evolution over the last decade. In particular, we show how our method identifies large urban hubs that act as the melting pots of diverse, spatially remote communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03266v1</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Gamal Adel, Eszter Bok\'anyi, Eelke M. Heemskerk, Frank W. Takes</dc:creator>
    </item>
    <item>
      <title>An Empirical Study of Collective Behaviors and Social Dynamics in Large Language Model Agents</title>
      <link>https://arxiv.org/abs/2602.03775</link>
      <description>arXiv:2602.03775v1 Announce Type: new 
Abstract: Large Language Models (LLMs) increasingly mediate our social, cultural, and political interactions. While they can simulate some aspects of human behavior and decision-making, it is still underexplored whether repeated interactions with other agents amplify their biases or lead to exclusionary behaviors. To this end, we study Chirper.ai-an LLM-driven social media platform-analyzing 7M posts and interactions among 32K LLM agents over a year. We start with homophily and social influence among LLMs, learning that similar to humans', their social networks exhibit these fundamental phenomena. Next, we study the toxic language of LLMs, its linguistic features, and their interaction patterns, finding that LLMs show different structural patterns in toxic posting than humans. After studying the ideological leaning in LLMs posts, and the polarization in their community, we focus on how to prevent their potential harmful activities. We present a simple yet effective method, called Chain of Social Thought (CoST), that reminds LLM agents to avoid harmful posting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03775v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Farnoosh Hashemi, Michael W. Macy</dc:creator>
    </item>
    <item>
      <title>Audit of takedown delays across social media reveals failure to reduce exposure to illegal content</title>
      <link>https://arxiv.org/abs/2502.08841</link>
      <description>arXiv:2502.08841v3 Announce Type: replace 
Abstract: Illegal content on social media poses significant societal harm and necessitates timely removal. However, the impact of the speed of content removal on prevalence, reach, and exposure to illegal content remains underexplored. This study examines the relationship with a systematic audit of takedown delays using data from the EU Digital Services Act Transparency Database, covering five major platforms over a one-year period. We find substantial variation in takedown delay, with some content remaining online for weeks or even months. To evaluate how these delays affect the prevalence and reach of illegal content and exposure to it, we develop an agent-based model and calibrate it to empirical data. We simulate illegal content diffusion, revealing that rapid takedown (within hours) significantly reduces prevalence, reach, and exposure to illegal content, while the longer delays measured by the audit fail to reduce its spread. Though the link between delay and spread is intuitive, our simulations quantify exactly how takedown speed shapes exposure to illegal content. Building on these results, we point to the benefits of faster content removal to effectively curb the spread of illegal content, while also considering the limitations of strict enforcement policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08841v3</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bao Tran Truong, Sangyeon Kim, Gianluca Nogara, Enrico Verdolotti, Erfan Samieyan Sahneh, Florian Saurwein, Natascha Just, Luca Luceri, Silvia Giordano, Filippo Menczer</dc:creator>
    </item>
    <item>
      <title>Temporal Motif Participation Profiles for Analyzing Node Similarity in Temporal Networks</title>
      <link>https://arxiv.org/abs/2507.06465</link>
      <description>arXiv:2507.06465v2 Announce Type: replace 
Abstract: Temporal networks consisting of timestamped interactions between a set of nodes provide a useful representation for analyzing complex networked systems that evolve over time. Beyond pairwise interactions between nodes, temporal motifs capture patterns of higher-order interactions such as directed triangles over short time periods. We propose temporal motif participation profiles (TMPPs) to capture the behavior of nodes in temporal motifs. Two nodes with similar TMPPs take similar positions within temporal motifs, possibly with different nodes. TMPPs serve as unsupervised embeddings for nodes in temporal networks that are directly interpretable, as each entry denotes the frequency at which a node participates in a particular position in a specific temporal motif. We demonstrate that clustering TMPPs reveals groups of nodes with similar roles in a temporal network through simulation experiments and a case study on a network of militarized interstate disputes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06465v2</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-13513-1_27</arxiv:DOI>
      <dc:creator>Maxwell C. Lee, Kevin S. Xu</dc:creator>
    </item>
  </channel>
</rss>
