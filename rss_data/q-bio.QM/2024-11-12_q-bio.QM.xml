<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Nov 2024 05:01:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>TourSynbio-Search: A Large Language Model Driven Agent Framework for Unified Search Method for Protein Engineering</title>
      <link>https://arxiv.org/abs/2411.06024</link>
      <description>arXiv:2411.06024v1 Announce Type: new 
Abstract: The exponential growth in protein-related databases and scientific literature, combined with increasing demands for efficient biological information retrieval, has created an urgent need for unified and accessible search methods in protein engineering research. We present TourSynbio-Search, a novel bioinformatics search agent framework powered by the TourSynbio-7B protein multimodal large language model (LLM), designed to address the growing challenges of information retrieval across rapidly expanding protein databases and corresponding online research literature. The agent's dual-module architecture consists of PaperSearch and ProteinSearch components, enabling comprehensive exploration of both scientific literature and protein data across multiple biological databases. At its core, TourSynbio-Search employs an intelligent agent system that interprets natural language queries, optimizes search parameters, and executes search operations across major platforms including UniProt, PDB, ArXiv, and BioRxiv. The agent's ability to process intuitive natural language queries reduces technical barriers, allowing researchers to efficiently access and analyze complex biological data without requiring extensive bioinformatics expertise. Through detailed case studies in literature retrieval and protein structure visualization, we demonstrate TourSynbio-Search's effectiveness in streamlining biological information retrieval and enhancing research productivity. This framework represents an advancement in bridging the accessibility gap between complex biological databases and researchers, potentially accelerating progress in protein engineering applications. Our codes are available at: https://github.com/tsynbio/Toursynbio-Search</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06024v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yungeng Liu, Zan Chen, Yu Guang Wang, Yiqing Shen</dc:creator>
    </item>
    <item>
      <title>Validation of an LLM-based Multi-Agent Framework for Protein Engineering in Dry Lab and Wet Lab</title>
      <link>https://arxiv.org/abs/2411.06029</link>
      <description>arXiv:2411.06029v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have enhanced efficiency across various domains, including protein engineering, where they offer promising opportunities for dry lab and wet lab experiment workflow automation. Previous work, namely TourSynbio-Agent, integrates a protein-specialized multimodal LLM (i.e. TourSynbio-7B) with domain-specific deep learning (DL) models to streamline both computational and experimental protein engineering tasks. While initial validation demonstrated TourSynbio-7B's fundamental protein property understanding, the practical effectiveness of the complete TourSynbio-Agent framework in real-world applications remained unexplored. This study presents a comprehensive validation of TourSynbio-Agent through five diverse case studies spanning both computational (dry lab) and experimental (wet lab) protein engineering. In three computational case studies, we evaluate the TourSynbio-Agent's capabilities in mutation prediction, protein folding, and protein design. Additionally, two wet-lab validations demonstrate TourSynbio-Agent's practical utility: engineering P450 proteins with up to 70% improved selectivity for steroid 19-hydroxylation, and developing reductases with 3.7x enhanced catalytic efficiency for alcohol conversion. Our findings from the five case studies establish that TourSynbio-Agent can effectively automate complex protein engineering workflows through an intuitive conversational interface, potentially accelerating scientific discovery in protein engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06029v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zan Chen, Yungeng Liu, Yu Guang Wang, Yiqing Shen</dc:creator>
    </item>
    <item>
      <title>Wind Tunnel Study of the Forces Due to Drafting in Dolphin Mother-Calf pairs</title>
      <link>https://arxiv.org/abs/2411.06118</link>
      <description>arXiv:2411.06118v1 Announce Type: new 
Abstract: Cetacean Calves keep up with their mothers while rapidly swimming, by a hydrodynamical effect called drafting. This has been observed in the wild and enclosed areas, and has been mathematically analyzed in the past, but no quantitative measures of the actual forces involved have been made. We built wind tunnel models of Mother-Calf pairs and present force measures for various geometries and relative placements.We show that under certain configurations, the drafting forces are large enough to carry the calf along effortlessly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06118v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>D Weihs, M Ringel</dc:creator>
    </item>
    <item>
      <title>GenoHoption: Bridging Gene Network Graphs and Single-Cell Foundation Models</title>
      <link>https://arxiv.org/abs/2411.06331</link>
      <description>arXiv:2411.06331v1 Announce Type: new 
Abstract: The remarkable success of foundation models has sparked growing interest in their application to single-cell biology. Models like Geneformer and scGPT promise to serve as versatile tools in this specialized field. However, representing a cell as a sequence of genes remains an open question since the order of genes is interchangeable. Injecting the gene network graph offers gene relative positions and compact data representation but poses a dilemma: limited receptive fields without in-layer message passing or parameter explosion with message passing in each layer. To pave the way forward, we propose GenoHoption, a new computational framework for single-cell sequencing data that effortlessly combines the strengths of these foundation models with explicit relationships in gene networks. We also introduce a constraint that lightens the model by focusing on learning the predefined graph structure while ensuring further hops are deducted to expand the receptive field. Empirical studies show that our model improves by an average of 1.27% on cell-type annotation and 3.86% on perturbation prediction. Furthermore, our method significantly decreases computational overhead and exhibits few-shot potential. GenoHoption can function as an efficient and expressive bridge, connecting existing single-cell foundation models to gene network graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06331v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiabei Cheng, Jiachen Li, Kaiyuan Yang, Hongbin Shen, Ye Yuan</dc:creator>
    </item>
    <item>
      <title>Evaluating tDCS Intervention Effectiveness via Functional Connectivity Network on Resting-State EEG Data in Major Depressive Disorder</title>
      <link>https://arxiv.org/abs/2411.06359</link>
      <description>arXiv:2411.06359v1 Announce Type: new 
Abstract: Transcranial direct current stimulation (tDCS) has emerged as a promising non-invasive therapeutic intervention for major depressive disorder (MDD), yet its effects on neural mechanisms remain incompletely understood. This study investigates the impact of tDCS in individuals with MDD using resting-state EEG data and network neuroscience to analyze functional connectivity. We examined power spectral density (PSD) changes and functional connectivity (FC) patterns across theta, alpha, and beta bands before and after tDCS intervention. A notable aspect of this research involves the modification of the binarizing threshold algorithm to assess functional connectivity networks, facilitating a meaningful comparison at the group level. Our analysis using optimal threshold binarization techniques revealed significant modifications in network topology, particularly evident in the beta band, indicative of reduced randomization or enhanced small-worldness after tDCS. Furthermore, the hubness analysis identified specific brain regions, notably the dorsolateral prefrontal cortex (DLPFC) regions across all frequency bands, exhibiting increased functional connectivity, suggesting their involvement in the antidepressant effects of tDCS. Notably, tDCS intervention transformed the dispersed high connectivity into localized connectivity and increased left-sided asymmetry across all frequency bands. Overall, this study provides valuable insights into the effects of tDCS on neural mechanisms in MDD, offering a potential direction for further research and therapeutic development in the field of neuromodulation for mental health disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06359v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vishwani Singh, Rohit Verma, Shaurya Shriyam, Tapan K. Gandhi</dc:creator>
    </item>
    <item>
      <title>NeuReg: Domain-invariant 3D Image Registration on Human and Mouse Brains</title>
      <link>https://arxiv.org/abs/2411.06315</link>
      <description>arXiv:2411.06315v1 Announce Type: cross 
Abstract: Medical brain imaging relies heavily on image registration to accurately curate structural boundaries of brain features for various healthcare applications. Deep learning models have shown remarkable performance in image registration in recent years. Still, they often struggle to handle the diversity of 3D brain volumes, challenged by their structural and contrastive variations and their imaging domains. In this work, we present NeuReg, a Neuro-inspired 3D image registration architecture with the feature of domain invariance. NeuReg generates domain-agnostic representations of imaging features and incorporates a shifting window-based Swin Transformer block as the encoder. This enables our model to capture the variations across brain imaging modalities and species. We demonstrate a new benchmark in multi-domain publicly available datasets comprising human and mouse 3D brain volumes. Extensive experiments reveal that our model (NeuReg) outperforms the existing baseline deep learning-based image registration models and provides a high-performance boost on cross-domain datasets, where models are trained on 'source-only' domain and tested on completely 'unseen' target domains. Our work establishes a new state-of-the-art for domain-agnostic 3D brain image registration, underpinned by Neuro-inspired Transformer-based architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06315v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Taha Razzaq, Asim Iqbal</dc:creator>
    </item>
    <item>
      <title>Causal Representation Learning from Multimodal Biological Observations</title>
      <link>https://arxiv.org/abs/2411.06518</link>
      <description>arXiv:2411.06518v1 Announce Type: cross 
Abstract: Prevalent in biological applications (e.g., human phenotype measurements), multimodal datasets can provide valuable insights into the underlying biological mechanisms. However, current machine learning models designed to analyze such datasets still lack interpretability and theoretical guarantees, which are essential to biological applications. Recent advances in causal representation learning have shown promise in uncovering the interpretable latent causal variables with formal theoretical certificates. Unfortunately, existing works for multimodal distributions either rely on restrictive parametric assumptions or provide rather coarse identification results, limiting their applicability to biological research which favors a detailed understanding of the mechanisms.
  In this work, we aim to develop flexible identification conditions for multimodal data and principled methods to facilitate the understanding of biological datasets. Theoretically, we consider a flexible nonparametric latent distribution (c.f., parametric assumptions in prior work) permitting causal relationships across potentially different modalities. We establish identifiability guarantees for each latent component, extending the subspace identification results from prior work. Our key theoretical ingredient is the structural sparsity of the causal connections among distinct modalities, which, as we will discuss, is natural for a large collection of biological systems. Empirically, we propose a practical framework to instantiate our theoretical insights. We demonstrate the effectiveness of our approach through extensive experiments on both numerical and synthetic datasets. Results on a real-world human phenotype dataset are consistent with established medical research, validating our theoretical and methodological framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06518v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuewen Sun, Lingjing Kong, Guangyi Chen, Loka Li, Gongxu Luo, Zijian Li, Yixuan Zhang, Yujia Zheng, Mengyue Yang, Petar Stojanov, Eran Segal, Eric P. Xing, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>Enhancing frozen histological section images using permanent-section-guided deep learning with nuclei attention</title>
      <link>https://arxiv.org/abs/2411.06583</link>
      <description>arXiv:2411.06583v1 Announce Type: cross 
Abstract: In histological pathology, frozen sections are often used for rapid diagnosis during surgeries, as they can be produced within minutes. However, they suffer from artifacts and often lack crucial diagnostic details, particularly within the cell nuclei region. Permanent sections, on the other hand, contain more diagnostic detail but require a time-intensive preparation process. Here, we present a generative deep learning approach to enhance frozen section images by leveraging guidance from permanent sections. Our method places a strong emphasis on the nuclei region, which contains critical information in both frozen and permanent sections. Importantly, our approach avoids generating artificial data in blank regions, ensuring that the network only enhances existing features without introducing potentially unreliable information. We achieve this through a segmented attention network, incorporating nuclei-segmented images during training and adding an additional loss function to refine the nuclei details in the generated permanent images. We validated our method across various tissues, including kidney, breast, and colon. This approach significantly improves histological efficiency and diagnostic accuracy, enhancing frozen section images within seconds, and seamlessly integrating into existing laboratory workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06583v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elad Yoshai, Gil Goldinger, Miki Haifler, Natan T. Shaked</dc:creator>
    </item>
    <item>
      <title>LA4SR: illuminating the dark proteome with generative AI</title>
      <link>https://arxiv.org/abs/2411.06798</link>
      <description>arXiv:2411.06798v1 Announce Type: cross 
Abstract: AI language models (LMs) show promise for biological sequence analysis. We re-engineered open-source LMs (GPT-2, BLOOM, DistilRoBERTa, ELECTRA, and Mamba, ranging from 70M to 12B parameters) for microbial sequence classification. The models achieved F1 scores up to 95 and operated 16,580x faster and at 2.9x the recall of BLASTP. They effectively classified the algal dark proteome - uncharacterized proteins comprising about 65% of total proteins - validated on new data including a new, complete Hi-C/Pacbio Chlamydomonas genome. Larger (&gt;1B) LA4SR models reached high accuracy (F1 &gt; 86) when trained on less than 2% of available data, rapidly achieving strong generalization capacity. High accuracy was achieved when training data had intact or scrambled terminal information, demonstrating robust generalization to incomplete sequences. Finally, we provide custom AI explainability software tools for attributing amino acid patterns to AI generative processes and interpret their outputs in evolutionary and biophysical contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06798v1</guid>
      <category>q-bio.GN</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David R. Nelson, Ashish Kumar Jaiswal, Noha Ismail, Alexandra Mystikou, Kourosh Salehi-Ashtiani</dc:creator>
    </item>
    <item>
      <title>BudgetIV: Optimal Partial Identification of Causal Effects with Mostly Invalid Instruments</title>
      <link>https://arxiv.org/abs/2411.06913</link>
      <description>arXiv:2411.06913v1 Announce Type: cross 
Abstract: Instrumental variables (IVs) are widely used to estimate causal effects in the presence of unobserved confounding between exposure and outcome. An IV must affect the outcome exclusively through the exposure and be unconfounded with the outcome. We present a framework for relaxing either or both of these strong assumptions with tuneable and interpretable budget constraints. Our algorithm returns a feasible set of causal effects that can be identified exactly given relevant covariance parameters. The feasible set may be disconnected but is a finite union of convex subsets. We discuss conditions under which this set is sharp, i.e., contains all and only effects consistent with the background assumptions and the joint distribution of observable variables. Our method applies to a wide class of semiparametric models, and we demonstrate how its ability to select specific subsets of instruments confers an advantage over convex relaxations in both linear and nonlinear settings. We also adapt our algorithm to form confidence sets that are asymptotically valid under a common statistical assumption from the Mendelian randomization literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06913v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan Penn (King's College London), Lee M. Gunderson (University College London), Gecia Bravo-Hermsdorff (University College London), Ricardo Silva (University College London), David S. Watson (King's College London)</dc:creator>
    </item>
    <item>
      <title>MULTILAND: a neutral landscape generator designed for theoretical studies</title>
      <link>https://arxiv.org/abs/1503.07215</link>
      <description>arXiv:1503.07215v2 Announce Type: replace-cross 
Abstract: The main goal of Multiland is to generate neutral landscapes made of several types of regions, with an exact control of the proportions occupied by each type of region. An important feature of the software is that it allows a control of the landscape fragmentation. It is intended to theoretical studies on the effect of landscape structure in applied sciences. It has been developed in the framework of the PEERLESS ANR project "Predictive Ecological Engineering for Landscape Ecosystem Services and Sustainability". It includes both Matlab and Python versions.</description>
      <guid isPermaLink="false">oai:arXiv.org:1503.07215v2</guid>
      <category>q-bio.PE</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lionel Roques</dc:creator>
    </item>
    <item>
      <title>Medication Recommendation via Dual Molecular Modalities and Multi-Step Enhancement</title>
      <link>https://arxiv.org/abs/2405.20358</link>
      <description>arXiv:2405.20358v3 Announce Type: replace-cross 
Abstract: Existing works based on molecular knowledge neglect the 3D geometric structure of molecules and fail to learn the high-dimensional information of medications, leading to structural confusion. Additionally, it does not extract key substructures from a single patient visit, resulting in the failure to identify medication molecules suitable for the current patient visit. To address the above limitations, we propose a bimodal molecular recommendation framework named BiMoRec, which introduces 3D molecular structures to obtain atomic 3D coordinates and edge indices, overcoming the inherent lack of high-dimensional molecular information in 2D molecular structures. To retain the fast training and prediction efficiency of the recommendation system, we use bimodal graph contrastive pretraining to maximize the mutual information between the two molecular modalities, achieving the fusion of 2D and 3D molecular graphs. Additionally, we designed a molecular multi-step enhancement mechanism to re-calibrate the molecular weights. Specifically, we employ a pre-training method that captures both 2D and 3D molecular structure representations, along with substructure representations, and leverages contrastive learning to extract mutual information. We then use the pre-trained encoder to generate molecular representations, enhancing them through a three-step process: intra-visit, molecular per-visit, and latest-visit. Finally, we apply temporal information aggregation to generate the final medication combinations. Our implementation on the MIMIC-III and MIMIC-IV datasets demonstrates that our method achieves state-of-the-art performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20358v3</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shi Mu, Chen Li, Xiang Li, Shunpan Liang</dc:creator>
    </item>
  </channel>
</rss>
