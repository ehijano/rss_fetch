<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 Aug 2024 01:33:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Advancing Mixed Reality Game Development: An Evaluation of a Visual Game Analytics Tool in Action-Adventure and FPS Genres</title>
      <link>https://arxiv.org/abs/2408.01573</link>
      <description>arXiv:2408.01573v1 Announce Type: new 
Abstract: In response to the unique challenges of Mixed Reality (MR) game development, we developed GAMR, an analytics tool specifically designed for MR games. GAMR aims to assist developers in identifying and resolving gameplay issues effectively. It features reconstructed gameplay sessions, heatmaps for data visualization, a comprehensive annotation system, and advanced tracking for hands, camera, input, and audio, providing in-depth insights for nuanced game analysis.
  To evaluate GAMR's effectiveness, we conducted an experimental study with game development students across two game genres: action-adventure and first-person shooter (FPS). The participants used GAMR and provided feedback on its utility. The results showed a significant positive impact of GAMR in both genres, particularly in action-adventure games. This study demonstrates GAMR's effectiveness in MR game development and suggests its potential to influence future MR game analytics, addressing the specific needs of developers in this evolving area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01573v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3677055</arxiv:DOI>
      <dc:creator>Parisa Sargolzaei, Mudit Rastogi, Loutfouz Zaman</dc:creator>
    </item>
    <item>
      <title>Degrade to Function: Towards Eco-friendly Morphing Devices that Function Through Programmed Sequential Degradation</title>
      <link>https://arxiv.org/abs/2408.01660</link>
      <description>arXiv:2408.01660v1 Announce Type: new 
Abstract: While it seems counterintuitive to think of degradation within an operating device as beneficial, one may argue that when rationally designed, the controlled breakdown of materials can be harnessed for specific functions. To apply this principle to the design of morphing devices, we introduce the concept of Degrade to Function (DtF). This concept aims to create eco-friendly and self-contained morphing devices that operate through a sequence of environmentally-triggered degradations. We explore its design considerations and implementation techniques by identifying environmental conditions and degradation types that can be exploited, evaluating potential materials capable of controlled degradation, suggesting designs for structures that can leverage degradation to achieve various transformations and functions, and developing sequential control approaches that integrate degradation triggers. To demonstrate the viability and versatility of this design strategy, we showcase several application examples across a range of environmental conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01660v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3654777.3676464</arxiv:DOI>
      <dc:creator>Qiuyu Lu, Semina Yi, Mentian Gan, Jihong Huang, Xiao Zhang, Yue Yang, Chenyi Shen, Lining Yao</dc:creator>
    </item>
    <item>
      <title>WaitGPT: Monitoring and Steering Conversational LLM Agent in Data Analysis with On-the-Fly Code Visualization</title>
      <link>https://arxiv.org/abs/2408.01703</link>
      <description>arXiv:2408.01703v1 Announce Type: new 
Abstract: Large language models (LLMs) support data analysis through conversational user interfaces, as exemplified in OpenAI's ChatGPT (formally known as Advanced Data Analysis or Code Interpreter). Essentially, LLMs produce code for accomplishing diverse analysis tasks. However, presenting raw code can obscure the logic and hinder user verification. To empower users with enhanced comprehension and augmented control over analysis conducted by LLMs, we propose a novel approach to transform LLM-generated code into an interactive visual representation. In the approach, users are provided with a clear, step-by-step visualization of the LLM-generated code in real time, allowing them to understand, verify, and modify individual data operations in the analysis. Our design decisions are informed by a formative study (N=8) probing into user practice and challenges. We further developed a prototype named WaitGPT and conducted a user study (N=12) to evaluate its usability and effectiveness. The findings from the user study reveal that WaitGPT facilitates monitoring and steering of data analysis performed by LLMs, enabling participants to enhance error detection and increase their overall confidence in the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01703v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3654777.3676374</arxiv:DOI>
      <dc:creator>Liwenhan Xie, Chengbo Zheng, Haijun Xia, Huamin Qu, Chen Zhu-Tian</dc:creator>
    </item>
    <item>
      <title>3DStoryline: Immersive Visual Storytelling</title>
      <link>https://arxiv.org/abs/2408.01775</link>
      <description>arXiv:2408.01775v1 Announce Type: new 
Abstract: Storyline visualization has emerged as an innovative method for illustrating the development and changes in stories across various domains. Traditional approaches typically represent stories with one line per character, progressing from left to right. While effective for simpler narratives, this method faces significant challenges when dealing with complex stories involving multiple characters, as well as temporal and spatial dynamics. In this study, we investigate the potential of immersive environments for enhancing storyline visualizations. We begin by summarizing the key design considerations for effective storyline visualization in virtual reality (VR). Guided by these principles, we develop 3DStoryline, a system that allows users to view and interact with 3D immersive storyline visualizations. To evaluate the effectiveness of 3DStoryline, we conduct a task-based user study, revealing that the system significantly enhances users' comprehension of complex narratives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01775v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haonan Yao, Lixiang Zhao, Boyuan Chen, Kaiwen Li, Hai-Ning Liang, Lingyun Yu</dc:creator>
    </item>
    <item>
      <title>Adaptic: A Shape Changing Prop with Haptic Retargeting</title>
      <link>https://arxiv.org/abs/2408.01789</link>
      <description>arXiv:2408.01789v1 Announce Type: new 
Abstract: We present Adaptic, a novel "hybrid" active/passive haptic device that can change shape to act as a proxy for a range of virtual objects in VR. We use Adaptic with haptic retargeting to redirect the user's hand to provide haptic feedback for several virtual objects in arm's reach using only a single prop. To evaluate the effectiveness of Adaptic with haptic retargeting, we conducted a within-subjects experiment employing a docking task to compare Adaptic to non-matching proxy objects (i.e., Styrofoam balls) and matching shape props. In our study, Adaptic sat on a desk in front of the user and changed shapes between grasps, to provide matching tactile feedback for various virtual objects placed in different virtual locations. Results indicate that the illusion was convincing: users felt they were manipulating several virtual objects in different virtual locations with a single Adaptic device. Docking performance (completion time and accuracy) with Adaptic was comparable to props without haptic retargeting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01789v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3485279.3485293</arxiv:DOI>
      <dc:creator>J. Felipe Gonzalez, John C. McClelland, Robert J. Teather, Pablo Figueroa, Audrey Girouard</dc:creator>
    </item>
    <item>
      <title>Understanding the Challenges of OpenSCAD Users for 3D Printing</title>
      <link>https://arxiv.org/abs/2408.01796</link>
      <description>arXiv:2408.01796v1 Announce Type: new 
Abstract: Direct manipulation has been established as the main interaction paradigm for Computer-Aided Design (CAD) for decades. It provides fast, incremental, and reversible actions that allow for an iterative process on a visual representation of the result. Despite its numerous advantages, some users prefer a programming-based approach where they describe the 3D model they design with a specific programming language, such as OpenSCAD. It allows users to create complex structured geometries and facilitates abstraction. Unfortunately, most current knowledge about CAD practices only focuses on direct manipulation programs. In this study, we interviewed 20 programming-based CAD users to understand their motivations and challenges. Our findings reveal that this programming-oriented population presents difficulties in the design process in tasks such as 3D spatial understanding, validation and code debugging, creation of organic shapes, and code-view navigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01796v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3613904.3642566</arxiv:DOI>
      <dc:creator>J. Felipe Gonzalez, Thomas Pietrzak, Audrey Girouard, G\'ery Casiez</dc:creator>
    </item>
    <item>
      <title>Introducing Bidirectional Programming in Constructive Solid Geometry-Based CAD</title>
      <link>https://arxiv.org/abs/2408.01801</link>
      <description>arXiv:2408.01801v1 Announce Type: new 
Abstract: 3D Computer-Aided Design (CAD) users need to overcome several obstacles to benefit from the flexibility of programmatic interface tools. Besides the barriers of any programming language, users face challenges inherent to 3D spatial interaction. Scripting simple operations, such as moving an element in 3D space, can be significantly more challenging than performing the same task using direct manipulation. We introduce the concept of bidirectional programming for Constructive Solid Geometry (CSG) CAD tools, informed by interviews we performed with programmatic interface users. We describe how users can navigate and edit the 3D model using direct manipulation in the view or code editing while the system ensures consistency between both spaces. We also detail a proof-of-concept implementation using a modified version of OpenSCAD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01801v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3607822.3614521</arxiv:DOI>
      <dc:creator>J. Felipe Gonzalez, Danny Kieken, Thomas Pietrzak, Audrey Girouard, G\'ery Casiez</dc:creator>
    </item>
    <item>
      <title>Facilitating the Parametric Definition of Geometric Properties in Programming-Based CAD</title>
      <link>https://arxiv.org/abs/2408.01815</link>
      <description>arXiv:2408.01815v1 Announce Type: new 
Abstract: Parametric Computer-aided design (CAD) enables the creation of reusable models by integrating variables into geometric properties, facilitating customization without a complete redesign. However, creating parametric designs in programming-based CAD presents significant challenges. Users define models in a code editor using a programming language, with the application generating a visual representation in a viewport. This process involves complex programming and arithmetic expressions to describe geometric properties, linking various object properties to create parametric designs. Unfortunately, these applications lack assistance, making the process unnecessarily demanding. We propose a solution that allows users to retrieve parametric expressions from the visual representation for reuse in the code, streamlining the design process. We demonstrated this concept through a proof-of-concept implemented in the programming-based CAD application, OpenSCAD, and conducted an experiment with 11 users. Our findings suggest that this solution could significantly reduce design errors, improve interactivity and engagement in the design process, and lower the entry barrier for newcomers by reducing the mathematical skills typically required in programming-based CAD applications</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01815v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3654777.3676417</arxiv:DOI>
      <dc:creator>J. Felipe Gonzalez, Thomas Pietrzak, Audrey Girouard, G\'ery Casiez</dc:creator>
    </item>
    <item>
      <title>MotionTrace: IMU-based Field of View Prediction for Smartphone AR Interactions</title>
      <link>https://arxiv.org/abs/2408.01850</link>
      <description>arXiv:2408.01850v1 Announce Type: new 
Abstract: For handheld smartphone AR interactions, bandwidth is a critical constraint. Streaming techniques have been developed to provide a seamless and high-quality user experience despite these challenges. To optimize streaming performance in smartphone-based AR, accurate prediction of the user's field of view is essential. This prediction allows the system to prioritize loading digital content that the user is likely to engage with, enhancing the overall interactivity and immersion of the AR experience. In this paper, we present MotionTrace, a method for predicting the user's field of view using a smartphone's inertial sensor. This method continuously estimates the user's hand position in 3D-space to localize the phone position. We evaluated MotionTrace over future hand positions at 50, 100, 200, 400, and 800ms time horizons using the large motion capture (AMASS) and smartphone-based full-body pose estimation (Pose-on-the-Go) datasets. We found that our method can estimate the future phone position of the user with an average MSE between 0.11 - 143.62 mm across different time horizons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01850v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Islam, Vasco Xu, Karan Ahuja</dc:creator>
    </item>
    <item>
      <title>MoodPupilar: Predicting Mood Through Smartphone Detected Pupillary Responses in Naturalistic Settings</title>
      <link>https://arxiv.org/abs/2408.01855</link>
      <description>arXiv:2408.01855v1 Announce Type: new 
Abstract: MoodPupilar introduces a novel method for mood evaluation using pupillary response captured by a smartphone's front-facing camera during daily use. Over a four-week period, data was gathered from 25 participants to develop models capable of predicting daily mood averages. Utilizing the GLOBEM behavior modeling platform, we benchmarked the utility of pupillary response as a predictor for mood. Our proposed model demonstrated a Matthew's Correlation Coefficient (MCC) score of 0.15 for Valence and 0.12 for Arousal, which is on par with or exceeds those achieved by existing behavioral modeling algorithms supported by GLOBEM. This capability to accurately predict mood trends underscores the effectiveness of pupillary response data in providing crucial insights for timely mental health interventions and resource allocation. The outcomes are encouraging, demonstrating the potential of real-time and predictive mood analysis to support mental health interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01855v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Islam, Tongze Zhang, Priyanshu Singh Bisen, Sang Won Bae</dc:creator>
    </item>
    <item>
      <title>Computational Trichromacy Reconstruction: Empowering the Color-Vision Deficient to Recognize Colors Using Augmented Reality</title>
      <link>https://arxiv.org/abs/2408.01895</link>
      <description>arXiv:2408.01895v1 Announce Type: new 
Abstract: We propose an assistive technology that helps individuals with Color Vision Deficiencies (CVD) to recognize/name colors. A dichromat's color perception is a reduced two-dimensional (2D) subset of a normal trichromat's three dimensional color (3D) perception, leading to confusion when visual stimuli that appear identical to the dichromat are referred to by different color names. Using our proposed system, CVD individuals can interactively induce distinct perceptual changes to originally confusing colors via a computational color space transformation. By combining their original 2D precepts for colors with the discriminative changes, a three dimensional color space is reconstructed, where the dichromat can learn to resolve color name confusions and accurately recognize colors. Our system is implemented as an Augmented Reality (AR) interface on smartphones, where users interactively control the rotation through swipe gestures and observe the induced color shifts in the camera view or in a displayed image. Through psychophysical experiments and a longitudinal user study, we demonstrate that such rotational color shifts have discriminative power (initially confusing colors become distinct under rotation) and exhibit structured perceptual shifts dichromats can learn with modest training. The AR App is also evaluated in two real-world scenarios (building with lego blocks and interpreting artistic works); users all report positive experience in using the App to recognize object colors that they otherwise could not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01895v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3654777.3676415</arxiv:DOI>
      <dc:creator>Yuhao Zhu, Ethan Chen, Colin Hascup, Yukang Yan, Gaurav Charma</dc:creator>
    </item>
    <item>
      <title>Public Transit of the Future: Enhancing Well-Being through Designing Human-centered Public Transportation Spaces</title>
      <link>https://arxiv.org/abs/2408.01908</link>
      <description>arXiv:2408.01908v1 Announce Type: new 
Abstract: Studies show that psychological effects are among one of the top concerns for public transportation users. While many Americans spend a significant portion of their time in public transportation spaces, the impact of the design and maintenance of these spaces on user well-being has not been fully studied. In this study, we conducted a survey to better understand the effect of implementing different designs on people's well-being and perceptual metrics (N=304). Participants were presented with six images depicting different cabin configurations, including (1) the current version of the cabin space, (2) a low-maintenance version, (3) an aesthetically enhanced version, (4) a bike rack-enabled version, (5) a version with an added workspace, and (6) an improved version with biophilic design. After viewing each image, participants' well-being metrics (e.g., stress, and emotion) and their public transportation perception metrics (e.g., perceptions of safety, and reasonable cost) were evaluated. Our results from linear mixed-effect modeling indicated that adding functional amenities and biophilic design elements led to an overall enhancement in well-being and perceptual metrics. Conversely, low maintenance worsened all measured well-being. This research lays the ground for developing human-centered public transportation spaces that can lead to an increase in public transportation adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01908v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasaman Hakiminejad, Elizabeth Pantesco, Arash Tavakoli</dc:creator>
    </item>
    <item>
      <title>The Implications of Open Generative Models in Human-Centered Data Science Work: A Case Study with Fact-Checking Organizations</title>
      <link>https://arxiv.org/abs/2408.01962</link>
      <description>arXiv:2408.01962v1 Announce Type: new 
Abstract: Calls to use open generative language models in academic research have highlighted the need for reproducibility and transparency in scientific research. However, the impact of generative AI extends well beyond academia, as corporations and public interest organizations have begun integrating these models into their data science pipelines. We expand this lens to include the impact of open models on organizations, focusing specifically on fact-checking organizations, which use AI to observe and analyze large volumes of circulating misinformation, yet must also ensure the reproducibility and impartiality of their work. We wanted to understand where fact-checking organizations use open models in their data science pipelines; what motivates their use of open models or proprietary models; and how their use of open or proprietary models can inform research on the societal impact of generative AI. To answer these questions, we conducted an interview study with N=24 professionals at 20 fact-checking organizations on six continents. Based on these interviews, we offer a five-component conceptual model of where fact-checking organizations employ generative AI to support or automate parts of their data science pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data Delivery, and Data Sharing. We then provide taxonomies of fact-checking organizations' motivations for using open models and the limitations that prevent them for further adopting open models, finding that they prefer open models for Organizational Autonomy, Data Privacy and Ownership, Application Specificity, and Capability Transparency. However, they nonetheless use proprietary models due to perceived advantages in Performance, Usability, and Safety, as well as Opportunity Costs related to participation in emerging generative AI ecosystems. Our work provides novel perspective on open models in data-driven organizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01962v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Robert Wolfe, Tanushree Mitra</dc:creator>
    </item>
    <item>
      <title>JobViz: Skill-driven Visual Exploration of Job Advertisements</title>
      <link>https://arxiv.org/abs/2408.01989</link>
      <description>arXiv:2408.01989v1 Announce Type: new 
Abstract: Online job advertisements on various job portals or websites have become the most popular way for people to find potential career opportunities nowadays. However, the majority of these job sites are limited to offering fundamental filters such as job titles, keywords, and compensation ranges. This often poses a challenge for job seekers in efficiently identifying relevant job advertisements that align with their unique skill sets amidst a vast sea of listings. Thus, we propose well-coordinated visualizations to provide job seekers with three levels of details of job information: a skill-job overview visualizes skill sets, employment posts as well as relationships between them with a hierarchical visualization design; a post exploration view leverages an augmented radar-chart glyph to represent job posts and further facilitates users' swift comprehension of the pertinent skills necessitated by respective positions; a post detail view lists the specifics of selected job posts for profound analysis and comparison. By using a real-world recruitment advertisement dataset collected from 51Job, one of the largest job websites in China, we conducted two case studies and user interviews to evaluate JobViz. The results demonstrated the usefulness and effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01989v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.visinf.2024.07.001</arxiv:DOI>
      <dc:creator>Ran Wang, Qianhe Chen, Yong Wang, Boyang Shen, Lewei Xiong</dc:creator>
    </item>
    <item>
      <title>User Experience of Visualizations in Motion: A Case Study and Design Considerations</title>
      <link>https://arxiv.org/abs/2408.01991</link>
      <description>arXiv:2408.01991v1 Announce Type: new 
Abstract: We present a systematic review, an empirical study, and a first set of considerations for designing visualizations in motion, derived from a concrete scenario in which these visualizations were used to support a primary task. In practice, when viewers are confronted with embedded visualizations, they often have to focus on a primary task and can only quickly glance at a visualization showing rich, often dynamically updated, information. As such, the visualizations must be designed so as not to distract from the primary task, while at the same time being readable and useful for aiding the primary task. For example, in games, players who are engaged in a battle have to look at their enemies but also read the remaining health of their own game character from the health bar over their character's head. Many trade-offs are possible in the design of embedded visualizations in such dynamic scenarios, which we explore in-depth in this paper with a focus on user experience. We use video games as an example of an application context with a rich existing set of visualizations in motion. We begin our work with a systematic review of in-game visualizations in motion. Next, we conduct an empirical user study to investigate how different embedded visualizations in motion designs impact user experience. We conclude with a set of considerations and trade-offs for designing visualizations in motion more broadly as derived from what we learned about video games. All supplemental materials of this paper are available at https://osf.io/3v8wm/}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01991v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lijie Yao, Federica Bucchieri, Victoria McArthur, Anastasia Bezerianos, Petra Isenberg</dc:creator>
    </item>
    <item>
      <title>CompositingVis: Exploring Interactions for Creating Composite Visualizations in Immersive Environments</title>
      <link>https://arxiv.org/abs/2408.02240</link>
      <description>arXiv:2408.02240v1 Announce Type: new 
Abstract: Composite visualization represents a widely embraced design that combines multiple visual representations to create an integrated view. However, the traditional approach of creating composite visualizations in immersive environments typically occurs asynchronously outside of the immersive space and is carried out by experienced experts. In this work, we aim to empower users to participate in the creation of composite visualization within immersive environments through embodied interactions. This could provide a flexible and fluid experience with immersive visualization and has the potential to facilitate understanding of the relationship between visualization views. We begin with developing a design space of embodied interactions to create various types of composite visualizations with the consideration of data relationships. Drawing inspiration from people's natural experience of manipulating physical objects, we design interactions \zq{based on the combination of 3D manipulations} in immersive environments. Building upon the design space, we present a series of case studies showcasing the interaction to create different kinds of composite visualizations in virtual reality. Subsequently, we conduct a user study to evaluate the usability of the derived interaction techniques and user experience of creating composite visualizations through embodied interactions. We find that empowering users to participate in composite visualizations through embodied interactions enables them to flexibly leverage different visualization views for understanding and communicating the relationships between different views, which underscores the potential of several future application scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02240v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>IEEE VIS 2024</arxiv:journal_reference>
      <dc:creator>Qian Zhu, Tao Lu, Shunan Guo, Xiaojuan Ma, Yalong Yang</dc:creator>
    </item>
    <item>
      <title>CHORDination: Evaluating Visual Design Choices in Chord Diagrams for Network Data</title>
      <link>https://arxiv.org/abs/2408.02268</link>
      <description>arXiv:2408.02268v1 Announce Type: new 
Abstract: Chord diagrams are widely used for visualizing data connectivity and flow between nodes in a network. They are effective for representing complex structures through an intuitive and visually appealing circular layout. While previous work has focused on improving aesthetics and interactivity, the influence of fundamental design elements on user perception and information retrieval remains under-explored. In this study, we explored the three primary components of chord diagram anatomy, namely the nodes, circular outline, and arc connections, in three sequential experiment phases. In phase one, we conducted a controlled experiment (N=90) to find the perceptually and information optimized node widths (narrow, medium, wide) and quantities (low, medium, high). This optimal set of node width and quantity sets the foundation for subsequent evaluations and were kept fixed for consistency. In phase two of the study, we conducted an expert design review for identifying the optimal radial tick marks and color gradients. Then in phase three, we evaluated the perceptual and information retrieval performance of the design choices in a controlled experiment (N=24) by comparing four chord diagram designs (baseline, radial tick marks, arc color gradients, both tick marks and color gradients). Results indicated that node width and quantity significantly affected users' information retrieval performance and subjective ratings, whereas the presence of tick marks predominantly influenced subjective experiences. Based on these findings, we discuss the design implications of these visual elements and offer guidance and recommendations for optimizing chord diagram designs in network visualization tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02268v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Wang (Xi'an Jiaotong-Liverpool University), Shuqi He (Xi'an Jiaotong-Liverpool University), Wenlu Wang (Xi'an Jiaotong-Liverpool University), Jinbei Yu (Xi'an Jiaotong-Liverpool University), Yu Liu (Xi'an Jiaotong-Liverpool University), Lingyun Yu (Xi'an Jiaotong-Liverpool University)</dc:creator>
    </item>
    <item>
      <title>Responsibility and Regulation: Exploring Social Measures of Trust in Medical AI</title>
      <link>https://arxiv.org/abs/2408.02386</link>
      <description>arXiv:2408.02386v1 Announce Type: new 
Abstract: This paper explores expert accounts of autonomous systems (AS) development in the medical device domain (MD) involving applications of artificial intelligence (AI), machine learning (ML), and other algorithmic and mathematical modelling techniques. We frame our observations with respect to notions of responsible innovation (RI) and the emerging problem of how to do RI in practice. In contribution to the ongoing discourse surrounding trustworthy autonomous system (TAS) [29], we illuminate practical challenges inherent in deploying novel AS within existing governance structures, including domain specific regulations and policies, and rigorous testing and development processes, and discuss the implications of these for the distribution of responsibility in novel AI deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02386v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3686038.3686041</arxiv:DOI>
      <dc:creator>Glenn McGarry, Andy Crabtree, Lachlan Urquhart, Alan Chamberlain</dc:creator>
    </item>
    <item>
      <title>PUREsuggest: Citation-based Literature Search and Visual Exploration with Keyword-controlled Rankings</title>
      <link>https://arxiv.org/abs/2408.02508</link>
      <description>arXiv:2408.02508v1 Announce Type: new 
Abstract: Citations allow quickly identifying related research. If multiple publications are selected as seeds, specific suggestions for related literature can be made based on the number of incoming and outgoing citation links to this selection. Interactively adding recommended publications to the selection refines the next suggestion and incrementally builds a relevant collection of publications. Following this approach, the paper presents a search and foraging approach, PUREsuggest, which combines citation-based suggestions with augmented visualizations of the citation network. The focus and novelty of the approach is, first, the transparency of how the rankings are explained visually and, second, that the process can be steered through user-defined keywords, which reflect topics of interests. The system can be used to build new literature collections, to update and assess existing ones, as well as to use the collected literature for identifying relevant experts in the field. We evaluated the recommendation approach through simulated sessions and performed a user study investigating search strategies and usage patterns supported by the interface.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02508v1</guid>
      <category>cs.HC</category>
      <category>cs.DL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Beck</dc:creator>
    </item>
    <item>
      <title>Single-tap Latency Reduction with Single- or Double- tap Prediction</title>
      <link>https://arxiv.org/abs/2408.02525</link>
      <description>arXiv:2408.02525v1 Announce Type: new 
Abstract: Touch surfaces are widely utilized for smartphones, tablet PCs, and laptops (touchpad), and single and double taps are the most basic and common operations on them. The detection of single or double taps causes the single-tap latency problem, which creates a bottleneck in terms of the sensitivity of touch inputs. To reduce the single-tap latency, we propose a novel machine-learning-based tap prediction method called PredicTaps. Our method predicts whether a detected tap is a single tap or the first contact of a double tap without having to wait for the hundreds of milliseconds conventionally required. We present three evaluations and one user evaluation that demonstrate its broad applicability and usability for various tap situations on two form factors (touchpad and smartphone). The results showed PredicTaps reduces the single-tap latency from 150-500 ms to 12 ms on laptops and to 17.6 ms on smartphones without reducing usability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02525v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3604271</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Hum.-Comput. Interact. 7, MHCI, Article 224, September 2023</arxiv:journal_reference>
      <dc:creator>Naoto Nishida, Kaori Ikematsu, Junichi Sato, Shota Yamanaka, Kota Tsubouchi</dc:creator>
    </item>
    <item>
      <title>DanModCap: Designing a Danmaku Moderation Tool for Video-Sharing Platforms that Leverages Impact Captions</title>
      <link>https://arxiv.org/abs/2408.02574</link>
      <description>arXiv:2408.02574v1 Announce Type: new 
Abstract: Online video platforms have gained increased popularity due to their ability to support information consumption and sharing and the diverse social interactions they afford. Danmaku, a real-time commentary feature that overlays user comments on a video, has been found to improve user engagement, however, the use of Danmaku can lead to toxic behaviors and inappropriate comments. To address these issues, we propose a proactive moderation approach inspired by Impact Captions, a visual technique used in East Asian variety shows. Impact Captions combine textual content and visual elements to construct emotional and cognitive resonance. Within the context of this work, Impact Captions were used to guide viewers towards positive Danmaku-related activities and elicit more pro-social behaviors. Leveraging Impact Captions, we developed DanModCap, an moderation tool that collected and analyzed Danmaku and used it as input to large generative language models to produce Impact Captions. Our evaluation of DanModCap demonstrated that Impact Captions reduced negative antagonistic emotions, increased users' desire to share positive content, and elicited self-control in Danmaku social action to fostering proactive community maintenance behaviors. Our approach highlights the benefits of using LLM-supported content moderation methods for proactive moderation in a large-scale live content contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02574v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siying Hu, Huanchen Wang, Yu Zhang, Piaohong Wang, Zhicong Lu</dc:creator>
    </item>
    <item>
      <title>Behind the Smile: Mental Health Implications of Mother-Infant Interactions Revealed Through Smile Analysis</title>
      <link>https://arxiv.org/abs/2408.01434</link>
      <description>arXiv:2408.01434v1 Announce Type: cross 
Abstract: Mothers of infants have specific demands in fostering emotional bonds with their children, characterized by dynamics that are different from adult-adult interactions, notably requiring heightened maternal emotional regulation. In this study, we analyzed maternal emotional state by modeling maternal emotion regulation reflected in smiles. The dataset comprises N=94 videos of approximately 3 plus or minus 1-minutes, capturing free play interactions between 6 and 12-month-old infants and their mothers. Corresponding demographic details of self-reported maternal mental health provide variables for determining mothers' relations to emotions measured during free play. In this work, we employ diverse methodological approaches to explore the temporal evolution of maternal smiles. Our findings reveal a correlation between the temporal dynamics of mothers' smiles and their emotional state. Furthermore, we identify specific smile features that correlate with maternal emotional state, thereby enabling informed inferences with existing literature on general smile analysis. This study offers insights into emotional labor, defined as the management of one's own emotions for the benefit of others, and emotion regulation entailed in mother-infant interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01434v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A'di Dust, Pat Levitt, Maja Matari\'c</dc:creator>
    </item>
    <item>
      <title>Framework for Adoption of Generative Artificial Intelligence (GenAI) in Education</title>
      <link>https://arxiv.org/abs/2408.01443</link>
      <description>arXiv:2408.01443v1 Announce Type: cross 
Abstract: Contributions: An adoption framework to include GenAI in the university curriculum. It identifies and highlights the role of different stakeholders (university management, students, staff, etc.) during the adoption process. It also proposes an objective approach based upon an evaluation matrix to assess the success and outcome of the GenAI adoption.
  Background: Universities worldwide are debating and struggling with the adoption of GenAI in their curriculum. Both the faculty and students are unsure about the approach in the absence of clear guidelines through the administration and regulators. This requires an established framework to define a process and articulate the roles and responsibilities of each stakeholder involved.
  Research Questions: Whether the academic ecosystem requires a methodology to adopt GenAI into its curriculum? A systematic approach for the academic staff to ensure the students' learning outcomes are met with the adoption of GenAI. How to measure and communicate the adoption of GenAI in the university setup?
  Methodology: The methodology employed in this study focuses on examining the university education system and assessing the opportunities and challenges related to incorporating GenAI in teaching and learning. Additionally, it identifies a gap and the absence of a comprehensive framework that obstructs the effective integration of GenAI within the academic environment.
  Findings: The literature survey results indicate the limited or no adoption of GenAI by the university, which further reflects the dilemma in the minds of different stakeholders. For the successful adoption of GenAI, a standard framework is proposed i) for effective redesign of the course curriculum, ii) for enabling staff and students, iii) to define an evaluation matrix to measure the effectiveness and success of the adoption process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01443v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samar Shailendra, Rajan Kadel, Aakanksha Sharma</dc:creator>
    </item>
    <item>
      <title>A systematic review and analysis of the viability of virtual reality (VR) in construction work and education</title>
      <link>https://arxiv.org/abs/2408.01450</link>
      <description>arXiv:2408.01450v1 Announce Type: cross 
Abstract: This systematic review explores the viability of virtual reality (VR) technologies for enhancing learning outcomes and operational efficiency within the construction industry. This study evaluates the current integration of VR in construction education and practice. Employing the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines, this review analyzed 36 peer-reviewed journal articles from databases such as the Web of Science, ERIC, and Scopus. The methodology focused on identifying, appraising, and synthesizing all relevant studies to assess the effectiveness of VR applications in construction-related fields. This review highlights that VR significantly enhances learning by providing immersive interactive simulations that improve the understanding of every complex construction process, such as structural elements or tunnel-boring machine operations. This review contributes by systematically compiling and evaluating evidence on using VR in construction, which has seen a limited comprehensive analysis. It provides practical examples of how VR can revolutionize education and work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01450v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zia Ud Din, Payam Mohammadi, Rachael Sherman</dc:creator>
    </item>
    <item>
      <title>Surveys Considered Harmful? Reflecting on the Use of Surveys in AI Research, Development, and Governance</title>
      <link>https://arxiv.org/abs/2408.01458</link>
      <description>arXiv:2408.01458v1 Announce Type: cross 
Abstract: Calls for engagement with the public in Artificial Intelligence (AI) research, development, and governance are increasing, leading to the use of surveys to capture people's values, perceptions, and experiences related to AI. In this paper, we critically examine the state of human participant surveys associated with these topics. Through both a reflexive analysis of a survey pilot spanning six countries and a systematic literature review of 44 papers featuring public surveys related to AI, we explore prominent perspectives and methodological nuances associated with surveys to date. We find that public surveys on AI topics are vulnerable to specific Western knowledge, values, and assumptions in their design, including in their positioning of ethical concepts and societal values, lack sufficient critical discourse surrounding deployment strategies, and demonstrate inconsistent forms of transparency in their reporting. Based on our findings, we distill provocations and heuristic questions for our community, to recognize the limitations of surveys for meeting the goals of engagement, and to cultivate shared principles to design, deploy, and interpret surveys cautiously and responsibly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01458v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammmad Tahaei, Daricia Wilkinson, Alisa Frik, Michael Muller, Ruba Abu-Salma, Lauren Wilcox</dc:creator>
    </item>
    <item>
      <title>Using a CNN Model to Assess Visual Artwork's Creativity</title>
      <link>https://arxiv.org/abs/2408.01481</link>
      <description>arXiv:2408.01481v1 Announce Type: cross 
Abstract: Assessing artistic creativity has long challenged researchers, with traditional methods proving time-consuming. Recent studies have applied machine learning to evaluate creativity in drawings, but not paintings. Our research addresses this gap by developing a CNN model to automatically assess the creativity of students' paintings. Using a dataset of 600 paintings by professionals and children, our model achieved 90% accuracy and faster evaluation times than human raters. This approach demonstrates the potential of machine learning in advancing artistic creativity assessment, offering a more efficient alternative to traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01481v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhehan Zhang, Meihua Qian, Li Luo, Ripon Saha, Qianyi Gao, Xinxin Song</dc:creator>
    </item>
    <item>
      <title>Analyzing LLMs' Capabilities to Establish Implicit User Sentiment of Software Desirability</title>
      <link>https://arxiv.org/abs/2408.01527</link>
      <description>arXiv:2408.01527v1 Announce Type: cross 
Abstract: This study explores the use of several LLMs for providing quantitative zero-shot sentiment analysis of implicit software desirability expressed by users. The study provides scaled numerical sentiment analysis unlike other methods that simply classify sentiment as positive, neutral, or negative. Numerical analysis provides deeper insights into the magnitude of sentiment, to drive better decisions regarding product desirability.
  Data is collected through the use of the Microsoft Product Desirability Toolkit (PDT), a well-known qualitative user experience analysis tool. For initial exploration, the PDT metric was given to users of ZORQ, a gamification system used in undergraduate computer science education. The PDT data collected was fed through several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT4o) and through a leading transfer learning technique, Twitter-Roberta-Base-Sentiment (TRBS), and through Vader, a leading sentiment analysis tool, for quantitative sentiment analysis. Each system was asked to evaluate the data in two ways, first by looking at the sentiment expressed in the PDT word/explanation pairs; and by looking at the sentiment expressed by the users in their grouped selection of five words and explanations, as a whole. Each LLM was also asked to provide its confidence (low, medium, high) in its sentiment score, along with an explanation of why it selected the sentiment value.
  All LLMs tested were able to statistically detect user sentiment from the users' grouped data, whereas TRBS and Vader were not. The confidence and explanation of confidence provided by the LLMs assisted in understanding the user sentiment. This study adds to a deeper understanding of evaluating user experiences, toward the goal of creating a universal tool that quantifies implicit sentiment expressed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01527v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sherri Weitl-Harms, John D. Hastings, Jonah Lum</dc:creator>
    </item>
    <item>
      <title>Music2P: A Multi-Modal AI-Driven Tool for Simplifying Album Cover Design</title>
      <link>https://arxiv.org/abs/2408.01651</link>
      <description>arXiv:2408.01651v1 Announce Type: cross 
Abstract: In today's music industry, album cover design is as crucial as the music itself, reflecting the artist's vision and brand. However, many AI-driven album cover services require subscriptions or technical expertise, limiting accessibility. To address these challenges, we developed Music2P, an open-source, multi-modal AI-driven tool that streamlines album cover creation, making it efficient, accessible, and cost-effective through Ngrok. Music2P automates the design process using techniques such as Bootstrapping Language Image Pre-training (BLIP), music-to-text conversion (LP-music-caps), image segmentation (LoRA), and album cover and QR code generation (ControlNet). This paper demonstrates the Music2P interface, details our application of these technologies, and outlines future improvements. Our ultimate goal is to provide a tool that empowers musicians and producers, especially those with limited resources or expertise, to create compelling album covers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01651v1</guid>
      <category>cs.MM</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joong Ho Choi, Geonyeong Choi, Ji-Eun Han, Wonjin Yang, Zhi-Qi Cheng</dc:creator>
    </item>
    <item>
      <title>Representation Bias of Adolescents in AI: A Bilingual, Bicultural Study</title>
      <link>https://arxiv.org/abs/2408.01961</link>
      <description>arXiv:2408.01961v1 Announce Type: cross 
Abstract: Popular and news media often portray teenagers with sensationalism, as both a risk to society and at risk from society. As AI begins to absorb some of the epistemic functions of traditional media, we study how teenagers in two countries speaking two languages: 1) are depicted by AI, and 2) how they would prefer to be depicted. Specifically, we study the biases about teenagers learned by static word embeddings (SWEs) and generative language models (GLMs), comparing these with the perspectives of adolescents living in the U.S. and Nepal. We find English-language SWEs associate teenagers with societal problems, and more than 50% of the 1,000 words most associated with teenagers in the pretrained GloVe SWE reflect such problems. Given prompts about teenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss societal problems, most commonly violence, but also drug use, mental illness, and sexual taboo. Nepali models, while not free of such associations, are less dominated by social problems. Data from workshops with N=13 U.S. adolescents and N=18 Nepalese adolescents show that AI presentations are disconnected from teenage life, which revolves around activities like school and friendship. Participant ratings of how well 20 trait words describe teens are decorrelated from SWE associations, with Pearson's r=.02, n.s. in English FastText and r=.06, n.s. in GloVe; and r=.06, n.s. in Nepali FastText and r=-.23, n.s. in GloVe. U.S. participants suggested AI could fairly present teens by highlighting diversity, while Nepalese participants centered positivity. Participants were optimistic that, if it learned from adolescents, rather than media sources, AI could help mitigate stereotypes. Our work offers an understanding of the ways SWEs and GLMs misrepresent a developmentally vulnerable group and provides a template for less sensationalized characterization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01961v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Robert Wolfe, Aayushi Dangol, Bill Howe, Alexis Hiniker</dc:creator>
    </item>
    <item>
      <title>Self-centering 3-DOF feet controller for hands-free locomotion control in telepresence and virtual reality</title>
      <link>https://arxiv.org/abs/2408.02319</link>
      <description>arXiv:2408.02319v1 Announce Type: cross 
Abstract: We present a novel seated foot controller for handling 3-DOF aimed to control locomotion for telepresence robotics and virtual reality environments. Tilting the feet on two axes yields in forward, backward and sideways motion. In addition, a separate rotary joint allows for rotation around the vertical axis. Attached springs on all joints self-center the controller. The HTC Vive tracker is used to translate the trackers' orientation into locomotion commands. The proposed self-centering foot controller was used successfully for the ANA Avatar XPRIZE competition, where a naive operator traversed the robot through a longer distance, surpassing obstacles while solving various interaction and manipulation tasks in between. We publicly provide the models of the mostly 3D-printed feet controller for reproduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02319v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Raphael Memmesheimer, Christian Lenz, Max Schwarz, Michael Schreiber, Sven Behnke</dc:creator>
    </item>
    <item>
      <title>Towards Coarse-grained Visual Language Navigation Task Planning Enhanced by Event Knowledge Graph</title>
      <link>https://arxiv.org/abs/2408.02535</link>
      <description>arXiv:2408.02535v1 Announce Type: cross 
Abstract: Visual language navigation (VLN) is one of the important research in embodied AI. It aims to enable an agent to understand the surrounding environment and complete navigation tasks. VLN instructions could be categorized into coarse-grained and fine-grained commands. Fine-grained command describes a whole task with subtasks step-by-step. In contrast, coarse-grained command gives an abstract task description, which more suites human habits. Most existing work focuses on the former kind of instruction in VLN tasks, ignoring the latter abstract instructions belonging to daily life scenarios. To overcome the above challenge in abstract instruction, we attempt to consider coarse-grained instruction in VLN by event knowledge enhancement. Specifically, we first propose a prompt-based framework to extract an event knowledge graph (named VLN-EventKG) for VLN integrally over multiple mainstream benchmark datasets. Through small and large language model collaboration, we realize knowledge-enhanced navigation planning (named EventNav) for VLN tasks with coarse-grained instruction input. Additionally, we design a novel dynamic history backtracking module to correct potential error action planning in real time. Experimental results in various public benchmarks show our knowledge-enhanced method has superiority in coarse-grained-instruction VLN using our proposed VLN-EventKG with over $5\%$ improvement in success rate. Our project is available at https://sites.google.com/view/vln-eventkg</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02535v1</guid>
      <category>cs.IR</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhao Kaichen, Song Yaoxian, Zhao Haiquan, Liu Haoyu, Li Tiefeng, Li Zhixu</dc:creator>
    </item>
    <item>
      <title>Language Model Can Listen While Speaking</title>
      <link>https://arxiv.org/abs/2408.02622</link>
      <description>arXiv:2408.02622v1 Announce Type: cross 
Abstract: Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM) have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses both channels for autoregressive generation and detects turn-taking in real time. Three fusion strategies -- early fusion, middle fusion, and late fusion -- are explored, with middle fusion achieving an optimal balance between speech generation and real-time interaction. Two experimental settings, command-based FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity to diverse instructions. Our results highlight LSLM's capability to achieve duplex communication with minimal impact on existing systems. This study aims to advance the development of interactive speech dialogue systems, enhancing their applicability in real-world contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02622v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyang Ma, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen</dc:creator>
    </item>
    <item>
      <title>EmoWrite: A Sentiment Analysis-Based Thought to Text Conversion for Paralytic People</title>
      <link>https://arxiv.org/abs/2103.02238</link>
      <description>arXiv:2103.02238v2 Announce Type: replace 
Abstract: Objective- The objective of this study is to introduce EmoWrite, a novel brain-computer interface (BCI) system aimed at addressing the limitations of existing BCI-based systems. Specifically, the objective includes improving typing speed, accuracy, user convenience, emotional state capturing, and sentiment analysis within the context of BCI technology. Method- The method involves the development and implementation of EmoWrite, utilizing a user-centric Recurrent Neural Network (RNN) for thought-to-text conversion. The system incorporates visual feedback and introduces a dynamic keyboard with a contextually adaptive character appearance. Comprehensive evaluation and comparison against existing approaches are conducted, considering various metrics such as accuracy, typing speed, sentiment analysis, emotional state capturing, and user interface latency. Results- EmoWrite achieves notable results, including a typing speed of 6.6 Words Per Minute (WPM) and 31.9 Characters Per Minute (CPM) with a high accuracy rate of 90.36%. It excels in capturing emotional states, with an Information Transfer Rate (ITR) of 87.55 bits/min for commands and 72.52 bits/min for letters, surpassing other systems. Additionally, it offers an intuitive user interface with low latency of 2.685 seconds. Conclusion- The introduction of EmoWrite represents a significant stride towards enhancing BCI usability and emotional integration. The findings suggest that EmoWrite holds promising potential for revolutionizing communication aids for individuals with motor disabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.02238v2</guid>
      <category>cs.HC</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Imran Raza (COMSATS University Islamabad, Lahore Campus), Syed Asad Hussain (COMSATS University Islamabad, Lahore Campus), Muhammad Hasan Jamal (COMSATS University Islamabad, Lahore Campus), Mejdl Safran (King Saud University, Riyadh), Sultan Alfarhood (King Saud University, Riyadh), Imran Ashraf (Yeungnam University)</dc:creator>
    </item>
    <item>
      <title>Empowering Private Tutoring by Chaining Large Language Models</title>
      <link>https://arxiv.org/abs/2309.08112</link>
      <description>arXiv:2309.08112v2 Announce Type: replace 
Abstract: Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However, few approaches has been made toward a complete AI-powered tutoring system. In this work, we explore the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models (LLMs), covering automatic course planning and adjusting, tailored instruction, and flexible quiz evaluation. To make the system robust to prolonged interaction and cater to individualized education, the system is decomposed into three inter-connected core processes-interaction, reflection, and reaction. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. Tools are LLMs prompted to execute one specific task at a time, while memories are data storage that gets updated during education process. Statistical results from learning logs demonstrate the effectiveness and mechanism of each tool usage. Subjective feedback from human users reveal the usability of each function, and comparison with ablation systems further testify the benefits of the designed processes in long-term interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08112v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulin Chen, Ning Ding, Hai-Tao Zheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou</dc:creator>
    </item>
    <item>
      <title>Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review</title>
      <link>https://arxiv.org/abs/2310.03392</link>
      <description>arXiv:2310.03392v2 Announce Type: replace 
Abstract: Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, existing research on HAII is limited, fragmented, and inconsistent. We present here a survey of that literature and recommendations for research best practices that should improve the field. We divided our investigation into the following areas: 1) terms used to describe HAII, 2) primary roles of AI-enabled systems, 3) factors that influence HAII, and 4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, seven factors influence HAII: user characteristics (e.g., user personality), user perceptions and attitudes (e.g., user biases), user expectations and experience (e.g., mismatched user expectations and experience), AI interface and features (e.g., interactive design), AI output (e.g., perceived accuracy), explainability and interpretability (e.g., level of detail, user understanding), and usage of AI (e.g., heterogeneity of environments). HAII is most measured with user-related subjective metrics (e.g., user perceptions, trust, and attitudes), and AI-assisted decision-making is the most common primary role of AI-enabled systems. Based on this review, we conclude that there are substantial research gaps in HAII. Researchers and developers need to codify HAII terminology, involve users throughout the AI lifecycle (especially during development), and tailor HAII in safety-critical industries to the users and environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03392v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2024.3437190</arxiv:DOI>
      <arxiv:journal_reference>Access-2024-19782</arxiv:journal_reference>
      <dc:creator>Tita A. Bach, Jenny K. Kristiansen, Aleksandar Babic, Alon Jacovi</dc:creator>
    </item>
    <item>
      <title>DrawTalking: Building Interactive Worlds by Sketching and Speaking</title>
      <link>https://arxiv.org/abs/2401.05631</link>
      <description>arXiv:2401.05631v4 Announce Type: replace 
Abstract: We introduce DrawTalking, an approach to building and controlling interactive worlds by sketching and speaking while telling stories. It emphasizes user control and flexibility, and gives programming-like capability without requiring code. An early open-ended study with our prototype shows that the mechanics resonate and are applicable to many creative-exploratory use cases, with the potential to inspire and inform research in future natural interfaces for creative exploration and authoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05631v4</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <category>cs.GR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karl Toby Rosenberg, Rubaiat Habib Kazi, Li-Yi Wei, Haijun Xia, Ken Perlin</dc:creator>
    </item>
    <item>
      <title>User Experience In Dataset Search</title>
      <link>https://arxiv.org/abs/2403.15861</link>
      <description>arXiv:2403.15861v2 Announce Type: replace 
Abstract: This research investigates User Experience (UX) issues in dataset search, targeting Google Dataset Search and data.europa.eu. It focuses on 6 areas within UX: Initial Interaction, Search Process, Dataset Exploration, Filtering and Sorting, Dataset Actions, and Assistance and Feedback. The evaluation method combines 'The Pandemic Puzzle' user task, think-aloud methods, and demographic and post-task questionnaires. 29 strengths and 63 weaknesses were collected from 19 participants involved in roles within technology firm or academia. While certain insights are specific to particular platforms, most are derived from features commonly observed in dataset search platforms across a variety of fields, implying that our findings are broadly applicable. Observations from commonly found features in dataset search platforms across various fields have led to the development of 10 new design prototypes. Unlike literature retrieval, dataset retrieval involves a significant focus on metadata accessibility and quality, each element of which can impact decision-making. To address issues like reading fatigue from metadata presentation, inefficient methods for results searching, filtering, and selection, along with other unresolved user-centric issues on current platforms. These prototypes concentrate on enhancing metadata-related features. They include a redesigned homepage, an improved search bar, better sorting options, an enhanced search result display, a metadata comparison tool, and a navigation guide. Our aim is to improve usability for a wide range of users, including both developers and researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15861v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihang Zhao, Albert Mero\~no-Pe\~nuela, Elena Simperl</dc:creator>
    </item>
    <item>
      <title>EVE: Enabling Anyone to Train Robots using Augmented Reality</title>
      <link>https://arxiv.org/abs/2404.06089</link>
      <description>arXiv:2404.06089v3 Announce Type: replace 
Abstract: The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task requires expensive trajectory data where a trained human annotator moves a physical robot to train it. Consequently, only those with access to robots produce demonstrations to train robots. In this work, we remove this restriction with EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations, without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories. In a user study (N=14, D=30) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically moving a physical robot-in completion time, usability, motion intent communication, enjoyment, and preference (mean of p=0.30). EVE allows users to train robots for personalized tasks, such as sorting desk supplies, organizing ingredients, or setting up board games. We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06089v3</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Wang, Chun-Cheng Chang, Jiafei Duan, Dieter Fox, Ranjay Krishna</dc:creator>
    </item>
    <item>
      <title>Past, Present, and Future of Citation Practices in HCI</title>
      <link>https://arxiv.org/abs/2405.16526</link>
      <description>arXiv:2405.16526v3 Announce Type: replace 
Abstract: Science is a complex system comprised of many scientists who individually make collective decisions that, due to the size and nature of the academic system, largely do not affect the system as a whole. However, certain decisions at the meso-level of research communities, such as the Human-Computer Interaction (HCI) community, may result in deep and long-lasting behavioral changes in scientists. In this article, we provide evidence on how a change in editorial policies introduced at the ACM CHI Conference in 2016 launched the CHI community on an expansive path, denoted by a year-by-year increase in the mean number of references included in CHI articles. If this near-linear trend continues undisrupted, an article in CHI 2030 will include on average almost 130 references. The trend towards more citations reflects a citation culture where quantity is prioritized over quality, contributing to both author and peer reviewer fatigue. This article underscores the value of meta-research for research communities and the profound impact that meso-level policy adjustments have on the evolution of scientific fields and disciplines, urging stakeholders to carefully consider the broader implications of such changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16526v3</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Oppenlaender</dc:creator>
    </item>
    <item>
      <title>MunchSonic: Tracking Fine-grained Dietary Actions through Active Acoustic Sensing on Eyeglasses</title>
      <link>https://arxiv.org/abs/2405.21004</link>
      <description>arXiv:2405.21004v2 Announce Type: replace 
Abstract: We introduce MunchSonic, an AI-powered active acoustic sensing system integrated into eyeglasses to track fine-grained dietary actions. MunchSonic emits inaudible ultrasonic waves from the eyeglass frame, with the reflected signals capturing detailed positions and movements of body parts, including the mouth, jaw, arms, and hands involved in eating. These signals are processed by a deep learning pipeline to classify six actions: hand-to-mouth movements for food intake, chewing, drinking, talking, face-hand touching, and other activities (null). In an unconstrained study with 12 participants, MunchSonic achieved a 93.5% macro F1-score in a user-independent evaluation with a 2-second resolution in tracking these actions, also demonstrating its effectiveness in tracking eating episodes and food intake frequency within those episodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.21004v2</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3675095.3676619</arxiv:DOI>
      <dc:creator>Saif Mahmud, Devansh Agarwal, Ashwin Ajit, Qikang Liang, Thalia Viranda, Francois Guimbretiere, Cheng Zhang</dc:creator>
    </item>
    <item>
      <title>FAVis: Visual Analytics of Factor Analysis for Psychological Research</title>
      <link>https://arxiv.org/abs/2407.14072</link>
      <description>arXiv:2407.14072v2 Announce Type: replace 
Abstract: Psychological research often involves understanding psychological constructs through conducting factor analysis on data collected by a questionnaire, which can comprise hundreds of questions. Without interactive systems for interpreting factor models, researchers are frequently exposed to subjectivity, potentially leading to misinterpretations or overlooked crucial information. This paper introduces FAVis, a novel interactive visualization tool designed to aid researchers in interpreting and evaluating factor analysis results. FAVis enhances the understanding of relationships between variables and factors by supporting multiple views for visualizing factor loadings and correlations, allowing users to analyze information from various perspectives. The primary feature of FAVis is to enable users to set optimal thresholds for factor loadings to balance clarity and information retention. FAVis also allows users to assign tags to variables, enhancing the understanding of factors by linking them to their associated psychological constructs. Our user study demonstrates the utility of FAVis in various tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14072v2</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yikai Lu, Chaoli Wang</dc:creator>
    </item>
    <item>
      <title>Generative User-Experience Research for Developing Domain-specific Natural Language Processing Applications</title>
      <link>https://arxiv.org/abs/2306.16143</link>
      <description>arXiv:2306.16143v5 Announce Type: replace-cross 
Abstract: User experience (UX) is a part of human-computer interaction (HCI) research and focuses on increasing intuitiveness, transparency, simplicity, and trust for the system users. Most UX research for machine learning (ML) or natural language processing (NLP) focuses on a data-driven methodology. It engages domain users mainly for usability evaluation. Moreover, more typical UX methods tailor the systems towards user usability, unlike learning about the user needs first. This paper proposes a new methodology for integrating generative UX research into developing domain NLP applications. Generative UX research employs domain users at the initial stages of prototype development, i.e., ideation and concept evaluation, and the last stage for evaluating system usefulness and user utility. The methodology emerged from and is evaluated on a case study about the full-cycle prototype development of a domain-specific semantic search for daily operations in the process industry. A key finding of our case study is that involving domain experts increases their interest and trust in the final NLP application. The combined UX+NLP research of the proposed method efficiently considers data- and user-driven opportunities and constraints, which can be crucial for developing NLP applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16143v5</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasia Zhukova, Lukas von Sperl, Christian E. Matt, Bela Gipp</dc:creator>
    </item>
    <item>
      <title>FreeDrag: Feature Dragging for Reliable Point-based Image Editing</title>
      <link>https://arxiv.org/abs/2307.04684</link>
      <description>arXiv:2307.04684v4 Announce Type: replace-cross 
Abstract: To serve the intricate and varied demands of image editing, precise and flexible manipulation in image content is indispensable. Recently, Drag-based editing methods have gained impressive performance. However, these methods predominantly center on point dragging, resulting in two noteworthy drawbacks, namely "miss tracking", where difficulties arise in accurately tracking the predetermined handle points, and "ambiguous tracking", where tracked points are potentially positioned in wrong regions that closely resemble the handle points. To address the above issues, we propose FreeDrag, a feature dragging methodology designed to free the burden on point tracking. The FreeDrag incorporates two key designs, i.e., template feature via adaptive updating and line search with backtracking, the former improves the stability against drastic content change by elaborately controls feature updating scale after each dragging, while the latter alleviates the misguidance from similar points by actively restricting the search area in a line. These two technologies together contribute to a more stable semantic dragging with higher efficiency. Comprehensive experimental results substantiate that our approach significantly outperforms pre-existing methodologies, offering reliable point-based editing even in various complex scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04684v4</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen, Yi Jin, Jinjin Zheng</dc:creator>
    </item>
    <item>
      <title>Cluster Exploration using Informative Manifold Projections</title>
      <link>https://arxiv.org/abs/2309.14857</link>
      <description>arXiv:2309.14857v2 Announce Type: replace-cross 
Abstract: Dimensionality reduction (DR) is one of the key tools for the visual exploration of high-dimensional data and uncovering its cluster structure in two- or three-dimensional spaces. The vast majority of DR methods in the literature do not take into account any prior knowledge a practitioner may have regarding the dataset under consideration. We propose a novel method to generate informative embeddings which not only factor out the structure associated with different kinds of prior knowledge but also aim to reveal any remaining underlying structure. To achieve this, we employ a linear combination of two objectives: firstly, contrastive PCA that discounts the structure associated with the prior information, and secondly, kurtosis projection pursuit which ensures meaningful data separation in the obtained embeddings. We formulate this task as a manifold optimization problem and validate it empirically across a variety of datasets considering three distinct types of prior knowledge. Lastly, we provide an automated framework to perform iterative visual exploration of high-dimensional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14857v2</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stavros Gerolymatos, Xenophon Evangelopoulos, Vladimir Gusev, John Y. Goulermas</dc:creator>
    </item>
    <item>
      <title>Tell me why: Training preferences-based RL with human preferences and step-level explanations</title>
      <link>https://arxiv.org/abs/2405.14244</link>
      <description>arXiv:2405.14244v2 Announce Type: replace-cross 
Abstract: Human-in-the-loop reinforcement learning allows the training of agents through various interfaces, even for non-expert humans. Recently, preference-based methods (PbRL), where the human has to give his preference over two trajectories, increased in popularity since they allow training in domains where more direct feedback is hard to formulate. However, the current PBRL methods have limitations and do not provide humans with an expressive interface for giving feedback. With this work, we propose a new preference-based learning method that provides humans with a more expressive interface to provide their preference over trajectories and a factual explanation (or annotation of why they have this preference). These explanations allow the human to explain what parts of the trajectory are most relevant for the preference. We allow the expression of the explanations over individual trajectory steps. We evaluate our method in various simulations using a simulated human oracle (with realistic restrictions), and our results show that our extended feedback can improve the speed of learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14244v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakob Karalus</dc:creator>
    </item>
    <item>
      <title>Reassessing Evaluation Functions in Algorithmic Recourse: An Empirical Study from a Human-Centered Perspective</title>
      <link>https://arxiv.org/abs/2405.14264</link>
      <description>arXiv:2405.14264v2 Announce Type: replace-cross 
Abstract: In this study, we critically examine the foundational premise of algorithmic recourse - a process of generating counterfactual action plans (i.e., recourses) assisting individuals to reverse adverse decisions made by AI systems. The assumption underlying algorithmic recourse is that individuals accept and act on recourses that minimize the gap between their current and desired states. This assumption, however, remains empirically unverified. To address this issue, we conducted a user study with 362 participants and assessed whether minimizing the distance function, a metric of the gap between the current and desired states, indeed prompts them to accept and act upon suggested recourses. Our findings reveal a nuanced landscape: participants' acceptance of recourses did not correlate with the recourse distance. Moreover, participants' willingness to act upon recourses peaked at the minimal recourse distance but was otherwise constant. These findings cast doubt on the prevailing assumption of algorithmic recourse research and signal the need to rethink the evaluation functions to pave the way for human-centered recourse generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14264v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.24963/ijcai.2024/876</arxiv:DOI>
      <dc:creator>Tomu Tominaga, Naomi Yamashita, Takeshi Kurashima</dc:creator>
    </item>
    <item>
      <title>Do Generative AI Models Output Harm while Representing Non-Western Cultures: Evidence from A Community-Centered Approach</title>
      <link>https://arxiv.org/abs/2407.14779</link>
      <description>arXiv:2407.14779v3 Announce Type: replace-cross 
Abstract: Our research investigates the impact of Generative Artificial Intelligence (GAI) models, specifically text-to-image generators (T2Is), on the representation of non-Western cultures, with a focus on Indian contexts. Despite the transformative potential of T2Is in content creation, concerns have arisen regarding biases that may lead to misrepresentations and marginalizations. Through a community-centered approach and grounded theory analysis of 5 focus groups from diverse Indian subcultures, we explore how T2I outputs to English prompts depict Indian culture and its subcultures, uncovering novel representational harms such as exoticism and cultural misappropriation. These findings highlight the urgent need for inclusive and culturally sensitive T2I systems. We propose design guidelines informed by a sociotechnical perspective, aiming to address these issues and contribute to the development of more equitable and representative GAI technologies globally. Our work also underscores the necessity of adopting a community-centered approach to comprehend the sociotechnical dynamics of these models, complementing existing work in this space while identifying and addressing the potential negative repercussions and harms that may arise when these models are deployed on a global scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14779v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sourojit Ghosh, Pranav Narayanan Venkit, Sanjana Gautam, Shomir Wilson, Aylin Caliskan</dc:creator>
    </item>
    <item>
      <title>An Alternative to Multi-Factor Authentication with a Triple-Identity Authentication Scheme</title>
      <link>https://arxiv.org/abs/2407.19459</link>
      <description>arXiv:2407.19459v2 Announce Type: replace-cross 
Abstract: Every user authentication scheme involves three login credentials, i.e. a username, a password and a hash value, but only one of them is associated with a user identity. However, this single identity is not robust enough to protect the whole system and the login entries (i.e., the username and password forms) have not been effectively authenticated. Therefore, a multi-factor authentication service is utilized to help guarantee the account security by transmitting a second factor to the user to use. If more identities can be employed for the two login forms to associate with all login credentials, and if the corresponding identifiers are not transmitted via the network and operated by users, such a system can be more robust even without relying on a third-party service. To this end, a triple-identity authentication scheme is designed within a dual-password login-authentication system, which defines identities for the username and the login password, respectively. Therefore, in addition to the traditional server verification, the system can verify the identifiers at the username and password forms in succession. In the triple-identity authentication, the identifiers are entirely managed by the system without involvement of users or any third-party service, and they are concealed, incommunicable, inaccessible and independent of personal information. Thus, they are useless in online attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19459v2</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Suyun Borjigin</dc:creator>
    </item>
    <item>
      <title>Does empirical evidence from healthy aging studies predict a practical difference between visualizations for different age groups?</title>
      <link>https://arxiv.org/abs/2407.21767</link>
      <description>arXiv:2407.21767v2 Announce Type: replace-cross 
Abstract: When communicating critical information to decision-makers, one of the major challenges in visualization is whether the communication is affected by different perceptual or cognitive abilities, one major influencing factor is age. We review both visualization and psychophysics literature to understand where quantitative evidence exists on age differences in visual perception. Using contrast sensitivity data from the literature we show how the differences between visualizations for different age groups can be predicted using a new model of visible frequency range with age. The model assumed that at threshold values some visual data will not be visible to older people (spatial frequency &gt; 2 and contrast &lt;=0.01). We apply this result to a practical visualization and show an example that at higher levels of contrast, the visual signal should be perceivable by all viewers over 20. Universally usable visualization should use a contrast of 0.02 or higher and be designed to avoid spatial frequencies greater than eight cycles per degree to accommodate all ages. There remains much research to do on to translate psychophysics results to practical quantitative guidelines for visualization producers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21767v2</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Shao, Y. Li, A. I. Meso, N. Holliman</dc:creator>
    </item>
  </channel>
</rss>
