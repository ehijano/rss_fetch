<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Jul 2025 04:00:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Customized Memory-aware Architecture for Biological Sequence Alignment</title>
      <link>https://arxiv.org/abs/2507.22221</link>
      <description>arXiv:2507.22221v1 Announce Type: new 
Abstract: Sequence alignment is a fundamental process in computational biology which identifies regions of similarity in biological sequences. With the exponential growth in the volume of data in bioinformatics databases, the time, processing power, and memory bandwidth for comparing a query sequence with the available databases grows proportionally. The sequence alignment algorithms often involve simple arithmetic operations and feature high degrees of inherent fine-grained and coarse-grained parallelism. These features can be potentially exploited by a massive parallel processor, such as a GPU, to increase throughput. In this paper, we show that the excessive memory bandwidth demand of the sequence alignment algorithms prevents exploiting the maximum achievable throughput on conventional parallel machines. We then propose a memory-aware architecture to reduce the bandwidth demand of the sequence alignment algorithms, effectively pushing the memory wall to extract higher throughput. The design is integrated at the logic layer of an emerging 3D DRAM as a processing-in-memory architecture to further increase the available bandwidth. The experimental results show that the proposed architecture results in up to 2.4x speedup over a GPU-based design. Moreover, by moving the computation closer to the memory, power consumption is reduced by 37%, on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22221v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nasrin Akbari, Mehdi Modarressi, Alireza Khadem</dc:creator>
    </item>
    <item>
      <title>No Redundancy, No Stall: Lightweight Streaming 3D Gaussian Splatting for Real-time Rendering</title>
      <link>https://arxiv.org/abs/2507.21572</link>
      <description>arXiv:2507.21572v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) enables high-quality rendering of 3D scenes and is getting increasing adoption in domains like autonomous driving and embodied intelligence. However, 3DGS still faces major efficiency challenges when faced with high frame rate requirements and resource-constrained edge deployment. To enable efficient 3DGS, in this paper, we propose LS-Gaussian, an algorithm/hardware co-design framework for lightweight streaming 3D rendering. LS-Gaussian is motivated by the core observation that 3DGS suffers from substantial computation redundancy and stalls. On one hand, in practical scenarios, high-frame-rate 3DGS is often applied in settings where a camera observes and renders the same scene continuously but from slightly different viewpoints. Therefore, instead of rendering each frame separately, LS-Gaussian proposes a viewpoint transformation algorithm that leverages inter-frame continuity for efficient sparse rendering. On the other hand, as different tiles within an image are rendered in parallel but have imbalanced workloads, frequent hardware stalls also slow down the rendering process. LS-Gaussian predicts the workload for each tile based on viewpoint transformation to enable more balanced parallel computation and co-designs a customized 3DGS accelerator to support the workload-aware mapping in real-time. Experimental results demonstrate that LS-Gaussian achieves 5.41x speedup over the edge GPU baseline on average and up to 17.3x speedup with the customized accelerator, while incurring only minimal visual quality degradation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21572v2</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linye Wei, Jiajun Tang, Fan Fei, Boxin Shi, Runsheng Wang, Meng Li</dc:creator>
    </item>
  </channel>
</rss>
