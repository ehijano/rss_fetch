<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Jan 2026 02:40:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Schema-based active inference supports rapid generalization of experience and frontal cortical coding of abstract structure</title>
      <link>https://arxiv.org/abs/2601.18946</link>
      <description>arXiv:2601.18946v1 Announce Type: new 
Abstract: Schemas -- abstract relational structures that capture the commonalities across experiences -- are thought to underlie humans' and animals' ability to rapidly generalize knowledge, rebind new experiences to existing structures, and flexibly adapt behavior across contexts. Despite their central role in cognition, the computational principles and neural mechanisms supporting schema formation and use remain elusive. Here, we introduce schema-based hierarchical active inference (S-HAI), a novel computational framework that combines predictive processing and active inference with schema-based mechanisms. In S-HAI, a higher-level generative model encodes abstract task structure, while a lower-level model encodes spatial navigation, with the two levels linked by a grounding likelihood that maps abstract goals to physical locations. Through a series of simulations, we show that S-HAI reproduces key behavioral signatures of rapid schema-based generalization in spatial navigation tasks, including the ability to flexibly remap abstract schemas onto novel contexts, resolve goal ambiguity, and balance reuse versus accommodation of novel mappings. Crucially, S-HAI also reproduces prominent neural codes reported in rodent medial prefrontal cortex during a schema-dependent navigation and decision task, including task-invariant goal-progress cells, goal-identity cells, and goal-and-spatially conjunctive cells, as well as place-like codes at the lower level. Taken together, these results provide a mechanistic account of schema-based learning and inference that bridges behavior, neural data, and theory. More broadly, our findings suggest that schema formation and generalization may arise from predictive processing principles implemented hierarchically across cortical and hippocampal circuits, enabling the generalization of experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18946v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Toon Van de Maele, Tim Verbelen, Dileep George, Giovanni Pezzulo</dc:creator>
    </item>
    <item>
      <title>Smooth embeddings in contracting recurrent networks driven by regular dynamics: A synthesis for neural representation</title>
      <link>https://arxiv.org/abs/2601.19019</link>
      <description>arXiv:2601.19019v1 Announce Type: new 
Abstract: Recurrent neural networks trained for time-series prediction often develop latent trajectories that preserve qualitative structure of the dynamical systems generating their inputs. Recent empirical work has documented topology-preserving latent organization in trained recurrent models, and recent theoretical results in reservoir computing establish conditions under which the synchronization map is an embedding. Here we synthesize these threads into a unified account of when contracting recurrent networks yield smooth, topology-preserving internal representations for a broad and biologically relevant class of inputs: regular dynamics on invariant circles and tori.
  Our contribution is an integrated framework that assembles (i) generalized synchronization and embedding guarantees for contracting reservoirs, (ii) regularity mechanisms ensuring differentiability of the synchronization map under mild constraints, and (iii) a base-system viewpoint in which the invariant manifold generating the input stream is treated as the driving system. In this regular setting, the conditions commonly viewed as restrictive in chaotic-attractor analyses become mild and readily satisfied by standard contractive architectures. The framework clarifies how representational content in recurrent circuits is inherently historical: the network state encodes finite windows of input history rather than instantaneous stimuli.
  By consolidating disparate empirical and theoretical results under common assumptions, the synthesis yields concrete, testable expectations about when prediction-trained recurrent circuits should (or should not) form smooth latent embeddings and how required state dimension scales with the intrinsic dimension of the driving dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19019v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vikas N. O'Reilly-Shah, Alessandro Maria Selvitella</dc:creator>
    </item>
    <item>
      <title>Stroboscopic motion reversals in delay-coupled neural fields</title>
      <link>https://arxiv.org/abs/2601.19125</link>
      <description>arXiv:2601.19125v1 Announce Type: new 
Abstract: Visual illusions provide a window into the mechanisms underlying visual processing, and dynamical neural circuit models offer a natural framework for proposing and testing theories of their emergence. We propose and analyze a delay-coupled neural field model that explains stroboscopic percepts arising from the subsampling of a moving, often rotating, stimulus, such as the wagon-wheel illusion. Motivated by the role of activity propagation delays in shaping visual percepts, we study neural fields with both uniform and spatially dependent delays, representing the finite time required for signals to travel along axonal projections. Each module is organized as a ring of neurons encoding angular preference, with instantaneous local coupling and delayed long-range coupling strongest between neurons with similar preference. We show that delays generate a family of coexisting traveling bump solutions with distinct, quantized propagation speeds. Using interface-based asymptotic methods, we reduce the neural field dynamics to a low-dimensional system of coupled delay differential equations, enabling a detailed analysis of speed selection, stability, entrainment, and state transitions. Regularly pulsed inputs induce transitions between distinct speed states, including motion opposite to the forcing direction, capturing key features of visual aliasing and stroboscopic motion reversal. These results demonstrate how delayed neural interactions organize perception into discrete dynamical states and provide a mechanistic explanation for stroboscopic visual illusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19125v1</guid>
      <category>q-bio.NC</category>
      <category>math.DS</category>
      <category>nlin.PS</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noah Parks, Zachary P Kilpatrick</dc:creator>
    </item>
    <item>
      <title>The Copernican Argument for Alien Consciousness; The Mimicry Argument Against Robot Consciousness</title>
      <link>https://arxiv.org/abs/2412.00008</link>
      <description>arXiv:2412.00008v3 Announce Type: replace 
Abstract: On broadly Copernican grounds, we are entitled to assume that apparently behaviorally sophisticated extraterrestrial entities ("aliens") would be conscious. Otherwise, we humans would be inexplicably, implausibly lucky to have consciousness, while similarly behaviorally sophisticated entities elsewhere would be mere shells, devoid of consciousness. However, this Copernican default assumption is canceled in the case of behaviorally sophisticated entities designed to mimic superficial features associated with consciousness ("consciousness mimics"), and in particular a broad class of current, near-future, and hypothetical robots. These considerations, which we formulate, respectively, as the Copernican and Mimicry Arguments, jointly defeat an otherwise potentially attractive parity principle, according to which we should apply the same types of behavioral or cognitive tests to aliens and robots, attributing or denying consciousness similarly to the extent they perform similarly. Our approach is unusual in the following respect: Instead of grounding speculations about alien and robot consciousness in a particular metaphysical or scientific theory about the physical or functional bases of consciousness, we appeal directly to the epistemic principles of Copernican mediocrity and inference to the best explanation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00008v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Eric Schwitzgebel, Jeremy Pober</dc:creator>
    </item>
    <item>
      <title>Decoding Cortical Microcircuits: A Generative Model for Latent Space Exploration and Controlled Synthesis</title>
      <link>https://arxiv.org/abs/2506.11062</link>
      <description>arXiv:2506.11062v2 Announce Type: replace 
Abstract: A central idea in understanding brains and building artificial intelligence is that structure determines function. Yet, how the brain's complex structure arises from a limited set of genetic instructions remains a key question. The ultra high-dimensional detail of neural connections vastly exceeds the information storage capacity of genes, suggesting a compact, low-dimensional blueprint must guide brain development. Our motivation is to uncover this blueprint. We introduce a generative model, to learn this underlying representation from detailed connectivity maps of mouse cortical microcircuits. Our model successfully captures the essential structural information of these circuits in a compressed latent space. We found that specific, interpretable directions within this space directly relate to understandable network properties. Building on this, we demonstrate a novel method to controllably generate new, synthetic microcircuits with desired structural features by navigating this latent space. This work offers a new way to investigate the design principles of neural circuits and explore how structure gives rise to function, potentially informing the development of more advanced artificial neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11062v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>AAAI 2026 NeuroAI Workshop</arxiv:journal_reference>
      <dc:creator>Xingyu Liu, Yubin Li, Guozhang Chen</dc:creator>
    </item>
    <item>
      <title>KOCOBrain: Kuramoto-Guided Graph Network for Uncovering Structure-Function Coupling in Adolescent Prenatal Drug Exposure</title>
      <link>https://arxiv.org/abs/2601.11018</link>
      <description>arXiv:2601.11018v2 Announce Type: replace 
Abstract: Exposure to psychoactive substances during pregnancy, such as cannabis, can disrupt neurodevelopment and alter large-scale brain networks, yet identifying their neural signatures remains challenging. We introduced KOCOBrain: KuramotO COupled Brain Graph Network; a unified graph neural network framework that integrates structural and functional connectomes via Kuramoto-based phase dynamics and cognition-aware attention. The Kuramoto layer models neural synchronization over anatomical connections, generating phase-informed embeddings that capture structure-function coupling, while cognitive scores modulate information routing in a subject-specific manner followed by a joint objective enhancing robustness under class imbalance scenario. Applied to the ABCD cohort, KOCOBrain improved prenatal drug exposure prediction over relevant baselines and revealed interpretable structure-function patterns that reflect disrupted brain network coordination associated with early exposure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11018v2</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Badhan Mazumder, Lei Wu, Sir-Lord Wiafe, Vince D. Calhoun, Dong Hye Ye</dc:creator>
    </item>
    <item>
      <title>Noradrenergic-inspired gain modulation attenuates the stability gap in joint training</title>
      <link>https://arxiv.org/abs/2507.14056</link>
      <description>arXiv:2507.14056v2 Announce Type: replace-cross 
Abstract: Recent work in continual learning has highlighted the stability gap -- a temporary performance drop on previously learned tasks when new ones are introduced. This phenomenon reflects a mismatch between rapid adaptation and strong retention at task boundaries, underscoring the need for optimization mechanisms that balance plasticity and stability over abrupt distribution changes. While optimizers such as momentum-SGD and Adam introduce implicit multi-timescale behavior, they still exhibit pronounced stability gaps. Importantly, these gaps persist even under ideal joint training, making it crucial to study them in this setting to isolate their causes from other sources of forgetting. Motivated by how noradrenergic (neuromodulatory) bursts transiently increase neuronal gain under uncertainty, we introduce a dynamic gain scaling mechanism as a two-timescale optimization technique that balances adaptation and retention by modulating effective learning rates and flattening the local landscape through an effective reparameterization. Across domain- and class-incremental MNIST, CIFAR, and mini-ImageNet benchmarks under task-agnostic joint training, dynamic gain scaling effectively attenuates stability gaps while maintaining competitive accuracy, improving robustness at task transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14056v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Rodriguez-Garcia, Anindya Ghosh, Srikanth Ramaswamy</dc:creator>
    </item>
  </channel>
</rss>
