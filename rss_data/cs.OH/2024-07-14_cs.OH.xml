<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.OH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.OH</link>
    <description>cs.OH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.OH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Jul 2024 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Genetic Bottleneck and the Emergence of High Intelligence by Scaling-out and High Throughput</title>
      <link>https://arxiv.org/abs/2407.08743</link>
      <description>arXiv:2407.08743v1 Announce Type: new 
Abstract: We study the biological evolution of low-latency natural neural networks for short-term survival, and its parallels in the development of low latency high-performance Central Processing Unit in computer design and architecture. The necessity of accurate high-quality display of motion picture led to the special processing units known as the GPU, just as how special visual cortex regions of animals produced such low-latency computational capacity. The human brain, especially considered as nothing but a scaled-up version of a primate brain evolved in response to genomic bottleneck, producing a brain that is trainable and prunable by society, and as a further extension, invents language, writing and storage of narratives displaced in time and space. We conclude that this modern digital invention of social media and the archived collective common corpus has further evolved from just simple CPU-based low-latency fast retrieval to high-throughput parallel processing of data using GPUs to train Attention based Deep Learning Neural Networks producing Generative AI with aspects like toxicity, bias, memorization, hallucination, with intriguing close parallels in humans and their society. We show how this paves the way for constructive approaches to eliminating such drawbacks from human society and its proxy and collective large-scale mirror, the Generative AI of the LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08743v1</guid>
      <category>cs.OH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Arifa Khan, Saravanan P, Venkatesan S. K.</dc:creator>
    </item>
  </channel>
</rss>
