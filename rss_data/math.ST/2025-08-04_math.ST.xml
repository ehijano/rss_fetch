<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Aug 2025 02:29:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Parametric convergence rate of some nonparametric estimators in mixtures of power series distributions</title>
      <link>https://arxiv.org/abs/2508.00163</link>
      <description>arXiv:2508.00163v1 Announce Type: new 
Abstract: We consider the problem of estimating a mixture of power series distributions with infinite support, to which belong very well-known models such as Poisson, Geometric, Logarithmic or Negative Binomial probability mass functions. We consider the nonparametric maximum likelihood estimator (NPMLE) and show that, under very mild assumptions, it converges to the true mixture distribution $\pi_0$ at a rate no slower than $(\log n)^{3/2} n^{-1/2}$ in the Hellinger distance. Recent work on minimax lower bounds suggests that the logarithmic factor in the obtained Hellinger rate of convergence can not be improved, at least for mixtures of Poisson distributions. Furthermore, we construct nonparametric estimators that are based on the NPMLE and show that they converge to $\pi_0$ at the parametric rate $n^{-1/2}$ in the $\ell_p$-norm ($p \in [1, \infty]$ or $p \in [2, \infty])$: The weighted least squares and hybrid estimators. Simulations and a real data application are considered to assess the performance of all estimators we study in this paper and illustrate the practical aspect of the theory. The simulations results show that the NPMLE has the best performance in the Hellinger, $\ell_1$ and $\ell_2$ distances in all scenarios. Finally, to construct confidence intervals of the true mixture probability mass function, both the nonparametric and parametric bootstrap procedures are considered. Their performances are compared with respect to the coverage and length of the resulting intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00163v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fadoua Balabdaoui, Harald Besdziek, Yong Wang</dc:creator>
    </item>
    <item>
      <title>Structural Causal Models for Extremes: an Approach Based on Exponent Measures</title>
      <link>https://arxiv.org/abs/2508.00223</link>
      <description>arXiv:2508.00223v1 Announce Type: new 
Abstract: We introduce a new formulation of structural causal models for extremes, called the extremal structural causal model (eSCM). Unlike conventional structural causal models, where randomness is governed by a probability distribution, eSCMs use an exponent measure--an infinite-mass law that naturally arises in the analysis of multivariate extremes. Central to this framework are activation variables, which abstract the single-big-jump principle, along with additional randomization that enriches the class of eSCM laws. This formulation encompasses all possible laws of directed graphical models under the recently introduced notion of extremal conditional independence. We also identify an inherent asymmetry in eSCMs under natural assumptions, enabling the identifiability of causal directions, a central challenge in causal inference. Finally, we propose a method that utilizes this causal asymmetry and demonstrate its effectiveness in both simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00223v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fei Fang, Shuyang Bai, Tiandong Wang</dc:creator>
    </item>
    <item>
      <title>Predictive information criterion for jump diffusion processes</title>
      <link>https://arxiv.org/abs/2508.00411</link>
      <description>arXiv:2508.00411v1 Announce Type: new 
Abstract: In this paper, we address a model selection problem for ergodic jump diffusion processes based on high-frequency samples. We evaluate the expected genuine log-likelihood function and derive an Akaike-type information criterion. In the derivation process, we also give new estimates of the transition density of jump diffusion processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00411v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuma Uehara</dc:creator>
    </item>
    <item>
      <title>Constructive Disintegration and Conditional Modes</title>
      <link>https://arxiv.org/abs/2508.00617</link>
      <description>arXiv:2508.00617v1 Announce Type: new 
Abstract: Conditioning, the central operation in Bayesian statistics, is formalised by the notion of disintegration of measures. However, due to the implicit nature of their definition, constructing disintegrations is often difficult. A folklore result in machine learning conflates the construction of a disintegration with the restriction of probability density functions onto the subset of events that are consistent with a given observation. We provide a comprehensive set of mathematical tools which can be used to construct disintegrations and apply these to find densities of disintegrations on differentiable manifolds. Using our results, we provide a disturbingly simple example in which the restricted density and the disintegration density drastically disagree. Motivated by applications in approximate Bayesian inference and Bayesian inverse problems, we further study the modes of disintegrations. We show that the recently introduced notion of a "conditional mode" does not coincide in general with the modes of the conditional measure obtained through disintegration, but rather the modes of the restricted measure. We also discuss the implications of the discrepancy between the two measures in practice, advocating for the utility of both approaches depending on the modelling context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00617v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natha\"el Da Costa, Marvin Pf\"ortner, Jon Cockayne</dc:creator>
    </item>
    <item>
      <title>On admissibility in post-hoc hypothesis testing</title>
      <link>https://arxiv.org/abs/2508.00770</link>
      <description>arXiv:2508.00770v1 Announce Type: new 
Abstract: The validity of classical hypothesis testing requires the significance level $\alpha$ be fixed before any statistical analysis takes place. This is a stringent requirement. For instance, it prohibits updating $\alpha$ during (or after) an experiment due to changing concern about the cost of false positives, or to reflect unexpectedly strong evidence against the null. Perhaps most disturbingly, witnessing a p-value $p\ll\alpha$ vs $p\leq \alpha$ has no (statistical) relevance for any downstream decision-making. Following recent work of Gr\"unwald (2024), we develop a theory of post-hoc hypothesis testing, enabling $\alpha$ to be chosen after seeing and analyzing the data. To study "good" post-hoc tests we introduce $\Gamma$-admissibility, where $\Gamma$ is a set of adversaries which map the data to a significance level. A test is $\Gamma$-admissible if, roughly speaking, there is no other test which performs at least as well and sometimes better across all adversaries in $\Gamma$. For point nulls and alternatives, we prove general properties of any $\Gamma$-admissible test for any $\Gamma$ and show that they must be based on e-values. We also classify the set of admissible tests for various specific $\Gamma$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00770v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Chugg, Tyron Lardy, Aaditya Ramdas, Peter Gr\"unwald</dc:creator>
    </item>
    <item>
      <title>Local Poisson Deconvolution for Discrete Signals</title>
      <link>https://arxiv.org/abs/2508.00824</link>
      <description>arXiv:2508.00824v1 Announce Type: new 
Abstract: We analyze the statistical problem of recovering an atomic signal, modeled as a discrete uniform distribution $\mu$, from a binned Poisson convolution model. This question is motivated, among others, by super-resolution laser microscopy applications, where precise estimation of $\mu$ provides insights into spatial formations of cellular protein assemblies. Our main results quantify the local minimax risk of estimating $\mu$ for a broad class of smooth convolution kernels. This local perspective enables us to sharply quantify optimal estimation rates as a function of the clustering structure of the underlying signal. Moreover, our results are expressed under a multiscale loss function, which reveals that different parts of the underlying signal can be recovered at different rates depending on their local geometry. Overall, these results paint an optimistic perspective on the Poisson deconvolution problem, showing that accurate recovery is achievable under a much broader class of signals than suggested by existing global minimax analyses. Beyond Poisson deconvolution, our results also allow us to establish the local minimax rate of parameter estimation in Gaussian mixture models with uniform weights.
  We apply our methods to experimental super-resolution microscopy data to identify the location and configuration of individual DNA origamis. In addition, we complement our findings with numerical experiments on runtime and statistical recovery that showcase the practical performance of our estimators and their trade-offs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00824v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shayan Hundrieser, Tudor Manole, Danila Litskevich, Axel Munk</dc:creator>
    </item>
    <item>
      <title>Formal Power Series Representations in Probability and Expected Utility Theory</title>
      <link>https://arxiv.org/abs/2508.00294</link>
      <description>arXiv:2508.00294v1 Announce Type: cross 
Abstract: We advance a general theory of coherent preference that surrenders restrictions embodied in orthodox doctrine. This theory enjoys the property that any preference system admits extension to a complete system of preferences, provided it satisfies a certain coherence requirement analogous to the one de Finetti advanced for his foundations of probability. Unlike de Finetti's theory, the one we set forth requires neither transitivity nor Archimedeanness nor boundedness nor continuity of preference. This theory also enjoys the property that any complete preference system meeting the standard of coherence can be represented by utility in an ordered field extension of the reals. Representability by utility is a corollary of this paper's central result, which at once extends H\"older's Theorem and strengthens Hahn's Embedding Theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00294v1</guid>
      <category>math.PR</category>
      <category>cs.AI</category>
      <category>econ.TH</category>
      <category>math.LO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur Paul Pedersen, Samuel Allen Alexander</dc:creator>
    </item>
    <item>
      <title>Smooth Backfitting for Additive Hazard Rates</title>
      <link>https://arxiv.org/abs/2302.09510</link>
      <description>arXiv:2302.09510v2 Announce Type: replace 
Abstract: Smooth backfitting was first introduced in an additive regression setting via a direct projection alternative to the classic backfitting method by Buja, Hastie and Tibshirani. This paper translates the original smooth backfitting concept to a survival model considering an additively structured hazard. The model allows for censoring and truncation patterns occurring in many applications such as medical studies or actuarial reserving. Our estimators are shown to be a projection of the data into the space of multivariate hazard functions with smooth additive components. Hence, our hazard estimator is the closest nonparametric additive fit even if the actual hazard rate is not additive. This is different to other additive structure estimators where it is not clear what is being estimated if the model is not true. We provide full asymptotic theory for our estimators. We propose an implementation of estimators that show good performance in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09510v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephan M. Bischofberger, Munir Hiabu, Enno Mammen, Jens Perch Nielsen</dc:creator>
    </item>
    <item>
      <title>Sharp Anti-Concentration Inequalities for Extremum Statistics via Copulas</title>
      <link>https://arxiv.org/abs/2502.07699</link>
      <description>arXiv:2502.07699v2 Announce Type: replace 
Abstract: We derive sharp upper and lower bounds for the pointwise concentration function of the maximum statistic of $d$ identically distributed real-valued random variables. Our first main result places no restrictions either on the common marginal law of the samples or on the copula describing their joint distribution. We show that, in general, strictly sublinear dependence of the concentration function on the dimension $d$ is not possible. We then introduce a new class of copulas, namely those with a convex diagonal section, and demonstrate that restricting to this class yields a sharper upper bound on the concentration function. This allows us to establish several new dimension-independent and poly-logarithmic-in-$d$ anti-concentration inequalities for a variety of marginal distributions under mild dependence assumptions. Our theory improves upon the best known results in certain special cases. Applications to high-dimensional statistical inference are presented, including a specific example pertaining to Gaussian mixture approximations for factor models, for which our main results lead to superior distributional guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07699v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Ricardo P. Masini, William G. Underwood</dc:creator>
    </item>
    <item>
      <title>Conformal changepoint localization</title>
      <link>https://arxiv.org/abs/2505.00292</link>
      <description>arXiv:2505.00292v3 Announce Type: replace 
Abstract: Changepoint localization is the problem of estimating the index at which a change occurred in the data generating distribution of an ordered list of data, or declaring that no change occurred. We present the broadly applicable CONCH (CONformal CHangepoint localization) algorithm, which uses a matrix of conformal p-values to produce a confidence interval for a (single) changepoint under the mild assumption that the pre-change and post-change distributions are each exchangeable. We exemplify the CONCH algorithm on a variety of synthetic and real-world datasets, including using black-box pre-trained classifiers to detect changes in sequences of images, text, and accelerometer data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00292v3</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Riesz representers for the rest of us</title>
      <link>https://arxiv.org/abs/2507.19413</link>
      <description>arXiv:2507.19413v2 Announce Type: replace 
Abstract: The application of semiparametric efficient estimators, particularly those that leverage machine learning, is rapidly expanding within epidemiology and causal inference. This literature is increasingly invoking the Riesz representation theorem and Riesz regression. This paper aims to introduce the Riesz representation theorem to an epidemiologic audience, explaining what it is and why it's useful, using step-by-step worked examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19413v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicholas T. Williams, Oliver J. Hines, Kara E. Rudolph</dc:creator>
    </item>
    <item>
      <title>Strong Laws of Large Numbers for Generalizations of Fr\'echet Mean Sets</title>
      <link>https://arxiv.org/abs/2012.12762</link>
      <description>arXiv:2012.12762v3 Announce Type: replace-cross 
Abstract: A Fr\'echet mean of a random variable $Y$ with values in a metric space $(\mathcal Q, d)$ is an element of the metric space that minimizes $q \mapsto \mathbb E[d(Y,q)^2]$. This minimizer may be non-unique. We study strong laws of large numbers for sets of generalized Fr\'echet means. Following generalizations are considered: the minimizers of $\mathbb E[d(Y, q)^\alpha]$ for $\alpha &gt; 0$, the minimizers of $\mathbb E[H(d(Y, q))]$ for integrals $H$ of non-decreasing functions, and the minimizers of $\mathbb E[\mathfrak c(Y, q)]$ for a quite unrestricted class of cost functions $\mathfrak c$. We show convergence of empirical versions of these sets in outer limit and in one-sided Hausdorff distance. The derived results require only minimal assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2012.12762v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christof Sch\"otz</dc:creator>
    </item>
    <item>
      <title>Bagged Regularized $k$-Distances for Anomaly Detection</title>
      <link>https://arxiv.org/abs/2312.01046</link>
      <description>arXiv:2312.01046v3 Announce Type: replace-cross 
Abstract: We consider the paradigm of unsupervised anomaly detection, which involves the identification of anomalies within a dataset in the absence of labeled examples. Though distance-based methods are top-performing for unsupervised anomaly detection, they suffer heavily from the sensitivity to the choice of the number of the nearest neighbors. In this paper, we propose a new distance-based algorithm called bagged regularized $k$-distances for anomaly detection (BRDAD), converting the unsupervised anomaly detection problem into a convex optimization problem. Our BRDAD algorithm selects the weights by minimizing the surrogate risk, i.e., the finite sample bound of the empirical risk of the bagged weighted $k$-distances for density estimation (BWDDE). This approach enables us to successfully address the sensitivity challenge of the hyperparameter choice in distance-based algorithms. Moreover, when dealing with large-scale datasets, the efficiency issues can be addressed by the incorporated bagging technique in our BRDAD algorithm. On the theoretical side, we establish fast convergence rates of the AUC regret of our algorithm and demonstrate that the bagging technique significantly reduces the computational complexity. On the practical side, we conduct numerical experiments to illustrate the insensitivity of the parameter selection of our algorithm compared with other state-of-the-art distance-based methods. Furthermore, our method achieves superior performance on real-world datasets with the introduced bagging technique compared to other approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01046v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchao Cai, Hanfang Yang, Yuheng Ma, Hanyuan Hang</dc:creator>
    </item>
    <item>
      <title>Gaussian universality for approximately polynomial functions of high-dimensional data</title>
      <link>https://arxiv.org/abs/2403.10711</link>
      <description>arXiv:2403.10711v3 Announce Type: replace-cross 
Abstract: We establish an invariance principle for polynomial functions of $n$ independent, high-dimensional random vectors, and also show that the obtained rates are nearly optimal. Both the dimension of the vectors and the degree of the polynomial are permitted to grow with $n$. Specifically, we obtain a finite sample upper bound for the error of approximation by a polynomial of Gaussians, measured in Kolmogorov distance, and extend it to functions that are approximately polynomial in a mean squared error sense. We give a corresponding lower bound that shows the invariance principle holds up to polynomial degree $o(\log n)$. The proof is constructive and adapts an asymmetrisation argument due to V. V. Senatov. We also give a necessary and sufficient condition for asymptotic normality via the fourth moment phenomenon of Nualart and Peccati. As applications, we obtain a higher-order delta method with possibly non-Gaussian limits, and generalise a number of known results on high-dimensional and infinite-order U-statistics, and on fluctuations of subgraph counts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10711v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Han Huang, Morgane Austern, Peter Orbanz</dc:creator>
    </item>
    <item>
      <title>"All-Something-Nothing" Phase Transitions in Planted k-Factor Recovery</title>
      <link>https://arxiv.org/abs/2503.08984</link>
      <description>arXiv:2503.08984v2 Announce Type: replace-cross 
Abstract: This paper studies the problem of inferring a $k$-factor, specifically a spanning $k$-regular graph, planted within an Erdos--Renyi random graph $G(n,\lambda/n)$. We uncover an interesting "all-something-nothing" phase transition. Specifically, we show that as the average degree $\lambda$ surpasses the critical threshold of $1/k$, the inference problem undergoes a transition from almost exact recovery ("all" phase) to partial recovery ("something" phase). Moreover, as $\lambda$ tends to infinity, the accuracy of recovery diminishes to zero, leading to the onset of the "nothing" phase. This finding complements the recent result by Mossel, Niles-Weed, Sohn, Sun, and Zadik who established that for certain sufficiently dense graphs, the problem undergoes an "all-or-nothing" phase transition, jumping from near-perfect to near-zero recovery. In addition, we characterize the recovery accuracy of a linear-time iterative pruning algorithm and show that it achieves almost exact recovery when $\lambda &lt; 1/k$. A key component of our analysis is a two-step cycle construction: we first build trees through local neighborhood exploration and then connect them by sprinkling using reserved edges. Interestingly, for proving impossibility of almost exact recovery, we construct $\Theta(n)$ many small trees of size $\Theta(1)$, whereas for establishing the algorithmic lower bound, a single large tree of size $\Theta(\sqrt{n\log n})$ suffices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08984v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Gaudio, Colin Sandon, Jiaming Xu, Dana Yang</dc:creator>
    </item>
    <item>
      <title>Conditional independence testing with a single realization of a multivariate nonstationary nonlinear time series</title>
      <link>https://arxiv.org/abs/2504.21647</link>
      <description>arXiv:2504.21647v2 Announce Type: replace-cross 
Abstract: Identifying relationships among stochastic processes is a core objective in many fields, such as economics. While the standard toolkit for multivariate time series analysis has many advantages, it can be difficult to capture nonlinear dynamics using linear vector autoregressive models. This difficulty has motivated the development of methods for causal discovery and variable selection for nonlinear time series, which routinely employ tests for conditional independence. In this paper, we introduce the first framework for conditional independence testing that works with a single realization of a nonstationary nonlinear process. We also show how our framework can be used to test for independence. The key technical ingredients of our framework are time-varying nonlinear regression, estimation of local long-run covariance matrices of products of error processes, and a distribution-uniform strong Gaussian approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21647v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Wieck-Sosa, Michel F. C. Haddad, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Batched Nonparametric Bandits via k-Nearest Neighbor UCB</title>
      <link>https://arxiv.org/abs/2505.10498</link>
      <description>arXiv:2505.10498v2 Announce Type: replace-cross 
Abstract: We study sequential decision-making in batched nonparametric contextual bandits, where actions are selected over a finite horizon divided into a small number of batches. Motivated by constraints in domains such as medicine and marketing -- where online feedback is limited -- we propose a nonparametric algorithm that combines adaptive k-nearest neighbor (k-NN) regression with the upper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully nonparametric, adapts to the context dimension, and is simple to implement. Unlike prior work relying on parametric or binning-based estimators, BaNk-UCB uses local geometry to estimate rewards and adaptively balances exploration and exploitation. We provide near-optimal regret guarantees under standard Lipschitz smoothness and margin assumptions, using a theoretically motivated batch schedule that balances regret across batches and achieves minimax-optimal rates. Empirical evaluations on synthetic and real-world datasets demonstrate that BaNk-UCB consistently outperforms binning-based baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10498v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakshi Arya</dc:creator>
    </item>
  </channel>
</rss>
