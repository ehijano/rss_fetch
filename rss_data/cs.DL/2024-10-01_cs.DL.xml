<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Oct 2024 02:01:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Unique Taste of LLMs for Papers: Potential issues in Using LLMs for Digital Library Document Recommendation Tasks</title>
      <link>https://arxiv.org/abs/2409.19868</link>
      <description>arXiv:2409.19868v2 Announce Type: new 
Abstract: This paper investigates the performance of several representative large models in the field of literature recommendation and explores potential biases. The results indicate that while some large models' recommendations can be somewhat satisfactory after simple manual screening, overall, the accuracy of these models in specific literature recommendation tasks is generally moderate. Additionally, the models tend to recommend literature that is timely, collaborative, and expands or deepens the field. In scholar recommendation tasks. There is no evidence to suggest that LLMs exacerbate inequalities related to gender, race, or the level of development of countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19868v2</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Tian, Yixin Liu, Yi Bu</dc:creator>
    </item>
    <item>
      <title>Trapped in Transformative Agreements? A Multifaceted Analysis of &gt;1,000 Contracts</title>
      <link>https://arxiv.org/abs/2409.20224</link>
      <description>arXiv:2409.20224v1 Announce Type: new 
Abstract: Transformative agreements between academic publishers and research institutions are ubiquitous. The 'Efficiency and Standards for Article Charges' (ESAC) Initiative lists more than 1,000 contracts in its database. We make use of this unique dataset by web-scraping the details of every contract to substantially expand the overview spreadsheet provided by the ESAC Initiative. Based on that hitherto unused data source, we combine qualitative and quantitative methods to conduct an in-depth analysis of the contract characteristics and the TA landscape. Our analysis demonstrates that research institutions seem to be 'trapped' in transformative agreements. Instead of being a bridge towards a fully Open Access world, academia is stuck in the hybrid system. This endows the legacy (non-Open Access) publishing houses with substantial market power. It raises entry barriers, lowers competition, and increases costs for libraries and universities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20224v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Rothfritz, W. Benedikt Schmal, Ulrich Herb</dc:creator>
    </item>
    <item>
      <title>Does the Use of Unusual Combinations of Datasets Contribute to Greater Scientific Impact?</title>
      <link>https://arxiv.org/abs/2402.05024</link>
      <description>arXiv:2402.05024v4 Announce Type: replace 
Abstract: Scientific datasets play a crucial role in contemporary data-driven research, as they allow for the progress of science by facilitating the discovery of new patterns and phenomena. This mounting demand for empirical research raises important questions on how strategic data utilization in research projects can stimulate scientific advancement. In this study, we examine the hypothesis inspired by the recombination theory, which suggests that innovative combinations of existing knowledge, including the use of unusual combinations of datasets, can lead to high-impact discoveries. Focusing on social science, we investigate the scientific outcomes of such atypical data combinations in more than 30,000 publications that leverage over 5,000 datasets curated within one of the largest social science databases, ICPSR. This study offers four important insights. First, combining datasets, particularly those infrequently paired, significantly contributes to both scientific and broader impacts (e.g., dissemination to the general public). Second, infrequently paired datasets maintain a strong association with citation even after controlling for the atypicality of dataset topics. In contrast, the atypicality of dataset topics has a much smaller positive impact on citation counts. Third, smaller and less experienced research teams tend to use atypical combinations of datasets in research more frequently than their larger and more experienced counterparts. Lastly, despite the benefits of data combination, papers that amalgamate data remain infrequent. This finding suggests that the unconventional combination of datasets is an under-utilized but powerful strategy correlated with the scientific impact and broader dissemination of scientific discoveries</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05024v4</guid>
      <category>cs.DL</category>
      <category>cs.SI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yulin Yu, Daniel M. Romero</dc:creator>
    </item>
    <item>
      <title>LLAssist: Simple Tools for Automating Literature Review Using Large Language Models</title>
      <link>https://arxiv.org/abs/2407.13993</link>
      <description>arXiv:2407.13993v2 Announce Type: replace 
Abstract: This paper introduces LLAssist, an open-source tool designed to streamline literature reviews in academic research. In an era of exponential growth in scientific publications, researchers face mounting challenges in efficiently processing vast volumes of literature. LLAssist addresses this issue by leveraging Large Language Models (LLMs) and Natural Language Processing (NLP) techniques to automate key aspects of the review process. Specifically, it extracts important information from research articles and evaluates their relevance to user-defined research questions. The goal of LLAssist is to significantly reduce the time and effort required for comprehensive literature reviews, allowing researchers to focus more on analyzing and synthesizing information rather than on initial screening tasks. By automating parts of the literature review workflow, LLAssist aims to help researchers manage the growing volume of academic publications more efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13993v2</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoforus Yoga Haryanto</dc:creator>
    </item>
    <item>
      <title>La R\'evolution D\'evore ses Enfants: Pricing Implications of Transformative Agreements</title>
      <link>https://arxiv.org/abs/2403.03597</link>
      <description>arXiv:2403.03597v2 Announce Type: replace-cross 
Abstract: With the widespread dissemination of the internet, academia envisioned free availability and rapid dissemination of new knowledge. However, most researchers continued publishing in established journals instead of switching to fully open-access alternatives. That preserved the market power of the large commercial publishing houses owning thousands of journals behind paywalls. To turn these portfolios into open access, research institutions around the globe have been negotiating `transformative agreements:' Papers are published fully open access, and universities pay only for the publication but not for subscriptions any longer. In this paper, I demonstrate that publishers controlling a large stock of paywalled publications can use them as leverage to ensure high revenues even with decreasing publication numbers. By that, this industrial policy may harm competitors that only publish under open access. This could harm competition and perpetuate the position of the incumbent players.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03597v2</guid>
      <category>econ.GN</category>
      <category>cs.DL</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>W. Benedikt Schmal</dc:creator>
    </item>
    <item>
      <title>An Intelligent Innovation Dataset on Scientific Research Outcomes</title>
      <link>https://arxiv.org/abs/2409.06936</link>
      <description>arXiv:2409.06936v2 Announce Type: replace-cross 
Abstract: Various stakeholders, such as researchers, government agencies, businesses, and research laboratories require a large volume of reliable scientific research outcomes including research articles and patent data to support their work. These data are crucial for a variety of application, such as advancing scientific research, conducting business evaluations, and undertaking policy analysis. However, collecting such data is often a time-consuming and laborious task. Consequently, many users turn to using openly accessible data for their research. However, these existing open dataset releases typically suffer from lack of relationship between different data sources and a limited temporal coverage. To address this issue, we present a new open dataset, the Intelligent Innovation Dataset (IIDS), which comprises six interrelated datasets spanning nearly 120 years, encompassing paper information, paper citation relationships, patent details, patent legal statuses, and funding information. The extensive contextual and extensive temporal coverage of the IIDS dataset will provide researchers and practitioners and policy maker with comprehensive data support, enabling them to conduct in-depth scientific research and comprehensive data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06936v2</guid>
      <category>cs.DB</category>
      <category>cs.DL</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinran Wu, Hui Zou, Yidan Xing, Jingjing Qu, Qiongxiu Li, Renxia Xue, Xiaoming Fu</dc:creator>
    </item>
    <item>
      <title>LLM-CARD: Towards a Description and Landscape of Large Language Models</title>
      <link>https://arxiv.org/abs/2409.17011</link>
      <description>arXiv:2409.17011v2 Announce Type: replace-cross 
Abstract: With the rapid growth of the Natural Language Processing (NLP) field, a vast variety of Large Language Models (LLMs) continue to emerge for diverse NLP tasks. As an increasing number of papers are presented, researchers and developers face the challenge of information overload. Thus, it is particularly important to develop a system that can automatically extract and organise key information about LLMs from academic papers (\textbf{LLM model card}). This work is to develop such a pioneer system by using Named Entity Recognition (\textbf{NER}) and Relation Extraction (\textbf{RE}) methods that automatically extract key information about large language models from the papers, helping researchers to efficiently access information about LLMs. These features include model \textit{licence}, model \textit{name}, and model \textit{application}. With these features, we can form a model card for each paper. \textbf{Data-contribution} wise, 106 academic papers were processed by defining three dictionaries - LLMs name, licence, and application. 11,051 sentences were extracted through dictionary lookup, and the dataset was constructed through manual review of the final selection of 129 sentences that have a link between the name and the licence, and 106 sentences that have a link between the model name and the application. Data and code in \textsc{LLM-Card} is openly hosted at \url{https://github.com/shengwei-tian/dependency-parser-visualization}</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17011v2</guid>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shengwei Tian, Lifeng Han, Erick Mendez Guzman, Goran Nenadic</dc:creator>
    </item>
  </channel>
</rss>
