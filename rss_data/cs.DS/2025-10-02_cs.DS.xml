<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Oct 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>One-Sided Local Crossing Minimization</title>
      <link>https://arxiv.org/abs/2510.00331</link>
      <description>arXiv:2510.00331v1 Announce Type: new 
Abstract: Drawing graphs with the minimum number of crossings is a classical problem that has been studied extensively. Many restricted versions of the problem have been considered. For example, bipartite graphs can be drawn such that the two sets in the bipartition of the vertex set are mapped to two parallel lines, and the edges are drawn as straight-line segments. In this setting, the number of crossings depends only on the ordering of the vertices on the two lines. Two natural variants of the problem have been studied. In the one-sided case, the order of the vertices on one of the two lines is given and fixed; in the two-sided case, no order is given. Both cases are important subproblems in the so-called Sugiyama framework for drawing layered graphs with few crossings, and both turned out to be NP-hard. For the one-sided case, Eades and Wormald [Algorithmica 1994] introduced the median heuristic and showed that it has an approximation ratio of 3.
  In recent years, researchers have focused on a local version of crossing minimization where the aim is not to minimize the total number of crossings but the maximum number of crossings per edge. Kobayashi, Okada, and Wolff [SoCG 2025] investigated the complexity of local crossing minimization parameterized by the natural parameter. They showed that the weighted one-sided problem is NP-hard and conjectured that the unweighted one-sided case remains NP-hard. In this work, we confirm their conjecture. We also prove that the median heuristic with a specific tie-breaking scheme has an approximation ratio of 3.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00331v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grzegorz Gutowski, Maarten L\"offler, Yuto Okada, Alexander Wolff</dc:creator>
    </item>
    <item>
      <title>Differentially Private Learning of Exponential Distributions: Adaptive Algorithms and Tight Bounds</title>
      <link>https://arxiv.org/abs/2510.00790</link>
      <description>arXiv:2510.00790v1 Announce Type: new 
Abstract: We study the problem of learning exponential distributions under differential privacy. Given $n$ i.i.d.\ samples from $\mathrm{Exp}(\lambda)$, the goal is to privately estimate $\lambda$ so that the learned distribution is close in total variation distance to the truth. We present two complementary pure DP algorithms: one adapts the classical maximum likelihood estimator via clipping and Laplace noise, while the other leverages the fact that the $(1-1/e)$-quantile equals $1/\lambda$. Each method excels in a different regime, and we combine them into an adaptive best-of-both algorithm achieving near-optimal sample complexity for all $\lambda$. We further extend our approach to Pareto distributions via a logarithmic reduction, prove nearly matching lower bounds using packing and group privacy \cite{Karwa2017FiniteSD}, and show how approximate $(\epsilon,\delta)$-DP removes the need for externally supplied bounds. Together, these results give the first tight characterization of exponential distribution learning under DP and illustrate the power of adaptive strategies for heavy-tailed laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00790v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bar Mahpud, Or Sheffet</dc:creator>
    </item>
    <item>
      <title>Boundaried Kernelization via Representative Sets</title>
      <link>https://arxiv.org/abs/2510.00832</link>
      <description>arXiv:2510.00832v1 Announce Type: new 
Abstract: A kernelization is an efficient algorithm that given an instance of a parameterized problem returns an equivalent instance of size bounded by some function of the input parameter value. It is quite well understood which problems do or (conditionally) do not admit a kernelization where this size bound is polynomial, a so-called polynomial kernelization. Unfortunately, such polynomial kernelizations are known only in fairly restrictive settings where a small parameter value corresponds to a strong restriction on the global structure on the instance. Motivated by this, Antipov and Kratsch [WG 2025] proposed a local variant of kernelization, called boundaried kernelization, that requires only local structure to achieve a local improvement of the instance, which is in the spirit of protrusion replacement used in meta-kernelization [Bodlaender et al.\ JACM 2016]. They obtain polynomial boundaried kernelizations as well as (unconditional) lower bounds for several well-studied problems in kernelization.
  In this work, we leverage the matroid-based techniques of Kratsch and Wahlstr\"om [JACM 2020] to obtain randomized polynomial boundaried kernelizations for \smultiwaycut, \dtmultiwaycut, \oddcycletransversal, and \vertexcoveroct, for which randomized polynomial kernelizations in the usual sense were known before. A priori, these techniques rely on the global connectivity of the graph to identify reducible (irrelevant) vertices. Nevertheless, the separation of the local part by its boundary turns out to be sufficient for a local application of these methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00832v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonid Antipov, Stefan Kratsch</dc:creator>
    </item>
    <item>
      <title>Degree-bounded Online Bipartite Matching: OCS vs. Ranking</title>
      <link>https://arxiv.org/abs/2510.00965</link>
      <description>arXiv:2510.00965v1 Announce Type: new 
Abstract: We revisit the online bipartite matching problem on $d$-regular graphs, for which Cohen and Wajc (SODA 2018) proposed an algorithm with a competitive ratio of $1-2\sqrt{H_d/d} = 1-O(\sqrt{(\log d)/d})$ and showed that it is asymptotically near-optimal for $d=\omega(1)$. However, their ratio is meaningful only for sufficiently large $d$, e.g., the ratio is less than $1-1/e$ when $d\leq 168$. In this work, we study the problem on $(d,d)$-bounded graphs (a slightly more general class of graphs than $d$-regular) and consider two classic algorithms for online matching problems: \Ranking and Online Correlated Selection (OCS). We show that for every fixed $d\geq 2$, the competitive ratio of OCS is at least $0.835$ and always higher than that of \Ranking. When $d\to \infty$, we show that OCS is at least $0.897$-competitive while \Ranking is at most $0.816$-competitive. We also show some extensions of our results to $(k,d)$-bounded graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00965v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yilong Feng, Haolong Li, Xiaowei Wu, Shengwei Zhou</dc:creator>
    </item>
    <item>
      <title>Perfect Fractional Matchings in Bipartite Graphs Via Proportional Allocations</title>
      <link>https://arxiv.org/abs/2510.01107</link>
      <description>arXiv:2510.01107v1 Announce Type: new 
Abstract: Given a bipartite graph that has a perfect matching, a prefect proportional allocation is an assignment of positive weights to the nodes of the right partition so that every left node is fractionally assigned to its neighbors in proportion to their weights, and these assignments define a fractional perfect matching. We prove that a bipartite graph has a perfect proportional allocation if and only if it is matching covered, by using a classical result on matrix scaling. We also present an extension of this result to provide simple proportional allocations in non-matching-covered bipartite graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01107v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Hathcock, R. Ravi</dc:creator>
    </item>
    <item>
      <title>Private Learning of Littlestone Classes, Revisited</title>
      <link>https://arxiv.org/abs/2510.00076</link>
      <description>arXiv:2510.00076v1 Announce Type: cross 
Abstract: We consider online and PAC learning of Littlestone classes subject to the constraint of approximate differential privacy. Our main result is a private learner to online-learn a Littlestone class with a mistake bound of $\tilde{O}(d^{9.5}\cdot \log(T))$ in the realizable case, where $d$ denotes the Littlestone dimension and $T$ the time horizon. This is a doubly-exponential improvement over the state-of-the-art [GL'21] and comes polynomially close to the lower bound for this task.
  The advancement is made possible by a couple of ingredients. The first is a clean and refined interpretation of the ``irreducibility'' technique from the state-of-the-art private PAC-learner for Littlestone classes [GGKM'21]. Our new perspective also allows us to improve the PAC-learner of [GGKM'21] and give a sample complexity upper bound of $\widetilde{O}(\frac{d^5 \log(1/\delta\beta)}{\varepsilon \alpha})$ where $\alpha$ and $\beta$ denote the accuracy and confidence of the PAC learner, respectively. This improves over [GGKM'21] by factors of $\frac{d}{\alpha}$ and attains an optimal dependence on $\alpha$.
  Our algorithm uses a private sparse selection algorithm to \emph{sample} from a pool of strongly input-dependent candidates. However, unlike most previous uses of sparse selection algorithms, where one only cares about the utility of output, our algorithm requires understanding and manipulating the actual distribution from which an output is drawn. In the proof, we use a sparse version of the Exponential Mechanism from [GKM'21] which behaves nicely under our framework and is amenable to a very easy utility proof.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00076v1</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Lyu</dc:creator>
    </item>
    <item>
      <title>Dynamic Necklace Splitting</title>
      <link>https://arxiv.org/abs/2510.00162</link>
      <description>arXiv:2510.00162v1 Announce Type: cross 
Abstract: The necklace splitting problem is a classic problem in fair division with many applications, including data-informed fair hash maps. We extend necklace splitting to a dynamic setting, allowing for relocation, insertion, and deletion of beads. We present linear-time, optimal algorithms for the two-color case that support all dynamic updates. For more than two colors, we give linear-time, optimal algorithms for relocation subject to a restriction on the number of agents. Finally, we propose a randomized algorithm for the two-color case that handles all dynamic updates, guarantees approximate fairness with high probability, and runs in polylogarithmic time when the number of agents is small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00162v1</guid>
      <category>cs.GT</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rishi Advani, Abolfazl Asudeh, Mohsen Dehghankar, Stavros Sintos</dc:creator>
    </item>
    <item>
      <title>Privacy-Preserving Learning-Augmented Data Structures</title>
      <link>https://arxiv.org/abs/2510.00165</link>
      <description>arXiv:2510.00165v1 Announce Type: cross 
Abstract: Learning-augmented data structures use predicted frequency estimates to retrieve frequently occurring database elements faster than standard data structures. Recent work has developed data structures that optimally exploit these frequency estimates while maintaining robustness to adversarial prediction errors. However, the privacy and security implications of this setting remain largely unexplored.
  In the event of a security breach, data structures should reveal minimal information beyond their current contents. This is even more crucial for learning-augmented data structures, whose layout adapts to the data. A data structure is history independent if its memory representation reveals no information about past operations except what is inferred from its current contents. In this work, we take the first step towards privacy and security guarantees in this setting by proposing the first learning-augmented data structure that is strongly history independent, robust, and supports dynamic updates.
  To achieve this, we introduce two techniques: thresholding, which automatically makes any learning-augmented data structure robust, and pairing, a simple technique that provides strong history independence in the dynamic setting. Our experimental results demonstrate a tradeoff between security and efficiency but are still competitive with the state of the art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00165v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prabhav Goyal, Vinesh Sridhar, Wilson Zheng</dc:creator>
    </item>
    <item>
      <title>Query-Optimal Estimation of Unitary Channels via Pauli Dimensionality</title>
      <link>https://arxiv.org/abs/2510.00168</link>
      <description>arXiv:2510.00168v1 Announce Type: cross 
Abstract: We study process tomography of unitary channels whose Pauli spectrum is supported on a small subgroup. Given query access to an unknown unitary channel whose Pauli spectrum is supported on a subgroup of size $2^k$, our goal is to output a classical description that is $\epsilon$-close to the unknown unitary in diamond distance. We present an algorithm that achieves this using $O(2^k/\epsilon)$ queries, and we prove matching lower bounds, establishing query optimality of our algorithm. When $k = 2n$, so that the support is the full Pauli group, our result recovers the query-optimal $O(4^n/\epsilon)$-query algorithm of Haah, Kothari, O'Donnell, and Tang [FOCS '23].
  Our result has two notable consequences. First, we give a query-optimal $O(4^k/\epsilon)$-query algorithm for learning quantum $k$-juntas -- unitary channels that act non-trivially on only $k$ of the $n$ qubits -- to accuracy $\epsilon$ in diamond distance. This represents an exponential improvement in both query and time complexity over prior work.
  Second, we give a computationally efficient algorithm for learning compositions of depth-$O(\log \log n)$ circuits with near-Clifford circuits, where "near-Clifford" means a Clifford circuit augmented with at most $O(\log n)$ non-Clifford single-qubit gates. This unifies prior work, which could handle only constant-depth circuits or near-Clifford circuits, but not their composition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00168v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sabee Grewal, Daniel Liang</dc:creator>
    </item>
    <item>
      <title>SPAM Tolerance for Pauli Error Estimation</title>
      <link>https://arxiv.org/abs/2510.00230</link>
      <description>arXiv:2510.00230v1 Announce Type: cross 
Abstract: The Pauli channel is a fundamental model of noise in quantum systems, motivating the task of Pauli error estimation. We present an algorithm that builds on the reduction to Population Recovery introduced in [FO21]. Addressing an open question from that work, our algorithm has the key advantage of robustness against even severe state preparation and measurement (SPAM) errors. To tolerate SPAM, we must analyze Population Recovery on a combined erasure/bit-flip channel, which necessitates extending the complex analysis techniques from [PSW17, DOS17]. For $n$-qubit channels, our Pauli error estimation algorithm requires only $\exp(n^{1/3})$ unentangled state preparations and measurements, improving on previous SPAM-tolerant algorithms that had $2^n$-dependence even for restricted families of Pauli channels. We also give evidence that no SPAM-tolerant method can make asymptotically fewer than $\exp(n^{1/3})$ uses of the channel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00230v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan O'Donnell, Samvitti Sharma</dc:creator>
    </item>
    <item>
      <title>Privately Estimating Black-Box Statistics</title>
      <link>https://arxiv.org/abs/2510.00322</link>
      <description>arXiv:2510.00322v1 Announce Type: cross 
Abstract: Standard techniques for differentially private estimation, such as Laplace or Gaussian noise addition, require guaranteed bounds on the sensitivity of the estimator in question. But such sensitivity bounds are often large or simply unknown. Thus we seek differentially private methods that can be applied to arbitrary black-box functions. A handful of such techniques exist, but all are either inefficient in their use of data or require evaluating the function on exponentially many inputs. In this work we present a scheme that trades off between statistical efficiency (i.e., how much data is needed) and oracle efficiency (i.e., the number of evaluations). We also present lower bounds showing the near-optimality of our scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00322v1</guid>
      <category>cs.CR</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G\"unter F. Steinke, Thomas Steinke</dc:creator>
    </item>
    <item>
      <title>Random Matrices, Intrinsic Freeness, and Sharp Non-Asymptotic Inequalities</title>
      <link>https://arxiv.org/abs/2510.01021</link>
      <description>arXiv:2510.01021v1 Announce Type: cross 
Abstract: Random matrix theory has played a major role in several areas of pure and applied mathematics, as well as statistics, physics, and computer science. This lecture aims to describe the intrinsic freeness phenomenon and how it provides new easy-to-use sharp non-asymptotic bounds on the spectrum of general random matrices. We will also present a couple of illustrative applications in high dimensional statistical inference. This article accompanies a lecture that will be given by the author at the International Congress of Mathematicians in Philadelphia in the Summer of 2026.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01021v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Afonso S. Bandeira</dc:creator>
    </item>
    <item>
      <title>Determinantal Sieving</title>
      <link>https://arxiv.org/abs/2304.02091</link>
      <description>arXiv:2304.02091v3 Announce Type: replace 
Abstract: We introduce determinantal sieving, a new, remarkably powerful tool in the toolbox of algebraic FPT algorithms. Given a polynomial $P(X)$ on a set of variables $X=\{x_1,\ldots,x_n\}$ and a linear matroid $M=(X,\mathcal{I})$ of rank $k$, both over a field $\mathbb{F}$ of characteristic 2, in $2^k$ evaluations we can sieve for those terms in the monomial expansion of $P$ which are multilinear and whose support is a basis for $M$. Alternatively, using $2^k$ evaluations of $P$ we can sieve for those monomials whose odd support spans $M$. Applying this framework, we improve on a range of algebraic FPT algorithms, such as:
  1. Solving $q$-Matroid Intersection in time $O^*(2^{(q-2)k})$ and $q$-Matroid Parity in time $O^*(2^{qk})$, improving on $O^*(4^{qk})$ over general fields (Brand and Pratt, ICALP 2021)
  2. Long $(s,t)$-Path in $O^*(1.66^k)$ time, improving on $O^*(2^k)$, and Rank $k$ $(S,T)$-Linkage in so-called frameworks in $O^*(2^k)$ time, improving on $O^*(2^{|S|+O(k^2 \log(k+|\mathbb{F}|))})$ over general fields (Fomin et al., SODA 2023).
  3. Many instances of the Diverse X paradigm, finding a collection of $r$ solutions to a problem with a minimum mutual distance of $d$ in time $O^*(2^{r(r-1)d/2})$, improving solutions for $k$-Distinct Branchings from time $2^{O(k \log k)}$ to $O^*(2^k)$ (Bang-Jensen et al., ESA 2021), and for Diverse Perfect Matchings from $O^*(2^{2^{O(rd)}})$ to $O^*(2^{r^2d/2})$ (Fomin et al., STACS 2021).
  Here, all matroids are assumed to be represented over fields of characteristic 2. Over general fields, we achieve similar results at the cost of using exponential space by working over the exterior algebra. For a class of arithmetic circuits we call strongly monotone, this is even achieved without any loss of running time. However, the odd support sieving result appears to be specific to working over characteristic 2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.02091v3</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.46298/theoretics.25.21</arxiv:DOI>
      <arxiv:journal_reference>TheoretiCS, Volume 4 (2025), Article 21, 1-75</arxiv:journal_reference>
      <dc:creator>Eduard Eiben, Tomohiro Koana, Magnus Wahlstr\"om</dc:creator>
    </item>
    <item>
      <title>A Lower Bound for the Max Entropy Algorithm for TSP</title>
      <link>https://arxiv.org/abs/2311.01950</link>
      <description>arXiv:2311.01950v2 Announce Type: replace 
Abstract: One of the most famous conjectures in combinatorial optimization is the four-thirds conjecture, which states that the integrality gap of the subtour LP relaxation of the TSP is equal to $\frac43$. For 40 years, the best known upper bound was 1.5, due to Wolsey (1980). Recently, Karlin, Klein, and Oveis Gharan (2022) showed that the max entropy algorithm for the TSP gives an improved bound of $1.5 - 10^{-36}$. In this paper, we show that the approximation ratio of the max entropy algorithm is at least 1.375, even for graphic TSP. Thus the max entropy algorithm does not appear to be the algorithm that will ultimately resolve the four-thirds conjecture in the affirmative, should that be possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01950v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Billy Jin, Nathan Klein, David P. Williamson</dc:creator>
    </item>
    <item>
      <title>Range (R\'enyi) Entropy Queries and Partitioning</title>
      <link>https://arxiv.org/abs/2312.15959</link>
      <description>arXiv:2312.15959v4 Announce Type: replace 
Abstract: Data partitioning that maximizes/minimizes the Shannon entropy, or more generally the R\'enyi entropy is a crucial subroutine in data compression, columnar storage, and cardinality estimation algorithms. These partition algorithms can be accelerated if we have a data structure to compute the entropy in different subsets of data when the algorithm needs to decide what block to construct. Such a data structure will also be useful for data analysts exploring different subsets of data to identify areas of interest. While it is generally known how to compute the Shannon or the R\'enyi entropy of a discrete distribution in the offline or streaming setting efficiently, we focus on the query setting where we aim to efficiently derive the entropy among a subset of data that satisfy some linear predicates. We solve this problem in a typical setting when we deal with real data, where data items are geometric points and each requested area is a query (hyper)rectangle. More specifically, we consider a set $P$ of $n$ weighted and colored points in $\mathbb{R}^d$, where $d$ is a constant. For the range S-entropy (resp. R-entropy) query problem, the goal is to construct a low space data structure, such that given a query (hyper)rectangle $R$, it computes the Shannon (resp. R\'enyi) entropy based on the colors and the weights of the points in $P\cap R$, in sublinear time. We show conditional lower bounds proving that we cannot hope for data structures with near-linear space and near-constant query time for both the range S-entropy and R-entropy query problems. Then, we propose exact data structures for $d=1$ and $d&gt;1$ with $o(n^{2d})$ space and $o(n)$ query time for both problems. Finally, we propose near linear space data structures for returning either an additive or a multiplicative approximation of the Shannon (resp. R\'enyi) entropy in $P\cap R$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15959v4</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aryan Esmailpour, Sanjay Krishnan, Stavros Sintos</dc:creator>
    </item>
    <item>
      <title>Streaming algorithms for products of probabilities</title>
      <link>https://arxiv.org/abs/2504.16507</link>
      <description>arXiv:2504.16507v2 Announce Type: replace 
Abstract: We consider streaming algorithms for approximating a product of input probabilities up to multiplicative error of $1-\epsilon$. It is shown that every randomized streaming algorithm for this problem needs space $\Omega(\log n + \log b - \log \epsilon) - \mathcal{O}(1)$, where $n$ is length of the input stream and $b$ is the bit length of the input numbers. This matches an upper bound from Alur et al.~up to a constant multiplicative factor. Moreover, we consider the threshold problem, where it is asked whether the product of the input probabilities is below a given threshold. It is shown that every randomized streaming algorithm for this problem needs space $\Omega(n \cdot b)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16507v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Lohrey, Leon Rische, Louisa Seelbach Benkner, Julio Xochitemol</dc:creator>
    </item>
    <item>
      <title>Geometric Interpretation of 3-SAT and Phase Transition</title>
      <link>https://arxiv.org/abs/2509.19740</link>
      <description>arXiv:2509.19740v2 Announce Type: replace 
Abstract: Interpretation of 3-SAT as a volume filling problem, and its use to explore the SAT/UNSAT phase transition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19740v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederic Gillet</dc:creator>
    </item>
    <item>
      <title>A Polylogarithmic Competitive Algorithm for Stochastic Online Sorting and TSP</title>
      <link>https://arxiv.org/abs/2509.26073</link>
      <description>arXiv:2509.26073v2 Announce Type: replace 
Abstract: In \emph{Online Sorting}, an array of $n$ initially empty cells is given. At each time step $t$, an element $x_t \in [0,1]$ arrives and must be placed irrevocably into an empty cell without any knowledge of future arrivals. We aim to minimize the sum of absolute differences between pairs of elements placed in consecutive array cells, seeking an online placement strategy that results in a final array close to a sorted one. An interesting multidimensional generalization, a.k.a. the \emph{Online Travelling Salesperson Problem}, arises when the request sequence consists of points in the $d$-dimensional unit cube and the objective is to minimize the sum of euclidean distances between points in consecutive cells. Motivated by the recent work of (Abrahamsen, Bercea, Beretta, Klausen and Kozma; ESA 2024), we consider the \emph{stochastic version} of Online Sorting (\textit{resp.} Online TSP), where each element (\textit{resp.} point) $x_t$ is an i.i.d. sample from the uniform distribution on $[0, 1]$ (\textit{resp.} $[0,1]^d$). By carefully decomposing the request sequence into a hierarchy of balls-into-bins instances, where the balls to bins ratio is large enough so that bin occupancy is sharply concentrated around its mean and small enough so that we can efficiently deal with the elements placed in the same bin, we obtain an online algorithm that approximates the optimal cost within a factor of $O(\log^2 n)$ with high probability. Our result comprises an exponential improvement on the previously best known competitive ratio of $\tilde{O}(n^{1/4})$ for Stochastic Online Sorting due to (Abrahamsen et al.; ESA 2024) and $O(\sqrt{n})$ for (adversarial) Online TSP due to (Bertram, ESA 2025).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26073v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Kalavas, Charalampos Platanos, Thanos Tolias</dc:creator>
    </item>
    <item>
      <title>Hot PATE: Private Aggregation of Distributions for Diverse Task</title>
      <link>https://arxiv.org/abs/2312.02132</link>
      <description>arXiv:2312.02132v4 Announce Type: replace-cross 
Abstract: The Private Aggregation of Teacher Ensembles (PATE) framework enables privacy-preserving machine learning by aggregating responses from disjoint subsets of sensitive data. Adaptations of PATE to tasks with inherent output diversity such as text generation, where the desired output is a sample from a distribution, face a core tension: as diversity increases, samples from different teachers are less likely to agree, but lower agreement results in reduced utility for the same privacy requirements. Yet suppressing diversity to artificially increase agreement is undesirable, as it distorts the output of the underlying model, and thus reduces output quality.
  We propose Hot PATE, a variant of PATE designed for diverse generative settings. We formalize the notion of a diversity-preserving ensemble sampler and introduce an efficient sampler that provably transfers diversity without incurring additional privacy cost. Hot PATE requires only API access to proprietary models and can be used as a drop-in replacement for existing Cold PATE samplers. Our empirical evaluations corroborate and quantify the benefits, showing significant improvements in the privacy utility trade-off on evaluated in-context learning tasks, both in preserving diversity and in returning relevant responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02132v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edith Cohen, Benjamin Cohen-Wang, Xin Lyu, Jelani Nelson, Tamas Sarlos, Uri Stemmer</dc:creator>
    </item>
    <item>
      <title>Synthetic Census Data Generation via Multidimensional Multiset Sum</title>
      <link>https://arxiv.org/abs/2404.10095</link>
      <description>arXiv:2404.10095v2 Announce Type: replace-cross 
Abstract: The US Decennial Census provides valuable data for both research and policy purposes. Census data are subject to a variety of disclosure avoidance techniques prior to release in order to preserve respondent confidentiality. While many are interested in studying the impacts of disclosure avoidance methods on downstream analyses, particularly with the introduction of differential privacy in the 2020 Decennial Census, these efforts are limited by a critical lack of data: The underlying "microdata," which serve as necessary input to disclosure avoidance methods, are kept confidential.
  In this work, we aim to address this limitation by providing tools to generate synthetic microdata solely from published Census statistics, which can then be used as input to any number of disclosure avoidance algorithms for the sake of evaluation and carrying out comparisons. We define a principled distribution over microdata given published Census statistics and design algorithms to sample from this distribution. We formulate synthetic data generation in this context as a knapsack-style combinatorial optimization problem and develop novel algorithms for this setting. While the problem we study is provably hard, we show empirically that our methods work well in practice, and we offer theoretical arguments to explain our performance. Finally, we verify that the data we produce are "close" to the desired ground truth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10095v2</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.29012/jpc.932</arxiv:DOI>
      <dc:creator>Cynthia Dwork, Kristjan Greenewald, Manish Raghavan</dc:creator>
    </item>
  </channel>
</rss>
