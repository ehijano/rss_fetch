<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Feb 2025 02:56:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Semiparametric Inference for Partially Identifiable Data Fusion Estimands via Double Machine Learning</title>
      <link>https://arxiv.org/abs/2502.05319</link>
      <description>arXiv:2502.05319v1 Announce Type: new 
Abstract: Many statistical estimands of interest (e.g., in regression or causality) are functions of the joint distribution of multiple random variables. But in some applications, data is not available that measures all random variables on each subject, and instead the only possible approach is one of data fusion, where multiple independent data sets, each measuring a subset of the random variables of interest, are combined for inference. In general, since all random variables are never observed jointly, their joint distribution, and hence also the estimand which is a function of it, is only partially identifiable. Unfortunately, the endpoints of the partially identifiable region depend in general on entire conditional distributions, rendering them hard both operationally and statistically to estimate. To address this, we present a novel outer-bound on the region of partial identifiability (and establish conditions under which it is tight) that depends only on certain conditional first and second moments. This allows us to derive semiparametrically efficient estimators of our endpoint outer-bounds that only require the standard machine learning toolbox which learns conditional means. We prove asymptotic normality and semiparametric efficiency of our estimators and provide consistent estimators of their variances, enabling asymptotically valid confidence interval construction for our original partially identifiable estimand. We demonstrate the utility of our method in simulations and a data fusion problem from economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05319v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yicong Jiang, Lucas Janson</dc:creator>
    </item>
    <item>
      <title>Pulling back the curtain: the road from statistical estimand to machine-learning based estimator for epidemiologists (no wizard required)</title>
      <link>https://arxiv.org/abs/2502.05363</link>
      <description>arXiv:2502.05363v1 Announce Type: new 
Abstract: Epidemiologists increasingly use causal inference methods that rely on machine learning, as these approaches can relax unnecessary model specification assumptions. While deriving and studying asymptotic properties of such estimators is a task usually associated with statisticians, it is useful for epidemiologists to understand the steps involved, as epidemiologists are often at the forefront of defining important new research questions and translating them into new parameters to be estimated. In this paper, our goal was to provide a relatively accessible guide through the process of (i) deriving an estimator based on the so-called efficient influence function (which we define and explain), and (ii) showing such an estimator's ability to validly incorporate machine learning, by demonstrating the so-called rate double robustness property. The derivations in this paper rely mainly on algebra and some foundational results from statistical inference, which are explained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05363v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Audrey Renson, Lina Montoya, Dana E. Goin, Iv\'an D\'iaz, Rachael K. Ross</dc:creator>
    </item>
    <item>
      <title>Overcoming data challenges to measure whole-person health in electronic health records</title>
      <link>https://arxiv.org/abs/2502.05380</link>
      <description>arXiv:2502.05380v2 Announce Type: new 
Abstract: The allostatic load index (ALI) is a composite measure of whole-person health. Data from electronic health records (EHR) present a huge opportunity to operationalize the ALI in the learning health system, except they are prone to missingness and errors. Validation of EHR data (e.g., through chart reviews) can provide better-quality data, but realistically, only a subset of patients' data can be validated, and most protocols do not recover missing data. Using a representative sample of 1000 patients from the EHR at an extensive learning health system (100 of whom could be validated), we propose methods to design, conduct, and analyze statistically efficient and robust studies of the ALI and healthcare utilization. With semiparametric maximum likelihood estimation, we robustly incorporate all available data into statistical models. Using targeted design strategies, we examine ways to select the most informative patients for validation. Incorporating clinical expertise, we devise a novel validation protocol to promote the quality and completeness of EHR data. Validating the EHR data uncovered relatively low error rates and recovered some missing data. Through simulation studies based on preliminary data, residual sampling was identified as the most informative strategy for completing our validation study. Statistical models of partially validated data indicated higher odds of engaging in the healthcare system were associated with worse whole-person health (i.e., higher ALI), adjusting for age. Targeted validation with an enriched protocol allowed us to ensure the quality and promote the completeness of the EHR. Findings from our validation study were incorporated into analyses as we operationalize the ALI as a whole-person health measure intended to predict healthcare utilization in the academic learning health system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05380v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Sheetal Kedar, Rabeya Tahir, Aidan D. Keleghan, Amelia Miranda, Stephany N. Duda, Michael P. Bancks, Brian J. Wells, Ashish K. Khanna, Joseph Rigdon</dc:creator>
    </item>
    <item>
      <title>False Discovery Rate Control via Frequentist-assisted Horseshoe</title>
      <link>https://arxiv.org/abs/2502.05460</link>
      <description>arXiv:2502.05460v1 Announce Type: new 
Abstract: The horseshoe prior, a widely used handy alternative to the spike-and-slab prior, has proven to be an exceptional default global-local shrinkage prior in Bayesian inference and machine learning. However, designing tests with frequentist false discovery rate (FDR) control using the horseshoe prior or the general class of global-local shrinkage priors remains an open problem. In this paper, we propose a frequentist-assisted horseshoe procedure that not only resolves this long-standing FDR control issue for the high dimensional normal means testing problem but also exhibits satisfactory finite-sample FDR control under any desired nominal level for both large-scale multiple independent and correlated tests. We carry out the frequentist-assisted horseshoe procedure in an easy and intuitive way by using the minimax estimator of the global parameter of the horseshoe prior while maintaining the remaining full Bayes vanilla horseshoe structure. The results of both intensive simulations under different sparsity levels, and real-world data demonstrate that the frequentist-assisted horseshoe procedure consistently achieves robust finite-sample FDR control. Existing frequentist or Bayesian FDR control procedures can lose finite-sample FDR control in a variety of common sparse cases. Based on the intimate relationship between the minimax estimation and the level of FDR control discovered in this work, we point out potential generalizations to achieve FDR control for both more complicated models and the general global-local shrinkage prior family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05460v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiaoyu Liang, Zihan Zhu, Ziang Fu, Michael Evans, Jinman Zhao</dc:creator>
    </item>
    <item>
      <title>Kernel Smoothing for Bounded Copula Densities</title>
      <link>https://arxiv.org/abs/2502.05470</link>
      <description>arXiv:2502.05470v1 Announce Type: new 
Abstract: Nonparametric estimation of copula density functions using kernel estimators presents significant challenges. One issue is the potential unboundedness of certain copula density functions at the corners of the unit square. Another is the boundary bias inherent in kernel density estimation. This paper presents a kernel-based method for estimating bounded copula density functions, addressing boundary bias through the mirror-reflection technique. Optimal smoothing parameters are derived via Asymptotic Mean Integrated Squared Error (AMISE) minimization and cross-validation, with theoretical guarantees of consistency and asymptotic normality. Two kernel smoothing strategies are proposed: the rule-of-thumb approach and least squares cross-validation (LSCV). Simulation studies highlight the efficacy of the rule-of-thumb method in bandwidth selection for copulas with unbounded marginal supports. The methodology is further validated through an application to the Wisconsin Breast Cancer Diagnostic Dataset (WBCDD), where LSCV is used for bandwidth selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05470v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias N. Muia, Olivia Atutey, Mahmud Hasan</dc:creator>
    </item>
    <item>
      <title>On the order statistics from the XLindley distribution and associated inference with an application to fatigue data</title>
      <link>https://arxiv.org/abs/2502.05659</link>
      <description>arXiv:2502.05659v1 Announce Type: new 
Abstract: In this paper, we consider the order statistics from a newly-introduced lifetime distribution called the XLindley distribution. We have derived explicit closed form expressions for the single moments and product moments of order statistics from the XLindley distribution. Utilizing these expressions, we calculated the means, variances, and covariances of order statistics for sample sizes ranging from n = 1 to n = 10 and arbitrarily selected parameter values. Additionally, these moments allow us to identify the best linear unbiased estimators and best linear invariant estimators for the location and scale parameters based on both complete samples and Type-II right censored samples. We also address the linear prediction of unobserved order statistics based on Type-II right-censored samples. We also explore the formulation of confidence intervals for location and scale parameters, along with prediction intervals for unobserved order statistics. To provide comparison and illustration, we conduct a simulation study and analyze a real data example. Finally, we conclude with several remarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05659v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zuber Akhter, S. M. T. K. MirMostafaee, Abu Bakar, Ehsan Ormoz</dc:creator>
    </item>
    <item>
      <title>Active multiple testing with proxy p-values and e-values</title>
      <link>https://arxiv.org/abs/2502.05715</link>
      <description>arXiv:2502.05715v1 Announce Type: new 
Abstract: Researchers often lack the resources to test every hypothesis of interest directly or compute test statistics comprehensively, but often possess auxiliary data from which we can compute an estimate of the experimental outcome. We introduce a novel approach for selecting which hypotheses to query a statistic (i.e., run an experiment, perform expensive computation, etc.) in a hypothesis testing setup by leveraging estimates (e.g., from experts, machine learning models, previous experiments, etc.) to compute proxy statistics. Our framework allows a scientist to propose a proxy statistic, and then query the true statistic with some probability based on the value of the proxy. We make no assumptions about how the proxy is derived and it can be arbitrarily dependent with the true statistic. If the true statistic is not queried, the proxy is used in its place. We characterize "active" methods that produce valid p-values and e-values in this setting and utilize this framework in the multiple testing setting to create procedures with false discovery rate (FDR) control. Through simulations and real data analysis of causal effects in scCRISPR screen experiments, we empirically demonstrate that our proxy framework has both high power and low resource usage when our proxies are accurate estimates of the respective true statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05715v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Xu, Catherine Wang, Larry Wasserman, Kathryn Roeder, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>A Response to Recent Critiques of Hainmueller, Mummolo and Xu (2019) on Estimating Conditional Relationships</title>
      <link>https://arxiv.org/abs/2502.05717</link>
      <description>arXiv:2502.05717v1 Announce Type: new 
Abstract: Simonsohn (2024a) and Simonsohn (2024b) critique Hainmueller, Mummolo and Xu (2019, HMX), arguing that failing to model nonlinear relationships between the treatment and moderator leads to biased marginal effect estimates and uncontrolled Type-I error rates. While these critiques highlight the issue of under-modeling nonlinearity in applied research, they are fundamentally flawed in several key ways. First, the causal estimand for interaction effects and the necessary identifying assumptions are not clearly defined in these critiques. Once properly stated, the critiques no longer hold. Second, the kernel estimator HMX proposes recovers the true causal effects in the scenarios presented in these recent critiques, which compared effects to the wrong benchmark, producing misleading conclusions. Third, while Generalized Additive Models (GAM) can be a useful exploratory tool (as acknowledged in HMX), they are not designed to estimate marginal effects, and better alternatives exist, particularly in the presence of additional covariates. Our response aims to clarify these misconceptions and provide updated recommendations for researchers studying interaction effects through the estimation of conditional marginal effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05717v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jens Hainmueller, Jiehan Liu, Ziyi Liu, Jonathan Mummolo, Yiqing Xu</dc:creator>
    </item>
    <item>
      <title>Approximate Bayesian inference for joint partially linear modeling of longitudinal measurements and spatial time-to-event data</title>
      <link>https://arxiv.org/abs/2502.05880</link>
      <description>arXiv:2502.05880v1 Announce Type: new 
Abstract: The integration of longitudinal measurements and survival time in statistical modeling offers a powerful framework for capturing the interplay between these two essential outcomes, particularly when they exhibit associations. However, in scenarios where spatial dependencies among entities are present due to geographic regions, traditional approaches may fall short. In response, this paper introduces a novel approximate Bayesian hierarchical model tailored for jointly analyzing longitudinal and spatial survival outcomes. The model leverages a conditional autoregressive structure to incorporate spatial effects, while simultaneously employing a joint partially linear model to capture the nonlinear influence of time on longitudinal responses. Through extensive simulation studies, the efficacy of the proposed method is rigorously evaluated. Furthermore, its practical utility is demonstrated through an application to real-world HIV/AIDS data sourced from various Brazilian states, showcasing its adaptability and relevance in epidemiological research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05880v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taban Baghfalaki, Mojtaba Ganjali, Rui Martins</dc:creator>
    </item>
    <item>
      <title>Causal Inference under Interference: Regression Adjustment and Optimality</title>
      <link>https://arxiv.org/abs/2502.06008</link>
      <description>arXiv:2502.06008v1 Announce Type: new 
Abstract: In randomized controlled trials without interference, regression adjustment is widely used to enhance the efficiency of treatment effect estimation. This paper extends this efficiency principle to settings with network interference, where a unit's response may depend on the treatments assigned to its neighbors in a network. We make three key contributions: (1) we establish a central limit theorem for a linear regression-adjusted estimator and prove its optimality in achieving the smallest asymptotic variance within a class of linear adjustments; (2) we develop a novel, consistent estimator for the asymptotic variance of this linear estimator; and (3) we propose a nonparametric estimator that integrates kernel smoothing and trimming techniques, demonstrating its asymptotic normality and its optimality in minimizing asymptotic variance within a broader class of nonlinear adjustments. Extensive simulations validate the superior performance of our estimators, and a real-world data application illustrates their practical utility. Our findings underscore the power of regression-based methods and reveal the potential of kernel-and-trimming-based approaches for further enhancing efficiency under network interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06008v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Fan, Chenlei Leng, Weichi Wu</dc:creator>
    </item>
    <item>
      <title>Estimation with missing not at random binary outcomes via exponential tilts</title>
      <link>https://arxiv.org/abs/2502.06046</link>
      <description>arXiv:2502.06046v1 Announce Type: new 
Abstract: We study the problem of missing not at random (MNAR) datasets with binary outcomes. We propose an exponential tilt based approach that bypasses any knowledge on 'nonresponse instruments' or 'shadow variables' that are usually required for statistical estimation. We establish a sufficient condition for identifiability of tilt parameters and propose an algorithm to estimate them. Based on these tilt parameter estimates, we propose importance weighted and doubly robust estimators for any mean functions of interest, and validate their performances in a synthetic dataset. In an experiment with the Waterbirds dataset, we utilize our tilt framework to perform unsupervised transfer learning, when the responses are missing from a target domain of interest, and achieve a prediction performance that is comparable to a gold standard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06046v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subha Maity</dc:creator>
    </item>
    <item>
      <title>Adversarial Transform Particle Filters</title>
      <link>https://arxiv.org/abs/2502.06165</link>
      <description>arXiv:2502.06165v1 Announce Type: new 
Abstract: The particle filter (PF) and the ensemble Kalman filter (EnKF) are widely used for approximate inference in state-space models. From a Bayesian perspective, these algorithms represent the prior by an ensemble of particles and update it to the posterior with new observations over time. However, the PF often suffers from weight degeneracy in high-dimensional settings, whereas the EnKF relies on linear Gaussian assumptions that can introduce significant approximation errors. In this paper, we propose the Adversarial Transform Particle Filter (ATPF), a novel filtering framework that combines the strengths of the PF and the EnKF through adversarial learning. Specifically, importance sampling is used to ensure statistical consistency as in the PF, while adversarially learned transformations, such as neural networks, allow accurate posterior matching for nonlinear and non-Gaussian systems. In addition, we incorporate kernel methods to ease optimization and leverage regularization techniques based on optimal transport for better statistical properties and numerical stability. We provide theoretical guarantees, including generalization bounds for both the analysis and forecast steps of ATPF. Extensive experiments across various nonlinear and non-Gaussian scenarios demonstrate the effectiveness and practical advantages of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06165v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chengxin Gong, Wei Lin, Cheng Zhang</dc:creator>
    </item>
    <item>
      <title>Falsification of Unconfoundedness by Testing Independence of Causal Mechanisms</title>
      <link>https://arxiv.org/abs/2502.06231</link>
      <description>arXiv:2502.06231v1 Announce Type: new 
Abstract: A major challenge in estimating treatment effects in observational studies is the reliance on untestable conditions such as the assumption of no unmeasured confounding. In this work, we propose an algorithm that can falsify the assumption of no unmeasured confounding in a setting with observational data from multiple heterogeneous sources, which we refer to as environments. Our proposed falsification strategy leverages a key observation that unmeasured confounding can cause observed causal mechanisms to appear dependent. Building on this observation, we develop a novel two-stage procedure that detects these dependencies with high statistical power while controlling false positives. The algorithm does not require access to randomized data and, in contrast to other falsification approaches, functions even under transportability violations when the environment has a direct effect on the outcome of interest. To showcase the practical relevance of our approach, we show that our method is able to efficiently detect confounding on both simulated and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06231v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rickard K. A. Karlsson, Jesse H. Krijthe</dc:creator>
    </item>
    <item>
      <title>Score-Based Causal Discovery with Temporal Background Information</title>
      <link>https://arxiv.org/abs/2502.06232</link>
      <description>arXiv:2502.06232v1 Announce Type: new 
Abstract: Temporal background information can improve causal discovery algorithms by orienting edges and identifying relevant adjustment sets. We develop the Temporal Greedy Equivalence Search (TGES) algorithm and terminology essential for score-based causal discovery with tiered background knowledge. TGES learns a restricted Markov equivalence class of directed acyclic graphs (DAGs) using observational data and tiered background knowledge. To construct TGES we formulate a scoring criterion that accounts for tiered background knowledge. We establish theoretical results for TGES, stating that the algorithm always returns a tiered maximally oriented partially directed acyclic graph (tiered MPDAG) and that this tiered MPDAG contains the true DAG in the large sample limit. We present a simulation study indicating a gain from using tiered background knowledge and an improved precision-recall trade-off compared to the temporal PC algorithm. We provide a real-world example on life-course health data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06232v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tobias Ellegaard Larsen, Claus Thorn Ekstr{\o}m, Anne Helby Petersen</dc:creator>
    </item>
    <item>
      <title>New Insight of Spatial Scan Statistics via Regression Model</title>
      <link>https://arxiv.org/abs/2502.06347</link>
      <description>arXiv:2502.06347v1 Announce Type: new 
Abstract: The spatial scan statistic is widely used to detect disease clusters in epidemiological surveillance. Since the seminal work by~\cite{kulldorff1997}, numerous extensions have emerged, including methods for defining scan regions, detecting multiple clusters, and expanding statistical models. Notably,~\cite{jung2009} and~\cite{ZHANG20092851} introduced a regression-based approach accounting for covariates, encompassing classical methods such as those of~\cite{kulldorff1997}. Another key extension is the expectation-based approach~\citep{neill2005anomalous,neillphdthesis}, which differs from the population-based approach represented by~\cite{kulldorff1997} in terms of hypothesis testing. In this paper, we bridge the regression-based approach with both expectation-based and population-based approaches. We reveal that the two approaches are separated by a simple difference: the presence or absence of an intercept term in the regression model. Exploiting the above simple difference, we propose new spatial scan statistics under the Gaussian and Bernoulli models. We further extend the regression-based approach by incorporating the well-known sparse L0 penalty and show that the derivation of spatial scan statistics can be expressed as an equivalent optimization problem. Our extended framework accommodates extensions such as space-time scan statistics and detecting multiple clusters while naturally connecting with existing spatial regression-based cluster detection. Considering the relation to case-specific models~\citep{she2011,10.1214/11-STS377}, clusters detected by spatial scan statistics can be viewed as outliers in terms of robust statistics. Numerical experiments with real data illustrate the behavior of our proposed statistics under specified settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06347v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takayuki Kawashima, Daisuke Yoneoka, Yuta Tanoue, Akifumi Eguchi, Shuhei Nomura</dc:creator>
    </item>
    <item>
      <title>Revisiting Optimal Proportions for Binary Responses: Insights from Incorporating the Absent Perspective of Type-I Error Rate Control</title>
      <link>https://arxiv.org/abs/2502.06381</link>
      <description>arXiv:2502.06381v1 Announce Type: new 
Abstract: This work revisits optimal response-adaptive designs from a type-I error rate perspective, highlighting when and how much these allocations exacerbate type-I error rate inflation - an issue previously undocumented. We explore a range of approaches from the literature that can be applied to reduce type-I error rate inflation. However, we found that all of these approaches fail to give a robust solution to the problem. To address this, we derive two optimal proportions, incorporating the more robust score test (instead of the Wald test) with finite sample estimators (instead of the unknown true values) in the formulation of the optimization problem. One proportion optimizes statistical power and the other minimizes the total number failures in a trail while maintaining a predefined power level. Through simulations based on an early-phase and a confirmatory trial we provide crucial practical insight into how these new optimal proportion designs can offer substantial patient outcomes advantages while controlling type-I error rate. While we focused on binary outcomes, the framework offers valuable insights that naturally extend to other outcome types, multi-armed trials and alternative measures of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06381v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pin, Sof\'ia S. Villar, William F. Rosenberger</dc:creator>
    </item>
    <item>
      <title>Factor Modelling for Biclustering Large-dimensional Matrix-valued Time Series</title>
      <link>https://arxiv.org/abs/2502.06397</link>
      <description>arXiv:2502.06397v1 Announce Type: new 
Abstract: A novel unsupervised learning method is proposed in this paper for biclustering large-dimensional matrix-valued time series based on an entirely new latent two-way factor structure. Each block cluster is characterized by its own row and column cluster-specific factors in addition to some common matrix factors which impact on all the matrix time series. We first estimate the global loading spaces by projecting the observation matrices onto the row or column loading space corresponding to common factors. The loading spaces for cluster-specific factors are then further recovered by projecting the observation matrices onto the orthogonal complement space of the estimated global loading spaces. To identify the latent row/column clusters simultaneously for matrix-valued time series, we provide a $K$-means algorithm based on the estimated row/column factor loadings of the cluster-specific weak factors. Theoretically, we derive faster convergence rates for global loading matrices than those of the state-of-the-art methods available in the literature under mild conditions. We also propose an one-pass eigenvalue-ratio method to estimate the numbers of global and cluster-specific factors. The consistency with explicit convergence rates is also established for the estimators of the local loading matrices, the factor numbers and the latent cluster memberships. Numerical experiments with both simulated data as well as a real data example are also reported to illustrate the usefulness of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06397v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong He, Xiaoyang Ma, Xingheng Wang, Yalin Wang</dc:creator>
    </item>
    <item>
      <title>Inference on the cointegration and the attractor spaces via functional approximation</title>
      <link>https://arxiv.org/abs/2502.06462</link>
      <description>arXiv:2502.06462v1 Announce Type: new 
Abstract: This paper discusses semiparametric inference on hypotheses on the cointegration and the attractor spaces for I(1) linear processes, using canonical correlation analysis and functional approximation of Brownian Motions. It proposes inference criteria based on the estimation of the number of common trends in various subsets of variables, and compares them to sequences of tests of hypotheses. The exact limit distribution for one of the test statistics is derived in the univariate case. Properties of the inferential tools are discussed theoretically and illustrated via a Monte Carlo study. An empirical analysis of exchange rates is also included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06462v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Massimo Franchi, Paolo Paruolo</dc:creator>
    </item>
    <item>
      <title>An Overview and Recent Developments in the Analysis of Multistate Processes</title>
      <link>https://arxiv.org/abs/2502.06492</link>
      <description>arXiv:2502.06492v1 Announce Type: new 
Abstract: Multistate models offer a powerful framework for studying disease processes and can be used to formulate intensity-based and more descriptive marginal regression models. They also represent a natural foundation for the construction of joint models for disease processes and dynamic marker processes, as well as joint models incorporating random censoring and intermittent observation times. This article reviews the ways multistate models can be formed and fitted to life history data. Recent work on pseudo-values and the incorporation of random effects to model dependence on the process history and between-process heterogeneity are also discussed. The software available to facilitate such analyses is listed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06492v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malka Gorfine, Richard J. Cook, Per Kragh Andersen, Terry M. Therneau, Pierre Joly, Hein Putter, Maja Pohar Perme, Michal Abrahamowicz, On Behalf of Topic Group 8 "Survival Analysis" of the STRATOS Initiative</dc:creator>
    </item>
    <item>
      <title>Quantile Forecast Matching with a Bayesian Quantile Gaussian Process Model</title>
      <link>https://arxiv.org/abs/2502.06605</link>
      <description>arXiv:2502.06605v1 Announce Type: new 
Abstract: A set of probabilities along with corresponding quantiles are often used to define predictive distributions or probabilistic forecasts. These quantile predictions offer easily interpreted uncertainty of an event, and quantiles are generally straightforward to estimate using standard statistical and machine learning methods. However, compared to a distribution defined by a probability density or cumulative distribution function, a set of quantiles has less distributional information. When given estimated quantiles, it may be desirable to estimate a fully defined continuous distribution function. Many researchers do so to make evaluation or ensemble modeling simpler. Most existing methods for fitting a distribution to quantiles lack accurate representation of the inherent uncertainty from quantile estimation or are limited in their applications. In this manuscript, we present a Gaussian process model, the quantile Gaussian process, which is based on established theory of quantile functions and sample quantiles, to construct a probability distribution given estimated quantiles. A Bayesian application of the quantile Gaussian process is evaluated for parameter inference and distribution approximation in simulation studies. The quantile Gaussian process is used to approximate the distributions of quantile forecasts from the 2023-24 US Centers for Disease Control collaborative flu forecasting initiative. The simulation studies and data analysis show that the quantile Gaussian process leads to accurate inference on model parameters, estimation of a continuous distribution, and uncertainty quantification of sample quantiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06605v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Spencer Wadsworth, Jarad Niemi</dc:creator>
    </item>
    <item>
      <title>Comment on "Generic machine learning inference on heterogeneous treatment effects in randomized experiments."</title>
      <link>https://arxiv.org/abs/2502.06758</link>
      <description>arXiv:2502.06758v1 Announce Type: new 
Abstract: We analyze the split-sample robust inference (SSRI) methodology proposed by Chernozhukov, Demirer, Duflo, and Fernandez-Val (CDDF) for quantifying uncertainty in heterogeneous treatment effect estimation. While SSRI effectively accounts for randomness in data splitting, its computational cost can be prohibitive when combined with complex machine learning (ML) models. We present an alternative randomization inference (RI) approach that maintains SSRI's generality without requiring repeated data splitting. By leveraging cross-fitting and design-based inference, RI achieves valid confidence intervals while significantly reducing computational burden. We compare the two methods through simulation, demonstrating that RI retains statistical efficiency while being more practical for large-scale applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06758v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosuke Imai, Michael Lingzhi Li</dc:creator>
    </item>
    <item>
      <title>GST-UNet: Spatiotemporal Causal Inference with Time-Varying Confounders</title>
      <link>https://arxiv.org/abs/2502.05295</link>
      <description>arXiv:2502.05295v1 Announce Type: cross 
Abstract: Estimating causal effects from spatiotemporal data is a key challenge in fields such as public health, social policy, and environmental science, where controlled experiments are often infeasible. However, existing causal inference methods relying on observational data face significant limitations: they depend on strong structural assumptions to address spatiotemporal challenges $\unicode{x2013}$ such as interference, spatial confounding, and temporal carryover effects $\unicode{x2013}$ or fail to account for $\textit{time-varying confounders}$. These confounders, influenced by past treatments and outcomes, can themselves shape future treatments and outcomes, creating feedback loops that complicate traditional adjustment strategies. To address these challenges, we introduce the $\textbf{GST-UNet}$ ($\textbf{G}$-computation $\textbf{S}$patio-$\textbf{T}$emporal $\textbf{UNet}$), a novel end-to-end neural network framework designed to estimate treatment effects in complex spatial and temporal settings. The GST-UNet leverages regression-based iterative G-computation to explicitly adjust for time-varying confounders, providing valid estimates of potential outcomes and treatment effects. To the best of our knowledge, the GST-UNet is the first neural model to account for complex, non-linear dynamics and time-varying confounders in spatiotemporal interventions. We demonstrate the effectiveness of the GST-UNet through extensive simulation studies and showcase its practical utility with a real-world analysis of the impact of wildfire smoke on respiratory hospitalizations during the 2018 California Camp Fire. Our results highlight the potential of GST-UNet to advance spatiotemporal causal inference across a wide range of policy-driven and scientific applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05295v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miruna Oprescu, David K. Park, Xihaier Luo, Shinjae Yoo, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Generalized Venn and Venn-Abers Calibration with Applications in Conformal Prediction</title>
      <link>https://arxiv.org/abs/2502.05676</link>
      <description>arXiv:2502.05676v1 Announce Type: cross 
Abstract: Ensuring model calibration is critical for reliable predictions, yet popular distribution-free methods, such as histogram binning and isotonic regression, provide only asymptotic guarantees. We introduce a unified framework for Venn and Venn-Abers calibration, generalizing Vovk's binary classification approach to arbitrary prediction tasks and loss functions. Venn calibration leverages binning calibrators to construct prediction sets that contain at least one marginally perfectly calibrated point prediction in finite samples, capturing epistemic uncertainty in the calibration process. The width of these sets shrinks asymptotically to zero, converging to a conditionally calibrated point prediction. Furthermore, we propose Venn multicalibration, a novel methodology for finite-sample calibration across subpopulations. For quantile loss, group-conditional and multicalibrated conformal prediction arise as special cases of Venn multicalibration, and Venn calibration produces novel conformal prediction intervals that achieve quantile-conditional coverage. As a separate contribution, we extend distribution-free conditional calibration guarantees of histogram binning and isotonic calibration to general losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05676v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Ahmed Alaa</dc:creator>
    </item>
    <item>
      <title>Lipschitz-Driven Inference: Bias-corrected Confidence Intervals for Spatial Linear Models</title>
      <link>https://arxiv.org/abs/2502.06067</link>
      <description>arXiv:2502.06067v1 Announce Type: cross 
Abstract: Linear models remain ubiquitous in modern spatial applications - including climate science, public health, and economics - due to their interpretability, speed, and reproducibility. While practitioners generally report a form of uncertainty, popular spatial uncertainty quantification methods do not jointly handle model misspecification and distribution shift - despite both being essentially always present in spatial problems. In the present paper, we show that existing methods for constructing confidence (or credible) intervals in spatial linear models fail to provide correct coverage due to unaccounted-for bias. In contrast to classical methods that rely on an i.i.d. assumption that is inappropriate in spatial problems, in the present work we instead make a spatial smoothness (Lipschitz) assumption. We are then able to propose a new confidence-interval construction that accounts for bias in the estimation procedure. We demonstrate that our new method achieves nominal coverage via both theory and experiments. Code to reproduce experiments is available at https://github.com/DavidRBurt/Lipschitz-Driven-Inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06067v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David R. Burt, Renato Berlinghieri, Stephen Bates, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Post-detection inference for sequential changepoint localization</title>
      <link>https://arxiv.org/abs/2502.06096</link>
      <description>arXiv:2502.06096v1 Announce Type: cross 
Abstract: This paper addresses a fundamental but largely unexplored challenge in sequential changepoint analysis: conducting inference following a detected change. We study the problem of localizing the changepoint using only the data observed up to a data-dependent stopping time at which a sequential detection algorithm $\mathcal A$ declares a change. We first construct confidence sets for the unknown changepoint when pre- and post-change distributions are assumed to be known. We then extend our framework to composite pre- and post-change scenarios. We impose no conditions on the observation space or on $\mathcal A$ -- we only need to be able to run $\mathcal A$ on simulated data sequences. In summary, this work offers both theoretically sound and practically effective tools for sequential changepoint localization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06096v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aytijhya Saha, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>A Locally Adaptive Algorithm for Multiple Testing with Network Structure</title>
      <link>https://arxiv.org/abs/2203.11461</link>
      <description>arXiv:2203.11461v5 Announce Type: replace 
Abstract: Incorporating auxiliary information alongside primary data can significantly enhance the accuracy of simultaneous inference. However, existing multiple testing methods face challenges in efficiently incorporating complex side information, especially when it differs in dimension or structure from the primary data, such as network side information. This paper introduces a locally adaptive structure learning algorithm (LASLA), a flexible framework designed to integrate a broad range of auxiliary information into the inference process. Although LASLA is specifically motivated by the challenges posed by network-structured data, it also proves highly effective with other types of side information, such as spatial locations and multiple auxiliary sequences. LASLA employs a $p$-value weighting approach, leveraging structural insights to derive data-driven weights that prioritize the importance of different hypotheses. Our theoretical analysis demonstrates that LASLA asymptotically controls the false discovery rate (FDR) under independent or weakly dependent $p$-values, and achieves enhanced power in scenarios where the auxiliary data provides valuable side information. Simulation studies are conducted to evaluate LASLA's numerical performance, and its efficacy is further illustrated through two real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.11461v5</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ziyi Liang, T. Tony Cai, Wenguang Sun, Yin Xia</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian inference for nondestructive one-shot device testing data under competing risk using Hamiltonian Monte Carlo method</title>
      <link>https://arxiv.org/abs/2307.12557</link>
      <description>arXiv:2307.12557v2 Announce Type: replace 
Abstract: The prevalence of one-shot devices is quite prolific in engineering and medical domains. Unlike typical one-shot devices, nondestructive one-shot devices (NOSD) may survive multiple tests and offer additional data for reliability estimation. This study aims to implement the Bayesian approach of the lifetime prognosis of NOSD when failures are subject to multiple risks. With small deviations from the assumed model conditions, conventional likelihood-based Bayesian estimation may result in misleading statistical inference, raising the need for a robust Bayesian method. This work develops Bayesian estimation by exploiting a robustified posterior based on the density power divergence measure for NOSD test data. Further, the testing of the hypothesis is carried out by applying a proposed Bayes factor derived from the robustified posterior. A flexible Hamiltonian Monte Carlo approach is applied to generate posterior samples. Additionally, we assess the extent of resistance of the proposed methods to small deviations from the assumed model conditions by applying the influence function (IF) approach. In testing of hypothesis, IF reflects how outliers impact the decision-making through Bayes factor under null hypothesis. Finally, this analytical development is validated through a simulation study and a data analysis based on cancer data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12557v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shanya Baghel, Shuvashree Mondal</dc:creator>
    </item>
    <item>
      <title>Testing for patterns and structures in covariance and correlation matrices</title>
      <link>https://arxiv.org/abs/2310.11799</link>
      <description>arXiv:2310.11799v3 Announce Type: replace 
Abstract: Covariance matrices of random vectors contain information that is crucial for modelling. Specific structures and patterns of the covariances (or correlations) may be used to justify parametric models, e.g., autoregressive models. Until now, there have been only a few approaches for testing such covariance structures and most of them can only be used for one particular structure. In the present paper, we propose a systematic and unified testing procedure working among others for the large class of linear covariance structures. Our approach requires only weak distributional assumptions. It covers common structures such as diagonal matrices, Toeplitz matrices and compound symmetry, as well as the more involved autoregressive matrices. We exemplify the approach for all these structures. We prove the correctness of these tests for large sample sizes and use bootstrap techniques for a better small-sample approximation. Moreover, the proposed tests invite adaptations to other covariance patterns by choosing the hypothesis matrix appropriately. With the help of a simulation study, we also assess the small sample properties of the tests. Finally, we illustrate the procedure in an application to a real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11799v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paavo Sattler, Dennis Dobler</dc:creator>
    </item>
    <item>
      <title>Joint clustering with alignment for temporal data in a one-point-per-experiment setting</title>
      <link>https://arxiv.org/abs/2311.10282</link>
      <description>arXiv:2311.10282v2 Announce Type: replace 
Abstract: Temporal data, obtained in the setting where it is only possible to observe one time point per experiment, is widely used in different research fields, yet remains insufficiently addressed from the statistical point of view. Such data often contain observations of a large number of entities, in which case it is of interest to identify a small number of representative behavior types. In this paper, we propose a new method that simultaneously performs clustering and alignment of temporal objects inferred from these data, providing insight into the relationships between entities. Simulations confirm the ability of the proposed approach to leverage multiple properties of the complex data we target such as accessible uncertainties, correlations and a small number of time points. We illustrate it on real data encoding cellular response to a radiation treatment with high energy, supported with the results of an enrichment analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10282v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Polina Arsenteva, Mohamed Amine Benadjaoud, Herv\'e Cardot</dc:creator>
    </item>
    <item>
      <title>Multivariate temporal dependence via mixtures of rotated copulas</title>
      <link>https://arxiv.org/abs/2403.12789</link>
      <description>arXiv:2403.12789v3 Announce Type: replace 
Abstract: Parametric copula families have been known to flexibly capture various dependence patterns, e.g., either positive or negative dependence in either the lower or upper tails of bivariate distributions. In this paper, our objective is to construct a model that is adaptable enough to capture several of these features simultaneously in $m$ dimensions. We propose a mixture of $2^m$ rotations of a parametric copula that can achieve this goal. We illustrate the construction using the Clayton family but the concept is general and can be applied to other families. In order to include dynamic dependence regimes, the approach is extended to a time-dependent sequence of mixture copulas in which the mixture probabilities are allowed to evolve in time via a moving average and seasonal types of relationship. The properties of the proposed model and its performance are examined using simulated and real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12789v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruyi Pan, Luis E. Nieto-Barajas, Radu Craiu</dc:creator>
    </item>
    <item>
      <title>Structured Conformal Inference for Matrix Completion with Applications to Group Recommender Systems</title>
      <link>https://arxiv.org/abs/2404.17561</link>
      <description>arXiv:2404.17561v2 Announce Type: replace 
Abstract: We develop a conformal inference method to construct a joint confidence region for a given group of missing entries within a sparsely observed matrix, focusing primarily on entries from the same column. Our method is model-agnostic and can be combined with any ``black-box'' matrix completion algorithm to provide reliable uncertainty estimation for group-level recommendations. For example, in the context of movie recommendations, it is useful to quantify the uncertainty in the ratings assigned by all members of a group to the same movie, enabling more informed decision-making when individual preferences may conflict. Unlike existing conformal techniques, which estimate uncertainty for one individual at a time, our method provides stronger group-level guarantees by assembling a structured calibration dataset that mimics the dependencies expected in the test group. To achieve this, we introduce a generalized weighted conformalization framework that addresses the lack of exchangeability arising from structured calibration, introducing several innovations to overcome associated computational challenges. We demonstrate the practicality and effectiveness of our approach through extensive numerical experiments and an analysis of the MovieLens 100K dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17561v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyi Liang, Tianmin Xie, Xin Tong, Matteo Sesia</dc:creator>
    </item>
    <item>
      <title>Causal K-Means Clustering</title>
      <link>https://arxiv.org/abs/2405.03083</link>
      <description>arXiv:2405.03083v3 Announce Type: replace 
Abstract: Causal effects are often characterized with population summaries. These might provide an incomplete picture when there are heterogeneous treatment effects across subgroups. Since the subgroup structure is typically unknown, it is more challenging to identify and evaluate subgroup effects than population effects. We propose a new solution to this problem: Causal k-Means Clustering, which harnesses the widely-used k-means clustering algorithm to uncover the unknown subgroup structure. Our problem differs significantly from the conventional clustering setup since the variables to be clustered are unknown counterfactual functions. We present a plug-in estimator which is simple and readily implementable using off-the-shelf algorithms, and study its rate of convergence. We also develop a new bias-corrected estimator based on nonparametric efficiency theory and double machine learning, and show that this estimator achieves fast root-n rates and asymptotic normality in large nonparametric models. Our proposed methods are especially useful for modern outcome-wide studies with multiple treatment levels. Further, our framework is extensible to clustering with generic pseudo-outcomes, such as partially observed outcomes or otherwise unknown functions. Finally, we explore finite sample properties via simulation, and illustrate the proposed methods in a study of treatment programs for adolescent substance abuse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03083v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kwangho Kim, Jisu Kim, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>Joint estimation of insurance loss development factors using Bayesian hidden Markov models</title>
      <link>https://arxiv.org/abs/2406.19903</link>
      <description>arXiv:2406.19903v2 Announce Type: replace 
Abstract: Loss development modelling is the actuarial practice of predicting the total 'ultimate' losses incurred on a set of policies once all claims are reported and settled. This poses a challenging prediction task as losses frequently take years to fully emerge from reported claims, and not all claims might yet be reported. Loss development models frequently estimate a set of 'link ratios' from insurance loss triangles, which are multiplicative factors transforming losses at one time point to ultimate. However, link ratios estimated using classical methods typically underestimate ultimate losses and cannot be extrapolated outside the domains of the triangle, requiring extension by 'tail factors' from another model. Although flexible, this two-step process relies on subjective decision points that might bias inference. Methods that jointly estimate 'body' link ratios and smooth tail factors offer an attractive alternative. This paper proposes a novel application of Bayesian hidden Markov models to loss development modelling, where discrete, latent states representing body and tail processes are automatically learned from the data. The hidden Markov development model is found to perform comparably to, and frequently better than, the two-step approach, as well as a latent change-point model, on numerical examples and industry datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19903v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Conor Goold</dc:creator>
    </item>
    <item>
      <title>Order selection in GARMA models for count time series: a Bayesian perspective</title>
      <link>https://arxiv.org/abs/2409.07263</link>
      <description>arXiv:2409.07263v2 Announce Type: replace 
Abstract: Estimation in GARMA models has traditionally been carried out under the frequentist approach. To date, Bayesian approaches for such estimation have been relatively limited. In the context of GARMA models for count time series, Bayesian estimation achieves satisfactory results in terms of point estimation. Model selection in this context often relies on the use of information criteria. Despite its prominence in the literature, the use of information criteria for model selection in GARMA models for count time series have been shown to present poor performance in simulations, especially in terms of their ability to correctly identify models, even under large sample sizes. In this study, we study the problem of order selection in GARMA models for count time series, adopting a Bayesian perspective through the application of the Reversible Jump Markov Chain Monte Carlo approach. Monte Carlo simulation studies are conducted to assess the finite sample performance of the developed ideas, including point and interval inference, sensitivity analysis, effects of burn-in and thinning, as well as the choice of related priors and hyperparameters. Two real-data applications are presented, one considering automobile production in Brazil and the other considering bus exportation in Brazil before and after the COVID-19 pandemic, showcasing the method's capabilities and further exploring its flexibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07263v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Katerine Zuniga Lastra, Guilherme Pumi, Taiane Schaedler Prass</dc:creator>
    </item>
    <item>
      <title>Integer Programming for Generalized Causal Bootstrap Designs</title>
      <link>https://arxiv.org/abs/2410.21464</link>
      <description>arXiv:2410.21464v2 Announce Type: replace 
Abstract: In experimental causal inference, we distinguish between two sources of uncertainty: design uncertainty, due to the treatment assignment mechanism, and sampling uncertainty, when the sample is drawn from a super-population. This distinction matters in settings with small fixed samples and heterogeneous treatment effects, as in geographical experiments. The standard bootstrap procedure most often used by practitioners primarily estimates sampling uncertainty, and the causal bootstrap procedure, which accounts for design uncertainty, was developed for the completely randomized design and the difference-in-means estimator, whereas non-standard designs and estimators are often used in these low-power regimes. We address this gap by proposing an integer program which computes numerically the worst-case copula used as an input to the causal bootstrap method, in a wide range of settings. Specifically, we prove the asymptotic validity of our approach for unconfounded, conditionally unconfounded, and and individualistic with bounded confoundedness assignments, as well as generalizing to any linear-in-treatment and quadratic-in-treatment estimators. We demonstrate the refined confidence intervals achieved through simulations of small geographical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21464v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jennifer Brennan, S\'ebastien Lahaie, Adel Javanmard, Nick Doudchenko, Jean Pouget-Abadie</dc:creator>
    </item>
    <item>
      <title>A Random-Effects Approach to Generalized Linear Mixed Model Analysis of Incomplete Longitudinal Data</title>
      <link>https://arxiv.org/abs/2411.14548</link>
      <description>arXiv:2411.14548v2 Announce Type: replace 
Abstract: We propose a random-effects approach to missing values for generalized linear mixed model (GLMM) analysis. The method converts a GLMM with missing covariates to another GLMM without missing covariates. The standard GLMM analysis tools for longitudinal data then apply. The method applies, in particular, to the cases of linear mixed models and logistic regression. Performance of the method is evaluated empirically, and compared with alternative approaches, including the popular MICE procedure of multiple imputation. Theoretical justification of the method is given, and explained, for the patterns observed in the simulation studies. Two real-data examples from healthcare studies are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14548v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thuan Nguyen, Jiangshan Zhang, Jiming Jiang</dc:creator>
    </item>
    <item>
      <title>Bhirkuti's Test of Bias Acceptance: Examining in Psychometric Simulations</title>
      <link>https://arxiv.org/abs/2411.18481</link>
      <description>arXiv:2411.18481v2 Announce Type: replace 
Abstract: This study introduces Bhirkuti's Test of Bias Acceptance, a systematic graphical framework for evaluating bias and determining its acceptability under varying experimental conditions. Absolute Relative Bias (ARB), while useful for understanding bias, is sensitive to outliers and population parameter magnitudes, often overstating bias for small values and understating it for larger ones. Similarly, Relative Efficiency (RE) can be influenced by variance differences and outliers, occasionally producing counterintuitive values exceeding 100%, which complicates interpretation. By addressing the limitations of traditional metrics such as Absolute Relative Bias (ARB) and Relative Efficiency (RE), the proposed graphical methodology framework leverages ridgeline plots and standardized estimate to provide a comprehensive visualization of parameter estimate distributions. Ridgeline plots done this way offer a robust alternative by visualizing full distributions, highlighting variability, trends, outliers, descriptives and facilitating more informed decision-making. This study employs multivariate Latent Growth Models (LGM) and Monte Carlo simulations to examine the performance of growth curve modeling under planned missing data designs, focusing on parameter estimate recovery and efficiency. By combining innovative visualization techniques with rigorous simulation methods, Bhirkuti's Test of Bias Acceptance provides two methods of versatile and interpretable toolset for advancing quantitative research in bias evaluation and efficiency assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18481v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aneel Bhusal, Todd D. Little</dc:creator>
    </item>
    <item>
      <title>Interacted two-stage least squares with treatment effect heterogeneity</title>
      <link>https://arxiv.org/abs/2502.00251</link>
      <description>arXiv:2502.00251v2 Announce Type: replace 
Abstract: Treatment effect heterogeneity with respect to covariates is common in instrumental variable (IV) analyses. An intuitive approach, which we term the interacted two-stage least squares (2SLS), is to postulate a linear working model of the outcome on the treatment, covariates, and treatment-covariate interactions, and instrument it by the IV, covariates, and IV-covariate interactions. We clarify the causal interpretation of the interacted 2SLS under the local average treatment effect (LATE) framework when the IV is valid conditional on covariates. Our contributions are threefold. First, we show that the interacted 2SLS with centered covariates is consistent for estimating the LATE if either of the following conditions holds: (i) the IV-covariate interactions are linear in the covariates; (ii) the linear outcome model underlying the interacted 2SLS is correct. Second, we show that the coefficients of the treatment-covariate interactions from the interacted 2SLS are consistent for estimating treatment effect heterogeneity with regard to covariates among compliers if either condition (i) or condition (ii) holds. Moreover, we connect the 2SLS estimator with the weighting perspective in Abadie (2003) and establish the necessity of condition (i) in the absence of additional assumptions on potential outcomes. Third, leveraging the consistency guarantees of the interacted 2SLS for categorical covariates, we propose a stratification strategy based on the IV propensity score to approximate the LATE and treatment effect heterogeneity with regard to the IV propensity score when neither condition (i) nor condition (ii) holds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00251v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anqi Zhao, Peng Ding, Fan Li</dc:creator>
    </item>
    <item>
      <title>A More Precise Elbow Method for Optimum K-means Clustering</title>
      <link>https://arxiv.org/abs/2502.00851</link>
      <description>arXiv:2502.00851v2 Announce Type: replace 
Abstract: K-means clustering is an unsupervised clustering method that requires an initial decision of number of clusters. One method to determine the number of clusters is the elbow method, a heuristic method that relies on visual representation. The method uses the number based on the elbow point, the point closest to 90 degrees that indicates the most optimum number of clusters. This research improves the elbow method such that it becomes an objective method. We use the analytical geometric formula to calculate an angle between lines and real analysis principle of derivative to simplify the elbow point determination. We also consider every possibility of the elbow method graph behaviour such that the algorithm is universally applicable. The result is that the elbow point can be measured precisely with a simple algorithm that does not involve complex functions or calculations. This improved method gives an alternative of more reliable cluster determination method that contributes to more optimum k-means clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00851v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Indra Herdiana, M Alfin Kamal,  Triyani, Mutia Nur Estri,  Renny</dc:creator>
    </item>
    <item>
      <title>Generalized Simple Graphical Rules for Assessing Selection Bias</title>
      <link>https://arxiv.org/abs/2502.00924</link>
      <description>arXiv:2502.00924v2 Announce Type: replace 
Abstract: Selection bias is a major obstacle toward valid causal inference in epidemiology. Over the past decade, several simple graphical rules based on causal diagrams have been proposed as the sufficient identification conditions for addressing selection bias and recovering causal effects. However, these simple graphical rules are usually coupled with specific identification strategies and estimators. In this article, we show two important cases of selection bias that cannot be addressed by these simple rules and their estimators: one case where selection is a descendant of a collider of the treatment and the outcome, and the other case where selection is affected by the mediator. To address selection bias in these two cases, we construct identification formulas by the g-computation and the inverse probability weighting (IPW) methods based on single-world intervention graphs (SWIGs). They are generalized to recover the average treatment effect by adjusting for post-treatment upstream causes of selection. We propose two IPW estimators and their variance estimators to recover the average treatment effect in the presence of selection bias in these two cases. We conduct simulation studies to verify the performance of the estimators when the traditional crude selected-sample analysis returns erroneous contradictory conclusions to the truth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00924v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yichi Zhang, Haidong Lu</dc:creator>
    </item>
    <item>
      <title>A retake on the analysis of scores truncated by terminal events</title>
      <link>https://arxiv.org/abs/2502.03942</link>
      <description>arXiv:2502.03942v3 Announce Type: replace 
Abstract: Analysis of data from randomized controlled trials in vulnerable populations requires special attention when assessing treatment effect by a score measuring, e.g., disease stage or activity together with onset of prevalent terminal events. In reality, it is impossible to disentangle a disease score from the terminal event, since the score is not clinically meaningful after this event. In this work, we propose to assess treatment interventions simultaneously on disease score and the terminal event. Our proposal is based on a natural data-generating mechanism respecting that a disease score does not exist beyond the terminal event. We use modern semi-parametric statistical methods to provide robust and efficient estimation of the risk of terminal event and expected disease score conditional on no terminal event at a pre-specified landmark time. We also use the simultaneous asymptotic behavior of our estimators to develop a powerful closed testing procedure for confirmatory assessment of treatment effect on both onset of terminal event and level of disease score. A simulation study mimicking a large-scale outcome trial in chronic kidney patients as well as an analysis of that trial is provided to assess performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03942v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Klaus K\"ahler Holst, Andreas Nordland, Julie Furberg, Lars Holm Damgaard, Christian Bressen Pipper</dc:creator>
    </item>
    <item>
      <title>A Model-Consistent Data-Driven Computational Strategy for PDE Joint Inversion Problems</title>
      <link>https://arxiv.org/abs/2210.09228</link>
      <description>arXiv:2210.09228v2 Announce Type: replace-cross 
Abstract: The task of simultaneously reconstructing multiple physical coefficients in partial differential equations (PDEs) from observed data is ubiquitous in applications. In this work, we propose an integrated data-driven and model-based iterative reconstruction framework for such joint inversion problems where additional data on the unknown coefficients are supplemented for better reconstructions. Our method couples the supplementary data with the PDE model to make the data-driven modeling process consistent with the model-based reconstruction procedure. We characterize the impact of learning uncertainty on the joint inversion results for two typical inverse problems. Numerical evidence is provided to demonstrate the feasibility of using data-driven models to improve the joint inversion of multiple coefficients in PDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.09228v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kui Ren, Lu Zhang</dc:creator>
    </item>
    <item>
      <title>Assessing Heterogeneity of Treatment Effects</title>
      <link>https://arxiv.org/abs/2306.15048</link>
      <description>arXiv:2306.15048v2 Announce Type: replace-cross 
Abstract: Treatment effect heterogeneity is of major interest in economics, but its assessment is often hindered by the fundamental lack of identification of the individual treatment effects. For example, we may want to assess the effect of a poverty reduction measure at different levels of poverty, but the causal effects on wealth at different wealth levels are not identified. Or, we may be interested in the proportion of workers who benefit from the minimum wage increase, but the proportion is not identified in the absence of counterfactuals. This paper derives bounds useful in such situations, which only depend on the marginal distributions of the outcomes. The bounds are nonparametrically sharp, making clear the maximum extent to which the data can speak about the heterogeneity of the treatment effects. An application to microfinance shows that the bounds can be informative even when the average treatment effects are not significant. Another application to the welfare reform identifies a nonnegligible portion of workers who increased and decreased working hours due to the reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15048v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetsuya Kaji, Jianfei Cao</dc:creator>
    </item>
    <item>
      <title>Random Forest for Dynamic Risk Prediction or Recurrent Events: A Pseudo-Observation Approach</title>
      <link>https://arxiv.org/abs/2312.00770</link>
      <description>arXiv:2312.00770v3 Announce Type: replace-cross 
Abstract: Recurrent events are common in clinical, healthcare, social and behavioral studies. A recent analysis framework for potentially censored recurrent event data is to construct a censored longitudinal data set consisting of times to the first recurrent event in multiple prespecified follow-up windows of length $\tau$. With the staggering number of potential predictors being generated from genetic, -omic, and electronic health records sources, machine learning approaches such as the random forest are growing in popularity, as they can incorporate information from highly correlated predictors with non-standard relationships. In this paper, we bridge this gap by developing a random forest approach for dynamically predicting probabilities of remaining event-free during a subsequent $\tau$-duration follow-up period from a reconstructed censored longitudinal data set. We demonstrate the increased ability of our random forest algorithm for predicting the probability of remaining event-free over a $\tau$-duration follow-up period when compared to the recurrent event modeling framework of Xia et al. (2020) in settings where association between predictors and recurrent event outcomes is complex in nature. The proposed random forest algorithm is demonstrated using recurrent exacerbation data from the Azithromycin for the Prevention of Exacerbations of Chronic Obstructive Pulmonary Disease (Albert et al., 2011).</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00770v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abigail Loe, Susan Murray, Zhenke Wu</dc:creator>
    </item>
    <item>
      <title>Randomization-based confidence sets for the local average treatment effect</title>
      <link>https://arxiv.org/abs/2404.18786</link>
      <description>arXiv:2404.18786v3 Announce Type: replace-cross 
Abstract: We consider the problem of generating confidence sets in randomized experiments with noncompliance. We show that a refinement of a randomization-based procedure proposed by Imbens and Rosenbaum (2005) has desirable properties. Namely, we show that using a studentized Anderson--Rubin-type statistic as a test statistic yields confidence sets that are finite-sample exact under treatment effect homogeneity, and remain asymptotically valid for the Local Average Treatment Effect when the treatment effect is heterogeneous. We provide a uniform analysis of this procedure and efficient algorithms to construct the confidence set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18786v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. M. Aronow, Haoge Chang, Patrick Lopatto</dc:creator>
    </item>
    <item>
      <title>Kernel Three Pass Regression Filter</title>
      <link>https://arxiv.org/abs/2405.07292</link>
      <description>arXiv:2405.07292v3 Announce Type: replace-cross 
Abstract: We forecast a single time series using a high-dimensional set of predictors. When these predictors share common underlying dynamics, an approximate latent factor model provides a powerful characterization of their co-movements Bai(2003). These latent factors succinctly summarize the data and can also be used for prediction, alleviating the curse of dimensionality in high-dimensional prediction exercises, see Stock &amp; Watson (2002a). However, forecasting using these latent factors suffers from two potential drawbacks. First, not all pervasive factors among the set of predictors may be relevant, and using all of them can lead to inefficient forecasts. The second shortcoming is the assumption of linear dependence of predictors on the underlying factors. The first issue can be addressed by using some form of supervision, which leads to the omission of irrelevant information. One example is the three-pass regression filter proposed by Kelly &amp; Pruitt (2015). We extend their framework to cases where the form of dependence might be nonlinear by developing a new estimator, which we refer to as the Kernel Three-Pass Regression Filter (K3PRF). This alleviates the aforementioned second shortcoming. The estimator is computationally efficient and performs well empirically. The short-term performance matches or exceeds that of established models, while the long-term performance shows significant improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07292v3</guid>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rajveer Jat, Daanish Padha</dc:creator>
    </item>
    <item>
      <title>On the Parameter Identifiability of Partially Observed Linear Causal Models</title>
      <link>https://arxiv.org/abs/2407.16975</link>
      <description>arXiv:2407.16975v2 Announce Type: replace-cross 
Abstract: Linear causal models are important tools for modeling causal dependencies and yet in practice, only a subset of the variables can be observed. In this paper, we examine the parameter identifiability of these models by investigating whether the edge coefficients can be recovered given the causal structure and partially observed data. Our setting is more general than that of prior research - we allow all variables, including both observed and latent ones, to be flexibly related, and we consider the coefficients of all edges, whereas most existing works focus only on the edges between observed variables. Theoretically, we identify three types of indeterminacy for the parameters in partially observed linear causal models. We then provide graphical conditions that are sufficient for all parameters to be identifiable and show that some of them are provably necessary. Methodologically, we propose a novel likelihood-based parameter estimation method that addresses the variance indeterminacy of latent variables in a specific way and can asymptotically recover the underlying parameters up to trivial indeterminacy. Empirical studies on both synthetic and real-world datasets validate our identifiability theory and the effectiveness of the proposed method in the finite-sample regime. Code: https://github.com/dongxinshuai/scm-identify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16975v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinshuai Dong, Ignavier Ng, Biwei Huang, Yuewen Sun, Songyao Jin, Roberto Legaspi, Peter Spirtes, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>Confidence Diagram of Nonparametric Ranking for Uncertainty Assessment in Large Language Models Evaluation</title>
      <link>https://arxiv.org/abs/2412.05506</link>
      <description>arXiv:2412.05506v2 Announce Type: replace-cross 
Abstract: We consider the inference for the ranking of large language models (LLMs). Alignment arises as a significant challenge to mitigate hallucinations in the use of LLMs. Ranking LLMs has proven to be an effective tool to improve alignment based on the best-of-$N$ policy. In this paper, we propose a new inferential framework for hypothesis testing among the ranking for language models. Our framework is based on a nonparametric contextual ranking framework designed to assess large language models' domain-specific expertise, leveraging nonparametric scoring methods to account for their sensitivity to the prompts. To characterize the combinatorial complexity of the ranking, we introduce a novel concept of confidence diagram, which leverages a Hasse diagram to represent the entire confidence set of rankings by a single directed graph. We show the validity of the proposed confidence diagram by advancing the Gaussian multiplier bootstrap theory to accommodate the supremum of independent empirical processes that are not necessarily identically distributed. Extensive numerical experiments conducted on both synthetic and real data demonstrate that our approach offers valuable insight into the evaluation for the performance of different LLMs across various medical domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05506v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zebin Wang, Yi Han, Ethan X. Fang, Lan Wang, Junwei Lu</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction Sets with Improved Conditional Coverage using Trust Scores</title>
      <link>https://arxiv.org/abs/2501.10139</link>
      <description>arXiv:2501.10139v2 Announce Type: replace-cross 
Abstract: Standard conformal prediction offers a marginal guarantee on coverage, but for prediction sets to be truly useful, they should ideally ensure coverage conditional on each test point. Unfortunately, it is impossible to achieve exact, distribution-free conditional coverage in finite samples. In this work, we propose an alternative conformal prediction algorithm that targets coverage where it matters most--in instances where a classifier is overconfident in its incorrect predictions. We start by dissecting miscoverage events in marginally-valid conformal prediction, and show that miscoverage rates vary based on the classifier's confidence and its deviation from the Bayes optimal classifier. Motivated by this insight, we develop a variant of conformal prediction that targets coverage conditional on a reduced set of two variables: the classifier's confidence in a prediction and a nonparametric trust score that measures its deviation from the Bayes classifier. Empirical evaluation on multiple image datasets shows that our method generally improves conditional coverage properties compared to standard conformal prediction, including class-conditional coverage, coverage over arbitrary subgroups, and coverage over demographic groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10139v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jivat Neet Kaur, Michael I. Jordan, Ahmed Alaa</dc:creator>
    </item>
    <item>
      <title>An efficient Monte Carlo method for valid prior-free possibilistic statistical inference</title>
      <link>https://arxiv.org/abs/2501.10585</link>
      <description>arXiv:2501.10585v2 Announce Type: replace-cross 
Abstract: Inferential models (IMs) offer prior-free, Bayesian-like, posterior degrees of belief designed for statistical inference, which feature a frequentist-like calibration property that ensures reliability of said inferences. The catch is that IMs' degrees of belief are possibilistic rather than probabilistic and, since the familiar Monte Carlo methods approximate probabilistic quantities, there are computational challenges associated with putting the IM framework into practice. The present paper addresses this shortcoming by developing a new Monte Carlo-based tool designed specifically to approximate the IM's possibilistic output. The proposal is based on a characterization of the possibilistic IM's credal set, which identifies the "best probabilistic approximation" of the IM as a mixture distribution that can be readily approximated and sampled from; these samples can then be transformed into a possibilistic approximation of the IM. Numerical results are presented highlighting the proposed approximation's accuracy and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10585v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Decentralized Inference for Spatial Data Using Low-Rank Models</title>
      <link>https://arxiv.org/abs/2502.00309</link>
      <description>arXiv:2502.00309v2 Announce Type: replace-cross 
Abstract: Advancements in information technology have enabled the creation of massive spatial datasets, driving the need for scalable and efficient computational methodologies. While offering viable solutions, centralized frameworks are limited by vulnerabilities such as single-point failures and communication bottlenecks. This paper presents a decentralized framework tailored for parameter inference in spatial low-rank models to address these challenges. A key obstacle arises from the spatial dependence among observations, which prevents the log-likelihood from being expressed as a summation-a critical requirement for decentralized optimization approaches. To overcome this challenge, we propose a novel objective function leveraging the evidence lower bound, which facilitates the use of decentralized optimization techniques. Our approach employs a block descent method integrated with multi-consensus and dynamic consensus averaging for effective parameter optimization. We prove the convexity of the new objective function in the vicinity of the true parameters, ensuring the convergence of the proposed method. Additionally, we present the first theoretical results establishing the consistency and asymptotic normality of the estimator within the context of spatial low-rank models. Extensive simulations and real-world data experiments corroborate these theoretical findings, showcasing the robustness and scalability of the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00309v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianwei Shi, Sameh Abdulah, Ying Sun, Marc G. Genton</dc:creator>
    </item>
  </channel>
</rss>
