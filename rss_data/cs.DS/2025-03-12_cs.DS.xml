<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Mar 2025 01:58:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Buffered Partially-Persistent External-Memory Search Trees</title>
      <link>https://arxiv.org/abs/2503.08211</link>
      <description>arXiv:2503.08211v1 Announce Type: new 
Abstract: We present an optimal partially-persistent external-memory search tree with amortized I/O bounds matching those achieved by the non-persistent $B^{\varepsilon}$-tree by Brodal and Fagerberg [SODA 2003]. In a partially-persistent data structure each update creates a new version of the data structure, where all past versions can be queried, but only the current version can be updated. All operations should be efficient with respect to the size $N_v$ of the accessed version $v$. For any parameter $0&lt;\varepsilon&lt;1$, our data structure supports insertions and deletions in amortized $O\!\left(\frac{1}{\varepsilon B^{1-\varepsilon}}\log_B N_v\right)$ I/Os, where $B$ is the external-memory block size. It also supports successor and range reporting queries in amortized $O\!\left(\frac{1}{\varepsilon}\log_B N_v+K/B\right)$ I/Os, where $K$ is the number of values reported. The space usage of the data structure is linear in the total number of updates. We make the standard and minimal assumption that the internal memory has size $M \geq 2B$. The previous state-of-the-art external-memory partially-persistent search tree by Arge, Danner and Teh [JEA 2003] supports all operations in worst-case $O\!\left(\log_B N_v+K/B\right)$ I/Os, matching the bounds achieved by the classical B-tree by Bayer and McCreight [Acta Informatica 1972]. Our data structure successfully combines buffering updates with partial persistence. The I/O bounds can also be achieved in the worst-case sense, by slightly modifying our data structure and under the requirement that the memory size $M = \Omega\!\left(B^{1-\varepsilon}\log_2(\max_v N_v)\right)$. The worst-case result slightly improves the memory requirement over the previous ephemeral external-memory dictionary by Das, Iacono, and Nekrich (ISAAC 2022), who achieved matching worst-case I/O bounds but required $M=\Omega\!\left(B\log_B N\right)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08211v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gerth St{\o}lting Brodal, Casper Moldrup Rysgaard, Rolf Svenning</dc:creator>
    </item>
    <item>
      <title>Cost-driven prunings for iterative solving of constrained routing problem with SRLG-disjoint protection</title>
      <link>https://arxiv.org/abs/2503.08262</link>
      <description>arXiv:2503.08262v1 Announce Type: new 
Abstract: The search for the optimal pair of active and protection paths in a network with Shared Risk Link Groups (SRLG) is a challenging but high-value problem in the industry that is inevitable in ensuring reliable connections on the modern Internet. We propose a new approach to solving this problem, with a novel use of statistical analysis of the distribution of paths with respect to their cost, which is an integral part of our innovation. The key idea in our algorithm is to employ iterative updates of cost bounds, allowing efficient pruning of suboptimal paths. This idea drives an efficacious exploration of the search space. We benchmark our algorithms against the state-of-the-art algorithms that exploit the alternative strategy of conflicting links exclusion, showing that our approach has the advantage of finding more feasible connections within a set time limit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08262v1</guid>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P. A. Mosharev, Choon-Meng Lee, Xu Shu, Xiaoshan Zhang, Man-Hong Yung</dc:creator>
    </item>
    <item>
      <title>The Computational Complexity of Positive Non-Clashing Teaching in Graphs</title>
      <link>https://arxiv.org/abs/2503.07665</link>
      <description>arXiv:2503.07665v1 Announce Type: cross 
Abstract: We study the classical and parameterized complexity of computing the positive non-clashing teaching dimension of a set of concepts, that is, the smallest number of examples per concept required to successfully teach an intelligent learner under the considered, previously established model. For any class of concepts, it is known that this problem can be effortlessly transferred to the setting of balls in a graph G. We establish (1) the NP-hardness of the problem even when restricted to instances with positive non-clashing teaching dimension k=2 and where all balls in the graph are present, (2) near-tight running time upper and lower bounds for the problem on general graphs, (3) fixed-parameter tractability when parameterized by the vertex integrity of G, and (4) a lower bound excluding fixed-parameter tractability when parameterized by the feedback vertex number and pathwidth of G, even when combined with k. Our results provide a nearly complete understanding of the complexity landscape of computing the positive non-clashing teaching dimension and answer open questions from the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07665v1</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Ganian, Liana Khazaliya, Fionn Mc Inerney, Mathis Rocton</dc:creator>
    </item>
    <item>
      <title>Counting with the quantum alternating operator ansatz</title>
      <link>https://arxiv.org/abs/2503.07720</link>
      <description>arXiv:2503.07720v1 Announce Type: cross 
Abstract: We introduce a variational algorithm based on the quantum alternating operator ansatz (QAOA) for the approximate solution of computationally hard counting problems. Our algorithm, dubbed VQCount, is based on the equivalence between random sampling and approximate counting and employs QAOA as a solution sampler. We first prove that VQCount improves upon previous work by reducing exponentially the number of samples needed to obtain an approximation within a multiplicative factor of the exact count. Using tensor network simulations, we then study the typical performance of VQCount with shallow circuits on synthetic instances of two #P-hard problems, positive #NAE3SAT and positive #1-in-3SAT. We employ the original quantum approximate optimization algorithm version of QAOA, as well as the Grover-mixer variant which guarantees a uniform solution probability distribution. We observe a tradeoff between QAOA success probability and sampling uniformity, which we exploit to achieve an exponential gain in efficiency over naive rejection sampling. Our results highlight the potential and limitations of variational algorithms for approximate counting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07720v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Drapeau, Shreya Banerjee, Stefanos Kourtis</dc:creator>
    </item>
    <item>
      <title>Algorithms for Distance Problems in Continuous Graphs</title>
      <link>https://arxiv.org/abs/2503.07769</link>
      <description>arXiv:2503.07769v1 Announce Type: cross 
Abstract: We study the problem of computing the diameter and the mean distance of a continuous graph, i.e., a connected graph where all points along the edges, instead of only the vertices, must be taken into account. It is known that for continuous graphs with $m$ edges these values can be computed in roughly $O(m^2)$ time. In this paper, we use geometric techniques to obtain subquadratic time algorithms to compute the diameter and the mean distance of a continuous graph for two well-established classes of sparse graphs. We show that the diameter and the mean distance of a continuous graph of treewidth at most $k$ can be computed in $O(n\log^{O(k)} n)$ time, where $n$ is the number of vertices in the graph. We also show that computing the diameter and mean distance of a continuous planar graph with $n$ vertices and $F$ faces takes $O(n F \log n)$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07769v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergio Cabello, Delia Garijo, Antonia Kalb, Fabian Klute, Irene Parada, Rodrigo I. Silveira</dc:creator>
    </item>
    <item>
      <title>Sublinear Algorithms for Wasserstein and Total Variation Distances: Applications to Fairness and Privacy Auditing</title>
      <link>https://arxiv.org/abs/2503.07775</link>
      <description>arXiv:2503.07775v1 Announce Type: cross 
Abstract: Resource-efficiently computing representations of probability distributions and the distances between them while only having access to the samples is a fundamental and useful problem across mathematical sciences. In this paper, we propose a generic algorithmic framework to estimate the PDF and CDF of any sub-Gaussian distribution while the samples from them arrive in a stream. We compute mergeable summaries of distributions from the stream of samples that require sublinear space w.r.t. the number of observed samples. This allows us to estimate Wasserstein and Total Variation (TV) distances between any two sub-Gaussian distributions while samples arrive in streams and from multiple sources (e.g. federated learning). Our algorithms significantly improves on the existing methods for distance estimation incurring super-linear time and linear space complexities. In addition, we use the proposed estimators of Wasserstein and TV distances to audit the fairness and privacy of the ML algorithms. We empirically demonstrate the efficiency of the algorithms for estimating these distances and auditing using both synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07775v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.DS</category>
      <category>stat.CO</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debabrota Basu, Debarshi Chanda</dc:creator>
    </item>
    <item>
      <title>Dynamic DBSCAN with Euler Tour Sequences</title>
      <link>https://arxiv.org/abs/2503.08246</link>
      <description>arXiv:2503.08246v1 Announce Type: cross 
Abstract: We propose a fast and dynamic algorithm for Density-Based Spatial Clustering of Applications with Noise (DBSCAN) that efficiently supports online updates. Traditional DBSCAN algorithms, designed for batch processing, become computationally expensive when applied to dynamic datasets, particularly in large-scale applications where data continuously evolves. To address this challenge, our algorithm leverages the Euler Tour Trees data structure, enabling dynamic clustering updates without the need to reprocess the entire dataset. This approach preserves a near-optimal accuracy in density estimation, as achieved by the state-of-the-art static DBSCAN method (Esfandiari et al., 2021) Our method achieves an improved time complexity of $O(d \log^3(n) + \log^4(n))$ for every data point insertion and deletion, where $n$ and $d$ denote the total number of updates and the data dimension, respectively. Empirical studies also demonstrate significant speedups over conventional DBSCANs in real-time clustering of dynamic datasets, while maintaining comparable or superior clustering quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08246v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seiyun Shin, Ilan Shomorony, Peter Macgregor</dc:creator>
    </item>
    <item>
      <title>Hierarchical Locality Sensitive Hashing for Structured Data: A Survey</title>
      <link>https://arxiv.org/abs/2204.11209</link>
      <description>arXiv:2204.11209v4 Announce Type: replace 
Abstract: Data similarity (or distance) computation is a fundamental research topic which fosters a variety of similarity-based machine learning and data mining applications. In big data analytics, it is impractical to compute the exact similarity of data instances due to high computational cost. To this end, the Locality Sensitive Hashing (LSH) technique has been proposed to provide accurate estimators for various similarity measures between sets or vectors in an efficient manner without the learning process. Structured data (e.g., sequences, trees and graphs), which are composed of elements and relations between the elements, are commonly seen in the real world, but the traditional LSH algorithms cannot preserve the structure information represented as relations between elements. In order to conquer the issue, researchers have been devoted to the family of the hierarchical LSH algorithms. In this paper, we explore the present progress of the research into hierarchical LSH from the following perspectives: 1) Data structures, where we review various hierarchical LSH algorithms for three typical data structures and uncover their inherent connections; 2) Applications, where we review the hierarchical LSH algorithms in multiple application scenarios; 3) Challenges, where we discuss some potential challenges as future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.11209v4</guid>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Wu, Bin Li</dc:creator>
    </item>
    <item>
      <title>Complete Decomposition of Symmetric Tensors in Linear Time and Polylogarithmic Precision</title>
      <link>https://arxiv.org/abs/2211.07407</link>
      <description>arXiv:2211.07407v2 Announce Type: replace 
Abstract: We study symmetric tensor decompositions, i.e. decompositions of the input symmetric tensor T of order 3 as sum of r 3rd-order tensor powers of u_i where u_i are vectors in \C^n. In order to obtain efficient decomposition algorithms, it is necessary to require additional properties from the u_i. In this paper we assume that the u_i are linearly independent. This implies that r is at most n, i.e., the decomposition of T is undercomplete. We will moreover assume that r=n (we plan to extend this work to the case where r is strictly less than n in a forthcoming paper). We give a randomized algorithm for the following problem: given T, an accuracy parameter epsilon, and an upper bound B on the condition number of the tensor, output vectors u'_i such that u_i and u'_i differ by at most epsilon (in the l_2 norm and up to permutation and multiplication by phases) with high probability. The main novel features of our algorithm are: (1) We provide the first algorithm for this problem that works in the computation model of finite arithmetic and requires only poly-logarithmic (in n, B and 1/epsilon) many bits of precision. (2) Moreover, this is also the first algorithm that runs in linear time in the size of the input tensor. It requires O(n^3) arithmetic operations for all accuracy parameters epsilon = 1/poly(n).</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.07407v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.tcs.2025.115159</arxiv:DOI>
      <dc:creator>Pascal Koiran, Subhayan Saha</dc:creator>
    </item>
    <item>
      <title>Undercomplete Decomposition of Symmetric Tensors in Linear Time, and Smoothed Analysis of the Condition Number</title>
      <link>https://arxiv.org/abs/2403.00643</link>
      <description>arXiv:2403.00643v2 Announce Type: replace 
Abstract: We study symmetric tensor decompositions, i.e., decompositions of the form $T = \sum_{i=1}^r u_i^{\otimes 3}$ where $T$ is a symmetric tensor of order 3 and $u_i \in \mathbb{C}^n$.In order to obtain efficient decomposition algorithms, it is necessary to require additional properties from $u_i$. In this paper we assume that the $u_i$ are linearly independent.This implies $r \leq n$,that is, the decomposition of T is undercomplete.
  We give a randomized algorithm for the following problem in the exact arithmetic model of computation: Let $T$ be an order-3 symmetric tensor that has an undercomplete decomposition. Then given some $T'$ close to $T$, an accuracy parameter $\varepsilon$, and an upper bound B on the condition number of the tensor, output vectors $u'_i$ such that $||u_i - u'_i|| \leq \varepsilon$ (up to permutation and multiplication by cube roots of unity) with high probability. The main novel features of our algorithm are:
  1) We provide the first algorithm for this problem that runs in linear time in the size of the input tensor. More specifically, it requires $O(n^3)$ arithmetic operations for all accuracy parameters $\varepsilon =$ 1/poly(n) and B = poly(n).
  2) Our algorithm is robust, that is, it can handle inverse-quasi-polynomial noise (in $n$,B,$\frac{1}{\varepsilon}$) in the input tensor.
  3) We present a smoothed analysis of the condition number of the tensor decomposition problem. This guarantees that the condition number is low with high probability and further shows that our algorithm runs in linear time, except for some rare badly conditioned inputs.
  Our main algorithm is a reduction to the complete case ($r=n$) treated in our previous work [Koiran,Saha,CIAC 2023]. For efficiency reasons we cannot use this algorithm as a blackbox. Instead, we show that it can be run on an implicitly represented tensor obtained from the input tensor by a change of basis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00643v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pascal Koiran, Subhayan Saha</dc:creator>
    </item>
    <item>
      <title>New simple and fast quicksort algorithm for equal keys</title>
      <link>https://arxiv.org/abs/2502.06461</link>
      <description>arXiv:2502.06461v3 Announce Type: replace 
Abstract: This paper introduces a novel and efficient partitioning technique for quicksort, specifically designed for real-world data with duplicate elements (50-year-old problem). The method is referred to as "equal quicksort" or "eqsort". Based on the experimental findings, it has been determined that the newly developed algorithm, eqsort, is competitive with the best current implementations,such as fat partitioning algorithms and dual-pivot quicksort. This method offers several advantages over the commonly used dual-pivot method and pdqsort partitioning, making it a potential replacement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06461v3</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Parviz Afereidoon</dc:creator>
    </item>
    <item>
      <title>\~Optimal Algorithm for Fully Dynamic LZ77</title>
      <link>https://arxiv.org/abs/2502.12000</link>
      <description>arXiv:2502.12000v2 Announce Type: replace 
Abstract: The Lempel-Ziv 77 (LZ77) factorization is a fundamental compression scheme widely used in text processing and data compression. In this work, we study the time complexity of maintaining the LZ77 factorization of a dynamic string. We present an algorithm that dynamically maintains the LZ77 factorization of a string $S$ that undergoes edit operations (character substitutions, insertions, and deletions). The data structure can be built in $\tilde{O}(n)$ time on an initial string $S$ of length $n$, and updates are supported in $\tilde{O}(n^{2/3})$ time, where $n$ is the current length of $S$. We also show that there is no algorithm with polynomially faster update time unless the Strong Exponential Time Hypothesis fails. Our lower bound holds even for the restricted settings in which only substitution operations are allowed, and only the length of the LZ77 factorization is maintained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12000v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Itai Boneh, Shay Golan, Matan Kraus</dc:creator>
    </item>
    <item>
      <title>Induced Minor Models. I. Structural Properties and Algorithmic Consequences</title>
      <link>https://arxiv.org/abs/2402.08332</link>
      <description>arXiv:2402.08332v2 Announce Type: replace-cross 
Abstract: A graph $H$ is said to be an induced minor of a graph $G$ if $H$ can be obtained from $G$ by a sequence of vertex deletions and edge contractions. Equivalently, $H$ is an induced minor of $G$ if there exists an induced minor model of $H$ in $G$, that is, a collection of pairwise disjoint subsets of vertices of $G$ labeled by the vertices of $H$, each inducing a connected subgraph in $G$, such that two vertices of $H$ are adjacent if and only if there is an edge in $G$ between the corresponding subsets.
  In this paper, we investigate structural properties of induced minor models, including bounds on treewidth and chromatic number of the subgraphs induced by minimal induced minor models. It is known that for some graphs $H$, testing a given graph $G$ contains $H$ as an induced minor is an NP-complete problem. Nevertheless, as algorithmic applications of our structural results, we make use of recent developments regarding tree-independence number to show that if $H$ is the $4$-wheel, the $5$-vertex complete graph minus an edge, or a complete bipartite graph $K_{2,q}$, then there is a polynomial-time algorithm to find in a given graph $G$ an induced minor model of $H$ in $G$, if there is one. We also develop an alternative polynomial-time algorithm for recognizing graphs that do not contain $K_{2,3}$ as an induced minor, which revolves around the idea of detecting the induced subgraphs whose presence is forced when the input graph contains $K_{2,3}$ as an induced minor, using the so-called shortest path detector. It turns out that all these induced subgraphs are Truemper configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08332v2</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Bousquet, Cl\'ement Dallard, Ma\"el Dumas, Claire Hilaire, Martin Milani\v{c}, Anthony Perez, Nicolas Trotignon</dc:creator>
    </item>
  </channel>
</rss>
