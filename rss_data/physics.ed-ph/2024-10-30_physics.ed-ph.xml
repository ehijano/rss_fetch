<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.ed-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.ed-ph</link>
    <description>physics.ed-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.ed-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2024 04:01:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Quantum cryptography visualized: assessing visual attention on multiple representations with eye tracking in an AR-enhanced quantum cryptography student experiment</title>
      <link>https://arxiv.org/abs/2410.21975</link>
      <description>arXiv:2410.21975v1 Announce Type: new 
Abstract: With the advent and development of real-world quantum technology applications, a practically-focused quantum education including student quantum experiments are gaining increasing importance in physics curricula. In this paper, using the DeFT framework, we present an analysis of the representations in our AR-enhanced quantum cryptography student experiment, in order to assess their potential for promoting conceptual learning. We also discuss learner visual attention with respect to the provided multiple representations based on the eye gaze data we have obtained from a pilot study where N=38 students carried out the tasks in our AR-enhanced quantum cryptography student experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21975v1</guid>
      <category>physics.ed-ph</category>
      <category>quant-ph</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Dzsotjan, Atakan Coban, Christoph Hoyer, Stefan K\"uchemann, J\"urgen Durst, Anna Donhauser, Jochen Kuhn</dc:creator>
    </item>
    <item>
      <title>Potential Pitfalls in Visual Models of Tipping Points -- And How to Fix Them</title>
      <link>https://arxiv.org/abs/2410.22171</link>
      <description>arXiv:2410.22171v1 Announce Type: new 
Abstract: Visual models play a crucial role in both science and science communication. However, the distinction between mere analogies and mathematically sound graphical representations is not easy and can be misunderstood not only by laypeople but also within academic literature itself. Moreover, even when the graphical representation exactly corresponds to the mathematical model, its interpretation is often far from obvious. In this paper we discuss the "potential landscape" visualization commonly used for tipping points in the context of nonlinear dynamics and reveal potential pitfalls, in particular when distinguishing bifurcation induced tipping (B-tipping) from noise-induced tipping (N-tipping). We propose new visualization techniques for tipping dynamics, carefully distinguishing between B- and N-tipping as well as between single systems and ensembles of systems. Explicitly, we apply these visualizations both to molecular cell biology and to climate science in order to reveal the crucial differences in the interpretation of the visual models. We find that it is crucial to explicitly discuss the assumptions made within the visual model and to be aware of the risk of misinterpretation. Based on these findings, we propose as a next step to investigate individual mental models induced by these visualizations in the framework of empirical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22171v1</guid>
      <category>physics.ed-ph</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Dechert, Svetlana Gurevich, Stefan Heusler</dc:creator>
    </item>
  </channel>
</rss>
