<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Dec 2024 05:01:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Comparing Clustering Approaches for Smart Meter Time Series: Investigating the Influence of Dataset Properties on Performance</title>
      <link>https://arxiv.org/abs/2412.02026</link>
      <description>arXiv:2412.02026v1 Announce Type: new 
Abstract: The widespread adoption of smart meters for monitoring energy consumption has generated vast quantities of high-resolution time series data which remains underutilised. While clustering has emerged as a fundamental tool for mining smart meter time series (SMTS) data, selecting appropriate clustering methods remains challenging despite numerous comparative studies. These studies often rely on problematic methodologies and consider a limited scope of methods, frequently overlooking compelling methods from the broader time series clustering literature. Consequently, they struggle to provide dependable guidance for practitioners designing their own clustering approaches. This paper presents a comprehensive comparative framework for SMTS clustering methods using expert-informed synthetic datasets that emphasise peak consumption behaviours as fundamental cluster concepts. Using a phased methodology, we first evaluated 31 distance measures and 8 representation methods using leave-one-out classification, then examined the better-suited methods in combination with 11 clustering algorithms. We further assessed the robustness of these combinations to systematic changes in key dataset properties that affect clustering performance on real-world datasets, including cluster balance, noise, and the presence of outliers. Our results revealed that methods accommodating local temporal shifts while maintaining amplitude sensitivity, particularly Dynamic Time Warping and $k$-sliding distance, consistently outperformed traditional approaches. Among other key findings, we identified that when combined with hierarchical clustering using Ward's linkage, these methods demonstrated consistent robustness across varying dataset characteristics without careful parameter tuning. These and other findings inform actionable recommendations for practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02026v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke W. Yerbury, Ricardo J. G. B. Campello, G. C. Livingston Jr, Mark Goldsworthy, Lachlan O'Neil</dc:creator>
    </item>
    <item>
      <title>TITE-CLRM: Towards efficient time-to-event dose-escalation guidance of multi-cycle cancer therapies</title>
      <link>https://arxiv.org/abs/2412.02355</link>
      <description>arXiv:2412.02355v1 Announce Type: new 
Abstract: Treatment of cancer has rapidly evolved over time in quite dramatic ways, for example from chemotherapies, targeted therapies to immunotherapies and chimeric antigen receptor T-cells. Nonetheless, the basic design of early phase I trials in oncology still follows pre-dominantly a dose-escalation design. These trials monitor safety over the first treatment cycle in order to escalate the dose of the investigated drug. However, over time studying additional factors such as drug combinations and/or variation in the timing of dosing became important as well. Existing designs were continuously enhanced and expanded to account for increased trial complexity. With toxicities occurring at later stages beyond the first cycle and the need to treat patients over multiple cycles, the focus on the first treatment cycle only is becoming a limitation in nowadays multi-cycle treatment therapies. Here we introduce a multi-cycle time-to-event model (TITE-CLRM: Time-Interval-To-Event Complementary-Loglog Regression Model) allowing guidance of dose-escalation trials studying multi-cycle therapies. The challenge lies in balancing the need to monitor safety of longer treatment periods with the need to continuously enroll patients safely. The proposed multi-cycle time to event model is formulated as an extension to established concepts like the escalation with over dose control principle. The model is motivated from a current drug development project and evaluated in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02355v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Andreas Widmer, Sebastian Weber, Yunnan Xu, Hans-Jochen Weber</dc:creator>
    </item>
    <item>
      <title>Use of surrogate endpoints in health technology assessment: a review of selected NICE technology appraisals in oncology</title>
      <link>https://arxiv.org/abs/2412.02380</link>
      <description>arXiv:2412.02380v1 Announce Type: new 
Abstract: Objectives: Surrogate endpoints, used to substitute for and predict final clinical outcomes, are increasingly being used to support submissions to health technology assessment agencies. The increase in use of surrogate endpoints has been accompanied by literature describing frameworks and statistical methods to ensure their robust validation. The aim of this review was to assess how surrogate endpoints have recently been used in oncology technology appraisals by the National Institute for Health and Care Excellence (NICE) in England and Wales.
  Methods: This paper identified technology appraisals in oncology published by NICE between February 2022 and May 2023. Data are extracted on methods for the use and validation of surrogate endpoints.
  Results: Of the 47 technology appraisals in oncology available for review, 18 (38 percent) utilised surrogate endpoints, with 37 separate surrogate endpoints being discussed. However, the evidence supporting the validity of the surrogate relationship varied significantly across putative surrogate relationships with 11 providing RCT evidence, 7 providing evidence from observational studies, 12 based on clinical opinion and 7 providing no evidence for the use of surrogate endpoints.
  Conclusions: This review supports the assertion that surrogate endpoints are frequently used in oncology technology appraisals in England and Wales. Despite increasing availability of statistical methods and guidance on appropriate validation of surrogate endpoints, this review highlights that use and validation of surrogate endpoints can vary between technology appraisals which can lead to uncertainty in decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02380v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorna Wheaton, Sylwia Bujkiewicz</dc:creator>
    </item>
    <item>
      <title>Model Determination for High-Dimensional Longitudinal Data with Missing Observations: An Application to Microfinance Data</title>
      <link>https://arxiv.org/abs/2412.02440</link>
      <description>arXiv:2412.02440v1 Announce Type: new 
Abstract: We propose an adaption of the multiple imputation random lasso procedure tailored to longitudinal data with unobserved fixed effects which provides robust variable selection in the presence of complex missingness, high dimensionality and multicollinearity. We apply it to identify social and financial success factors of microfinance institutions (MFIs) in a data-driven way from a comprehensive, balanced, and global panel with 136 characteristics for 213 MFIs over a six-year period. We discover the importance of staff structure for MFI success and find that profitability is the most important determinant of financial success. Our results indicate that financial sustainability and breadth of outreach can be increased simultaneously while the relationship with depth of outreach is more mixed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02440v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssa/qnae144</arxiv:DOI>
      <dc:creator>Lotta R\"uter, Melanie Schienle</dc:creator>
    </item>
    <item>
      <title>Dynamic Prediction of High-density Generalized Functional Data with Fast Generalized Functional Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2412.02014</link>
      <description>arXiv:2412.02014v1 Announce Type: cross 
Abstract: Dynamic prediction, which typically refers to the prediction of future outcomes using historical records, is often of interest in biomedical research. For datasets with large sample sizes, high measurement density, and complex correlation structures, traditional methods are often infeasible because of the computational burden associated with both data scale and model complexity. Moreover, many models do not directly facilitate out-of-sample predictions for generalized outcomes. To address these issues, we develop a novel approach for dynamic predictions based on a recently developed method estimating complex patterns of variation for exponential family data: fast Generalized Functional Principal Components Analysis (fGFPCA). Our method is able to handle large-scale, high-density repeated measures much more efficiently with its implementation feasible even on personal computational resources (e.g., a standard desktop or laptop computer). The proposed method makes highly flexible and accurate predictions of future trajectories for data that exhibit high degrees of nonlinearity, and allows for out-of-sample predictions to be obtained without reestimating any parameters. A simulation study is designed and implemented to illustrate the advantages of this method. To demonstrate its practical utility, we also conducted a case study to predict diurnal active/inactive patterns using accelerometry data from the National Health and Nutrition Examination Survey (NHANES) 2011-2014. Both the simulation study and the data application demonstrate the better predictive performance and high computational efficiency of the proposed method compared to existing methods. The proposed method also obtains more personalized prediction that improves as more information becomes available, which is an essential goal of dynamic prediction that other methods fail to achieve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02014v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ying Jin, Andrew Leroux</dc:creator>
    </item>
    <item>
      <title>Crash Severity Risk Modeling Strategies under Data Imbalance</title>
      <link>https://arxiv.org/abs/2412.02094</link>
      <description>arXiv:2412.02094v1 Announce Type: cross 
Abstract: This study investigates crash severity risk modeling strategies for work zones involving large vehicles (i.e., trucks, buses, and vans) when there are crash data imbalance between low-severity (LS) and high-severity (HS) crashes. We utilized crash data, involving large vehicles in South Carolina work zones for the period between 2014 and 2018, which included 4 times more LS crashes compared to HS crashes. The objective of this study is to explore crash severity prediction performance of various models under different feature selection and data balancing techniques. The findings of this study highlight a disparity between LS and HS predictions, with less-accurate prediction of HS crashes compared to LS crashes due to class imbalance and feature overlaps between LS and HS crashes. Combining features from multiple feature selection techniques: statistical correlation, feature importance, recursive elimination, statistical tests, and mutual information, slightly improves HS crash prediction performance. Data balancing techniques such as NearMiss-1 and RandomUnderSampler, maximize HS recall when paired with certain prediction models, such as Bayesian Mixed Logit (BML), NeuralNet, and RandomForest, making them suitable for HS crash prediction. Conversely, RandomOverSampler, HS Class Weighting, and Kernel-based Synthetic Minority Oversampling (K-SMOTE), used with certain prediction models such as BML, CatBoost, and LightGBM, achieve a balanced performance, defined as achieving an equitable trade-off between LS and HS prediction performance metrics. These insights provide safety analysts with guidance to select models, feature selection techniques, and data balancing techniques that align with their specific safety objectives, offering a robust foundation for enhancing work-zone crash severity prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02094v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abdullah Al Mamun (Graduate Student, Glenn Department of Civil Engineering, Clemson University), Abyad Enan (Graduate Student, Glenn Department of Civil Engineering, Clemson University), Debbie A. Indah (Graduate Student, Department of Engineering, South Carolina State University), Judith Mwakalonge (Professor, Department of Engineering, South Carolina State University), Gurcan Comert (Associate Professor, Computational Data Science and Engineering Department, North Carolina A&amp;T State University), Mashrur Chowdhury (Professor, Glenn Department of Civil Engineering, Clemson University)</dc:creator>
    </item>
    <item>
      <title>Vector Optimization with Gaussian Process Bandits</title>
      <link>https://arxiv.org/abs/2412.02484</link>
      <description>arXiv:2412.02484v1 Announce Type: cross 
Abstract: Learning problems in which multiple conflicting objectives must be considered simultaneously often arise in various fields, including engineering, drug design, and environmental management. Traditional methods for dealing with multiple black-box objective functions, such as scalarization and identification of the Pareto set under the componentwise order, have limitations in incorporating objective preferences and exploring the solution space accordingly. While vector optimization offers improved flexibility and adaptability via specifying partial orders based on ordering cones, current techniques designed for sequential experiments either suffer from high sample complexity or lack theoretical guarantees. To address these issues, we propose Vector Optimization with Gaussian Process (VOGP), a probably approximately correct adaptive elimination algorithm that performs black-box vector optimization using Gaussian process bandits. VOGP allows users to convey objective preferences through ordering cones while performing efficient sampling by exploiting the smoothness of the objective function, resulting in a more effective optimization process that requires fewer evaluations. We establish theoretical guarantees for VOGP and derive information gain-based and kernel-specific sample complexity bounds. We also conduct experiments on both real-world and synthetic datasets to compare VOGP with the state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02484v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\.Ilter Onat Korkmaz, Ya\c{s}ar Cahit Y{\i}ld{\i}r{\i}m, \c{C}a\u{g}{\i}n Ararat, Cem Tekin</dc:creator>
    </item>
    <item>
      <title>Towards the use of multiple ROIs for radiomics-based survival modelling: finding a strategy of aggregating lesions</title>
      <link>https://arxiv.org/abs/2405.17668</link>
      <description>arXiv:2405.17668v2 Announce Type: replace 
Abstract: Background. Radiomic features, derived from a region of interest (ROI) in medical images, are valuable as prognostic factors. Selecting an appropriate ROI is critical, and many recent studies have focused on leveraging multiple ROIs by segmenting analogous regions across patients - such as the primary tumour and peritumoral area or subregions of the tumour. These can be straightforwardly incorporated into models as additional features. However, a more complex scenario arises for example in a regionally disseminated disease, when multiple distinct lesions are present. Aim. This study aims to evaluate the feasibility of integrating radiomic data from multiple lesions into survival models. We explore strategies for incorporating these ROIs and hypothesise that including all available lesions can improve model performance. Methods. While each lesion produces a feature vector, the desired result is a unified prediction. We propose methods to aggregate either the feature vectors to form a representative ROI or the modeling results to compute a consolidated risk score. As a proof of concept, we apply these strategies to predict distant metastasis risk in a cohort of 115 non-small cell lung cancer patients, 60% of whom exhibit regionally advanced disease. Two feature sets (radiomics extracted from PET and PET interpolated to CT resolution) are tested across various survival models using a Monte Carlo Cross-Validation framework. Results. Across both feature sets, incorporating all available lesions - rather than limiting analysis to the primary tumour - consistently improved the c-index, irrespective of the survival model used. Conclusion. Lesions beyond the primary tumour carry information that should be utilised in radiomics-based models to enhance predictive ability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17668v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Agata Ma{\l}gorzata Wilk, Andrzej Swierniak, Andrea d'Amico, Rafa{\l} Suwi\'nski, Krzysztof Fujarewicz, Damian Borys</dc:creator>
    </item>
    <item>
      <title>Estimating Treatment Effect under Additive Hazards Models with High-dimensional Covariates</title>
      <link>https://arxiv.org/abs/1907.00287</link>
      <description>arXiv:1907.00287v2 Announce Type: replace-cross 
Abstract: Estimating causal effects for survival outcomes in the high-dimensional setting is an extremely important topic for many biomedical applications as well as areas of social sciences. We propose a new orthogonal score method for treatment effect estimation and inference that results in asymptotically valid confidence intervals assuming only good estimation properties of the hazard outcome model and the conditional probability of treatment. This guarantee allows us to provide valid inference for the conditional treatment effect under the high-dimensional additive hazards model under considerably more generality than existing approaches. In addition, we develop a new Hazards Difference (HDi), estimator. We showcase that our approach has double-robustness properties in high dimensions: with cross-fitting, the HDi estimate is consistent under a wide variety of treatment assignment models; the HDi estimate is also consistent when the hazards model is misspecified and instead the true data generating mechanism follows a partially linear additive hazards model. We further develop a novel sparsity doubly robust result, where either the outcome or the treatment model can be a fully dense high-dimensional model. We apply our methods to study the treatment effect of radical prostatectomy versus conservative management for prostate cancer patients using the SEER-Medicare Linked Data.</description>
      <guid isPermaLink="false">oai:arXiv.org:1907.00287v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2021.1930546</arxiv:DOI>
      <arxiv:journal_reference>Journal of the American Statistical Association 118(541) 327-342 (2021)</arxiv:journal_reference>
      <dc:creator>Jue Hou, Jelena Bradic, Ronghui Xu</dc:creator>
    </item>
    <item>
      <title>Supervised Multiple Kernel Learning approaches for multi-omics data integration</title>
      <link>https://arxiv.org/abs/2403.18355</link>
      <description>arXiv:2403.18355v2 Announce Type: replace-cross 
Abstract: Advances in high-throughput technologies have originated an ever-increasing availability of omics datasets. The integration of multiple heterogeneous data sources is currently an issue for biology and bioinformatics. Multiple kernel learning (MKL) has shown to be a flexible and valid approach to consider the diverse nature of multi-omics inputs, despite being an underused tool in genomic data mining. We provide novel MKL approaches based on different kernel fusion strategies. To learn from the meta-kernel of input kernels, we adapted unsupervised integration algorithms for supervised tasks with support vector machines. We also tested deep learning architectures for kernel fusion and classification. The results show that MKL-based models can outperform more complex, state-of-the-art, supervised multi-omics integrative approaches. Multiple kernel learning offers a natural framework for predictive models in multi-omics data. It proved to provide a fast and reliable solution that can compete with and outperform more complex architectures. Our results offer a direction for bio-data mining research, biomarker discovery and further development of methods for heterogeneous data integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18355v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1186/s13040-024-00406-9</arxiv:DOI>
      <arxiv:journal_reference>BioData Mining 17, 53 (2024)</arxiv:journal_reference>
      <dc:creator>Mitja Briscik, Gabriele Tazza, Marie-Agnes Dillies, L\'aszl\'o Vid\'acs, S\'ebastien Dejean</dc:creator>
    </item>
  </channel>
</rss>
