<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Sep 2024 04:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Legend of Holy Sword: An Immersive Experience for Concentration Enhancement</title>
      <link>https://arxiv.org/abs/2408.16782</link>
      <description>arXiv:2408.16782v1 Announce Type: new 
Abstract: Concentration is significant for maximizing potential in any activity. However, traditional methods to improve it often lack direct and natural feedback. We propose an innovative and inspiring VR system to experience and improve concentration. In the experience of pulling out the holy sword, which cannot be achieved simply by force, the player receives multimodal concentration feedback in visual, auditory, and haptic senses. We believe that this experience will help the user confront his/her concentration and improve the ability to control it consciously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16782v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hirosuke Asahi, Ryoma Sonoyama, Chihiro Shoda, Nanami Kotani</dc:creator>
    </item>
    <item>
      <title>A Bibliometric Analysis of Trust in Conversational Agents over the Past Fifteen Years</title>
      <link>https://arxiv.org/abs/2408.16837</link>
      <description>arXiv:2408.16837v1 Announce Type: new 
Abstract: Conversational agents (CA) have become increasingly prevalent in various domains, driving significant interest in understanding the dynamics of trust in CA. This study addresses the need for a comprehensive analysis of research trends in this field, especially given the rapid advancements and growing use of CA technologies like ChatGPT. Through bibliometric analysis, we aim to identify key keywords, disciplines, research clusters, and international collaborations related to CA and trust. We analyzed 955 studies published between 2009 and 2024, all sourced from the Scopus database. Additionally, we conducted a text clustering analysis to identify the main themes in the publications and understand their distribution. Our findings highlight the increasing interest in CA, particularly with the introduction of ChatGPT. The USA leads in research output, followed by Germany, China, and the UK. Furthermore, there is a notable rise in interdisciplinary research, especially in the fields of human-computer interaction and artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16837v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Meltem Aksoy, Annika Bush</dc:creator>
    </item>
    <item>
      <title>Legacy Learning Using Few-Shot Font Generation Models for Automatic Text Design in Metaverse Content: Cases Studies in Korean and Chinese</title>
      <link>https://arxiv.org/abs/2408.16900</link>
      <description>arXiv:2408.16900v1 Announce Type: new 
Abstract: Generally, the components constituting a metaverse are classified into hardware, software, and content categories. As a content component, text design is known to positively affect user immersion and usability. Unlike English, where designing texts involves only 26 letters, designing texts in Korean and Chinese requires creating 11,172 and over 60,000 individual glyphs, respectively, owing to the nature of the languages. Consequently, applying new text designs to enhance user immersion within the metaverse can be tedious and expensive, particularly for certain languages. Recently, efforts have been devoted toward addressing this issue using generative artificial intelligence (AI). However, challenges remain in creating new text designs for the metaverse owing to inaccurate character structures. This study proposes a new AI learning method known as Legacy Learning, which enables high-quality text design at a lower cost. Legacy Learning involves recombining existing text designs and intentionally introducing variations to produce fonts that are distinct from the originals while maintaining high quality. To demonstrate the effectiveness of the proposed method in generating text designs for the metaverse, we performed evaluations from the following three aspects: 1) Quantitative performance evaluation 2) Qualitative evaluationand 3) User usability evaluation. The quantitative and qualitative performance results indicated that the generated text designs differed from the existing ones by an average of over 30% while still maintaining high visual quality. Additionally, the SUS test performed with metaverse content designers achieved a score of 95.8, indicating high usability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16900v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Younghwi Kim, Seok Chan Jeong, Sunghyun Sim</dc:creator>
    </item>
    <item>
      <title>The Future of Open Human Feedback</title>
      <link>https://arxiv.org/abs/2408.16961</link>
      <description>arXiv:2408.16961v1 Announce Type: new 
Abstract: Human feedback on conversations with language language models (LLMs) is central to how these systems learn about the world, improve their capabilities, and are steered toward desirable and safe behaviors. However, this feedback is mostly collected by frontier AI labs and kept behind closed doors. In this work, we bring together interdisciplinary experts to assess the opportunities and challenges to realizing an open ecosystem of human feedback for AI. We first look for successful practices in peer production, open source, and citizen science communities. We then characterize the main challenges for open human feedback. For each, we survey current approaches and offer recommendations. We end by envisioning the components needed to underpin a sustainable and open human feedback ecosystem. In the center of this ecosystem are mutually beneficial feedback loops, between users and specialized models, incentivizing a diverse stakeholders community of model trainers and feedback providers to support a general open feedback pool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16961v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shachar Don-Yehiya, Ben Burtenshaw, Ramon Fernandez Astudillo, Cailean Osborne, Mimansa Jaiswal, Tzu-Sheng Kuo, Wenting Zhao, Idan Shenfeld, Andi Peng, Mikhail Yurochkin, Atoosa Kasirzadeh, Yangsibo Huang, Tatsunori Hashimoto, Yacine Jernite, Daniel Vila-Suero, Omri Abend, Jennifer Ding, Sara Hooker, Hannah Rose Kirk, Leshem Choshen</dc:creator>
    </item>
    <item>
      <title>Tonal Cognition in Sonification: Exploring the Needs of Practitioners in Sonic Interaction Design</title>
      <link>https://arxiv.org/abs/2408.17012</link>
      <description>arXiv:2408.17012v1 Announce Type: new 
Abstract: Research into tonal music examines the structural relationships among sounds and how they align with our auditory perception. The exploration of integrating tonal cognition into sonic interaction design, particularly for practitioners lacking extensive musical knowledge, and developing an accessible software tool, remains limited. We report on a study of designers to understand the sound creation practices of industry experts and explore how infusing tonal music principles into a sound design tool can better support their craft and enhance the sonic experiences they create. Our study collected qualitative data through semi-structured individual and focus group interviews with six participants. We developed a low-fidelity prototype sound design tool that involves practical methods of functional harmony and interaction design discussed in focus groups. We identified four themes through reflexive thematic analysis: decision-making, domain knowledge and terminology, collaboration, and contexts in sound creation. Finally, we discussed design considerations for an accessible sonic interaction design tool that aligns auditory experience more closely with tonal cognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17012v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3678299.3678321</arxiv:DOI>
      <dc:creator>Minsik Choi, Josh Andres, Charles Patrick Martin</dc:creator>
    </item>
    <item>
      <title>Exploring User Acceptance Of Portable Intelligent Personal Assistants: A Hybrid Approach Using PLS-SEM And fsQCA</title>
      <link>https://arxiv.org/abs/2408.17119</link>
      <description>arXiv:2408.17119v1 Announce Type: new 
Abstract: This research explores the factors driving user acceptance of Rabbit R1, a newly developed portable intelligent personal assistant (PIPA) that aims to redefine user interaction and control. The study extends the technology acceptance model (TAM) by incorporating artificial intelligence-specific factors (conversational intelligence, task intelligence, and perceived naturalness), user interface design factors (simplicity in information design and visual aesthetics), and user acceptance and loyalty. Using a purposive sampling method, we gathered data from 824 users in the US and analyzed the sample through partial least squares structural equation modeling (PLS-SEM) and fuzzy set qualitative comparative analysis (fsQCA). The findings reveal that all hypothesized relationships, including both direct and indirect effects, are supported. Additionally, fsQCA supports the PLS-SEM findings and identifies three configurations leading to high and low user acceptance. This research enriches the literature and provides valuable insights for system designers and marketers of PIPAs, guiding strategic decisions to foster widespread adoption and long-term engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17119v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gustave Florentin Nkoulou Mvondo, Ben Niu</dc:creator>
    </item>
    <item>
      <title>"Benefit Game: Alien Seaweed Swarms" -- Real-time Gamification of Digital Seaweed Ecology</title>
      <link>https://arxiv.org/abs/2408.17186</link>
      <description>arXiv:2408.17186v1 Announce Type: new 
Abstract: "Benefit Game: Alien Seaweed Swarms" combines artificial life art and interactive game with installation to explore the impact of human activity on fragile seaweed ecosystems. The project aims to promote ecological consciousness by creating a balance in digital seaweed ecologies. Inspired by the real species "Laminaria saccharina", the author employs Procedural Content Generation via Machine Learning technology to generate variations of virtual seaweeds and symbiotic fungi. The audience can explore the consequences of human activities through gameplay and observe the ecosystem's feedback on the benefits and risks of seaweed aquaculture. This Benefit Game offers dynamic and real-time responsive artificial seaweed ecosystems for an interactive experience that enhances ecological consciousness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17186v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan-Lu Fei, Zi-Wei Wu, Kang Zhang</dc:creator>
    </item>
    <item>
      <title>XULIA -- Comprehensive control system for Windows$^{tm}$ devices designed for people with tetraplegia</title>
      <link>https://arxiv.org/abs/2408.17314</link>
      <description>arXiv:2408.17314v1 Announce Type: new 
Abstract: XULIA is a comprehensive control system for Windows computers designed specifically to be used by quadriplegic people or people who do not have the ability to move their upper limbs accurately. XULIA allows you to manage all the functions necessary to control all Windows functions using only your voice. As a voice-to-text transcription system, it uses completely free modules combining the Windows SAPI voice recognition libraries for command recognition with Google's cloud-based voice recognition systems indirectly through a Google Chrome browser, which allows you to use Google's paid voice-to-text transcription services completely free of charge. XULIA manages multiple grammars simultaneously with automatic activation to ensure that the set of commands to be recognized is reduced to a minimum at all times, which allows false positives in command recognition to be reduced to a minimum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17314v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Losada Gonzalez</dc:creator>
    </item>
    <item>
      <title>Technostress and Resistance to Change in Maritime Digital Transformation: A Focused Review</title>
      <link>https://arxiv.org/abs/2408.17408</link>
      <description>arXiv:2408.17408v1 Announce Type: new 
Abstract: The maritime industry is undergoing a significant digital transformation (DT) to enhance efficiency and sustainability. This focused review investigates the current state of literature on technostress and resistance to change among seafarers as they adapt to new digital technologies. By critically reviewing a focused selection of peer-reviewed articles, we identify the main themes and trends within maritime research on DT. Findings indicate that while mental health issues are a predominant concern, this is yet to also be investigated in the context of new technology introduction in an industry that is already setting seafarers under pressure. Additionally, change management is not addressed, and DT is limited to specific functionalities rather than embracing broad work practice transformations</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17408v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benedicte Frederikke Rex Fleron, Raluca A. Stana</dc:creator>
    </item>
    <item>
      <title>Measuring Transparency in Intelligent Robots</title>
      <link>https://arxiv.org/abs/2408.16865</link>
      <description>arXiv:2408.16865v1 Announce Type: cross 
Abstract: As robots become increasingly integrated into our daily lives, the need to make them transparent has never been more critical. Yet, despite its importance in human-robot interaction, a standardized measure of robot transparency has been missing until now. This paper addresses this gap by presenting the first comprehensive scale to measure perceived transparency in robotic systems, available in English, German, and Italian languages. Our approach conceptualizes transparency as a multidimensional construct, encompassing explainability, legibility, predictability, and meta-understanding. The proposed scale was a product of a rigorous three-stage process involving 1,223 participants. Firstly, we generated the items of our scale, secondly, we conducted an exploratory factor analysis, and thirdly, a confirmatory factor analysis served to validate the factor structure of the newly developed TOROS scale. The final scale encompasses 26 items and comprises three factors: Illegibility, Explainability, and Predictability. TOROS demonstrates high cross-linguistic reliability, inter-factor correlation, model fit, internal consistency, and convergent validity across the three cross-national samples. This empirically validated tool enables the assessment of robot transparency and contributes to the theoretical understanding of this complex construct. By offering a standardized measure, we facilitate consistent and comparable research in human-robot interaction in which TOROS can serve as a benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16865v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgios Angelopoulos, Dimitri Lacroix, Ricarda Wullenkord, Alessandra Rossi, Silvia Rossi, Friederike Eyssel</dc:creator>
    </item>
    <item>
      <title>From "Made In" to Mukokuseki: Exploring the Visual Perception of National Identity in Robots</title>
      <link>https://arxiv.org/abs/2408.16949</link>
      <description>arXiv:2408.16949v1 Announce Type: cross 
Abstract: People read human characteristics into the design of social robots, a visual process with socio-cultural implications. One factor may be nationality, a complex social characteristic that is linked to ethnicity, culture, and other factors of identity that can be embedded in the visual design of robots. Guided by social identity theory (SIT), we explored the notion of "mukokuseki," a visual design characteristic defined by the absence of visual cues to national and ethnic identity in Japanese cultural exports. In a two-phase categorization study (n=212), American (n=110) and Japanese (n=92) participants rated a random selection of nine robot stimuli from America and Japan, plus multinational Pepper. We found evidence of made-in and two kinds of mukokuseki effects. We offer suggestions for the visual design of mukokuseki robots that may interact with people from diverse backgrounds. Our findings have implications for robots and social identity, the viability of robotic exports, and the use of robots internationally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16949v1</guid>
      <category>cs.RO</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3681782</arxiv:DOI>
      <arxiv:journal_reference>(2024) ACM Transactions on Human-Robot Interaction.</arxiv:journal_reference>
      <dc:creator>Katie Seaborn, Haruki Kotani, Peter Pennefather</dc:creator>
    </item>
    <item>
      <title>CinePreGen: Camera Controllable Video Previsualization via Engine-powered Diffusion</title>
      <link>https://arxiv.org/abs/2408.17424</link>
      <description>arXiv:2408.17424v1 Announce Type: cross 
Abstract: With advancements in video generative AI models (e.g., SORA), creators are increasingly using these techniques to enhance video previsualization. However, they face challenges with incomplete and mismatched AI workflows. Existing methods mainly rely on text descriptions and struggle with camera placement, a key component of previsualization. To address these issues, we introduce CinePreGen, a visual previsualization system enhanced with engine-powered diffusion. It features a novel camera and storyboard interface that offers dynamic control, from global to local camera adjustments. This is combined with a user-friendly AI rendering workflow, which aims to achieve consistent results through multi-masked IP-Adapter and engine simulation guidelines. In our comprehensive evaluation study, we demonstrate that our system reduces development viscosity (i.e., the complexity and challenges in the development process), meets users' needs for extensive control and iteration in the design process, and outperforms other AI video production workflows in cinematic camera movement, as shown by our experiments and a within-subjects user study. With its intuitive camera controls and realistic rendering of camera motion, CinePreGen shows great potential for improving video production for both individual creators and industry professionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17424v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Chen, Anyi Rao, Xuekun Jiang, Shishi Xiao, Ruiqing Ma, Zeyu Wang, Hui Xiong, Bo Dai</dc:creator>
    </item>
    <item>
      <title>EEGMatch: Learning with Incomplete Labels for Semi-Supervised EEG-based Cross-Subject Emotion Recognition</title>
      <link>https://arxiv.org/abs/2304.06496</link>
      <description>arXiv:2304.06496v2 Announce Type: replace-cross 
Abstract: Electroencephalography (EEG) is an objective tool for emotion recognition and shows promising performance. However, the label scarcity problem is a main challenge in this field, which limits the wide application of EEG-based emotion recognition. In this paper, we propose a novel semi-supervised learning framework (EEGMatch) to leverage both labeled and unlabeled EEG data. First, an EEG-Mixup based data augmentation method is developed to generate more valid samples for model learning. Second, a semi-supervised two-step pairwise learning method is proposed to bridge prototype-wise and instance-wise pairwise learning, where the prototype-wise pairwise learning measures the global relationship between EEG data and the prototypical representation of each emotion class and the instance-wise pairwise learning captures the local intrinsic relationship among EEG data. Third, a semi-supervised multi-domain adaptation is introduced to align the data representation among multiple domains (labeled source domain, unlabeled source domain, and target domain), where the distribution mismatch is alleviated. Extensive experiments are conducted on two benchmark databases (SEED and SEED-IV) under a cross-subject leave-one-subject-out cross-validation evaluation protocol. The results show the proposed EEGmatch performs better than the state-of-the-art methods under different incomplete label conditions (with 6.89% improvement on SEED and 1.44% improvement on SEED-IV), which demonstrates the effectiveness of the proposed EEGMatch in dealing with the label scarcity problem in emotion recognition using EEG signals. The source code is available at https://github.com/KAZABANA/EEGMatch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.06496v2</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rushuang Zhou, Weishan Ye, Zhiguo Zhang, Yanyang Luo, Li Zhang, Linling Li, Gan Huang, Yining Dong, Yuan-Ting Zhang, Zhen Liang</dc:creator>
    </item>
    <item>
      <title>Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing</title>
      <link>https://arxiv.org/abs/2310.12404</link>
      <description>arXiv:2310.12404v2 Announce Type: replace-cross 
Abstract: Creating music is iterative, requiring varied methods at each stage. However, existing AI music systems fall short in orchestrating multiple subsystems for diverse needs. To address this gap, we introduce Loop Copilot, a novel system that enables users to generate and iteratively refine music through an interactive, multi-round dialogue interface. The system uses a large language model to interpret user intentions and select appropriate AI models for task execution. Each backend model is specialized for a specific task, and their outputs are aggregated to meet the user's requirements. To ensure musical coherence, essential attributes are maintained in a centralized table. We evaluate the effectiveness of the proposed system through semi-structured interviews and questionnaires, highlighting its utility not only in facilitating music creation but also its potential for broader applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12404v2</guid>
      <category>cs.SD</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>eess.AS</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yixiao Zhang, Akira Maezawa, Gus Xia, Kazuhiko Yamamoto, Simon Dixon</dc:creator>
    </item>
    <item>
      <title>Evolving Virtual World with Delta-Engine</title>
      <link>https://arxiv.org/abs/2408.05842</link>
      <description>arXiv:2408.05842v3 Announce Type: replace-cross 
Abstract: In this paper, we focus on the \emph{virtual world}, a cyberspace where people can live in. An ideal virtual world shares great similarity with our real world. One of the crucial aspects is its evolving nature, reflected by individuals' capability to grow and thereby influence the objective world. Such dynamics is unpredictable and beyond the reach of existing systems. For this, we propose a special engine called \textbf{\emph{Delta-Engine}} to drive this virtual world. $\Delta$ associates the world's evolution to the engine's scalability. It consists of a base engine and a neural proxy. The base engine programs the prototype of the virtual world; given a trigger, the neural proxy generates new snippets on the base engine through \emph{incremental prediction}. This paper presents a full-stack introduction to the delta-engine. The key feature of the delta-engine is its scalability to unknown elements within the world, Technically, it derives from the prefect co-work of the neural proxy and the base engine, and the alignment with high-quality data. We introduce an engine-oriented fine-tuning method that embeds the base engine into the proxy. We then discuss the human-LLM collaborative design to produce novel and interesting data efficiently. Eventually, we propose three evaluation principles to comprehensively assess the performance of a delta engine: naive evaluation, incremental evaluation, and adversarial evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05842v3</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongqiu Wu, Zekai Xu, Tianyang Xu, Shize Wei, Yan Wang, Jiale Hong, Weiqi Wu, Hai Zhao, Min Zhang, Zhezhi He</dc:creator>
    </item>
    <item>
      <title>Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming</title>
      <link>https://arxiv.org/abs/2408.16725</link>
      <description>arXiv:2408.16725v2 Announce Type: replace-cross 
Abstract: Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method "Any Model Can Talk". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16725v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhifei Xie, Changqiao Wu</dc:creator>
    </item>
  </channel>
</rss>
