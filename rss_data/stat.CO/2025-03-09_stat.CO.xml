<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Mar 2025 04:01:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>regMMD: a package for parametric estimation and regression with maximum mean discrepancy</title>
      <link>https://arxiv.org/abs/2503.05297</link>
      <description>arXiv:2503.05297v1 Announce Type: new 
Abstract: The Maximum Mean Discrepancy (MMD) is a kernel-based metric widely used for nonparametric tests and estimation. Recently, it has also been studied as an objective function for parametric estimation, as it has been shown to yield robust estimators. We have implemented MMD minimization for parameter inference in a wide range of statistical models, including various regression models, within an R package called regMMD. This paper provides an introduction to the regMMD package. We describe the available kernels and optimization procedures, as well as the default settings. Detailed applications to simulated and real data are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05297v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Alquier, Mathieu Gerber</dc:creator>
    </item>
    <item>
      <title>Bhirkuti's Relative Efficiency (BRE): Examining in Psychometric Simulations</title>
      <link>https://arxiv.org/abs/2503.04775</link>
      <description>arXiv:2503.04775v1 Announce Type: cross 
Abstract: Traditional Relative Efficiency (RE), based solely on variance, has limitations in estimator performance evaluation, especially in planned missing data designs. We introduce Bhirkuti's Relative Efficiency (BRE), a novel metric that integrates precision and accuracy for a more robust assessment. BRE is computed using interquartile range (IQR) overlap for precision and a bias adjustment factor based on the absolute median relative bias (AMRB). Monte Carlo simulations using a Latent Growth Model (LGM) with planned missing data (SWMD-6) illustrate that BRE remains theoretically consistent and interpretable, avoiding paradoxes such as RE exceeding 100%. Visualizations via boxplots and ridgeline plots confirm that BRE provides a stable and meaningful estimator efficiency evaluation, making it a valuable advancement in psychometric and statistical modeling. By addressing fundamental weaknesses in traditional RE, BRE provides a superior, theoretically justified alternative for relative efficiency assessment in psychometric modeling, structural equation modeling, and missing data research. This advancement enhances data-driven decision-making and offers a methodologically rigorous tool for researchers analyzing incomplete datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04775v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aneel Bhusal, Todd D. Little</dc:creator>
    </item>
    <item>
      <title>Exploiting Inexact Computations in Multilevel Sampling Methods</title>
      <link>https://arxiv.org/abs/2503.05533</link>
      <description>arXiv:2503.05533v1 Announce Type: cross 
Abstract: Multilevel sampling methods, such as multilevel and multifidelity Monte Carlo, multilevel stochastic collocation, or delayed acceptance Markov chain Monte Carlo, have become standard uncertainty quantification tools for a wide class of forward and inverse problems. The underlying idea is to achieve faster convergence by leveraging a hierarchy of models, such as partial differential equation (PDE) or stochastic differential equation (SDE) discretisations with increasing accuracy. By optimally redistributing work among the levels, multilevel methods can achieve significant performance improvement compared to single level methods working with one high-fidelity model. Intuitively, approximate solutions on coarser levels can tolerate large computational error without affecting the overall accuracy. We show how this can be used in high-performance computing applications to obtain a significant performance gain.
  As a use case, we analyse the computational error in the standard multilevel Monte Carlo method and formulate an adaptive algorithm which determines a minimum required computational accuracy on each level of discretisation. We show two examples of how the inexactness can be converted into actual gains using an elliptic PDE with lognormal random coefficients. Using a low precision sparse direct solver combined with iterative refinement results in a simulated gain in memory references of up to $3.5\times$ compared to the reference double precision solver; while using a MINRES iterative solver, a practical speedup of up to $1.5\times$ in terms of FLOPs is achieved. These results provide a step in the direction of energy-aware scientific computing, with significant potential for energy savings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05533v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Josef Mart\'inek, Erin Carson, Robert Scheichl</dc:creator>
    </item>
    <item>
      <title>Exact discovery is polynomial for certain sparse causal Bayesian networks</title>
      <link>https://arxiv.org/abs/2406.15012</link>
      <description>arXiv:2406.15012v2 Announce Type: replace 
Abstract: Causal Bayesian networks are widely used tools for summarising the dependencies between variables and elucidating their putative causal relationships. By restricting the search to trees, for example, learning the optimum from data is polynomial, but this does not guarantee finding the optimal network overall. Without similar restrictions, exact discovery of the optimum is computationally hard in general and no polynomial results are known. The current state-of-the-art approaches are integer linear programming over the underlying space of directed acyclic graphs, dynamic programming and shortest-path searches over the space of topological orders, and constraint programming combining both. For dynamic programming over orders, the computational complexity is known to be exponential base 2 in the number of variables in the network. We demonstrate how to use properties of Bayesian networks to prune the search space and lower the computational cost, while still guaranteeing exact discovery of the provably optimal network. We also include new path-search and divide-and-conquer criteria. Without a priori constraining the search to certain types of networks, the algorithm completes in quadratic time when the optimum is a matching, and in polynomial time when the optimum belongs to any network class with logarithmically-bound largest connected components. In simulation studies we observe the polynomial dependence for sparse networks and that, beyond some critical value, the logarithm of the base grows with the network density. Our approach then out-competes the state-of-the-art at lower densities. These results therefore pave the way for faster exact causal discovery in larger and sparser networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15012v2</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Felix L. Rios, Giusi Moffa, Jack Kuipers</dc:creator>
    </item>
    <item>
      <title>Filtered Markovian Projection: Dimensionality Reduction in Filtering for Stochastic Reaction Networks</title>
      <link>https://arxiv.org/abs/2502.07918</link>
      <description>arXiv:2502.07918v2 Announce Type: replace-cross 
Abstract: Stochastic reaction networks (SRNs) model stochastic effects for various applications, including intracellular chemical or biological processes and epidemiology. A typical challenge in practical problems modeled by SRNs is that only a few state variables can be dynamically observed. Given the measurement trajectories, one can estimate the conditional probability distribution of unobserved (hidden) state variables by solving a stochastic filtering problem. In this setting, the conditional distribution evolves over time according to an extensive or potentially infinite-dimensional system of coupled ordinary differential equations with jumps, known as the filtering equation. The current numerical filtering techniques, such as the Filtered Finite State Projection (D'Ambrosio et al., 2022), are hindered by the curse of dimensionality, significantly affecting their computational performance. To address these limitations, we propose to use a dimensionality reduction technique based on the Markovian projection (MP), initially introduced for forward problems (Ben Hammouda et al., 2024). In this work, we explore how to adapt the existing MP approach to the filtering problem and introduce a novel version of the MP, the Filtered MP, that guarantees the consistency of the resulting estimator. The novel method employs a reduced-variance particle filter for estimating the jump intensities of the projected model and solves the filtering equations in a low-dimensional space. The analysis and empirical results highlight the superior computational efficiency of projection methods compared to the existing filtered finite state projection in the large dimensional setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07918v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiheb Ben Hammouda, Maksim Chupin, Sophia M\"unker, Ra\'ul Tempone</dc:creator>
    </item>
  </channel>
</rss>
