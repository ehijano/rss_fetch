<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Jul 2025 01:38:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On Combining Two Server Control Policies for Energy Efficiency</title>
      <link>https://arxiv.org/abs/2507.03510</link>
      <description>arXiv:2507.03510v1 Announce Type: new 
Abstract: Two popular server control policies are available for reducing energy consumption while maintaining acceptable performance levels: server speed scaling and the ability to turn servers off (and on). In this work, we explore the question of whether there are synergistic effects between these two mechanisms. To do this, we employ a continuous-time Markov chain model where the server can be turned off (and turning the server back on takes some time) and where the speed of the server can take on two values: a nominal operating speed and a reduced operating speed. For a cost function that is linear in the mean response time and server power consumption, we suggest that the mechanisms are not synergistic in that for all system loads, one mechanism is dominant in that if the other mechanism is also employed, there is only a small decrease in cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03510v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingze Dai, Douglas G. Down</dc:creator>
    </item>
    <item>
      <title>Affine Frequency Division Multiplexing Over Wideband Doubly-Dispersive Channels With Time-Scaling Effects</title>
      <link>https://arxiv.org/abs/2507.03537</link>
      <description>arXiv:2507.03537v1 Announce Type: new 
Abstract: The recently proposed affine frequency division multiplexing (AFDM) modulation has been considered as a promising technology for narrowband doubly-dispersive channels. However, the time-scaling effects, i.e., pulse widening and pulse shortening phenomena, in extreme wideband doubly-dispersive channels have not been considered in the literatures. In this paper, we investigate such wideband transmission and develop an efficient transmission structure with chirp-periodic prefix (CPP) and chirp-periodic suffix (CPS) for AFDM system. We derive the input-output relationship of AFDM system under time-scaled wideband doubly-dispersive channels and demonstrate the sparsity in discrete affine Fourier (DAF) domain equivalent channels. We further optimize the AFDM chirp parameters to accommodate the time-scaling characteristics in wideband doubly-dispersive channels and verify the superiority of the derived chirp parameters by pairwise error probability (PEP) analysis. We also develop an efficient cross domain distributed orthogonal approximate message passing (CD-D-OAMP) algorithm for AFDM symbol detection and analyze its corresponding state evolution. By analyzing the detection complexity of CD-D-OAMP detector and evaluating the error performance of AFDM systems based on simulations, we demonstrate that the AFDM system with our optimized chirp parameters outperforms the existing competitive modulation schemes in time-scaled wideband doubly-dispersive channels. Moreover, our proposed CD-D-OAMP detector can achieve the desirable trade-off between the complexity and performance, while supporting parallel computing to significantly reduce the computational latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03537v1</guid>
      <category>cs.PF</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangxiang Li, Haiyan Wang, Yao Ge, Xiaohong Shen, Yong Liang Guan, Miaowen Wen, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>DistZO2: High-Throughput and Memory-Efficient Zeroth-Order Fine-tuning LLMs with Distributed Parallel Computing</title>
      <link>https://arxiv.org/abs/2507.03211</link>
      <description>arXiv:2507.03211v1 Announce Type: cross 
Abstract: Fine-tuning large language models (LLMs) remains resource-intensive due to their sheer scale. While zeroth-order (ZO) optimization provides a memory-efficient alternative by eliminating backward passes, its application to multi-hundred-billion-parameter models is constrained by GPU memory and compute throughput. The ZO2 framework addresses the memory bottleneck by offloading model parameters to CPU memory and overlapping transformer block transfer with dual forward computation on a single GPU. However, ZO2 remains limited by its single-device execution and achieves modest throughput. In this work, we present DistZO2, a high-throughput, memory-efficient framework for distributed zeroth-order fine-tuning of LLMs. DistZO2 introduces three parallel strategies: (1) Perturbation Parallelism (PertP), which parallelizes the two perturbed forward passes across devices; (2) Distributed Data Parallelism (DDP), adapted to the scalar-gradient nature of ZO training; and (3) a unified 2D Parallelism design that combines PertP and DDP. To further mitigate communication bottlenecks introduced by parameter offloading, we propose a hardware-aware communication strategy that slices parameter blocks and redistributes them across GPUs via high-speed interconnects such as NVLink. DistZO2 scales zeroth-order fine-tuning to modern multi-GPU systems, preserving ZO2's memory efficiency while substantially improving training throughput. In our experiments on OPT-175B, DistZO2 achieves a 3x speedup over ZO2 with distributed computing. DistZO2's code has been open-sourced in https://github.com/liangyuwang/zo2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03211v1</guid>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liangyu Wang, Huanyi Xie, Di Wang</dc:creator>
    </item>
    <item>
      <title>Dominance or Fair Play in Social Networks? A Model of Influencer Popularity Dynamic</title>
      <link>https://arxiv.org/abs/2507.03448</link>
      <description>arXiv:2507.03448v1 Announce Type: cross 
Abstract: This paper presents a data-driven mean-field approach to model the popularity dynamics of users seeking public attention, i.e., influencers. We propose a novel analytical model that integrates individual activity patterns, expertise in producing viral content, exogenous events, and the platform's role in visibility enhancement, ultimately determining each influencer's success. We analytically derive sufficient conditions for system ergodicity, enabling predictions of popularity distributions. A sensitivity analysis explores various system configurations, highlighting conditions favoring either dominance or fair play among influencers. Our findings offer valuable insights into the potential evolution of social networks towards more equitable or biased influence ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03448v1</guid>
      <category>cs.SI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Franco Galante, Chiara Ravazzi, Luca Vassio, Michele Garetto, Emilio Leonardi</dc:creator>
    </item>
    <item>
      <title>Reconstructing Biological Pathways by Applying Selective Incremental Learning to (Very) Small Language Models</title>
      <link>https://arxiv.org/abs/2507.04432</link>
      <description>arXiv:2507.04432v1 Announce Type: cross 
Abstract: The use of generative artificial intelligence (AI) models is becoming ubiquitous in many fields. Though progress continues to be made, general purpose large language AI models (LLM) show a tendency to deliver creative answers, often called "hallucinations", which have slowed their application in the medical and biomedical fields where accuracy is paramount. We propose that the design and use of much smaller, domain and even task-specific LM may be a more rational and appropriate use of this technology in biomedical research. In this work we apply a very small LM by today's standards to the specialized task of predicting regulatory interactions between molecular components to fill gaps in our current understanding of intracellular pathways. Toward this we attempt to correctly posit known pathway-informed interactions recovered from manually curated pathway databases by selecting and using only the most informative examples as part of an active learning scheme. With this example we show that a small (~110 million parameters) LM based on a Bidirectional Encoder Representations from Transformers (BERT) architecture can propose molecular interactions relevant to tuberculosis persistence and transmission with over 80% accuracy using less than 25% of the ~520 regulatory relationships in question. Using information entropy as a metric for the iterative selection of new tuning examples, we also find that increased accuracy is driven by favoring the use of the incorrectly assigned statements with the highest certainty (lowest entropy). In contrast, the concurrent use of correct but least certain examples contributed little and may have even been detrimental to the learning rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04432v1</guid>
      <category>q-bio.MN</category>
      <category>cs.CL</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <category>math.IT</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pranta Saha, Joyce Reimer, Brook Byrns, Connor Burbridge, Neeraj Dhar, Jeffrey Chen, Steven Rayan, Gordon Broderick</dc:creator>
    </item>
    <item>
      <title>Hardware-efficient tractable probabilistic inference for TinyML Neurosymbolic AI applications</title>
      <link>https://arxiv.org/abs/2507.05141</link>
      <description>arXiv:2507.05141v1 Announce Type: cross 
Abstract: Neurosymbolic AI (NSAI) has recently emerged to mitigate limitations associated with deep learning (DL) models, e.g. quantifying their uncertainty or reason with explicit rules. Hence, TinyML hardware will need to support these symbolic models to bring NSAI to embedded scenarios. Yet, although symbolic models are typically compact, their sparsity and computation resolution contrasts with low-resolution and dense neuro models, which is a challenge on resource-constrained TinyML hardware severely limiting the size of symbolic models that can be computed. In this work, we remove this bottleneck leveraging a tight hardware/software integration to present a complete framework to compute NSAI with TinyML hardware. We focus on symbolic models realized with tractable probabilistic circuits (PCs), a popular subclass of probabilistic models for hardware integration. This framework: (1) trains a specific class of hardware-efficient \emph{deterministic} PCs, chosen for the symbolic task; (2) \emph{compresses} this PC until it can be computed on TinyML hardware with minimal accuracy degradation, using our $n^{th}$-root compression technique, and (3) \emph{deploys} the complete NSAI model on TinyML hardware. Compared to a 64b precision baseline necessary for the PC without compression, our workflow leads to significant hardware reduction on FPGA (up to 82.3\% in FF, 52.6\% in LUTs, and 18.0\% in Flash usage) and an average inference speedup of 4.67x on ESP32 microcontroller.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05141v1</guid>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jelin Leslin, Martin Trapp, Martin Andraud</dc:creator>
    </item>
    <item>
      <title>Denoising Application Performance Models with Noise-Resilient Priors</title>
      <link>https://arxiv.org/abs/2504.10996</link>
      <description>arXiv:2504.10996v2 Announce Type: replace 
Abstract: When scaling parallel codes to larger machines, performance models help identify potential bottlenecks. Since analytically designing these mathematical representations is usually challenging, empirical models based on performance measurements offer a practical alternative. Yet, measurements on HPC systems are typically affected by noise, leading to potentially misleading model predictions. To reduce the influence of noise, we introduce application-specific dynamic priors into the modeling process, which we derive from noise-resilient measurements of computational effort and knowledge of typical algorithms used in communication routines. These priors then narrow the search space for our performance models, excluding complexity classes that reflect noise rather than performance. Our approach keeps the models much closer to theoretical expectations and significantly improves their predictive power. Finally, it cuts experimental costs in half by minimizing the number of repeated measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10996v2</guid>
      <category>cs.PF</category>
      <category>cs.DC</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gustavo de Morais, Alexander Gei{\ss}, Alexandru Calotoiu, Gregor Corbin, Ahmad Tarraf, Torsten Hoefler, Bernd Mohr, Felix Wolf</dc:creator>
    </item>
    <item>
      <title>Hiku: Pull-Based Scheduling for Serverless Computing</title>
      <link>https://arxiv.org/abs/2502.15534</link>
      <description>arXiv:2502.15534v2 Announce Type: replace-cross 
Abstract: Serverless computing promises convenient abstractions for developing and deploying functions that execute in response to events. In such Function-as-a-Service (FaaS) platforms, scheduling is an integral task, but current scheduling algorithms often struggle with maintaining balanced loads, minimizing cold starts, and adapting to commonly occurring bursty workloads. In this work, we propose pull-based scheduling as a novel scheduling algorithm for serverless computing. Our key idea is to decouple worker selection from task assignment, with idle workers requesting new tasks proactively. Experimental evaluation on an open-source FaaS platform shows that pull-based scheduling, compared to other existing scheduling algorithms, significantly improves the performance and load balancing of serverless workloads, especially under high concurrency. The proposed algorithm improves response latencies by 14.9% compared to hash-based scheduling, reduces the frequency of cold starts from 43% to 30%, increases throughput by 8.3%, and achieves a more even load distribution by 12.9% measured by the requests assigned per worker.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15534v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/CCGRID64434.2025.00034</arxiv:DOI>
      <dc:creator>Saman Akbari, Manfred Hauswirth</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Memory Benchmarking Toolkit</title>
      <link>https://arxiv.org/abs/2505.00901</link>
      <description>arXiv:2505.00901v2 Announce Type: replace-cross 
Abstract: This paper presents an open-source kernel-level heterogeneous memory characterization framework (MemScope) for embedded systems. MemScope enables precise characterization of the temporal behavior of available memory modules under configurable contention stress scenarios. MemScope leverages kernel-level control over physical memory allocation, cache maintenance, CPU state, interrupts, and I/O device activity to accurately benchmark heterogeneous memory subsystems. This gives us the privilege to directly map pieces of contiguous physical memory and instantiate allocators, allowing us to finely control cores to create and eliminate interference. Additionally, we can minimize noise and interruptions, guaranteeing more consistent and precise results compared to equivalent user-space solutions. Running our Framework on a Xilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to precisely benchmark bandwidth and latency across various memory types, including PL-side DRAM and BRAM, in a multi-core system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00901v2</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Golsana Ghaemi, Gabriel Franco, Kazem Taram, Renato Mancuso</dc:creator>
    </item>
    <item>
      <title>Fast solvers for Tokamak fluid models with PETSC -- Part I</title>
      <link>https://arxiv.org/abs/2506.16676</link>
      <description>arXiv:2506.16676v3 Announce Type: replace-cross 
Abstract: This work begins the development of fast, scalable solvers for scientific and engineering-relevant magnetohydrodynamics (MHD) models of tokamaks using multigrid methods. These tokamak models are characterized by a distinguished direction in the toroidal coordinate that is partially aligned with the magnetic guide field, which dominates the plasma dynamics. All tokamak models exploit this structure, for example, NIMROD at https://nimrodteam.org uses $2D$, unstructured, high-order finite elements in the poloidal plane with Fourier modes in the toroidal coordinate, and the $3D$, extended MHD code \textit{M3D-C1}\footnote{https://m3dc1.pppl.gov} uses $2D$, unstructured $C^1$ elements in the poloidal plane with cubic Hermite functions in the toroidal direction. This structure suggests addressing the toroidal coordinate first, which \textit{NIMROD} does at the formulation level, but the \textit{M3D-C1} approach leaves in the algebraic system to be solved at each time step in an implicit time integrator. This work addressed the toroidal coordinate in the \textit{M3D-C1} velocity solve by adding semi-coarsening multigrid to the existing PETSC at https://petsc.org -- Portable, Extensible Toolkit for Scientific Computation -- block Jacobi solver, with the addition of little new code that allows for smaller Jacobi subdomains that are better suited to contemporary, highly parallel, hardware. Competitive performance of this new solver configuration is demonstrated on a self-consistent runaway electron model of a SPARC at https://cfs.energy/technology/sparc disruption, and the next steps in the development of this new approach are outlined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16676v3</guid>
      <category>physics.plasm-ph</category>
      <category>cs.PF</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mark F. Adams, Jin Chen, Benjamin Sturdevant</dc:creator>
    </item>
    <item>
      <title>PEVLM: Parallel Encoding for Vision-Language Models</title>
      <link>https://arxiv.org/abs/2506.19651</link>
      <description>arXiv:2506.19651v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) have demonstrated strong capabilities in multimodal understanding and generation tasks. However, their application to long video understanding remains hindered by the quadratic complexity of standard attention mechanisms. In this work, we introduce \textbf{PEVLM}, a fine-tuning-free parallel encoding method designed to enhance the prefilling efficiency of VLMs in long video scenarios. PEVLM partitions the input video into context blocks with a shared sink block, while preserving sequential position embeddings to align the attention weight distribution with that of Full-Attention. This design reduces attention complexity from $O((T \times N)^2)$ to $O(T \times N)$ where $T$ is the number of frames and $N$ the number of tokens per frame, without sacrificing accuracy. Extensive experiments across multiple state-of-the-art models and benchmarks demonstrate that PEVLM consistently outperforms existing parallel encoding approaches, achieving up to \textbf{7.47x} speedup in attention computation and reducing end-to-end latency by \textbf{40\%}. Remarkably, PEVLM not only maintains high accuracy, but in some settings even surpasses Full-Attention performance. Under strict latency constraints, it achieves substantial gains, improving accuracy from \textbf{23.26\%} to \textbf{61.03\%}. These results underscore the effectiveness of PEVLM for low-latency, long-context video understanding, making it a promising solution for real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19651v2</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Letian Kang, Shixian Luo, Yiqiang Li, Xiaoyang Yu, Shenxuan Zhou, Yong Wu</dc:creator>
    </item>
  </channel>
</rss>
