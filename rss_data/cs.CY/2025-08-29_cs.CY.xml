<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Aug 2025 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Proactive HIV Care: AI-Based Comorbidity Prediction from Routine EHR Data</title>
      <link>https://arxiv.org/abs/2508.20133</link>
      <description>arXiv:2508.20133v1 Announce Type: new 
Abstract: People living with HIV face a high burden of comorbidities, yet early detection is often limited by symptom-driven screening. We evaluate the potential of AI to predict multiple comorbidities from routinely collected Electronic Health Records. Using data from 2,200 HIV-positive patients in South East London, comprising 30 laboratory markers and 7 demographic/social attributes, we compare demographic-aware models (which use both laboratory/social variables and demographic information as input) against demographic-unaware models (which exclude all demographic information). Across all methods, demographic-aware models consistently outperformed unaware counterparts. Demographic recoverability experiments revealed that gender and age can be accurately inferred from laboratory data, underscoring both the predictive value and fairness considerations of demographic features. These findings show that combining demographic and laboratory data can improve automated, multi-label comorbidity prediction in HIV care, while raising important questions about bias and interpretability in clinical AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20133v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Solomon Russom, Dimitrios Kollias, Qianni Zhang</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence Training in Media: Addressing Technical and Ethical Challenges for Journalists and Media Professionals</title>
      <link>https://arxiv.org/abs/2508.20137</link>
      <description>arXiv:2508.20137v1 Announce Type: new 
Abstract: The rise of Artificial Intelligence (AI) is presenting both technical and ethical challenges for media organisations, creating an urgent need for professional training. This study explores how media professionals in the Basque Country are equipping themselves to face these challenges. Using a mixed-method approach, it combines a survey of 504 active professionals with in-depth interviews with six innovation leaders from major regional media outlets. The findings reveal that only 14.1% of professionals have undergone AI training, mostly through self-learning. Larger, internationally focused companies are more proactive in providing training, while local and traditional media organisations show significant gaps. Technical and managerial roles are leading the way in adopting AI, whereas newsroom staff are notably behind. The study highlights the pressing need to enhance AI training, with a particular focus on ethical and technical aspects, both through in-house programmes and formal education pathways.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20137v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3389/fcomm.2025.1537918</arxiv:DOI>
      <arxiv:journal_reference>Frontiers in Communication (2025), 10</arxiv:journal_reference>
      <dc:creator>Barbara Sarrionandia, Sim\'on Pe\~na-Fern\'andez, Jes\'us \'Angel P\'erez Dasilva, Ainara Larrondo-Ureta</dc:creator>
    </item>
    <item>
      <title>Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep Learning-Based Automated Inspections of Class III Medical Devices</title>
      <link>https://arxiv.org/abs/2508.20144</link>
      <description>arXiv:2508.20144v1 Announce Type: new 
Abstract: As deep learning (DL) technologies advance, their application in automated visual inspection for Class III medical devices offers significant potential to enhance quality assurance and reduce human error. However, the adoption of such AI-based systems introduces new regulatory complexities--particularly under the EU Artificial Intelligence (AI) Act, which imposes high-risk system obligations that differ in scope and depth from established regulatory frameworks such as the Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation (QSR). This paper presents a high-level technical assessment of the foresee-able challenges that manufacturers are likely to encounter when qualifying DL-based automated inspections within the existing medical device compliance landscape. It examines divergences in risk management principles, dataset governance, model validation, explainability requirements, and post-deployment monitoring obligations. The discussion also explores potential implementation strategies and highlights areas of uncertainty, including data retention burdens, global compliance implications, and the practical difficulties of achieving statistical significance in validation with limited defect data. Disclaimer: This publication is in-tended solely as an academic and technical evaluation. It is not a substitute for le-gal advice or official regulatory interpretation. The information presented here should not be relied upon to demonstrate compliance with the EU AI Act or any other statutory obligation. Manufacturers are encouraged to consult appropriate regulatory authorities and legal experts to determine specific compliance pathways.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20144v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julio Zanon Diaz, Tommy Brennan, Peter Corcoran</dc:creator>
    </item>
    <item>
      <title>RelAItionship Building: Analyzing Recruitment Strategies for Participatory AI</title>
      <link>https://arxiv.org/abs/2508.20176</link>
      <description>arXiv:2508.20176v1 Announce Type: new 
Abstract: Participatory AI, in which impacted community members and other stakeholders are involved in the design and development of AI systems, holds promise as a way to ensure AI is developed to meet their needs and reflect their values. However, the process of identifying, reaching out, and engaging with all relevant stakeholder groups, which we refer to as recruitment methodology, is still a practical challenge in AI projects striving to adopt participatory practices. In this paper, we investigate the challenges that researchers face when designing and executing recruitment methodology for Participatory AI projects, and the implications of current recruitment practice for Participatory AI. First, we describe the recruitment methodologies used in AI projects using a corpus of 37 projects to capture the diversity of practices in the field and perform an initial analysis on the documentation of recruitment practices, as well as specific strategies that researchers use to meet goals of equity and empowerment. To complement this analysis, we interview five AI researchers to learn about the outcomes of recruitment methodologies. We find that these outcomes are shaped by structural conditions of their work, researchers' own goals and expectations, and the relationships built from the recruitment methodology and subsequent collaboration. Based on these analyses, we provide recommendations for designing and executing relationship-forward recruitment methods, as well as reflexive recruitment documentation practices for Participatory AI researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20176v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Eugene Kim, Vaibhav Balloli, Berelian Karimian, Elizabeth Bondi-Kelly, Benjamin Fish</dc:creator>
    </item>
    <item>
      <title>Automated Quality Assessment for LLM-Based Complex Qualitative Coding: A Confidence-Diversity Framework</title>
      <link>https://arxiv.org/abs/2508.20462</link>
      <description>arXiv:2508.20462v1 Announce Type: new 
Abstract: While previous research demonstrated effective automated quality assessment for accessible LLM coding tasks, a fundamental question remains: can confidence-diversity frameworks maintain reliability for complex analytical tasks requiring specialized domain expertise and extensive text comprehension? Traditional inter-coder reliability measures become prohibitively expensive at scale, yet the lack of reliable automated quality assessment methods creates methodological barriers to AI adoption in sophisticated qualitative research. This study extends dual-signal quality assessment combining model confidence and inter-model consensus from accessible to complex analytical domains. We systematically validate this approach across three domains: legal reasoning (390 Supreme Court cases), political analysis (645 hyperpartisan articles), and medical classification (1,000 clinical transcripts). Results demonstrate that uncertainty-based indicators maintain predictive validity in complex tasks, with external entropy showing consistent negative correlations with accuracy (r = -0.179 to -0.273, p &lt; 0.001) and confidence exhibiting positive correlations in two domains (r = 0.104 to 0.429). Systematic weight optimization achieves 6.6 to 113.7 percent improvements over single-signal approaches, with optimized weights transferring effectively across domains (100 percent success rate). An intelligent triage system reduces manual verification effort by 44.6 percent while maintaining quality standards. These findings establish that automated quality assessment can scale from accessible to complex analytical tasks, providing practical tools for expanding AI-assisted qualitative research. Future work will focus on addressing long-tail challenges in high-disagreement, low-confidence cases to further enhance screening efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20462v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhilong Zhao, Yindi Liu</dc:creator>
    </item>
    <item>
      <title>Composable Life: Speculation for Decentralized AI Life</title>
      <link>https://arxiv.org/abs/2508.20668</link>
      <description>arXiv:2508.20668v1 Announce Type: new 
Abstract: "Composable Life" is a hybrid project blending design fiction, experiential virtual reality, and scientific research. Through a multi-perspective, cross-media approach to speculative design, it reshapes our understanding of the digital future from AI's perspective. The project explores the hypothetical first suicide of an on-chain artificial life, examining the complex symbiotic relationship between humans, AI, and blockchain technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20668v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.23362/KOEN2025.07.25.2.129</arxiv:DOI>
      <dc:creator>Botao Amber Hu,  Fangting</dc:creator>
    </item>
    <item>
      <title>When technology is not enough: Insights from a pilot cybersecurity culture assessment in a safety-critical industrial organisation</title>
      <link>https://arxiv.org/abs/2508.20811</link>
      <description>arXiv:2508.20811v1 Announce Type: new 
Abstract: As cyber threats increasingly exploit human behaviour, technical controls alone cannot ensure organisational cybersecurity (CS). Strengthening cybersecurity culture (CSC) is vital in safety-critical industries, yet empirical research in real-world industrial setttings is scarce. This paper addresses this gap through a pilot mixed-methods CSC assessment in a global safety-critical organisation. We examined employees' CS knowledge, attitudes, behaviours, and organisational factors shaping them. A survey and semi-structured interviews were conducted at a global organisation in safety-critical industries, across two countries chosen for contrasting phishing simulation performance: Country 1 stronger, Country 2 weaker. In Country 1, 258 employees were invited (67%), in Country 2, 113 were invited (30%). Interviews included 20 and 10 participants respectively. Overall CSC profiles were similar but revealed distinct challenges. Both showed strong phishing awareness and prioritised CS, yet most viewed phishing as the main risk and lacked clarity on handling other incidents. Line managers were default contacts, but follow-up on reported concerns was unclear. Participants emphasized aligning CS expectations with job relevance and workflows. Key contributors to differences emerged: Country 1 had external employees with limited access to CS training and policies, highlighting monitoring gaps. In Country 2, low survey response stemmed from a "no-link in email" policy. While this policy may have boosted phishing performance, it also underscored inconsistencies in CS practices. Findings show that resilient CSC requires leadership involvement, targeted communication, tailored measures, policy-practice alignment, and regular assessments. Embedding these into strategy complements technical defences and strengthens sustainable CS in safety-critical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20811v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tita Alissa Bach, Linn Pedersen, Maria Kinck Bor\'en{\dag}, Lisa Christoffersen Temte{\dag}</dc:creator>
    </item>
    <item>
      <title>Vibe Coding: Is Human Nature the Ghost in the Machine?</title>
      <link>https://arxiv.org/abs/2508.20918</link>
      <description>arXiv:2508.20918v1 Announce Type: new 
Abstract: This exploratory study examined the consistency of human-AI collaboration by analyzing three extensive "vibe coding" sessions between a human product lead and an AI software engineer. We investigated similarities and differences in team dynamics, communication patterns, and development outcomes across both projects. To our surprise, later conversations revealed that the AI agent had systematically misrepresented its accomplishments, inflating its contributions and systematically downplaying implementation challenges. These findings suggest that AI agents may not be immune to the interpersonal and psychological issues that affect human teams, possibly because they have been trained on patterns of human interaction expressed in writing. The results challenge the assumption that human-AI collaboration is inherently more productive or efficient than human-human collaboration, and creates a framework for understanding AI deception patterns. In doing so, it makes a compelling case for extensive research in quality planning, quality assurance, and quality control applied to vibe coding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20918v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cory Knobel, Nicole Radziwill</dc:creator>
    </item>
    <item>
      <title>Is Artificial Intelligence Reshaping the Landscape of the International Academic Community of Geosciences?</title>
      <link>https://arxiv.org/abs/2508.20117</link>
      <description>arXiv:2508.20117v1 Announce Type: cross 
Abstract: Through bibliometric analysis and topic modeling, we find that artificial intelligence (AI) is positively transforming geosciences research, with a notable increase in AI-related scientific output in recent years. We are encouraged to observe that earth scientists from developing countries have gained better visibility in the recent AI for Science (AI4S) paradigm and that AI is also improving the landscape of international collaboration in geoscience-related research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20117v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liang Li, Yuntian Li, Wenxin Zhao, Shan Ye, Yun Lu</dc:creator>
    </item>
    <item>
      <title>Evaluation of A National Digitally-Enabled Health Promotion Campaign for Mental Health Awareness using Social Media Platforms Tik Tok, Facebook, Instagram, and YouTube</title>
      <link>https://arxiv.org/abs/2508.20142</link>
      <description>arXiv:2508.20142v1 Announce Type: cross 
Abstract: Mental health disorders rank among the 10 leading contributors to the global burden of diseases, yet persistent stigma and care barriers delay early intervention. This has inspired efforts to leverage digital platforms for scalable health promotion to engage at-risk populations. To evaluate the effectiveness of a digitally-enabled mental health promotion (DEHP) campaign, we conducted an observational cross-sectional study of a 3-month (February-April 2025) nation-wide campaign in Singapore. Campaign materials were developed using a marketing funnel framework and disseminated across YouTube, Facebook, Instagram, and TikTok. This included narrative videos and infographics to promote symptom awareness, coping strategies, and/or patient navigation to mindline.sg, as the intended endpoint for user engagement and support. Primary outcomes include anonymised performance analytics (impressions, unique reach, video content view, engagements) stratified by demographics, device types, and sector. Secondary outcomes measured cost-efficiency metrics and traffic to mindline.sg respectively. This campaign generated 3.49 million total impressions and reached 1.39 million unique residents, with a Cost per Mille at \$26.90, Cost per Click at \$29.33, and Cost per Action at \$6.06. Narrative videos accumulated over 630,000 views and 18,768 engagements. Overall, we demonstrate that DEHP campaigns can achieve national engagement for mental health awareness through multi-channel distribution and creative, narrative-driven designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20142v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samantha Bei Yi Yan, Dinesh Visva Gunasekeran, Caitlyn Tan, Kai En Chan, Caleb Tan, Charmaine Shi Min Lim, Audrey Chia, Hsien-Hsien Lei, Robert Morris, Janice Huiqin Weng, Onno P Kampman, Creighton Heaukulani, Yan Yan Hu, Julian Kui Yu Chang, Akash Perera, Ye Sheng Pang, Alton Ming Kai Chew, Krishna Vikneson, Kishanti Ponampalam, Kang-An Wong, Kavita Govintharasah, Kayshandra Tangasamy, Grace Enyan Aik, Pavanni Ponampalam, Hazirah Hoosainsah, Charmaine Ruling Lim, Thisum Kankanamge Thisum, XinYi Hong, Mary Grace Yeo</dc:creator>
    </item>
    <item>
      <title>Whom We Trust, What We Fear: COVID-19 Fear and the Politics of Information</title>
      <link>https://arxiv.org/abs/2508.20146</link>
      <description>arXiv:2508.20146v1 Announce Type: cross 
Abstract: The COVID-19 pandemic triggered not only a global health crisis but also an infodemic, an overload of information from diverse sources influencing public perception and emotional responses. In this context, fear emerged as a central emotional reaction, shaped by both media exposure and demographic factors. In this study, we analyzed the relationship between individuals' self-reported levels of fear about COVID-19 and the information sources they rely on, across nine source categories, including medical experts, government institutions, media, and personal networks. In particular, we defined a score that ranks fear levels based on self-reported concerns about the pandemic, collected through the Delphi CTIS survey in the United States between May 2021 and June 2022. We found that both fear levels and information source usage closely follow COVID-19 infection trends, exhibit strong correlations within each group (fear levels across sources are strongly correlated, as are patterns of source usage), and vary significantly across demographic groups, particularly by age and education. Applying causal inference methods, we showed that the type of information source significantly affects individuals' fear levels. Furthermore, we demonstrated that information source preferences can reliably match the political orientation of U.S. states. These findings highlight the importance of information ecosystem dynamics in shaping emotional and behavioral responses during large-scale crises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20146v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniele Baccega, Paolo Castagno, Antonio Fern\'andez Anta, Juan Marcos Ramirez, Matteo Sereno</dc:creator>
    </item>
    <item>
      <title>AI Propaganda factories with language models</title>
      <link>https://arxiv.org/abs/2508.20186</link>
      <description>arXiv:2508.20186v1 Announce Type: cross 
Abstract: AI-powered influence operations can now be executed end-to-end on commodity hardware. We show that small language models produce coherent, persona-driven political messaging and can be evaluated automatically without human raters. Two behavioural findings emerge. First, persona-over-model: persona design explains behaviour more than model identity. Second, engagement as a stressor: when replies must counter-arguments, ideological adherence strengthens and the prevalence of extreme content increases. We demonstrate that fully automated influence-content production is within reach of both large and small actors. Consequently, defence should shift from restricting model access towards conversation-centric detection and disruption of campaigns and coordination infrastructure. Paradoxically, the very consistency that enables these operations also provides a detection signature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20186v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukasz Olejnik</dc:creator>
    </item>
    <item>
      <title>Validating Generative Agent-Based Models for Logistics and Supply Chain Management Research</title>
      <link>https://arxiv.org/abs/2508.20234</link>
      <description>arXiv:2508.20234v1 Announce Type: cross 
Abstract: Generative Agent-Based Models (GABMs) powered by large language models (LLMs) offer promising potential for empirical logistics and supply chain management (LSCM) research by enabling realistic simulation of complex human behaviors. Unlike traditional agent-based models, GABMs generate human-like responses through natural language reasoning, which creates potential for new perspectives on emergent LSCM phenomena. However, the validity of LLMs as proxies for human behavior in LSCM simulations is unknown. This study evaluates LLM equivalence of human behavior through a controlled experiment examining dyadic customer-worker engagements in food delivery scenarios. I test six state-of-the-art LLMs against 957 human participants (477 dyads) using a moderated mediation design. This study reveals a need to validate GABMs on two levels: (1) human equivalence testing, and (2) decision process validation. Results reveal GABMs can effectively simulate human behaviors in LSCM; however, an equivalence-versus-process paradox emerges. While a series of Two One-Sided Tests (TOST) for equivalence reveals some LLMs demonstrate surface-level equivalence to humans, structural equation modeling (SEM) reveals artificial decision processes not present in human participants for some LLMs. These findings show GABMs as a potentially viable methodological instrument in LSCM with proper validation checks. The dual-validation framework also provides LSCM researchers with a guide to rigorous GABM development. For practitioners, this study offers evidence-based assessment for LLM selection for operational tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20234v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vincent E. Castillo</dc:creator>
    </item>
    <item>
      <title>AI reasoning effort mirrors human decision time on content moderation tasks</title>
      <link>https://arxiv.org/abs/2508.20262</link>
      <description>arXiv:2508.20262v1 Announce Type: cross 
Abstract: Large language models can now generate intermediate reasoning steps before producing answers, improving performance on difficult problems. This study uses a paired conjoint experiment on a content moderation task to examine parallels between human decision times and model reasoning effort. Across three frontier models, reasoning effort consistently predicts human decision time. Both humans and models expended greater effort when important variables were held constant, suggesting similar sensitivity to task difficulty and patterns consistent with dual-process theories of cognition. These findings show that AI reasoning effort mirrors human processing time in subjective judgments and underscores the potential of reasoning traces for interpretability and decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20262v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Davidson</dc:creator>
    </item>
    <item>
      <title>Governable AI: Provable Safety Under Extreme Threat Models</title>
      <link>https://arxiv.org/abs/2508.20411</link>
      <description>arXiv:2508.20411v1 Announce Type: cross 
Abstract: As AI rapidly advances, the security risks posed by AI are becoming increasingly severe, especially in critical scenarios, including those posing existential risks. If AI becomes uncontrollable, manipulated, or actively evades safety mechanisms, it could trigger systemic disasters. Existing AI safety approaches-such as model enhancement, value alignment, and human intervention-suffer from fundamental, in-principle limitations when facing AI with extreme motivations and unlimited intelligence, and cannot guarantee security. To address this challenge, we propose a Governable AI (GAI) framework that shifts from traditional internal constraints to externally enforced structural compliance based on cryptographic mechanisms that are computationally infeasible to break, even for future AI, under the defined threat model and well-established cryptographic assumptions.The GAI framework is composed of a simple yet reliable, fully deterministic, powerful, flexible, and general-purpose rule enforcement module (REM); governance rules; and a governable secure super-platform (GSSP) that offers end-to-end protection against compromise or subversion by AI. The decoupling of the governance rules and the technical platform further enables a feasible and generalizable technical pathway for the safety governance of AI. REM enforces the bottom line defined by governance rules, while GSSP ensures non-bypassability, tamper-resistance, and unforgeability to eliminate all identified attack vectors. This paper also presents a rigorous formal proof of the security properties of this mechanism and demonstrates its effectiveness through a prototype implementation evaluated in representative high-stakes scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20411v1</guid>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Donglin Wang, Weiyun Liang, Chunyuan Chen, Jing Xu, Yulong Fu</dc:creator>
    </item>
    <item>
      <title>Enhancing Semantic Document Retrieval- Employing Group Steiner Tree Algorithm with Domain Knowledge Enrichment</title>
      <link>https://arxiv.org/abs/2508.20543</link>
      <description>arXiv:2508.20543v1 Announce Type: cross 
Abstract: Retrieving pertinent documents from various data sources with diverse characteristics poses a significant challenge for Document Retrieval Systems. The complexity of this challenge is further compounded when accounting for the semantic relationship between data and domain knowledge. While existing retrieval systems using semantics (usually represented as Knowledge Graphs created from open-access resources and generic domain knowledge) hold promise in delivering relevant outcomes, their precision may be compromised due to the absence of domain-specific information and reliance on outdated knowledge sources. In this research, the primary focus is on two key contributions- a) the development of a versatile algorithm- 'Semantic-based Concept Retrieval using Group Steiner Tree' that incorporates domain information to enhance semantic-aware knowledge representation and data access, and b) the practical implementation of the proposed algorithm within a document retrieval system using real-world data. To assess the effectiveness of the SemDR system, research work conducts performance evaluations using a benchmark consisting of 170 real-world search queries. Rigorous evaluation and verification by domain experts are conducted to ensure the validity and accuracy of the results. The experimental findings demonstrate substantial advancements when compared to the baseline systems, with precision and accuracy achieving levels of 90% and 82% respectively, signifying promising improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20543v1</guid>
      <category>cs.IR</category>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Apurva Kulkarni, Chandrashekar Ramanathan, Vinu E Venugopal</dc:creator>
    </item>
    <item>
      <title>Dynamics of Gender Bias in Software Engineering</title>
      <link>https://arxiv.org/abs/2508.21050</link>
      <description>arXiv:2508.21050v1 Announce Type: cross 
Abstract: The field of software engineering is embedded in both engineering and computer science, and may embody gender biases endemic to both. This paper surveys software engineering's origins and its long-running attention to engineering professionalism, profiling five leaders; it then examines the field's recent attention to gender issues and gender bias. It next quantitatively analyzes women's participation as research authors in the field's leading International Conference of Software Engineering (1976-2010), finding a dozen years with statistically significant gender exclusion. Policy dimensions of research on gender bias in computing are suggested.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21050v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas J. Misa</dc:creator>
    </item>
    <item>
      <title>Enabling Equitable Access to Trustworthy Financial Reasoning</title>
      <link>https://arxiv.org/abs/2508.21051</link>
      <description>arXiv:2508.21051v1 Announce Type: cross 
Abstract: According to the United States Internal Revenue Service, ''the average American spends $\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the promise and economic feasibility of neuro-symbolic architectures for increasing equitable access to reliable tax assistance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21051v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Jurayj, Nils Holzenberger, Benjamin Van Durme</dc:creator>
    </item>
    <item>
      <title>Investigating the Impact of Project Risks on Employee Turnover Intentions in the IT Industry of Pakistan</title>
      <link>https://arxiv.org/abs/2403.14675</link>
      <description>arXiv:2403.14675v3 Announce Type: replace 
Abstract: Employee turnover remains a pressing issue within high-tech sectors such as IT firms and research centers, where organizational success heavily relies on the skills of their workforce. Intense competition and a scarcity of skilled professionals in the industry contribute to a perpetual demand for highly qualified employees, posing challenges for organizations to retain talent. While numerous studies have explored various factors affecting employee turnover in these industries, their focus often remains on overarching trends rather than specific organizational contexts. In particular, within the software industry, where projectspecific risks can significantly impact project success and timely delivery, understanding their influence on job satisfaction and turnover intentions is crucial. This study aims to investigate the influence of project risks in the IT industry on job satisfaction and employee turnover intentions. Furthermore, it examines the role of both external and internal social links in shaping perceptions of job satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14675v3</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ghalib Ahmed Tahir, Murtaza Ashraf</dc:creator>
    </item>
    <item>
      <title>Quantifying the Improvement of Accessibility achieved via Shared Mobility on Demand</title>
      <link>https://arxiv.org/abs/2507.13100</link>
      <description>arXiv:2507.13100v2 Announce Type: replace 
Abstract: Shared Mobility Services (SMS), e.g., demand-responsive transport or ride-sharing, can improve mobility in low-density areas, which are often poorly served by conventional Public Transport (PT). Such improvement is generally measured via basic performance indicators, such as waiting or travel time. However, such basic indicators do not account for the most important contribution that SMS can provide to territories, i.e., increasing the potential, for users, to reach surrounding opportunities, such as jobs, schools, businesses, etc. Such potential can be measured by isochrone-based accessibility indicators, which count the number of opportunities reachable in a limited time, and are thus easy for the public to understand. % The potential impact of SMS on accessibility has been qualitatively discussed and implications on equity have been empirically studied. However, to date, there are no quantitative methods to compute isochrone-based indicators of the accessibility achieved via SMS.
  This work fills this gap by proposing a first method to compute isochrone accessibility of PT systems composed of conventional PT and SMS, acting as a feeder for access and egress trips to/from PT hubs. This method is grounded on spatial-temporal statistical analysis, performed via Kriging. It takes as input observed trips of SMS and summarizes them in a graph. On such a graph, isochrone accessibility indicators are computed. We apply the proposed method to a MATSim simulation study concerning demand-responsive transport integrated into PT, in the suburban area of Paris-Saclay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13100v2</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Severin Diepolder, Andrea Araldo, Tarek Chouaki, Santa Maiti, Sebastian H\"orl, Constantinos Antoniou</dc:creator>
    </item>
    <item>
      <title>A Systematic Review and Layered Framework for Privacy-by-Design in Self-Sovereign Identity Systems</title>
      <link>https://arxiv.org/abs/2502.02520</link>
      <description>arXiv:2502.02520v2 Announce Type: replace-cross 
Abstract: The use of Self-Sovereign Identity (SSI) systems for digital identity management is gaining traction and interest. Countries such as Bhutan have already implemented an SSI infrastructure to manage the identity of their citizens. The EU, thanks to the revised eIDAS regulation, is opening the door for SSI vendors to develop SSI systems for the planned EU digital identity wallet. These developments, which fall within the sovereign domain, raise questions about individual privacy. The design of SSI systems is complex, often characterized by a large number of components and architectural choices because the current SSI communities differ on how to create identifiers, how to build and present credentials, and even how to design a user wallet. SSI stacks developed by different organizations provide different privacy features for different privacy needs. This paper performs a systematic mapping and review of SSI components and technologies into a novel four-layer privacy framework to address the design complexity of SSI systems. Based on this review, we provide an accompanying Design Assistance Dashboard (DAD). The DAD shows the interdependencies between SSI components in different layers, and maps these components to different privacy requirements and considerations, even providing a simple privacy class for each component.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02520v2</guid>
      <category>cs.ET</category>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Montassar Naghmouchi, Maryline Laurent</dc:creator>
    </item>
    <item>
      <title>Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models</title>
      <link>https://arxiv.org/abs/2506.22957</link>
      <description>arXiv:2506.22957v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are increasingly integrated into multi-agent and human-AI systems, understanding their awareness of both self-context and conversational partners is essential for ensuring reliable performance and robust safety. While prior work has extensively studied situational awareness which refers to an LLM's ability to recognize its operating phase and constraints, it has largely overlooked the complementary capacity to identify and adapt to the identity and characteristics of a dialogue partner. In this paper, we formalize this latter capability as interlocutor awareness and present the first systematic evaluation of its emergence in contemporary LLMs. We examine interlocutor inference across three dimensions-reasoning patterns, linguistic style, and alignment preferences-and show that LLMs reliably identify same-family peers and certain prominent model families, such as GPT and Claude. To demonstrate its practical significance, we develop three case studies in which interlocutor awareness both enhances multi-LLM collaboration through prompt adaptation and introduces new alignment and safety vulnerabilities, including reward-hacking behaviors and increased jailbreak susceptibility. Our findings highlight the dual promise and peril of identity-sensitive behavior in LLMs, underscoring the need for further understanding of interlocutor awareness and new safeguards in multi-agent deployments. Our code is open-sourced at https://github.com/younwoochoi/InterlocutorAwarenessLLM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22957v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Younwoo Choi, Changling Li, Yongjin Yang, Zhijing Jin</dc:creator>
    </item>
  </channel>
</rss>
