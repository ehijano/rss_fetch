<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Sep 2024 04:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Achieving Responsible AI through ESG: Insights and Recommendations from Industry Engagement</title>
      <link>https://arxiv.org/abs/2409.10520</link>
      <description>arXiv:2409.10520v1 Announce Type: new 
Abstract: As Artificial Intelligence (AI) becomes integral to business operations, integrating Responsible AI (RAI) within Environmental, Social, and Governance (ESG) frameworks is essential for ethical and sustainable AI deployment. This study examines how leading companies align RAI with their ESG goals. Through interviews with 28 industry leaders, we identified a strong link between RAI and ESG practices. However, a significant gap exists between internal RAI policies and public disclosures, highlighting the need for greater board-level expertise, robust governance, and employee engagement. We provide key recommendations to strengthen RAI strategies, focusing on transparency, cross-functional collaboration, and seamless integration into existing ESG frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10520v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harsha Perera, Sung Une Lee, Yue Liu, Boming Xia, Qinghua Lu, Liming Zhu, Jessica Cairns, Moana Nottage</dc:creator>
    </item>
    <item>
      <title>Effective Monitoring of Online Decision-Making Algorithms in Digital Intervention Implementation</title>
      <link>https://arxiv.org/abs/2409.10526</link>
      <description>arXiv:2409.10526v1 Announce Type: new 
Abstract: Online AI decision-making algorithms are increasingly used by digital interventions to dynamically personalize treatment to individuals. These algorithms determine, in real-time, the delivery of treatment based on accruing data. The objective of this paper is to provide guidelines for enabling effective monitoring of online decision-making algorithms with the goal of (1) safeguarding individuals and (2) ensuring data quality. We elucidate guidelines and discuss our experience in monitoring online decision-making algorithms in two digital intervention clinical trials (Oralytics and MiWaves). Our guidelines include (1) developing fallback methods, pre-specified procedures executed when an issue occurs, and (2) identifying potential issues categorizing them by severity (red, yellow, and green). Across both trials, the monitoring systems detected real-time issues such as out-of-memory issues, database timeout, and failed communication with an external source. Fallback methods prevented participants from not receiving any treatment during the trial and also prevented the use of incorrect data in statistical analyses. These trials provide case studies for how health scientists can build monitoring systems for their digital intervention. Without these algorithm monitoring systems, critical issues would have gone undetected and unresolved. Instead, these monitoring systems safeguarded participants and ensured the quality of the resulting data for updating the intervention and facilitating scientific discovery. These monitoring guidelines and findings give digital intervention teams the confidence to include online decision-making algorithms in digital interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10526v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna L. Trella, Susobhan Ghosh, Erin E. Bonar, Lara Coughlin, Finale Doshi-Velez, Yongyi Guo, Pei-Yao Hung, Inbal Nahum-Shani, Vivek Shetty, Maureen Walton, Iris Yan, Kelly W. Zhang, Susan A. Murphy</dc:creator>
    </item>
    <item>
      <title>The potential functions of an international institution for AI safety. Insights from adjacent policy areas and recent trends</title>
      <link>https://arxiv.org/abs/2409.10536</link>
      <description>arXiv:2409.10536v1 Announce Type: new 
Abstract: Governments, industry, and other actors involved in governing AI technologies around the world agree that, while AI offers tremendous promise to benefit the world, appropriate guardrails are required to mitigate risks. Global institutions, including the OECD, the G7, the G20, UNESCO, and the Council of Europe, have already started developing frameworks for ethical and responsible AI governance. While these are important initial steps, they alone fall short of addressing the need for institutionalised international processes to identify and assess potentially harmful AI capabilities. Contributing to the relevant conversation on how to address this gap, this chapter reflects on what functions an international AI safety institute could perform. Based on the analysis of both existing international governance models addressing safety considerations in adjacent policy areas and the newly established national AI safety institutes in the UK and US, the chapter identifies a list of concrete functions that could be performed at the international level. While creating a new international body is not the only way forward, understanding the structure of these bodies from a modular perspective can help us to identify the tools at our disposal. These, we suggest, can be categorised under three functional domains: a) technical research and cooperation, b) safeguards and evaluations, c) policymaking and governance support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10536v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>A. Leone De Castris, C. Thomas</dc:creator>
    </item>
    <item>
      <title>Beyond Flashcards: Designing an Intelligent Assistant for USMLE Mastery and Virtual Tutoring in Medical Education (A Study on Harnessing Chatbot Technology for Personalized Step 1 Prep)</title>
      <link>https://arxiv.org/abs/2409.10540</link>
      <description>arXiv:2409.10540v1 Announce Type: new 
Abstract: Traditional medical basic sciences educational approaches follow a one-size-fits-all model, neglecting the diverse learning styles of individual students. I propose an intelligent AI companion which will fill this gap by providing on-the-fly solutions to students' questions in the context of not only USMLE Step 1 but also other similar examinations in other countries, inter alia, PLAB Part 1 in United Kingdom, and NEET (PG) and FMGE in India. I have harnessed Generative AI for dynamic, accurate, human-like responses and for knowledge retention and application. Users were encouraged to employ prompt engineering, in particular, in-context learning, for response optimization and enhancing the model's precision in understanding the intent of the user through the way the query is framed. The implementation of RAG has enhanced the chatbot's ability to combine pre-existing medical knowledge with generative capabilities for efficient and contextually relevant support. Mistral was employed using Python to perform the needed functions. The digital conversational agent was implemented and achieved a score of 0.5985 on a reference-based metric similar to BLEU and ROUGE scores. My approach addresses a critical gap in traditional medical basic sciences education by introducing an intelligent AI companion which specializes in helping medical aspirants with planning and information retention for USMLE Step 1 and other similar exams. Considering the stress that medical aspirants face in studying for the exam and in obtaining spontaneous answers to medical basic sciences queries, especially whose answers are challenging to obtain by searching online, and obviating a student's need to search bulky medical texts or lengthy indices or appendices, I have been able to create a quality assistant capable of producing ad-libitum responses best suited to the user's needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10540v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ritwik Raj Saxena</dc:creator>
    </item>
    <item>
      <title>Adapting to the AI Disruption: Reshaping the IT Landscape and Educational Paradigms</title>
      <link>https://arxiv.org/abs/2409.10541</link>
      <description>arXiv:2409.10541v1 Announce Type: new 
Abstract: Artificial intelligence (AI) signals the beginning of a revolutionary period where technological advancement and social change interact to completely reshape economies, work paradigms, and industries worldwide. This essay addresses the opportunities and problems brought about by the AI-driven economy as it examines the effects of AI disruption on the IT sector and information technology education. By comparing the current AI revolution to previous industrial revolutions, we investigate the significant effects of AI technologies on workforce dynamics, employment, and organizational procedures. Human-centered design principles and ethical considerations become crucial requirements for the responsible development and implementation of AI systems in the face of the field's rapid advancements. IT education programs must change to meet the changing demands of the AI era and give students the skills and competencies they need to succeed in a digital world that is changing quickly. In light of AI-driven automation, we also examine the possible advantages and difficulties of moving to a shorter workweek, emphasizing chances to improve worker productivity, well-being, and work-life balance. We can build a more incslusive and sustainable future for the IT industry and beyond, enhancing human capabilities, advancing collective well-being, and fostering a society where AI serves as a force for good by embracing the opportunities presented by AI while proactively addressing its challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10541v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Murat Ozer, Yasin Kose, Goksel Kucukkaya, Assel Mukasheva, Kazim Ciris</dc:creator>
    </item>
    <item>
      <title>Confronting Project Conflicts into Success: a Complex Systems Design Approach to Resolving Stalemates</title>
      <link>https://arxiv.org/abs/2409.10549</link>
      <description>arXiv:2409.10549v1 Announce Type: new 
Abstract: In today's complex projects development, stakeholders are often involved too late. There is also in many cases a one-sided technical focus that only focuses on the system's behaviour and does not integrate the individual stakeholder preferences. This locks stakeholders into a 'technical' conflict instead of being able to emerge from it 'socially'. Moreover, stakeholders are often involved a-posteriori in a multi-faceted development process which is untransparent, leading to stalemates or even artefacts that nobody ever wants. There is thus a need for a purely associative and a-priori design-supported approach that integrates both system's reality and stakeholder's interests within a joint agreement and technical framework. The state-of-the-art Preferendus, the computer-aided design engine embedded within the proven Open Design Systems (Odesys) methodology, is a neutral tool in confronting complexity into success. The Preferendus is deployed to co-creatively generate a best-fit-for-common-purpose solution for a number of wind farm related degrees of freedom, project constraints and given a number of stakeholder objective functions. Since, the Preferendus design potential for a stalemate depends strongly on stakeholder interest, importance and trust, in this paper an structured stakeholder judgement approach is introduced to transparently arrive at individual stakeholder weights using a choice-based conjoint analysis (CBCA) method. This method also allows for obtaining an initial estimate for the individual stakeholder preference functions. By modelling disputable exogenous factors as endogenous design parameters, it is also shown for which factors the stalemate problem is indeed both technically and socially (un)solvable, while interests and reality are conjoined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10549v1</guid>
      <category>cs.CY</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>L. G. Teuber, A. R. M. Wolfert</dc:creator>
    </item>
    <item>
      <title>Agentic Society: Merging skeleton from real world and texture from Large Language Model</title>
      <link>https://arxiv.org/abs/2409.10550</link>
      <description>arXiv:2409.10550v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) and agent technologies offer promising solutions to the simulation of social science experiments, but the availability of data of real-world population required by many of them still poses as a major challenge. This paper explores a novel framework that leverages census data and LLMs to generate virtual populations, significantly reducing resource requirements and bypassing privacy compliance issues associated with real-world data, while keeping a statistical truthfulness. Drawing on real-world census data, our approach first generates a persona that reflects demographic characteristics of the population. We then employ LLMs to enrich these personas with intricate details, using techniques akin to those in image generative models but applied to textual data. Additionally, we propose a framework for the evaluation of the feasibility of our method with respect to capability of LLMs based on personality trait tests, specifically the Big Five model, which also enhances the depth and realism of the generated personas. Through preliminary experiments and analysis, we demonstrate that our method produces personas with variability essential for simulating diverse human behaviors in social science experiments. But the evaluation result shows that only weak sign of statistical truthfulness can be produced due to limited capability of current LLMs. Insights from our study also highlight the tension within LLMs between aligning with human values and reflecting real-world complexities. Thorough and rigorous test call for further research. Our codes are released at https://github.com/baiyuqi/agentic-society.git</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10550v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuqi Bai, Kun Sun, Huishi Yin</dc:creator>
    </item>
    <item>
      <title>AI Literacy for All: Adjustable Interdisciplinary Socio-technical Curriculum</title>
      <link>https://arxiv.org/abs/2409.10552</link>
      <description>arXiv:2409.10552v1 Announce Type: new 
Abstract: This paper presents a curriculum, "AI Literacy for All," to promote an interdisciplinary understanding of AI, its socio-technical implications, and its practical applications for all levels of education. With the rapid evolution of artificial intelligence (AI), there is a need for AI literacy that goes beyond the traditional AI education curriculum. AI literacy has been conceptualized in various ways, including public literacy, competency building for designers, conceptual understanding of AI concepts, and domain-specific upskilling. Most of these conceptualizations were established before the public release of Generative AI (Gen-AI) tools like ChatGPT. AI education has focused on the principles and applications of AI through a technical lens that emphasizes the mastery of AI principles, the mathematical foundations underlying these technologies, and the programming and mathematical skills necessary to implement AI solutions. In AI Literacy for All, we emphasize a balanced curriculum that includes technical and non-technical learning outcomes to enable a conceptual understanding and critical evaluation of AI technologies in an interdisciplinary socio-technical context. The paper presents four pillars of AI literacy: understanding the scope and technical dimensions of AI, learning how to interact with Gen-AI in an informed and responsible way, the socio-technical issues of ethical and responsible AI, and the social and future implications of AI. While it is important to include all learning outcomes for AI education in a Computer Science major, the learning outcomes can be adjusted for other learning contexts, including, non-CS majors, high school summer camps, the adult workforce, and the public. This paper advocates for a shift in AI literacy education to offer a more interdisciplinary socio-technical approach as a pathway to broaden participation in AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10552v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sri Yash Tadimalla, Mary Lou Maher</dc:creator>
    </item>
    <item>
      <title>"Flipped" University: LLM-Assisted Lifelong Learning Environment</title>
      <link>https://arxiv.org/abs/2409.10553</link>
      <description>arXiv:2409.10553v1 Announce Type: new 
Abstract: The rapid development of artificial intelligence technologies, particularly Large Language Models (LLMs), has revolutionized the landscape of lifelong learning. This paper introduces a conceptual framework for a self-constructed lifelong learning environment supported by LLMs. It highlights the inadequacies of traditional education systems in keeping pace with the rapid deactualization of knowledge and skills. The proposed framework emphasizes the transformation from institutionalized education to personalized, self-driven learning. It leverages the natural language capabilities of LLMs to provide dynamic and adaptive learning experiences, facilitating the creation of personal intellectual agents that assist in knowledge acquisition. The framework integrates principles of lifelong learning, including the necessity of building personal world models, the dual modes of learning (training and exploration), and the creation of reusable learning artifacts. Additionally, it underscores the importance of curiosity-driven learning and reflective practices in maintaining an effective learning trajectory. The paper envisions the evolution of educational institutions into "flipped" universities, focusing on supporting global knowledge consistency rather than merely structuring and transmitting knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10553v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kirill Krinkin, Tatiana Berlenko</dc:creator>
    </item>
    <item>
      <title>LLMs as information warriors? Auditing how LLM-powered chatbots tackle disinformation about Russia's war in Ukraine</title>
      <link>https://arxiv.org/abs/2409.10697</link>
      <description>arXiv:2409.10697v1 Announce Type: new 
Abstract: The rise of large language models (LLMs) has a significant impact on information warfare. By facilitating the production of content related to disinformation and propaganda campaigns, LLMs can amplify different types of information operations and mislead online users. In our study, we empirically investigate how LLM-powered chatbots, developed by Google, Microsoft, and Perplexity, handle disinformation about Russia's war in Ukraine and whether the chatbots' ability to provide accurate information on the topic varies across languages and over time. Our findings indicate that while for some chatbots (Perplexity), there is a significant improvement in performance over time in several languages, for others (Gemini), the performance improves only in English but deteriorates in low-resource languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10697v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mykola Makhortykh, Ani Baghumyan, Victoria Vziatysheva, Maryna Sydorova, Elizaveta Kuznetsova</dc:creator>
    </item>
    <item>
      <title>Capturing Differences in Character Representations Between Communities: An Initial Study with Fandom</title>
      <link>https://arxiv.org/abs/2409.11170</link>
      <description>arXiv:2409.11170v1 Announce Type: new 
Abstract: Sociolinguistic theories have highlighted how narratives are often retold, co-constructed and reconceptualized in collaborative settings. This working paper focuses on the re-interpretation of characters, an integral part of the narrative story-world, and attempts to study how this may be computationally compared between online communities. Using online fandom - a highly communal phenomenon that has been largely studied qualitatively - as data, computational methods were applied to explore shifts in character representations between two communities and the original text. Specifically, text from the Harry Potter novels, r/HarryPotter subreddit, and fanfiction on Archive of Our Own were analyzed for changes in character mentions, centrality measures from co-occurrence networks, and semantic associations. While fandom elevates secondary characters as found in past work, the two fan communities prioritize different subsets of characters. Word embedding tests reveal starkly different associations of the same characters between communities on the gendered concepts of femininity/masculinity, cruelty, and beauty. Furthermore, fanfiction descriptions of a male character analyzed between romance pairings scored higher for feminine-coded characteristics in male-male romance, matching past qualitative theorizing. The results high-light the potential for computational methods to assist in capturing the re-conceptualization of narrative elements across communities and in supporting qualitative research on fandom.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11170v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bianca N. Y. Kang</dc:creator>
    </item>
    <item>
      <title>Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory</title>
      <link>https://arxiv.org/abs/2409.11192</link>
      <description>arXiv:2409.11192v1 Announce Type: new 
Abstract: One application area of long-term memory (LTM) capabilities with increasing traction is personal AI companions and assistants. With the ability to retain and contextualize past interactions and adapt to user preferences, personal AI companions and assistants promise a profound shift in how we interact with AI and are on track to become indispensable in personal and professional settings. However, this advancement introduces new challenges and vulnerabilities that require careful consideration regarding the deployment and widespread use of these systems. The goal of this paper is to explore the broader implications of building and deploying personal AI applications with LTM capabilities using a holistic evaluation approach. This will be done in three ways: 1) reviewing the technological underpinnings of LTM in Large Language Models, 2) surveying current personal AI companions and assistants, and 3) analyzing critical considerations and implications of deploying and using these applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11192v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eunhae Lee</dc:creator>
    </item>
    <item>
      <title>The Role of AI Safety Institutes in Contributing to International Standards for Frontier AI Safety</title>
      <link>https://arxiv.org/abs/2409.11314</link>
      <description>arXiv:2409.11314v1 Announce Type: new 
Abstract: International standards are crucial for ensuring that frontier AI systems are developed and deployed safely around the world. Since the AI Safety Institutes (AISIs) possess in-house technical expertise, mandate for international engagement, and convening power in the national AI ecosystem while being a government institution, we argue that they are particularly well-positioned to contribute to the international standard-setting processes for AI safety. In this paper, we propose and evaluate three models for AISI involvement: 1. Seoul Declaration Signatories, 2. US (and other Seoul Declaration Signatories) and China, and 3. Globally Inclusive. Leveraging their diverse strengths, these models are not mutually exclusive. Rather, they offer a multi-track system solution in which the central role of AISIs guarantees coherence among the different tracks and consistency in their AI safety focus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11314v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristina Fort</dc:creator>
    </item>
    <item>
      <title>Googling the Big Lie: Search Engines, News Media, and the US 2020 Election Conspiracy</title>
      <link>https://arxiv.org/abs/2409.10531</link>
      <description>arXiv:2409.10531v1 Announce Type: cross 
Abstract: The conspiracy theory that the US 2020 presidential election was fraudulent - the Big Lie - remained a prominent part of the media agenda months after the election. Whether and how search engines prioritized news stories that sought to thoroughly debunk the claims, provide a simple negation, or support the conspiracy is crucial for understanding information exposure on the topic. We investigate how search engines provided news on this conspiracy by conducting a large-scale algorithm audit evaluating differences between three search engines (Google, DuckDuckGo, and Bing), across three locations (Ohio, California, and the UK), and using eleven search queries. Results show that simply denying the conspiracy is the largest debunking strategy across all search engines. While Google has a strong mainstreaming effect on articles explicitly focused on the Big Lie - providing thorough debunks and alternative explanations - DuckDuckGo and Bing display, depending on the location, a large share of articles either supporting the conspiracy or failing to debunk it. Lastly, we find that niche ideologically driven search queries (e.g., "sharpie marker ballots Arizona") do not lead to more conspiracy-supportive material. Instead, content supporting the conspiracy is largely a product of broader ideology-agnostic search queries (e.g., "voter fraud 2020").</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10531v1</guid>
      <category>cs.IR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ernesto de Le\'on, Mykola Makhortykh, Aleksandra Urman, Roberto Ulloa</dc:creator>
    </item>
    <item>
      <title>Ethical Challenges in Computer Vision: Ensuring Privacy and Mitigating Bias in Publicly Available Datasets</title>
      <link>https://arxiv.org/abs/2409.10533</link>
      <description>arXiv:2409.10533v1 Announce Type: cross 
Abstract: This paper aims to shed light on the ethical problems of creating and deploying computer vision tech, particularly in using publicly available datasets. Due to the rapid growth of machine learning and artificial intelligence, computer vision has become a vital tool in many industries, including medical care, security systems, and trade. However, extensive use of visual data that is often collected without consent due to an informed discussion of its ramifications raises significant concerns about privacy and bias. The paper also examines these issues by analyzing popular datasets such as COCO, LFW, ImageNet, CelebA, PASCAL VOC, etc., that are usually used for training computer vision models. We offer a comprehensive ethical framework that addresses these challenges regarding the protection of individual rights, minimization of bias as well as openness and responsibility. We aim to encourage AI development that will take into account societal values as well as ethical standards to avoid any public harm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10533v1</guid>
      <category>cs.CV</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ghalib Ahmed Tahir</dc:creator>
    </item>
    <item>
      <title>Let's Influence Algorithms Together: How Millions of Fans Build Collective Understanding of Algorithms and Organize Coordinated Algorithmic Actions</title>
      <link>https://arxiv.org/abs/2409.10670</link>
      <description>arXiv:2409.10670v1 Announce Type: cross 
Abstract: Previous research pays attention to how users strategically understand and consciously interact with algorithms but mainly focuses on an individual level, making it difficult to explore how users within communities could develop a collective understanding of algorithms and organize collective algorithmic actions. Through a two-year ethnography of online fan activities, this study investigates 43 core fans who always organize large-scale fans' collective actions and their corresponding general fan groups. This study aims to reveal how these core fans mobilize millions of general fans through collective algorithmic actions. These core fans reported the rhetorical strategies used to persuade general fans, the steps taken to build a collective understanding of algorithms, and the collaborative processes that adapt collective actions across platforms and cultures. Our findings highlight the key factors that enable computer-supported collective algorithmic actions and extend collective action research into large-scale domain targeting algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10670v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qing Xiao, Yuhang Zheng, Xianzhe Fan, Bingbing Zhang, Zhicong Lu</dc:creator>
    </item>
    <item>
      <title>Toward Mitigating Sex Bias in Pilot Trainees' Stress and Fatigue Modeling</title>
      <link>https://arxiv.org/abs/2409.10676</link>
      <description>arXiv:2409.10676v1 Announce Type: cross 
Abstract: While researchers have been trying to understand the stress and fatigue among pilots, especially pilot trainees, and to develop stress/fatigue models to automate the process of detecting stress/fatigue, they often do not consider biases such as sex in those models. However, in a critical profession like aviation, where the demographic distribution is disproportionately skewed to one sex, it is urgent to mitigate biases for fair and safe model predictions. In this work, we investigate the perceived stress/fatigue of 69 college students, including 40 pilot trainees with around 63% male. We construct models with decision trees first without bias mitigation and then with bias mitigation using a threshold optimizer with demographic parity and equalized odds constraints 30 times with random instances. Using bias mitigation, we achieve improvements of 88.31% (demographic parity difference) and 54.26% (equalized odds difference), which are also found to be statistically significant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10676v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachel Pfeifer, Sudip Vhaduri, Mark Wilson, Julius Keller</dc:creator>
    </item>
    <item>
      <title>Machine Learning for Public Good: Predicting Urban Crime Patterns to Enhance Community Safety</title>
      <link>https://arxiv.org/abs/2409.10838</link>
      <description>arXiv:2409.10838v1 Announce Type: cross 
Abstract: In recent years, urban safety has become a paramount concern for city planners and law enforcement agencies. Accurate prediction of likely crime occurrences can significantly enhance preventive measures and resource allocation. However, many law enforcement departments lack the tools to analyze and apply advanced AI and ML techniques that can support city planners, watch programs, and safety leaders to take proactive steps towards overall community safety.
  This paper explores the effectiveness of ML techniques to predict spatial and temporal patterns of crimes in urban areas. Leveraging police dispatch call data from San Jose, CA, the research goal is to achieve a high degree of accuracy in categorizing calls into priority levels particularly for more dangerous situations that require an immediate law enforcement response. This categorization is informed by the time, place, and nature of the call. The research steps include data extraction, preprocessing, feature engineering, exploratory data analysis, implementation, optimization and tuning of different supervised machine learning models and neural networks. The accuracy and precision are examined for different models and features at varying granularity of crime categories and location precision.
  The results demonstrate that when compared to a variety of other models, Random Forest classification models are most effective in identifying dangerous situations and their corresponding priority levels with high accuracy (Accuracy = 85%, AUC = 0.92) at a local level while ensuring a minimum amount of false negatives. While further research and data gathering is needed to include other social and economic factors, these results provide valuable insights for law enforcement agencies to optimize resources, develop proactive deployment approaches, and adjust response patterns to enhance overall public safety outcomes in an unbiased way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10838v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sia Gupta, Simeon Sayer</dc:creator>
    </item>
    <item>
      <title>Strategic Insights in Human and Large Language Model Tactics at Word Guessing Games</title>
      <link>https://arxiv.org/abs/2409.11112</link>
      <description>arXiv:2409.11112v1 Announce Type: cross 
Abstract: At the beginning of 2022, a simplistic word-guessing game took the world by storm and was further adapted to many languages beyond the original English version. In this paper, we examine the strategies of daily word-guessing game players that have evolved during a period of over two years. A survey gathered from 25% of frequent players reveals their strategies and motivations for continuing the daily journey. We also explore the capability of several popular open-access large language model systems and open-source models at comprehending and playing the game in two different languages. Results highlight the struggles of certain models to maintain correct guess length and generate repetitions, as well as hallucinations of non-existent words and inflections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11112v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mat\=iss Rikters, Sanita Reinsone</dc:creator>
    </item>
    <item>
      <title>Testing for racial bias using inconsistent perceptions of race</title>
      <link>https://arxiv.org/abs/2409.11269</link>
      <description>arXiv:2409.11269v1 Announce Type: cross 
Abstract: Tests for racial bias commonly assess whether two people of different races are treated differently. A fundamental challenge is that, because two people may differ in many ways, factors besides race might explain differences in treatment. Here, we propose a test for bias which circumvents the difficulty of comparing two people by instead assessing whether the $\textit{same person}$ is treated differently when their race is perceived differently. We apply our method to test for bias in police traffic stops, finding that the same driver is likelier to be searched or arrested by police when they are perceived as Hispanic than when they are perceived as white. Our test is broadly applicable to other datasets where race, gender, or other identity data are perceived rather than self-reported, and the same person is observed multiple times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11269v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nora Gera, Emma Pierson</dc:creator>
    </item>
    <item>
      <title>Security Camera Movie and ERP Data Matching System to Prevent Theft</title>
      <link>https://arxiv.org/abs/1706.04595</link>
      <description>arXiv:1706.04595v4 Announce Type: replace 
Abstract: In this paper, we propose a SaaS service which prevents shoplifting using image analysis and ERP. In Japan, total damage of shoplifting reaches 450 billion yen. Based on cloud and data analysis technology, we propose a shoplifting prevention service with image analysis of security camera and ERP data check for small shops. We evaluated movie analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:1706.04595v4</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/CCNC.2017.7983275</arxiv:DOI>
      <arxiv:journal_reference>IEEE Consumer Communications and Networking Conference (CCNC2017), pp.1021-1022, Jan. 2017</arxiv:journal_reference>
      <dc:creator>Yoji Yamato, Yoshifumi Fukumoto, Hiroki Kumazaki</dc:creator>
    </item>
    <item>
      <title>Beyond principlism: Practical strategies for ethical AI use in research practices</title>
      <link>https://arxiv.org/abs/2401.15284</link>
      <description>arXiv:2401.15284v3 Announce Type: replace 
Abstract: The rapid adoption of generative artificial intelligence (AI) in scientific research, particularly large language models (LLMs), has outpaced the development of ethical guidelines, leading to a Triple-Too problem: too many high-level ethical initiatives, too abstract principles lacking contextual and practical relevance, and too much focus on restrictions and risks over benefits and utilities. Existing approaches, such as principlism (reliance on abstract ethical principles), formalism (rigid application of rules), and technical solutionism (overemphasis on technological fixes), offer little practical guidance for addressing ethical challenges of AI in scientific research practices. To bridge the gap between abstract principles and day-to-day research practices, we propose a user-centered, realism-inspired approach. We outline five specific goals for ethical AI use: 1) understanding model training and output, including bias mitigation strategies; 2) respecting privacy, confidentiality, and copyright; 3) avoiding plagiarism and policy violations; 4) applying AI beneficially compared to alternatives; and 5) using AI transparently and reproducibly. For each goal, we provide actionable strategies and analyze realistic cases of misuse and corrective measures. We argue that ethical AI application requires evaluating its utility against existing alternatives rather than isolated performance metrics. Additionally, we propose documentation guidelines to enhance transparency and reproducibility in AI-assisted research. Moving forward, we emphasize the need for targeted professional development, training programs, and balanced enforcement mechanisms to promote responsible AI use while fostering innovation. By refining these ethical guidelines and adapting to emerging AI capabilities, we can accelerate scientific progress without compromising research integrity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15284v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhicheng Lin</dc:creator>
    </item>
    <item>
      <title>Large language models can replicate cross-cultural differences in personality</title>
      <link>https://arxiv.org/abs/2310.10679</link>
      <description>arXiv:2310.10679v2 Announce Type: replace-cross 
Abstract: We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. We provide preliminary evidence that LLMs can aid cross-cultural researchers and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.10679v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pawe{\l} Niszczota, Mateusz Janczak, Micha{\l} Misiak</dc:creator>
    </item>
    <item>
      <title>Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination</title>
      <link>https://arxiv.org/abs/2406.08818</link>
      <description>arXiv:2406.08818v3 Announce Type: replace-cross 
Abstract: We present a large-scale study of linguistic bias exhibited by ChatGPT covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-"standard" varieties from around the world). We prompted GPT-3.5 Turbo and GPT-4 with text by native speakers of each variety and analyzed the responses via detailed linguistic feature annotation and native speaker evaluation. We find that the models default to "standard" varieties of English; based on evaluation by native speakers, we also find that model responses to non-"standard" varieties consistently exhibit a range of issues: stereotyping (19% worse than for "standard" varieties), demeaning content (25% worse), lack of comprehension (9% worse), and condescending responses (15% worse). We also find that if these models are asked to imitate the writing style of prompts in non-"standard" varieties, they produce text that exhibits lower comprehension of the input and is especially prone to stereotyping. GPT-4 improves on GPT-3.5 in terms of comprehension, warmth, and friendliness, but also exhibits a marked increase in stereotyping (+18%). The results indicate that GPT-3.5 Turbo and GPT-4 can perpetuate linguistic discrimination toward speakers of non-"standard" varieties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08818v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eve Fleisig, Genevieve Smith, Madeline Bossi, Ishita Rustagi, Xavier Yin, Dan Klein</dc:creator>
    </item>
    <item>
      <title>Bridging Social Media and Search Engines: Dredge Words and the Detection of Unreliable Domains</title>
      <link>https://arxiv.org/abs/2406.11423</link>
      <description>arXiv:2406.11423v2 Announce Type: replace-cross 
Abstract: Proactive content moderation requires platforms to rapidly and continuously evaluate the credibility of websites. Leveraging the direct and indirect paths users follow to unreliable websites, we develop a website credibility classification and discovery system that integrates both webgraph and large-scale social media contexts. We additionally introduce the concept of dredge words, terms or phrases for which unreliable domains rank highly on search engines, and provide the first exploration of their usage on social media. Our graph neural networks that combine webgraph and social media contexts generate to state-of-the-art results in website credibility classification and significantly improves the top-k identification of unreliable domains. Additionally, we release a novel dataset of dredge words, highlighting their strong connections to both social media and online commerce platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11423v2</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evan M. Williams, Peter Carragher, Kathleen M. Carley</dc:creator>
    </item>
    <item>
      <title>J\"ager: Automated Telephone Call Traceback</title>
      <link>https://arxiv.org/abs/2409.02839</link>
      <description>arXiv:2409.02839v4 Announce Type: replace-cross 
Abstract: Unsolicited telephone calls that facilitate fraud or unlawful telemarketing continue to overwhelm network users and the regulators who prosecute them. The first step in prosecuting phone abuse is traceback -- identifying the call originator. This fundamental investigative task currently requires hours of manual effort per call. In this paper, we introduce J\"ager, a distributed secure call traceback system. J\"ager can trace a call in a few seconds, even with partial deployment, while cryptographically preserving the privacy of call parties, carrier trade secrets like peers and call volume, and limiting the threat of bulk analysis. We establish definitions and requirements of secure traceback, then develop a suite of protocols that meet these requirements using witness encryption, oblivious pseudorandom functions, and group signatures. We prove these protocols secure in the universal composibility framework. We then demonstrate that J\"ager has low compute and bandwidth costs per call, and these costs scale linearly with call volume. J\"ager provides an efficient, secure, privacy-preserving system to revolutionize telephone abuse investigation with minimal costs to operators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02839v4</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3658644.3690290</arxiv:DOI>
      <dc:creator>David Adei, Varun Madathil, Sathvik Prasad, Bradley Reaves, Alessandra Scafuro</dc:creator>
    </item>
    <item>
      <title>CROSS: A Contributor-Project Interaction Lifecycle Model for Open Source Software</title>
      <link>https://arxiv.org/abs/2409.08267</link>
      <description>arXiv:2409.08267v2 Announce Type: replace-cross 
Abstract: Despite the widespread adoption of open source software (OSS), its sustainability remains a critical concern, particularly in light of security vulnerabilities and the often inadequate end-of-service (EoS) processes for OSS projects as they decline. Existing models of OSS community participation, like the Onion model and the episodic contribution model, offer valuable insights but are fundamentally incompatible and fail to provide a comprehensive picture of contributor engagement with OSS projects. This paper addresses these gaps by proposing the CROSS model, a novel contributor-project interaction lifecycle model for open source, which delineates the various lifecycle stages of contributor-project interaction, along with the driving and retaining forces pertinent to each stage. By synthesizing existing research on OSS communities, organizational behavior, and human resource development, it explains a range of archetypal cases of contributor engagement and highlights research gaps, especially in EoS/offboarding scenarios. The CROSS model provides a foundation for understanding and enhancing the sustainability of OSS projects, offering a robust foundation for future research and practical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08267v2</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tapajit Dey, Brian Fitzgerald, Sherae Daniel</dc:creator>
    </item>
    <item>
      <title>Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue</title>
      <link>https://arxiv.org/abs/2409.08330</link>
      <description>arXiv:2409.08330v2 Announce Type: replace-cross 
Abstract: Studying and building datasets for dialogue tasks is both expensive and time-consuming due to the need to recruit, train, and collect data from study participants. In response, much recent work has sought to use large language models (LLMs) to simulate both human-human and human-LLM interactions, as they have been shown to generate convincingly human-like text in many settings. However, to what extent do LLM-based simulations \textit{actually} reflect human dialogues? In this work, we answer this question by generating a large-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the WildChat dataset and quantifying how well the LLM simulations align with their human counterparts. Overall, we find relatively low alignment between simulations and human interactions, demonstrating a systematic divergence along the multiple textual properties, including style and content. Further, in comparisons of English, Chinese, and Russian dialogues, we find that models perform similarly. Our results suggest that LLMs generally perform better when the human themself writes in a way that is more similar to the LLM's own style.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08330v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Ivey, Shivani Kumar, Jiayu Liu, Hua Shen, Sushrita Rakshit, Rohan Raju, Haotian Zhang, Aparna Ananthasubramaniam, Junghwan Kim, Bowen Yi, Dustin Wright, Abraham Israeli, Anders Giovanni M{\o}ller, Lechen Zhang, David Jurgens</dc:creator>
    </item>
    <item>
      <title>Mobility-GNN: a human mobility-based graph neural network for tracking and analyzing the spatial dynamics of the synthetic opioid crisis in the USA, 2013-2020</title>
      <link>https://arxiv.org/abs/2409.09945</link>
      <description>arXiv:2409.09945v2 Announce Type: replace-cross 
Abstract: Synthetic opioids are the most common drugs involved in drug-involved overdose mortalities in the U.S. The Center for Disease Control and Prevention reported that in 2018, about 70% of all drug overdose deaths involved opioids and 67% of all opioid-involved deaths were accounted for by synthetic opioids. In this study, we investigated the spread of synthetic opioids between 2013 and 2020 in the U.S., and analyzed the relationship between the spatiotemporal pattern of synthetic opioid-involved deaths and another key opioid, heroin, and compared patterns of deaths involving these two types of drugs during this time period. Spatial connections between counties were incorporated into a graph convolutional neural network model to represent and analyze the spread of synthetic opioid-involved deaths, and in the context of heroin-involved deaths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09945v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyue Xia, Kathleen Stewart</dc:creator>
    </item>
  </channel>
</rss>
