<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 07 Apr 2025 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bayesian sequential analysis of adverse events with binary data</title>
      <link>https://arxiv.org/abs/2504.02959</link>
      <description>arXiv:2504.02959v1 Announce Type: new 
Abstract: We propose a Bayesian Sequential procedure to test hypotheses concerning the Relative Risk between two specific treatments based on the binary data obtained from the two-arm clinical trial. Our development is based on the optimal sequential test of \citet{wang2024early}, which is cast within the Bayesian framework. This approach enables us to provide, in a straightforward manner based on the Stopping Rule Principle (SRP), an assessment of the various error probabilities via posterior probabilities and conditional error probabilities. Additionally, we present the connection to the notion of the Uniformly Most Powerful Bayesian Test (UMPBT). To illustrate our procedure, we utilized the data from \citet{silva2020optimal} to analyze the results obtained from the standard Bayesian and the modified Bayesian test of \citet{berger1997unified} under several different prior distributions of the parameters involved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02959v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayue Wang, Ben Boukai</dc:creator>
    </item>
    <item>
      <title>Nonparametric Estimation of Local Treatment Effects with Continuous Instruments</title>
      <link>https://arxiv.org/abs/2504.03063</link>
      <description>arXiv:2504.03063v1 Announce Type: new 
Abstract: Instrumental variable methods are widely used to address unmeasured confounding, yet much of the existing literature has focused on the canonical binary instrument setting. Extensions to continuous instruments often impose strong parametric assumptions for identification and estimation, which can be difficult to justify and may limit their applicability in complex real-world settings. In this work, we develop theory and methods for nonparametric estimation of treatment effects with a continuous instrumental variable. We introduce a new estimand that, under a monotonicity assumption, quantifies the treatment effect among the maximal complier class, generalizing the local average treatment effect framework to continuous instruments. Considering this estimand and the local instrumental variable curve, we draw connections to the dose-response function and its derivative, and propose doubly robust estimation methods. We establish convergence rates and conditions for asymptotic normality, providing valuable insights into the role of nuisance function estimation when the instrument is continuous. Additionally, we present practical procedures for bandwidth selection and variance estimation. Through extensive simulations, we demonstrate the advantages of the proposed nonparametric estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03063v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenghao Zeng, Alexander W. Levis, JungHo Lee, Edward H. Kennedy, Luke Keele</dc:creator>
    </item>
    <item>
      <title>Interim Analysis in Sequential Multiple Assignment Randomized Trials for Survival Outcomes</title>
      <link>https://arxiv.org/abs/2504.03143</link>
      <description>arXiv:2504.03143v1 Announce Type: new 
Abstract: Sequential multiple assignment randomized trials mimic the actual treatment processes experienced by physicians and patients in clinical settings and inform the comparative effectiveness of dynamic treatment regimes. In such trials, patients go through multiple stages of treatment, and the treatment assignment is adapted over time based on individual patient characteristics such as disease status and treatment history. In this work, we develop and evaluate statistically valid interim monitoring approaches to allow for early termination of sequential multiple assignment randomized trials for efficacy targeting survival outcomes. We propose a weighted log-rank Chi-square statistic to account for overlapping treatment paths and quantify how the log-rank statistics at two different analysis points are correlated. Efficacy boundaries at multiple interim analyses can then be established using the Pocock, O'Brien Fleming, and Lan-Demets boundaries. We run extensive simulations to comparatively evaluate the operating characteristics (type I error and power) of our interim monitoring procedure based on the proposed statistic and another existing statistic. The methods are demonstrated via an analysis of a neuroblastoma dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03143v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zi Wang, Yu Cheng, Abdus S. Wahed</dc:creator>
    </item>
    <item>
      <title>Modified Tests of Linear Hypotheses Under Heteroscedasticity for Multivariate Functional Data with Finite Sample Sizes</title>
      <link>https://arxiv.org/abs/2504.03161</link>
      <description>arXiv:2504.03161v1 Announce Type: new 
Abstract: As big data continues to grow, statistical inference for multivariate functional data (MFD) has become crucial. Although recent advancements have been made in testing the equality of mean functions, research on testing linear hypotheses for mean functions remains limited. Current methods primarily consist of permutation-based tests or asymptotic tests. However, permutation-based tests are known to be time-consuming, while asymptotic tests typically require larger sample sizes to maintain an accurate Type I error rate. This paper introduces three finite-sample tests that modify traditional MANOVA methods to tackle the general linear hypothesis testing problem for MFD. The test statistics rely on two symmetric, nonnegative-definite matrices, approximated by Wishart distributions, with degrees of freedom estimated via a U-statistics-based method. The proposed tests are affine-invariant, computationally more efficient than permutation-based tests, and better at controlling significance levels in small samples compared to asymptotic tests. A real-data example further showcases their practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03161v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianming Zhu</dc:creator>
    </item>
    <item>
      <title>Spatiotemporal causal inference with arbitrary spillover and carryover effects</title>
      <link>https://arxiv.org/abs/2504.03464</link>
      <description>arXiv:2504.03464v1 Announce Type: new 
Abstract: Micro-level data with granular spatial and temporal information are becoming increasingly available to social scientists. Most researchers aggregate such data into a convenient panel data format and apply standard causal inference methods. This approach, however, has two limitations. First, data aggregation results in the loss of detailed geo-location and temporal information, leading to potential biases. Second, most panel data methods either ignore spatial spillover and temporal carryover effects or impose restrictive assumptions on their structure. We introduce a general methodological framework for spatiotemporal causal inference with arbitrary spillover and carryover effects. Under this general framework, we demonstrate how to define and estimate causal quantities of interest, explore heterogeneous treatment effects, investigate causal mechanisms, and visualize the results to facilitate their interpretation. We illustrate the proposed methodology through an analysis of airstrikes and insurgent attacks in Iraq. The open-source software package geocausal implements all of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03464v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mitsuru Mukaigawara, Kosuke Imai, Jason Lyall, Georgia Papadogeorgou</dc:creator>
    </item>
    <item>
      <title>Multivariate Causal Effects: a Bayesian Causal Regression Factor Model</title>
      <link>https://arxiv.org/abs/2504.03480</link>
      <description>arXiv:2504.03480v1 Announce Type: new 
Abstract: The impact of wildfire smoke on air quality is a growing concern, contributing to air pollution through a complex mixture of chemical species with important implications for public health. While previous studies have primarily focused on its association with total particulate matter (PM2.5), the causal relationship between wildfire smoke and the chemical composition of PM2.5 remains largely unexplored. Exposure to these chemical mixtures plays a critical role in shaping public health, yet capturing their relationships requires advanced statistical methods capable of modeling the complex dependencies among chemical species. To fill this gap, we propose a Bayesian causal regression factor model that estimates the multivariate causal effects of wildfire smoke on the concentration of 27 chemical species in PM2.5 across the United States. Our approach introduces two key innovations: (i) a causal inference framework for multivariate potential outcomes, and (ii) a novel Bayesian factor model that employs a probit stick-breaking process as prior for treatment-specific factor scores. By focusing on factor scores, our method addresses the missing data challenge common in causal inference and enables a flexible, data-driven characterization of the latent factor structure, which is crucial to capture the complex correlation among multivariate outcomes. Through Monte Carlo simulations, we show the model's accuracy in estimating the causal effects in multivariate outcomes and characterizing the treatment-specific latent structure. Finally, we apply our method to US air quality data, estimating the causal effect of wildfire smoke on 27 chemical species in PM2.5, providing a deeper understanding of their interdependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03480v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dafne Zorzetto, Jenna Landy, Corwin Zigler, Giovanni Parmigiani, Roberta De Vito</dc:creator>
    </item>
    <item>
      <title>Flatness-Robust Critical Bandwidth</title>
      <link>https://arxiv.org/abs/2504.03594</link>
      <description>arXiv:2504.03594v1 Announce Type: new 
Abstract: Critical bandwidth (CB) is used to test the multimodality of densities and regression functions, as well as for clustering methods. CB tests are known to be inconsistent if the function of interest is constant ("flat") over even a small interval, and to suffer from low power and incorrect size in finite samples if the function has a relatively small derivative over an interval. This paper proposes a solution, flatness-robust CB (FRCB), that exploits the novel observation that the inconsistency manifests only from regions consistent with the null hypothesis, and thus identifying and excluding them does not alter the null or alternative sets. I provide sufficient conditions for consistency of FRCB, and simulations of a test of regression monotonicity demonstrate the finite-sample properties of FRCB compared with CB for various regression functions. Surprisingly, FRCB performs better than CB in some cases where there are no flat regions, which can be explained by FRCB essentially giving more importance to parts of the function where there are larger violations of the null hypothesis. I illustrate the usefulness of FRCB with an empirical analysis of the monotonicity of the conditional mean function of radiocarbon age with respect to calendar age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03594v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Scott Kostyshak</dc:creator>
    </item>
    <item>
      <title>Choosing an analytic approach: Key study design considerations in state policy evaluation</title>
      <link>https://arxiv.org/abs/2504.03609</link>
      <description>arXiv:2504.03609v1 Announce Type: new 
Abstract: This paper reviews and details methods for state policy evaluation to guide selection of a research approach based on evaluation setting and available data. We highlight key design considerations for an analysis, including treatment and control group selection, timing of policy adoption, expected effect heterogeneity, and data considerations. We then provide an overview of analytic approaches and differentiate between methods based on evaluation context, such as settings with no control units, a single treated unit, multiple treated units, or with multiple treatment cohorts. Methods discussed include interrupted time series models, difference-in-differences estimators, autoregressive models, and synthetic control methods, along with method extensions which address issues like staggered policy adoption and heterogenous treatment effects. We end with an illustrative example, applying the developed framework to evaluate the impacts of state-level naloxone standing order policies on overdose rates. Overall, we provide researchers with an approach for deciding on methods for state policy evaluations, which can be used to select study designs and inform methodological choices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03609v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Elizabeth M. Stone, Megan S. Schuler, Elizabeth A. Stuart, Max Rubinstein, Max Griswold, Bradley D. Stein, Beth Ann Griffin</dc:creator>
    </item>
    <item>
      <title>Enhancing Causal Effect Estimation with Diffusion-Generated Data</title>
      <link>https://arxiv.org/abs/2504.03630</link>
      <description>arXiv:2504.03630v1 Announce Type: new 
Abstract: Estimating causal effects from observational data is inherently challenging due to the lack of observable counterfactual outcomes and even the presence of unmeasured confounding. Traditional methods often rely on restrictive, untestable assumptions or necessitate valid instrumental variables, significantly limiting their applicability and robustness. In this paper, we introduce Augmented Causal Effect Estimation (ACEE), an innovative approach that utilizes synthetic data generated by a diffusion model to enhance causal effect estimation. By fine-tuning pre-trained generative models, ACEE simulates counterfactual scenarios that are otherwise unobservable, facilitating accurate estimation of individual and average treatment effects even under unmeasured confounding. Unlike conventional methods, ACEE relaxes the stringent unconfoundedness assumption, relying instead on an empirically checkable condition. Additionally, a bias-correction mechanism is introduced to mitigate synthetic data inaccuracies. We provide theoretical guarantees demonstrating the consistency and efficiency of the ACEE estimator, alongside comprehensive empirical validation through simulation studies and benchmark datasets. Results confirm that ACEE significantly improves causal estimation accuracy, particularly in complex settings characterized by nonlinear relationships and heteroscedastic noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03630v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Chen, Xiaotong Shen, Wei Pan</dc:creator>
    </item>
    <item>
      <title>Early Detection of Treatments Side Effect: A Sequential Approach</title>
      <link>https://arxiv.org/abs/2401.13760</link>
      <description>arXiv:2401.13760v1 Announce Type: cross 
Abstract: With the emergence and spread of infectious diseases with pandemic potential, such as COVID- 19, the urgency for vaccine development have led to unprecedented compressed and accelerated schedules that shortened the standard development timeline. In a relatively short time, the leading pharmaceutical companies1, received an Emergency Use Authorization (EUA) for vaccine\prime s en-mass deployment To monitor the potential side effect(s) of the vaccine during the (initial) vaccination campaign, we developed an optimal sequential test that allows for the early detection of potential side effect(s). This test employs a rule to stop the vaccination process once the observed number of side effect incidents exceeds a certain (pre-determined) threshold. The optimality of the proposed sequential test is justified when compared with the ({\alpha}, {\beta}) optimality of the non-randomized fixed-sample Uniformly Most Powerful (UMP) test. In the case of a single side effect, we study the properties of the sequential test and derive the exact expressions of the Average Sample Number (ASN) curve of the stopping time (and its variance) via the regularized incomplete beta function. Additionally, we derive the asymptotic distribution of the relative savings in ASN as compared to maximal sample size. Moreover, we construct the post-test parameter estimate and studied its sampling properties, including its asymptotic behavior under local-type alternatives. These limiting behavior results are the consistency and asymptotic normality of the post-test parameter estimator. We conclude the paper with a small simulation study illustrating the asymptotic performance of the point and interval estimation and provide a detailed example, based on COVID-19 side effect data (see Beatty et al. (2021)) of our suggested testing procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13760v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayue Wang, Ben Boukai</dc:creator>
    </item>
    <item>
      <title>Statistical applications of the 20/60/20 rule in risk management and portfolio optimization</title>
      <link>https://arxiv.org/abs/2504.02840</link>
      <description>arXiv:2504.02840v1 Announce Type: cross 
Abstract: This paper explores the applications of the 20/60/20 rule-a heuristic method that segments data into top-performing, average-performing, and underperforming groups-in mathematical finance. We review the statistical foundations of this rule and demonstrate its usefulness in risk management and portfolio optimization. Our study highlights three key applications. First, we apply the rule to stock market data, showing that it enables effective population clustering. Second, we introduce a novel, easy-to-implement method for extracting heavy-tail characteristics in risk management. Third, we integrate spatial reasoning based on the 20/60/20 rule into portfolio optimization, enhancing robustness and improving performance. To support our findings, we develop a new measure for quantifying tail heaviness and employ conditional statistics to reconstruct the unconditional distribution from the core data segment. This reconstructed distribution is tested on real financial data to evaluate whether the 20/60/20 segmentation effectively balances capturing extreme risks with maintaining the stability of central returns. Our results offer insights into financial data behavior under heavy-tailed conditions and demonstrate the potential of the 20/60/20 rule as a complementary tool for decision-making in finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02840v1</guid>
      <category>q-fin.PM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kewin P\k{a}czek, Damian Jelito, Marcin Pitera, Agnieszka Wy{\l}oma\'nska</dc:creator>
    </item>
    <item>
      <title>Enhanced ECG Arrhythmia Detection Accuracy by Optimizing Divergence-Based Data Fusion</title>
      <link>https://arxiv.org/abs/2504.02842</link>
      <description>arXiv:2504.02842v1 Announce Type: cross 
Abstract: AI computation in healthcare faces significant challenges when clinical datasets are limited and heterogeneous. Integrating datasets from multiple sources and different equipments is critical for effective AI computation but is complicated by their diversity, complexity, and lack of representativeness, so we often need to join multiple datasets for analysis. The currently used method is fusion after normalization. But when using this method, it can introduce redundant information, decreasing the signal-to-noise ratio and reducing classification accuracy. To tackle this issue, we propose a feature-based fusion algorithm utilizing Kernel Density Estimation (KDE) and Kullback-Leibler (KL) divergence. Our approach involves initially preprocessing and continuous estimation on the extracted features, followed by employing the gradient descent method to identify the optimal linear parameters that minimize the KL divergence between the feature distributions. Using our in-house datasets consisting of ECG signals collected from 2000 healthy and 2000 diseased individuals by different equipments and verifying our method by using the publicly available PTB-XL dataset which contains 21,837 ECG recordings from 18,885 patients. We employ a Light Gradient Boosting Machine (LGBM) model to do the binary classification. The results demonstrate that the proposed fusion method significantly enhances feature-based classification accuracy for abnormal ECG cases in the merged datasets, compared to the normalization method. This data fusion strategy provides a new approach to process heterogeneous datasets for the optimal AI computation results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02842v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baozhuo Su, Qingli Dou, Kang Liu, Zhengxian Qu, Jerry Deng, Ting Tan, Yanan Gu</dc:creator>
    </item>
    <item>
      <title>High-dimensional ridge regression with random features for non-identically distributed data with a variance profile</title>
      <link>https://arxiv.org/abs/2504.03035</link>
      <description>arXiv:2504.03035v1 Announce Type: cross 
Abstract: The behavior of the random feature model in the high-dimensional regression framework has become a popular issue of interest in the machine learning literature}. This model is generally considered for feature vectors $x_i = \Sigma^{1/2} x_i'$, where $x_i'$ is a random vector made of independent and identically distributed (iid) entries, and $\Sigma$ is a positive definite matrix representing the covariance of the features.
  In this paper, we move beyond {\CB this standard assumption by studying the performances of the random features model in the setting of non-iid feature vectors}. Our approach is related to the analysis of the spectrum of large random matrices through random matrix theory (RMT) {\CB and free probability} results. We turn to the analysis of non-iid data by using the notion of variance profile {\CB which} is {\CB well studied in RMT.} Our main contribution is then the study of the limits of the training and {\CB prediction} risks associated to the ridge estimator in the random features model when its dimensions grow. We provide asymptotic equivalents of these risks that capture the behavior of ridge regression with random features in a {\CB high-dimensional} framework. These asymptotic equivalents, {\CB which prove to be sharp in numerical experiments}, are retrieved by adapting, to our setting, established results from operator-valued free probability theory. Moreover, {\CB for various classes of random feature vectors that have not been considered so far in the literature}, our approach allows to show the appearance of the double descent phenomenon when the ridge regularization parameter is small enough.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03035v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Issa-Mbenard Dabo, J\'er\'emie Bigot</dc:creator>
    </item>
    <item>
      <title>Scalable Fitting Methods for Multivariate Gaussian Additive Models with Covariate-dependent Covariance Matrices</title>
      <link>https://arxiv.org/abs/2504.03368</link>
      <description>arXiv:2504.03368v1 Announce Type: cross 
Abstract: We propose efficient computational methods to fit multivariate Gaussian additive models, where the mean vector and the covariance matrix are allowed to vary with covariates, in an empirical Bayes framework. To guarantee the positive-definiteness of the covariance matrix, we model the elements of an unconstrained parametrisation matrix, focussing particularly on the modified Cholesky decomposition and the matrix logarithm. A key computational challenge arises from the fact that, for the model class considered here, the number of parameters increases quadratically with the dimension of the response vector. Hence, here we discuss how to achieve fast computation and low memory footprint in moderately high dimensions, by exploiting parsimonious model structures, sparse derivative systems and by employing block-oriented computational methods. Methods for building and fitting multivariate Gaussian additive models are provided by the SCM R package, available at https://github.com/VinGioia90/SCM, while the code for reproducing the results in this paper is available at https://github.com/VinGioia90/SACM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03368v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincenzo Gioia, Matteo Fasiolo, Ruggero Bellio, Simon N. Wood</dc:creator>
    </item>
    <item>
      <title>Semiparametric Estimation of the Shape of the Limiting Bivariate Point Cloud</title>
      <link>https://arxiv.org/abs/2306.13257</link>
      <description>arXiv:2306.13257v4 Announce Type: replace 
Abstract: We propose a model to flexibly estimate joint tail properties by exploiting the convergence of an appropriately scaled point cloud onto a compact limit set. Characteristics of the shape of the limit set correspond to key tail dependence properties. We directly model the shape of the limit set using Bezier splines, which allow flexible and parsimonious specification of shapes in two dimensions. We fit the Bezier splines to data in pseudo-polar coordinates using Markov chain Monte Carlo sampling, utilizing a limiting approximation to the conditional likelihood of the radii given angles. We propose a novel prior on the shape of the limit set via constraints on the parameters of the Bezier splines. A direct advantage of our Bayesian approach is that the support of this prior guarantees that each posterior sample is a valid limit set boundary, allowing direct posterior analysis of any quantity derived from the shape of the curve. Furthermore, we obtain interpretable inference on the asymptotic dependence class by using mixture priors with point masses on the corner of the unit box. Finally, we apply our model to bivariate datasets of extremes of variables related to fire risk and air pollution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13257v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/25-BA1514</arxiv:DOI>
      <dc:creator>Reetam Majumder, Benjamin A. Shaby, Brian J. Reich, Daniel Cooley</dc:creator>
    </item>
    <item>
      <title>Adaptive functional principal components analysis</title>
      <link>https://arxiv.org/abs/2306.16091</link>
      <description>arXiv:2306.16091v4 Announce Type: replace 
Abstract: Functional data analysis almost always involves smoothing discrete observations into curves, because they are never observed in continuous time and rarely without error. Although smoothing parameters affect the subsequent inference, data-driven methods for selecting these parameters are not well-developed, frustrated by the difficulty of using all the information shared by curves while being computationally efficient. On the one hand, smoothing individual curves in an isolated, albeit sophisticated way, ignores useful signals present in other curves. On the other hand, bandwidth selection by automatic procedures such as cross-validation after pooling all the curves together quickly become computationally unfeasible due to the large number of data points. In this paper we propose a new data-driven, adaptive kernel smoothing, specifically tailored for functional principal components analysis through the derivation of sharp, explicit risk bounds for the eigen-elements. The minimization of these quadratic risk bounds provide refined, yet computationally efficient bandwidth rules for each eigen-element separately. Both common and independent design cases are allowed. Rates of convergence for the estimators are derived. An extensive simulation study, designed in a versatile manner to closely mimic the characteristics of real data sets supports our methodological contribution. An illustration on a real data application is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16091v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sunny G. W. Wang, Valentin Patilea, Nicolas Klutchnikoff</dc:creator>
    </item>
    <item>
      <title>Latent Gaussian dynamic factor modeling and forecasting for multivariate count time series</title>
      <link>https://arxiv.org/abs/2307.10454</link>
      <description>arXiv:2307.10454v3 Announce Type: replace 
Abstract: This work considers estimation and forecasting in a multivariate, possibly high-dimensional count time series model constructed from a transformation of a latent Gaussian dynamic factor series. The estimation of the latent model parameters is based on second-order properties of the count and underlying Gaussian time series, yielding estimators of the underlying covariance matrices for which standard principal component analysis applies. Theoretical consistency results are established for the proposed estimation, building on certain concentration results for the models of the type considered. They also involve the memory of the latent Gaussian process, quantified through a spectral gap, shown to be suitably bounded as the model dimension increases, which is of independent interest. In addition, novel cross-validation schemes are suggested for model selection. The forecasting is carried out through a particle-based sequential Monte Carlo, leveraging Kalman filtering techniques. A simulation study and an application are also considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10454v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younghoon Kim, Marie-Christine D\"uker, Zachary F. Fisher, Vladas Pipiras</dc:creator>
    </item>
    <item>
      <title>Separation-based distance measures for causal graphs</title>
      <link>https://arxiv.org/abs/2402.04952</link>
      <description>arXiv:2402.04952v4 Announce Type: replace 
Abstract: Assessing the accuracy of the output of causal discovery algorithms is crucial in developing and comparing novel methods. Common evaluation metrics such as the structural Hamming distance are useful for assessing individual links of causal graphs. However, many state-of-the-art causal discovery methods do not output single causal graphs, but rather their Markov equivalence classes (MECs) which encode all of the graph's separation and connection statements. In this work, we propose additional measures of distance that capture the difference in separations of two causal graphs which link-based distances are not fit to assess. The proposed distances have low polynomial time complexity and are applicable to directed acyclic graphs (DAGs) as well as to maximal ancestral graph (MAGs) that may contain bidirected edges. We complement our theoretical analysis with toy examples and empirical experiments that highlight the differences to existing comparison metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04952v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Wahl, Jakob Runge</dc:creator>
    </item>
    <item>
      <title>New User Event Prediction Through the Lens of Causal Inference</title>
      <link>https://arxiv.org/abs/2407.05625</link>
      <description>arXiv:2407.05625v3 Announce Type: replace 
Abstract: Modeling and analysis for event series generated by users of heterogeneous behavioral patterns are closely involved in our daily lives, including credit card fraud detection, online platform user recommendation, and social network analysis. The most commonly adopted approach to this task is to assign users to behavior-based categories and analyze each of them separately. However, this requires extensive data to fully understand the user behavior, presenting challenges in modeling newcomers without significant historical knowledge. In this work, we propose a novel discrete event prediction framework for new users with limited history, without needing to know the user's category. We treat the user event history as the "treatment" for future events and the user category as the key confounder. Thus, the prediction problem can be framed as counterfactual outcome estimation, where each event is re-weighted by its inverse propensity score. We demonstrate the improved performance of the proposed framework with a numerical simulation study and two real-world applications, including Netflix rating prediction and seller contact prediction for customer support at Amazon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05625v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henry Shaowu Yuchi, Shixiang Zhu, Li Dong, Yigit M. Arisoy, Matthew C. Spencer</dc:creator>
    </item>
    <item>
      <title>Mixing Samples to Address Weak Overlap in Causal Inference</title>
      <link>https://arxiv.org/abs/2411.10801</link>
      <description>arXiv:2411.10801v3 Announce Type: replace 
Abstract: In observational studies, the assumption of sufficient overlap (positivity) is fundamental for the identification and estimation of causal effects. Failing to account for this assumption yields inaccurate and potentially infeasible estimators. To address this issue, we introduce a simple yet novel approach, \textit{mixing}, which mitigates overlap violations by constructing a synthetic treated group that combines treated and control units. Our strategy offers three key advantages. First, it improves the accuracy of the estimator by preserving unbiasedness while reducing variance. The benefit is particularly significant in settings with weak overlap, though the method remains effective regardless of the overlap level. This phenomenon results from the shrinkage of propensity scores in the mixed sample, which enhances robustness to poor overlap. Second, it enables direct estimation of the target estimand without discarding extreme observations or modifying the target population, thus facilitating a straightforward interpretation of the results. Third, the mixing approach is highly adaptable to various weighting schemes, including contemporary methods such as entropy balancing. The estimation of the Mixed IPW (MIPW) estimator is done via M-estimation, and the method extends to a broader class of weighting estimators through a resampling algorithm. We illustrate the mixing approach through extensive simulation studies and provide practical guidance with a real-data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10801v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaehyuk Jang, Suehyun Kim, Kwonsang Lee</dc:creator>
    </item>
    <item>
      <title>Coherent Disaggregation and Uncertainty Quantification for Spatially Misaligned Data</title>
      <link>https://arxiv.org/abs/2502.10584</link>
      <description>arXiv:2502.10584v3 Announce Type: replace 
Abstract: Spatial misalignment problems arise from both data aggregation and attempts to align misaligned data, leading to information loss. We propose a Bayesian disaggregation framework that links misaligned data to a continuous domain model using an iteratively linearised integration method via integrated nested Laplace approximation (INLA). The framework supports point pattern and aggregated count models under four covariate field scenarios: \textit{Raster at Full Resolution (RastFull), Raster Aggregation (RastAgg), Polygon Aggregation (PolyAgg), and Point Values (PointVal)}. The first three involve aggregation, while the latter two have incomplete fields. For PolyAgg and PointVal, we estimate the full covariate field using \textit{Value Plugin, Joint Uncertainty, and Uncertainty Plugin} methods, with the latter two accounting for uncertainty propagation. These methods demonstrate superior performance, and remain more robust even under model misspecification (i.e.\ modelling a nonlinear field as linear).
  In landslide studies, landslide occurrences are often aggregated into counts based on slope units, reducing spatial detail. The results indicate that point pattern observations and full-resolution covariate fields should be prioritized. For incomplete fields, methods incorporating uncertainty propagation are preferred. This framework supports landslide susceptibility and other spatial mapping, integrating seamlessly with INLA-extension packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10584v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Man Ho Suen, Mark Naylor, Finn Lindgren</dc:creator>
    </item>
    <item>
      <title>Two-Round Distributed Principal Component Analysis: Closing the Statistical Efficiency Gap</title>
      <link>https://arxiv.org/abs/2503.03123</link>
      <description>arXiv:2503.03123v2 Announce Type: replace 
Abstract: We enhance Fan et al.'s (2019) one-round distributed principal component analysis algorithm by adding a second fixed-point iteration round. Random matrix theory reveals the one-round estimator exhibits higher asymptotic error than the pooling estimator under moderate local signal-to-noise ratios. Remarkably, our second iteration round eliminates this efficiency gap. It follows from a careful analysis of the first-order perturbation of eigenspaces. Empirical experiments on synthetic and benchmark datasets consistently demonstrate the two-round method's statistical advantage over the one-round approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03123v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>ZeYu Li, Xinsheng Zhang, Wang Zhou</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction Under Feedback Covariate Shift for Biomolecular Design</title>
      <link>https://arxiv.org/abs/2202.03613</link>
      <description>arXiv:2202.03613v5 Announce Type: replace-cross 
Abstract: Many applications of machine learning methods involve an iterative protocol in which data are collected, a model is trained, and then outputs of that model are used to choose what data to consider next. For example, one data-driven approach for designing proteins is to train a regression model to predict the fitness of protein sequences, then use it to propose new sequences believed to exhibit greater fitness than observed in the training data. Since validating designed sequences in the wet lab is typically costly, it is important to quantify the uncertainty in the model's predictions. This is challenging because of a characteristic type of distribution shift between the training and test data in the design setting -- one in which the training and test data are statistically dependent, as the latter is chosen based on the former. Consequently, the model's error on the test data -- that is, the designed sequences -- has an unknown and possibly complex relationship with its error on the training data. We introduce a method to quantify predictive uncertainty in such settings. We do so by constructing confidence sets for predictions that account for the dependence between the training and test data. The confidence sets we construct have finite-sample guarantees that hold for any prediction algorithm, even when a trained model chooses the test-time input distribution. As a motivating use case, we demonstrate with several real data sets how our method quantifies uncertainty for the predicted fitness of designed proteins, and can therefore be used to select design algorithms that achieve acceptable trade-offs between high predicted fitness and low predictive uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.03613v5</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1073/pnas.2204569119</arxiv:DOI>
      <arxiv:journal_reference>Proc. Natl. Acad. Sci. 119 (43) e2204569119 (2022)</arxiv:journal_reference>
      <dc:creator>Clara Fannjiang, Stephen Bates, Anastasios N. Angelopoulos, Jennifer Listgarten, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Contextual Dynamic Pricing: Algorithms, Optimality, and Local Differential Privacy Constraints</title>
      <link>https://arxiv.org/abs/2406.02424</link>
      <description>arXiv:2406.02424v3 Announce Type: replace-cross 
Abstract: We study contextual dynamic pricing problems where a firm sells products to $T$ sequentially-arriving consumers, behaving according to an unknown demand model. The firm aims to minimize its regret over a clairvoyant that knows the model in advance. The demand follows a generalized linear model (GLM), allowing for stochastic feature vectors in $\mathbb R^d$ encoding product and consumer information. We first show the optimal regret is of order $\sqrt{dT}$, up to logarithmic factors, improving existing upper bounds by a $\sqrt{d}$ factor. This optimal rate is materialized by two algorithms: a confidence bound-type algorithm and an explore-then-commit (ETC) algorithm. A key insight is an intrinsic connection between dynamic pricing and contextual multi-armed bandit problems with many arms with a careful discretization. We further study contextual dynamic pricing under local differential privacy (LDP) constraints. We propose a stochastic gradient descent-based ETC algorithm achieving regret upper bounds of order $d\sqrt{T}/\epsilon$, up to logarithmic factors, where $\epsilon&gt;0$ is the privacy parameter. The upper bounds with and without LDP constraints are matched by newly constructed minimax lower bounds, characterizing costs of privacy. Moreover, we extend our study to dynamic pricing under mixed privacy constraints, improving the privacy-utility tradeoff by leveraging public data. This is the first time such setting is studied in the dynamic pricing literature and our theoretical results seamlessly bridge dynamic pricing with and without LDP. Extensive numerical experiments and real data applications are conducted to illustrate the efficiency and practical value of our algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02424v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zifeng Zhao, Feiyu Jiang, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Multi-marginal Schr\"odinger Bridges with Iterative Reference Refinement</title>
      <link>https://arxiv.org/abs/2408.06277</link>
      <description>arXiv:2408.06277v4 Announce Type: replace-cross 
Abstract: Practitioners often aim to infer an unobserved population trajectory using sample snapshots at multiple time points. E.g., given single-cell sequencing data, scientists would like to learn how gene expression changes over a cell's life cycle. But sequencing any cell destroys that cell. So we can access data for any particular cell only at a single time point, but we have data across many cells. The deep learning community has recently explored using Schr\"odinger bridges (SBs) and their extensions in similar settings. However, existing methods either (1) interpolate between just two time points or (2) require a single fixed reference dynamic (often set to Brownian motion within SBs). But learning piecewise from adjacent time points can fail to capture long-term dependencies. And practitioners are typically able to specify a model family for the reference dynamic but not the exact values of the parameters within it. So we propose a new method that (1) learns the unobserved trajectories from sample snapshots across multiple time points and (2) requires specification only of a family of reference dynamics, not a single fixed one. We demonstrate the advantages of our method on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06277v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunyi Shen, Renato Berlinghieri, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>The Amenability Framework: Rethinking Causal Ordering Without Estimating Causal Effects</title>
      <link>https://arxiv.org/abs/2504.02456</link>
      <description>arXiv:2504.02456v2 Announce Type: replace-cross 
Abstract: Who should we prioritize for intervention when we cannot estimate intervention effects? In many applied domains (e.g., advertising, customer retention, and behavioral nudging) prioritization is guided by predictive models that estimate outcome probabilities rather than causal effects. This paper investigates when these predictions (scores) can effectively rank individuals by their intervention effects, particularly when direct effect estimation is infeasible or unreliable. We propose a conceptual framework based on amenability: an individual's latent proclivity to be influenced by an intervention. We then formalize conditions under which predictive scores serve as effective proxies for amenability. These conditions justify using non-causal scores for intervention prioritization, even when the scores do not directly estimate effects. We further show that, under plausible assumptions, predictive models can outperform causal effect estimators in ranking individuals by intervention effects. Empirical evidence from an advertising context supports our theoretical findings, demonstrating that predictive modeling can offer a more robust approach to targeting than effect estimation. Our framework suggests a shift in focus, from estimating effects to inferring who is amenable, as a practical and theoretically grounded strategy for prioritizing interventions in resource-constrained environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02456v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Fern\'andez-Lor\'ia, Jorge Lor\'ia</dc:creator>
    </item>
  </channel>
</rss>
