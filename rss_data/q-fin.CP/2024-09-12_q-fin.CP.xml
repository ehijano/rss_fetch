<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.CP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.CP</link>
    <description>q-fin.CP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.CP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Sep 2024 04:01:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>MLP, XGBoost, KAN, TDNN, and LSTM-GRU Hybrid RNN with Attention for SPX and NDX European Call Option Pricing</title>
      <link>https://arxiv.org/abs/2409.06724</link>
      <description>arXiv:2409.06724v1 Announce Type: new 
Abstract: We explore the performance of various artificial neural network architectures, including a multilayer perceptron (MLP), Kolmogorov-Arnold network (KAN), LSTM-GRU hybrid recursive neural network (RNN) models, and a time-delay neural network (TDNN) for pricing European call options. In this study, we attempt to leverage the ability of supervised learning methods, such as ANNs, KANs, and gradient-boosted decision trees, to approximate complex multivariate functions in order to calibrate option prices based on past market data. The motivation for using ANNs and KANs is the Universal Approximation Theorem and Kolmogorov-Arnold Representation Theorem, respectively. Specifically, we use S\&amp;P 500 (SPX) and NASDAQ 100 (NDX) index options traded during 2015-2023 with times to maturity ranging from 15 days to over 4 years (OptionMetrics IvyDB US dataset). Black \&amp; Scholes's (BS) PDE \cite{Black1973} model's performance in pricing the same options compared to real data is used as a benchmark. This model relies on strong assumptions, and it has been observed and discussed in the literature that real data does not match its predictions. Supervised learning methods are widely used as an alternative for calibrating option prices due to some of the limitations of this model. In our experiments, the BS model underperforms compared to all of the others. Also, the best TDNN model outperforms the best MLP model on all error metrics. We implement a simple self-attention mechanism to enhance the RNN models, significantly improving their performance. The best-performing model overall is the LSTM-GRU hybrid RNN model with attention. Also, the KAN model outperforms the TDNN and MLP models. We analyze the performance of all models by ticker, moneyness category, and over/under/correctly-priced percentage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06724v1</guid>
      <category>q-fin.CP</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.32372.56963</arxiv:DOI>
      <arxiv:journal_reference>Recognition Technologies, Inc. Technical Report August 22, 2024</arxiv:journal_reference>
      <dc:creator>Boris Ter-Avanesov, Homayoon Beigi</dc:creator>
    </item>
    <item>
      <title>A deep primal-dual BSDE method for optimal stopping problems</title>
      <link>https://arxiv.org/abs/2409.06937</link>
      <description>arXiv:2409.06937v1 Announce Type: new 
Abstract: We present a new deep primal-dual backward stochastic differential equation framework based on stopping time iteration to solve optimal stopping problems. A novel loss function is proposed to learn the conditional expectation, which consists of subnetwork parameterization of a continuation value and spatial gradients from present up to the stopping time. Notable features of the method include: (i) The martingale part in the loss function reduces the variance of stochastic gradients, which facilitates the training of the neural networks as well as alleviates the error propagation of value function approximation; (ii) this martingale approximates the martingale in the Doob-Meyer decomposition, and thus leads to a true upper bound for the optimal value in a non-nested Monte Carlo way. We test the proposed method in American option pricing problems, where the spatial gradient network yields the hedging ratio directly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06937v1</guid>
      <category>q-fin.CP</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiefei Yang, Guanglian Li</dc:creator>
    </item>
    <item>
      <title>Robust financial calibration: a Bayesian approach for neural SDEs</title>
      <link>https://arxiv.org/abs/2409.06551</link>
      <description>arXiv:2409.06551v2 Announce Type: replace 
Abstract: The paper presents a Bayesian framework for the calibration of financial models using neural stochastic differential equations (neural SDEs). The method is based on the specification of a prior distribution on the neural network weights and an adequately chosen likelihood function. The resulting posterior distribution can be seen as a mixture of different classical neural SDE models yielding robust bounds on the implied volatility surface. Both, historical financial time series data and option price data are taken into consideration, which necessitates a methodology to learn the change of measure between the risk-neutral and the historical measure. The key ingredient for a robust numerical optimization of the neural networks is to apply a Langevin-type algorithm, commonly used in the Bayesian approaches to draw posterior samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06551v2</guid>
      <category>q-fin.CP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christa Cuchiero, Eva Flonner, Kevin Kurt</dc:creator>
    </item>
  </channel>
</rss>
