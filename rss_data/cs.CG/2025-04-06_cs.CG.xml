<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CG updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CG</link>
    <description>cs.CG updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CG" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 07 Apr 2025 04:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Finding a Shortest Curve that Separates Few Objects from Many</title>
      <link>https://arxiv.org/abs/2504.03558</link>
      <description>arXiv:2504.03558v1 Announce Type: new 
Abstract: We present a fixed-parameter tractable (FPT) algorithm to find a shortest curve that encloses a set of k required objects in the plane while paying a penalty for enclosing unwanted objects.
  The input is a set of interior-disjoint simple polygons in the plane, where k of the polygons are required to be enclosed and the remaining optional polygons have non-negative penalties. The goal is to find a closed curve that is disjoint from the polygon interiors and encloses the k required polygons, while minimizing the length of the curve plus the penalties of the enclosed optional polygons. If the penalties are high, the output is a shortest curve that separates the required polygons from the others. The problem is NP-hard if k is not fixed, even in very special cases. The runtime of our algorithm is $O(3^kn^3)$, where n is the number of vertices of the input polygons.
  We extend the result to a graph version of the problem where the input is a connected plane graph with positive edge weights. There are k required faces; the remaining faces are optional and have non-negative penalties. The goal is to find a closed walk in the graph that encloses the k required faces, while minimizing the weight of the walk plus the penalties of the enclosed optional faces. We also consider an inverted version of the problem where the required objects must lie outside the curve. Our algorithms solve some other well-studied problems, such as geometric knapsack.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03558v1</guid>
      <category>cs.CG</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.SoCG.2025.15</arxiv:DOI>
      <dc:creator>Therese Biedl, \'Eric Colin de Verdi\`ere, Fabrizio Frati, Anna Lubiw, G\"unter Rote</dc:creator>
    </item>
    <item>
      <title>Optimization of a Triangular Delaunay Mesh Generator using Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2504.03610</link>
      <description>arXiv:2504.03610v1 Announce Type: new 
Abstract: In this work we introduce a triangular Delaunay mesh generator that can be trained using reinforcement learning to maximize a given mesh quality metric. Our mesh generator consists of a graph neural network that distributes and modifies vertices, and a standard Delaunay algorithm to triangulate the vertices. We explore various design choices and evaluate our mesh generator on various tasks including mesh generation, mesh improvement, and producing variable resolution meshes. The learned mesh generator outputs meshes that are comparable to those produced by Triangle and DistMesh, two popular Delaunay-based mesh generators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03610v1</guid>
      <category>cs.CG</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Will Thacher, Per-Olof Persson, Yulong Pan</dc:creator>
    </item>
    <item>
      <title>Dudeney's Dissection is Optimal</title>
      <link>https://arxiv.org/abs/2412.03865</link>
      <description>arXiv:2412.03865v2 Announce Type: replace 
Abstract: In 1907, Henry Ernest Dudeney posed a puzzle: ``cut any equilateral triangle \dots\ into as few pieces as possible that will fit together and form a perfect square'' (without overlap, via translation and rotation).
  Four weeks later, Dudeney demonstrated a beautiful four-piece solution, which today remains perhaps the most famous example of dissection.
  In this paper (over a century later), we finally solve Dudeney's puzzle, by proving that the equilateral triangle and square have no common dissection with three or fewer polygonal pieces.
  We reduce the problem to the analysis of discrete graph structures representing the correspondence between the edges and the vertices of the pieces forming each polygon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03865v2</guid>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <category>math.GT</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erik D. Demaine, Tonan Kamata, Ryuhei Uehara</dc:creator>
    </item>
    <item>
      <title>Hard diagrams of split links</title>
      <link>https://arxiv.org/abs/2412.03372</link>
      <description>arXiv:2412.03372v2 Announce Type: replace-cross 
Abstract: Deformations of knots and links in ambient space can be studied combinatorially on their diagrams via local modifications called Reidemeister moves. While it is well-known that, in order to move between equivalent diagrams with Reidemeister moves, one sometimes needs to insert excess crossings, there are significant gaps between the best known lower and upper bounds on the required number of these added crossings. In this article, we study the problem of turning a diagram of a split link into a split diagram, and we show that there exist split links with diagrams requiring an arbitrarily large number of such additional crossings. More precisely, we provide a family of diagrams of split links, so that any sequence of Reidemeister moves transforming a diagram with $c$ crossings into a split diagram requires going through a diagram with $\Omega(\sqrt{c})$ extra crossings. Our proof relies on the framework of bubble tangles, as introduced by the first two authors, and a technique of Chambers and Liokumovitch to turn homotopies into isotopies in the context of Riemannian geometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03372v2</guid>
      <category>math.GT</category>
      <category>cs.CG</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Corentin Lunel, Arnaud de Mesmay, Jonathan Spreer</dc:creator>
    </item>
    <item>
      <title>Sparsification of the Generalized Persistence Diagrams for Scalability through Gradient Descent</title>
      <link>https://arxiv.org/abs/2412.05900</link>
      <description>arXiv:2412.05900v2 Announce Type: replace-cross 
Abstract: The generalized persistence diagram (GPD) is a natural extension of the classical persistence barcode to the setting of multi-parameter persistence and beyond. The GPD is defined as an integer-valued function whose domain is the set of intervals in the indexing poset of a persistence module, and is known to be able to capture richer topological information than its single-parameter counterpart. However, computing the GPD is computationally prohibitive due to the sheer size of the interval set. Restricting the GPD to a subset of intervals provides a way to manage this complexity, compromising discriminating power to some extent. However, identifying and computing an effective restriction of the domain that minimizes the loss of discriminating power remains an open challenge.
  In this work, we introduce a novel method for optimizing the domain of the GPD through gradient descent optimization. To achieve this, we introduce a loss function tailored to optimize the selection of intervals, balancing computational efficiency and discriminative accuracy. The design of the loss function is based on the known erosion stability property of the GPD. We showcase the efficiency of our sparsification method for dataset classification in supervised machine learning. Experimental results demonstrate that our sparsification method significantly reduces the time required for computing the GPDs associated to several datasets, while maintaining classification accuracies comparable to those achieved using full GPDs. Our method thus opens the way for the use of GPD-based methods to applications at an unprecedented scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05900v2</guid>
      <category>math.AT</category>
      <category>cs.CG</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathieu Carri\`ere, Seunghyun Kim, Woojin Kim</dc:creator>
    </item>
    <item>
      <title>Sublinear Data Structures for Nearest Neighbor in Ultra High Dimensions</title>
      <link>https://arxiv.org/abs/2503.03079</link>
      <description>arXiv:2503.03079v2 Announce Type: replace-cross 
Abstract: Geometric data structures have been extensively studied in the regime where the dimension is much smaller than the number of input points. But in many scenarios in Machine Learning, the dimension can be much higher than the number of points and can be so high that the data structure might be unable to read and store all coordinates of the input and query points.
  Inspired by these scenarios and related studies in feature selection and explainable clustering, we initiate the study of geometric data structures in this ultra-high dimensional regime. Our focus is the {\em approximate nearest neighbor} problem.
  In this problem, we are given a set of $n$ points $C\subseteq \mathbb{R}^d$ and have to produce a {\em small} data structure that can {\em quickly} answer the following query: given $q\in \mathbb{R}^d$, return a point $c\in C$ that is approximately nearest to $q$.
  The main question in this paper is: {\em Is there a data structure with sublinear ($o(nd)$) space and sublinear ($o(d)$) query time when $d\gg n$?} In this paper, we answer this question affirmatively. We present $(1+\epsilon)$-approximation data structures with the following guarantees. For $\ell_1$- and $\ell_2$-norm distances: $\tilde O(n \log(d)/\mathrm{poly}(\epsilon))$ space and $\tilde O(n/\mathrm{poly}(\epsilon))$ query time. We show that these space and time bounds are tight up to $\mathrm{poly}{(\log n/\epsilon)}$ factors. For $\ell_p$-norm distances: $\tilde O(n^2 \log(d) (\log\log (n)/\epsilon)^p)$ space and $\tilde O\left(n(\log\log (n)/\epsilon)^p\right)$ query time.
  Via simple reductions, our data structures imply sublinear-in-$d$ data structures for some other geometric problems; e.g. approximate orthogonal range search, furthest neighbor, and give rise to a sublinear $O(1)$-approximate representation of $k$-median and $k$-means clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03079v2</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin G. Herold, Danupon Nanongkai, Joachim Spoerhase, Nithin Varma, Zihang Wu</dc:creator>
    </item>
  </channel>
</rss>
