<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Feb 2025 05:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Hydrogen liquid-liquid transition from first principles and machine learning</title>
      <link>https://arxiv.org/abs/2502.02447</link>
      <description>arXiv:2502.02447v1 Announce Type: new 
Abstract: The molecular-to-atomic liquid-liquid transition (LLT) in high-pressure hydrogen is a fundamental topic touching domains from planetary science to materials modeling. Yet, the nature of the LLT is still under debate. To resolve it, numerical simulations must cover length and time scales spanning several orders of magnitude. We overcome these size and time limitations by constructing a fast and accurate machine-learning interatomic potential (MLIP) built on the MACE neural network architecture. The MLIP is trained on Perdew-Burke-Ernzerhof (PBE) density functional calculations and uses a modified loss function correcting for an energy bias in the molecular phase. Classical and path-integral molecular dynamics driven by this MLIP show that the LLT is always supercritical above the melting temperature. The position of the corresponding Widom line agrees with previous ab initio PBE calculations, which in contrast predicted a first-order LLT. According to our calculations, the crossover line becomes a first-order transition only inside the molecular crystal region. These results call for a reconsideration of the LLT picture previously drawn.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02447v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mtrl-sci</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giacomo Tenti, Bastian J\"ackl, Kousuke Nakano, Matthias Rupp, Michele Casula</dc:creator>
    </item>
    <item>
      <title>How to Build a Quantum Supercomputer: Scaling from Hundreds to Millions of Qubits</title>
      <link>https://arxiv.org/abs/2411.10406</link>
      <description>arXiv:2411.10406v2 Announce Type: cross 
Abstract: In the span of four decades, quantum computation has evolved from an intellectual curiosity to a potentially realizable technology. Today, small-scale demonstrations have become possible for quantum algorithmic primitives on hundreds of physical qubits and proof-of-principle error-correction on a single logical qubit. Nevertheless, despite significant progress and excitement, the path toward a full-stack scalable technology is largely unknown. There are significant outstanding quantum hardware, fabrication, software architecture, and algorithmic challenges that are either unresolved or overlooked. These issues could seriously undermine the arrival of utility-scale quantum computers for the foreseeable future. Here, we provide a comprehensive review of these scaling challenges. We show how the road to scaling could be paved by adopting existing semiconductor technology to build much higher-quality qubits, employing system engineering approaches, and performing distributed quantum computation within heterogeneous high-performance computing infrastructures. These opportunities for research and development could unlock certain promising applications, in particular, efficient quantum simulation/learning of quantum data generated by natural or engineered quantum systems. To estimate the true cost of such promises, we provide a detailed resource and sensitivity analysis for classically hard quantum chemistry calculations on surface-code error-corrected quantum computers given current, target, and desired hardware specifications based on superconducting qubits, accounting for a realistic distribution of errors. Furthermore, we argue that, to tackle industry-scale classical optimization and machine learning problems in a cost-effective manner, heterogeneous quantum-probabilistic computing with custom-designed accelerators should be considered as a complementary path toward scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10406v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masoud Mohseni, Artur Scherer, K. Grace Johnson, Oded Wertheim, Matthew Otten, Navid Anjum Aadit, Yuri Alexeev, Kirk M. Bresniker, Kerem Y. Camsari, Barbara Chapman, Soumitra Chatterjee, Gebremedhin A. Dagnew, Aniello Esposito, Farah Fahim, Marco Fiorentino, Archit Gajjar, Abdullah Khalid, Xiangzhou Kong, Bohdan Kulchytskyy, Elica Kyoseva, Ruoyu Li, P. Aaron Lott, Igor L. Markov, Robert F. McDermott, Giacomo Pedretti, Pooja Rao, Eleanor Rieffel, Allyson Silva, John Sorebo, Panagiotis Spentzouris, Ziv Steiner, Boyan Torosov, Davide Venturelli, Robert J. Visser, Zak Webb, Xin Zhan, Yonatan Cohen, Pooya Ronagh, Alan Ho, Raymond G. Beausoleil, John M. Martinis</dc:creator>
    </item>
    <item>
      <title>Postselection-free experimental observation of the measurement-induced phase transition in circuits with universal gates</title>
      <link>https://arxiv.org/abs/2502.01735</link>
      <description>arXiv:2502.01735v1 Announce Type: cross 
Abstract: Monitored many-body systems can exhibit a phase transition between entangling and disentangling dynamical phases by tuning the strength of measurements made on the system as it evolves. This phenomenon is called the measurement-induced phase transition (MIPT). Understanding the properties of the MIPT is a prominent challenge for both theory and experiment at the intersection of many-body physics and quantum information. Realizing the MIPT experimentally is particularly challenging due to the postselection problem, which demands a number of experimental realizations that grows exponentially with the number of measurements made during the dynamics. Proposed approaches that circumvent the postselection problem typically rely on a classical decoding process that infers the final state based on the measurement record. But the complexity of this classical process generally also grows exponentially with the system size unless the dynamics is restricted to a fine-tuned set of unitary operators. In this work we overcome these difficulties. We construct a tree-shaped quantum circuit whose nodes are Haar-random unitary operators followed by weak measurements of tunable strength. For these circuits, we show that the MIPT can be detected without postselection using only a simple classical decoding process whose complexity grows linearly with the number of qubits. Our protocol exploits the recursive structure of tree circuits, which also enables a complete theoretical description of the MIPT, including an exact solution for its critical point and scaling behavior. We experimentally realize the MIPT on Quantinuum's H1-1 trapped-ion quantum computer and show that the experimental results are precisely described by theory. Our results close the gap between analytical theory and postselection-free experimental observation of the MIPT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01735v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaozhou Feng, Jeremy C\^ot\'e, Stefanos Kourtis, Brian Skinner</dc:creator>
    </item>
    <item>
      <title>Grokking vs. Learning: Same Features, Different Encodings</title>
      <link>https://arxiv.org/abs/2502.01739</link>
      <description>arXiv:2502.01739v1 Announce Type: cross 
Abstract: Grokking typically achieves similar loss to ordinary, "steady", learning. We ask whether these different learning paths - grokking versus ordinary training - lead to fundamental differences in the learned models. To do so we compare the features, compressibility, and learning dynamics of models trained via each path in two tasks. We find that grokked and steadily trained models learn the same features, but there can be large differences in the efficiency with which these features are encoded. In particular, we find a novel "compressive regime" of steady training in which there emerges a linear trade-off between model loss and compressibility, and which is absent in grokking. In this regime, we can achieve compression factors 25x times the base model, and 5x times the compression achieved in grokking. We then track how model features and compressibility develop through training. We show that model development in grokking is task-dependent, and that peak compressibility is achieved immediately after the grokking plateau. Finally, novel information-geometric measures are introduced which demonstrate that models undergoing grokking follow a straight path in information space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01739v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dmitry Manning-Coe, Jacopo Gliozzi, Alexander G. Stapleton, Edward Hirst, Giuseppe De Tomasi, Barry Bradlyn, David S. Berman</dc:creator>
    </item>
    <item>
      <title>Deep Linear Network Training Dynamics from Random Initialization: Data, Width, Depth, and Hyperparameter Transfer</title>
      <link>https://arxiv.org/abs/2502.02531</link>
      <description>arXiv:2502.02531v1 Announce Type: cross 
Abstract: We theoretically characterize gradient descent dynamics in deep linear networks trained at large width from random initialization and on large quantities of random data. Our theory captures the ``wider is better" effect of mean-field/maximum-update parameterized networks as well as hyperparameter transfer effects, which can be contrasted with the neural-tangent parameterization where optimal learning rates shift with model width. We provide asymptotic descriptions of both non-residual and residual neural networks, the latter of which enables an infinite depth limit when branches are scaled as $1/\sqrt{\text{depth}}$. We also compare training with one-pass stochastic gradient descent to the dynamics when training data are repeated at each iteration. Lastly, we show that this model recovers the accelerated power law training dynamics for power law structured data in the rich regime observed in recent works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02531v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blake Bordelon, Cengiz Pehlevan</dc:creator>
    </item>
    <item>
      <title>Optimal Spectral Transitions in High-Dimensional Multi-Index Models</title>
      <link>https://arxiv.org/abs/2502.02545</link>
      <description>arXiv:2502.02545v1 Announce Type: cross 
Abstract: We consider the problem of how many samples from a Gaussian multi-index model are required to weakly reconstruct the relevant index subspace. Despite its increasing popularity as a testbed for investigating the computational complexity of neural networks, results beyond the single-index setting remain elusive. In this work, we introduce spectral algorithms based on the linearization of a message passing scheme tailored to this problem. Our main contribution is to show that the proposed methods achieve the optimal reconstruction threshold. Leveraging a high-dimensional characterization of the algorithms, we show that above the critical threshold the leading eigenvector correlates with the relevant index subspace, a phenomenon reminiscent of the Baik-Ben Arous-Peche (BBP) transition in spiked models arising in random matrix theory. Supported by numerical experiments and a rigorous theoretical framework, our work bridges critical gaps in the computational limits of weak learnability in multi-index model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02545v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonardo Defilippis, Yatin Dandi, Pierre Mergny, Florent Krzakala, Bruno Loureiro</dc:creator>
    </item>
    <item>
      <title>Breakdown of random-matrix universality in persistent Lotka--Volterra communities</title>
      <link>https://arxiv.org/abs/2202.09140</link>
      <description>arXiv:2202.09140v3 Announce Type: replace 
Abstract: The eigenvalue spectrum of a random matrix often only depends on the first and second moments of its elements, but not on the specific distribution from which they are drawn. The validity of this universality principle is often assumed without proof in applications. In this letter, we offer a pertinent counterexample in the context of the generalised Lotka--Volterra equations. Using dynamic mean-field theory, we derive the statistics of the interactions between species in an evolved ecological community. We then show that the full statistics of these interactions, beyond those of a Gaussian ensemble, are required to correctly predict the eigenvalue spectrum and therefore stability. Consequently, the universality principle fails in this system. We thus show that the eigenvalue spectra of random matrices can be used to deduce the stability of `feasible' ecological communities, but only if the emergent non-Gaussian statistics of the interactions between species are taken into account.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.09140v3</guid>
      <category>cond-mat.dis-nn</category>
      <category>q-bio.PE</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevLett.130.137401</arxiv:DOI>
      <dc:creator>Joseph W. Baron, Thomas Jun Jewell, Christopher Ryder, Tobias Galla</dc:creator>
    </item>
    <item>
      <title>A path integral approach to sparse non-Hermitian random matrices</title>
      <link>https://arxiv.org/abs/2308.13605</link>
      <description>arXiv:2308.13605v3 Announce Type: replace 
Abstract: The theory of large random matrices has proved an invaluable tool for the study of systems with disordered interactions in many quite disparate research areas. Widely applicable results, such as the celebrated elliptic law for dense random matrices, allow one to deduce the statistical properties of the interactions in a complex dynamical system that permit stability. However, such simple and universal results have so far proved difficult to come by in the case of sparse random matrices. Here, we perform an expansion in the inverse connectivity, and thus derive general modified versions of the classic elliptic and semi-circle laws, taking into account the sparse correction. This is accomplished using a dynamical approach, which maps the hermitized resolvent of a random matrix onto the response functions of a linear dynamical system. The response functions are then evaluated using a path integral formalism, enabling one to construct Feynman diagrams, which facilitate the perturbative analysis. Additionally, in order to demonstrate the broad utility of the path integral framework, we derive a generic non-Hermitian generalization of the Marchenko-Pastur law, and we also show how one can handle non-negligible higher-order statistics (i.e. non-Gaussian statistics) in dense ensembles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13605v3</guid>
      <category>cond-mat.dis-nn</category>
      <category>q-bio.PE</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph W. Baron</dc:creator>
    </item>
    <item>
      <title>Explosive neural networks via higher-order interactions in curved statistical manifolds</title>
      <link>https://arxiv.org/abs/2408.02326</link>
      <description>arXiv:2408.02326v2 Announce Type: replace 
Abstract: Higher-order interactions underlie complex phenomena in systems such as biological and artificial neural networks, but their study is challenging due to the scarcity of tractable models. By leveraging a generalisation of the maximum entropy principle, here we introduce curved neural networks as a class of prototypical models with a limited number of parameters that are particularly well-suited for studying higher-order phenomena. Through exact mean-field descriptions, we show that these curved neural networks implement a self-regulating annealing process that can accelerate memory retrieval, leading to explosive order-disorder phase transitions with multi-stability and hysteresis effects. Moreover, by analytically exploring their memory-retrieval capacity using the replica trick near ferromagnetic and spin-glass phase boundaries, we demonstrate that these networks can enhance memory capacity and robustness of retrieval over classical associative-memory networks. Overall, the proposed framework provides parsimonious models amenable to analytical study, revealing novel higher-order phenomena in complex networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02326v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>nlin.AO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miguel Aguilera, Pablo A. Morales, Fernando E. Rosas, Hideaki Shimazaki</dc:creator>
    </item>
  </channel>
</rss>
