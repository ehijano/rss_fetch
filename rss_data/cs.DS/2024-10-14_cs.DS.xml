<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Oct 2024 03:30:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Parameterized Spanning Tree Congestion</title>
      <link>https://arxiv.org/abs/2410.08314</link>
      <description>arXiv:2410.08314v1 Announce Type: new 
Abstract: In this paper we study the Spanning Tree Congestion problem, where we are given a graph $G=(V,E)$ and are asked to find a spanning tree $T$ of minimum maximum congestion. Here, the congestion of an edge $e\in T$ is the number of edges $uv\in E$ such that the (unique) path from $u$ to $v$ in $T$ traverses $e$. We consider this well-studied NP-hard problem from the point of view of (structural) parameterized complexity and obtain the following results.
  We resolve a natural open problem by showing that Spanning Tree Congestion is not FPT parameterized by treewidth (under standard assumptions). More strongly, we present a generic reduction which applies to (almost) any parameter of the form ``vertex-deletion distance to class $\mathcal{C}$'', thus obtaining W[1]-hardness for parameters more restricted than treewidth, including tree-depth plus feedback vertex set, or incomparable to treewidth, such as twin cover. Via a slight tweak of the same reduction we also show that the problem is NP-complete on graphs of modular-width $4$.
  Even though it is known that Spanning Tree Congestion remains NP-hard on instances with only one vertex of unbounded degree, it is currently open whether the problem remains hard on bounded-degree graphs. We resolve this question by showing NP-hardness on graphs of maximum degree 8.
  Complementing the problem's W[1]-hardness for treewidth...</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08314v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lampis, Valia Mitsou, Edouard Nemery, Yota Otachi, Manolis Vasilakis, Daniel Vaz</dc:creator>
    </item>
    <item>
      <title>Subgraph Counting in Subquadratic Time for Bounded Degeneracy Graphs</title>
      <link>https://arxiv.org/abs/2410.08376</link>
      <description>arXiv:2410.08376v1 Announce Type: new 
Abstract: We study the classic problem of subgraph counting, where we wish to determine the number of occurrences of a fixed pattern graph $H$ in an input graph $G$ of $n$ vertices. Our focus is on \emph{bounded degeneracy} inputs, a rich class of graphs that also characterizes real-world massive networks. Building on the seminal techniques introduced by Chiba-Nishizeki (SICOMP 1985), a recent line of work has built subgraph counting algorithms for bounded degeneracy graphs. Assuming fine-grained complexity conjectures, there is a complete characterization of patterns $H$ for which linear time subgraph counting is possible. For every $r \geq 6$, there exists an $H$ with $r$ vertices that cannot be counted in linear time.
  In this paper, we initiate a study of subquadratic algorithms for subgraph counting on bounded degeneracy graphs. We prove that when $H$ has at most $7$ vertices, then subgraph counting can be done in $O(n^{1.41})$ time. Moreover, if this running time can be beaten, it would imply faster algorithms for triangle counting on arbitrary graphs (a bound that has stood for two decades). We also prove that all cycles of length at most $10$ can be counted in $O(n^{1.63})$ time for bounded degeneracy graphs.
  Previously, no subquadratic (and superlinear) algorithms were known for subgraph counting on bounded degeneracy graphs. Moreover, if the input is an arbitrary (sparse) graph, the best known algorithms for the above problems run in at least cubic time.
  Our main conceptual contribution is a framework that reduces subgraph counting in bounded degeneracy graphs to counting smaller cycles in arbitrary graphs. We believe that our results will help build a general theory of subgraph counting for bounded degeneracy graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08376v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Paul-Pena, C. Seshadhri</dc:creator>
    </item>
    <item>
      <title>A Simple yet Exact Analysis of the MultiQueue</title>
      <link>https://arxiv.org/abs/2410.08714</link>
      <description>arXiv:2410.08714v1 Announce Type: new 
Abstract: The MultiQueue is a relaxed concurrent priority queue consisting of $n$ internal priority queues, where an insertion uses a random queue and a deletion considers two random queues and deletes the minimum from the one with the smaller minimum. The rank error of the deletion is the number of smaller elements in the MultiQueue.
  Alistarh et al. [2] have demonstrated in a sophisticated potential argument that the expected rank error remains bounded by $O(n)$ over long sequences of deletions.
  In this paper we present a simpler analysis by identifying the stable distribution of an underlying Markov chain and with it the long-term distribution of the rank error exactly. Simple calculations then reveal the expected long-term rank error to be $\tfrac{5}{6}n-1+\tfrac{1}{6n}$. Our arguments generalize to deletion schemes where the probability to delete from a given queue depends only on the rank of the queue. Specifically, this includes deleting from the best of $c$ randomly selected queues for any $c&gt;1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08714v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefan Walzer, Marvin Williams</dc:creator>
    </item>
    <item>
      <title>Grand-children weight-balanced binary search trees</title>
      <link>https://arxiv.org/abs/2410.08825</link>
      <description>arXiv:2410.08825v1 Announce Type: new 
Abstract: We revisit weight-balanced trees, also known as trees of bounded balance. This class of binary search trees was invented by Nievergelt and Reingold in 1972. Such trees are obtained by assigning a weight to each node and requesting that the weight of each node should be quite larger than the weights of its children, the precise meaning of ``quite larger'' depending on a real-valued parameter~$\gamma$. Blum and Mehlhorn then showed how to maintain these trees in a recursive (bottom-up) fashion when~$2/11 \leqslant \gamma \leqslant 1-1/\sqrt{2}$, their algorithm requiring only an amortised constant number of tree rebalancing operations per update (insertion or deletion). Later, in 1993, Lai and Wood proposed a top-down procedure for updating these trees when~$2/11 \leqslant \gamma \leqslant 1/4$.
  Our contribution is two-fold. First, we strengthen the requirements of Nievergelt and Reingold, by also requesting that each node should have a substantially larger weight than its grand-children, thereby obtaining what we call grand-children balanced trees. Grand-children balanced trees are not harder to maintain than weight-balanced trees, but enjoy a smaller node depth, both in the worst case (with a 6~\% decrease) and on average (with a 1.6~\% decrease). In particular, unlike standard weight-balanced trees, all grand-children balanced trees with $n$ nodes are of height less than $2 \log_2(n)$.
  Second, we adapt the algorithm of Lai and Wood to all weight-balanced trees, i.e., to all parameter values~$\gamma$ such that~$2/11 \leqslant \gamma \leqslant 1-1/\sqrt{2}$. More precisely, we adapt it to all grand-children balanced trees for which~$1/4 &lt; \gamma \leqslant 1 - 1/\sqrt{2}$. Finally, we show that, except in critical cases, all these algorithms result in making a constant amortised number of tree rebalancing operations per tree update.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08825v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Jug\'e</dc:creator>
    </item>
    <item>
      <title>On Wagner's k-Tree Algorithm Over Integers</title>
      <link>https://arxiv.org/abs/2410.06856</link>
      <description>arXiv:2410.06856v2 Announce Type: cross 
Abstract: The k-Tree algorithm [Wagner 02] is a non-trivial algorithm for the average-case k-SUM problem that has found widespread use in cryptanalysis. Its input consists of k lists, each containing n integers from a range of size m. Wagner's original heuristic analysis suggested that this algorithm succeeds with constant probability if n = m^{1/(\log{k}+1)}, and that in this case it runs in time O(kn). Subsequent rigorous analysis of the algorithm [Lyubashevsky 05, Shallue 08, Joux-Kippen-Loss 24] has shown that it succeeds with high probability if the input list sizes are significantly larger than this.
  We present a broader rigorous analysis of the k-Tree algorithm, showing upper and lower bounds on its success probability and complexity for any size of the input lists. Our results confirm Wagner's heuristic conclusions, and also give meaningful bounds for a wide range of list sizes that are not covered by existing analyses. We present analytical bounds that are asymptotically tight, as well as an efficient algorithm that computes (provably correct) bounds for a wide range of concrete parameter settings. We also do the same for the k-Tree algorithm over Z_m. Finally, we present experimental evaluation of the tightness of our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06856v2</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Haoxing Lin, Prashant Nalini Vasudevan</dc:creator>
    </item>
    <item>
      <title>Utilizing ChatGPT in a Data Structures and Algorithms Course: A Teaching Assistant's Perspective</title>
      <link>https://arxiv.org/abs/2410.08899</link>
      <description>arXiv:2410.08899v1 Announce Type: cross 
Abstract: Integrating large language models (LLMs) like ChatGPT is revolutionizing the field of computer science education. These models offer new possibilities for enriching student learning and supporting teaching assistants (TAs) in providing prompt feedback and supplementary learning resources. This research delves into the use of ChatGPT in a data structures and algorithms (DSA) course, particularly when combined with TA supervision. The findings demonstrate that incorporating ChatGPT with structured prompts and active TA guidance enhances students' understanding of intricate algorithmic concepts, boosts engagement, and elevates academic performance. However, challenges exist in addressing academic integrity and the limitations of LLMs in tackling complex problems. The study underscores the importance of active TA involvement in reducing students' reliance on AI-generated content and amplifying the overall educational impact. The results suggest that while LLMs can be advantageous for education, their successful integration demands continuous oversight and a thoughtful balance between AI and human guidance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08899v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pooriya Jamie, Reyhaneh Hajihashemi, Sharareh Alipour</dc:creator>
    </item>
    <item>
      <title>Top-Down Drawings of Compound Graphs</title>
      <link>https://arxiv.org/abs/2312.07319</link>
      <description>arXiv:2312.07319v2 Announce Type: replace 
Abstract: Bottom-up layout algorithms for compound graphs are suitable for presenting the microscale view of models and are often used in model-driven engineering. However, they have difficulties at the macroscale where maintaining the overview of large models becomes challenging. We propose top-down layout, which utilizes scale to hide low-level details at high zoom levels. The entire high-level view can fit into the viewport and remain readable, while the ability to zoom in to see the details is still maintained. Top-down layout is an abstract high-level layout process that can be used in conjunction with classic layout algorithms to produce visually compelling and readable diagrams of large compound graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07319v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maximilian Kasperowski, Reinhard von Hanxleden</dc:creator>
    </item>
    <item>
      <title>Low-complexity Image and Video Coding Based on an Approximate Discrete Tchebichef Transform</title>
      <link>https://arxiv.org/abs/1609.07630</link>
      <description>arXiv:1609.07630v4 Announce Type: replace-cross 
Abstract: The usage of linear transformations has great relevance for data decorrelation applications, like image and video compression. In that sense, the discrete Tchebichef transform (DTT) possesses useful coding and decorrelation properties. The DTT transform kernel does not depend on the input data and fast algorithms can be developed to real time applications. However, the DTT fast algorithm presented in literature possess high computational complexity. In this work, we introduce a new low-complexity approximation for the DTT. The fast algorithm of the proposed transform is multiplication-free and requires a reduced number of additions and bit-shifting operations. Image and video compression simulations in popular standards shows good performance of the proposed transform. Regarding hardware resource consumption for FPGA shows 43.1% reduction of configurable logic blocks and ASIC place and route realization shows 57.7% reduction in the area-time figure when compared with the 2-D version of the exact DTT.</description>
      <guid isPermaLink="false">oai:arXiv.org:1609.07630v4</guid>
      <category>cs.MM</category>
      <category>cs.CV</category>
      <category>cs.DS</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCSVT.2016.2515378</arxiv:DOI>
      <dc:creator>P. A. M. Oliveira, R. J. Cintra, F. M. Bayer, S. Kulasekera, A. Madanayake, V. A. Coutinho</dc:creator>
    </item>
    <item>
      <title>Calibration Error for Decision Making</title>
      <link>https://arxiv.org/abs/2404.13503</link>
      <description>arXiv:2404.13503v5 Announce Type: replace-cross 
Abstract: Calibration allows predictions to be reliably interpreted as probabilities by decision makers. We propose a decision-theoretic calibration error, the Calibration Decision Loss (CDL), defined as the maximum improvement in decision payoff obtained by calibrating the predictions, where the maximum is over all payoff-bounded decision tasks. Vanishing CDL guarantees the payoff loss from miscalibration vanishes simultaneously for all downstream decision tasks. We show separations between CDL and existing calibration error metrics, including the most well-studied metric Expected Calibration Error (ECE). Our main technical contribution is a new efficient algorithm for online calibration that achieves near-optimal $O(\frac{\log T}{\sqrt{T}})$ expected CDL, bypassing the $\Omega(T^{-0.472})$ lower bound for ECE by Qiao and Valiant (2021).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13503v5</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lunjia Hu, Yifan Wu</dc:creator>
    </item>
  </channel>
</rss>
