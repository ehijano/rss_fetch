<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MA</link>
    <description>cs.MA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Feb 2025 02:57:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Low-Rank Agent-Specific Adaptation (LoRASA) for Multi-Agent Policy Learning</title>
      <link>https://arxiv.org/abs/2502.05573</link>
      <description>arXiv:2502.05573v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) often relies on \emph{parameter sharing (PS)} to scale efficiently. However, purely shared policies can stifle each agent's unique specialization, reducing overall performance in heterogeneous environments. We propose \textbf{Low-Rank Agent-Specific Adaptation (LoRASA)}, a novel approach that treats each agent's policy as a specialized ``task'' fine-tuned from a shared backbone. Drawing inspiration from parameter-efficient transfer methods, LoRASA appends small, low-rank adaptation matrices to each layer of the shared policy, naturally inducing \emph{parameter-space sparsity} that promotes both specialization and scalability. We evaluate LoRASA on challenging benchmarks including the StarCraft Multi-Agent Challenge (SMAC) and Multi-Agent MuJoCo (MAMuJoCo), implementing it atop widely used algorithms such as MAPPO and A2PO. Across diverse tasks, LoRASA matches or outperforms existing baselines \emph{while reducing memory and computational overhead}. Ablation studies on adapter rank, placement, and timing validate the method's flexibility and efficiency. Our results suggest LoRASA's potential to establish a new norm for MARL policy parameterization: combining a shared foundation for coordination with low-rank agent-specific refinements for individual specialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05573v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beining Zhang, Aditya Kapoor, Mingfei Sun</dc:creator>
    </item>
    <item>
      <title>Kalman Filter-Based Distributed Gaussian Process for Unknown Scalar Field Estimation in Wireless Sensor Networks</title>
      <link>https://arxiv.org/abs/2502.05802</link>
      <description>arXiv:2502.05802v1 Announce Type: new 
Abstract: In this letter, we propose an online scalar field estimation algorithm of unknown environments using a distributed Gaussian process (DGP) framework in wireless sensor networks (WSNs). While the kernel-based Gaussian process (GP) has been widely employed for estimating unknown scalar fields, its centralized nature is not well-suited for handling a large amount of data from WSNs. To overcome the limitations of the kernel-based GP, recent advancements in GP research focus on approximating kernel functions as products of E-dimensional nonlinear basis functions, which can handle large WSNs more efficiently in a distributed manner. However, this approach requires a large number of basis functions for accurate approximation, leading to increased computational and communication complexities. To address these complexity issues, the paper proposes a distributed GP framework by incorporating a Kalman filter scheme (termed as K-DGP), which scales linearly with the number of nonlinear basis functions. Moreover, we propose a new consensus protocol designed to handle the unique data transmission requirement residing in the proposed K-DGP framework. This protocol preserves the inherent elements in the form of a certain column in the nonlinear function matrix of the communicated message; it enables wireless sensors to cooperatively estimate the environment and reach the global consensus through distributed learning with faster convergence than the widely-used average consensus protocol. Simulation results demonstrate rapid consensus convergence and outstanding estimation accuracy achieved by the proposed K-DGP algorithm. The scalability and efficiency of the proposed approach are further demonstrated by online dynamic environment estimation using WSNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05802v1</guid>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jaemin Seo, Geunsik Bae, Hyondong Oh</dc:creator>
    </item>
    <item>
      <title>Optimization under Attack: Resilience, Vulnerability, and the Path to Collapse</title>
      <link>https://arxiv.org/abs/2502.05954</link>
      <description>arXiv:2502.05954v1 Announce Type: new 
Abstract: Optimization is instrumental for improving operations of large-scale socio-technical infrastructures of Smart Cities, for instance, energy and traffic systems. In particular, understanding the performance of multi-agent discrete-choice combinatorial optimization under distributed adversary attacks is a compelling and underexplored problem, since multi-agent systems exhibit a large number of remote control variables that can influence in an unprecedented way the cost-effectiveness of distributed optimization heuristics. This paper unravels for the first time the trajectories of distributed optimization from resilience to vulnerability, and finally to collapse under varying adversary influence. Using real-world data to emulate over 28 billion multi-agent optimization scenarios, we exhaustively assess how the number of agents with different adversarial severity and network positioning influences optimization performance, including the influence on Pareto optimal points. With this novel large-scale dataset, made openly available as a benchmark, we disentangle how optimization remains resilient to adversaries and which adversary conditions are required to make optimization vulnerable or collapsed. These new findings can provide new insights for designing self-healing strategies for fault-tolerance and fault-correction in adversarial distributed optimization that have been missing so far.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05954v1</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amal Aldawsari, Evangelos Pournaras</dc:creator>
    </item>
    <item>
      <title>Enhancing healthcare infrastructure resilience through agent-based simulation methods</title>
      <link>https://arxiv.org/abs/2502.06636</link>
      <description>arXiv:2502.06636v1 Announce Type: new 
Abstract: Critical infrastructures face demanding challenges due to natural and human-generated threats, such as pandemics, workforce shortages or cyber-attacks, which might severely compromise service quality. To improve system resilience, decision-makers would need intelligent tools for quick and efficient resource allocation. This article explores an agent-based simulation model that intends to capture a part of the complexity of critical infrastructure systems, particularly considering the interdependencies of healthcare systems with information and telecommunication systems. Such a model enables to implement a simulation-based optimization approach in which the exposure of critical systems to risks is evaluated, while comparing the mitigation effects of multiple tactical and strategical decision alternatives to enhance their resilience. The proposed model is designed to be parameterizable, to enable adapting it to risk scenarios with different severity, and it facilitates the compilation of relevant performance indicators enabling monitoring at both agent level and system level. To validate the agent-based model, a literature-supported methodology has been used to perform cross-validation, sensitivity analysis and test the usefulness of the proposed model through a use case. The use case analyzes the impact of a concurrent pandemic and a cyber-attack on a hospital and compares different resiliency-enhancing countermeasures using contingency tables. Overall, the use case illustrates the feasibility and versatility of the proposed approach to enhance resiliency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06636v1</guid>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.comcom.2025.108070</arxiv:DOI>
      <arxiv:journal_reference>Computer Communications, vol. 234, p. 108070, mar. 2025</arxiv:journal_reference>
      <dc:creator>David Carrami\~nana, Ana M. Bernardos, Juan A. Besada, Jos\'e R. Casar</dc:creator>
    </item>
    <item>
      <title>Optimizing Wealth by a Game within Cellular Automata</title>
      <link>https://arxiv.org/abs/2502.05246</link>
      <description>arXiv:2502.05246v1 Announce Type: cross 
Abstract: The objective is to find a Cellular Automata (CA) rule that can evolve 2D patterns that are optimal with respect to a global fitness function. The global fitness is defined as the sum of local computed utilities. A utility or value function computes a score depending on the states in the local neighborhood. First the method is explained that was followed to find such a CA rule. Then this method is applied to find a rule that maximizes social wealth. Here wealth is defined as the sum of the payoffs that all players (agents, cells) receive in a prisoner's dilemma game, and then shared equally among them. The problem is solved in four steps: (0) Defining the utility function, (1) Finding optimal master patterns with a Genetic Algorithm, (2) Extracting templates (local neighborhood configurations), (3) Inserting the templates in a general CA rule. The constructed CA rule finds optimal and near-optimal patterns for even and odd grid sizes. Optimal patterns of odd size contain exactly one singularity, a 2 x 2 block of cooperators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05246v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rolf Hoffmann, Franciszek Seredy\'nski, Dominique D\'es\'erable</dc:creator>
    </item>
    <item>
      <title>Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires</title>
      <link>https://arxiv.org/abs/2502.05248</link>
      <description>arXiv:2502.05248v1 Announce Type: cross 
Abstract: Psychological assessment tools have long helped humans understand behavioural patterns. While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits. To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles. Using established trait-based questionnaires such as the Big Five Inventory and by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs across five core personality dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Our findings reveal that LLMs exhibit unique dominant traits, varying characteristics, and distinct personality profiles even within the same family of models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05248v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3701716.3715504</arxiv:DOI>
      <dc:creator>Pranav Bhandari, Usman Naseem, Amitava Datta, Nicolas Fay, Mehwish Nasim</dc:creator>
    </item>
    <item>
      <title>Decentralized Online Ensembles of Gaussian Processes for Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2502.05301</link>
      <description>arXiv:2502.05301v1 Announce Type: cross 
Abstract: Flexible and scalable decentralized learning solutions are fundamentally important in the application of multi-agent systems. While several recent approaches introduce (ensembles of) kernel machines in the distributed setting, Bayesian solutions are much more limited. We introduce a fully decentralized, asymptotically exact solution to computing the random feature approximation of Gaussian processes. We further address the choice of hyperparameters by introducing an ensembling scheme for Bayesian multiple kernel learning based on online Bayesian model averaging. The resulting algorithm is tested against Bayesian and frequentist methods on simulated and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05301v1</guid>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>eess.SP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fernando Llorente, Daniel Waxman, Petar M. Djuri\'c</dc:creator>
    </item>
    <item>
      <title>ITBench: Evaluating AI Agents across Diverse Real-World IT Automation Tasks</title>
      <link>https://arxiv.org/abs/2502.05352</link>
      <description>arXiv:2502.05352v1 Announce Type: cross 
Abstract: Realizing the vision of using AI agents to automate critical IT tasks depends on the ability to measure and understand effectiveness of proposed solutions. We introduce ITBench, a framework that offers a systematic methodology for benchmarking AI agents to address real-world IT automation tasks. Our initial release targets three key areas: Site Reliability Engineering (SRE), Compliance and Security Operations (CISO), and Financial Operations (FinOps). The design enables AI researchers to understand the challenges and opportunities of AI agents for IT automation with push-button workflows and interpretable metrics. ITBench includes an initial set of 94 real-world scenarios, which can be easily extended by community contributions. Our results show that agents powered by state-of-the-art models resolve only 13.8% of SRE scenarios, 25.2% of CISO scenarios, and 0% of FinOps scenarios. We expect ITBench to be a key enabler of AI-driven IT automation that is correct, safe, and fast.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05352v1</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Saurabh Jha (IBM), Rohan Arora (IBM), Yuji Watanabe (IBM), Takumi Yanagawa (IBM), Yinfang Chen (University of Illinois at Urbana-Champaign), Jackson Clark (University of Illinois at Urbana-Champaign), Bhavya Bhavya (IBM), Mudit Verma (IBM), Harshit Kumar (IBM), Hirokuni Kitahara (IBM), Noah Zheutlin (IBM), Saki Takano (IBM), Divya Pathak (IBM), Felix George (IBM), Xinbo Wu (University of Illinois at Urbana-Champaign), Bekir O. Turkkan (IBM), Gerard Vanloo (IBM), Michael Nidd (IBM), Ting Dai (IBM), Oishik Chatterjee (IBM), Pranjal Gupta (IBM), Suranjana Samanta (IBM), Pooja Aggarwal (IBM), Rong Lee (IBM), Pavankumar Murali (IBM), Jae-wook Ahn (IBM), Debanjana Kar (IBM), Ameet Rahane (IBM), Carlos Fonseca (IBM), Amit Paradkar (IBM), Yu Deng (IBM), Pratibha Moogi (IBM), Prateeti Mohapatra (IBM), Naoki Abe (IBM), Chandrasekhar Narayanaswami (IBM), Tianyin Xu (University of Illinois at Urbana-Champaign), Lav R. Varshney (University of Illinois at Urbana-Champaign), Ruchi Mahindru (IBM), Anca Sailer (IBM), Laura Shwartz (IBM), Daby Sow (IBM), Nicholas C. M. Fuller (IBM), Ruchir Puri (IBM)</dc:creator>
    </item>
    <item>
      <title>LLM-Powered Decentralized Generative Agents with Adaptive Hierarchical Knowledge Graph for Cooperative Planning</title>
      <link>https://arxiv.org/abs/2502.05453</link>
      <description>arXiv:2502.05453v1 Announce Type: cross 
Abstract: Developing intelligent agents for long-term cooperation in dynamic open-world scenarios is a major challenge in multi-agent systems. Traditional Multi-agent Reinforcement Learning (MARL) frameworks like centralized training decentralized execution (CTDE) struggle with scalability and flexibility. They require centralized long-term planning, which is difficult without custom reward functions, and face challenges in processing multi-modal data. CTDE approaches also assume fixed cooperation strategies, making them impractical in dynamic environments where agents need to adapt and plan independently. To address decentralized multi-agent cooperation, we propose Decentralized Adaptive Knowledge Graph Memory and Structured Communication System (DAMCS) in a novel Multi-agent Crafter environment. Our generative agents, powered by Large Language Models (LLMs), are more scalable than traditional MARL agents by leveraging external knowledge and language for long-term planning and reasoning. Instead of fully sharing information from all past experiences, DAMCS introduces a multi-modal memory system organized as a hierarchical knowledge graph and a structured communication protocol to optimize agent cooperation. This allows agents to reason from past interactions and share relevant information efficiently. Experiments on novel multi-agent open-world tasks show that DAMCS outperforms both MARL and LLM baselines in task efficiency and collaboration. Compared to single-agent scenarios, the two-agent scenario achieves the same goal with 63% fewer steps, and the six-agent scenario with 74% fewer steps, highlighting the importance of adaptive memory and structured communication in achieving long-term goals. We publicly release our project at: https://happyeureka.github.io/damcs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05453v1</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanqing Yang, Jingdi Chen, Marie Siew, Tania Lorido-Botran, Carlee Joe-Wong</dc:creator>
    </item>
    <item>
      <title>Motion Planning of Nonholonomic Cooperative Mobile Manipulators</title>
      <link>https://arxiv.org/abs/2502.05462</link>
      <description>arXiv:2502.05462v1 Announce Type: cross 
Abstract: We propose a real-time implementable motion planning technique for cooperative object transportation by nonholonomic mobile manipulator robots (MMRs) in an environment with static and dynamic obstacles. The proposed motion planning technique works in two steps. A novel visibility vertices-based path planning algorithm computes a global piece-wise linear path between the start and the goal location in the presence of static obstacles offline. It defines the static obstacle free space around the path with a set of convex polygons for the online motion planner. We employ a Nonliner Model Predictive Control (NMPC) based online motion planning technique for nonholonomic MMRs that jointly plans for the mobile base and the manipulators arm. It efficiently utilizes the locomotion capability of the mobile base and the manipulation capability of the arm. The motion planner plans feasible motion for the MMRs and generates trajectory for object transportation considering the kinodynamic constraints and the static and dynamic obstacles. The efficiency of our approach is validated by numerical simulation and hardware experiments in varied environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05462v1</guid>
      <category>cs.RO</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keshab Patra, Arpita Sinha, Anirban Guha</dc:creator>
    </item>
    <item>
      <title>Riemannian Manifold Learning for Stackelberg Games with Neural Flow Representations</title>
      <link>https://arxiv.org/abs/2502.05498</link>
      <description>arXiv:2502.05498v1 Announce Type: cross 
Abstract: We present a novel framework for online learning in Stackelberg general-sum games, where two agents, the leader and follower, engage in sequential turn-based interactions. At the core of this approach is a learned diffeomorphism that maps the joint action space to a smooth Riemannian manifold, referred to as the Stackelberg manifold. This mapping, facilitated by neural normalizing flows, ensures the formation of tractable isoplanar subspaces, enabling efficient techniques for online learning. By assuming linearity between the agents' reward functions on the Stackelberg manifold, our construct allows the application of standard bandit algorithms. We then provide a rigorous theoretical basis for regret minimization on convex manifolds and establish finite-time bounds on simple regret for learning Stackelberg equilibria. This integration of manifold learning into game theory uncovers a previously unrecognized potential for neural normalizing flows as an effective tool for multi-agent learning. We present empirical results demonstrating the effectiveness of our approach compared to standard baselines, with applications spanning domains such as cybersecurity and economic supply chain optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05498v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Larkin Liu, Kashif Rasul, Yutong Chao, Jalal Etesami</dc:creator>
    </item>
    <item>
      <title>Towards Learning Scalable Agile Dynamic Motion Planning for Robosoccer Teams with Policy Optimization</title>
      <link>https://arxiv.org/abs/2502.05526</link>
      <description>arXiv:2502.05526v1 Announce Type: cross 
Abstract: In fast-paced, ever-changing environments, dynamic Motion Planning for Multi-Agent Systems in the presence of obstacles is a universal and unsolved problem. Be it from path planning around obstacles to the movement of robotic arms, or in planning navigation of robot teams in settings such as Robosoccer, dynamic motion planning is needed to avoid collisions while reaching the targeted destination when multiple agents occupy the same area. In continuous domains where the world changes quickly, existing classical Motion Planning algorithms such as RRT* and A* become computationally expensive to rerun at every time step. Many variations of classical and well-formulated non-learning path-planning methods have been proposed to solve this universal problem but fall short due to their limitations of speed, smoothness, optimally, etc. Deep Learning models overcome their challenges due to their ability to adapt to varying environments based on past experience. However, current learning motion planning models use discretized environments, do not account for heterogeneous agents or replanning, and build up to improve the classical motion planners' efficiency, leading to issues with scalability. To prevent collisions between heterogenous team members and collision to obstacles while trying to reach the target location, we present a learning-based dynamic navigation model and show our model working on a simple environment in the concept of a simple Robosoccer Game.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05526v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brandon Ho, Batuhan Altundas, Matthew Gombolay</dc:creator>
    </item>
    <item>
      <title>Fairness Driven Slot Allocation Problem in Billboard Advertisement</title>
      <link>https://arxiv.org/abs/2502.05851</link>
      <description>arXiv:2502.05851v1 Announce Type: cross 
Abstract: In billboard advertisement, a number of digital billboards are owned by an influence provider, and several commercial houses (which we call advertisers) approach the influence provider for a specific number of views of their advertisement content on a payment basis. Though the billboard slot allocation problem has been studied in the literature, this problem still needs to be addressed from a fairness point of view. In this paper, we introduce the Fair Billboard Slot Allocation Problem, where the objective is to allocate a given set of billboard slots among a group of advertisers based on their demands fairly and efficiently. As fairness criteria, we consider the maximin fair share, which ensures that each advertiser will receive a subset of slots that maximizes the minimum share for all the advertisers. We have proposed a solution approach that generates an allocation and provides an approximate maximum fair share. The proposed methodology has been analyzed to understand its time and space requirements and a performance guarantee. It has been implemented with real-world trajectory and billboard datasets, and the results have been reported. The results show that the proposed approach leads to a balanced allocation by satisfying the maximin fairness criteria. At the same time, it maximizes the utility of advertisers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05851v1</guid>
      <category>cs.GT</category>
      <category>cs.DB</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dildar Ali, Suman Banerjee, Shweta Jain, Yamuna Prasad</dc:creator>
    </item>
    <item>
      <title>Barriers and Pathways to Human-AI Alignment: A Game-Theoretic Approach</title>
      <link>https://arxiv.org/abs/2502.05934</link>
      <description>arXiv:2502.05934v1 Announce Type: cross 
Abstract: Under what conditions can capable AI agents efficiently align their actions with human preferences? More specifically, when they are proficient enough to collaborate with us, how long does coordination take, and when is it computationally feasible? These foundational questions of AI alignment help define what makes an AI agent ``sufficiently safe'' and valuable to humans. Since such generally capable systems do not yet exist, a theoretical analysis is needed to establish when guarantees hold -- and what they even are.
  We introduce a game-theoretic framework that generalizes prior alignment approaches with fewer assumptions, allowing us to analyze the computational complexity of alignment across $M$ objectives and $N$ agents, providing both upper and lower bounds. Unlike previous work, which often assumes common priors, idealized communication, or implicit tractability, our framework formally characterizes the difficulty of alignment under minimal assumptions.
  Our main result shows that even when agents are fully rational and computationally \emph{unbounded}, alignment can be achieved with high probability in time \emph{linear} in the task space size. Therefore, in real-world settings, where task spaces are often \emph{exponential} in input length, this remains impractical. More strikingly, our lower bound demonstrates that alignment is \emph{impossible} to speed up when scaling to exponentially many tasks or agents, highlighting a fundamental computational barrier to scalable alignment.
  Relaxing these idealized assumptions, we study \emph{computationally bounded} agents with noisy messages (representing obfuscated intent), showing that while alignment can still succeed with high probability, it incurs additional \emph{exponential} slowdowns in the task space size, number of agents, and number of tasks.
  We conclude by identifying conditions that make alignment more feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05934v1</guid>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aran Nayebi</dc:creator>
    </item>
    <item>
      <title>Preventing Rogue Agents Improves Multi-Agent Collaboration</title>
      <link>https://arxiv.org/abs/2502.05986</link>
      <description>arXiv:2502.05986v1 Announce Type: cross 
Abstract: Multi-agent systems, where specialized agents collaborate to solve a shared task hold great potential, from increased modularity to simulating complex environments. However, they also have a major caveat -- a single agent can cause the entire system to fail. Consider a simple game where the knowledge to solve the task is distributed between agents, which share information in a communication channel. At each round, any of the agents can terminate the game and make the final prediction, even if they are uncertain about the outcome of their action. Detection of such rogue agents $\textit{before they act}$ may prevent the system's failure. In this work, we propose to $\textit{monitor}$ agents during action prediction and $\textit{intervene}$ when a future error is likely to occur. To test our approach, we introduce WhoDunitEnv, a multi-agent collaboration environment that allows modular control over task complexity and communication structure. Experiments on two variants of WhoDunitEnv and the GovSim environment for resource sustainability show that our approach leads to substantial performance gains up to 17.4% and 20%, respectively. Moreover, a thorough analysis shows that our monitors successfully identify critical points of agent confusion and our interventions effectively stop agent errors from propagating.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05986v1</guid>
      <category>cs.CL</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ohav Barbi, Ori Yoran, Mor Geva</dc:creator>
    </item>
    <item>
      <title>Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2502.06060</link>
      <description>arXiv:2502.06060v1 Announce Type: cross 
Abstract: Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent's goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model's listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model's speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at https://socialdeductionllm.github.io/</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06060v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bidipta Sarkar, Warren Xia, C. Karen Liu, Dorsa Sadigh</dc:creator>
    </item>
    <item>
      <title>SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding</title>
      <link>https://arxiv.org/abs/2502.06440</link>
      <description>arXiv:2502.06440v1 Announce Type: cross 
Abstract: The Multi-Agent Path Finding (MAPF) problem aims to determine the shortest and collision-free paths for multiple agents in a known, potentially obstacle-ridden environment. It is the core challenge for robotic deployments in large-scale logistics and transportation. Decentralized learning-based approaches have shown great potential for addressing the MAPF problems, offering more reactive and scalable solutions. However, existing learning-based MAPF methods usually rely on agents making decisions based on a limited field of view (FOV), resulting in short-sighted policies and inefficient cooperation in complex scenarios. There, a critical challenge is to achieve consensus on potential movements between agents based on limited observations and communications. To tackle this challenge, we introduce a new framework that applies sheaf theory to decentralized deep reinforcement learning, enabling agents to learn geometric cross-dependencies between each other through local consensus and utilize them for tightly cooperative decision-making. In particular, sheaf theory provides a mathematical proof of conditions for achieving global consensus through local observation. Inspired by this, we incorporate a neural network to approximately model the consensus in latent space based on sheaf theory and train it through self-supervised learning. During the task, in addition to normal features for MAPF as in previous works, each agent distributedly reasons about a learned consensus feature, leading to efficient cooperation on pathfinding and collision avoidance. As a result, our proposed method demonstrates significant improvements over state-of-the-art learning-based MAPF planners, especially in relatively large and complex scenarios, demonstrating its superiority over baselines in various simulations and real-world robot experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06440v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhao Liao, Weihang Xia, Yuhong Cao, Weiheng Dai, Chengyang He, Wenjun Wu, Guillaume Sartoretti</dc:creator>
    </item>
    <item>
      <title>Learning to Communicate Through Implicit Communication Channels</title>
      <link>https://arxiv.org/abs/2411.01553</link>
      <description>arXiv:2411.01553v2 Announce Type: replace 
Abstract: Effective communication is an essential component in collaborative multi-agent systems. Situations where explicit messaging is not feasible have been common in human society throughout history, which motivate the study of implicit communication. Previous works on learning implicit communication mostly rely on theory of mind (ToM), where agents infer the mental states and intentions of others by interpreting their actions. However, ToM-based methods become less effective in making accurate inferences in complex tasks. In this work, we propose the Implicit Channel Protocol (ICP) framework, which allows agents to communicate through implicit communication channels similar to the explicit ones. ICP leverages a subset of actions, denoted as the scouting actions, and a mapping between information and these scouting actions that encodes and decodes the messages. We propose training algorithms for agents to message and act, including learning with a randomly initialized information map and with a delayed information map. The efficacy of ICP has been tested on the tasks of Guessing Numbers, Revealing Goals, and Hanabi, where ICP significantly outperforms baseline methods through more efficient information transmission.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01553v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Wang, Binbin Chen, Tieying Zhang, Baoxiang Wang</dc:creator>
    </item>
    <item>
      <title>Free Agent in Agent-Based Mixture-of-Experts Generative AI Framework</title>
      <link>https://arxiv.org/abs/2501.17903</link>
      <description>arXiv:2501.17903v2 Announce Type: replace 
Abstract: Multi-agent systems commonly distribute tasks among specialized, autonomous agents, yet they often lack mechanisms to replace or reassign underperforming agents in real time. Inspired by the free-agency model of Major League Baseball, the Reinforcement Learning Free Agent (RLFA) algorithm introduces a reward-based mechanism to detect and remove agents exhibiting persistent underperformance and seamlessly insert more capable ones. Each agent internally uses a mixture-of-experts (MoE) approach, delegating incoming tasks to specialized sub-models under the guidance of a gating function. A primary use case is fraud detection, where RLFA promptly swaps out an agent whose detection accuracy dips below a preset threshold. A new agent is tested in a probationary mode, and upon demonstrating superior performance, fully replaces the underperformer. This dynamic, free-agency cycle ensures sustained accuracy, quicker adaptation to emerging threats, and minimal disruption to ongoing operations. By continually refreshing its roster of agents, the system fosters ongoing improvements and more resilient collaboration in multi-agent Generative AI environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17903v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jung-Hua Liu</dc:creator>
    </item>
    <item>
      <title>Quantized distributed Nash equilibrium seeking under DoS attacks</title>
      <link>https://arxiv.org/abs/2308.12617</link>
      <description>arXiv:2308.12617v3 Announce Type: replace-cross 
Abstract: This paper studies distributed Nash equilibrium (NE) seeking under Denial-of-Service (DoS) attacks and quantization. The players can only exchange information with their own direct neighbors. The transmitted information is subject to quantization and packet losses induced by malicious DoS attacks. We propose a quantized distributed NE seeking strategy based on the approach of dynamic quantized consensus. To solve the quantizer saturation problem caused by DoS attacks, the quantization mechanism is equipped to have zooming-in and holding capabilities, in which the holding capability is consistent with the results in quantized consensus under DoS. A sufficient condition on the number of quantizer levels is provided, under which the quantizers are free from saturation under DoS attacks. The proposed distributed quantized NE seeking strategy is shown to have the so-called maximum resilience to DoS attacks. Namely, if the bound characterizing the maximum resilience is violated, an attacker can deny all the transmissions and hence distributed NE seeking is impossible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12617v3</guid>
      <category>eess.SY</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuai Feng, Maojiao Ye, Lihua Xie, Shengyuan Xu</dc:creator>
    </item>
    <item>
      <title>Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective</title>
      <link>https://arxiv.org/abs/2403.11345</link>
      <description>arXiv:2403.11345v2 Announce Type: replace-cross 
Abstract: We address in this paper Reinforcement Learning (RL) among agents that are grouped into teams such that there is cooperation within each team but general-sum (non-zero sum) competition across different teams. To develop an RL method that provably achieves a Nash equilibrium, we focus on a linear-quadratic structure. Moreover, to tackle the non-stationarity induced by multi-agent interactions in the finite population setting, we consider the case where the number of agents within each team is infinite, i.e., the mean-field setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTG). We characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard invertibility condition. This MFTG NE is then shown to be $O(1/M)$-NE for the finite population game where $M$ is a lower bound on the number of agents in each team. These structural results motivate an algorithm called Multi-player Receding-horizon Natural Policy Gradient (MRNPG), where each team minimizes its cumulative cost \emph{independently} in a receding-horizon manner. Despite the non-convexity of the problem, we establish that the resulting algorithm converges to a global NE through a novel problem decomposition into sub-problems using backward recursive discrete-time Hamilton-Jacobi-Isaacs (HJI) equations, in which \emph{independent natural policy gradient} is shown to exhibit linear convergence under time-independent diagonal dominance. Numerical studies included corroborate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11345v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Aneeq uz Zaman, Alec Koppel, Mathieu Lauri\`ere, Tamer Ba\c{s}ar</dc:creator>
    </item>
    <item>
      <title>Towards Rationality in Language and Multimodal Agents: A Survey</title>
      <link>https://arxiv.org/abs/2406.00252</link>
      <description>arXiv:2406.00252v5 Announce Type: replace-cross 
Abstract: This work discusses how to build more rational language and multimodal agents and what criteria define rationality in intelligent systems.Rationality is the quality of being guided by reason, characterized by decision-making that aligns with evidence and logical principles. It plays a crucial role in reliable problem-solving by ensuring well-grounded and consistent solutions. Despite their progress, large language models (LLMs) often fall short of rationality due to their bounded knowledge space and inconsistent outputs. In response, recent efforts have shifted toward developing multimodal and multi-agent systems, as well as integrating modules like external tools, programming codes, symbolic reasoners, utility function, and conformal risk controls rather than relying solely on a single LLM for decision-making. This paper surveys state-of-the-art advancements in language and multimodal agents, assesses their role in enhancing rationality, and outlines open challenges and future research directions. We maintain an open repository at https://github.com/bowen-upenn/Agent_Rationality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00252v5</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Yuan Yuan, Zhuoqun Hao, Xinyi Bai, Weijie J. Su, Camillo J. Taylor, Tanwi Mallick</dc:creator>
    </item>
    <item>
      <title>Learning to Steer Markovian Agents under Model Uncertainty</title>
      <link>https://arxiv.org/abs/2407.10207</link>
      <description>arXiv:2407.10207v3 Announce Type: replace-cross 
Abstract: Designing incentives for an adapting population is a ubiquitous problem in a wide array of economic applications and beyond. In this work, we study how to design additional rewards to steer multi-agent systems towards desired policies \emph{without} prior knowledge of the agents' underlying learning dynamics. Motivated by the limitation of existing works, we consider a new and general category of learning dynamics called \emph{Markovian agents}. We introduce a model-based non-episodic Reinforcement Learning (RL) formulation for our steering problem. Importantly, we focus on learning a \emph{history-dependent} steering strategy to handle the inherent model uncertainty about the agents' learning dynamics. We introduce a novel objective function to encode the desiderata of achieving a good steering outcome with reasonable cost. Theoretically, we identify conditions for the existence of steering strategies to guide agents to the desired policies. Complementing our theoretical contributions, we provide empirical algorithms to approximately solve our objective, which effectively tackles the challenge in learning history-dependent strategies. We demonstrate the efficacy of our algorithms through empirical evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10207v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Huang, Vinzenz Thoma, Zebang Shen, Heinrich H. Nax, Niao He</dc:creator>
    </item>
  </channel>
</rss>
