<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Apr 2025 01:41:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Message-Passing Perspective on Ptychographic Phase Retrieval</title>
      <link>https://arxiv.org/abs/2504.05668</link>
      <description>arXiv:2504.05668v1 Announce Type: new 
Abstract: We introduce a probabilistic approach to ptychographic reconstruction in computational imaging. Ptychography is an imaging method where the complex amplitude of an object is estimated from a sequence of diffraction measurements. We formulate this reconstruction as a Bayesian inverse problem and derive an inference algorithm, termed "Ptycho-EP," based on belief propagation and Vector Approximate Message Passing from information theory. Prior knowledge about the unknown object can be integrated into the probabilistic model, and the Bayesian framework inherently provides uncertainty quantification of the reconstruction. Numerical experiments demonstrate that, when the probe's illumination function is known, our algorithm accurately retrieves the object image at a sampling ratio approaching the information theoretic limit. In scenarios where the illumination function is unknown, both the object and the probe can be jointly reconstructed via an Expectation-Maximization algorithm. We evaluate the performance of our algorithm against conventional methods, highlighting its superior convergence speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05668v1</guid>
      <category>stat.AP</category>
      <category>physics.optics</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hajime Ueda, Shun Katakami, Masato Okada</dc:creator>
    </item>
    <item>
      <title>Microbial correlation: a semi-parametric model for investigating microbial co-metabolism</title>
      <link>https://arxiv.org/abs/2504.05450</link>
      <description>arXiv:2504.05450v1 Announce Type: cross 
Abstract: The gut microbiome plays a crucial role in human health, yet the mechanisms underlying host-microbiome interactions remain unclear, limiting its translational potential. Recent microbiome multiomics studies, particularly paired microbiome-metabolome studies (PM2S), provide valuable insights into gut metabolism as a key mediator of these interactions. Our preliminary data reveal strong correlations among certain gut metabolites, suggesting shared metabolic pathways and microbial co-metabolism. However, these findings are confounded by various factors, underscoring the need for a more rigorous statistical approach. Thus, we introduce microbial correlation, a novel metric that quantifies how two metabolites are co-regulated by the same gut microbes while accounting for confounders. Statistically, it is based on a partially linear model that isolates microbial-driven associations, and a consistent estimator is established based on semi-parametric theory. To improve efficiency, we develop a calibrated estimator with a parametric rate, maximizing the use of large external metagenomic datasets without paired metabolomic profiles. This calibrated estimator also enables efficient p-value calculation for identifying significant microbial co-metabolism signals. Through extensive numerical analysis, our method identifies important microbial co-metabolism patterns for healthy individuals, serving as a benchmark for future studies in diseased populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05450v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Shi, Yue Wang, Dan Cheng</dc:creator>
    </item>
    <item>
      <title>Modeling Multivariate Degradation Data with Dynamic Covariates Under a Bayesian Framework</title>
      <link>https://arxiv.org/abs/2504.05484</link>
      <description>arXiv:2504.05484v1 Announce Type: cross 
Abstract: Degradation data are essential for determining the reliability of high-end products and systems, especially when covering multiple degradation characteristics (DCs). Modern degradation studies not only measure these characteristics but also record dynamic system usage and environmental factors, such as temperature, humidity, and ultraviolet exposures, referred to as the dynamic covariates. Most current research either focuses on a single DC with dynamic covariates or multiple DCs with fixed covariates. This paper presents a Bayesian framework to analyze data with multiple DCs, which incorporates dynamic covariates. We develop a Bayesian framework for mixed effect nonlinear general path models to describe the degradation path and use Bayesian shape-constrained P-splines to model the effects of dynamic covariates. We also detail algorithms for estimating the failure time distribution induced by our degradation model, validate the developed methods through simulation, and illustrate their use in predicting the lifespan of organic coatings in dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05484v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengzhi Lin, Xiao Liu, Yisha Xiang, Yili Hong</dc:creator>
    </item>
    <item>
      <title>Bayesian Shrinkage in High-Dimensional VAR Models: A Comparative Study</title>
      <link>https://arxiv.org/abs/2504.05489</link>
      <description>arXiv:2504.05489v1 Announce Type: cross 
Abstract: High-dimensional vector autoregressive (VAR) models offer a versatile framework for multivariate time series analysis, yet face critical challenges from over-parameterization and uncertain lag order. In this paper, we systematically compare three Bayesian shrinkage priors (horseshoe, lasso, and normal) and two frequentist regularization approaches (ridge and nonparametric shrinkage) under three carefully crafted simulation scenarios. These scenarios encompass (i) overfitting in a low-dimensional setting, (ii) sparse high-dimensional processes, and (iii) a combined scenario where both large dimension and overfitting complicate inference.
  We evaluate each method in quality of parameter estimation (root mean squared error, coverage, and interval length) and out-of-sample forecasting (one-step-ahead forecast RMSE). Our findings show that local-global Bayesian methods, particularly the horseshoe, dominate in maintaining accurate coverage and minimizing parameter error, even when the model is heavily over-parameterized. Frequentist ridge often yields competitive point forecasts but underestimates uncertainty, leading to sub-nominal coverage. A real-data application using macroeconomic variables from Canada illustrates how these methods perform in practice, reinforcing the advantages of local-global priors in stabilizing inference when dimension or lag order is inflated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05489v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Harrison Katz, Robert E. Weiss</dc:creator>
    </item>
    <item>
      <title>FORCE: Feature-Oriented Representation with Clustering and Explanation</title>
      <link>https://arxiv.org/abs/2504.05530</link>
      <description>arXiv:2504.05530v1 Announce Type: cross 
Abstract: Learning about underlying patterns in data using latent unobserved structures to improve the accuracy of predictive models has become an active avenue of deep learning research. Most approaches cluster the original features to capture certain latent structures. However, the information gained in the process can often be implicitly derived by sufficiently complex models. Thus, such approaches often provide minimal benefits. We propose a SHAP (Shapley Additive exPlanations) based supervised deep learning framework FORCE which relies on two-stage usage of SHAP values in the neural network architecture, (i) an additional latent feature to guide model training, based on clustering SHAP values, and (ii) initiating an attention mechanism within the architecture using latent information. This approach gives a neural network an indication about the effect of unobserved values that modify feature importance for an observation. The proposed framework is evaluated on three real life datasets. Our results demonstrate that FORCE led to dramatic improvements in overall performance as compared to networks that did not incorporate the latent feature and attention framework (e.g., F1 score for presence of heart disease 0.80 vs 0.72). Using cluster assignments and attention based on SHAP values guides deep learning, enhancing latent pattern learning and overall discriminative capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05530v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rishav Mukherjee, Jeffrey Ahearn Thompson</dc:creator>
    </item>
    <item>
      <title>Financial resilience of agricultural and food production companies in Spain: A compositional cluster analysis of the impact of the Ukraine-Russia war (2021-2023)</title>
      <link>https://arxiv.org/abs/2504.05912</link>
      <description>arXiv:2504.05912v1 Announce Type: cross 
Abstract: This study analyzes the financial resilience of agricultural and food production companies in Spain amid the Ukraine-Russia war using cluster analysis based on financial ratios. This research utilizes centered log-ratios to transform financial ratios for compositional data analysis. The dataset comprises financial information from 1197 firms in Spain's agricultural and food sectors over the period 2021-2023. The analysis reveals distinct clusters of firms with varying financial performance, characterized by metrics of solvency and profitability. The results highlight an increase in resilient firms by 2023, underscoring sectoral adaptation to the conflict's economic challenges. These findings together provide insights for stakeholders and policymakers to improve sectorial stability and strategic planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05912v1</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mike Hernandez Romero, Germ\`a Coenders</dc:creator>
    </item>
    <item>
      <title>Optimizing Data-driven Weights In Multidimensional Indexes</title>
      <link>https://arxiv.org/abs/2504.06012</link>
      <description>arXiv:2504.06012v1 Announce Type: cross 
Abstract: Multidimensional indexes are ubiquitous, and popular, but present non-negligible normative choices when it comes to attributing weights to their dimensions. This paper provides a more rigorous approach to the choice of weights by defining a set of desirable properties that weighting models should meet. It shows that Bayesian Networks is the only model across statistical, econometric, and machine learning computational models that meets these properties. An example with EU-SILC data illustrates this new approach highlighting its potential for policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06012v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lidia Ceriani, Chiara Gigliarano, Paolo Verme</dc:creator>
    </item>
    <item>
      <title>NNN: Next-Generation Neural Networks for Marketing Mix Modeling</title>
      <link>https://arxiv.org/abs/2504.06212</link>
      <description>arXiv:2504.06212v1 Announce Type: cross 
Abstract: We present NNN, a Transformer-based neural network approach to Marketing Mix Modeling (MMM) designed to address key limitations of traditional methods. Unlike conventional MMMs which rely on scalar inputs and parametric decay functions, NNN uses rich embeddings to capture both quantitative and qualitative aspects of marketing and organic channels (e.g., search queries, ad creatives). This, combined with its attention mechanism, enables NNN to model complex interactions, capture long-term effects, and potentially improve sales attribution accuracy. We show that L1 regularization permits the use of such expressive models in typical data-constrained settings. Evaluating NNN on simulated and real-world data demonstrates its efficacy, particularly through considerable improvement in predictive power. Beyond attribution, NNN provides valuable, complementary insights through model probing, such as evaluating keyword or creative effectiveness, enhancing model interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06212v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Mulc, Mike Anderson, Paul Cubre, Huikun Zhang, Ivy Liu, Saket Kumar</dc:creator>
    </item>
    <item>
      <title>The Rhythm of Aging: Stability and Drift in Human Senescence</title>
      <link>https://arxiv.org/abs/2504.04143</link>
      <description>arXiv:2504.04143v2 Announce Type: replace 
Abstract: Human aging is marked by a steady rise in mortality risk with age - a process demographers describe as senescence. While life expectancy has improved dramatically over the past century, a fundamental question remains: is the rate at which mortality accelerates biologically fixed, or has it shifted across generations? Vaupel's hypothesis suggests that the pace of aging is stable - that humans are not aging more slowly, but simply starting later. To test this, we analyze cohort mortality data from France, Denmark, Italy, and Sweden. We use a two-step framework to first isolate senescent mortality, then decompose the Gompertz slope into three parts: a biological constant, a potential trend, and a cumulative period effect. The results show that most variation in the rate of aging is not biological in origin. Once non-senescent deaths and historical shocks are accounted for, the Gompertz slope is remarkably stable. The fluctuations we see are not signs of changing senescence, but echoes of shared history. Aging itself, it seems, has stayed the same. These findings suggest that while longevity has shifted, the fundamental rhythm of human aging may be biologically fixed - shaped not by evolution, but by history.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04143v2</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Silvio Cabral Patricio</dc:creator>
    </item>
    <item>
      <title>A New Approach to Radiocarbon Summarisation: Rigorous Identification of Variations/Changepoints in the Occurrence Rate of Radiocarbon Samples using a Poisson Process</title>
      <link>https://arxiv.org/abs/2501.15980</link>
      <description>arXiv:2501.15980v2 Announce Type: replace-cross 
Abstract: A commonly-used paradigm to estimate changes in the frequency of past events or the size of populations is to consider the occurrence rate of archaeological/environmental samples found at a site over time. The reliability of such a "dates-as-data" approach is highly dependent upon how the occurrence rates are estimated from the underlying samples, particularly when calendar age information for the samples is obtained from radiocarbon (14C). The most frequently used "14C-dates-as-data" approach of creating Summed Probability Distributions (SPDs) is not statistically valid, or coherent, and can provide highly misleading inference. Here, we provide an alternative method with a rigorous statistical underpinning that also provides valuable additional information on potential changepoints in the rate of events. Furthermore, unlike current SPD alternatives, our summarisation approach does not restrict users to pre-specified, rigid, summary formats (e.g., exponential or logistic growth) but instead flexibly adapts to the dates themselves. Our methodology ensures more reliable "14C-dates-as-data" analyses, allowing us to better assess and identify potential signals present. We model the occurrence of events, each assumed to leave a radiocarbon sample in the archaeological/environmental record, as an inhomogeneous Poisson process. The varying rate of samples over time is then estimated within a fully-Bayesian framework using reversible-jump Markov Chain Monte Carlo (RJ-MCMC). Given a set of radiocarbon samples, we reconstruct how their occurrence rate varies over calendar time and identify if that rate contains statistically-significant changes, i.e., specific times at which the rate of events abruptly changes. We illustrate our method with both a simulation study and a practical example concerning late-Pleistocene megafaunal population changes in Alaska and Yukon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15980v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy J Heaton, Sara Al-assam, Edouard Bard</dc:creator>
    </item>
    <item>
      <title>Constructing the Truth: Text Mining and Linguistic Networks in Public Hearings of Case 03 of the Special Jurisdiction for Peace (JEP)</title>
      <link>https://arxiv.org/abs/2504.04325</link>
      <description>arXiv:2504.04325v2 Announce Type: replace-cross 
Abstract: Case 03 of the Special Jurisdiction for Peace (JEP), focused on the so-called false positives in Colombia, represents one of the most harrowing episodes of the Colombian armed conflict. This article proposes an innovative methodology based on natural language analysis and semantic co-occurrence models to explore, systematize, and visualize narrative patterns present in the public hearings of victims and appearing parties. By constructing skipgram networks and analyzing their modularity, the study identifies thematic clusters that reveal regional and procedural status differences, providing empirical evidence on dynamics of victimization, responsibility, and acknowledgment in this case. This computational approach contributes to the collective construction of both judicial and extrajudicial truth, offering replicable tools for other transitional justice cases. The work is grounded in the pillars of truth, justice, reparation, and non-repetition, proposing a critical and in-depth reading of contested memories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04325v2</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Alejandro Urrego-L\'opez, Cesar Prieto, Emma J. Camargo-D\'iaz</dc:creator>
    </item>
  </channel>
</rss>
