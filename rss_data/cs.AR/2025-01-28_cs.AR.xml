<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Jan 2025 02:32:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Benchmarking Platform for DDR4 Memory Performance in Data-Center-Class FPGAs</title>
      <link>https://arxiv.org/abs/2501.15582</link>
      <description>arXiv:2501.15582v1 Announce Type: new 
Abstract: FPGAs are increasingly utilized in data centers due to their capacity to exploit data parallelism in computationally intensive workloads. Furthermore, the processing of modern data center workloads requires moving vast amounts of data, making it essential to optimize data exchange between FPGAs and memory. This paper introduces a novel benchmarking platform for the evaluation of DDR4 memory performance in data-center-class FPGAs. The proposed solution features highly configurable traffic generation with complex memory access patterns defined at run time and can be flexibly instantiated on the target FPGA to support multiple memory channels and varying data rates. An extensive experimental campaign, targeting the AMD Kintex UltraScale 115 FPGA and encompassing up to three memory channels with data rates ranging from 1600 to 2400 MT/s and various memory traffic configurations, demonstrates the benchmarking platform's capability to effectively evaluate DDR4 memory performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15582v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Galimberti, Gabriele Montanaro, Andrea Motta, Federico Proverbio, Davide Zoni</dc:creator>
    </item>
    <item>
      <title>Hierarchical Recording Architecture for Three-Dimensional Magnetic Recording</title>
      <link>https://arxiv.org/abs/2501.16053</link>
      <description>arXiv:2501.16053v1 Announce Type: new 
Abstract: Three-dimensional magnetic recording (3DMR) is a highly promising approach to achieving ultra-large data storage capacity in hard disk drives. One of the greatest challenges for 3DMR lies in performing sequential and correct writing of bits into the multi-layer recording medium. In this work, we have proposed a hierarchical recording architecture based on layered heat-assisted writing with a multi-head array. The feasibility of the architecture is validated in a dual-layer 3DMR system with FePt-based thin films via micromagnetic simulation. Our results reveal the magnetization reversal mechanism of the grains, ultimately attaining appreciable switching probability and medium signal-to-noise ratio (SNR) for each layer. In particular, an optimal head-to-head distance is identified as the one that maximizes the medium SNR. Optimizing the system's noise resistance will improve the overall SNR and allow for a smaller optimal head-to-head distance, which can pave the way for scaling 3DMR to more recording layers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16053v1</guid>
      <category>cs.AR</category>
      <category>physics.app-ph</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yugen Jian, Ke Luo, Jincai Chen, Xuanyao Fong</dc:creator>
    </item>
    <item>
      <title>Datapath Combinational Equivalence Checking With Hybrid Sweeping Engines and Parallelization</title>
      <link>https://arxiv.org/abs/2501.14740</link>
      <description>arXiv:2501.14740v1 Announce Type: cross 
Abstract: In the application of IC design for microprocessors, there are often demands for optimizing the implementation of datapath circuits, on which various arithmetic operations are performed. Combinational equivalence checking (CEC) plays an essential role in ensuring the correctness of design optimization. The most prevalent CEC algorithms are based on SAT sweeping, which utilizes SAT to prove the equivalence of the internal node pairs in topological order, and the equivalent nodes are merged. Datapath circuits usually contain equivalent pairs for which the transitive fan-in cones are small but have a high XOR chain density, and proving such node pairs is very difficult for SAT solvers. An exact probability-based simulation (EPS) is suitable for verifying such pairs, while this method is not suitable for pairs with many primary inputs due to the memory cost. We first reduce the memory cost of EPS and integrate it to improve the SAT sweeping method. Considering the complementary abilities of SAT and EPS, we design an engine selection heuristic to dynamically choose SAT or EPS in the sweeping process, according to XOR chain density. Our method is further improved by reducing unnecessary engine calls by detecting regularity. Furthermore, we parallelized the SAT and EPS engines of HybridCEC, leading to the parallel CEC prover. Experiments on a benchmark suite from industrial datapath circuits show that our method is much faster than the state-of-the-art CEC tool namely ABC &amp;cec on nearly all instances, and is more than 100x faster on 30% of the instances, 1000x faster on 12% of the instances. In addition, the 64 threads version of our method achieved 77x speedup.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14740v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICCAD57390.2023.10323876</arxiv:DOI>
      <dc:creator>Zhihan Chen, Xindi Zhang, Yuhang Qian, Shaowei Cai</dc:creator>
    </item>
    <item>
      <title>Implementation and Evaluation of GBDI Memory Compression Algorithm Using C/C++ on a Broader Range of Workloads</title>
      <link>https://arxiv.org/abs/2501.14812</link>
      <description>arXiv:2501.14812v1 Announce Type: cross 
Abstract: Memory compression is an important approach in computer architecture for decreasing memory footprint and improving system performance. In this paper, we use C/C++ to develop a current memory compression algorithm; the Global Bases Delta Immediate (GBDI) algorithm, which was proposed at HPCA'2022. By using global bases and enabling deltas within the same block to vary in size, the GBDI compression algorithm decreases the size of encoded data. The goal of this research is to assess the effectivenessof the GBDI algorithm by examining its compression ratios under a broader range of workloads. Our research leads to a better knowledge of the GBDI algorithm's effectiveness and the potential benefits of memory compression techniques for various sorts of applications. Furthermore, our C/C++ version of the algorithm gives academics and practitioners a high degree of freedom over customizing the algorithm for individual workloads and optimizing its performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14812v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.11020.28804</arxiv:DOI>
      <dc:creator>Adeyemi Aina</dc:creator>
    </item>
    <item>
      <title>A Tale of Two Sides of Wafer: Physical Implementation and Block-Level PPA on Flip FET with Dual-sided Signals</title>
      <link>https://arxiv.org/abs/2501.15275</link>
      <description>arXiv:2501.15275v1 Announce Type: cross 
Abstract: As the conventional scaling of logic devices comes to an end, functional wafer backside and 3D transistor stacking are consensus for next-generation logic technology, offering considerable design space extension for powers, signals or even devices on the wafer backside. The Flip FET (FFET), a novel transistor architecture combining 3D transistor stacking and fully functional wafer backside, was recently proposed. With symmetric dual-sided standard cell design, the FFET can deliver around 12.5% cell area scaling and faster but more energy-efficient libraries beyond other stacked transistor technologies such as CFET. Besides, thanks to the novel cell design with dual-sided pins, the FFET supports dual-sided signal routing, delivering better routability and larger backside design space. In this work, we demonstrated a comprehensive FFET evaluation framework considering physical implementation and block-level power-performance-area (PPA) assessment for the first time, in which key functions are dual-sided routing and dual-sided RC extraction. A 32-bit RISC-V core was used for the evaluation here. Compared to the CFET with single-sided signals, the FFET with single-sided signals achieved 23.3% post-P&amp;R core area reduction, 25.0% higher frequency and 11.9% lower power at the same utilization, and 16.0 % higher frequency at the same core area. Meanwhile, the FFET supports dual-sided signals, which can further benefit more from flexible allocation of cell input pins on both sides. By optimizing the input pin density and BEOL routing layer number on each side, 10.6% frequency gain was realized without power degradation compared to the one with single-sided signal routing. Moreover, the routability and power efficiency of FFET barely degrades even with the routing layer number reduced from 12 to 5 on each side, validating the great space for cost-friendly design enabled by FFET.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15275v1</guid>
      <category>physics.app-ph</category>
      <category>cond-mat.mes-hall</category>
      <category>cs.AR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proc. of DATE 2025</arxiv:journal_reference>
      <dc:creator>Haoran Lu, Xun Jiang, Yanbang Chu, Ziqiao Xu, Rui Guo, Wanyue Peng, Yibo Lin, Runsheng Wang, Heng Wu, Ru Huang</dc:creator>
    </item>
    <item>
      <title>SQ-DM: Accelerating Diffusion Models with Aggressive Quantization and Temporal Sparsity</title>
      <link>https://arxiv.org/abs/2501.15448</link>
      <description>arXiv:2501.15448v1 Announce Type: cross 
Abstract: Diffusion models have gained significant popularity in image generation tasks. However, generating high-quality content remains notably slow because it requires running model inference over many time steps. To accelerate these models, we propose to aggressively quantize both weights and activations, while simultaneously promoting significant activation sparsity. We further observe that the stated sparsity pattern varies among different channels and evolves across time steps. To support this quantization and sparsity scheme, we present a novel diffusion model accelerator featuring a heterogeneous mixed-precision dense-sparse architecture, channel-last address mapping, and a time-step-aware sparsity detector for efficient handling of the sparsity pattern. Our 4-bit quantization technique demonstrates superior generation quality compared to existing 4-bit methods. Our custom accelerator achieves 6.91x speed-up and 51.5% energy reduction compared to traditional dense accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15448v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zichen Fan, Steve Dai, Rangharajan Venkatesan, Dennis Sylvester, Brucek Khailany</dc:creator>
    </item>
    <item>
      <title>EPOCH: Enabling Preemption Operation for Context Saving in Heterogeneous FPGA Systems</title>
      <link>https://arxiv.org/abs/2501.16205</link>
      <description>arXiv:2501.16205v1 Announce Type: cross 
Abstract: FPGAs are increasingly used in multi-tenant cloud environments to offload compute-intensive tasks from the main CPU. The operating system (OS) plays a vital role in identifying tasks suitable for offloading and coordinating between the CPU and FPGA for seamless task execution. The OS leverages preemption to manage CPU efficiently and balance CPU time; however, preempting tasks running on FPGAs without context loss remains challenging. Despite growing reliance on FPGAs, vendors have yet to deliver a solution that fully preserves and restores task context.
  This paper presents EPOCH, the first out-of-the-box framework to seamlessly preserve the state of tasks running on multi-tenant cloud FPGAs. EPOCH enables interrupting a tenant's execution at any arbitrary clock cycle, capturing its state, and saving this 'state snapshot' in off-chip memory with fine-grain granularity. Subsequently, when task resumption is required, EPOCH can resume execution from the saved 'state snapshot', eliminating the need to restart the task from scratch. EPOCH automates intricate processes, shields users from complexities, and synchronizes all underlying logic in a common clock domain, mitigating timing violations and ensuring seamless handling of interruptions.
  EPOCH proficiently captures the state of fundamental FPGA elements, such as look-up tables, flip-flops, block--RAMs, and digital signal processing units. On real hardware, ZynQ-XC7Z020 SoC, the proposed solution achieves context save and restore operations per frame in 62.2us and 67.4us, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16205v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arsalan Ali Malik, Emre Karabulut, Aydin Aysu</dc:creator>
    </item>
    <item>
      <title>Evaluation of isolation design flow (IDF) for Single Chip Cryptography (SCC) application</title>
      <link>https://arxiv.org/abs/2501.16217</link>
      <description>arXiv:2501.16217v1 Announce Type: cross 
Abstract: Field Programmable Gate Arrays (FPGAs) are increasingly in various applications. This is due to the fact that they provide flexibility to reprogram and modify in realtime with minimum effort. The increasing usage of FPGA must also ensure its end users with a guarantee that they are able to overcome failures and work in the harshest of environments. Today's end user demands a guarantee that the system he/she is buying remains functional.
  FPGA Vendor Xilinx provides its users with that guarantee in the name of Isolation Design flow or IDF for short. Xilinx claims that IDF can help ensure users reliability while providing flexibility. If true, application that can benefit from IDF are vast, such as Avionics, Unmanned probes, Self-driving fully autonomous vehicles, etc.
  This thesis puts the Xilinx claim regarding IDF to the test by implementing a single-chip cryptographic application, namely Advanced Encryption Standard, according to the rules and regulations defined by isolation design flow. This thesis does so by replicating and injecting faults in the system that conforms to the rules of IDF and records its behavior firsthand to observe IDF effectiveness. This thesis can also help end users and system evaluators interested in effectiveness of IDF with an independent view other than Xilinx by providing them with statistical data collected to remove all doubts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16217v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arsalan Ali Malik</dc:creator>
    </item>
    <item>
      <title>General-Purpose Multicore Architectures</title>
      <link>https://arxiv.org/abs/2408.12999</link>
      <description>arXiv:2408.12999v2 Announce Type: replace 
Abstract: The first years of the 2000s led to an inflection point in computer architectures: while the number of available transistors on a chip continued to grow, crucial transistor scaling properties started to break down and result in increasing power consumption, while aggressive single-core performance optimizations were resulting in diminishing returns due to inherent limits in instruction-level parallelism. This led to the rise of multicore CPU architectures, which are now commonplace in modern computers at all scales. In this chapter, we discuss the evolution of multicore CPUs since their introduction. Starting with a historic overview of multiprocessing, we explore the basic microarchitecture of a multicore CPU, key challenges resulting from shared memory resources, operating system modifications to optimize multicore CPU support, popular metrics for multicore evaluation, and recent trends in multicore CPU design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12999v2</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saugata Ghose</dc:creator>
    </item>
    <item>
      <title>MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs</title>
      <link>https://arxiv.org/abs/2411.03471</link>
      <description>arXiv:2411.03471v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have been applied to various hardware design tasks, including Verilog code generation, EDA tool scripting, and RTL bug fixing. Despite this extensive exploration, LLMs are yet to be used for the task of post-synthesis metric reasoning and estimation of HDL designs. In this paper, we assess the ability of LLMs to reason about post-synthesis metrics of Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868 Verilog HDL designs and their corresponding post-synthesis metrics, namely area, delay, and static power. MetRex incorporates a Chain of Thought (CoT) template to enhance LLMs' reasoning about these metrics. Extensive experiments show that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities on average by 37.0\%, 25.3\%, and 25.7\% on the area, delay, and static power, respectively. While SFT improves performance on our benchmark, it remains far from achieving optimal results, especially on complex problems. Comparing to state-of-the-art regression models, our approach delivers accurate post-synthesis predictions for 17.4\% more designs (within a 5\% error margin), in addition to offering a 1.7x speedup by eliminating the need for pre-processing. This work lays the groundwork for advancing LLM-based Verilog code metric reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03471v2</guid>
      <category>cs.AR</category>
      <category>cs.CL</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Manar Abdelatty, Jingxiao Ma, Sherief Reda</dc:creator>
    </item>
    <item>
      <title>Hierarchical Mixture of Experts: Generalizable Learning for High-Level Synthesis</title>
      <link>https://arxiv.org/abs/2410.19225</link>
      <description>arXiv:2410.19225v3 Announce Type: replace-cross 
Abstract: High-level synthesis (HLS) is a widely used tool in designing Field Programmable Gate Array (FPGA). HLS enables FPGA design with software programming languages by compiling the source code into an FPGA circuit. The source code includes a program (called ``kernel'') and several pragmas that instruct hardware synthesis, such as parallelization, pipeline, etc. While it is relatively easy for software developers to design the program, it heavily relies on hardware knowledge to design the pragmas, posing a big challenge for software developers. Recently, different machine learning algorithms, such as GNNs, have been proposed to automate the pragma design via performance prediction. However, when applying the trained model on new kernels, the significant domain shift often leads to unsatisfactory performance. We propose a more domain-generalizable model structure: a two-level hierarchical Mixture of Experts (MoE), that can be flexibly adapted to any GNN model. Different expert networks can learn to deal with different regions in the representation space, and they can utilize similar patterns between the old kernels and new kernels. In the low-level MoE, we apply MoE on three natural granularities of a program: node, basic block, and graph. The high-level MoE learns to aggregate the three granularities for the final decision. To stably train the hierarchical MoE, we further propose a two-stage training method. Extensive experiments verify the effectiveness of the hierarchical MoE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19225v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weikai Li, Ding Wang, Zijian Ding, Atefeh Sohrabizadeh, Zongyue Qin, Jason Cong, Yizhou Sun</dc:creator>
    </item>
  </channel>
</rss>
