<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Jul 2024 04:01:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Max Bense as a Visionary: from Entropy to the Dialectics of Programmed Images</title>
      <link>https://arxiv.org/abs/2407.02492</link>
      <description>arXiv:2407.02492v1 Announce Type: cross 
Abstract: In 1960 in Stuttgart, Max Bense published the book Programming the Beautiful [Programmierung des Sch{\"o}nen]. Bense looks in cybernetics for scientific concepts and instigates the thought of programming in the field of literature. His information aesthetics influences a whole generation of scientists and artists - including the Stuttgart Circle, which takes hold of the new aesthetics to carry out the first programmed artistic images. Is Max Bense a visionary? How is he revolutionizing the world of images? The article discusses the cybernetics that inspired Bense: a science of probability that contrasts with the principles of Newtonian physics. Moreover, in the sixties, Max Bense, together with Elisabeth Walther, launched the experimental magazine Rot, which devoted its pages to the concrete poetry and the first computer-generated images of Georg Nees. As Frieder Nake defends through his pioneering work and theory, these images oppose the visible and the computable. This dialectic opens to a critical thinking on the algorithmic image in art and science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02492v1</guid>
      <category>cs.GL</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.4000/imagesrevues.10395</arxiv:DOI>
      <arxiv:journal_reference>Images Re-Vues, 2021, 19</arxiv:journal_reference>
      <dc:creator>Ga\"etan Robillard (UP8)</dc:creator>
    </item>
    <item>
      <title>Meta 3D Gen</title>
      <link>https://arxiv.org/abs/2407.02599</link>
      <description>arXiv:2407.02599v1 Announce Type: cross 
Abstract: We introduce Meta 3D Gen (3DGen), a new state-of-the-art, fast pipeline for text-to-3D asset generation. 3DGen offers 3D asset creation with high prompt fidelity and high-quality 3D shapes and textures in under a minute. It supports physically-based rendering (PBR), necessary for 3D asset relighting in real-world applications. Additionally, 3DGen supports generative retexturing of previously generated (or artist-created) 3D shapes using additional textual inputs provided by the user. 3DGen integrates key technical components, Meta 3D AssetGen and Meta 3D TextureGen, that we developed for text-to-3D and text-to-texture generation, respectively. By combining their strengths, 3DGen represents 3D objects simultaneously in three ways: in view space, in volumetric space, and in UV (or texture) space. The integration of these two techniques achieves a win rate of 68% with respect to the single-stage model. We compare 3DGen to numerous industry baselines, and show that it outperforms them in terms of prompt fidelity and visual quality for complex textual prompts, while being significantly faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02599v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raphael Bensadoun, Tom Monnier, Yanir Kleiman, Filippos Kokkinos, Yawar Siddiqui, Mahendra Kariya, Omri Harosh, Roman Shapovalov, Benjamin Graham, Emilien Garreau, Animesh Karnewar, Ang Cao, Idan Azuri, Iurii Makarov, Eric-Tuan Le, Antoine Toisoul, David Novotny, Oran Gafni, Natalia Neverova, Andrea Vedaldi</dc:creator>
    </item>
    <item>
      <title>Design of a UE5-based digital twin platform</title>
      <link>https://arxiv.org/abs/2407.03107</link>
      <description>arXiv:2407.03107v1 Announce Type: cross 
Abstract: Aiming at the current mainstream 3D scene engine learning and building cost is too high, this thesis proposes a digital twin platform design program based on Unreal Engine 5 (UE5). It aims to provide a universal platform construction design process to effectively reduce the learning cost of large-scale scene construction. Taking an actual project of a unit as an example, the overall cycle work of platform building is explained, and the digital twin and data visualization technologies and applications based on UE5 are analyzed. By summarizing the project implementation into a process approach, the standardization and operability of the process pathway is improved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03107v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <category>cs.MM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shaoqiu Lyu, Muzhi Wang, Sunrui Zhang, Shengzhi Wang</dc:creator>
    </item>
    <item>
      <title>Consistent Point Orientation for Manifold Surfaces via Boundary Integration</title>
      <link>https://arxiv.org/abs/2407.03165</link>
      <description>arXiv:2407.03165v1 Announce Type: cross 
Abstract: This paper introduces a new approach for generating globally consistent normals for point clouds sampled from manifold surfaces. Given that the generalized winding number (GWN) field generated by a point cloud with globally consistent normals is a solution to a PDE with jump boundary conditions and possesses harmonic properties, and the Dirichlet energy of the GWN field can be defined as an integral over the boundary surface, we formulate a boundary energy derived from the Dirichlet energy of the GWN. Taking as input a point cloud with randomly oriented normals, we optimize this energy to restore the global harmonicity of the GWN field, thereby recovering the globally consistent normals. Experiments show that our method outperforms state-of-the-art approaches, exhibiting enhanced robustness to noise, outliers, complex topologies, and thin structures. Our code can be found at \url{https://github.com/liuweizhou319/BIM}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03165v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weizhou Liu, Xingce Wang, Haichuan Zhao, Xingfei Xue, Zhongke Wu, Xuequan Lu, Ying He</dc:creator>
    </item>
    <item>
      <title>ColorVideoVDP: A visual difference predictor for image, video and display distortions</title>
      <link>https://arxiv.org/abs/2401.11485</link>
      <description>arXiv:2401.11485v2 Announce Type: replace-cross 
Abstract: ColorVideoVDP is a video and image quality metric that models spatial and temporal aspects of vision, for both luminance and color. The metric is built on novel psychophysical models of chromatic spatiotemporal contrast sensitivity and cross-channel contrast masking. It accounts for the viewing conditions, geometric, and photometric characteristics of the display. It was trained to predict common video streaming distortions (e.g. video compression, rescaling, and transmission errors), and also 8 new distortion types related to AR/VR displays (e.g. light source and waveguide non-uniformities). To address the latter application, we collected our novel XR-Display-Artifact-Video quality dataset (XR-DAVID), comprised of 336 distorted videos. Extensive testing on XR-DAVID, as well as several datasets from the literature, indicate a significant gain in prediction performance compared to existing metrics. ColorVideoVDP opens the doors to many novel applications which require the joint automated spatiotemporal assessment of luminance and color distortions, including video streaming, display specification and design, visual comparison of results, and perceptually-guided quality optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11485v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3658144</arxiv:DOI>
      <arxiv:journal_reference>SIGGRAPH 2024 Technical Papers, Article 129</arxiv:journal_reference>
      <dc:creator>Rafal K. Mantiuk, Param Hanji, Maliha Ashraf, Yuta Asano, Alexandre Chapiro</dc:creator>
    </item>
  </channel>
</rss>
