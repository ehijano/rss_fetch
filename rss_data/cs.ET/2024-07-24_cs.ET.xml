<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Jul 2024 04:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Blockchain in Healthcare: Implementing Hyperledger Fabric for Electronic Health Records at Frere Provincial Hospital</title>
      <link>https://arxiv.org/abs/2407.15876</link>
      <description>arXiv:2407.15876v1 Announce Type: cross 
Abstract: As healthcare systems worldwide continue to grapple with the challenges of interoperability, data security, and accessibility, integrating emerging technologies becomes imperative. This paper investigates the implementation of blockchain technology, specifically Hyperledger Fabric, for Electronic Health Records (EHR) management at Frere Hospital in the Eastern Cape province of South Africa. The paper examines the benefits and challenges of integrating blockchain into healthcare information systems. Hyperledger Fabric's modular architecture is harnessed to create a secure, transparent, and decentralized platform for storing, managing, and sharing EHRs among stakeholders. The study used a mixed-methods approach, integrating case studies and data collection methods through observation and informal questions, with the specific goal of understanding current record management methods and challenges. This method offers practical insights and validates the approach. The result demonstrates the role of blockchain in transforming healthcare, framed within a rigorous exploration and analysis. The findings of this study have broader implications for healthcare institutions seeking advanced solutions to address the persistent challenges in electronic health record management. Ultimately, the research underscores the transformative potential of blockchain technology in healthcare settings, fostering trust, security, and efficiency in the management of sensitive patient data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15876v1</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abayomi Agbeyangi, Olukayode Oki, Aphelele Mgidi</dc:creator>
    </item>
    <item>
      <title>Sparks of Quantum Advantage and Rapid Retraining in Machine Learning</title>
      <link>https://arxiv.org/abs/2407.16020</link>
      <description>arXiv:2407.16020v1 Announce Type: cross 
Abstract: The advent of quantum computing holds the potential to revolutionize various fields by solving complex problems more efficiently than classical computers. Despite this promise, practical quantum advantage is hindered by current hardware limitations, notably the small number of qubits and high noise levels. In this study, we leverage adiabatic quantum computers to optimize Kolmogorov-Arnold Networks, a powerful neural network architecture for representing complex functions with minimal parameters. By modifying the network to use Bezier curves as the basis functions and formulating the optimization problem into a Quadratic Unconstrained Binary Optimization problem, we create a fixed-sized solution space, independent of the number of training samples. Our approach demonstrates sparks of quantum advantage through faster training times compared to classical optimizers such as the Adam, Stochastic Gradient Descent, Adaptive Gradient, and simulated annealing. Additionally, we introduce a novel rapid retraining capability, enabling the network to be retrained with new data without reprocessing old samples, thus enhancing learning efficiency in dynamic environments. Experimental results on initial training of classification and regression tasks validate the efficacy of our approach, showcasing significant speedups and comparable performance to classical methods. While experiments on retraining demonstrate a sixty times speed up using adiabatic quantum computing based optimization compared to that of the gradient descent based optimizers, with theoretical models allowing this speed up to be even larger! Our findings suggest that with further advancements in quantum hardware and algorithm optimization, quantum-optimized machine learning models could have broad applications across various domains, with initial focus on rapid retraining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16020v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Troy</dc:creator>
    </item>
    <item>
      <title>Artificial Agency and Large Language Models</title>
      <link>https://arxiv.org/abs/2407.16190</link>
      <description>arXiv:2407.16190v1 Announce Type: cross 
Abstract: The arrival of Large Language Models (LLMs) has stirred up philosophical debates about the possibility of realizing agency in an artificial manner. In this work we contribute to the debate by presenting a theoretical model that can be used as a threshold conception for artificial agents. The model defines agents as systems whose actions and goals are always influenced by a dynamic framework of factors that consists of the agent's accessible history, its adaptive repertoire and its external environment. This framework, in turn, is influenced by the actions that the agent takes and the goals that it forms. We show with the help of the model that state-of-the-art LLMs are not agents yet, but that there are elements to them that suggest a way forward. The paper argues that a combination of the agent architecture presented in Park et al. (2023) together with the use of modules like the Coscientist in Boiko et al. (2023) could potentially be a way to realize agency in an artificial manner. We end the paper by reflecting on the obstacles one might face in building such an artificial agent and by presenting possible directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16190v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Maud Van Lier, Gorka Mu\~noz-Gil</dc:creator>
    </item>
    <item>
      <title>VidyaRANG: Conversational Learning Based Platform powered by Large Language Model</title>
      <link>https://arxiv.org/abs/2407.16209</link>
      <description>arXiv:2407.16209v1 Announce Type: cross 
Abstract: Providing authoritative information tailored to a student's specific doubt is a hurdle in this era where search engines return an overwhelming number of article links. Large Language Models such as GPTs fail to provide answers to questions that were derived from sensitive confidential information. This information which is specific to some organisations is not available to LLMs due to privacy constraints. This is where knowledge-augmented retrieval techniques become particularly useful. The proposed platform is designed to cater to the needs of learners from divergent fields. Today, the most common format of learning is video and books, which our proposed platform allows learners to interact and ask questions. This increases learners' focus time exponentially by restricting access to pertinent content and, at the same time allowing personalized access and freedom to gain in-depth knowledge. Instructor's roles and responsibilities are significantly simplified allowing them to train a larger audience. To preserve privacy, instructors can grant course access to specific individuals, enabling personalized conversation on the provided content. This work includes an extensive spectrum of software development and product management skills, which also circumscribe knowledge of cloud computing for running Large Language Models and maintaining the application. For Frontend development, which is responsible for user interaction and user experience, Streamlit and React framework have been utilized. To improve security and privacy, the server is routed to a domain with an SSL certificate, and all the API key/s are stored securely on an AWS EC2 instance, to enhance user experience, web connectivity to an Android Studio-based mobile app has been established, and in-process to publish the app on play store, thus addressing all major software engineering disciplines</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16209v1</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chitranshu Harbola, Anupam Purwar</dc:creator>
    </item>
    <item>
      <title>A Programming Model for Disaggregated Memory over CXL</title>
      <link>https://arxiv.org/abs/2407.16300</link>
      <description>arXiv:2407.16300v1 Announce Type: cross 
Abstract: CXL (Compute Express Link) is an emerging open industry-standard interconnect between processing and memory devices that is expected to revolutionize the way systems are designed in the near future. It enables cache-coherent shared memory pools in a disaggregated fashion at unprecedented scales, allowing algorithms to interact with a variety of storage devices using simple loads and stores in a cacheline granularity. Alongside with unleashing unique opportunities for a wide range of applications, CXL introduces new challenges of data management and crash consistency. Alas, CXL lacks an adequate programming model, which makes reasoning about the correctness and expected behaviors of algorithms and systems on top of it nearly impossible.
  In this work, we present CXL0, the first programming model for concurrent programs running on top of CXL. We propose a high-level abstraction for CXL memory accesses and formally define operational semantics on top of that abstraction. We provide a set of general transformations that adapt concurrent algorithms to the new disruptive technology. Using these transformations, every linearizable algorithm can be easily transformed into its provably correct version in the face of a full-system or sub-system crash. We believe that this work will serve as the stepping stone for systems design and modelling on top of CXL, and support the development of future models as software and hardware evolve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16300v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gal Assa, Michal Friedman, Ori Lahav</dc:creator>
    </item>
    <item>
      <title>A Quantum Leaky Integrate-and-Fire Spiking Neuron and Network</title>
      <link>https://arxiv.org/abs/2407.16398</link>
      <description>arXiv:2407.16398v1 Announce Type: cross 
Abstract: Quantum machine learning is in a period of rapid development and discovery, however it still lacks the resources and diversity of computational models of its classical complement. With the growing difficulties of classical models requiring extreme hardware and power solutions, and quantum models being limited by noisy intermediate-scale quantum (NISQ) hardware, there is an emerging opportunity to solve both problems together. Here we introduce a new software model for quantum neuromorphic computing -- a quantum leaky integrate-and-fire (QLIF) neuron, implemented as a compact high-fidelity quantum circuit, requiring only 2 rotation gates and no CNOT gates. We use these neurons as building blocks in the construction of a quantum spiking neural network (QSNN), and a quantum spiking convolutional neural network (QSCNN), as the first of their kind. We apply these models to the MNIST, Fashion-MNIST, and KMNIST datasets for a full comparison with other classical and quantum models. We find that the proposed models perform competitively, with comparative accuracy, with efficient scaling and fast computation in classical simulation as well as on quantum devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16398v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dean Brand, Francesco Petruccione</dc:creator>
    </item>
    <item>
      <title>Analysis of 3GPP and Ray-Tracing Based Channel Model for 5G Industrial Network Planning</title>
      <link>https://arxiv.org/abs/2407.16528</link>
      <description>arXiv:2407.16528v1 Announce Type: cross 
Abstract: Appropriate channel models tailored to the specific needs of industrial environments are crucial for the 5G private industrial network design and guiding deployment strategies. This paper scrutinizes the applicability of 3GPP's channel model for industrial scenarios. The challenges in accurately modeling industrial channels are addressed, and a refinement strategy is proposed employing a ray-tracing (RT) based channel model calibrated with continuous-wave received power measurements collected in a manufacturing facility in Sweden. The calibration helps the RT model achieve a root mean square error (RMSE) and standard deviation of less than 7 dB. The 3GPP and the calibrated RT model are statistically compared with the measurements, and the coverage maps of both models are also analyzed. The calibrated RT model is used to simulate the network deployment in the factory to satisfy the reference signal received power (RSRP) requirement. The deployment performance is compared with the prediction from the 3GPP model in terms of the RSRP coverage map and coverage rate. Evaluation of deployment performance provides crucial insights into the efficacy of various channel modeling techniques for optimizing 5G industrial network planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16528v1</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gurjot Singh Bhatia, Yoann Corre, Linus Thrybom, M. Di Renzo</dc:creator>
    </item>
    <item>
      <title>Mobile Technology: A Panacea to Food Insecurity In Nigeria -- A Case Study of SELL HARVEST Application</title>
      <link>https://arxiv.org/abs/2407.16614</link>
      <description>arXiv:2407.16614v1 Announce Type: cross 
Abstract: Over time, agriculture is the most consistent activity, and it evolves every day. It contributes to a vast majority of the Gross Domestic Product (GDP) of Nigeria but as ironic as it may be, there is still hunger in significant parts of the country due to low productivity in the agricultural sector and comparison to the geometric population growth. During the first half of 2022, agriculture contributed about 23% of the country's GDP while the industry and services sector had a share of the remaining 77%. This showed that with the high rate of agricultural activities, Nigeria has not achieved food security for the teeming population. and more productivity levels can be attained. Technology can/will assist Nigeria in overcoming global poverty and hunger quicker in both rural and urban areas. Today, there are many types of agricultural technologies available for farmers all over the world to increase productivity. Major technological advancements include indoor vertical farming, automation, robotics, livestock technology, modern greenhouse practices, precision agriculture, artificial intelligence, and blockchain. Mobile phones have one of the highest adoption rates of technologies developed within the last century. Digitalization will bring consumers and farmers closer together to access the shortest supply chain possible and reduce rural poverty and hunger. The paper will review the different agricultural technologies and propose a mobile solution, code Sell Harvest, to make farming more sustainable and secure food.
  Keywords: Sell Harvest, Agriculture, Technology, Artificial Intelligence, and Digital Farming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16614v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mudathir Muhammad Salahudeen, Muhammad Auwal Mukhtar, Saadu Salihu Abubakar, Salawu I. S</dc:creator>
    </item>
    <item>
      <title>6G at $\frac{1}{6}g$: The Future of Cislunar Communications</title>
      <link>https://arxiv.org/abs/2407.16672</link>
      <description>arXiv:2407.16672v1 Announce Type: cross 
Abstract: What will the future of cislunar communications be? The ever-expanding horizons of the space exploration missions, and the need for establishing sustainable space communication and navigation infrastructure necessitate to think this question thoroughly. In this article, we examine how some of the concepts of 6G technologies developed for terrestrial networks can be relevant in the context of cislunar networks. We discuss how 6G concepts, such as reconfigurable intelligent surfaces, quantum-resistant physical layer security, private information read/write/cache networks, semantic and goal-oriented communications, information freshness based quality of communication metrics, multi-relay and cooperative networks, hold the potential to shape the future of cislunar communications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16672v1</guid>
      <category>cs.IT</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sahan Liyanaarachchi, Stavros Mitrolaris, Purbesh Mitra, Sennur Ulukus</dc:creator>
    </item>
    <item>
      <title>Event-Based Simulation of Stochastic Memristive Devices for Neuromorphic Computing</title>
      <link>https://arxiv.org/abs/2407.04718</link>
      <description>arXiv:2407.04718v2 Announce Type: replace 
Abstract: In this paper, we build a general model of memristors suitable for the simulation of event-based systems, such as hardware spiking neural networks, and more generally, neuromorphic computing systems. We extend an existing general model of memristors - the Generalised Metastable Switch Model - to an event-driven setting, eliminating errors associated discrete time approximation, as well as offering potential improvements in terms of computational efficiency for simulation. We introduce the notion of a volatility state variable, to allow for the modelling of memory-dependent and dynamic switching behaviour, succinctly capturing and unifying a variety of volatile phenomena present in memristive devices, including state relaxation, structural disruption, Joule heating, and drift acceleration phenomena. We supply a drift dataset for titanium dioxide memristors and introduce a linear conductance model to simulate the drift characteristics, motivated by a proposed physical model of filament growth. We then demonstrate an approach for fitting the parameters of the event-based model to the drift model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04718v2</guid>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <category>physics.app-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Waleed El-Geresy, Christos Papavassiliou, Deniz G\"und\"uz</dc:creator>
    </item>
    <item>
      <title>An Algorithm for Reversible Logic Circuit Synthesis Based on Tensor Decomposition</title>
      <link>https://arxiv.org/abs/2107.04298</link>
      <description>arXiv:2107.04298v4 Announce Type: replace-cross 
Abstract: An algorithm for reversible logic synthesis is proposed. The task is, for a given $n$-bit substitution map $P_n: \{0,1\}^n \rightarrow \{0,1\}^n$, to find a sequence of reversible logic gates that implements the map. The gate library adopted in this work consists of multiple-controlled Toffoli gates denoted by $C^m\!X$, where $m$ is the number of control bits that ranges from 0 to $n-1$. Controlled gates with large $m \,\,(&gt;2)$ are then further decomposed into $C^0\!X$, $C^1\!X$, and $C^2\!X$ gates. A primary concern in designing the algorithm is to reduce the use of $C^2\!X$ gate (also known as Toffoli gate) which is known to be universal.
  The main idea is to view an $n$-bit substitution map as a rank-$2n$ tensor and to transform it such that the resulting map can be written as a tensor product of a rank-($2n-2$) tensor and the $2\times 2$ identity matrix. Let $\mathcal{P}_n$ be a set of all $n$-bit substitution maps. What we try to find is a size reduction map $\mathcal{A}_{\rm red}: \mathcal{P}_n \rightarrow \{P_n: P_n = P_{n-1} \otimes I_2\}$. %, where $I_m$ is the $m\times m$ identity matrix. One can see that the output $P_{n-1} \otimes I_2$ acts nontrivially on $n-1$ bits only, meaning that the map to be synthesized becomes $P_{n-1}$. The size reduction process is iteratively applied until it reaches tensor product of only $2 \times 2$ matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.04298v4</guid>
      <category>cs.LO</category>
      <category>cs.ET</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3673242</arxiv:DOI>
      <arxiv:journal_reference>ACM Transactions on Quantum Computing 5, 3, Article 15 (September 2024), 28 pages</arxiv:journal_reference>
      <dc:creator>Hochang Lee, Kyung Chul Jeong, Daewan Han, Panjin Kim</dc:creator>
    </item>
    <item>
      <title>Grover Speedup from Many Forms of the Zeno Effect</title>
      <link>https://arxiv.org/abs/2305.11146</link>
      <description>arXiv:2305.11146v3 Announce Type: replace-cross 
Abstract: It has previously been established that adiabatic quantum computation, operating based on a continuous Zeno effect due to dynamical phases between eigenstates, is able to realise an optimal Grover-like quantum speedup. In other words is able to solve an unstructured search problem with the same $\sqrt{N}$ scaling as Grover's original algorithm. A natural question is whether other manifestations of the Zeno effect can also support an optimal speedup in a physically realistic model (through direct analog application rather than indirectly by supporting a universal gateset). In this paper we show that they can support such a speedup, whether due to measurement, decoherence, or even decay of the excited state into a computationally useless state. Our results also suggest a wide variety of methods to realise speedup which do not rely on Zeno behaviour. We group these algorithms into three families to facilitate a structured understanding of how speedups can be obtained: one based on phase kicks, containing adiabatic computation and continuous-time quantum walks; one based on dephasing and measurement; and finally one based on destruction of the amplitude within the excited state, for which we are not aware of any previous results. These results suggest that there may be exciting opportunities for new paradigms of analog quantum computing based on these effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11146v3</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jesse Berwald, Nick Chancellor, Raouf Dridi</dc:creator>
    </item>
    <item>
      <title>Unsupervised End-to-End Training with a Self-Defined Target</title>
      <link>https://arxiv.org/abs/2403.12116</link>
      <description>arXiv:2403.12116v2 Announce Type: replace-cross 
Abstract: Designing algorithms for versatile AI hardware that can learn on the edge using both labeled and unlabeled data is challenging. Deep end-to-end training methods incorporating phases of self-supervised and supervised learning are accurate and adaptable to input data but self-supervised learning requires even more computational and memory resources than supervised learning, too high for current embedded hardware. Conversely, unsupervised layer-by-layer training, such as Hebbian learning, is more compatible with existing hardware but does not integrate well with supervised learning. To address this, we propose a method enabling networks or hardware designed for end-to-end supervised learning to also perform high-performance unsupervised learning by adding two simple elements to the output layer: Winner-Take-All (WTA) selectivity and homeostasis regularization. These mechanisms introduce a "self-defined target" for unlabeled data, allowing purely unsupervised training for both fully-connected and convolutional layers using backpropagation or equilibrium propagation on datasets like MNIST (up to 99.2%), Fashion-MNIST (up to 90.3%), and SVHN (up to 81.5%). We extend this method to semi-supervised learning, adjusting targets based on data type, achieving 96.6% accuracy with only 600 labeled MNIST samples in a multi-layer perceptron. Our results show that this approach can effectively enable networks and hardware initially dedicated to supervised learning to also perform unsupervised learning, adapting to varying availability of labeled data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12116v2</guid>
      <category>cs.NE</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongshu Liu, J\'er\'emie Laydevant, Adrien Pontlevy, Damien Querlioz, Julie Grollier</dc:creator>
    </item>
    <item>
      <title>Received Power Maximization Using Nonuniform Discrete Phase Shifts for RISs With a Limited Phase Range</title>
      <link>https://arxiv.org/abs/2406.16210</link>
      <description>arXiv:2406.16210v2 Announce Type: replace-cross 
Abstract: To maximize the received power at a user equipment, the problem of optimizing a reconfigurable intelligent surface (RIS) with a limited phase range R &lt; 2{\pi} and nonuniform discrete phase shifts with adjustable gains is addressed. Necessary and sufficient conditions to achieve this maximization are given. These conditions are employed in two algorithms to achieve the global optimum in linear time for R {\ge} {\pi} and R &lt; {\pi}, where R is the limited RIS phase range. With a total number of N(2K + 1) complex vector additions, it is shown for R {\ge} {\pi} and R &lt; {\pi} that the global optimality is achieved in NK or fewer and N(K + 1) or fewer steps, respectively, where N is the number of RIS elements and K is the number of discrete phase shifts which may be placed nonuniformly over the limited phase range R. In addition, we define two quantization algorithms that we call nonuniform polar quantization (NPQ) algorithm and extended nonuniform polar quantization (ENPQ) algorithm, where the latter is a novel quantization algorithm for RISs with a significant phase range restriction, i.e., R &lt; {\pi}. With NPQ, we provide a closed-form solution for the approximation ratio with which an arbitrary set of nonuniform discrete phase shifts can approximate the continuous solution. We also show that with a phase range limitation, equal separation among the nonuniform discrete phase shifts maximizes the normalized performance. Furthermore, we show that the gain of using K {\ge} 3 with R &lt; {\pi}/2 and K {\ge} 4 with R &lt; {\pi} is only marginal. Finally, we prove that when R &lt; 2{\pi}/3, ON/OFF selection for the RIS elements brings significant performance compared to the case when the RIS elements are strictly ON.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16210v2</guid>
      <category>eess.SY</category>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dogan Kutay Pekcan, Hongyi Liao, Ender Ayanoglu</dc:creator>
    </item>
    <item>
      <title>Prioritizing High-Consequence Biological Capabilities in Evaluations of Artificial Intelligence Models</title>
      <link>https://arxiv.org/abs/2407.13059</link>
      <description>arXiv:2407.13059v2 Announce Type: replace-cross 
Abstract: As a result of rapidly accelerating AI capabilities, over the past year, national governments and multinational bodies have announced efforts to address safety, security and ethics issues related to AI models. One high priority among these efforts is the mitigation of misuse of AI models. Many biologists have for decades sought to reduce the risks of scientific research that could lead, through accident or misuse, to high-consequence disease outbreaks. Scientists have carefully considered what types of life sciences research have the potential for both benefit and risk (dual-use), especially as scientific advances have accelerated our ability to engineer organisms and create novel variants of pathogens. Here we describe how previous experience and study by scientists and policy professionals of dual-use capabilities in the life sciences can inform risk evaluations of AI models with biological capabilities. We argue that AI model evaluations should prioritize addressing high-consequence risks (those that could cause large-scale harm to the public, such as pandemics), and that these risks should be evaluated prior to model deployment so as to allow potential biosafety and/or biosecurity measures. Scientists' experience with identifying and mitigating dual-use biological risks can help inform new approaches to evaluating biological AI models. Identifying which AI capabilities post the greatest biosecurity and biosafety concerns is necessary in order to establish targeted AI safety evaluation methods, secure these tools against accident and misuse, and avoid impeding immense potential benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13059v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.2139/ssrn.4873106</arxiv:DOI>
      <dc:creator>Jaspreet Pannu, Doni Bloomfield, Alex Zhu, Robert MacKnight, Gabe Gomes, Anita Cicero, Thomas V. Inglesby</dc:creator>
    </item>
  </channel>
</rss>
