<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Aug 2024 01:47:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>[Invited Discussion] Randomization Tests to Address Disruptions in Clinical Trials: A Report from the NISS Ingram Olkin Forum Series on Unplanned Clinical Trial Disruptions</title>
      <link>https://arxiv.org/abs/2408.09060</link>
      <description>arXiv:2408.09060v1 Announce Type: new 
Abstract: Disruptions in clinical trials may be due to external events like pandemics, warfare, and natural disasters. Resulting complications may lead to unforeseen intercurrent events (events that occur after treatment initiation and affect the interpretation of the clinical question of interest or the existence of the measurements associated with it). In Uschner et al. (2023), several example clinical trial disruptions are described: treatment effect drift, population shift, change of care, change of data collection, and change of availability of study medication. A complex randomized controlled trial (RCT) setting with (planned or unplanned) intercurrent events is then described, and randomization tests are presented as a means for non-parametric inference that is robust to violations of assumption typically made in clinical trials. While estimation methods like Targeted Learning (TL) are valid in such settings, we do not see where the authors make the case that one should be going for a randomization test in such disrupted RCTs. In this discussion, we comment on the appropriateness of TL and the accompanying TL Roadmap in the context of disrupted clinical trials. We highlight a few key articles related to the broad applicability of TL for RCTs and real-world data (RWD) analyses with intercurrent events. We begin by introducing TL and motivating its utility in Section 2, and then in Section 3 we provide a brief overview of the TL Roadmap. In Section 4 we recite the example clinical trial disruptions presented in Uschner et al. (2023), discussing considerations and solutions based on the principles of TL. We request in an authors' rejoinder a clear theoretical demonstration with specific examples in this setting that a randomization test is the only valid inferential method relative to one based on following the TL Roadmap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09060v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachael V. Phillips, Mark J. van der Laan</dc:creator>
    </item>
    <item>
      <title>Atlanta Gun Violence Modeling via Nonstationary Spatio-temporal Point Processes</title>
      <link>https://arxiv.org/abs/2408.09258</link>
      <description>arXiv:2408.09258v1 Announce Type: new 
Abstract: Analysis of gun violence in the United States has utilized various models based on spatiotemporal point processes. Previous studies have identified a contagion effect in gun violence, characterized by bursts of diffusion across urban environments, which can be effectively represented using the self-excitatory spatiotemporal Hawkes process. The Hawkes process and its variants have been successful in modeling self-excitatory events, including earthquakes, disease outbreaks, financial market movements, neural activity, and the viral spread of memes on social networks. However, existing Hawkes models applied to gun violence often rely on simplistic stationary kernels, which fail to account for the complex, non-homogeneous spread of influence and impact over space and time. To address this limitation, we adopt a non-stationary spatiotemporal point process model that incorporates a neural network-based kernel to better represent the varied correlations among events of gun violence. Our study analyzes a comprehensive dataset of approximately 16,000 gunshot events in the Atlanta metropolitan area from 2021 to 2023. The cornerstone of our approach is the innovative non-stationary kernel, designed to enhance the model's expressiveness while preserving its interpretability. This approach not only demonstrates strong predictive performance but also provides insights into the spatiotemporal dynamics of gun violence and its propagation within urban settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09258v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheng Dong, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Markov modeling for a satellite tag data record of whale diving behavior</title>
      <link>https://arxiv.org/abs/2408.09557</link>
      <description>arXiv:2408.09557v1 Announce Type: new 
Abstract: Cuvier's beaked whales (Ziphius cavirostris) are the deepest diving marine mammal, consistently diving to depths exceeding 1,000m for durations longer than an hour, making them difficult animals to study. They are important to study because they are sensitive to disturbances from naval sonar. Satellite-linked telemetry devices provide up to 14-day long records of dive behavior. However, the time series of depths is discretized to coarse bins due to bandwidth limitations. We analyze telemetry data from beaked whales that were exposed to moderate levels of sonar within controlled exposure experiments (CEEs) to study behavioral responses to sound exposure. We model the data as a hidden Markov model (HMM) over the time series of discrete depth bins, introducing partially observed movement types and recent diving activity covariates to model marginal non-stationarity. Movement types provide more flexible modeling for CEEs than partially observed dive stages, which are more commonly used in dive behavior HMMs. We estimate the proposed model within a hierarchical Bayesian framework, using HMM methods to compute marginalized likelihoods and posterior predictive distributions. We assess behavioral response by comparing observed post-exposure behavior to usual unexposed behavior via the posterior predictive distribution. The model quantifies patterns in baseline diving behavior and finds evidence that beaked whales deviate in response to sound. We find evidence that (i) beaked whales initially shorten the time they spend between deep dives, which may have physiological effects and (ii) subsequently avoid deep dives, which can result in lost foraging opportunities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09557v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Hewitt, Nicola J. Quick, Alan E. Gelfand, Robert S. Schick</dc:creator>
    </item>
    <item>
      <title>Bayesian Uncertainty Quantification and Reliability Assessment for Mars Sample Return</title>
      <link>https://arxiv.org/abs/2408.10083</link>
      <description>arXiv:2408.10083v1 Announce Type: new 
Abstract: In this paper, we employ a Bayesian approach to assess the reliability of a critical component in the Mars Sample Return program, focusing on the Earth Entry System's risk of containment not assured upon reentry. Our study uses Gaussian Process modeling under a Bayesian regime to analyze the Earth Entry System's resilience against operational stress. This Bayesian framework allows for a detailed probabilistic evaluation of the risk of containment not assured, indicating the feasibility of meeting the mission's stringent safety goal of 0.999999 probability of success. The findings underscore the effectiveness of Bayesian methods for complex uncertainty quantification analyses of computer simulations, providing valuable insights for computational reliability analysis in a risk-averse setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10083v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dawn L. Sanderson, Amy Braverman, Giuseppe Cataldo, Ralph C. Smith, Richard L. Smith</dc:creator>
    </item>
    <item>
      <title>Autoregressive models for panel data causal inference with application to state-level opioid policies</title>
      <link>https://arxiv.org/abs/2408.09012</link>
      <description>arXiv:2408.09012v1 Announce Type: cross 
Abstract: Motivated by the study of state opioid policies, we propose a novel approach that uses autoregressive models for causal effect estimation in settings with panel data and staggered treatment adoption. Specifically, we seek to estimate of the impact of key opioid-related policies by quantifying the effects of must access prescription drug monitoring programs (PDMPs), naloxone access laws (NALs), and medical marijuana laws on opioid prescribing. Existing methods, such as differences-in-differences and synthetic controls, are challenging to apply in these types of dynamic policy landscapes where multiple policies are implemented over time and sample sizes are small. Autoregressive models are an alternative strategy that have been used to estimate policy effects in similar settings, but until this paper have lacked formal justification. We outline a set of assumptions that tie these models to causal effects, and we study biases of estimates based on this approach when key causal assumptions are violated. In a set of simulation studies that mirror the structure of our application, we also show that our proposed estimators frequently outperform existing estimators. In short, we justify the use of autoregressive models to provide robust evidence on the effectiveness of four state policies in combating the opioid crisis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09012v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Antonelli, Max Rubinstein, Denis Agniel, Rosanna Smart, Elizabeth Stuart, Matthew Cefalu, Terry Schell, Joshua Eagan, Elizabeth Stone, Max Griswold, Mark Sorbero, Beth Ann Griffin</dc:creator>
    </item>
    <item>
      <title>From Urban Clusters to Megaregions: Mapping Australia's Evolving Urban Regions</title>
      <link>https://arxiv.org/abs/2408.09054</link>
      <description>arXiv:2408.09054v1 Announce Type: cross 
Abstract: This study employs percolation theory to investigate the hierarchical organisation of Australian urban centres through the connectivity of their road networks. The analysis demonstrates how discrete urban clusters have developed into integrated regional entities, delineating the pivotal distance thresholds that regulate these urban transitions. The study reveals the interconnections between disparate urban clusters, shaped by their functional differentiation and historical development. Furthermore, the study identifies a dichotomy of urban agglomeration forces and a persistent spatial disconnection between Australia's wider urban landscape. This highlights the interplay between urban densification and peripheral growth. It suggests the need for new thinking on potential integrated governance structures that bridge urban development with broader social and economic policies across regional and national scales. Additionally, the study emphasises the growing importance of national coordination in Australian urban development planning to ensure regional consistency, equity, and productivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09054v1</guid>
      <category>physics.soc-ph</category>
      <category>physics.app-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. K. M Ng, Z. Shabrina, S. Sarkar, H. Han, C. Pettit</dc:creator>
    </item>
    <item>
      <title>Approximate Differentiable Likelihoods for Astroparticle Physics Experiments</title>
      <link>https://arxiv.org/abs/2408.09057</link>
      <description>arXiv:2408.09057v1 Announce Type: cross 
Abstract: Traditionally, inference in liquid xenon direct detection dark matter experiments has used estimators of event energy or density estimation of simulated data. Such methods have drawbacks compared to the computation of explicit likelihoods, such as an inability to conduct statistical inference in high-dimensional parameter spaces, or a failure to make use of all available information. In this work, we implement a continuous approximation of an event simulator model within a probabilistic programming framework, allowing for the application of high performance gradient-based inference methods such as the No-U-Turn Sampler. We demonstrate an improvement in inference results, with percent-level decreases in measurement uncertainties. Finally, in the case where some observables can be measured using multiple independent channels, such a method also enables the incorporation of additional information seamlessly, allowing for full use of the available information to be made.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09057v1</guid>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juehang Qin, Christopher Tunnell</dc:creator>
    </item>
    <item>
      <title>A Likelihood-Free Approach to Goal-Oriented Bayesian Optimal Experimental Design</title>
      <link>https://arxiv.org/abs/2408.09582</link>
      <description>arXiv:2408.09582v1 Announce Type: cross 
Abstract: Conventional Bayesian optimal experimental design seeks to maximize the expected information gain (EIG) on model parameters. However, the end goal of the experiment often is not to learn the model parameters, but to predict downstream quantities of interest (QoIs) that depend on the learned parameters. And designs that offer high EIG for parameters may not translate to high EIG for QoIs. Goal-oriented optimal experimental design (GO-OED) thus directly targets to maximize the EIG of QoIs.
  We introduce LF-GO-OED (likelihood-free goal-oriented optimal experimental design), a computational method for conducting GO-OED with nonlinear observation and prediction models. LF-GO-OED is specifically designed to accommodate implicit models, where the likelihood is intractable. In particular, it builds a density ratio estimator from samples generated from approximate Bayesian computation (ABC), thereby sidestepping the need for likelihood evaluations or density estimations. The overall method is validated on benchmark problems with existing methods, and demonstrated on scientific applications of epidemiology and neural science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09582v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atlanta Chakraborty, Xun Huan, Tommie Catanach</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Regression with Imputed Binary Covariates with Application to Emotion Recognition</title>
      <link>https://arxiv.org/abs/2408.09619</link>
      <description>arXiv:2408.09619v1 Announce Type: cross 
Abstract: In the flourishing live streaming industry, accurate recognition of streamers' emotions has become a critical research focus, with profound implications for audience engagement and content optimization. However, precise emotion coding typically requires manual annotation by trained experts, making it extremely expensive and time-consuming to obtain complete observational data for large-scale studies. Motivated by this challenge in streamer emotion recognition, we develop here a novel imputation method together with a principled statistical inference procedure for analyzing partially observed binary data. Specifically, we assume for each observation an auxiliary feature vector, which is sufficiently cheap to be fully collected for the whole sample. We next assume a small pilot sample with both the target binary covariates (i.e., the emotion status) and the auxiliary features fully observed, of which the size could be considerably smaller than that of the whole sample. Thereafter, a regression model can be constructed for the target binary covariates and the auxiliary features. This enables us to impute the missing binary features using the fully observed auxiliary features for the entire sample. We establish the associated asymptotic theory for principled statistical inference and present extensive simulation experiments, demonstrating the effectiveness and theoretical soundness of our proposed method. Furthermore, we validate our approach using a comprehensive dataset on emotion recognition in live streaming, demonstrating that our imputation method yields smaller standard errors and is more statistically efficient than using pilot data only. Our findings have significant implications for enhancing user experience and optimizing engagement on streaming platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09619v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziqian Lin, Danyang Huang, Ziyu Xiong, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Regional and spatial dependence of poverty factors in Thailand, and its use into Bayesian hierarchical regression analysis</title>
      <link>https://arxiv.org/abs/2408.09760</link>
      <description>arXiv:2408.09760v1 Announce Type: cross 
Abstract: Poverty is a serious issue that harms humanity progression. The simplest solution is to use one-shirt-size policy to alleviate it. Nevertheless, each region has its unique issues, which require a unique solution to solve them. In the aspect of spatial analysis, neighbor regions can provide useful information to analyze issues of a given region. In this work, we proposed inferred boundaries of regions of Thailand that can explain better the poverty dynamics, instead of the usual government administrative regions. The proposed regions maximize a trade-off between poverty-related features and geographical coherence. We use a spatial analysis together with Moran's cluster algorithms and Bayesian hierarchical regression models, with the potential of assist the implementation of the right policy to alleviate the poverty phenomenon. We found that all variables considered show a positive spatial autocorrelation. The results of analysis illustrate that 1) Northern, Northeastern Thailand, and in less extend Northcentral Thailand are the regions that require more attention in the aspect of poverty issues, 2) Northcentral, Northeastern, Northern and Southern Thailand present dramatically low levels of education, income and amount of savings contrasted with large cities such as Bangkok-Pattaya and Central Thailand, and 3) Bangkok-Pattaya is the only region whose average years of education is above 12 years, which corresponds (approx.) with a complete senior high school.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09760v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Irving G\'omez-M\'endez, Chainarong Amornbunchornvej</dc:creator>
    </item>
    <item>
      <title>A non-parametric U-statistic testing approach for multi-arm clinical trials with multivariate longitudinal data</title>
      <link>https://arxiv.org/abs/2408.10149</link>
      <description>arXiv:2408.10149v1 Announce Type: cross 
Abstract: Randomized clinical trials (RCTs) often involve multiple longitudinal primary outcomes to comprehensively assess treatment efficacy. The Longitudinal Rank-Sum Test (LRST), a robust U-statistics-based, non-parametric, rank-based method, effectively controls Type I error and enhances statistical power by leveraging the temporal structure of the data without relying on distributional assumptions. However, the LRST is limited to two-arm comparisons. To address the need for comparing multiple doses against a control group in many RCTs, we extend the LRST to a multi-arm setting. This novel multi-arm LRST provides a flexible and powerful approach for evaluating treatment efficacy across multiple arms and outcomes, with a strong capability for detecting the most effective dose in multi-arm trials. Extensive simulations demonstrate that this method maintains excellent Type I error control while providing greater power compared to the two-arm LRST with multiplicity adjustments. Application to the Bapineuzumab (Bapi) 301 trial further validates the multi-arm LRST's practical utility and robustness, confirming its efficacy in complex clinical trial analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10149v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, Sheng Luo</dc:creator>
    </item>
    <item>
      <title>Measuring Diagnostic Test Performance Using Imperfect Reference Tests: A Partial Identification Approach</title>
      <link>https://arxiv.org/abs/2204.00180</link>
      <description>arXiv:2204.00180v4 Announce Type: replace 
Abstract: Diagnostic tests are almost never perfect. Studies quantifying their performance use knowledge of the true health status, measured with a reference diagnostic test. Researchers commonly assume that the reference test is perfect, which is often not the case in practice. When the assumption fails, conventional studies identify "apparent" performance or performance with respect to the reference, but not true performance. This paper provides the smallest possible bounds on the measures of true performance - sensitivity (true positive rate) and specificity (true negative rate), or equivalently false positive and negative rates, in standard settings. Implied bounds on policy-relevant parameters are derived: 1) Prevalence in screened populations; 2) Predictive values. Methods for inference based on moment inequalities are used to construct uniformly consistent confidence sets in level over a relevant family of data distributions. Emergency Use Authorization (EUA) and independent study data for the BinaxNOW COVID-19 antigen test demonstrate that the bounds can be very informative. Analysis reveals that the estimated false negative rates for symptomatic and asymptomatic patients are up to 3.17 and 4.59 times higher than the frequently cited "apparent" false negative rate. Further applicability of the results in the context of imperfect proxies such as survey responses and imputed protected classes is indicated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.00180v4</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filip Obradovi\'c</dc:creator>
    </item>
    <item>
      <title>Integrating socioeconomic and geographic data to enhance infectious disease prediction in Brazilian cities</title>
      <link>https://arxiv.org/abs/2405.01422</link>
      <description>arXiv:2405.01422v2 Announce Type: replace 
Abstract: Supervised machine learning models and public surveillance data has been employed for infectious disease forecasting in many settings. These models leverage various data sources capturing drivers of disease spread, such as climate conditions or human behavior. However, few models have incorporated the organizational structure of different geographic locations for forecasting. Traveling waves of seasonal outbreaks have been reported for dengue, influenza, and other infectious diseases, and many of the drivers of infectious disease dynamics may be shared across different cities, either due to their geographic or socioeconomic proximity. In this study, we developed a machine learning model to predict case counts of four infectious diseases across Brazilian cities one week ahead by incorporating information from related cities. We compared selecting related cities using both geographic distance and GDP per capita. Incorporating information from geographically proximate cities improved predictive performance for two of the four diseases, specifically COVID-19 and Zika. We also discuss the impact on forecasts in the presence of anomalous contagion patterns and the limitations of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01422v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luiza Lober, Kirstin O. Roster, Francisco A. Rodrigues</dc:creator>
    </item>
    <item>
      <title>Evaluating Binary Outcome Classifiers Estimated from Survey Data</title>
      <link>https://arxiv.org/abs/2311.00596</link>
      <description>arXiv:2311.00596v4 Announce Type: replace-cross 
Abstract: Surveys are commonly used to facilitate research in epidemiology, health, and the social and behavioral sciences. Often, these surveys are not simple random samples, and respondents are given weights reflecting their probability of selection into the survey. It is well known that analysts can use these survey weights to produce unbiased estimates of population quantities like totals. In this article, we show that survey weights also can be beneficial for evaluating the quality of predictive models when splitting data into training and test sets. In particular, we characterize model assessment statistics, such as sensitivity and specificity, as finite population quantities, and compute survey-weighted estimates of these quantities with sample test data comprising a random subset of the original data.Using simulations with data from the National Survey on Drug Use and Health and the National Comorbidity Survey, we show that unweighted metrics estimated with sample test data can misrepresent population performance, but weighted metrics appropriately adjust for the complex sampling design. We also show that this conclusion holds for models trained using upsampling for mitigating class imbalance. The results suggest that weighted metrics should be used when evaluating performance on sample test data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00596v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1097/EDE.0000000000001776</arxiv:DOI>
      <arxiv:journal_reference>Epidemiology (2024)</arxiv:journal_reference>
      <dc:creator>Adway S. Wadekar, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Transfer Learning for Building High-Dimensional Generalized Linear Models with Disparate Datasets</title>
      <link>https://arxiv.org/abs/2312.12786</link>
      <description>arXiv:2312.12786v2 Announce Type: replace-cross 
Abstract: Development of comprehensive prediction models are often of great interest in many disciplines of science, but datasets with information on all desired features often have small sample sizes. We describe a transfer learning approach for building high-dimensional generalized linear models using data from a main study with detailed information on all predictors and an external, potentially much larger, study that has ascertained a more limited set of predictors. We propose using the external dataset to build a reduced model and then "transfer" the information on underlying parameters for the analysis of the main study through a set of calibration equations which can account for the study-specific effects of design variables. We then propose a penalized generalized method of moment framework for inference and a one-step estimation method that could be implemented using standard glmnet package. We develop asymptotic theory and conduct extensive simulation studies to investigate both predictive performance and post-selection inference properties of the proposed method. Finally, we illustrate an application of the proposed method for the development of risk models for five common diseases using the UK Biobank study, combining information on low-dimensional risk factors and high throughout proteomic biomarkers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12786v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruzhang Zhao, Prosenjit Kundu, Arkajyoti Saha, Nilanjan Chatterjee</dc:creator>
    </item>
    <item>
      <title>Wilcoxon Nonparametric CFAR Scheme for Ship Detection in SAR Image</title>
      <link>https://arxiv.org/abs/2402.18579</link>
      <description>arXiv:2402.18579v2 Announce Type: replace-cross 
Abstract: The parametric constant false alarm rate (CFAR) detection algorithms which are based on various statistical distributions, such as Gaussian, Gamma, Weibull, log-normal, G0 distribution, alpha-stable distribution, etc, are most widely used to detect the ship targets in SAR image at present. However, the clutter background in SAR images is complicated and variable. When the actual clutter background deviates from the assumed statistical distribution, the performance of the parametric CFAR detector will deteriorate. In addition to the parametric CFAR schemes, there is another class of nonparametric CFAR detectors which can maintain a constant false alarm rate for the target detection without the assumption of a known clutter distribution. In this work, the Wilcoxon nonparametric CFAR scheme for ship detection in SAR image is proposed and analyzed, and a closed form of the false alarm rate for the Wilcoxon nonparametric detector to determine the decision threshold is presented. By comparison with several typical parametric CFAR schemes on Radarsat-2, ICEYE-X6 and Gaofen-3 SAR images, the robustness of the Wilcoxon nonparametric detector to maintain a good false alarm performance in different detection backgrounds is revealed, and its detection performance for the weak ship in rough sea surface is improved to some extent. Moreover, the Wilcoxon nonparametric detector can suppress the false alarms resulting from the sidelobes at some degree and its detection speed is fast.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18579v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangwei Meng</dc:creator>
    </item>
    <item>
      <title>Tackling GenAI Copyright Issues: Originality Estimation and Genericization</title>
      <link>https://arxiv.org/abs/2406.03341</link>
      <description>arXiv:2406.03341v3 Announce Type: replace-cross 
Abstract: The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. While various techniques for mitigating copyright issues have been studied, significant risks remain. Here, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright. To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework. This metric can be practically estimated by drawing samples from a generative model, which is then used for the genericization process. As a practical implementation, we introduce PREGen, which combines our genericization method with an existing mitigation technique. Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images. PREGen dramatically improves the performance of the existing method, reducing the likelihood of generating copyrighted characters by more than half when the names of copyrighted characters are used as the prompt. Additionally, although generative models have been found to generate copyrighted characters even when the names of characters are not directly mentioned in the prompt, PREGen almost completely suppresses the generation of copyrighted characters for such prompts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03341v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroaki Chiba-Okabe, Weijie J. Su</dc:creator>
    </item>
  </channel>
</rss>
