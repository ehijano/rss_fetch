<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Jun 2025 04:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Transcendental Encoding conjecture</title>
      <link>https://arxiv.org/abs/2506.18921</link>
      <description>arXiv:2506.18921v1 Announce Type: new 
Abstract: We propose the Transcendental Encoding Conjecture for decision problems, which asserts that every language in complexity class P encodes to an algebraic real (possibly rational or algebraic irrational) under its binary characteristic encoding or other relevant encodings, whereas every NP-complete language encodes to a transcendental real. In particular, we exhibit languages whose encodings are provably rational (hence algebraic), discuss the status of encodings for other "natural" languages such as PRIMES (its encoding is irrational but not known to be algebraic).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18921v1</guid>
      <category>cs.CC</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anand Kumar Keshavan, Sunu Engineer</dc:creator>
    </item>
    <item>
      <title>A primer on the closure of algebraic complexity classes under factoring</title>
      <link>https://arxiv.org/abs/2506.19604</link>
      <description>arXiv:2506.19604v1 Announce Type: new 
Abstract: Polynomial factorization is a fundamental problem in computational algebra. Over the past half century, a variety of algorithmic techniques have been developed to tackle different variants of this problem. In parallel, algebraic complexity theory classifies polynomials into complexity classes based on their perceived `hardness'. This raises a natural question: Do these classes afford efficient factorization?
  In this survey, we revisit two pivotal techniques in polynomial factorization: Hensel lifting and Newton iteration. Though they are variants of the same theme, their distinct applications across the literature warrant separate treatment. These techniques have played an important role in resolving key factoring questions in algebraic complexity theory. We examine and organise the known results through the lens of these techniques to highlight their impact. We also discuss their equivalence while reflecting on how their use varies with the context of the problem.
  We focus on four prominent complexity classes: circuits of polynomial size ($\text{VP}_{\text{nb}}$), circuits with both polynomial size and degree (VP and its border $\overline{\text{VP}}$), verifier circuits of polynomial size and degree (VNP), and polynomial-size algebraic branching programs (VBP). We also examine more restricted models, such as formulas and bounded-depth circuits. Along the way, we list several open problems that remain unresolved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19604v1</guid>
      <category>cs.CC</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C. S. Bhargav, Prateek Dwivedi, Nitin Saxena</dc:creator>
    </item>
    <item>
      <title>Algorithmic hardness of the partition function for nucleic acid strands</title>
      <link>https://arxiv.org/abs/2506.19756</link>
      <description>arXiv:2506.19756v1 Announce Type: new 
Abstract: To understand and engineer biological and artificial nucleic acid systems, algorithms are employed for prediction of secondary structures at thermodynamic equilibrium. Dynamic programming algorithms are used to compute the most favoured, or Minimum Free Energy (MFE), structure, and the Partition Function (PF), a tool for assigning a probability to any structure. However, in some situations, such as when there are large numbers of strands, or pseudoknoted systems, NP-hardness results show that such algorithms are unlikely, but only for MFE. Curiously, algorithmic hardness results were not shown for PF, leaving two open questions on the complexity of PF for multiple strands and single strands with pseudoknots. The challenge is that while the MFE problem cares only about one, or a few structures, PF is a summation over the entire secondary structure space, giving theorists the vibe that computing PF should not only be as hard as MFE, but should be even harder.
  We answer both questions. First, we show that computing PF is #P-hard for systems with an unbounded number of strands, answering a question of Condon Hajiaghayi, and Thachuk [DNA27]. Second, for even a single strand, but allowing pseudoknots, we find that PF is #P-hard. Our proof relies on a novel magnification trick that leads to a tightly-woven set of reductions between five key thermodynamic problems: MFE, PF, their decision versions, and #SSEL that counts structures of a given energy. Our reductions show these five problems are fundamentally related for any energy model amenable to magnification. That general classification clarifies the mathematical landscape of nucleic acid energy models and yields several open questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19756v1</guid>
      <category>cs.CC</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gwendal Ducloz, Ahmed Shalaby, Damien Woods</dc:creator>
    </item>
    <item>
      <title>The Autonomy of the Lightning Network: A Mathematical and Economic Proof of Structural Decoupling from BTC</title>
      <link>https://arxiv.org/abs/2506.19333</link>
      <description>arXiv:2506.19333v1 Announce Type: cross 
Abstract: This paper presents a formal analysis of the Lightning Network as a monetary system structurally diverging from Bitcoin's base-layer settlement model. We demonstrate that under increasing transaction demand, BTC transaction fees rise superlinearly due to throughput constraints, while Lightning Network routing costs approach a bounded asymptote. Using mathematical modeling, game-theoretic proofs, and complexity analysis, we show that Lightning enables indefinite off-chain operation via the emergence of liquidity hub oligopolies. These hubs exhibit properties of unregulated financial intermediaries, including rent extraction, opacity, and systemic fragility. Strategic agent models show that channel closure becomes economically infeasible, and routing problems approach hardness limits in P-Space complexity. We conclude that Lightning does not merely extend Bitcoin, but constitutes a synthetic financial system with shadowbank characteristics, lacking reserve discipline, transparency, or enforceable settlement guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19333v1</guid>
      <category>cs.DC</category>
      <category>cs.CC</category>
      <category>cs.ET</category>
      <category>cs.GT</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Craig Steven Wright</dc:creator>
    </item>
    <item>
      <title>Subcoloring of (Unit) Disk Graphs</title>
      <link>https://arxiv.org/abs/2506.19452</link>
      <description>arXiv:2506.19452v1 Announce Type: cross 
Abstract: A subcoloring of a graph is a partition of its vertex set into subsets (called colors), each inducing a disjoint union of cliques. It is a natural generalization of the classical proper coloring, in which each color must instead induce an independent set. Similarly to proper coloring, we define the subchromatic number of a graph as the minimum integer k such that it admits a subcoloring with k colors, and the corresponding problem k-Subcoloring which asks whether a graph has subchromatic number at most k. In this paper, we initiate the study of the subcoloring of (unit) disk graphs. One motivation stems from the fact that disk graphs can be seen as a dense generalization of planar graphs where, intuitively, each vertex can be blown into a large clique--much like subcoloring generalizes proper coloring. Interestingly, it can be observed that every unit disk graph admits a subcoloring with at most 7 colors. We first prove that the subchromatic number can be 3-approximated in polynomial-time in unit disk graphs. We then present several hardness results for special cases of unit disk graphs which somehow prevents the use of classical approaches for improving this result. We show in particular that 2-subcoloring remains NP-hard in triangle-free unit disk graphs, as well as in unit disk graphs representable within a strip of bounded height. We also solve an open question of Broersma, Fomin, Ne\v{s}et\v{r}il, and Woeginger (2002) by proving that 3-Subcoloring remains NP-hard in co-comparability graphs. Finally, we prove that every $n$-vertex disk graph admits a subcoloring with at most $O(\log^3(n))$ colors and present a $O(\log^2(n))$-approximation algorithm for computing the subchromatic number of such graphs. This is achieved by defining a decomposition and a special type of co-comparability disk graph, called $\Delta$-disk graphs, which might be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19452v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malory Marin, R\'emi Watrigant</dc:creator>
    </item>
    <item>
      <title>From Worst-Case Hardness of $\mathsf{NP}$ to Quantum Cryptography via Quantum Indistinguishability Obfuscation</title>
      <link>https://arxiv.org/abs/2506.19542</link>
      <description>arXiv:2506.19542v1 Announce Type: cross 
Abstract: Indistinguishability obfuscation (iO) has emerged as a powerful cryptographic primitive with many implications. While classical iO, combined with the infinitely-often worst-case hardness of $\mathsf{NP}$, is known to imply one-way functions (OWFs) and a range of advanced cryptographic primitives, the cryptographic implications of quantum iO remain poorly understood. In this work, we initiate a study of the power of quantum iO. We define several natural variants of quantum iO, distinguished by whether the obfuscation algorithm, evaluation algorithm, and description of obfuscated program are classical or quantum. For each variant, we identify quantum cryptographic primitives that can be constructed under the assumption of quantum iO and the infinitely-often quantum worst-case hardness of $\mathsf{NP}$ (i.e., $\mathsf{NP} \not\subseteq \mathsf{i.o.BQP}$). In particular, we construct pseudorandom unitaries, QCCC quantum public-key encryption and (QCCC) quantum symmetric-key encryption, and several primitives implied by them such as one-way state generators, (efficiently-verifiable) one-way puzzles, and EFI pairs, etc. While our main focus is on quantum iO, even in the classical setting, our techniques yield a new and arguably simpler construction of OWFs from classical (imperfect) iO and the infinitely-often worst-case hardness of $\mathsf{NP}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19542v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomoyuki Morimae, Yuki Shirakawa, Takashi Yamakawa</dc:creator>
    </item>
    <item>
      <title>Collapses in quantum-classical probabilistically checkable proofs and the quantum polynomial hierarchy</title>
      <link>https://arxiv.org/abs/2506.19792</link>
      <description>arXiv:2506.19792v1 Announce Type: cross 
Abstract: We investigate structural properties of quantum proof systems by establishing collapse results that uncover simplifications in their complexity landscape. We extend classical results such as the Karp-Lipton theorem to quantum polynomial hierarchy with quantum proofs and establish uniqueness preservation for quantum-classical probabilistically checkable proof systems.
  Our main contributions are threefold. First, we prove that restricting quantum-classical PCP systems to uniqueness does not reduce computational power: $\mathsf{UniqueQCPCP} = \mathsf{QCPCP}$ under $\mathsf{BQ}$-operator and randomized reductions, demonstrating robustness similar to the $\mathsf{UniqueQCMA} = \mathsf{QCMA}$ result. Second, we establish a non-uniform quantum analogue of the Karp-Lipton theorem, showing that if $\mathsf{QMA} \subseteq \mathsf{BQP}/\mathsf{qpoly}$, then $\mathsf{QPH} \subseteq \mathsf{Q\Sigma}_2/\mathsf{qpoly}$, extending the classical collapse theorem to quantum complexity with quantum advice. Third, we introduce a consistent variant of the quantum polynomial hierarchy ($\mathsf{CQPH}$) with consistency constraints across interaction rounds while maintaining product-state proofs, proving its unconditional collapse $\mathsf{CQPH} = \mathsf{CQ\Sigma}_2$. This contrasts with prior work on quantum-entangled polynomial hierarchy, showing that consistency rather than entanglement drives the collapse.
  These results contribute to understanding structural boundaries in quantum complexity theory and the interplay between constraint types in quantum proof systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19792v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kartik Anand, Kabgyun Jeong, Junseo Lee</dc:creator>
    </item>
    <item>
      <title>Treewidth Inapproximability and Tight ETH Lower Bound</title>
      <link>https://arxiv.org/abs/2406.11628</link>
      <description>arXiv:2406.11628v2 Announce Type: replace 
Abstract: We present a simple, self-contained, linear reduction from 3-SAT to Treewidth. Specifically, it shows that 1.00005-approximating Treewidth is NP-hard, and solving Treewidth exactly requires $2^{\Omega(n)}$ time, unless the Exponential-Time Hypothesis fails. We further derive, under the latter assumption, that there are some constants $\delta &gt; 1$ and $c&gt;0$ such that $\delta$-approximating Treewidth requires time $2^{\Omega(n/\log^c n)}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11628v2</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Edouard Bonnet</dc:creator>
    </item>
    <item>
      <title>Some easy optimization problems have the overlap-gap property</title>
      <link>https://arxiv.org/abs/2411.01836</link>
      <description>arXiv:2411.01836v3 Announce Type: replace 
Abstract: We show that the shortest $s$-$t$ path problem has the overlap-gap property in (i) sparse $\mathbf{G}(n,p)$ graphs and (ii) complete graphs with i.i.d. Exponential edge weights. Furthermore, we demonstrate that in sparse $\mathbf{G}(n,p)$ graphs, shortest path is solved by $O(\log n)$-degree polynomial estimators, and a uniform approximate shortest path can be sampled in polynomial time. This constitutes the first example in which the overlap-gap property is not predictive of algorithmic intractability for a (non-algebraic) average-case optimization problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01836v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuangping Li, Tselil Schramm</dc:creator>
    </item>
  </channel>
</rss>
