<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.OT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.OT</link>
    <description>q-bio.OT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.OT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Aug 2025 04:30:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Computer Vision and Depth Sensor-Powered Smart Cane for Real-Time Obstacle Detection and Navigation Assistance for the Visually Impaired</title>
      <link>https://arxiv.org/abs/2508.16698</link>
      <description>arXiv:2508.16698v1 Announce Type: new 
Abstract: Visual impairment impacts more than 2.2 billion people worldwide, and it greatly restricts independent mobility and access. Conventional mobility aids - white canes and ultrasound-based intelligent canes - are inherently limited in the feedback they can offer and generally will not be able to differentiate among types of obstacles in dense or complex environments. Here, we introduce the IoT Cane, an internet of things assistive navigation tool that integrates real-time computer vision with a transformer-based RT-DETRv3-R50 model alongside depth sensing through the Intel RealSense camera. Our prototype records a mAP of 53.4% and an AP50 of 71.7% when tested on difficult datasets with low Intersection over Union (IoU) boundaries, outperforming similar ultrasound-based systems. Latency in end-to-end mode is around 150 ms per frame, accounting for preprocessing (1-3 ms), inference (50-70 ms), and post-processing (0.5-1.0 ms per object detected). Feedback is provided through haptic vibration motors and audio notifications driven by a LiPo battery, which controls power using a PowerBoost module. Future directions involve iOS integration to tap into more compute, hardware redesign to minimize cost, and mobile companion app support over Bluetooth. This effort offers a strong, extensible prototype toward large-scale vision-based assistive technology for the visually impaired.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16698v1</guid>
      <category>q-bio.OT</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunkalp Chandra, Umang Sharma, Devesh Khilnani</dc:creator>
    </item>
    <item>
      <title>EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models</title>
      <link>https://arxiv.org/abs/2506.01608</link>
      <description>arXiv:2506.01608v2 Announce Type: replace-cross 
Abstract: Understanding behavior requires datasets that capture humans while carrying out complex tasks. The kitchen is an excellent environment for assessing human motor and cognitive function, as many complex actions are naturally exhibited in kitchens from chopping to cleaning. Here, we introduce the EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture platform inside a kitchen environment. Nine static RGB-D cameras, inertial measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is a multi-view action dataset with synchronized exocentric, egocentric, depth, IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects cooking four different recipes. Action sequences were densely annotated with 33.78 action segments per minute. Leveraging this multi-modal dataset, we propose four benchmarks to advance behavior understanding and modeling through 1) a vision-language benchmark, 2) a semantic text-to-motion generation benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to pave the way for better methods as well as insights to understand the nature of ecologically-valid human behavior. Code and data are available at https://github.com/amathislab/EPFL-Smart-Kitchen</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01608v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.OT</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andy Bonnetto, Haozhe Qi, Franklin Leong, Matea Tashkovska, Mahdi Rad, Solaiman Shokur, Friedhelm Hummel, Silvestro Micera, Marc Pollefeys, Alexander Mathis</dc:creator>
    </item>
    <item>
      <title>GloBIAS: strengthening the foundations of BioImage Analysis</title>
      <link>https://arxiv.org/abs/2507.06407</link>
      <description>arXiv:2507.06407v2 Announce Type: replace-cross 
Abstract: There is a global need for BioImage Analysis (BIA) as advances in life sciences increasingly rely on cutting-edge imaging systems that have dramatically expanded the complexity and dimensionality of biological images. Turning these data into scientific discoveries requires people with effective data management skills and knowledge of state-of-the-art image processing and data analysis, in other words, BioImage Analysts. The Global BioImage Analysts' Society (GloBIAS) aims to enhance the profile of BioImage Analysts as a key role in science and research. Its vision encompasses fostering a global network, democratising access to BIA by providing educational resources tailored to various proficiency levels and disciplines, while also establishing guidelines for BIA courses. By collaboratively shaping the education of BioImage Analysts, GloBIAS aims to unlock the full potential of BIA in advancing life science research and to consolidate BIA as a career path. To better understand the needs and geographical representation of the BIA community, a worldwide survey was conducted and 291 responses were collected across people from all career stages and continents. This work discusses how GloBIAS aims to address community-identified shortcomings in work environment, funding, and scientific activities. The survey underscores a strong interest from the BIA community in activities proposed by GloBIAS and their interest to actively contribute. With 72% of respondents willing to pay for membership, the community's enthusiasm for both online and in-person events is set to drive the growth and sustainability of GloBIAS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06407v2</guid>
      <category>physics.bio-ph</category>
      <category>eess.IV</category>
      <category>q-bio.OT</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agustin Andres Corbat, Christa G Walther, Laura Rodr\'iguez de la Ballina, Nicholas David Condon, Alessandro A Felder, Martin Sch\"atz, Bettina Schmerl, Ko Sugawara, Clara Prats, Anna Klemm, Florian Levet, Kota Miura, Paula Sampaio, Christian Tischer, Rocco D'Antuono, Beth A Cimini, Robert Haase</dc:creator>
    </item>
  </channel>
</rss>
