<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Jan 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A new Monte Carlo method for valid prior-free possibilistic statistical inference</title>
      <link>https://arxiv.org/abs/2501.10585</link>
      <description>arXiv:2501.10585v1 Announce Type: new 
Abstract: Inferential models (IMs) offer prior-free, Bayesian-like, posterior degrees of belief designed for statistical inference, which feature a frequentist-like calibration property that ensures reliability of said inferences. The catch is that IMs' degrees of belief are possibilistic rather than probabilistic and, since the familiar Monte Carlo methods approximate probabilistic quantities, there are computational challenges associated with putting the IM framework into practice. The present paper addresses this shortcoming by developing a new Monte Carlo-based tool designed specifically to approximate the IM's possibilistic output. The proposal is based on a characterization of the possibilistic IM's credal set, which identifies the "best probabilistic approximation" of the IM as a mixture distribution that can be readily approximated and sampled from; these samples can then be transformed into a possibilistic approximation of the IM. Numerical results are presented highlighting the proposed approximation's accuracy and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10585v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>A Multi-fidelity Estimator of the Expected Information Gain for Bayesian Optimal Experimental Design</title>
      <link>https://arxiv.org/abs/2501.10845</link>
      <description>arXiv:2501.10845v1 Announce Type: new 
Abstract: Optimal experimental design (OED) is a framework that leverages a mathematical model of the experiment to identify optimal conditions for conducting the experiment. Under a Bayesian approach, the design objective function is typically chosen to be the expected information gain (EIG). However, EIG is intractable for nonlinear models and must be estimated numerically. Estimating the EIG generally entails some variant of Monte Carlo sampling, requiring repeated data model and likelihood evaluations $\unicode{x2013}$ each involving solving the governing equations of the experimental physics $\unicode{x2013}$ under different sample realizations. This computation becomes impractical for high-fidelity models.
  We introduce a novel multi-fidelity EIG (MF-EIG) estimator under the approximate control variate (ACV) framework. This estimator is unbiased with respect to the high-fidelity mean, and minimizes variance under a given computational budget. We achieve this by first reparameterizing the EIG so that its expectations are independent of the data models, a requirement for compatibility with ACV. We then provide specific examples under different data model forms, as well as practical enhancements of sample size optimization and sample reuse techniques. We demonstrate the MF-EIG estimator in two numerical examples: a nonlinear benchmark and a turbulent flow problem involving the calibration of shear-stress transport turbulence closure model parameters within the Reynolds-averaged Navier-Stokes model. We validate the estimator's unbiasedness and observe one- to two-orders-of-magnitude variance reduction compared to existing single-fidelity EIG estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10845v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas E. Coons, Xun Huan</dc:creator>
    </item>
    <item>
      <title>An accuracy-runtime trade-off comparison of scalable Gaussian process approximations for spatial data</title>
      <link>https://arxiv.org/abs/2501.11448</link>
      <description>arXiv:2501.11448v1 Announce Type: new 
Abstract: Gaussian processes (GPs) are flexible, probabilistic, non-parametric models widely employed in various fields such as spatial statistics, time series analysis, and machine learning. A drawback of Gaussian processes is their computational cost having $\mathcal{O}(N^3)$ time and $\mathcal{O}(N^2)$ memory complexity which makes them prohibitive for large datasets. Numerous approximation techniques have been proposed to address this limitation. In this work, we systematically compare the accuracy of different Gaussian process approximations concerning marginal likelihood evaluation, parameter estimation, and prediction taking into account the time required to achieve a certain accuracy. We analyze this trade-off between accuracy and runtime on multiple simulated and large-scale real-world datasets and find that Vecchia approximations consistently emerge as the most accurate in almost all experiments. However, for certain real-world data sets, low-rank inducing point-based methods, i.e., full-scale and modified predictive process approximations, can provide more accurate predictive distributions for extrapolation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11448v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Filippo Rambelli, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>A revisit to maximum likelihood estimation of Weibull model parameters</title>
      <link>https://arxiv.org/abs/2501.11604</link>
      <description>arXiv:2501.11604v1 Announce Type: new 
Abstract: In this work, we revisit the estimation of the model parameters of a Weibull distribution based on iid observations, using the maximum likelihood estimation (MLE) method which does not yield closed expressions of the estimators. Among other results, it has been shown analytically that the MLEs obtained by solving the highly non-linear equations do exist (i.e., finite), and are unique. We then proceed to study the sampling distributions of the MLEs through both theoretical as well as computational means. It has been shown that the sampling distributions of the two model parameters' MLEs can be approximated fairly well by suitable Weibull distributions too. Results of our comprehensive simulation study corroborate some recent results on the first-order bias and first-order mean squared error (MSE) expressions of the MLEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11604v1</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Buu-Chau Truong (Faculty of Mathematics and Statistics, Ton Duc Thang University, Ho Chi Minh City, Vietnam), Peter Mphekgwana (Department of Research Administration and Development, University of Limpopo, South Africa), Nabendu Pal (Faculty of Mathematics and Statistics, Ton Duc Thang University, Ho Chi Minh City, Vietnam, Department of Mathematics, University of Louisiana at Lafayette, USA)</dc:creator>
    </item>
    <item>
      <title>Median of Means Sampling for the Keister Function</title>
      <link>https://arxiv.org/abs/2501.10440</link>
      <description>arXiv:2501.10440v1 Announce Type: cross 
Abstract: This study investigates the performance of median-of-means sampling compared to traditional mean-of-means sampling for computing the Keister function integral using Randomized Quasi-Monte Carlo (RQMC) methods. The research tests both lattice points and digital nets as point distributions across dimensions 2, 3, 5, and 8, with sample sizes ranging from 2^8 to 2^19 points. Results demonstrate that median-of-means sampling consistently outperforms mean-of-means for sample sizes larger than 10^3 points, while mean-of-means shows better accuracy with smaller sample sizes, particularly for digital nets. The study also confirms previous theoretical predictions about median-of-means' superior performance with larger sample sizes and reflects the known challenges of maintaining accuracy in higher-dimensional integration. These findings support recent research suggesting median-of-means as a promising alternative to traditional sampling methods in numerical integration, though limitations in sample size and dimensionality warrant further investigation with different test functions and larger parameter spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10440v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bocheng Zhang</dc:creator>
    </item>
    <item>
      <title>Simulation of Random LR Fuzzy Intervals</title>
      <link>https://arxiv.org/abs/2501.10482</link>
      <description>arXiv:2501.10482v1 Announce Type: cross 
Abstract: Random fuzzy variables join the modeling of the impreciseness (due to their ``fuzzy part'') and randomness. Statistical samples of such objects are widely used, and their direct, numerically effective generation is therefore necessary. Usually, these samples consist of triangular or trapezoidal fuzzy numbers. In this paper, we describe theoretical results and simulation algorithms for another family of fuzzy numbers -- LR fuzzy numbers with interval-valued cores. Starting from a simulation perspective on the piecewise linear LR fuzzy numbers with the interval-valued cores, their limiting behavior is then considered. This leads us to the numerically efficient algorithm for simulating a sample consisting of such fuzzy values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10482v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maciej Romaniuk, Abbas Parchami, Przemys{\l}aw Grzegorzewski</dc:creator>
    </item>
    <item>
      <title>Softplus and Neural Architectures for Enhanced Negative Binomial INGARCH Modeling</title>
      <link>https://arxiv.org/abs/2501.10655</link>
      <description>arXiv:2501.10655v1 Announce Type: cross 
Abstract: The study addresses a significant gap in the literature by introducing the Softplus negative binomial Integer-valued Generalized Autoregressive Conditional Heteroskedasticity (sp NB- INGARCH) model and establishing its stationarity properties, alongside methodology for parameter estimation. Building upon this foundation, the Neural negative binomial INGARCH (neu - NB-INGARCH) model is proposed, designed to enhance predictive accuracy while accommodating moderate non-stationarity in count time series data. A simulation study and data analysis demonstrate the efficacy of the sp NB-INGARCH model, while the practical utility of the neu - NB - INGARCH model is showcased through a comprehensive analysis of a healthcare data. Additionally, a thorough literature review is presented, focusing on the application of neural networks in time series modeling, with particular emphasis on count time series. In short, this work contributes to advancing the theoretical understanding and practical application of neural network-based models in count time series forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10655v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Divya Kuttenchalil Andrews, N. Balakrishna</dc:creator>
    </item>
    <item>
      <title>An Online Algorithm for Bayesian Variable Selection in Logistic Regression Models With Streaming Data</title>
      <link>https://arxiv.org/abs/2501.10930</link>
      <description>arXiv:2501.10930v1 Announce Type: cross 
Abstract: In several modern applications, data are generated continuously over time, such as data generated from smartwatches. We assume data are collected and analyzed sequentially, in batches. Since traditional or offline methods can be extremely slow, Ghosh et al. (2025) proposed an online method for Bayesian model averaging (BMA). Inspired by the literature on renewable estimation, they developed an online Bayesian method for generalized linear models (GLMs) that reduces storage and computational demands dramatically compared to traditional methods for BMA. The method of Ghosh et al. (2025) works very well when the number of models is small. It can also work reasonably well in moderately large model spaces. For the latter case, the method relies on a screening stage to identify important models in the first several batches via offline methods. Thereafter, the model space remains fixed in all subsequent batches. In the post-screening stage, online updates are made to the model specific parameters, for models selected in the screening stage. For high-dimensional model spaces, the chance of missing important models in the screening stage is more likely. This necessitates the development of a method, which permits the model space to be updated as new batches of data arrive. In this article, we develop an online Bayesian model selection method for logistic regression, where the selected model can potentially change throughout the data collection process. We use simulation studies to show that our new method can outperform the method of Ghosh et al. (2025). Furthermore, we describe scenarios under which the gain from our new method is expected to be small. We revisit the traffic crash data analyzed by Ghosh et al. (2025) and illustrate that our new model selection method can have better performance for variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10930v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Payel Ghosal, Shamriddha De, Joyee Ghosh</dc:creator>
    </item>
    <item>
      <title>The Dynamical Behavior of Detected vs. Undetected Targets</title>
      <link>https://arxiv.org/abs/2501.11189</link>
      <description>arXiv:2501.11189v1 Announce Type: cross 
Abstract: This paper is a sequel of the 2019 paper [5]. It demonstrates the following: a) the Poisson multi-Bernoulli mixture (PMBM) approach to detected vs. undetected (U/D) targets cannot be rigorously formulated using either the two-step or single-step multitarget recursive Bayes filter (MRBF); b) it can, however, be partially salvaged using a novel single-step MRBF; c) probability hypothesis density (PHD) filters can be derived for both the original "S-U/D" approach in [5] and the novel "D-U/D" approach; d) important U/D formulas in [5] can be verified using purely algebraic methods rather than the intricate statistical analysis employed in that paper; and e) the claim, that PMBM filters can propagate detected and undetected targets separately in parallel, is doubtful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11189v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Mahler</dc:creator>
    </item>
    <item>
      <title>Counting the number of group orbits by marrying the Burnside process with importance sampling</title>
      <link>https://arxiv.org/abs/2501.11731</link>
      <description>arXiv:2501.11731v1 Announce Type: cross 
Abstract: This paper introduces a novel and general algorithm for approximately counting the number of orbits under group actions. The method is based on combining the Burnside process and importance sampling. Specializing to unitriangular groups yields an efficient algorithm for estimating the number of conjugacy classes of such groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11731v1</guid>
      <category>math.PR</category>
      <category>math.CO</category>
      <category>math.GR</category>
      <category>stat.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Persi Diaconis, Chenyang Zhong</dc:creator>
    </item>
    <item>
      <title>Non-Reversible Langevin Algorithms for Constrained Sampling</title>
      <link>https://arxiv.org/abs/2501.11743</link>
      <description>arXiv:2501.11743v1 Announce Type: cross 
Abstract: We consider the constrained sampling problem where the goal is to sample from a target distribution on a constrained domain. We propose skew-reflected non-reversible Langevin dynamics (SRNLD), a continuous-time stochastic differential equation with skew-reflected boundary. We obtain non-asymptotic convergence rate of SRNLD to the target distribution in both total variation and 1-Wasserstein distances. By breaking reversibility, we show that the convergence is faster than the special case of the reversible dynamics. Based on the discretization of SRNLD, we propose skew-reflected non-reversible Langevin Monte Carlo (SRNLMC), and obtain non-asymptotic discretization error from SRNLD, and convergence guarantees to the target distribution in 1-Wasserstein distance. We show better performance guarantees than the projected Langevin Monte Carlo in the literature that is based on the reversible dynamics. Numerical experiments are provided for both synthetic and real datasets to show efficiency of the proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11743v1</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengrong Du, Qi Feng, Changwei Tu, Xiaoyu Wang, Lingjiong Zhu</dc:creator>
    </item>
    <item>
      <title>Bayesian variable selection in sample selection models using spike-and-slab priors</title>
      <link>https://arxiv.org/abs/2312.03538</link>
      <description>arXiv:2312.03538v3 Announce Type: replace 
Abstract: Sample selection models represent a common methodology for correcting bias induced by data missing not at random. These models are not empirically identifiable without exclusion restrictions. In other words, some variables predictive of missingness do not affect the outcome model of interest. The drive to establish this requirement often leads to the inclusion of irrelevant variables in the model. A recent proposal uses adaptive LASSO to circumvent this problem, but its performance depends on the so-called covariance assumption, which can be violated in small to moderate samples. Additionally, there are no tools yet for post-selection inference for this model. To address these challenges, we propose two families of spike-and-slab priors to conduct Bayesian variable selection in sample selection models. These prior structures allow for constructing a Gibbs sampler with tractable conditionals, which is scalable to the dimensions of practical interest. We illustrate the performance of the proposed methodology through a simulation study and present a comparison against adaptive LASSO and stepwise selection. We also provide two applications using publicly available real data. An implementation and code to reproduce the results in this paper can be found at https://github.com/adam-iqbal/selection-spike-slab.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03538v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Iqbal, Emmanuel O. Ogundimu, F. Javier Rubio</dc:creator>
    </item>
    <item>
      <title>Fast and light-weight energy statistics using the \textit{R} package \textsf{Rfast}</title>
      <link>https://arxiv.org/abs/2501.02849</link>
      <description>arXiv:2501.02849v3 Announce Type: replace 
Abstract: Energy statistics ($\mathcal{\varepsilon}$-statistics) are functions of distances between statistical observations. This class of functions has enabled the development of non-linear statistical concepts, termed distance variance, distance covariance, distance correlation, etc. The computational burden associated with the $\mathcal{\varepsilon}$-statistical quantities is really heavy and when the data reside in the multivariate space, the task becomes even harder. We alleviate this cost by tremendously reducing the memory requirements and essentially making the computations faster. We show the process for the cases of (univariate and multivariate) distance variance, distance covariance, (partial) distance correlation, energy distance and hypothesis testing for the equality of univariate distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02849v3</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris, Manos Papadakis</dc:creator>
    </item>
    <item>
      <title>Useful Compact Representations for Data-Fitting</title>
      <link>https://arxiv.org/abs/2403.12206</link>
      <description>arXiv:2403.12206v2 Announce Type: replace-cross 
Abstract: For minimization problems without 2nd derivative information, methods that estimate Hessian matrices can be very effective. However, conventional techniques generate dense matrices that are prohibitive for large problems. Limited-memory compact representations express the dense arrays in terms of a low rank representation and have become the state-of-the-art for software implementations on large deterministic problems. We develop new compact representations that are parameterized by a choice of vectors and that reduce to existing well known formulas for special choices. We demonstrate effectiveness of the compact representations for large eigenvalue computations, tensor factorizations and nonlinear regressions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12206v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes J. Brust</dc:creator>
    </item>
  </channel>
</rss>
