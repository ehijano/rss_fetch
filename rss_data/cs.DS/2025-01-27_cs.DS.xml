<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 03:45:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On Extended Concentration Inequalities for Fast JL Embeddings of Infinite Sets</title>
      <link>https://arxiv.org/abs/2501.14010</link>
      <description>arXiv:2501.14010v1 Announce Type: new 
Abstract: The Johnson-Lindenstrauss (JL) lemma allows subsets of a high-dimensional space to be embedded into a lower-dimensional space while approximately preserving all pairwise Euclidean distances. This important result has inspired an extensive literature, with a significant portion dedicated to constructing structured random matrices with fast matrix-vector multiplication algorithms that generate such embeddings for finite point sets. In this paper, we briefly consider fast JL embedding matrices for {\it infinite} subsets of $\mathbb{R}^d$. Prior work in this direction such as \cite{oymak2018isometric, mendelson2023column} has focused on constructing fast JL matrices $HD \in \mathbb{R}^{k \times d}$ by multiplying structured matrices with RIP(-like) properties $H \in \mathbb{R}^{k \times d}$ against a random diagonal matrix $D \in \mathbb{R}^{d \times d}$. However, utilizing RIP(-like) matrices $H$ in this fashion necessarily has the unfortunate side effect that the resulting embedding dimension $k$ must depend on the ambient dimension $d$ no matter how simple the infinite set is that one aims to embed. Motivated by this, we explore an alternate strategy for removing this $d$-dependence from $k$ herein: Extending a concentration inequality proven by Ailon and Liberty \cite{Ailon2008fast} in the hope of later utilizing it in a chaining argument to obtain a near-optimal result for infinite sets. %, and $(ii)$ utilizing a simple secondary Gaussian embedding of an initial fast JL embedding of a given infinite set.
  Though this strategy ultimately fails to provide the near-optimal embedding dimension we seek, along the way we obtain a stronger-than-sub-exponential extension of the concentration inequality in \cite{Ailon2008fast} which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14010v1</guid>
      <category>cs.DS</category>
      <category>math.PR</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edem Boahen, March T. Boedihardjo, Rafael Chiclana, Mark Iwen</dc:creator>
    </item>
    <item>
      <title>RadiK: Scalable and Optimized GPU-Parallel Radix Top-K Selection</title>
      <link>https://arxiv.org/abs/2501.14336</link>
      <description>arXiv:2501.14336v1 Announce Type: new 
Abstract: Top-k selection, which identifies the largest or smallest k elements from a data set, is a fundamental operation in data-intensive domains such as databases and deep learning, so its scalability and efficiency are critical for these high-performance systems. However, previous studies on its efficient GPU implementation are mostly merge-based and rely heavily on the fast but size-limited on-chip memory, thereby limiting the scalability with a restricted upper bound on k. This work introduces a scalable and optimized GPU-parallel radix top-k selection that supports significantly larger k values than existing methods without compromising efficiency, regardless of input length and batch size. Our method incorporates a novel optimization framework tailored for high memory bandwidth and resource utilization, achieving up to 2.5x speedup over the prior art for non-batch queries and up to 4.8x speedup for batch queries. In addition, we propose an adaptive scaling technique that strengthens the robustness, which further provides up to 2.7x speedup on highly adversarial input distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14336v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3650200.3656596</arxiv:DOI>
      <arxiv:journal_reference>ICS '24: Proceedings of the 38th ACM International Conference on Supercomputing, 2024, Pages 537-548</arxiv:journal_reference>
      <dc:creator>Yifei Li, Bole Zhou, Jiejing Zhang, Xuechao Wei, Yinghan Li, Yingda Chen</dc:creator>
    </item>
    <item>
      <title>Changing Induced Subgraph Isomorphisms Under Extended Reconfiguration Rules</title>
      <link>https://arxiv.org/abs/2501.14450</link>
      <description>arXiv:2501.14450v1 Announce Type: new 
Abstract: In a reconfiguration problem, we are given two feasible solutions of a combinatorial problem and our goal is to determine whether it is possible to reconfigure one into the other, with the steps dictated by specific reconfiguration rules. Traditionally, most studies on reconfiguration problems have focused on rules that allow changing a single element at a time. In contrast, this paper considers scenarios in which $k \ge 2$ elements can be changed simultaneously. We investigate the general reconfiguration problem of isomorphisms. For the Induced Subgraph Isomorphism Reconfiguration problem, we show that the problem remains $\textsf{PSPACE}$-complete even under stringent constraints on the pattern graph when $k$ is constant. We then give two meta-theorems applicable when $k$ is slightly less than the number of vertices in the pattern graph. In addition, we investigate the complexity of the Independent Set Reconfiguration problem, which is a special case of the Induced Subgraph Isomorphism Reconfiguration problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14450v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuhiro Suga, Akira Suzuki, Yuma Tamura, Xiao Zhou</dc:creator>
    </item>
    <item>
      <title>Efficient parameterized approximation</title>
      <link>https://arxiv.org/abs/2501.14461</link>
      <description>arXiv:2501.14461v1 Announce Type: new 
Abstract: Many problems are NP-hard and, unless P = NP, do not admit polynomial-time exact algorithms. The fastest known exact algorithms exactly usually take time exponential in the input size. Much research effort has gone into obtaining faster exact algorithms for instances that are sufficiently well-structured, e.g., through parameterized algorithms with running time $f(k)\cdot n^{\mathcal{O}(1)}$ where n is the input size and k quantifies some structural property such as treewidth. When k is small, this is comparable to a polynomial-time exact algorithm and outperforms the fastest exact exponential-time algorithms for a large range of k.
  In this work, we are interested instead in leveraging instance structure for polynomial-time approximation algorithms. We aim for polynomial-time algorithms that produce a solution of value at most or at least (depending on minimization vs. maximization) $c\mathrm{OPT}\pm f(k)$ where c is a constant. Unlike for standard parameterized algorithms, we do not assume that structural information is provided with the input. Ideally, we can obtain algorithms with small additive error, i.e., $c=1$ and $f(k)$ is polynomial or even linear in $k$. For small k, this is similarly comparable to a polynomial-time exact algorithm and will beat general case approximation for a large range of k.
  We study Vertex Cover, Connected Vertex Cover, Chromatic Number, and Triangle Packing. The parameters we consider are the size of minimum modulators to graph classes on which the respective problem is tractable. For most problem-parameter combinations we give algorithms that compute a solution of size at least or at most $\mathrm{OPT}\pm k$. In the case of Vertex Cover, most of our algorithms are tight under the Unique Games Conjecture and provide better approximation guarantees than standard 2-approximations if the modulator is smaller than the optimum solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14461v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Kratsch, Pascal Kunz</dc:creator>
    </item>
    <item>
      <title>Forbidden Subgraph Problems with Predictions</title>
      <link>https://arxiv.org/abs/2501.14537</link>
      <description>arXiv:2501.14537v1 Announce Type: new 
Abstract: In the Online Delayed Connected H-Node-Deletion Problem, an unweighted graph is revealed vertex by vertex and it must remain free of any induced copies of a specific connected induced forbidden subgraph H at each point in time. To achieve this, an algorithm must, upon each occurrence of H, identify and irrevocably delete one or more vertices. The objective is to delete as few vertices as possible. We provide tight bounds on the competitive ratio for forbidden subgraphs H that do not contain two true twins or that do not contain two false twins.
  We further consider the problem within the model of predictions, where the algorithm is provided with a single bit of advice for each revealed vertex. These predictions are considered to be provided by an untrusted source and may be incorrect. We present a family of algorithms solving the Online Delayed Connected H-Node-Deletion Problem with predictions and show that it is Pareto-optimal with respect to competitivity and robustness for the online vertex cover problem for 2-connected forbidden subgraphs that do not contain two true twins or that do not contain two false twins, as well as for forbidden paths of length greater than four. We also propose subgraphs for which a better algorithm might exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14537v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hans-Joachim B\"ockenhauer, Melvin Jahn, Dennis Komm, Moritz Stocker</dc:creator>
    </item>
    <item>
      <title>Tree independence number V. Walls and claws</title>
      <link>https://arxiv.org/abs/2501.14658</link>
      <description>arXiv:2501.14658v1 Announce Type: cross 
Abstract: Given a family $\mathcal{H}$ of graphs, we say that a graph $G$ is $\mathcal{H}$-free if no induced subgraph of $G$ is isomorphic to a member of $\mathcal{H}$. Let $S_{t,t,t}$ be the graph obtained from $K_{1,3}$ by subdividing each edge $t-1$ times, and let $W_{t\times t}$ be the $t$-by-$t$ hexagonal grid. Let $\mathcal{L}_t$ be the family of all graphs $G$ such that $G$ is the line graph of some subdivision of $W_{t \times t}$. We prove that for every positive integer $t$ there exists $c(t)$ such that every $\mathcal{L}_t \cup \{S_{t,t,t}, K_{t,t}\}$-free $n$-vertex graph admits a tree decomposition in which the maximum size of an independent set in each bag is at most $c(t)\log^4n$. This is a variant of a conjecture of Dallard, Krnc, Kwon, Milani\v{c}, Munaro, \v{S}torgel, and Wiederrecht from 2024. This implies that the Maximum Weight Independent Set problem, as well as many other natural algorithmic problems, that are known to be NP-hard in general, can be solved in quasi-polynomial time if the input graph is $\mathcal{L}_t \cup \{S_{t,t,t},K_{t,t}\}$-free. As part of our proof, we show that for every positive integer $t$ there exists an integer $d$ such that every $\mathcal{L}_t \cup \{S_{t,t,t}\}$-free graph admits a balanced separator that is contained in the neighborhood of at most $d$ vertices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14658v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Chudnovsky, Julien Codsi, Daniel Lokshtanov, Martin Milani\v{c}, Varun Sivashankar</dc:creator>
    </item>
    <item>
      <title>SDPs and Robust Satisfiability of Promise CSP</title>
      <link>https://arxiv.org/abs/2211.08373</link>
      <description>arXiv:2211.08373v4 Announce Type: replace 
Abstract: For a constraint satisfaction problem (CSP), a robust satisfaction algorithm is one that outputs an assignment satisfying most of the constraints on instances that are near-satisfiable. It is known that the CSPs that admit efficient robust satisfaction algorithms are precisely those of bounded width, i.e., CSPs whose satisfiability can be checked by a simple local consistency algorithm (eg., 2-SAT or Horn-SAT in the Boolean case). While the exact satisfiability of a bounded width CSP can be checked by combinatorial algorithms, the robust algorithm is based on rounding a canonical Semidefinite Programming (SDP) relaxation.
  In this work, we initiate the study of robust satisfaction algorithms for promise CSPs, which are a vast generalization of CSPs that have received much attention recently. The motivation is to extend the theory beyond CSPs, as well as to better understand the power of SDPs. We present robust SDP rounding algorithms under some general conditions, namely the existence of particular high-dimensional Boolean symmetries known as majority or alternating threshold polymorphisms. On the hardness front, we prove that the lack of such polymorphisms makes the PCSP hard for all pairs of symmetric Boolean predicates. Our approach relies on SDP integrality gaps argued via the absence of certain colorings of the sphere, with connections to sphere Ramsey theory.
  We conjecture that PCSPs with robust satisfaction algorithms are precisely those for which the feasibility of the canonical SDP implies (exact) satisfiability. We also give a precise algebraic condition, known as a minion characterization, of which PCSPs have the latter property.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.08373v4</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.LO</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Brakensiek, Venkatesan Guruswami, Sai Sandeep</dc:creator>
    </item>
    <item>
      <title>A note on the complexity of the picker routing problem in multi-block warehouses and related problems</title>
      <link>https://arxiv.org/abs/2312.01857</link>
      <description>arXiv:2312.01857v2 Announce Type: replace 
Abstract: The Picker Routing Problem (PRP), which consists of finding a minimum-length tour between a set of storage locations in a warehouse, is one of the most important problems in the warehousing logistics literature. Despite its popularity, the tractability of the PRP in multi-block warehouses remains an open question. This technical note aims to fill this research gap by establishing that the problem is strongly NP-hard. As a corollary, the complexity status of other related problems is settled.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01857v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.OC</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10479-025-06481-3</arxiv:DOI>
      <dc:creator>Thibault Prunet, Nabil Absi, Diego Cattaruzza</dc:creator>
    </item>
    <item>
      <title>Color Refinement for Relational Structures</title>
      <link>https://arxiv.org/abs/2407.16022</link>
      <description>arXiv:2407.16022v2 Announce Type: replace 
Abstract: Color Refinement, also known as Naive Vertex Classification, is a classical method to distinguish graphs by iteratively computing a coloring of their vertices. While it is mainly used as an imperfect way to test for isomorphism, the algorithm permeated many other, seemingly unrelated, areas of computer science. The method is algorithmically simple, and it has a well-understood distinguishing power: It is logically characterized by Cai, F\"urer and Immerman (1992), who showed that it distinguishes precisely those graphs that can be distinguished by a sentence of first-order logic with counting quantifiers and only two variables. A combinatorial characterization is given by Dvo\v{r}\'ak (2010), who shows that it distinguishes precisely those graphs that can be distinguished by the number of homomorphisms from some tree.
  In this paper, we introduce Relational Color Refinement (RCR, for short), a generalization of the Color Refinement method from graphs to arbitrary relational structures, whose distinguishing power admits the equivalent combinatorial and logical characterizations as Color Refinement has on graphs: We show that RCR distinguishes precisely those structures that can be distinguished by the number of homomorphisms from an acyclic relational structure. Further, we show that RCR distinguishes precisely those structures that can be distinguished by a sentence of the guarded fragment of first-order logic with counting quantifiers.
  Additionally, we show that for every fixed finite relational signature, RCR can be implemented to run on structures of that signature in time $O(N\cdot \log N)$, where $N$ denotes the number of tuples present in the structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16022v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.LO</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Scheidt, Nicole Schweikardt</dc:creator>
    </item>
    <item>
      <title>Certificates in P and Subquadratic-Time Computation of Radius, Diameter, and all Eccentricities in Graphs</title>
      <link>https://arxiv.org/abs/1803.04660</link>
      <description>arXiv:1803.04660v4 Announce Type: replace-cross 
Abstract: In the context of fine-grained complexity, we investigate the notion of certificate enabling faster polynomial-time algorithms. We specifically target radius (minimum eccentricity), diameter (maximum eccentricity), and all-eccentricity computations for which quadratic-time lower bounds are known under plausible conjectures. In each case, we introduce a notion of certificate as a specific set of nodes from which appropriate bounds on all eccentricities can be derived in subquadratic time when this set has sublinear size. The existence of small certificates is a barrier against SETH-based lower bounds for these problems. We indeed prove that for graph classes with small certificates, there exist randomized subquadratic-time algorithms for computing the radius, the diameter, and all eccentricities respectively.Moreover, these notions of certificates are tightly related to algorithms probing the graph through one-to-all distance queries and allow to explain the efficiency of practical radius and diameter algorithms from the literature. Our formalization enables a novel primal-dual analysis of a classical approach for diameter computation that leads to algorithms for radius, diameter and all eccentricities with theoretical guarantees with respect to certain graph parameters. This is complemented by experimental results on various types of real-world graphs showing that these parameters appear to be low in practice. Finally, we obtain refined results for several graph classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:1803.04660v4</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/1.9781611978322.70</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2025 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), Jan 2025, New Orleans (LA), United States. pp.2157-2193</arxiv:journal_reference>
      <dc:creator>Feodor F. Dragan (UniBuc, ICI), Guillaume Ducoffe (UniBuc, ICI), Michel Habib (IRIF), Laurent Viennot (DI-ENS, ARGO)</dc:creator>
    </item>
    <item>
      <title>Honeybee: Byzantine Tolerant Decentralized Peer Sampling with Verifiable Random Walks</title>
      <link>https://arxiv.org/abs/2402.16201</link>
      <description>arXiv:2402.16201v3 Announce Type: replace-cross 
Abstract: Popular blockchains today have hundreds of thousands of nodes and need to be able to support sophisticated scaling solutions$\unicode{x2013}$such as sharding, data availability sampling, and layer-2 methods. Designing secure and efficient peer-to-peer (p2p) networking protocols at these scales to support the tight demands of the upper layer crypto-economic primitives is a highly non-trivial endeavor. We identify decentralized, uniform random sampling of nodes as a fundamental capability necessary for building robust p2p networks in emerging blockchain networks. Sampling algorithms used in practice today (primarily for address discovery) rely on either distributed hash tables (e.g., Kademlia) or sharing addresses with neighbors (e.g., GossipSub), and are not secure in a Sybil setting. We present Honeybee, a decentralized algorithm for sampling nodes that uses verifiable random walks and peer consistency checks. Honeybee is secure against attacks even in the presence of an overwhelming number of Byzantine nodes (e.g., $\geq50\%$ of the network). We evaluate Honeybee through experiments and show that the quality of sampling achieved by Honeybee is significantly better compared to the state-of-the-art. Our proposed algorithm has implications for network design in both full nodes and light nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16201v3</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.MA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunqi Zhang, Shaileshh Bojja Venkatakrishnan</dc:creator>
    </item>
    <item>
      <title>Designing Exploration Contracts</title>
      <link>https://arxiv.org/abs/2403.02317</link>
      <description>arXiv:2403.02317v3 Announce Type: replace-cross 
Abstract: We study a natural application of contract design in the context of sequential exploration problems. In our principal-agent setting, a search task is delegated to an agent. The agent performs a sequential exploration of $n$ boxes, suffers the exploration cost for each inspected box, and selects the content (called the prize) of one inspected box as outcome. Agent and principal obtain an individual value based on the selected prize. To influence the search, the principal a-priori designs a contract with a non-negative payment to the agent for each potential prize. The goal of the principal is to maximize her expected reward, i.e., value minus payment. Interestingly, this natural contract scenario shares close relations with the Pandora's Box problem.
  We show how to compute optimal contracts for the principal in several scenarios. A popular and important subclass is that of linear contracts, and we show how to compute optimal linear contracts in polynomial time. For general contracts, we obtain optimal contracts under the standard assumption that the agent suffers cost but obtains value only from the transfers by the principal. More generally, for general contracts with non-zero agent values for outcomes we show how to compute an optimal contract in two cases: (1) when each box has only one prize with non-zero value for principal and agent, (2) for i.i.d. boxes with a single prize with positive value for the principal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02317v3</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Hoefer, Conrad Schecker, Kevin Schewior</dc:creator>
    </item>
  </channel>
</rss>
