<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Assessing Human Intelligence Augmentation Strategies Using Brain Machine Interfaces and Brain Organoids in the Era of AI Advancement</title>
      <link>https://arxiv.org/abs/2503.15508</link>
      <description>arXiv:2503.15508v1 Announce Type: cross 
Abstract: The rapid advancement of Artificial Intelligence (AI) technologies, including the potential emergence of Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), has raised concerns about AI surpassing human cognitive capabilities. To address this challenge, intelligence augmentation approaches, such as Brain Machine Interfaces (BMI) and Brain Organoid (BO) integration have been proposed. In this study, we compare three intelligence augmentation strategies, namely BMI, BO, and a hybrid approach combining both. These strategies are evaluated from three key perspectives that influence user decisions in selecting an augmentation method: information processing capacity, identity risk, and consent authenticity risk. First, we model these strategies and assess them across the three perspectives. The results reveal that while BO poses identity risks and BMI has limitations in consent authenticity capacity, the hybrid approach mitigates these weaknesses by striking a balance between the two. Second, we investigate how users might choose among these intelligence augmentation strategies in the context of evolving AI capabilities over time. As the result, we find that BMI augmentation alone is insufficient to compete with advanced AI, and while BO augmentation offers scalability, BO increases identity risks as the scale grows. Moreover, the hybrid approach provides a balanced solution by adapting to AI advancements. This study provides a novel framework for human capability augmentation in the era of advancing AI and serves as a guideline for adapting to AI development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15508v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenta Kitamura</dc:creator>
    </item>
    <item>
      <title>Superhuman AI Disclosure: Impacts on Toxicity, Fairness, and Trust Vary by Expertise and Persona Attributes</title>
      <link>https://arxiv.org/abs/2503.15514</link>
      <description>arXiv:2503.15514v1 Announce Type: cross 
Abstract: As artificial intelligence demonstrates surpassing human performance across real-world tasks, disclosing superhuman capabilities poses challenges for fairness, accountability, and trust. To investigate how transparency impacts attitudes and perceptions, we introduce a grounded and validated set of synthetic personas reflecting diverse fairness concerns and technology acceptance levels. Then we evaluate responses in two contrasting domains: (1) a competitive player in StarCraft II, where strategy and high-skill gameplay often elicit toxic interactions, and (2) a cooperative personal-assistant in providing information. Across numerous interactions spanning persona profiles, we test non-disclosure versus explicit superhuman labelling under controlled game outcomes and usage contexts. Our findings reveal sharp domain-specific effects: in StarCraft II, explicitly labelling AI as superhuman, novice personas who learned of it reported lower toxicity and higher fairness-attributing defeat to advanced skill rather than hidden cheating-whereas expert personas found the disclosure statements irksome but still less deceptive than non-disclosure. Conversely, in the LLM as personal-assistant setting, disclosure of superhuman capabilities improved perceived trustworthiness, though it risked AI overreliance among certain persona segments. We release Dataset X-containing persona cards-including profile attributes, disclosure prompts, and detailed interaction logs, accompanied by reproducible protocols and disclaimers for adapting them to diverse tasks. Our results demonstrate that transparency is not a cure-all: while it reduces suspicion and enhances trust in cooperative contexts, it may inflame resistance or disappointment in competitive domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15514v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaymari Chua, Chen Wang, Lina Yao</dc:creator>
    </item>
    <item>
      <title>KHAIT: K-9 Handler Artificial Intelligence Teaming for Collaborative Sensemaking</title>
      <link>https://arxiv.org/abs/2503.15524</link>
      <description>arXiv:2503.15524v1 Announce Type: cross 
Abstract: In urban search and rescue (USAR) operations, communication between handlers and specially trained canines is crucial but often complicated by challenging environments and the specific behaviors canines are trained to exhibit when detecting a person. Since a USAR canine often works out of sight of the handler, the handler lacks awareness of the canine's location and situation, known as the 'sensemaking gap.' In this paper, we propose KHAIT, a novel approach to close the sensemaking gap and enhance USAR effectiveness by integrating object detection-based Artificial Intelligence (AI) and Augmented Reality (AR). Equipped with AI-powered cameras, edge computing, and AR headsets, KHAIT enables precise and rapid object detection from a canine's perspective, improving survivor localization. We evaluate this approach in a real-world USAR environment, demonstrating an average survival allocation time decrease of 22%, enhancing the speed and accuracy of operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15524v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <category>cs.MA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3708359.3712107</arxiv:DOI>
      <dc:creator>Matthew Wilchek, Linhan Wang, Sally Dickinson, Erica Feuerbacher, Kurt Luther, Feras A. Batarseh</dc:creator>
    </item>
    <item>
      <title>Identifying Likely-Reputable Blockchain Projects on Ethereum</title>
      <link>https://arxiv.org/abs/2503.15542</link>
      <description>arXiv:2503.15542v1 Announce Type: cross 
Abstract: Identifying reputable Ethereum projects remains a critical challenge within the expanding blockchain ecosystem. The ability to distinguish between legitimate initiatives and potentially fraudulent schemes is non-trivial. This work presents a systematic approach that integrates multiple data sources with advanced analytics to evaluate credibility, transparency, and overall trustworthiness. The methodology applies machine learning techniques to analyse transaction histories on the Ethereum blockchain.
  The study classifies accounts based on a dataset comprising 2,179 entities linked to illicit activities and 3,977 associated with reputable projects. Using the LightGBM algorithm, the approach achieves an average accuracy of 0.984 and an average AUC of 0.999, validated through 10-fold cross-validation. Key influential factors include time differences between transactions and received_tnx.
  The proposed methodology provides a robust mechanism for identifying reputable Ethereum projects, fostering a more secure and transparent investment environment. By equipping stakeholders with data-driven insights, this research enables more informed decision-making, risk mitigation, and the promotion of legitimate blockchain initiatives. Furthermore, it lays the foundation for future advancements in trust assessment methodologies, contributing to the continued development and maturity of the Ethereum ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15542v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cyrus Malik, Josef Bajada, Joshua Ellul</dc:creator>
    </item>
    <item>
      <title>LLM-Aided Customizable Profiling of Code Data Based On Programming Language Concepts</title>
      <link>https://arxiv.org/abs/2503.15571</link>
      <description>arXiv:2503.15571v1 Announce Type: cross 
Abstract: Data profiling is critical in machine learning for generating descriptive statistics, supporting both deeper understanding and downstream tasks like data valuation and curation. This work addresses profiling specifically in the context of code datasets for Large Language Models (code-LLMs), where data quality directly influences tasks such as code generation and summarization. Characterizing code datasets in terms of programming language concepts enables better insights and targeted data curation. Our proposed methodology decomposes code data profiling into two phases: (1) an offline phase where LLMs are leveraged to derive and learn rules for extracting syntactic and semantic concepts across various programming languages, including previously unseen or low-resource languages, and (2) an online deterministic phase applying these derived rules for efficient real-time analysis. This hybrid approach is customizable, extensible to new syntactic and semantic constructs, and scalable to multiple languages. Experimentally, our LLM-aided method achieves a mean accuracy of 90.33% for syntactic extraction rules and semantic classification accuracies averaging 80% and 77% across languages and semantic concepts, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15571v1</guid>
      <category>cs.SE</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pankaj Thorat, Adnan Qidwai, Adrija Dhar, Aishwariya Chakraborty, Anand Eswaran, Hima Patel, Praveen Jayachandran</dc:creator>
    </item>
    <item>
      <title>Enhancing Physical Layer Security in Cognitive Radio-Enabled NTNs with Beyond Diagonal RIS</title>
      <link>https://arxiv.org/abs/2503.15787</link>
      <description>arXiv:2503.15787v1 Announce Type: cross 
Abstract: Beyond diagonal reconfigurable intelligent surfaces (BD-RIS) have emerged as a transformative technology for enhancing wireless communication by intelligently manipulating the propagation environment. This paper explores the potential of BD-RIS in improving cognitive radio enabled multilayer non-terrestrial networks (NTNs). It is assumed that a high-altitude platform station (HAPS) has set up the primary network, while an uncrewed aerial vehicle (UAV) establishes the secondary network in the HAPS footprint. We formulate a joint optimization problem to maximize the secrecy rate by optimizing BD-RIS phase shifts and the secondary transmitter power allocation while controlling the interference temperature from the secondary network to the primary network. To solve this problem efficiently, we decouple the original problem into two sub-problems, which are solved iteratively by relying on alternating optimization. Simulation results demonstrate the effectiveness of BD-RIS in cognitive radio-enabled multilayer NTNs to accommodate the secondary network while satisfying the constraints imposed from the primary network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15787v1</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wali Ullah Khan, Chandan Kumar Sheemar, Eva Lagunas, Symeon Chatzinotas</dc:creator>
    </item>
    <item>
      <title>Reducing T Gates with Unitary Synthesis</title>
      <link>https://arxiv.org/abs/2503.15843</link>
      <description>arXiv:2503.15843v1 Announce Type: cross 
Abstract: Quantum error correction is essential for achieving practical quantum computing but has a significant computational overhead. Among fault-tolerant (FT) gate operations, non-Clifford gates, such as $T$, are particularly expensive due to their reliance on magic state distillation. These costly $T$ gates appear frequently in FT circuits as many quantum algorithms require arbitrary single-qubit rotations, such as $R_x$ and $R_z$ gates, which must be decomposed into a sequence of $T$ and Clifford gates. In many quantum circuits, $R_x$ and $R_z$ gates can be fused to form a single $U3$ unitary. However, existing synthesis methods, such as gridsynth, rely on indirect decompositions, requiring separate $R_z$ decompositions that result in a threefold increase in $T$ count.
  This work presents a novel FT synthesis algorithm that directly synthesizes arbitrary single-qubit unitaries, avoiding the overhead of separate $R_z$ decompositions. By leveraging tensor network-based search, our approach enables native $U3$ synthesis, reducing the $T$ count, Clifford gate count, and approximation error. Compared to gridsynth-based circuit synthesis, for 187 representative benchmarks, our design reduces the $T$ count by up to $3.5\times$, and Clifford gates by $7\times$, resulting in up to $4\times$ improvement in overall circuit infidelity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15843v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Hao, Amanda Xu, Swamit Tannu</dc:creator>
    </item>
    <item>
      <title>Neuromorphic Cameras in Astronomy: Unveiling the Future of Celestial Imaging Beyond Conventional Limits</title>
      <link>https://arxiv.org/abs/2503.15883</link>
      <description>arXiv:2503.15883v1 Announce Type: cross 
Abstract: To deepen our understanding of optical astronomy, we must advance imaging technology to overcome conventional frame-based cameras' limited dynamic range and temporal resolution. Our Perspective paper examines how neuromorphic cameras can effectively address these challenges. Drawing inspiration from the human retina, neuromorphic cameras excel in speed and high dynamic range by utilizing asynchronous pixel operation and logarithmic photocurrent conversion, making them highly effective for celestial imaging. We use 1300 mm terrestrial telescope to demonstrate the neuromorphic camera's ability to simultaneously capture faint and bright celestial sources while preventing saturation effects. We illustrate its photometric capabilities through aperture photometry of a star field with faint stars. Detection of the faint gas cloud structure of the Trapezium cluster during a full moon night highlights the camera's high dynamic range, effectively mitigating static glare from lunar illumination. Our investigations also include detecting meteorite passing near the Moon and Earth, as well as imaging satellites and anthropogenic debris with exceptionally high temporal resolution using a 200mm telescope. Our observations show the immense potential of neuromorphic cameras in advancing astronomical optical imaging and pushing the boundaries of observational astronomy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15883v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satyapreet Singh Yadav, Bikram Pradhan, Kenil Rajendrabhai Ajudiya, T. S. Kumar, Nirupam Roy, Andre Van Schaik, Chetan Singh Thakur</dc:creator>
    </item>
    <item>
      <title>DevOps Automation Pipeline Deployment with IaC (Infrastructure as Code)</title>
      <link>https://arxiv.org/abs/2503.16038</link>
      <description>arXiv:2503.16038v1 Announce Type: cross 
Abstract: DevOps pipeline is a set of automated tasks or processes or jobs that has tasks assigned to execute automatically that allow the Development team and Operations team to collaborate for building and deployment of the software or services. DevOps as a culture includes better collaboration between different teams within an organization and the removal of silos between them. This paper aims to streamline the current software development and deployment process that is being followed in most of today's generation DevOps deployment as Continuous Integration and Continuous Delivery (CI/CD) pipelines. Centered to the level of software development life cycle (SDLC), it also describes the current ambiguous definition to clarify the implementation of DevOps in practice along a sample CI/CD pipeline deployment. The further objective of the paper is to demonstrate the implementation strategy of DevOps Infrastructure as Code (IaC) and Pipeline as a code and the removal of ambiguity in the definition of DevOps Infrastructure as a Code methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16038v1</guid>
      <category>cs.SE</category>
      <category>cs.ET</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/SILCON63976.2024.10910699</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE Silchar Subsection Conference (SILCON 2024), Agartala, India, 2024</arxiv:journal_reference>
      <dc:creator>Adarsh Saxena, Sudhakar Singh, Shiv Prakash, Tiansheng Yang, Rajkumar Singh Rathore</dc:creator>
    </item>
    <item>
      <title>Flight Testing an Optionally Piloted Aircraft: a Case Study on Trust Dynamics in Human-Autonomy Teaming</title>
      <link>https://arxiv.org/abs/2503.16227</link>
      <description>arXiv:2503.16227v1 Announce Type: cross 
Abstract: This paper examines how trust is formed, maintained, or diminished over time in the context of human-autonomy teaming with an optionally piloted aircraft. Whereas traditional factor-based trust models offer a static representation of human confidence in technology, here we discuss how variations in the underlying factors lead to variations in trust, trust thresholds, and human behaviours. Over 200 hours of flight test data collected over a multi-year test campaign from 2021 to 2023 were reviewed. The dispositional-situational-learned, process-performance-purpose, and IMPACTS homeostasis trust models are applied to illuminate trust trends during nominal autonomous flight operations. The results offer promising directions for future studies on trust dynamics and design-for-trust in human-autonomy teaming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16227v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy C. -H. Wang, Ming Hou, David Dunwoody, Marko Ilievski, Justin Tomasi, Edward Chao, Carl Pigeon</dc:creator>
    </item>
    <item>
      <title>Empirical Analysis of Privacy-Fairness-Accuracy Trade-offs in Federated Learning: A Step Towards Responsible AI</title>
      <link>https://arxiv.org/abs/2503.16233</link>
      <description>arXiv:2503.16233v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative machine learning while preserving data privacy but struggles to balance privacy preservation (PP) and fairness. Techniques like Differential Privacy (DP), Homomorphic Encryption (HE), and Secure Multi-Party Computation (SMC) protect sensitive data but introduce trade-offs. DP enhances privacy but can disproportionately impact underrepresented groups, while HE and SMC mitigate fairness concerns at the cost of computational overhead. This work explores the privacy-fairness trade-offs in FL under IID (Independent and Identically Distributed) and non-IID data distributions, benchmarking q-FedAvg, q-MAML, and Ditto on diverse datasets. Our findings highlight context-dependent trade-offs and offer guidelines for designing FL systems that uphold responsible AI principles, ensuring fairness, privacy, and equitable real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16233v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dawood Wasif, Dian Chen, Sindhuja Madabushi, Nithin Alluru, Terrence J. Moore, Jin-Hee Cho</dc:creator>
    </item>
    <item>
      <title>RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning by Balancing Privacy, Fairness and Utility in Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2503.16251</link>
      <description>arXiv:2503.16251v1 Announce Type: cross 
Abstract: Autonomous vehicles (AVs) increasingly rely on Federated Learning (FL) to enhance perception models while preserving privacy. However, existing FL frameworks struggle to balance privacy, fairness, and robustness, leading to performance disparities across demographic groups. Privacy-preserving techniques like differential privacy mitigate data leakage risks but worsen fairness by restricting access to sensitive attributes needed for bias correction. This work explores the trade-off between privacy and fairness in FL-based object detection for AVs and introduces RESFL, an integrated solution optimizing both. RESFL incorporates adversarial privacy disentanglement and uncertainty-guided fairness-aware aggregation. The adversarial component uses a gradient reversal layer to remove sensitive attributes, reducing privacy risks while maintaining fairness. The uncertainty-aware aggregation employs an evidential neural network to weight client updates adaptively, prioritizing contributions with lower fairness disparities and higher confidence. This ensures robust and equitable FL model updates. We evaluate RESFL on the FACET dataset and CARLA simulator, assessing accuracy, fairness, privacy resilience, and robustness under varying conditions. RESFL improves detection accuracy, reduces fairness disparities, and lowers privacy attack success rates while demonstrating superior robustness to adversarial conditions compared to other approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16251v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dawood Wasif, Terrence J. Moore, Jin-Hee Cho</dc:creator>
    </item>
    <item>
      <title>Enhancing Software Quality Assurance with an Adaptive Differential Evolution based Quantum Variational Autoencoder-Transformer Model</title>
      <link>https://arxiv.org/abs/2503.16335</link>
      <description>arXiv:2503.16335v1 Announce Type: cross 
Abstract: An AI-powered quality engineering platform uses artificial intelligence to boost software quality assessments through automated defect prediction and optimized performance alongside improved feature extraction. Existing models result in difficulties addressing noisy data types together with imbalances, pattern recognition complexities, ineffective feature extraction, and generalization weaknesses. To overcome those existing challenges in this research, we develop a new model Adaptive Differential Evolution based Quantum Variational Autoencoder-Transformer Model (ADE-QVAET), that combines a Quantum Variational Autoencoder-Transformer (QVAET) to obtain high-dimensional latent features and maintain sequential dependencies together with contextual relationships, resulting in superior defect prediction accuracy. Adaptive Differential Evolution (ADE) Optimization utilizes an adaptive parameter tuning method that enhances model convergence and predictive performance. ADE-QVAET integrates advanced AI techniques to create a robust solution for scalable and accurate software defect prediction that represents a top-level AI-driven technology for quality engineering applications. The proposed ADE-QVAET model attains high accuracy, precision, recall, and f1-score during the training percentage (TP) 90 of 98.08%, 92.45%, 94.67%, and 98.12%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16335v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seshu Babu Barma, Mohanakrishnan Hariharan, Satish Arvapalli</dc:creator>
    </item>
    <item>
      <title>Hoare meets Heisenberg: A Lightweight Logic for Quantum Programs</title>
      <link>https://arxiv.org/abs/2101.08939</link>
      <description>arXiv:2101.08939v5 Announce Type: replace-cross 
Abstract: We show that Gottesman's (1998) semantics for Clifford circuits based on the Heisenberg representation gives rise to a lightweight Hoare-like logic for efficiently characterizing a common subset of quantum programs. Our applications include (i) certifying whether auxiliary qubits can be safely disposed of, (ii) determining if a system is separable across a given bipartition, (iii) checking the transversality of a gate with respect to a given stabilizer code, and (iv) computing post-measurement states for computational basis measurements. Further, this logic is extended to accommodate universal quantum computing by deriving Hoare triples for the $T$-gate, multiply-controlled unitaries such as the Toffoli gate, and some gate injection circuits that use associated magic states. A number of interesting results emerge from this logic, including a lower bound on the number of $T$ gates necessary to perform a multiply-controlled $Z$ gate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.08939v5</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aarthi Sundaram, Robert Rand, Kartik Singhal, Brad Lackey</dc:creator>
    </item>
    <item>
      <title>Approximate Computing Survey, Part I: Terminology and Software &amp; Hardware Approximation Techniques</title>
      <link>https://arxiv.org/abs/2307.11124</link>
      <description>arXiv:2307.11124v2 Announce Type: replace-cross 
Abstract: The rapid growth of demanding applications in domains applying multimedia processing and machine learning has marked a new era for edge and cloud computing. These applications involve massive data and compute-intensive tasks, and thus, typical computing paradigms in embedded systems and data centers are stressed to meet the worldwide demand for high performance. Concurrently, over the last 15 years, the semiconductor industry has established power efficiency as a first-class design concern. As a result, the community of computing systems is forced to find alternative design approaches to facilitate high-performance and power-efficient computing. Among the examined solutions, Approximate Computing has attracted an ever-increasing interest, which has resulted in novel approximation techniques for all the layers of the traditional computing stack. More specifically, during the last decade, a plethora of approximation techniques in software (programs, frameworks, compilers, runtimes, languages), hardware (circuits, accelerators), and architectures (processors, memories) have been proposed in the literature. The current article is Part I of a comprehensive survey on Approximate Computing. It reviews its motivation, terminology and principles, as well it classifies the state-of-the-art software &amp; hardware approximation techniques, presents their technical details, and reports a comparative quantitative analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11124v2</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.PL</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3716845</arxiv:DOI>
      <arxiv:journal_reference>ACM Computing Surveys, Volume 57, Issue 7, Article 185, 2025</arxiv:journal_reference>
      <dc:creator>Vasileios Leon, Muhammad Abdullah Hanif, Giorgos Armeniakos, Xun Jiao, Muhammad Shafique, Kiamal Pekmestzi, Dimitrios Soudris</dc:creator>
    </item>
    <item>
      <title>Approximate Computing Survey, Part II: Application-Specific &amp; Architectural Approximation Techniques and Applications</title>
      <link>https://arxiv.org/abs/2307.11128</link>
      <description>arXiv:2307.11128v2 Announce Type: replace-cross 
Abstract: The challenging deployment of compute-intensive applications from domains such as Artificial Intelligence (AI) and Digital Signal Processing (DSP), forces the community of computing systems to explore new design approaches. Approximate Computing appears as an emerging solution, allowing to tune the quality of results in the design of a system in order to improve the energy efficiency and/or performance. This radical paradigm shift has attracted interest from both academia and industry, resulting in significant research on approximation techniques and methodologies at different design layers (from system down to integrated circuits). Motivated by the wide appeal of Approximate Computing over the last 10 years, we conduct a two-part survey to cover key aspects (e.g., terminology and applications) and review the state-of-the art approximation techniques from all layers of the traditional computing stack. Part II of the survey classifies and presents the technical details of application-specific and architectural approximation techniques, which both target the design of resource-efficient processors/accelerators and systems. Moreover, it reports a quantitative analysis of the techniques and a detailed analysis of the application spectrum of Approximate Computing, and finally, it discusses open challenges and future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11128v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.PL</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3711683</arxiv:DOI>
      <arxiv:journal_reference>ACM Computing Surveys, Volume 57, Issue 7, Article 177, 2025</arxiv:journal_reference>
      <dc:creator>Vasileios Leon, Muhammad Abdullah Hanif, Giorgos Armeniakos, Xun Jiao, Muhammad Shafique, Kiamal Pekmestzi, Dimitrios Soudris</dc:creator>
    </item>
  </channel>
</rss>
