<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Aug 2025 04:02:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Adaptive Divide and Conquer with Two Rounds of Communication</title>
      <link>https://arxiv.org/abs/2508.17073</link>
      <description>arXiv:2508.17073v1 Announce Type: new 
Abstract: We introduce a two-round adaptive communication strategy that enables rate-optimal estimation in the white noise model without requiring prior knowledge of the underlying smoothness. In the first round, local machines send summary statistics using $(\log_2(n))^2$ bits to enable the central machine to select the tuning parameters of the procedure. In the second round, another set of statistics are transmitted using optimal number of bits, enabling the central machine to aggregate and produce a final estimator that adapts to the true smoothness level. This approach achieves optimal convergence rates across a wider range of regularities, offering a potential improvement in the adaptability and efficiency of distributed estimation compared to existing one-round methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17073v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niladri Kal, Botond Szab\'o, Rajarshi Guhaniyogi, Natesh Pillai, Debdeep Pati</dc:creator>
    </item>
    <item>
      <title>Source-Condition Analysis of Kernel Adversarial Estimators</title>
      <link>https://arxiv.org/abs/2508.17181</link>
      <description>arXiv:2508.17181v1 Announce Type: new 
Abstract: In many applications, the target parameter depends on a nuisance function defined by a conditional moment restriction, whose estimation often leads to an ill-posed inverse problem. Classical approaches, such as sieve-based GMM, approximate the restriction using a fixed set of test functions and may fail to capture important aspects of the solution. Adversarial estimators address this limitation by framing estimation as a game between an estimator and an adaptive critic. We study the class of Regularized Adversarial Stabilized (RAS) estimators that employ reproducing kernel Hilbert spaces (RKHSs) for both estimation and testing, with regularization via the RKHS norm. Our first contribution is a novel analysis that establishes finite-sample bounds for both the weak error and the root mean squared error (RMSE) of these estimators under interpretable source conditions, in contrast to existing results. Our second contribution is a detailed comparison of the assumptions underlying this RKHS-norm-regularized approach with those required for (i) RAS estimators using $\mathcal{L}^2$ penalties, and (ii) recently proposed, computationally stable Kernel Maximal Moment estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17181v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Olivas-Martinez, Andrea Rotnitzky</dc:creator>
    </item>
    <item>
      <title>The Root Finding Problem Revisited: Beyond the Robbins-Monro procedure</title>
      <link>https://arxiv.org/abs/2508.17591</link>
      <description>arXiv:2508.17591v1 Announce Type: new 
Abstract: We introduce Sequential Probability Ratio Bisection (SPRB), a novel stochastic approximation algorithm that adapts to the local behavior of the (regression) function of interest around its root. We establish theoretical guarantees for SPRB's asymptotic performance, showing that it achieves the optimal convergence rate and minimal asymptotic variance even when the target function's derivative at the root is small (at most half the step size), a regime where the classical Robbins-Monro procedure typically suffers reduced convergence rates. Further, we show that if the regression function is discontinuous at the root, Robbins-Monro converges at a rate of $1/n$ whilst SPRB attains exponential convergence. If the regression function has vanishing first-order derivative, SPRB attains a faster rate of convergence compared to stochastic approximation. As part of our analysis, we derive a nonasymptotic bound on the expected sample size and establish a generalized Central Limit Theorem under random stopping times. Remarkably, SPRB automatically provides nonasymptotic time-uniform confidence sequences that do not explicitly require knowledge of the convergence rate. We demonstrate the practical effectiveness of SPRB through simulation results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17591v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Yu, Moulinath Banerjee, Ya'acov Ritov</dc:creator>
    </item>
    <item>
      <title>Quasi-likelihood inference for SDE with mixed-effects observed at high frequency</title>
      <link>https://arxiv.org/abs/2508.17910</link>
      <description>arXiv:2508.17910v1 Announce Type: new 
Abstract: We consider statistical inference for a class of dynamic mixed-effect models described by stochastic differential equations whose drift and diffusion coefficients simultaneously depend on fixed- and random-effect parameters. Assuming that each process is observed at high frequency and the number of individuals goes to infinity, we propose a stepwise inference procedure and prove its theoretical properties. The methodology is based on suitable quasi-likelihood functions by profiling the random effect in the diffusion coefficient at the first stage, and then taking the marginal distribution in the drift coefficient in the second stage, resulting in a fully explicit and computationally convenient method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17910v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maud Delattre, Hiroki Masuda</dc:creator>
    </item>
    <item>
      <title>Efficient Semiparametric Inference for Distributed Data with Blockwise Missingness</title>
      <link>https://arxiv.org/abs/2508.16902</link>
      <description>arXiv:2508.16902v1 Announce Type: cross 
Abstract: We consider statistical inference for a finite-dimensional parameter in a regular semiparametric model under a distributed setting with blockwise missingness, where entire blocks of variables are unavailable at certain sites and sharing individual-level data is not allowed. To improve efficiency of the internal study, we propose a class of augmented one-step estimators that incorporate information from external sites through ``transfer functions.'' The proposed approach has several advantages. First, it is communication-efficient, requiring only one-round communication of summary-level statistics. Second, it satisfies a do-no-harm property in the sense that the augmented estimator is no less efficient than the original one based solely on the internal data. Third, it is statistically optimal, achieving the semiparametric efficiency bound when the transfer function is appropriately estimated from data. Finally, it is scalable, remaining asymptotically normal even when the number of external sites and the data dimension grow exponentially with the internal sample size. Simulation studies confirm both the statistical efficiency and computational feasibility of our method in distributed settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16902v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyue Huang, Huiyuan Wang, Yuqing Lei, Yong Chen</dc:creator>
    </item>
    <item>
      <title>Factor Informed Double Deep Learning For Average Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2508.17136</link>
      <description>arXiv:2508.17136v1 Announce Type: cross 
Abstract: We investigate the problem of estimating the average treatment effect (ATE) under a very general setup where the covariates can be high-dimensional, highly correlated, and can have sparse nonlinear effects on the propensity and outcome models. We present the use of a Double Deep Learning strategy for estimation, which involves combining recently developed factor-augmented deep learning-based estimators, FAST-NN, for both the response functions and propensity scores to achieve our goal. By using FAST-NN, our method can select variables that contribute to propensity and outcome models in a completely nonparametric and algorithmic manner and adaptively learn low-dimensional function structures through neural networks. Our proposed novel estimator, FIDDLE (Factor Informed Double Deep Learning Estimator), estimates ATE based on the framework of augmented inverse propensity weighting AIPW with the FAST-NN-based response and propensity estimates. FIDDLE consistently estimates ATE even under model misspecification and is flexible to also allow for low-dimensional covariates. Our method achieves semiparametric efficiency under a very flexible family of propensity and outcome models. We present extensive numerical studies on synthetic and real datasets to support our theoretical guarantees and establish the advantages of our methods over other traditional choices, especially when the data dimension is large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17136v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianqing Fan, Soham Jana, Sanjeev Kulkarni, Qishuo Yin</dc:creator>
    </item>
    <item>
      <title>Two-Sample Testing with Block-Wise Missingness in Multi-Source Data</title>
      <link>https://arxiv.org/abs/2508.17411</link>
      <description>arXiv:2508.17411v1 Announce Type: cross 
Abstract: Multi-source and multi-modal datasets are increasingly common in scientific research, yet they often exhibit block-wise missingness, where entire data sources or modalities are systematically absent for subsets of subjects. This structured form of missingness presents significant challenges for statistical analysis, particularly for two-sample hypothesis testing. Standard approaches such as imputation or complete-case analysis can introduce bias or result in substantial information loss, especially when the missingness mechanism is not random. To address this methodological gap, we propose the Block-Pattern Enhanced Test (BPET), a general framework for two-sample testing that directly accounts for block-wise missingness without requiring imputation or deletion of observations. As a concrete instantiation, we develop the Block-wise Rank In Similarity graph Edge-count (BRISE) test, which extends rank-based similarity graph methods to settings with block-wise missing data. Under mild conditions, we establish that the null distribution of BRISE converges to a chi-squared distribution. Simulation studies show that BRISE consistently controls the type I error rate and achieves good statistical power under a wide range of alternatives. Applications to two real-world datasets with block-wise missingness further demonstrate the practical utility of our method in identifying meaningful distributional differences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17411v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kejian Zhang, Muxuan Liang, Robert Maile, Doudou Zhou</dc:creator>
    </item>
    <item>
      <title>Exploring Quantum Bootstrap Sampling for AQP Error Assessment: A Pilot Study</title>
      <link>https://arxiv.org/abs/2508.17500</link>
      <description>arXiv:2508.17500v1 Announce Type: cross 
Abstract: Error assessment for Approximate Query Processing (AQP) is a challenging problem. Bootstrap sampling can produce error assessment even when the population data distribution is unknown. However, bootstrap sampling needs to produce a large number of resamples with replacement, which is a computationally intensive procedure. In this paper, we introduce a quantum bootstrap sampling (QBS) framework to generate bootstrap samples on a quantum computer and produce an error assessment for AQP query estimations. The quantum circuit design is included in this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17500v1</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-02049-9_14</arxiv:DOI>
      <dc:creator>Feng Yu, Raya Jahan</dc:creator>
    </item>
    <item>
      <title>Limits of message passing for node classification: How class-bottlenecks restrict signal-to-noise ratio</title>
      <link>https://arxiv.org/abs/2508.17822</link>
      <description>arXiv:2508.17822v1 Announce Type: cross 
Abstract: Message passing neural networks (MPNNs) are powerful models for node classification but suffer from performance limitations under heterophily (low same-class connectivity) and structural bottlenecks in the graph. We provide a unifying statistical framework exposing the relationship between heterophily and bottlenecks through the signal-to-noise ratio (SNR) of MPNN representations. The SNR decomposes model performance into feature-dependent parameters and feature-independent sensitivities. We prove that the sensitivity to class-wise signals is bounded by higher-order homophily -- a generalisation of classical homophily to multi-hop neighbourhoods -- and show that low higher-order homophily manifests locally as the interaction between structural bottlenecks and class labels (class-bottlenecks). Through analysis of graph ensembles, we provide a further quantitative decomposition of bottlenecking into underreaching (lack of depth implying signals cannot arrive) and oversquashing (lack of breadth implying signals arriving on fewer paths) with closed-form expressions. We prove that optimal graph structures for maximising higher-order homophily are disjoint unions of single-class and two-class-bipartite clusters. This yields BRIDGE, a graph ensemble-based rewiring algorithm that achieves near-perfect classification accuracy across all homophily regimes on synthetic benchmarks and significant improvements on real-world benchmarks, by eliminating the ``mid-homophily pitfall'' where MPNNs typically struggle, surpassing current standard rewiring techniques from the literature. Our framework, whose code we make available for public use, provides both diagnostic tools for assessing MPNN performance, and simple yet effective methods for enhancing performance through principled graph modification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17822v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Rubin, Sahil Loomba, Nick S. Jones</dc:creator>
    </item>
    <item>
      <title>On Asymptotic Analysis of the Two-Stage Approach: Towards Data-Driven Parameter Estimation</title>
      <link>https://arxiv.org/abs/2508.18201</link>
      <description>arXiv:2508.18201v1 Announce Type: cross 
Abstract: In this paper, we analyze the asymptotic properties of the Two-Stage (TS) estimator -- a simulation-based parameter estimation method that constructs estimators offline from synthetic data. While TS offers significant computational advantages compared to standard approaches to estimation, its statistical properties have not been previously analyzed in the literature. Under simple assumptions, we establish that the TS estimator is strongly consistent and asymptotically normal, providing the first theoretical guarantees for this class of estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18201v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Braghadeesh Lakshminarayanan, Cristian R. Rojas</dc:creator>
    </item>
    <item>
      <title>On the probability of linear separability through intrinsic volumes</title>
      <link>https://arxiv.org/abs/2404.12889</link>
      <description>arXiv:2404.12889v3 Announce Type: replace 
Abstract: A dataset with two labels is linearly separable if it can be split into its two classes with a hyperplane. This inflicts a curse on some statistical tools (such as logistic regression) but forms a blessing for others (e.g. support vector machines). Recently, the following question has regained interest: What is the probability that the data are linearly separable?
  We provide a formula for the probability of linear separability for Gaussian features and labels depending only on one marginal of the features (as in generalized linear models). In this setting, we derive an upper bound that complements the recent result by Hayakawa, Lyons, and Oberhauser [2023], and a sharp upper bound for sign-flip noise.
  To prove our results, we exploit that this probability can be expressed as a sum of the intrinsic volumes of a polyhedral cone of the form $\text{span}\{v\}\oplus[0,\infty)^n$, as shown in Cand\`es and Sur [2020]. After providing the inequality description for this cone, and an algorithm to project onto it, we calculate its intrinsic volumes. In doing so, we encounter Youden's demon problem, for which we provide a formula following Kabluchko and Zaporozhets [2020]. The key insight of this work is the following: The number of correctly labeled observations in the data affects the structure of this polyhedral cone, allowing the translation of insights from geometry into statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12889v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Kuchelmeister</dc:creator>
    </item>
    <item>
      <title>Kinetic interacting particle system: parameter estimation from complete and partial discrete observations</title>
      <link>https://arxiv.org/abs/2410.10226</link>
      <description>arXiv:2410.10226v2 Announce Type: replace 
Abstract: In this paper, we study the estimation of drift and diffusion coefficients in a two dimensional system of N interacting particles modeled by a degenerate stochastic differential equation. We consider both complete and partial observation cases over a fixed time horizon [0, T] and propose novel contrast functions for parameter estimation. In the partial observation scenario, we tackle the challenge posed by unobserved velocities by introducing a surrogate process based on the increments of the observed positions. This requires a modified contrast function to account for the correlation between successive increments. Our analysis demonstrates that, despite the loss of Markovianity due to the velocity approximation in the partial observation case, the estimators converge to a Gaussian distribution (with a correction factor in the partial observation case). The proofs are based on Ito like bounds and an adaptation of the Euler scheme. Additionally, we provide insights into H\"ormander's condition, which helps establish hypoellipticity in our model within the framework of stochastic calculus of variations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10226v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Amorino, Vytaut\.e Pilipauskait\.e</dc:creator>
    </item>
    <item>
      <title>Improved estimation of the positive powers ordered restricted standard deviation of two normal populations</title>
      <link>https://arxiv.org/abs/2412.05620</link>
      <description>arXiv:2412.05620v2 Announce Type: replace 
Abstract: The present manuscript is concerned with component-wise estimation of the positive power of ordered restricted standard deviation of two normal populations with certain restrictions on the means. We propose several improved estimators under a general scale invariant bowl-shaped loss function. Also, we proposed a class of improved estimators. It has been shown that the boundary estimator of this class is a generalized Bayes. As an application, the improved estimators are obtained with respect to quadratic loss, entropy loss, and a symmetric loss function. We have conducted extensive Monte Carlo simulations to study and compare the risk performance of the proposed estimators. Finally, a real life data analysis is given to illustrate our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05620v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Somnath Mondal, Lakshmi Kanta Patra</dc:creator>
    </item>
    <item>
      <title>A new measure of dependence: Integrated $R^2$</title>
      <link>https://arxiv.org/abs/2505.18146</link>
      <description>arXiv:2505.18146v3 Announce Type: replace 
Abstract: We propose a new measure of dependence that quantifies the degree to which a random variable $Y$ depends on a random vector $X$. This measure is zero if and only if $Y$ and $X$ are independent, and equals one if and only if $Y$ is a measurable function of $X$. We introduce a simple and interpretable estimator that is comparable in ease of computation to classical correlation coefficients such as Pearson's, Spearman's, or Chatterjee's. Building on this coefficient, we develop a model-free variable selection algorithm, feature ordering by dependence (FORD), inspired by FOCI. FORD requires no tuning parameters and is provably consistent under suitable sparsity assumptions. We demonstrate its effectiveness and improvements over FOCI through experiments on both synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18146v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mona Azadkia, Pouya Roudaki</dc:creator>
    </item>
    <item>
      <title>On the attainment of the Wasserstein--Cramer--Rao lower bound</title>
      <link>https://arxiv.org/abs/2506.12732</link>
      <description>arXiv:2506.12732v3 Announce Type: replace 
Abstract: Recently, a Wasserstein analogue of the Cramer--Rao inequality has been developed using the Wasserstein information matrix (Otto metric). This inequality provides a lower bound on the Wasserstein variance of an estimator, which quantifies its robustness against additive noise. In this study, we investigate conditions for an estimator to attain the Wasserstein--Cramer--Rao lower bound (asymptotically), which we call the (asymptotic) Wasserstein efficiency. We show a condition under which Wasserstein efficient estimators exist for one-parameter statistical models. This condition corresponds to a recently proposed Wasserstein analogue of one-parameter exponential families (e-geodesics). We also show that the Wasserstein estimator, a Wasserstein analogue of the maximum likelihood estimator based on the Wasserstein score function, is asymptotically Wasserstein efficient in location-scale families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12732v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hayato Nishimori, Takeru Matsuda</dc:creator>
    </item>
    <item>
      <title>ROC curves for LDA classifiers</title>
      <link>https://arxiv.org/abs/2507.18307</link>
      <description>arXiv:2507.18307v2 Announce Type: replace 
Abstract: In the paper, we derive an analytic formula for the ROC curves of the LDA classifiers. We establish elementary properties of these curves (monotonicity and concavity), provide formula for the area under curve (AUC) and compute the Youden J-index. Finally, we illustrate the performance of our results on a real--life dataset of Wisconsin breast cancer patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18307v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mateusz Krukowski</dc:creator>
    </item>
    <item>
      <title>Causal Inference in Longitudinal Data under Unknown Interference</title>
      <link>https://arxiv.org/abs/2106.15074</link>
      <description>arXiv:2106.15074v5 Announce Type: replace-cross 
Abstract: In longitudinal studies where units are embedded in space or a social network, interference may arise, meaning that a unit's outcome can depend on treatment histories of others. The presence of interference poses significant challenges for causal inference, particularly when the interference structure -- how a unit's outcome responds to others' influences -- is complex, heterogeneous, and unknown to researchers. This paper develops a general framework for identifying and estimating both direct and spillover effects of treatment histories under minimal assumptions about the interference structure. We introduce a class of causal estimands that capture the effects of treatment histories at any specified proximity level and show that they can be represented by a modified marginal structural model. Under sequential exchangeability, these estimands are identifiable and can be estimated using inverse probability weighting. We derive conditions for consistency and asymptotic normality of the estimators and provide procedures for constructing asymptotically conservative confidence intervals. The method's utility is demonstrated through applications in both social science and biomedical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.15074v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Wang, Michael Jetsupphasuk</dc:creator>
    </item>
    <item>
      <title>Poisson Hierarchical Indian Buffet Processes-With Indications for Microbiome Species Sampling Models</title>
      <link>https://arxiv.org/abs/2502.01919</link>
      <description>arXiv:2502.01919v2 Announce Type: replace-cross 
Abstract: We introduce the Poisson Hierarchical Indian Buffet Process (PHIBP), a new class of species sampling models designed to address the challenges of complex, sparse count data by facilitating information sharing across and within groups. Our theoretical developments enable a tractable Bayesian nonparametric framework with machine learning elements, accommodating a potentially infinite number of species (taxa) whose parameters are learned from data. Focusing on microbiome analysis, we address key gaps by providing a flexible multivariate count model that accounts for overdispersion and robustly handles diverse data types (OTUs, ASVs). We introduce novel parameters reflecting species abundance and diversity. The model borrows strength across groups while explicitly distinguishing between technical and biological zeros to interpret sparse co-occurrence patterns. This results in a framework with tractable posterior inference, exact generative sampling, and a principled solution to the unseen species problem. We describe extensions where domain experts can incorporate knowledge through covariates and structured priors, with potential for strain-level analysis. While motivated by ecology, our work provides a broadly applicable methodology for hierarchical count modeling in genetics, commerce, and text analysis, and has significant implications for the broader theory of species sampling models arising in probability and statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01919v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lancelot F. James, Juho Lee, Abhinav Pandey</dc:creator>
    </item>
    <item>
      <title>Learning an Optimal Assortment Policy under Observational Data</title>
      <link>https://arxiv.org/abs/2502.06777</link>
      <description>arXiv:2502.06777v4 Announce Type: replace-cross 
Abstract: We study the fundamental problem of offline assortment optimization under the Multinomial Logit (MNL) model, where sellers must determine the optimal subset of the products to offer based solely on historical customer choice data. While most existing approaches to learning-based assortment optimization focus on the online learning of the optimal assortment through repeated interactions with customers, such exploration can be costly or even impractical in many real-world settings. In this paper, we consider the offline learning paradigm and investigate the minimal data requirements for efficient offline assortment optimization. To this end, we introduce Pessimistic Rank-Breaking (PRB), an algorithm that combines rank-breaking with pessimistic estimation. We prove that PRB is nearly minimax optimal by establishing the tight suboptimality upper bound and a nearly matching lower bound. This further shows that "optimal item coverage" - where each item in the optimal assortment appears sufficiently often in the historical data - is both sufficient and necessary for efficient offline learning. This significantly relaxes the previous requirement of observing the complete optimal assortment in the data. Our results provide fundamental insights into the data requirements for offline assortment optimization under the MNL model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06777v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxuan Han, Han Zhong, Miao Lu, Jose Blanchet, Zhengyuan Zhou</dc:creator>
    </item>
    <item>
      <title>Deep spatio-temporal point processes: Advances and new directions</title>
      <link>https://arxiv.org/abs/2504.06364</link>
      <description>arXiv:2504.06364v2 Announce Type: replace-cross 
Abstract: Spatio-temporal point processes (STPPs) model discrete events distributed in time and space, with important applications in areas such as criminology, seismology, epidemiology, and social networks. Traditional models often rely on parametric kernels, limiting their ability to capture heterogeneous, nonstationary dynamics. Recent innovations integrate deep neural architectures -- either by modeling the conditional intensity function directly or by learning flexible, data-driven influence kernels, substantially broadening their expressive power. This article reviews the development of the deep influence kernel approach, which enjoys statistical explainability, since the influence kernel remains in the model to capture the spatiotemporal propagation of event influence and its impact on future events, while also possessing strong expressive power, thereby benefiting from both worlds. We explain the main components in developing deep kernel point processes, leveraging tools such as functional basis decomposition and graph neural networks to encode complex spatial or network structures, as well as estimation using both likelihood-based and likelihood-free methods, and address computational scalability for large-scale data. We also discuss the theoretical foundation of kernel identifiability. Simulated and real-data examples highlight applications to crime analysis, earthquake aftershock prediction, and sepsis prediction modeling, and we conclude by discussing promising directions for the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06364v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiuyuan Cheng, Zheng Dong, Yao Xie</dc:creator>
    </item>
  </channel>
</rss>
