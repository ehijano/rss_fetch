<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Jun 2025 01:42:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Visualizing the treatment effect on kidney hierarchical composite endpoints: From mosaic to maraca plots</title>
      <link>https://arxiv.org/abs/2506.02287</link>
      <description>arXiv:2506.02287v1 Announce Type: new 
Abstract: Visualizations, alongside summary tables and participant-level listings, are essential for presenting clinical trial results transparently and comprehensively. When reporting the results of clinical trials, the goal of visualization is to communicate the results of specific pre-planned analyses with visualization that are tailored to the endpoint and analysis being reported. We are considering the visualization of HCEs, combining multiple time-to-event outcomes, ordered according to a given prioritization and the timing of events, with a single continuous outcome. An illustrative example is the kidney disease progression HCE with a straightforward structure of the composite of clinical events of death and kidney failure and declines in eGFR as surrogates for kidney failure. The HCEs are analyzed by win statistics and visualized using maraca plots. Although maraca plots are very granular and allow for a detailed presentation of the distribution of HCE, researchers are still tasked with explanation of the magnitude of the treatment effect estimated by win odds. In explaining the magnitude of the treatment effect, we propose a comprehensive visualization approach. In the clinical trial design stage, we propose the sunset plots to visualize all possible treatment effects that can be observed based on the treatment effects on components. In reporting the results of the trial, we recommend the maraca plots as the primary method of visualization of the results. While the 2-d mosaic plot with the ordinal dominance graph directly corresponds to the win odds as treatment effect measure and can be used as the primary analysis-specific visualization method. And finally, we propose the Dustin plot to visualize the supportive analysis of the components, added cumulatively from the event of highest priority to assess the consistency of the treatment effect on all outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02287v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Karpefors, Dustin J Little, Hiddo J L Heerspink, Samvel B Gasparyan</dc:creator>
    </item>
    <item>
      <title>A Bayesian Spatio-Temporal Top-Down Framework for Estimating Opioid Use Disorder Risk Under Data Sparsity</title>
      <link>https://arxiv.org/abs/2506.02303</link>
      <description>arXiv:2506.02303v1 Announce Type: new 
Abstract: County-level estimates of opioid use disorder (OUD) are essential for understanding the influence of local economic and social conditions. They provide policymakers with the granular information needed to identify, target, and implement effective interventions and allocate resources appropriately. Traditional disease mapping methods typically rely on Poisson regression, modeling observed counts while adjusting for local covariates that are treated as fixed and known. However, these methods may fail to capture the complexities and uncertainties in areas with sparse or absent data. To address this challenge, we developed a Bayesian hierarchical spatio-temporal top-down approach designed to estimate county-level OUD rates when direct small-area (county) data is unavailable. This method allows us to infer small-area OUD rates and quantify associated uncertainties, even in data-sparse environments using observed state-level OUD rates and a combination of state and county level informative covariates. We applied our approach to estimate OUD rates for 3,143 counties in the United States between 2010 and 2025. Model performance was assessed through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02303v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emily N Peterson, Alex Edwards, Martha Wetzel, Lance A Waller, Hannah Cooper, Courtney Yarbrough</dc:creator>
    </item>
    <item>
      <title>Forecasting Extreme High Summer Temperatures in Paris and Cairo Using Gradient Boosting and Conformal Prediction Regions</title>
      <link>https://arxiv.org/abs/2506.02349</link>
      <description>arXiv:2506.02349v1 Announce Type: new 
Abstract: In this paper, gradient boosting is used to forecast the Q(.95) values of air temperature and the Steadman Heat Index. Paris, France during late the spring and summer months is the major focus. Predictors and responses are drawn from the Paris-Montsouris weather station for the years 2018 through 2024. Q(.95) values are used because of interest in summer heat that is statistically rare and extreme. The data are curated as a multiple time series for each year. Predictors include seven routinely collected indicators of weather conditions. They each are lagged by 14 days such that temperature and heat index forecasts are provided two weeks in advance. Forecasting uncertainty is addressed with conformal prediction regions. Forecasting accuracy is promising. Cairo, Egypt is a second location using data from the weather station at the Cairo Internal Airport over the same years and months. Cairo is a more challenging setting for temperature forecasting because its desert climate can create abrupt and erratic temperature changes. Yet, there is some progress forecasting record-setting hot days.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02349v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard Berk</dc:creator>
    </item>
    <item>
      <title>Evaluating crop rotations around the world using satellite imagery and causal machine learning</title>
      <link>https://arxiv.org/abs/2506.02384</link>
      <description>arXiv:2506.02384v1 Announce Type: new 
Abstract: Building sustainable food systems that are resilient to climate change will require improved agricultural management and policy. One common practice that is well-known to benefit crop yields is crop rotation, yet there remains limited understanding of how the benefits of crop rotation vary for different crop sequences and under different weather conditions. To address these gaps, we leverage crop type maps, satellite data, and causal machine learning to study how precrop effects on subsequent yields vary with cropping sequence choice and weather. Complementing and going beyond what is known from randomized field trials, we find that (i) for those farmers who do rotate, the most common precrop choices tend to be among the most beneficial, (ii) the effects of switching from a simple rotation (which alternates between two crops) to a more diverse rotation were typically small and sometimes even negative, (iii) precrop effects tended to be greater under rainier conditions, (iv) precrop effects were greater under warmer conditions for soybean yields but not for other crops, and (v) legume precrops conferred smaller benefits under warmer conditions. Our results and the methods we use can enable farmers and policy makers to identify which rotations will be most effective at improving crop yields in a changing climate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02384v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan M. Kluger, Stefania Di Tommaso, David B. Lobell</dc:creator>
    </item>
    <item>
      <title>Partially Regularized Ordinal Regression to Adjust Teams' Scoring for Strength of Schedule and Complementary Unit Performance in American Football</title>
      <link>https://arxiv.org/abs/2506.03057</link>
      <description>arXiv:2506.03057v1 Announce Type: new 
Abstract: American football is unique in that offensive and defensive units typically consist of separate players who don't share the field simultaneously, which tempts one to evaluate them independently. However, a team's offensive and defensive performances often complement each other. For instance, turnovers forced by the defense can create easier scoring opportunities for the offense. Using drive-by-drive data from 2014-2020 Division-I college football (Football Bowl Subdivision, FBS) and 2009-2017 National Football League (NFL) seasons, we identify complementary football features that impact scoring the most. We employ regularized ordinal regression with an elastic penalty, enabling variable selection and partially relaxing the proportional odds assumption. Moreover, given the importance of accounting for strength of the opposition, we incorporate unpenalized components to ensure full adjustment for strength of schedule. For residual diagnostics of our ordinal regression models we apply the surrogate approach, creatively extending its use to non-proportional odds models. We then adjust each team's offensive (defensive) performance to project it onto a league-average complementary unit, showcasing the effects of these adjustments on team scoring. Lastly, we evaluate the out-of-sample prediction performance of our selected model, highlighting improvements gained from incorporating complementary football features alongside strength-of-schedule adjustments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03057v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrey Skripnikov, Sujit Sivadanam</dc:creator>
    </item>
    <item>
      <title>Validating remotely sensed biomass estimates with forest inventory data in the western US</title>
      <link>https://arxiv.org/abs/2506.03120</link>
      <description>arXiv:2506.03120v1 Announce Type: new 
Abstract: Monitoring aboveground biomass (AGB) and its density (AGBD) at high resolution is essential for carbon accounting and ecosystem management. While NASA's spaceborne Global Ecosystem Dynamics Investigation (GEDI) LiDAR mission provides globally distributed reference measurements for AGBD estimation, the majority of commercial remote sensing products based on GEDI remain without rigorous or independent validation. Here, we present an independent regional validation of an AGBD dataset offered by terraPulse, Inc., based on independent reference data from the US Forest Service Forest Inventory and Analysis (FIA) program. Aggregated to 64,000-hectare hexagons and US counties across the US states of Utah, Nevada, and Washington, we found very strong agreement between terraPulse and FIA estimates. At the hexagon scale, we report R2 = 0.88, RMSE = 26.68 Mg/ha, and a correlation coefficient (r) of 0.94. At the county scale, agreement improves to R2 = 0.90, RMSE =32.62 Mg/ha, slope = 1.07, and r = 0.95. Spatial and statistical analyses indicated that terraPulse AGBD values tended to exceed FIA estimates in non-forest areas, likely due to FIA's limited sampling of non-forest vegetation. The terraPulse AGBD estimates also exhibited lower values in high-biomass forests, likely due to saturation effects in its optical remote-sensing covariates. This study advances operational carbon monitoring by delivering a scalable framework for comprehensive AGBD validation using independent FIA data, as well as a benchmark validation of a new commercial dataset for global biomass monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03120v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiuyu Cao, Joseph O. Sexton, Panshi Wang, Dimitrios Gounaridis, Neil H. Carter, Kai Zhu</dc:creator>
    </item>
    <item>
      <title>Party Ideologies and Political Polarization-Driven Conflicts: A Study of the Global South</title>
      <link>https://arxiv.org/abs/2506.02004</link>
      <description>arXiv:2506.02004v1 Announce Type: cross 
Abstract: Post-World War II armed conflicts have often been viewed with higher scrutiny in order to avoid a full-scale global war. This scrutiny has led to the establishment of determinants of war such as poverty, inequalities, literacy, and many more. There is a gap that exists in probing countries in the Global South for political party fragmentation and examining ideology-driven polarization's effect on armed conflicts. This paper fills this gap by asking the question: How does political identity-induced polarization affect conflicts in the Global South region? Polarization indices are created based on socially relevant issues and party stances from the V-Party Dataset. Along with control variables, they are tested against the response variables conflict frequency and conflict severity created from the UCDP (Uppsala Conflict Data Program). Through Chow's test, Regional Structural Breaks are found between regions when accounting for polarization-conflict dynamics. A multilevel mixed effects modelling approach is used to create region-specific models to find what types of polarization affect conflict in different geographies and their adherence to normative current developments. The paper highlights that vulnerable regions of the world are prone to higher polarization-induced violence. Modelling estimates indicate polarization of party credo on Minority Rights, Rejection of Political Violence, Religious Principles, and Political Pluralism are strong proponents of cultivated violence. The Global South's inhibitions and slow progress towards development are caused by hindrances from armed conflicts; this paper's results show self-inflicted political instability and fragmentation's influence on these events, making the case for urgency in addressing and building inter-group homogeneity and tolerance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02004v1</guid>
      <category>physics.soc-ph</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shreyansh Padarha</dc:creator>
    </item>
    <item>
      <title>Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?</title>
      <link>https://arxiv.org/abs/2506.02058</link>
      <description>arXiv:2506.02058v1 Announce Type: cross 
Abstract: Accurate evaluation of large language models (LLMs) is crucial for understanding their capabilities and guiding their development. However, current evaluations often inconsistently reflect the actual capacities of these models. In this paper, we demonstrate that one of many contributing factors to this \textit{evaluation crisis} is the oversight of unseen knowledge -- information encoded by LLMs but not directly observed or not yet observed during evaluations. We introduce KnowSum, a statistical framework designed to provide a more comprehensive assessment by quantifying the unseen knowledge for a class of evaluation tasks. KnowSum estimates the unobserved portion by extrapolating from the appearance frequencies of observed knowledge instances. We demonstrate the effectiveness and utility of KnowSum across three critical applications: estimating total knowledge, evaluating information retrieval effectiveness, and measuring output diversity. Our experiments reveal that a substantial volume of knowledge is omitted when relying solely on observed LLM performance. Importantly, KnowSum yields significantly different comparative rankings for several common LLMs based on their internal knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02058v1</guid>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Li, Jiayi Xin, Qi Long, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements</title>
      <link>https://arxiv.org/abs/2506.02260</link>
      <description>arXiv:2506.02260v1 Announce Type: cross 
Abstract: The growing prevalence of digital health technologies has led to the generation of complex multi-modal data, such as physical activity measurements simultaneously collected from various sensors of mobile and wearable devices. These data hold immense potential for advancing health studies, but current methods predominantly rely on supervised learning, requiring extensive labeled datasets that are often expensive or impractical to obtain, especially in clinical studies. To address this limitation, we propose a self-supervised learning framework called Multi-modal Cross-masked Autoencoder (MoCA) that leverages cross-modality masking and the Transformer autoencoder architecture to utilize both temporal correlations within modalities and cross-modal correlations between data streams. We also provide theoretical guarantees to support the effectiveness of the cross-modality masking scheme in MoCA. Comprehensive experiments and ablation studies demonstrate that our method outperforms existing approaches in both reconstruction and downstream tasks. We release open-source code for data processing, pre-training, and downstream tasks in the supplementary materials. This work highlights the transformative potential of self-supervised learning in digital health and multi-modal data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02260v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Howon Ryu, Yuliang Chen, Yacun Wang, Andrea Z. LaCroix, Chongzhi Di, Loki Natarajan, Yu Wang, Jingjing Zou</dc:creator>
    </item>
    <item>
      <title>Memory Access Vectors: Improving Sampling Fidelity for CPU Performance Simulations</title>
      <link>https://arxiv.org/abs/2506.02344</link>
      <description>arXiv:2506.02344v1 Announce Type: cross 
Abstract: Accurate performance projection of large-scale benchmarks is essential for CPU architects to evaluate and optimize future processor designs. SimPoint sampling, which uses Basic Block Vectors (BBVs), is a widely adopted technique to reduce simulation time by selecting representative program phases. However, BBVs often fail to capture the behavior of applications with extensive array-indirect memory accesses, leading to inaccurate projections. In particular, the 523.xalancbmk_r benchmark exhibits complex data movement patterns that challenge traditional SimPoint methods. To address this, we propose enhancing SimPoint's BBV methodology by incorporating Memory Access Vectors (MAV), a microarchitecture independent technique that tracks functional memory access patterns. This combined approach significantly improves the projection accuracy of 523.xalancbmk_r on a 192-core system-on-chip, increasing it from 80% to 98%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02344v1</guid>
      <category>cs.AR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sriyash Caculo, Mahesh Madhav, Jeff Baxter</dc:creator>
    </item>
    <item>
      <title>Gender Inequality in English Textbooks Around the World: an NLP Approach</title>
      <link>https://arxiv.org/abs/2506.02425</link>
      <description>arXiv:2506.02425v1 Announce Type: cross 
Abstract: Textbooks play a critical role in shaping children's understanding of the world. While previous studies have identified gender inequality in individual countries' textbooks, few have examined the issue cross-culturally. This study applies natural language processing methods to quantify gender inequality in English textbooks from 22 countries across 7 cultural spheres. Metrics include character count, firstness (which gender is mentioned first), and TF-IDF word associations by gender. The analysis also identifies gender patterns in proper names appearing in TF-IDF word lists, tests whether large language models can distinguish between gendered word lists, and uses GloVe embeddings to examine how closely keywords associate with each gender. Results show consistent overrepresentation of male characters in terms of count, firstness, and named entities. All regions exhibit gender inequality, with the Latin cultural sphere showing the least disparity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02425v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tairan Liu</dc:creator>
    </item>
    <item>
      <title>Prenatal phthalate exposures and adiposity outcomes trajectories: a multivariate Bayesian factor regression approach</title>
      <link>https://arxiv.org/abs/2506.02518</link>
      <description>arXiv:2506.02518v1 Announce Type: cross 
Abstract: We aim to assess the longitudinal effects of prenatal exposure to phthalates on the risk of childhood obesity in children aged 4 to 7, with potential time-varying and sex-specific effects. Multiple body-composition-related outcomes, such as BMI z-score, fat mass percentage, and waist circumference, are available in the data. Existing chemical mixture analyses often look at these outcomes individually due to the limited availability of multivariate models for mixture exposures. We propose a multivariate Bayesian factor regression that handles multicollinearity in chemical exposures and borrows information across highly correlated outcomes to improve estimation efficiency. We demonstrate the proposed method's utility through simulation studies and an analysis of data from the Mount Sinai Children's Environmental Health Study. We find the associations between prenatal phthalate exposures and adiposity outcomes in male children to be negative at early ages but to become positive as the children get older.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02518v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phuc H. Nguyen, Stephanie M. Engel, Amy H. Herring</dc:creator>
    </item>
    <item>
      <title>Variable Selection in Functional Linear Cox Model</title>
      <link>https://arxiv.org/abs/2506.02524</link>
      <description>arXiv:2506.02524v1 Announce Type: cross 
Abstract: Modern biomedical studies frequently collect complex, high-dimensional physiological signals using wearables and sensors along with time-to-event outcomes, making efficient variable selection methods crucial for interpretation and improving the accuracy of survival models. We propose a novel variable selection method for a functional linear Cox model with multiple functional and scalar covariates measured at baseline. We utilize a spline-based semiparametric estimation approach for the functional coefficients and a group minimax concave type penalty (MCP), which effectively integrates smoothness and sparsity into the estimation of functional coefficients. An efficient group descent algorithm is used for optimization, and an automated procedure is provided to select optimal values of the smoothing and sparsity parameters. Through simulation studies, we demonstrate the method's ability to perform accurate variable selection and estimation. The method is applied to 2003-06 cohort of the National Health and Nutrition Examination Survey (NHANES) data, identifying the key temporally varying distributional patterns of physical activity and demographic predictors related to all-cause mortality. Our analysis sheds light on the intricate association between daily distributional patterns of physical activity and all-cause mortality among older US adults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02524v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanzhen Yue, Stella Self, Yichao Wu, Jiajia Zhang, Rahul Ghosal</dc:creator>
    </item>
    <item>
      <title>Incorporating Correlated Nugget Effects in Multivariate Spatial Models: An Application to Argo Ocean Data</title>
      <link>https://arxiv.org/abs/2506.03042</link>
      <description>arXiv:2506.03042v1 Announce Type: cross 
Abstract: Accurate analysis of global oceanographic data, such as temperature and salinity profiles from the Argo program, requires geostatistical models capable of capturing complex spatial dependencies. This study introduces Gaussian and non-Gaussian hierarchical multivariate Mat\'ern-SPDE models with correlated nugget effects to account for small-scale variability and measurement error correlations. Using simulations and Argo data, we demonstrate that incorporating correlated nugget effects significantly improves the accuracy of parameter estimation and spatial prediction in both Gaussian and non-Gaussian multivariate spatial processes. When applied to global ocean temperature and salinity data, our model yields lower correlation estimates between fields compared to models that assume independent noise. This suggests that traditional models may overestimate the underlying field correlation. By separating these effects, our approach captures fine-scale oceanic patterns more effectively. These findings show the importance of relaxing the assumption of independent measurement errors in multivariate hierarchical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03042v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damilya Saduakhas, David Bolin, Xiaotian Jin, Alexandre B. Simas, Jonas Wallin</dc:creator>
    </item>
    <item>
      <title>An IPCW Adjusted Win Statistics Approach in Clinical Trials Incorporating Equivalence Margins to Define Ties</title>
      <link>https://arxiv.org/abs/2506.03050</link>
      <description>arXiv:2506.03050v1 Announce Type: cross 
Abstract: In clinical trials, multiple outcomes of different priorities commonly occur as the patient's response may not be adequately characterized by a single outcome. Win statistics are appealing summary measures for between-group difference at more than one endpoint. When defining the result of pairwise comparisons of a time-to-event endpoint, it is desirable to allow ties to account for incomplete follow-up and not clinically meaningful difference in endpoints of interest. In this paper, we propose a class of win statistics for time-to-event endpoints with a user-specified equivalence margin. These win statistics are identifiable in the presence of right-censoring and do not depend on the censoring distribution. We then develop estimation and inference procedures for the proposed win statistics based on inverse-probability-of-censoring {weighting} (IPCW) adjustment to handle right-censoring. We conduct extensive simulations to investigate the operational characteristics of the proposed procedure in the finite sample setting. A real oncology trial is used to illustrate the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03050v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ying Cui, Bo Huang, Gaohong Dong, Ryuji Uozumi, Lu Tian</dc:creator>
    </item>
    <item>
      <title>Constructing Evidence-Based Tailoring Variables for Adaptive Interventions</title>
      <link>https://arxiv.org/abs/2506.03054</link>
      <description>arXiv:2506.03054v1 Announce Type: cross 
Abstract: Background: An adaptive intervention (ADI) uses individual information in order to select treatment, to improve effectiveness while reducing cost and burden. ADIs require tailoring variables: person- and potentially time-specific information used to decide whether and how to deliver treatment. Specifying a tailoring variable for an intervention requires specifying what to measure, when to measure it, when to make the resulting decisions, and what cutoffs should be used in making those decisions. This involves tradeoffs between specificity versus sensitivity, and between waiting for sufficient information versus intervening quickly. These questions are causal and prescriptive (what should be done and when), not merely predictive (what would happen if current conditions persist).
  Purpose: There is little specific guidance in the literature on how to empirically choose tailoring variables, including cutoffs, measurement times, and decision times. Methods: We review possible approaches for comparing potential tailoring variables and propose a framework for systematically developing tailoring variables.
  Results: Although secondary observational data can be used to select tailoring variables, additional assumptions are needed. A specifically designed randomized experiment for optimization purposes (an optimization randomized clinical trial or ORCT), in the form of a multi-arm randomized trial, sequential multiple assignment randomized trial, a factorial experiment, or hybrid among them, may provide a more direct way to answer these questions.
  Conclusions: Using randomization directly to inform tailoring variables would provide the most direct causal evidence, but designing a trial to compare both tailoring variables and treatments adds complexity; further methodological research is warranted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03054v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John J. Dziak, Inbal Nahum-Shani</dc:creator>
    </item>
    <item>
      <title>Two-Phase Treatment with Noncompliance: Identifying the Cumulative Average Treatment Effect via Multisite Instrumental Variables</title>
      <link>https://arxiv.org/abs/2506.03104</link>
      <description>arXiv:2506.03104v1 Announce Type: cross 
Abstract: In evaluating a multi-phase intervention, the cumulative average treatment effect (ATE) is often the causal estimand of key interest. Yet some individuals who do not respond well to the Phase-I treatment may subsequently display noncompliant behaviors. However, noncompliance tends to be constrained by the stochastic availability of slots under the alternative treatment condition in Phase II, which makes the notion of the "complier average treatment effect" problematic. Moreover, the Phase-I treatment is expected to affect an individual's potential outcomes through additional pathways that violate the exclusion restriction. Extending an instrumental variable (IV) strategy for multisite trials, we clarify conditions for identifying the cumulative ATE of a two-phase treatment by employing the random assignment of the Phase-I treatment as the IV. Our strategy relaxes the exclusion restriction and the sequential ignorability in their conventional forms. We evaluate the performance of the new strategy through simulations. Reanalyzing data from the Tennessee class size study in which students and teachers were assigned at random to either a small or a regular class in kindergarten (Phase I) yet noncompliance occurred in Grade 1 (Phase II), we estimate the cumulative ATE of receiving two years of instruction in a small class versus a regular class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03104v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guanglei Hong, Xu Qin, Zhengyan Xu, Fan Yang</dc:creator>
    </item>
    <item>
      <title>Modelling benthic animals in space and time using Bayesian Point Process with cross validation: the case of Holoturians</title>
      <link>https://arxiv.org/abs/2506.01763</link>
      <description>arXiv:2506.01763v2 Announce Type: replace 
Abstract: Understanding the spatial distribution of Holothurians is an essential task for ecosystem monitoring and sustainable management, particularly in the Mediterranean habitats. However, species distribution modeling is often complicated by the presence-only nature of the data and heterogeneous sampling designs. This study develops a spatio-temporal framework based on Log-Gaussian Cox Processes to analyze Holothurians' positions collected across nine survey campaigns conducted from 2022 to 2024 near Giglio Island, Italy. The surveys combined high-resolution photogrammetry with diver-based visual censuses, leading to varying detection probabilities across habitats, especially within Posidonia oceanica meadows. We adopt a model with a shared spatial Gaussian process component to accommodate this complexity, accounting for habitat structure, environmental covariates, and temporal variability. Model estimation is performed using Integrated Nested Laplace Approximation. We evaluate the predictive performances of alternative model specifications through a novel k-fold cross-validation strategy for point processes, using the Continuous Ranked Probability Score. Our approach provides a flexible and computationally efficient framework for integrating heterogeneous presence-only data in marine ecology and comparing the predictive ability of alternative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01763v2</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Poggio, Gian Mario Sangiovanni, Gianluca Mastrantonio, Giovanna Jona Lasinio, Edoardo Casoli, Stefano Moro, Daniele Ventura</dc:creator>
    </item>
    <item>
      <title>An Isotonic Mechanism for Overlapping Ownership</title>
      <link>https://arxiv.org/abs/2306.11154</link>
      <description>arXiv:2306.11154v3 Announce Type: replace-cross 
Abstract: Motivated by the problem of improving peer review at large scientific conferences, this paper studies how to elicit self-evaluations to improve review scores in a natural many-to-many owner-item (e.g., author-paper) situation with overlapping ownership. We design a simple, efficient and truthful mechanism to elicit self-evaluations from item owners that can be used to calibrate their noisy review scores in the existing evaluation process (e.g., papers' review scores from peers).
  Our approach starts by partitioning the owner-item relation structure into disjoint blocks, each sharing a common set of co-owners. We then elicit the ranking of items from each owner and employ isotonic regression to produce adjusted item scores, aligning with both the reported rankings and raw item review scores. We prove that truth-telling by all owners is a payoff dominant Nash equilibrium for any valid partition of the overlapping ownership sets under natural conditions. Moreover, the truthfulness depends on eliciting rankings independently within each block, making block partition optimization crucial for improving statistical efficiency. Despite being computationally intractable in general, we develop a nearly linear-time greedy algorithm that provably finds a performant block partition with appealing robust approximation guarantees. Extensive experiments on both synthetic data and real-world conference review data demonstrate the effectiveness of our mechanism in a pressing real-world problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11154v3</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jibang Wu, Haifeng Xu, Yifan Guo, Weijie Su</dc:creator>
    </item>
    <item>
      <title>High-Throughput Asset Pricing</title>
      <link>https://arxiv.org/abs/2311.10685</link>
      <description>arXiv:2311.10685v3 Announce Type: replace-cross 
Abstract: We apply empirical Bayes (EB) to mine data on 136,000 long-short strategies constructed from accounting ratios, past returns, and ticker symbols. This ``high-throughput asset pricing'' matches the out-of-sample performance of top journals while eliminating look-ahead bias. Naively mining for the largest Sharpe ratios leads to similar performance, consistent with our theoretical results, though EB uniquely provides unbiased predictions with transparent intuition. Predictability is concentrated in accounting strategies, small stocks, and pre-2004 periods, consistent with limited attention theories. Multiple testing methods popular in finance fail to identify most out-of-sample performers. High-throughput methods provide a rigorous, unbiased framework for understanding asset prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10685v3</guid>
      <category>q-fin.GN</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Y. Chen, Chukwuma Dim</dc:creator>
    </item>
    <item>
      <title>Forecasting Company Fundamentals</title>
      <link>https://arxiv.org/abs/2411.05791</link>
      <description>arXiv:2411.05791v2 Announce Type: replace-cross 
Abstract: Company fundamentals are key to assessing companies' financial and overall success and stability. Forecasting them is important in multiple fields, including investing and econometrics. While statistical and contemporary machine learning methods have been applied to many time series tasks, there is a lack of comparison of these approaches on this particularly challenging data regime. To this end, we try to bridge this gap and thoroughly evaluate the theoretical properties and practical performance of 24 deterministic and probabilistic company fundamentals forecasting models on real company data. We observe that deep learning models provide superior forecasting performance to classical models, in particular when considering uncertainty estimation. To validate the findings, we compare them to human analyst expectations and find that their accuracy is comparable to the automatic forecasts. We further show how these high-quality forecasts can benefit automated stock allocation. We close by presenting possible ways of integrating domain experts to further improve performance and increase reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05791v2</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Transactions on Machine Learning Research (2025)</arxiv:journal_reference>
      <dc:creator>Felix Divo, Eric Endress, Kevin Endler, Kristian Kersting, Devendra Singh Dhami</dc:creator>
    </item>
    <item>
      <title>De-Biasing Structure Function Estimates From Sparse Time Series of the Solar Wind: A Data-Driven Approach</title>
      <link>https://arxiv.org/abs/2412.10053</link>
      <description>arXiv:2412.10053v2 Announce Type: replace-cross 
Abstract: Structure functions, which represent the moments of the increments of a stochastic process, are essential complementary statistics to power spectra for analysing the self-similar behaviour of a time series. However, many real-world environmental datasets, such as those collected by spacecraft monitoring the solar wind, contain gaps, which inevitably corrupt the statistics. The nature of this corruption for structure functions remains poorly understood - indeed, often overlooked. Here we simulate gaps in a large set of magnetic field intervals from Parker Solar Probe in order to characterize the behaviour of the structure function of a sparse time series of solar wind turbulence. We quantify the resultant error with regards to the overall shape of the structure function, and its slope in the inertial range. Noting the consistent underestimation of the true curve when using linear interpolation, we demonstrate the ability of an empirical correction factor to de-bias these estimates. This correction, "learnt" from the data from a single spacecraft, is shown to generalize well to data from a solar wind regime elsewhere in the heliosphere, producing smaller errors, on average, for missing fractions &gt;25%. Given this success, we apply the correction to gap-affected Voyager intervals from the inner heliosheath and local interstellar medium, obtaining spectral indices similar to those from previous studies. This work provides a tool for future studies of fragmented solar wind time series, such as those from Voyager, MAVEN, and OMNI, as well as sparsely-sampled astrophysical and geophysical processes more generally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10053v2</guid>
      <category>astro-ph.SR</category>
      <category>physics.space-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3847/1538-4357/addc6a 10.3847/1538-4357/addc6a 10.3847/1538-4357/addc6a</arxiv:DOI>
      <dc:creator>Daniel Wrench, Tulasi N. Parashar</dc:creator>
    </item>
    <item>
      <title>Do Large Language Models (Really) Need Statistical Foundations?</title>
      <link>https://arxiv.org/abs/2505.19145</link>
      <description>arXiv:2505.19145v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) represent a new paradigm for processing unstructured data, with applications across an unprecedented range of domains. In this paper, we address, through two arguments, whether the development and application of LLMs would genuinely benefit from foundational contributions from the statistics discipline. First, we argue affirmatively, beginning with the observation that LLMs are inherently statistical models due to their profound data dependency and stochastic generation processes, where statistical insights are naturally essential for handling variability and uncertainty. Second, we argue that the persistent black-box nature of LLMs -- stemming from their immense scale, architectural complexity, and development practices often prioritizing empirical performance over theoretical interpretability -- renders closed-form or purely mechanistic analyses generally intractable, thereby necessitating statistical approaches due to their flexibility and often demonstrated effectiveness. To substantiate these arguments, the paper outlines several research areas -- including alignment, watermarking, uncertainty quantification, evaluation, and data mixture optimization -- where statistical methodologies are critically needed and are already beginning to make valuable contributions. We conclude with a discussion suggesting that statistical research concerning LLMs will likely form a diverse ``mosaic'' of specialized topics rather than deriving from a single unifying theory, and highlighting the importance of timely engagement by our statistics community in LLM research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19145v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weijie Su</dc:creator>
    </item>
    <item>
      <title>Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series</title>
      <link>https://arxiv.org/abs/2505.20697</link>
      <description>arXiv:2505.20697v2 Announce Type: replace-cross 
Abstract: The field of hypothesis generation promises to reduce costs in neuroscience by narrowing the range of interventional studies needed to study various phenomena. Existing machine learning methods can generate scientific hypotheses from complex datasets, but many approaches assume causal relationships are static over time, limiting their applicability to systems with dynamic, state-dependent behavior, such as the brain. While some techniques attempt dynamic causal discovery through factor models, they often restrict relationships to linear patterns or impose other simplifying assumptions. We propose a novel method that models dynamic graphs as a conditionally weighted superposition of static graphs, where each static graph can capture nonlinear relationships. This approach enables the detection of complex, time-varying interactions between variables beyond linear limitations. Our method improves f1-scores of predicted dynamic causal patterns by roughly 22-28% on average over baselines in some of our experiments, with some improvements reaching well over 60%. A case study on real brain data demonstrates our method's ability to uncover relationships linked to specific behavioral states, offering valuable insights into neural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20697v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachary C. Brown, David Carlson</dc:creator>
    </item>
  </channel>
</rss>
