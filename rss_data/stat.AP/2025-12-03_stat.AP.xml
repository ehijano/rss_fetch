<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Dec 2025 02:36:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>From 'What-is' to 'What-if' in Human-Factor Analysis: A Post-Occupancy Evaluation Case</title>
      <link>https://arxiv.org/abs/2512.02060</link>
      <description>arXiv:2512.02060v1 Announce Type: new 
Abstract: Human-factor analysis typically employs correlation analysis and significance testing to identify relationships between variables. However, these descriptive ('what-is') methods, while effective for identifying associations, are often insufficient for answering causal ('what-if') questions. Their application in such contexts often overlooks confounding and colliding variables, potentially leading to bias and suboptimal or incorrect decisions.
  We advocate for explicitly distinguishing descriptive from interventional questions in human-factor analysis, and applying causal inference frameworks specifically to these problems to prevent methodological mismatches. This approach disentangles complex variable relationships and enables counterfactual reasoning. Using post-occupancy evaluation (POE) data from the Center for the Built Environment's (CBE) Occupant Survey as a demonstration case, we show how causal discovery reveals intervention hierarchies and directional relationships that traditional associational analysis misses. The systematic distinction between causally associated and independent variables, combined with intervention prioritization capabilities, offers broad applicability to complex human-centric systems, for example, in building science or ergonomics, where understanding intervention effects is critical for optimization and decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02060v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xia Chen, Ruiji Sun, Philipp Geyer, Andr\'e Borrmann, Stefano Schiavon</dc:creator>
    </item>
    <item>
      <title>Probabilistic Analysis of Various Squash Shots and Skill Study of Different Levels of Squash Players and Teams</title>
      <link>https://arxiv.org/abs/2512.02210</link>
      <description>arXiv:2512.02210v1 Announce Type: new 
Abstract: We introduce a compact probabilistic model for two-player and two-team (four-player) squash matches, along with a practical skill-comparison rule derived from point-scoring probabilities. Using recorded shot types and court locations, we analyze how shot distributions differ between professional-level and intermediate-level players. Our analysis shows that professional players use a wider variety of shots and favor backcourt play to maintain control, while intermediate players concentrate more on mid-court shots, generate more errors, and exercise less positional control. These results quantify strategic differences in squash, offer a simple method to compare player and team skill, and provide actionable insights for sports analytics and coaching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02210v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prathamesh Anwekar, Kaushal Kirpekar, Mahesh B, Sainath Bitragunta</dc:creator>
    </item>
    <item>
      <title>Estimating excess mortality during the Covid-19 pandemic in Aotearoa New Zealand: Addendum</title>
      <link>https://arxiv.org/abs/2512.02266</link>
      <description>arXiv:2512.02266v1 Announce Type: new 
Abstract: In our previous article, we estimated excess mortality during in Aotearoa New Zealand for 2020 to 2023. Since our work was published, updated population estimates have been released by Statistics NZ. In this short letter, we provide the results of applying our original model to the new population data. Our updated excess mortality estimate of 2.0% (95% CI [0.5%, 3.3%]) is 1.3 percentage points higher than our original estimate because the new population estimates for the period 2020 to 2023 are smaller, but the main conclusions of our original article still apply.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02266v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael J. Plank, Pubudu Senanayake, Richard Lyon</dc:creator>
    </item>
    <item>
      <title>Leveraging ontologies to predict biological activity of chemicals across genes</title>
      <link>https://arxiv.org/abs/2512.02327</link>
      <description>arXiv:2512.02327v1 Announce Type: new 
Abstract: High-throughput screening (HTS) is useful for evaluating chemicals for potential human health risks. However, given the extraordinarily large number of genes, assay endpoints, and chemicals of interest, available data are sparse, with dose-response curves missing for the vast majority of chemical-gene pairs. Although gene ontologies characterize similarity among genes with respect to known cellular functions and biological pathways, the sensitivity of various pathways to environmental contaminants remains unclear. We propose a novel Dose-Activity Response Tracking (DART) approach to predict the biological activity of chemicals across genes using information on chemical structural properties and gene ontologies within a Bayesian factor model. Designed to provide toxicologists with a flexible tool applicable across diverse HTS assay platforms, DART reveals the latent processes driving dose-response behavior and predicts new activity profiles for chemical-gene pairs lacking experimental data. We demonstrate the performance of DART through simulation studies and an application to a vast new multi-experiment data set consisting of dose-response observations generated by the exposure of HepG2 cells to per- and polyfluoroalkyl substances (PFAS), where it provides actionable guidance for chemical prioritization and inference on the structural and functional mechanisms underlying assay activation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02327v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jennifer N. Kampe, David B. Dunson, Celeste K. Carberry, Julia E. Rager, Daniel Zilber, Kyle P. Messier</dc:creator>
    </item>
    <item>
      <title>Statistical Analysis of weather variables of Antofagasta</title>
      <link>https://arxiv.org/abs/1911.04232</link>
      <description>arXiv:1911.04232v1 Announce Type: cross 
Abstract: The statistical behavior of weather variables of Antofagasta is described, especially the daily data of air as temperature, pressure and relative humidity measured at 08:00, 14:00 and 20:00. In this article, we use a time series deseasonalization technique, Q-Q plot, skewness, kurtosis and the Pearson correlation coefficient. We found that the distributions of the records are symmetrical and have positive kurtosis, so they have heavy tails. In addition, the variables are highly autocorrelated, extending up to one year in the case of pressure and temperature.</description>
      <guid isPermaLink="false">oai:arXiv.org:1911.04232v1</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H. Farfan, A. Castillo, S. Curilef</dc:creator>
    </item>
    <item>
      <title>Opening the Black Box: Nowcasting Singapore's GDP Growth and its Explainability</title>
      <link>https://arxiv.org/abs/2512.02092</link>
      <description>arXiv:2512.02092v1 Announce Type: cross 
Abstract: Timely assessment of current conditions is essential especially for small, open economies such as Singapore, where external shocks transmit rapidly to domestic activity. We develop a real-time nowcasting framework for quarterly GDP growth using a high-dimensional panel of approximately 70 indicators, encompassing economic and financial indicators over 1990Q1-2023Q2. The analysis covers penalized regressions, dimensionality-reduction methods, ensemble learning algorithms, and neural architectures, benchmarked against a Random Walk, an AR(3), and a Dynamic Factor Model. The pipeline preserves temporal ordering through an expanding-window walk-forward design with Bayesian hyperparameter optimization, and uses moving block-bootstrap procedures both to construct prediction intervals and to obtain confidence bands for feature-importance measures. It adopts model-specific and XAI-based explainability tools. A Model Confidence Set procedure identifies statistically superior learners, which are then combined through simple, weighted, and exponentially weighted schemes; the resulting time-varying weights provide an interpretable representation of model contributions. Predictive ability is assessed via Giacomini-White tests. Empirical results show that penalized regressions, dimensionality-reduction models, and GRU networks consistently outperform all benchmarks, with RMSFE reductions of roughly 40-60%; aggregation delivers further gains. Feature-attribution methods highlight industrial production, external trade, and labor-market indicators as dominant drivers of Singapore's short-run growth dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02092v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Attolico</dc:creator>
    </item>
    <item>
      <title>Efficient and Intuitive Two-Phase Validation Across Multiple Models via Principal Components</title>
      <link>https://arxiv.org/abs/2512.02182</link>
      <description>arXiv:2512.02182v1 Announce Type: cross 
Abstract: Two-phase sampling offers a cost-effective way to validate error-prone measurements in observational databases or randomized trials. Inexpensive or easy-to-obtain information is collected for the entire study in Phase I. Then, a subset of patients undergoes cost-intensive validation to collect more accurate data in Phase II. Critically, any Phase I variables can be used to strategically select the Phase II subset, often enriched for a particular model of interest. However, when balancing primary and secondary analyses in the same study, competing models and priorities can result in poorly defined objectives for the most informative Phase II sampling criterion. We propose an intuitive, easy-to-use solution that balances and prioritizes explaining the largest amount of variability across all models of interest. Using principal components to succinctly summarize the inherent variability of the error-prone covariates for all models. Then, we sample patients with the most "extreme" principal components (i.e., the smallest or largest values) for validation. Through simulations and an application to data from the National Health and Nutrition Examination Survey (NHANES), we show that extreme tail sampling on the first principal component offers simultaneous efficiency gains across multiple models of interest relative to sampling for one specific model. Our proposed sampling strategy is implemented in the open-source R package, auditDesignR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02182v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Cole Manschot</dc:creator>
    </item>
    <item>
      <title>Unbiased Estimation of Multi-Way Gravity Models</title>
      <link>https://arxiv.org/abs/2512.02203</link>
      <description>arXiv:2512.02203v1 Announce Type: cross 
Abstract: Maximum likelihood estimators, such as the Poisson Pseudo-Maximum Likelihood (PPML), suffer from the incidental parameter problem: a bias in the estimation of structural parameters that arises from the joint estimation of structural and nuisance parameters. To address this issue in multi-way gravity models, we propose a novel, asymptotically unbiased estimator. Our method reframes the estimation as a series of classification tasks and is agnostic to both the number and structure of fixed effects. In sparse data environments, common in the network formation literature, it is also computationally faster than PPML. We provide empirical evidence that our estimator yields more accurate point estimates and confidence intervals than PPML and its bias-correction strategies. These improvements hold even under model misspecification and are more pronounced in sparse settings. While PPML remains competitive in dense, low-dimensional data, our approach offers a robust alternative for multi-way models that scales efficiently with sparsity. The method is applied to estimate the effect of a policy reform on spatial accessibility to health care in France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02203v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Resende, Guillaume Lecu\'e, Lionel Wilner, Philippe Chon\'e</dc:creator>
    </item>
    <item>
      <title>Implicit score-driven filters for time-varying parameter models</title>
      <link>https://arxiv.org/abs/2512.02744</link>
      <description>arXiv:2512.02744v1 Announce Type: cross 
Abstract: We propose an observation-driven modeling framework that permits time variation in the model parameters using an implicit score-driven (ISD) update. The ISD update maximizes the logarithmic observation density with respect to the parameter vector, while penalizing the weighted L2 norm relative to a one-step-ahead predicted parameter. This yields an implicit stochastic-gradient update. We show that the popular class of explicit score-driven (ESD) models arises if the observation log density is linearly approximated around the prediction. By preserving the full density, the ISD update globalizes favorable local properties of the ESD update. Namely, for log-concave observation densities, whether correctly specified or not, the ISD filter is stable for all learning rates, while its updates are contractive in mean squared error toward the (pseudo-)true parameter at every time step. We demonstrate the usefulness of ISD filters in simulations and empirical illustrations in finance and macroeconomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02744v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rutger-Jan Lange, Bram van Os, Dick van Dijk</dc:creator>
    </item>
    <item>
      <title>Q-triplet characterization of atmospheric time series at Antofagasta: A missing values problem</title>
      <link>https://arxiv.org/abs/2512.02820</link>
      <description>arXiv:2512.02820v1 Announce Type: cross 
Abstract: Located in northern Chile (23.7{\deg}S, 70.4{\deg}W), Antofagasta has an exceptionally arid and stable climate characterized by minimal precipitation and consistent weather patterns. Nevertheless, despite these climate conditions being meaningful for several research and practical applications, our understanding of weather dynamics remains limited. The available meteorological data from 1969 to 2016 is analogical, which presents a significant challenge to analyze because these records are riddled with missing values, some measurements were taken at irregular measuring intervals, making it an interesting puzzle to grasp the Antofagasta's climate scenario. To overcome this issue, we present a comprehensive statistical analysis of atmospheric temperature, pressure, and humidity time series. Our analytical approach involves the q-triplet calculation method, serving as a powerful tool to identify distinctive behavior within systems under non-equilibrium states. Our results suggest that, in general, the q-triplet values satisfy the condition $q_\text{sens}&lt;1&lt;q_\text{stat}&lt;q_\text{rel}$, a pattern that has been observed in previous studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02820v1</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1742-6596/2839/1/012009</arxiv:DOI>
      <arxiv:journal_reference>Journal of Physics: Conference Series, 2839 (2024) 012009</arxiv:journal_reference>
      <dc:creator>Hishan Farf\'an-Bachiloglu, Francisco A. Calder\'on, Dar\'io G. P\'erez</dc:creator>
    </item>
    <item>
      <title>Leveraging generative adversarial networks with spatially adaptive denormalization for multivariate stochastic seismic data inversion</title>
      <link>https://arxiv.org/abs/2512.02863</link>
      <description>arXiv:2512.02863v1 Announce Type: cross 
Abstract: Probabilistic seismic inverse modeling often requires the prediction of both spatially correlated geological heterogeneities (e.g., facies) and continuous parameters (e.g., rock and elastic properties). Generative adversarial networks (GANs) provide an efficient training-image-based simulation framework capable of reproducing complex geological models with high accuracy and comparably low generative cost. However, their application in stochastic geophysical inversion for multivariate property prediction is limited, as representing multiple coupled properties requires large and unstable networks with high memory and training demands. A more recent variant of GANs with spatially adaptive denormalization (SPADE-GAN) enables the direct conditioning of facies spatial distributions on local probability maps. Leveraging on such features, an iterative geostatistical inversion algorithm is proposed, SPADE-GANInv, integrating a pre-trained SPADE-GAN with geostatistical simulation, for the prediction of facies and multiple correlated continuous properties from seismic data. The SPADE-GAN is trained to reproduce realistic facies geometries, while sequential stochastic co-simulation predicts the spatial variability of the facies-dependent continuous properties. At each iteration, a set of subsurface realizations is generated and used to compute synthetic seismic data. The realizations providing the highest similarity coefficient to the observed data are used to update the subsurface probability models in the next iteration. The method is demonstrated on both 2-D synthetic scenarios and field data, targeting the prediction of facies, porosity, and acoustic impedance from full-stack seismic data. Results show that the algorithm enables accurate multivariate prediction, mitigates the impact of biased prior data, and accommodates additional local conditioning such as well logs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02863v1</guid>
      <category>physics.geo-ph</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Roberto Miele, Leonardo Azevedo</dc:creator>
    </item>
    <item>
      <title>Geometric Modeling of Hippocampal Tau Deposition: A Surface-Based Framework for Covariate Analysis and Off-Target Contamination Detection</title>
      <link>https://arxiv.org/abs/2511.01732</link>
      <description>arXiv:2511.01732v2 Announce Type: replace 
Abstract: We introduce a framework combining geometric modeling with disease progression analysis to investigate tau deposition in Alzheimer's disease (AD) using positron emission tomography (PET) data. Focusing on the hippocampus, we construct a principal surface that captures the spatial distribution and morphological changes of tau pathology. By projecting voxels onto this surface, we quantify tau coverage, intensity, and thickness through bidirectional projection distances and interpolated standardized uptake value ratios (SUVR). This low-dimensional embedding preserves spatial specificity while mitigating multiple comparison issues. Covariate effects are analyzed using a two-stage regression model with inverse probability weighting to adjust for signal sparsity and selection bias. Using the SuStaIn model, we identify subtypes and stages of AD, revealing distinct tau dynamics: the limbic-predominant subtype shows age-related nonlinear accumulation in coverage and thickness, whereas the posterior subtype exhibits uniform SUVR increases across disease progression. Model-based predictions show that hippocampal tau deposition follows a structured spatial trajectory expanding bidirectionally with increasing thickness, while subtype differences highlight posterior hippocampal involvement consistent with whole-brain patterns. Finally, directional signal patterns on the principal surface reveal contamination from the choroid plexus, demonstrating the broader applicability of the proposed framework across modalities including amyloid PET.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01732v2</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>stat.OT</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liangkang Wang, Akhil Ambekar, Ani Eloyan</dc:creator>
    </item>
    <item>
      <title>Can-SAVE: Deploying Low-Cost and Population-Scale Cancer Screening via Survival Analysis Variables and EHR</title>
      <link>https://arxiv.org/abs/2309.15039</link>
      <description>arXiv:2309.15039v4 Announce Type: replace-cross 
Abstract: Conventional medical cancer screening methods are costly, labor-intensive, and extremely difficult to scale. Although AI can improve cancer detection, most systems rely on complex or specialized medical data, making them impractical for large-scale screening. We introduce Can-SAVE, a lightweight AI system that ranks population-wide cancer risks solely based on medical history events. By integrating survival model outputs into a gradient-boosting framework, our approach detects subtle, long-term patient risk patterns - often well before clinical symptoms manifest. Can-SAVE was rigorously evaluated on a real-world dataset of 2.5 million adults spanning five Russian regions, marking the study as one of the largest and most comprehensive deployments of AI-driven cancer risk assessment. In a retrospective oncologist-supervised study over 1.9M patients, Can-SAVE achieves a 4-10x higher detection rate at identical screening volumes and an Average Precision (AP) of 0.228 vs. 0.193 for the best baseline (LoRA-tuned Qwen3-Embeddings via DeepSeek-R1 summarization). In a year-long prospective pilot (426K patients), our method almost doubled the cancer detection rate (+91%) and increased population coverage by 36% over the national screening protocol. The system demonstrates practical scalability: a city-wide population of 1 million patients can be processed in under three hours using standard hardware, enabling seamless clinical integration. This work proves that Can-SAVE achieves nationally significant cancer detection improvements while adhering to real-world public healthcare constraints, offering immediate clinical utility and a replicable framework for population-wide screening. Code for training and feature engineering is available at https://github.com/sb-ai-lab/Can-SAVE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15039v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petr Philonenko, Vladimir Kokh, Pavel Blinov</dc:creator>
    </item>
    <item>
      <title>On Statistical Inference for High-Dimensional Binary Time Series</title>
      <link>https://arxiv.org/abs/2512.00338</link>
      <description>arXiv:2512.00338v2 Announce Type: replace-cross 
Abstract: The analysis of non-real-valued data, such as binary time series, has attracted great interest in recent years. This manuscript proposes a post-selection estimator for estimating the coefficient matrices of a high-dimensional generalized binary vector autoregressive process and establishes a Gaussian approximation theorem for the proposed estimator. Furthermore, it introduces a second-order wild bootstrap algorithm to enable statistical inference on the coefficient matrices. Numerical studies and empirical applications demonstrate the good finite-sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00338v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dehao Dai, Yunyi Zhang</dc:creator>
    </item>
  </channel>
</rss>
