<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Oct 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Speeding up SQL subqueries via decoupling of non-correlated predicate (extended version)</title>
      <link>https://arxiv.org/abs/2510.05907</link>
      <description>arXiv:2510.05907v1 Announce Type: cross 
Abstract: In this paper, we discuss a novel technique for processing correlated subqueries in SQL. The core idea is to isolate the non-correlated part of the predicate and use it to reduce the number of evaluations of the correlated part. We begin by providing an overview of several classes of queries that may benefit from this technique. For each class, we propose a potential rewrite and discuss the conditions under which it is advantageous. Next, we address the evaluation aspects of the proposed rewrites: 1) we describe our approach to adapting the block-based Volcano query processing model, and 2) we discuss the benefits of implementing that technique within a position-enabled column-store with late materialization support. Finally, we present a simple cost model that allows estimation of the benefits of said rewrites.
  Our evaluation has a quantitative part and a qualitative part. The former focuses on studying the impact of non-correlated predicate selectivity on our technique. The latter identifies the limitations of our approach by comparing it with alternative approaches available in existing systems. Overall, experiments conducted using PosDB (a position-enabled column-store) and PostgreSQL demonstrated that, under suitable conditions, our technique can achieve a 5x improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05907v1</guid>
      <category>cs.DB</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dmitrii Radivonchik, Yakov Kuzin, Anton Chizhov, Dmitriy Shcheka, Mikhail Firsov, Kirill Smirnov, George Chernishev</dc:creator>
    </item>
    <item>
      <title>lm-Meter: Unveiling Runtime Inference Latency for On-Device Language Models</title>
      <link>https://arxiv.org/abs/2510.06126</link>
      <description>arXiv:2510.06126v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into everyday applications, but their prevalent cloud-based deployment raises growing concerns around data privacy and long-term sustainability. Running LLMs locally on mobile and edge devices (on-device LLMs) offers the promise of enhanced privacy, reliability, and reduced communication costs. However, realizing this vision remains challenging due to substantial memory and compute demands, as well as limited visibility into performance-efficiency trade-offs on resource-constrained hardware. We propose lm-Meter, the first lightweight, online latency profiler tailored for on-device LLM inference. lm-Meter captures fine-grained, real-time latency at both phase (e.g., embedding, prefill, decode, softmax, sampling) and kernel levels without auxiliary devices. We implement lm-Meter on commercial mobile platforms and demonstrate its high profiling accuracy with minimal system overhead, e.g., only 2.58% throughput reduction in prefill and 0.99% in decode under the most constrained Powersave governor. Leveraging lm-Meter, we conduct comprehensive empirical studies revealing phase- and kernel-level bottlenecks in on-device LLM inference, quantifying accuracy-efficiency trade-offs, and identifying systematic optimization opportunities. lm-Meter provides unprecedented visibility into the runtime behavior of LLMs on constrained platforms, laying the foundation for informed optimization and accelerating the democratization of on-device LLM systems. Code and tutorials are available at https://github.com/amai-gsu/LM-Meter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06126v1</guid>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3769012.3770614</arxiv:DOI>
      <dc:creator>Haoxin Wang, Xiaolong Tu, Hongyu Ke, Huirong Chai, Dawei Chen, Kyungtae Han</dc:creator>
    </item>
  </channel>
</rss>
