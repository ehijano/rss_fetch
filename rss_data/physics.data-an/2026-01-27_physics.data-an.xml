<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Jan 2026 02:48:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>McSAS3: improved Monte Carlo small-angle scattering analysis software for dilute and dense scatterers</title>
      <link>https://arxiv.org/abs/2601.18659</link>
      <description>arXiv:2601.18659v1 Announce Type: new 
Abstract: McSAS3 is the refactored successor to the original McSAS Monte Carlo small-angle scattering analysis software. It is intended to be integrated in automated data processing pipelines, but can also be used to process individual (batches of) scattering data.
  McSAS3 comes with a graphical user interface (McSAS3GUI), complete with guides, examples and videos. McSAS3GUI will help to generate and test the three configuration files that McSAS3 needs for data read-in, Monte Carlo optimization and histogramming. The user interface can also be used to process individual files or batches, and can be augmented with machine-specific use templates.
  The Monte Carlo (MC) approach is able to fit most practical scattering patterns extremely well, resulting in form-free model parameter distributions. Theoretically, these can be distributions on any model parameter, but in practice the MC-optimized parameter is usually a (volume-weighted) size distribution, in absolute volume fraction for absolute-scaled data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18659v1</guid>
      <category>physics.data-an</category>
      <category>cond-mat.mtrl-sci</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian Richard Pauw, Ingo Bre{\ss}ler</dc:creator>
    </item>
    <item>
      <title>Recursive Manifold Coherence: A Geometric Framework for Deadtime Recovery in Distributed Trigger Systems</title>
      <link>https://arxiv.org/abs/2601.17043</link>
      <description>arXiv:2601.17043v1 Announce Type: cross 
Abstract: Large-scale neutrino observatories operate under unavoidable detector deadtime and signal pile-up, leading to systematic inefficiencies in conventional coincidence-based trigger systems. Such triggers typically rely on binary temporal windows and assume continuous sensor availability, causing partial or complete loss of correlated signal information during non-live intervals. We introduce Recursive Manifold Coherence (RMC), a geometric framework that reformulates distributed trigger logic as a continuous state estimation problem in a low-dimensional information space defined by correlated charge and timing observables. Instead of applying hard vetoes during deadtime, the proposed method employs a recursive update rule that propagates a coherence state across sensor nodes, allowing partially obscured signals to be retained and evaluated consistently. Using simulation studies representative of large optical detector arrays, we demonstrate that RMC successfully recovers event-level coherence for high-multiplicity topologies even when direct coincidence chains are broken. By treating the detector response as a smooth manifold rather than discrete hits, the framework achieves superior robustness against data fragmentation compared to standard binary logic. The framework is detector-agnostic and compatible with software-defined trigger pipelines, providing a flexible foundation for deadtime-aware analysis and triggering strategies in future distributed detector systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17043v1</guid>
      <category>physics.ins-det</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thammarat Yawisit, Pittaya Pannil</dc:creator>
    </item>
    <item>
      <title>Non-parametric finite-sample credible intervals with one-dimensional priors: a middle ground between Bayesian and frequentist intervals</title>
      <link>https://arxiv.org/abs/2601.17621</link>
      <description>arXiv:2601.17621v1 Announce Type: cross 
Abstract: We propose a new type of statistical interval obtained by weakening the definition of a p% credible interval: Having observed the interval (rather than the full dataset) we should put at least a p% belief in it. From a decision-theoretical point of view the resulting intervals occupy a middle ground between frequentist and fully Bayesian statistical intervals, both practically and philosophically: To a p% Bayesian credible interval we should assign (at least a) p% belief also after seeing the full dataset, while p% frequentist intervals we in general only assign a p% belief before seeing either the data or the interval.
  We derive concrete implementations for two cases: estimation of the fraction of a distribution that falls below a certain value (i.e., the CDF), and of the mean of a distribution with bounded support. Even though the problems are fully non parametric, these methods require only one-dimensional priors. They share many of the practical advantages of Bayesian methods while avoiding the complexity of assigning high-dimensional priors altogether. Asymptotically they give intervals equivalent to the fully Bayesian approach and somewhat wider intervals, respectively. We discuss promising directions where the proposed type of interval may provide significant advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17621v1</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tim Ritmeester</dc:creator>
    </item>
    <item>
      <title>Quantum-Inspired Algorithms beyond Unitary Circuits: the Laplace Transform</title>
      <link>https://arxiv.org/abs/2601.17724</link>
      <description>arXiv:2601.17724v1 Announce Type: cross 
Abstract: Quantum-inspired algorithms can deliver substantial speedups over classical state-of-the-art methods by executing quantum algorithms with tensor networks on conventional hardware. Unlike circuit models restricted to unitary gates, tensor networks naturally accommodate non-unitary maps. This flexibility lets us design quantum-inspired methods that start from a quantum algorithmic structure, yet go beyond unitarity to achieve speedups. Here we introduce a tensor-network approach to compute the discrete Laplace transform, a non-unitary, aperiodic transform (in contrast to the Fourier transform). We encode a length-$N$ signal on two paired $n$-qubit registers and decompose the overall map into a non-unitary exponential Damping Transform followed by a Quantum Fourier Transform, both compressed in a single matrix-product operator. This decomposition admits strong MPO compression to low bond dimension resulting in significant acceleration. We demonstrate simulations up to $N=2^{30}$ input data points, with up to $2^{60}$ output data points, and quantify how bond dimension controls runtime and accuracy, including precise and efficient pole identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17724v1</guid>
      <category>quant-ph</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noufal Jaseem, Sergi Ramos-Calderer, Gauthameshwar S., Dingzu Wang, Jos\'e Ignacio Latorre, Dario Poletti</dc:creator>
    </item>
    <item>
      <title>Neural-Inspired Multi-Agent Molecular Communication Networks for Collective Intelligence</title>
      <link>https://arxiv.org/abs/2601.18018</link>
      <description>arXiv:2601.18018v1 Announce Type: cross 
Abstract: Molecular Communication (MC) is a pivotal enabler for the Internet of Bio-Nano Things (IoBNT). However, current research often relies on super-capable individual agents with complex transceiver architectures that defy the energy and processing constraints of realistic nanomachines. This paper proposes a paradigm shift towards collective intelligence, inspired by the cortical networks of the biological brain. We introduce a decentralized network architecture where simple nanomachines interact via a diffusive medium using a threshold-based firing mechanism modeled by Greenberg-Hastings (GH) cellular automata. We derive fixed-point equations for steady-state populations via mean-field analysis and validate them against stochastic simulations. We demonstrate that the network undergoes a second-order phase transition at a specific activation threshold. Crucially, we show that both pairwise and collective mutual information peak exactly at this critical transition point, confirming that the system maximizes information propagation and processing capacity at the edge of chaos.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18018v1</guid>
      <category>nlin.AO</category>
      <category>cs.ET</category>
      <category>eess.SP</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boran A. Kilic, Ozgur B. Akan</dc:creator>
    </item>
    <item>
      <title>Mitigating Deadtime in Distributed Optical Arrays: A Liveness-Aware Trigger Approach for High-Energy Neutrino Detection</title>
      <link>https://arxiv.org/abs/2601.18114</link>
      <description>arXiv:2601.18114v1 Announce Type: cross 
Abstract: Large-scale neutrino observatories operate under unavoidable detector deadtime arising from photomultiplier saturation, digitizer limits, and front-end readout constraints. Conventional coincidence-based trigger logic implicitly assumes continuous sensor availability and therefore suffers systematic efficiency loss when channels become temporarily non-live. This work presents the design of a liveness-aware trigger architecture targeting low-latency FPGA deployment in distributed optical arrays. We introduce a recursive Infinite Impulse Response (IIR) update law implemented as a fully synthesizable pipeline that constructs a continuity-preserving effective observable at each sensor node. Rather than collapsing during non-liveness intervals, the observable decays smoothly while retaining phase and amplitude information relevant for network-level coherence estimation. By explicitly separating continuous measurement construction from discrete trigger decision logic, the proposed architecture enables graceful degradation under partial channel non-liveness. Performance is evaluated using a hybrid validation framework that combines representative event topologies derived from IceCube Open Data with a hardware-accurate signal and noise model spanning a wide dynamic range. Simulation results demonstrate that the proposed trigger sustains high event recovery efficiency in regimes of elevated deadtime probability, where conventional coincidence logic degrades substantially. Furthermore, the continuity-preserving observable yields up to a two-order-of-magnitude improvement in effective signal-to-noise ratio, enabling robust detection under strong saturation and non-ideal operating conditions. This method provides a robust foundation for next-generation firmware-level trigger strategies in large-scale, noise-dominated detector systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18114v1</guid>
      <category>physics.ins-det</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thammarat Yawisit, Pittaya Pannil</dc:creator>
    </item>
    <item>
      <title>A strictly geostrophic product of sea-surface velocities from the SWOT fast-sampling phase</title>
      <link>https://arxiv.org/abs/2601.18182</link>
      <description>arXiv:2601.18182v1 Announce Type: cross 
Abstract: While geostrophy remains the simplest and most practical balance to extract velocity information from sea-surface height anomaly (SSHa), confusions remain within the oceanographic community to what extent this balance can be applied to altimetric observations with the launch of the Surface Water and Ocean Topography (SWOT) satellite. Given the limited temporal resolution of SWOT, many studies have resorted to claiming that the spatially filtered SSHa fields correspond to the geostrophic component. This introduces the ambiguity of which spatial scale to choose. Here, we build upon the recent developments in internal tide (IT) corrections (Yadidya et al., 2025) and apply a dynamic mode decomposition (DMD)-based method introduced by Lapo et al. (2025) to robustly extract the geostrophic component associated with sub-inertial frequencies from the SWOT one-day-repeat orbit; we distribute the global dataset as a public good. We provide the joint probability density function (PDF) of vorticity and strain, and spectra of SSHa at a few cross-over regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18182v1</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Takaya Uchida, Badarvada Yadidya, Vadim Bertrand, Jia-Xian Chang, Brian Arbic, Jay Shriver, Julien Le Sommer</dc:creator>
    </item>
    <item>
      <title>Constraining Reionization Morphology and Source Properties with 21cm-Galaxy Cross-Correlation Surveys</title>
      <link>https://arxiv.org/abs/2601.18627</link>
      <description>arXiv:2601.18627v1 Announce Type: cross 
Abstract: Cross-correlations between 21cm observations and galaxy surveys provide a powerful probe of reionization by reducing foreground sensitivity while linking ionization morphology to galaxies. We quantify the constraining power of 21cm-Galaxy cross-power spectra for inferring neutral hydrogen fraction $x_\mathrm{HI}(z)$ and mean overdensity $\langle 1+\delta_\mathrm{HI} \rangle(z)$, exploring dependence on field of view, redshift precision $\sigma_z$, and minimum halo mass $M_\mathrm{h,min}$. We employ our simulation-based inference framework EoRFlow for likelihood-free parameter estimation. Mock observations include thermal noise for 100h SKA-Low with foreground avoidance and realistic galaxy survey effects. For a fiducial survey ($\mathrm{FOV}=100\,\mathrm{deg}^2$, $\sigma_z=0.001$, $M_\mathrm{h,min}=10^{11}\mathrm{M}_\odot$), cross-power spectra yield unbiased constraints with posterior volumes (PV) of $\sim$10% relative to priors. Cross-power measurements reduce PV by 20-30% versus 21cm auto-power alone. With foreground avoidance, spectroscopic redshift precision is essential; photometric redshifts render cross-correlations uninformative. Notably, cross-power spectra constrain ionizing source properties, the escape fraction $f_\mathrm{esc}$ and star formation efficiency $f_*$, which remain degenerate in auto-power (PV $&gt;$60%). Tight constraints require either deep surveys detecting faint galaxies ($M_\mathrm{h,min} \sim 10^{10}\mathrm{M}_\odot$) with moderate foregrounds, or conservative mass limits with optimistic foreground removal (PV $&lt;$15%). 21cm-Galaxy cross-correlations enhance morphology constraints beyond auto-power while enabling previously inaccessible source property constraints. Realizing full potential requires precise redshifts and either faint galaxy detection limits or improved 21cm foreground cleaning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18627v1</guid>
      <category>astro-ph.CO</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yannic Pietschke, Anne Hutter, Caroline Heneka</dc:creator>
    </item>
    <item>
      <title>Log Gaussian Cox Process Background Modeling in High Energy Physics</title>
      <link>https://arxiv.org/abs/2508.11740</link>
      <description>arXiv:2508.11740v2 Announce Type: replace 
Abstract: Background modeling is one of the most critical components in high energy physics data analyses, and for smooth backgrounds it is often performed by fitting using an analytic functional form. In this paper a novel method based on Log Gaussian Cox Processes (LGCP) is introduced to model smooth backgrounds while making minimal assumptions on the underlying shape. In LGCP, samples are assumed to be drawn from a non-homogeneous Poisson process, with an intensity function drawn from a Gaussian process. Markov Chain Monte Carlo is used for optimizing the hyper parameters and drawing the final fit for the background estimate from the posterior. Synthetic experiments comparing background modeling from functional forms and the LGCP are used to compare the different methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11740v2</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuval Frid, Liron Barak, Pavani Jairam, Michael Kagan, Rachel Jordan Hyneman</dc:creator>
    </item>
    <item>
      <title>How to pick the best anomaly detector?</title>
      <link>https://arxiv.org/abs/2511.14832</link>
      <description>arXiv:2511.14832v2 Announce Type: replace-cross 
Abstract: Anomaly detection has the potential to discover new physics in unexplored regions of the data. However, choosing the best anomaly detector for a given data set in a model-agnostic way is an important challenge which has hitherto largely been neglected. In this paper, we introduce the data-driven ARGOS metric, which has a sound theoretical foundation and is empirically shown to robustly select the most sensitive anomaly detection model given the data. Focusing on weakly-supervised, classifier-based anomaly detection methods, we show that the ARGOS metric outperforms other model selection metrics previously used in the literature, in particular the binary cross-entropy loss. We explore several realistic applications, including hyperparameter tuning as well as architecture and feature selection, and in all cases we demonstrate that ARGOS is robust to the noisy conditions of anomaly detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14832v2</guid>
      <category>hep-ph</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marie Hein, Gregor Kasieczka, Michael Kr\"amer, Louis Moureaux, Alexander M\"uck, David Shih</dc:creator>
    </item>
    <item>
      <title>Learnability Window in Gated Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2512.05790</link>
      <description>arXiv:2512.05790v3 Announce Type: replace-cross 
Abstract: We develop a theoretical framework that explains how gating mechanisms determine the learnability window $\mathcal{H}_N$ of recurrent neural networks, defined as the largest temporal horizon over which gradient information remains statistically recoverable. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone is insufficient: learnability is governed instead by the effective learning rates $\mu_{t,\ell}$, per-lag and per-neuron quantities obtained from first-order expansions of gate-induced Jacobian products in Backpropagation Through Time. These effective learning rates act as multiplicative filters that control both the magnitude and anisotropy of gradient transport. Under heavy-tailed ($\alpha$-stable) gradient noise, we prove that the minimal sample size required to detect a dependency at lag~$\ell$ scales as $N(\ell)\propto f(\ell)^{-\kappa_\alpha}$, where $f(\ell)=\|\mu_{t,\ell}\|_1$ is the effective learning rate envelope and $\kappa_\alpha=\alpha/(\alpha-1)$ is the concentration exponent governing empirical averages. This yields an explicit characterization of $\mathcal{H}_N$ and closed-form scaling laws for logarithmic, polynomial, and exponential decay of $f(\ell)$. The theory shows that the time-scale spectra induced by the effective learning rates are the dominant determinants of learnability: broader or more heterogeneous spectra slow the decay of $f(\ell)$, enlarging the learnability window, while heavy-tailed noise uniformly compresses $\mathcal{H}_N$ by slowing statistical concentration to $N^{-1/\kappa_{\alpha}}$. By integrating gate-induced time-scale geometry with gradient noise and sample complexity, the framework identifies effective learning rates as the primary objects that determine whether, when, and over what horizons recurrent networks can learn long-range temporal dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05790v3</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Livi</dc:creator>
    </item>
  </channel>
</rss>
