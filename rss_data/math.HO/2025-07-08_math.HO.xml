<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.HO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.HO</link>
    <description>math.HO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.HO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Jul 2025 04:01:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Integrators at War: Mediating in AI-assisted Resort-to-Force Decisions</title>
      <link>https://arxiv.org/abs/2501.06861</link>
      <description>arXiv:2501.06861v1 Announce Type: cross 
Abstract: The integration of AI systems into the military domain is changing the way war-related decisions are made. It binds together three disparate groups of actors - developers, integrators, users - and creates a relationship between these groups and the machine, embedded in the (pre-)existing organisational and system structures. In this article, we focus on the important, but often neglected, group of integrators within such a sociotechnical system. In complex human-machine configurations, integrators carry responsibility for linking the disparate groups of developers and users in the political and military system. To act as the mediating group requires a deep understanding of the other groups' activities, perspectives and norms. We thus ask which challenges and shortcomings emerge from integrating AI systems into resort-to-force (RTF) decision-making processes, and how to address them. To answer this, we proceed in three steps. First, we conceptualise the relationship between different groups of actors and AI systems as a sociotechnical system. Second, we identify challenges within such systems for human-machine teaming in RTF decisions. We focus on challenges that arise a) from the technology itself, b) from the integrators' role in the sociotechnical system, c) from the human-machine interaction. Third, we provide policy recommendations to address these shortcomings when integrating AI systems into RTF decision-making structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06861v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>math.HO</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dennis M\"uller, Maurice Chiodo, Mitja Sienknecht</dc:creator>
    </item>
    <item>
      <title>Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility</title>
      <link>https://arxiv.org/abs/2505.10426</link>
      <description>arXiv:2505.10426v1 Announce Type: cross 
Abstract: The legal compliance and safety of different Human-in-the-loop (HITL) setups for AI can vary greatly. This manuscript aims to identify new ways of choosing between such setups, and shows that there is an unavoidable trade-off between the attribution of legal responsibility and the technical explainability of AI. We begin by using the notion of oracle machines from computability theory to formalise different HITL setups, distinguishing between trivial human monitoring, single endpoint human action, and highly involved interaction between the human(s) and the AI. These correspond to total functions, many-one reductions, and Turing reductions respectively. A taxonomy categorising HITL failure modes is then presented, highlighting the limitations on what any HITL setup can actually achieve. Our approach then identifies oversights from UK and EU legal frameworks, which focus on certain HITL setups which may not always achieve the desired ethical, legal, and sociotechnical outcomes. We suggest areas where the law should recognise the effectiveness of different HITL setups and assign responsibility in these contexts, avoiding unnecessary and unproductive human "scapegoating". Overall, we show how HITL setups involve many technical design decisions, and can be prone to failures which are often out of the humans' control. This opens up a new analytic perspective on the challenges arising in the creation of HITL setups, helping inform AI developers and lawmakers on designing HITL to better achieve their desired outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10426v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>math.HO</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maurice Chiodo, Dennis M\"uller, Paul Siewert, Jean-Luc Wetherall, Zoya Yasmine, John Burden</dc:creator>
    </item>
    <item>
      <title>The Problem of Algorithmic Collisions: Mitigating Unforeseen Risks in a Connected World</title>
      <link>https://arxiv.org/abs/2505.20181</link>
      <description>arXiv:2505.20181v1 Announce Type: cross 
Abstract: The increasing deployment of Artificial Intelligence (AI) and other autonomous algorithmic systems presents the world with new systemic risks. While focus often lies on the function of individual algorithms, a critical and underestimated danger arises from their interactions, particularly when algorithmic systems operate without awareness of each other, or when those deploying them are unaware of the full algorithmic ecosystem deployment is occurring in. These interactions can lead to unforeseen, rapidly escalating negative outcomes - from market crashes and energy supply disruptions to potential physical accidents and erosion of public trust - often exceeding the human capacity for effective monitoring and the legal capacities for proper intervention. Current governance frameworks are inadequate as they lack visibility into this complex ecosystem of interactions. This paper outlines the nature of this challenge and proposes some initial policy suggestions centered on increasing transparency and accountability through phased system registration, a licensing framework for deployment, and enhanced monitoring capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20181v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>math.HO</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maurice Chiodo, Dennis M\"uller</dc:creator>
    </item>
    <item>
      <title>Is there Ethics in Mathematics?</title>
      <link>https://arxiv.org/abs/2507.01185</link>
      <description>arXiv:2507.01185v2 Announce Type: replace 
Abstract: This is a critical response to some arguments and general recommendations presented in a discussion paper Four Levels of Ethical Engagement [EiM Discussion Paper 1/2018 University of Cambridge Ethics in Mathematics Project, https://www.ethics-in-mathematics.com/assets/dp/18 1.pdf] by Maurice Chiodo and Piers Bursill-Hall. Much in their article is based on certain observations about characteristic psychological traits of mathematicians and their patterns of behavior that I find to be in stark contrast to my own observations. I argue against their assumptions and conclusions using examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01185v2</guid>
      <category>math.HO</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roman Kossak</dc:creator>
    </item>
    <item>
      <title>Integral Invariants and Hamiltonian Systems</title>
      <link>https://arxiv.org/abs/2507.02878</link>
      <description>arXiv:2507.02878v2 Announce Type: replace 
Abstract: In this review and methodological article we discuss the main ideas of the integral invariants theory. This theory was originated by Poincare and Cartan. We show how ideas of this theory connect such a different fields of mathematical physics as Hamiltonian dynamics, optics and hydrodynamics. We focus our attention on the results that are rarely expounded in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02878v2</guid>
      <category>math.HO</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.18744.87043</arxiv:DOI>
      <dc:creator>Oleg Zubelevich</dc:creator>
    </item>
  </channel>
</rss>
