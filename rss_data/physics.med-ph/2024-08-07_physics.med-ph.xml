<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.med-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.med-ph</link>
    <description>physics.med-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.med-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Aug 2024 01:36:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Self-calibrating Intelligent OCT-SLO System</title>
      <link>https://arxiv.org/abs/2408.02703</link>
      <description>arXiv:2408.02703v2 Announce Type: new 
Abstract: A unique sample independent 3D self calibration methodology is tested on a unique optical coherence tomography and multi-spectral scanning laser ophthalmoscope (OCT-SLO) hybrid system. Operators visual cognition is replaced by computer vision using the proposed novel fully automatic AI-driven system design. Sample specific automatic contrast adjustment of the beam is achieved on the pre-instructed region of interest. The AI model deduces infrared, fluorescence, and visual spectrum optical alignment by estimating pre-instructed features quantitatively. The tested approach, however, is flexible enough to utilize any apt AI model. Relative comparison with classical signal-to-noise-driven automation is shown to be 200 percent inferior and 130 percent slower than the AI-driven approach. The best spatial resolution of the system is found to be (a) 2.41 microns in glass bead eye phantom, 0.76 with STD 0.46 microns in the mouse retina in the axial direction, and (b) better than 228 line pair per millimeter (lp per mm) or 2 microns for all three spectrums, i.e., 488 nm, 840 nm, and 520 to 550 nm emission in coronal, frontal or x-y plane. Intelligent automation reduces the possibility of developing cold cataracts (especially in mouse imaging) and patient-associated discomfort due to delay during manual alignment by facilitating easy handling for swift ocular imaging and better accuracy. The automatic novel tabletop compact system provides true functional 3D images in three different spectrums for dynamic sample profiles. This is especially useful for photodynamic imaging treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02703v2</guid>
      <category>physics.med-ph</category>
      <category>physics.app-ph</category>
      <category>physics.ins-det</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mayank Goswami</dc:creator>
    </item>
    <item>
      <title>A Review on Organ Deformation Modeling Approaches for Reliable Surgical Navigation using Augmented Reality</title>
      <link>https://arxiv.org/abs/2408.02713</link>
      <description>arXiv:2408.02713v1 Announce Type: new 
Abstract: Augmented Reality (AR) holds the potential to revolutionize surgical procedures by allowing surgeons to visualize critical structures within the patient's body. This is achieved through superimposing preoperative organ models onto the actual anatomy. Challenges arise from dynamic deformations of organs during surgery, making preoperative models inadequate for faithfully representing intraoperative anatomy. To enable reliable navigation in augmented surgery, modeling of intraoperative deformation to obtain an accurate alignment of the preoperative organ model with the intraoperative anatomy is indispensable. Despite the existence of various methods proposed to model intraoperative organ deformation, there are still few literature reviews that systematically categorize and summarize these approaches. This review aims to fill this gap by providing a comprehensive and technical-oriented overview of modeling methods for intraoperative organ deformation in augmented reality in surgery. Through a systematic search and screening process, 112 closely relevant papers were included in this review. By presenting the current status of organ deformation modeling methods and their clinical applications, this review seeks to enhance the understanding of organ deformation modeling in AR-guided surgery, and discuss the potential topics for future advancements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02713v1</guid>
      <category>physics.med-ph</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1080/24699322.2024.2357164</arxiv:DOI>
      <dc:creator>Zheng Han, Qi Dou</dc:creator>
    </item>
    <item>
      <title>Characterization of 3D printed micro-blades for cutting tissue-embedding material</title>
      <link>https://arxiv.org/abs/2408.03155</link>
      <description>arXiv:2408.03155v1 Announce Type: new 
Abstract: Cutting soft materials on the microscale has emerging applications in single-cell studies, tissue microdissection for organoid culture, drug screens, and other analyses. However, the cutting process is complex and remains incompletely understood. Furthermore, precise control over blade geometries, such as the blade tip radius, has been difficult to achieve. In this work, we use the Nanoscribe 3D printer to precisely fabricate micro-blades (i.e., blades &lt;1 mm in length) and blade grid geometries. This fabrication method enables a systematic study of the effect of blade geometry on the indentation cutting of paraffin wax, a common tissue-embedding material. First, we print straight micro-blades with tip radius ranging from ~100 nm to 10 um. The micro-blades are mounted in a custom nanoindentation setup to measure the cutting energy during indentation cutting of paraffin. Cutting energy, measured as the difference in dissipated energy between the first and second loading cycles, decreases as blade tip radius decreases, until ~357 nm when the cutting energy plateaus despite further decrease in tip radius. Second, we expand our method to blades printed in unconventional configurations, including parallel blade structures and blades arranged in a square grid. Under the conditions tested, the cutting energy scales approximately linearly with the total length of the blades comprising the blade structure. The experimental platform described can be extended to investigate other blade geometries and guide the design of microscale cutting of soft materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03155v1</guid>
      <category>physics.med-ph</category>
      <category>cond-mat.soft</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Saisneha Koppaka, David Doan, Wei Cai, Wendy Gu, Sindy K. Y. Tang</dc:creator>
    </item>
    <item>
      <title>Fast Whole-Brain MR Multi-Parametric Mapping with Scan-Specific Self-Supervised Networks</title>
      <link>https://arxiv.org/abs/2408.02988</link>
      <description>arXiv:2408.02988v1 Announce Type: cross 
Abstract: Quantification of tissue parameters using MRI is emerging as a powerful tool in clinical diagnosis and research studies. The need for multiple long scans with different acquisition parameters prohibits quantitative MRI from reaching widespread adoption in routine clinical and research exams. Accelerated parameter mapping techniques leverage parallel imaging, signal modelling and deep learning to offer more practical quantitative MRI acquisitions. However, the achievable acceleration and the quality of maps are often limited. Joint MAPLE is a recent state-of-the-art multi-parametric and scan-specific parameter mapping technique with promising performance at high acceleration rates. It synergistically combines parallel imaging, model-based and machine learning approaches for joint mapping of T1, T2*, proton density and the field inhomogeneity. However, Joint MAPLE suffers from prohibitively long reconstruction time to estimate the maps from a multi-echo, multi-flip angle (MEMFA) dataset at high resolution in a scan-specific manner. In this work, we propose a faster version of Joint MAPLE which retains the mapping performance of the original version. Coil compression, random slice selection, parameter-specific learning rates and transfer learning are synergistically combined in the proposed framework. It speeds-up the reconstruction time up to 700 times than the original version and processes a whole-brain MEMFA dataset in 21 minutes on average, which originally requires ~260 hours for Joint MAPLE. The mapping performance of the proposed framework is ~2-fold better than the standard and the state-of-the-art evaluated reconstruction techniques on average in terms of the root mean squared error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02988v1</guid>
      <category>q-bio.QM</category>
      <category>physics.med-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amir Heydari, Abbas Ahmadi, Tae Hyung Kim, Berkin Bilgic</dc:creator>
    </item>
    <item>
      <title>Iterative CT Reconstruction via Latent Variable Optimization of Shallow Diffusion Models</title>
      <link>https://arxiv.org/abs/2408.03156</link>
      <description>arXiv:2408.03156v1 Announce Type: cross 
Abstract: Image generative AI has garnered significant attention in recent years. In particular, the diffusion model, a core component of recent generative AI, produces high-quality images with rich diversity. In this study, we propose a novel CT reconstruction method by combining the denoising diffusion probabilistic model with iterative CT reconstruction. In sharp contrast to previous studies, we optimize the fidelity loss of CT reconstruction with respect to the latent variable of the diffusion model, instead of the image and model parameters. To suppress anatomical structure changes produced by the diffusion model, we shallow the diffusion and reverse processes, and fix a set of added noises in the reverse process to make it deterministic during inference. We demonstrate the effectiveness of the proposed method through sparse view CT reconstruction of 1/10 view projection data. Despite the simplicity of the implementation, the proposed method shows the capability of reconstructing high-quality images while preserving the patient's anatomical structure, and outperforms existing methods including iterative reconstruction, iterative reconstruction with total variation, and the diffusion model alone in terms of quantitative indices such as SSIM and PSNR. We also explore further sparse view CT using 1/20 view projection data with the same trained diffusion model. As the number of iterations increases, image quality improvement comparable to that of 1/10 sparse view CT reconstruction is achieved. In principle, the proposed method can be widely applied not only to CT but also to other imaging modalities such as MRI, PET, and SPECT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03156v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>physics.med-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sho Ozaki, Shizuo Kaji, Toshikazu Imae, Kanabu Nawa, Hideomi Yamashita, Keiichi Nakagawa</dc:creator>
    </item>
    <item>
      <title>Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery</title>
      <link>https://arxiv.org/abs/2408.03208</link>
      <description>arXiv:2408.03208v1 Announce Type: cross 
Abstract: Personalized federated learning (PFL) for surgical instrument segmentation (SIS) is a promising approach. It enables multiple clinical sites to collaboratively train a series of models in privacy, with each model tailored to the individual distribution of each site. Existing PFL methods rarely consider the personalization of multi-headed self-attention, and do not account for appearance diversity and instrument shape similarity, both inherent in surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait priors for SIS, incorporating global-personalized disentanglement (GPD), appearance-regulation personalized enhancement (APE), and shape-similarity global enhancement (SGE), to boost SIS performance in each site. GPD represents the first attempt at head-wise assignment for multi-headed self-attention personalization. To preserve the unique appearance representation of each site and gradually leverage the inter-site difference, APE introduces appearance regulation and provides customized layer-wise aggregation solutions via hypernetworks for each site's personalized parameters. The mutual shape information of instruments is maintained and shared via SGE, which enhances the cross-style shape consistency on the image level and computes the shape-similarity contribution of each site on the prediction level for updating the global parameters. PFedSIS outperforms state-of-the-art methods with +1.51% Dice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding code and models will be released at https://github.com/wzjialang/PFedSIS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03208v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <category>physics.med-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialang Xu, Jiacheng Wang, Lequan Yu, Danail Stoyanov, Yueming Jin, Evangelos B. Mazomenos</dc:creator>
    </item>
    <item>
      <title>Positronium Physics and Biomedical Applications</title>
      <link>https://arxiv.org/abs/2302.09246</link>
      <description>arXiv:2302.09246v2 Announce Type: replace 
Abstract: Positronium is the simplest bound state, built of an electron and a positron. Studies of positronium in vacuum and its decays in medium tell us about Quantum Electrodynamics, QED, and about the structure of matter and biological processes of living organisms at the nanoscale, respectively. Spectroscopic measurements constrain our understanding of QED bound state theory. Searches for rare decays and measurements of the effect of gravitation on positronium are used to look for new physics phenomena. In biological materials positronium decays are sensitive to the inter- and intra-molecular structure and to the metabolism of living organisms ranging from single cells to human beings. This leads to new ideas of positronium imaging in medicine using the fact that during positron emission tomography (PET) as much as 40% of positron annihilation occurs through the production of positronium atoms inside the patient's body. A new generation of the high sensitivity and multi-photon total-body PET systems opens perspectives for clinical applications of positronium as a biomarker of tissue pathology and the degree of tissue oxidation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09246v2</guid>
      <category>physics.med-ph</category>
      <category>hep-ph</category>
      <category>physics.ins-det</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/RevModPhys.95.021002</arxiv:DOI>
      <arxiv:journal_reference>Rev. Mod. Phys. 95 (2023), 021002</arxiv:journal_reference>
      <dc:creator>Steven D. Bass, Sebastiano Mariazzi, Pawel Moskal, Ewa Stepien</dc:creator>
    </item>
    <item>
      <title>Reproducibility Made Easy: A Tool for Methodological Transparency and Efficient Standardized Reporting based on the proposed MRSinMRS Consensus</title>
      <link>https://arxiv.org/abs/2403.19594</link>
      <description>arXiv:2403.19594v2 Announce Type: replace 
Abstract: A recent expert consensus found that non-standard reporting in MRS studies led to poor reproducibility. In order to address this, MRSinMRS guidelines were introduced; however, because of the disparate nomenclature and data formats, adoption has been slow. To get around this problem, REMY, a toolbox that supports major vendor formats, was created. By efficiently filling in important fields in the MRSinMRS table, it improves reproducibility. Even with certain hardware-related restrictions, REMY makes a substantial contribution to the completion of acquisition parameters, which facilitates reporting. Its compatibility and user-friendly interface should promote widespread adoption of MRSinMRS, raising the caliber of MRS research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19594v2</guid>
      <category>physics.med-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Antonia Susnjar, Antonia Kaiser, Dunja Simicic, Gianna Nossa, Alexander Lin, Georg Oeltzschner, Aaron Gudmundson</dc:creator>
    </item>
    <item>
      <title>Investigation of Novel Preclinical Total Body PET Designed With J-PET Technology:A Simulation Study</title>
      <link>https://arxiv.org/abs/2408.00574</link>
      <description>arXiv:2408.00574v2 Announce Type: replace 
Abstract: The growing interest in human-grade total body positron emission tomography (PET) systems has also application in small animal research. Due to the existing limitations in human-based studies involving drug development and novel treatment monitoring, animal-based research became a necessary step for testing and protocol preparation. In this simulation-based study two unconventional, cost-effective small animal total body PET scanners (for mouse and rat studies) have been investigated in order to inspect their feasibility for preclinical research. They were designed with the novel technology explored by the Jagiellonian-PET (J-PET) Collaboration. Two main PET characteristics: sensitivity and spatial resolution were mainly inspected to evaluate their performance. Moreover, the impact of the scintillator dimension and time-of-flight on the latter parameter was examined in order to design the most efficient tomographs. The presented results show that for mouse TB J-PET the achievable system sensitivity is equal to 2.35% and volumetric spatial resolution to 9.46 +- 0.54 mm3, while for rat TB J-PET they are equal to 2.6% and 14.11 +- 0.80 mm3, respectively. Furthermore, it was shown that the designed tomographs are almost parallax-free systems, hence, they resolve the problem of the acceptance criterion tradeoff between enhancing spatial resolution and reducing sensitivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00574v2</guid>
      <category>physics.med-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Dadgar, S. Parzych, F. Tayefi Ardebili, J. Baran, N. Chug, C. Curceanu, E. Czerwinski, K. Dulski, K. Eliyan, A. Gajos, B. C. Hiesmayr, K. Kacprzak, L. Kaplon, K. Klimaszewski, P. Konieczka, G. Korcyl, T. Kozik, W. Krzemien, D. Kumar, S. Niedzwiecki, D. Panek, E. Perez del Rio, L. Raczynski, S. Sharma,  Shivani, R. Y. Shopa, M. Skurzok, K. Tayefi Ardebili, S. Vandenberghe, W. Wislicki, E. Stepien, P. Moskal</dc:creator>
    </item>
    <item>
      <title>Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study</title>
      <link>https://arxiv.org/abs/2401.10107</link>
      <description>arXiv:2401.10107v4 Announce Type: replace-cross 
Abstract: Study Objectives: Polysomnography (PSG) currently serves as the benchmark for evaluating sleep disorders. Its discomfort makes long-term monitoring unfeasible, leading to bias in sleep quality assessment. Hence, less invasive, cost-effective, and portable alternatives need to be explored. One promising contender is the in-ear-EEG sensor. This study aims to establish a methodology to assess the similarity between the single-channel in-ear-EEG and standard PSG derivations.
  Methods: The study involves four-hour signals recorded from ten healthy subjects aged 18 to 60 years. Recordings are analyzed following two complementary approaches: (i) a hypnogram-based analysis aimed at assessing the agreement between PSG and in-ear-EEG-derived hypnograms; and (ii) a feature-based analysis based on time- and frequency- domain feature extraction, unsupervised feature selection, and definition of Feature-based Similarity Index via Jensen-Shannon Divergence (JSD-FSI).
  Results: We find large variability between PSG and in-ear-EEG hypnograms scored by the same sleep expert according to Cohen's kappa metric, with significantly greater agreements for PSG scorers than for in-ear-EEG scorers (p &lt; 0.001) based on Fleiss' kappa metric. On average, we demonstrate a high similarity between PSG and in-ear-EEG signals in terms of JSD-FSI (0.79 +/- 0.06 -awake, 0.77 +/- 0.07 -NREM, and 0.67 +/- 0.10 -REM) and in line with the similarity values computed independently on standard PSG-channel-combinations.
  Conclusions: In-ear-EEG is a valuable solution for home-based sleep monitoring, however further studies with a larger and more heterogeneous dataset are needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10107v4</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>physics.med-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gianpaolo Palo, Luigi Fiorillo, Giuliana Monachino, Michal Bechny, Michel Walti, Elias Meier, Francesca Pentimalli Biscaretti di Ruffia, Mark Melnykowycz, Athina Tzovara, Valentina Agostini, Francesca Dalia Faraci</dc:creator>
    </item>
  </channel>
</rss>
