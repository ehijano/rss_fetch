<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Oct 2025 01:46:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Two-Stage Sketch-Based Smoke Illustration Generation using Stream Function</title>
      <link>https://arxiv.org/abs/2510.15873</link>
      <description>arXiv:2510.15873v1 Announce Type: new 
Abstract: In this paper, we propose a two-stage sketch-based smoke illustration generation framework using stream function and latent diffusion models (LDM). The user sketch is used to guide the generation of the stream function, which serves as the control condition for the velocity field generator. The generated velocity field can be used to guide the smoke simulation to align with the intended flow. We adopt streamlines to encode global flow dynamics as sketch guidance during training. The stream function constitutes the intermediate representation that captures continuous variation and rotational flow details absent from sketches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15873v1</guid>
      <category>cs.GR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hengyuan Chang, Xiaoxuan Xie, Syuhei Sato, Haoran Xie</dc:creator>
    </item>
    <item>
      <title>Sketch-based Fluid Video Generation Using Motion-Guided Diffusion Models in Still Landscape Images</title>
      <link>https://arxiv.org/abs/2510.15874</link>
      <description>arXiv:2510.15874v1 Announce Type: new 
Abstract: Integrating motion into static images not only enhances visual expressiveness but also creates a sense of immersion and temporal depth, establishing it as a longstanding and impactful theme in artistic expression. Fluid elements such as waterfall, river, and oceans are common features in landscape, but their complex dynamic characteristics pose significant challenges in modeling and controlling their motion within visual computing. Physics-based methods are often used in fluid animation to track particle movement. However, they are easily affected by boundary conditions. Recently, latent diffusion models have been applied to video generation tasks, demonstrating impressive capabilities in producing high-quality and temporally coherent results. However, it is challenging for the existing methods to animate fluid smooth and temporally consistent motion. To solve these issues, this paper introduces a framework for generating landscape videos by animating fluid in still images under the guidance of motion sketches. We propose a finetuned conditional latent diffusion model for generating motion field from user-provided sketches, which are subsequently integrated into a latent video diffusion model via a motion adapter to precisely control the fluid movement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15874v1</guid>
      <category>cs.GR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Jin, Haoran Xie</dc:creator>
    </item>
    <item>
      <title>Adaptive Frameless Rendering</title>
      <link>https://arxiv.org/abs/2510.15876</link>
      <description>arXiv:2510.15876v1 Announce Type: new 
Abstract: We propose an adaptive form of frameless rendering with the potential to dramatically increase rendering speed over conventional interactive rendering approaches. Without the rigid sampling patterns of framed renderers, sampling and reconstruction can adapt with very fine granularity to spatio-temporal color change. A sampler uses closed-loop feedback to guide sampling toward edges or motion in the image. Temporally deep buffers store all the samples created over a short time interval for use in reconstruction and as sampler feedback. GPU-based reconstruction responds both to sampling density and space-time color gradients. Where the displayed scene is static, spatial color change dominates and older samples are given significant weight in reconstruction, resulting in sharper and eventually antialiased images. Where the scene is dynamic, more recent samples are emphasized, resulting in less sharp but more up-to-date images. We also use sample reprojection to improve reconstruction and guide sampling toward occlusion edges, undersampled regions, and specular highlights. In simulation our frameless renderer requires an order of magnitude fewer samples than traditional rendering of similar visual quality (as measured by RMS error), while introducing overhead amounting to 15% of computation time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15876v1</guid>
      <category>cs.GR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.2312/EGWR/EGSR05/265-275</arxiv:DOI>
      <arxiv:journal_reference>Proc. Eurographics Symposium on Rendering (Konstanz, June), 265-275. 2005</arxiv:journal_reference>
      <dc:creator>Abhinav Dayal, Cliff Woolley, Benjamin Watson, David Luebke</dc:creator>
    </item>
    <item>
      <title>Procedural modeling of urban land use</title>
      <link>https://arxiv.org/abs/2510.15877</link>
      <description>arXiv:2510.15877v1 Announce Type: new 
Abstract: Cities are important elements of content in digital productions, but their complexity and size make them very challenging to model. Few tools exist that can help artists with this work, even as rapid improvements in graphics hardware create demand for richer content without matching increases in production cost. We propose a method for procedurally generating realistic patterns of land use in cities, automating placement of buildings and roads for artists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15877v1</guid>
      <category>cs.GR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Lechner, Ben Watson, Uri Wilenski, Seth Tisue, Martin Felsen, Andy Moddrell, Pin Ren, Craig Brozefsky</dc:creator>
    </item>
    <item>
      <title>Structural Tree Extraction from 3D Surfaces</title>
      <link>https://arxiv.org/abs/2510.15886</link>
      <description>arXiv:2510.15886v1 Announce Type: new 
Abstract: This paper introduces a method to extract a hierarchical tree representation from 3D unorganized polygonal data. The proposed approach first extracts a graph representation of the surface, which serves as the foundation for structural analysis. A Steiner tree is then generated to establish an optimized connection between key terminal points, defined according to application-specific criteria. The structure can be further refined by leveraging line-of-sight constraints, reducing redundancy while preserving essential connectivity. Unlike traditional skeletonization techniques, which often assume volumetric interpretations, this method operates directly on the surface, ensuring that the resulting representation remains relevant for navigation-aware geometric analysis. The method is validated through two use cases: extracting structural representations from tile-based elements for procedural content generation, and identifying key points and structural metrics for automated level analysis. Results demonstrate its ability to produce simplified, coherent representations, supporting applications in procedural generation, spatial reasoning, and map analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15886v1</guid>
      <category>cs.GR</category>
      <category>cs.CG</category>
      <category>cs.HC</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diogo de Andrade, Nuno Fachada</dc:creator>
    </item>
    <item>
      <title>Procedural Scene Programs for Open-Universe Scene Generation: LLM-Free Error Correction via Program Search</title>
      <link>https://arxiv.org/abs/2510.16147</link>
      <description>arXiv:2510.16147v1 Announce Type: new 
Abstract: Synthesizing 3D scenes from open-vocabulary text descriptions is a challenging, important, and recently-popular application. One of its critical subproblems is layout generation: given a set of objects, lay them out to produce a scene matching the input description. Nearly all recent work adopts a declarative paradigm for this problem: using an LLM to generate a specification of constraints between objects, then solving those constraints to produce the final layout. In contrast, we explore an alternative imperative paradigm, in which an LLM iteratively places objects, with each object's position and orientation computed as a function of previously-placed objects. The imperative approach allows for a simpler scene specification language while also handling a wider variety and larger complexity of scenes. We further improve the robustness of our imperative scheme by developing an error correction mechanism that iteratively improves the scene's validity while staying as close as possible to the original layout generated by the LLM. In forced-choice perceptual studies, participants preferred layouts generated by our imperative approach 82% and 94% of the time when compared against two declarative layout generation methods. We also present a simple, automated evaluation metric for 3D scene layout generation that aligns well with human preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16147v1</guid>
      <category>cs.GR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxim Gumin, Do Heon Han, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Kailiang Fu, Rio Aguina-Kang, Stewart Morris, Daniel Ritchie</dc:creator>
    </item>
    <item>
      <title>Region-Aware Wasserstein Distances of Persistence Diagrams and Merge Trees</title>
      <link>https://arxiv.org/abs/2510.16486</link>
      <description>arXiv:2510.16486v1 Announce Type: new 
Abstract: This paper presents a generalization of the Wasserstein distance for both persistence diagrams and merge trees [20], [66] that takes advantage of the regions of their topological features in the input domain. Specifically, we redefine the comparison of topological features as a distance between the values of their extrema-aligned regions. It results in a more discriminative metric than the classical Wasserstein distance and generalizes it through an input parameter adjusting the impact of the region properties in the distance. We present two strategies to control both computation time and memory storage of our method by respectively enabling the use of subsets of the regions in the computation, and by compressing the regions' properties to obtain low-memory representations. Extensive experiments on openly available ensemble data demonstrate the efficiency of our method, with running times on the orders of minutes on average. We show the utility of our contributions with two applications. First, we use the assignments between topological features provided by our method to track their evolution in time-varying ensembles and propose the temporal persistence curves to facilitate the understanding of how these features appear, disappear and change over time. Second, our method allows to compute a distance matrix of an ensemble that can be used for dimensionality reduction purposes and visually represent in 2D all its members, we show that such distance matrices also allow to detect key phases in the ensemble. Finally, we provide a C++ implementation that can be used to reproduce our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16486v1</guid>
      <category>cs.GR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathieu Pont, Christoph Garth</dc:creator>
    </item>
    <item>
      <title>Filtering of Small Components for Isosurface Generation</title>
      <link>https://arxiv.org/abs/2510.16684</link>
      <description>arXiv:2510.16684v1 Announce Type: new 
Abstract: Let $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ be a scalar field. An isosurface is a piecewise linear approximation of a level set $f^{-1}(\sigma)$ for some $\sigma \in \mathbb{R}$ built from some regular grid sampling of $f$. Isosurfaces constructed from scanned data such as CT scans or MRIs often contain extremely small components that distract from the visualization and do not form part of any geometric model produced from the data. Simple prefiltering of the data can remove such small components while having no effect on the large components that form the body of the visualization. We present experimental results on such filtering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16684v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Devin Zhao, Rephael Wenger</dc:creator>
    </item>
    <item>
      <title>A Scalable In Transit Solution for Comprehensive Exploration of Simulation Data</title>
      <link>https://arxiv.org/abs/2510.16966</link>
      <description>arXiv:2510.16966v1 Announce Type: new 
Abstract: As simulations produce more data than available disk space on supercomputers, many simulations are employing in situ analysis and visualization to reduce the amount of data that needs to be stored. While in situ visualization offers potential for substantial data reduction, its efficacy is hindered by the need for a priori knowledge. First, we need to know what visualization parameters to use to highlight features of interest. Second, we do not know ahead of time how much resources will be needed to run the in situ workflows, e.g. how many compute nodes will be needed for in situ work. In this work, we present SeerX, a lightweight, scalable in-transit in situ service that supports dynamic resource allocation and lossy compression of 3D simulation data. SeerX enables multiple simulations to offload analysis to a shared, elastic service infrastructure without MPI synchronization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16966v1</guid>
      <category>cs.GR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paascal Grosset, James Ahrens</dc:creator>
    </item>
    <item>
      <title>Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors</title>
      <link>https://arxiv.org/abs/2510.17101</link>
      <description>arXiv:2510.17101v1 Announce Type: new 
Abstract: Human motion capture with sparse inertial sensors has gained significant attention recently. However, existing methods almost exclusively rely on a template adult body shape to model the training data, which poses challenges when generalizing to individuals with largely different body shapes (such as a child). This is primarily due to the variation in IMU-measured acceleration caused by changes in body shape. To fill this gap, we propose Shape-aware Inertial Poser (SAIP), the first solution considering body shape differences in sparse inertial-based motion capture. Specifically, we decompose the sensor measurements related to shape and pose in order to effectively model their joint correlations. Firstly, we train a regression model to transfer the IMU-measured accelerations of a real body to match the template adult body model, compensating for the shape-related sensor measurements. Then, we can easily follow the state-of-the-art methods to estimate the full body motions of the template-shaped body. Finally, we utilize a second regression model to map the joint velocities back to the real body, combined with a shape-aware physical optimization strategy to calculate global motions on the subject. Furthermore, our method relies on body shape awareness, introducing the first inertial shape estimation scheme. This is accomplished by modeling the shape-conditioned IMU-pose correlation using an MLP-based network. To validate the effectiveness of SAIP, we also present the first IMU motion capture dataset containing individuals of different body sizes. This dataset features 10 children and 10 adults, with heights ranging from 110 cm to 190 cm, and a total of 400 minutes of paired IMU-Motion samples. Extensive experimental results demonstrate that SAIP can effectively handle motion capture tasks for diverse body shapes. The code and dataset are available at https://github.com/yinlu5942/SAIP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17101v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Yin, Ziying Shi, Yinghao Wu, Xinyu Yi, Feng Xu, Shihui Guo</dc:creator>
    </item>
    <item>
      <title>A virtual airplane for fear of flying therapy</title>
      <link>https://arxiv.org/abs/2510.15875</link>
      <description>arXiv:2510.15875v1 Announce Type: cross 
Abstract: Fear of flying is a serious problem that affects millions of individuals. Exposure therapy for fear of flying is an effective therapy technique. However, exposure therapy is also expensive, logistically difficult to arrange, and presents significant problems of patient confidentiality and potential embarrassment. We have developed a virtual airplane for use in fear of flying therapy. Using the virtual airplane for exposure therapy is a potential solution to many of the current problems of fear of flying exposure therapy. We describe the design of the virtual airplane and present a case report on its use for fear of flying exposure therapy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15875v1</guid>
      <category>physics.med-ph</category>
      <category>cs.GR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/VRAIS.1996.490515</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium Pages 86-93</arxiv:journal_reference>
      <dc:creator>Larry F Hodges, Barbara O Rothbaum, Benjamin Watson, G Drew Kessler, Dan Opdyke</dc:creator>
    </item>
    <item>
      <title>GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer</title>
      <link>https://arxiv.org/abs/2510.16136</link>
      <description>arXiv:2510.16136v1 Announce Type: cross 
Abstract: Transferring appearance to 3D assets using different representations of the appearance object - such as images or text - has garnered interest due to its wide range of applications in industries like gaming, augmented reality, and digital content creation. However, state-of-the-art methods still fail when the geometry between the input and appearance objects is significantly different. A straightforward approach is to directly apply a 3D generative model, but we show that this ultimately fails to produce appealing results. Instead, we propose a principled approach inspired by universal guidance. Given a pretrained rectified flow model conditioned on image or text, our training-free method interacts with the sampling process by periodically adding guidance. This guidance can be modeled as a differentiable loss function, and we experiment with two different types of guidance including part-aware losses for appearance and self-similarity. Our experiments show that our approach successfully transfers texture and geometric details to the input 3D asset, outperforming baselines both qualitatively and quantitatively. We also show that traditional metrics are not suitable for evaluating the task due to their inability of focusing on local details and comparing dissimilar inputs, in absence of ground truth data. We thus evaluate appearance transfer quality with a GPT-based system objectively ranking outputs, ensuring robust and human-like assessment, as further confirmed by our user study. Beyond showcased scenarios, our method is general and could be extended to different types of diffusion models and guidance functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16136v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sayan Deb Sarkar, Sinisa Stekovic, Vincent Lepetit, Iro Armeni</dc:creator>
    </item>
    <item>
      <title>Predictability of Complex Systems</title>
      <link>https://arxiv.org/abs/2510.16312</link>
      <description>arXiv:2510.16312v1 Announce Type: cross 
Abstract: The study of complex systems has attracted widespread attention from researchers in the fields of natural sciences, social sciences, and engineering. Prediction is one of the central issues in this field. Although most related studies have focused on prediction methods, research on the predictability of complex systems has received increasing attention across disciplines--aiming to provide theories and tools to address a key question: What are the limits of prediction accuracy? Predictability itself can serve as an important feature for characterizing complex systems, and accurate estimation of predictability can provide a benchmark for the study of prediction algorithms. This allows researchers to clearly identify the gap between current prediction accuracy and theoretical limits, thereby helping them determine whether there is still significant room to improve existing algorithms. More importantly, investigating predictability often requires the development of new theories and methods, which can further inspire the design of more effective algorithms. Over the past few decades, this field has undergone significant evolution. In particular, the rapid development of data science has introduced a wealth of data-driven approaches for understanding and quantifying predictability. This review summarizes representative achievements, integrating both data-driven and mechanistic perspectives. After a brief introduction to the significance of the topic in focus, we will explore three core aspects: the predictability of time series, the predictability of network structures, and the predictability of dynamical processes. Finally, we will provide extensive application examples across various fields and outline open challenges for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16312v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.GR</category>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MP</category>
      <category>nlin.AO</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>En Xu, Yilin Bi, Hongwei Hu, Xin Chen, Zhiwen Yu, Yong Li, Yanqing Hu, Tao Zhou</dc:creator>
    </item>
    <item>
      <title>From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display</title>
      <link>https://arxiv.org/abs/2510.16833</link>
      <description>arXiv:2510.16833v1 Announce Type: cross 
Abstract: Mannequin-based clothing displays offer a cost-effective alternative to real-model showcases for online fashion presentation, but lack realism and expressive detail. To overcome this limitation, we introduce a new task called mannequin-to-human (M2H) video generation, which aims to synthesize identity-controllable, photorealistic human videos from footage of mannequins. We propose M2HVideo, a pose-aware and identity-preserving video generation framework that addresses two key challenges: the misalignment between head and body motion, and identity drift caused by temporal modeling. In particular, M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial semantics with body pose to produce consistent identity embeddings across frames. To address the loss of fine facial details due to latent space compression, we introduce a mirror loss applied in pixel space through a denoising diffusion implicit model (DDIM)-based one-step denoising. Additionally, we design a distribution-aware adapter that aligns statistical distributions of identity and clothing features to enhance temporal coherence. Extensive experiments on the UBC fashion dataset, our self-constructed ASOS dataset, and the newly collected MannequinVideos dataset captured on-site demonstrate that M2HVideo achieves superior performance in terms of clothing consistency, identity preservation, and video fidelity in comparison to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16833v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangyu Mu, Dongliang Zhou, Jie Hou, Haijun Zhang, Weili Guan</dc:creator>
    </item>
    <item>
      <title>MoReFlow: Motion Retargeting Learning through Unsupervised Flow Matching</title>
      <link>https://arxiv.org/abs/2509.25600</link>
      <description>arXiv:2509.25600v2 Announce Type: replace 
Abstract: Motion retargeting holds a premise of offering a larger set of motion data for characters and robots with different morphologies. Many prior works have approached this problem via either handcrafted constraints or paired motion datasets, limiting their applicability to humanoid characters or narrow behaviors such as locomotion. Moreover, they often assume a fixed notion of retargeting, overlooking domain-specific objectives like style preservation in animation or task-space alignment in robotics. In this work, we propose MoReFlow, Motion Retargeting via Flow Matching, an unsupervised framework that learns correspondences between characters' motion embedding spaces. Our method consists of two stages. First, we train tokenized motion embeddings for each character using a VQ-VAE, yielding compact latent representations. Then, we employ flow matching with conditional coupling to align the latent spaces across characters, which simultaneously learns conditioned and unconditioned matching to achieve robust but flexible retargeting. Once trained, MoReFlow enables flexible and reversible retargeting without requiring paired data. Experiments demonstrate that MoReFlow produces high-quality motions across diverse characters and tasks, offering improved controllability, generalization, and motion realism compared to the baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25600v2</guid>
      <category>cs.GR</category>
      <category>cs.RO</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wontaek Kim, Tianyu Li, Sehoon Ha</dc:creator>
    </item>
    <item>
      <title>Creative synthesis of kinematic mechanisms</title>
      <link>https://arxiv.org/abs/2510.03308</link>
      <description>arXiv:2510.03308v2 Announce Type: replace 
Abstract: In this paper, we formulate the problem of kinematic synthesis for planar linkages as a cross-domain image generation task. We develop a planar linkages dataset using RGB image representations, covering a range of mechanisms: from simple types such as crank-rocker and crank-slider to more complex eight-bar linkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE) is employed to explore the potential of image generative models for synthesizing unseen motion curves and simulating novel kinematics. By encoding the drawing speed of trajectory points as color gradients, the same architecture also supports kinematic synthesis conditioned on both trajectory shape and velocity profiles. We validate our method on three datasets of increasing complexity: a standard four-bar linkage set, a mixed set of four-bar and crank-slider mechanisms, and a complex set including multi-loop mechanisms. Preliminary results demonstrate the effectiveness of image-based representations for generative mechanical design, showing that mechanisms with revolute and prismatic joints, and potentially cams and gears, can be represented and synthesized within a unified image generation framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03308v2</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiong Lin, Jialong Ning, Judah Goldfeder, Hod Lipson</dc:creator>
    </item>
    <item>
      <title>HUMAP: Hierarchical Uniform Manifold Approximation and Projection</title>
      <link>https://arxiv.org/abs/2106.07718</link>
      <description>arXiv:2106.07718v5 Announce Type: replace-cross 
Abstract: Dimensionality reduction (DR) techniques help analysts to understand patterns in high-dimensional spaces. These techniques, often represented by scatter plots, are employed in diverse science domains and facilitate similarity analysis among clusters and data samples. For datasets containing many granularities or when analysis follows the information visualization mantra, hierarchical DR techniques are the most suitable approach since they present major structures beforehand and details on demand. This work presents HUMAP, a novel hierarchical dimensionality reduction technique designed to be flexible on preserving local and global structures and preserve the mental map throughout hierarchical exploration. We provide empirical evidence of our technique's superiority compared with current hierarchical approaches and show a case study applying HUMAP for dataset labelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.07718v5</guid>
      <category>cs.LG</category>
      <category>cs.GR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TVCG.2024.3471181</arxiv:DOI>
      <dc:creator>Wilson E. Marc\'ilio-Jr, Danilo M. Eler, Fernando V. Paulovich, Rafael M. Martins</dc:creator>
    </item>
    <item>
      <title>Capsule: Efficient Player Isolation for Datacenters</title>
      <link>https://arxiv.org/abs/2506.11483</link>
      <description>arXiv:2506.11483v3 Announce Type: replace-cross 
Abstract: Cloud gaming is increasingly popular. A challenge for cloud provider is to keep datacenter utilization high: a non-trivial task due to application variety. These applications come in different shapes and sizes. So do cloud datacenter resources, e.g., CPUs, GPUs, NPUs. Part of the challenge stems from game engines being predominantly designed to run only one player. For example, one player in a lightweight game might utilize only a fraction of the cloud server GPU. The remaining GPU capacity will be left underutilized, an undesired outcome for the cloud provider.
  We introduce Capsule, a mechanism to seamlessly share one GPU, and other cloud servers resources, across multiple players. Sharing makes the cost of multiple players sublinear. We implemented Capsule in O3DE, a popular open source game engine. Our evaluations show that Capsule increases datacenter resource utilization by accommodating up to 2.25x more players, without degrading player gaming experience. This is the product of Capsule using up to 1.43x less GPU, 3.11x less VRAM, 3.7x less CPU, and 3.87x less RAM compared to the baseline. Capsule is also application agnostic. We ran four applications on Capsule-based O3DE with no application changes. Our experiences with four applications, three servers with different hardware specifications, including the one with four GPUs, and multi-server cluster show that Capsule design can be adopted by other game engines to increase datacenter utilization across cloud providers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11483v3</guid>
      <category>cs.DC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhouheng Du, Nima Davari, Li Li, Wei Sen Loi, Nodir Kodirov</dc:creator>
    </item>
    <item>
      <title>Agentic Design of Compositional Machines</title>
      <link>https://arxiv.org/abs/2510.14980</link>
      <description>arXiv:2510.14980v2 Announce Type: replace-cross 
Abstract: The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. With this simplification, machine design is expressed as writing XML-like code that explicitly specifies pairwise part connections. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14980v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenqian Zhang, Weiyang Liu, Zhen Liu</dc:creator>
    </item>
  </channel>
</rss>
